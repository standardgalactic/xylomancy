Yeah, and we have our another specialist speaker today.
This is such a pleasure for me to have Shiri Ghinosar.
She's a computing in innovation postdoctoral fellow at UC Berkeley, and she has done so many interesting work and very creative.
These, you know, these models of current ARs on many interesting projects that she will hopefully share a gist of that with us.
I can, you know, tell you about the understanding the evolution of images that was very striking to me as well as the dance project that she did and other interesting things that she tries to go beyond the
pixels, you know. So, Shiri, go ahead and please start your talk.
Oh, thank you so much. So, hi, I'm Shiri, and I'd like to talk to you today about the art of deception or basically using perception as a creative material.
And I have a lot of material, but I want this, I know it's kind of strange to do this over zoom. People are kind of shy to, to, to budge in, barge in, but it can really be a conversation we can take it this anywhere that you want.
It could be kind of like a fireside chat. It could be, you know, we can make it this into different things. But so I want to encourage you if you have questions, you could just like just to start talking because I don't have the chat on and I can't man all these things but just
feel free to just stop me at any point and I want to say that because I know that it's, it's, you know, this whole remote thing is kind of, you know, strange.
So, okay, so let's start talking and I want to get you into the mood. So I want us to look at a video together to start with. And this is a really nice music video by Michel Gondry that he made for the Chemical Brothers.
And I want you to really notice what is going on. I want you to listen, and I want you to look at what is going at what is going on in this video.
Thank you.
Thank you.
So, if you, if you zoned out a little bit in the beginning there because, you know, your
look at this thing and you think yourself okay, we're looking out the window or trade I've been there it's it's kind of nice and relaxing but it's also a little bit mind numbing so you kind of zone out and you're like okay, but then suddenly it hits you that there's a surprise here.
So, actually, all the visual.
Yes, I think that the, did you want the students to tell you what they think, or you just want to tell them.
They can tell me what I, I can't really see you guys I can only see you.
Okay, so if you want to say something just like say it. Yeah, if you please if your students want to say something just unmute and say, I can also actually before you do that now that we we stop.
I want to ask the audio.
Is it reasonable is it too loud is it too low of the of the videos it's a good time to fix this because we're going to have a lot of audio and videos.
I think it was reasonable maybe a little, maybe a little less like, but it was good.
So, okay, we made a little bit less love if it's not going forward complaint. Yes. Excellent. So if anyone wants to say what they think or should I read it.
So Ben says, it's repeating seen in sync to the music sections, moving on to the next loop of the scene, as the track moves to the next section, Simon says, the camera feels like it is bouncing in rhythm.
That is, that is wonderful that those are wonderful observations but like guys, just turn on your microphones and just talk okay don't make Ali read all of your all of your things.
So, it will be more just more interactive, you know.
Okay, so, yes, that's exactly what's going on. Everything is being repeated and is synced to the music and sync to the beat. And Michelle Audrey is very good at this he did this a couple of times and amazingly he did this by hand so there's a very interesting YouTube video it's
kind of like a documentary about the making of this video and you see how they charted out the entire score of the of the music and planned out exactly what they're going to show.
And the visuals and literally, literally compose this by hand there's no there's no I hear anything like that. So, the interesting thing is that this kind of combination between your different senses plays a trick on you and gives you this surprise
interesting, but if you didn't notice what was going on you would think oh yeah you know it looks like it's real, but it's not there's nothing real about this is completely composed.
Okay, so, so let's take a step back and and talk a little about but put ourselves in context and look at the history of what people did in order to depict the world from the very beginning of art.
And the main goal of art was to, you know, capture the world realistically so they started with with cave drawings of prehistoric people who saw these big cows with the big big horns and wanted to wanted to capture them with coal on the on the
walls of their caves.
So going forward many many years people got more sophisticated. This is a very nice mosaic from the Byzantine time and there's already a lot of dress here and different people and a lot of details.
And at this time, all of the depiction of people was very, very flat. So there's almost no depth in the image, and everybody is kind of frontal and has a very just nondescript facial emotions that everybody's kind of like severe.
Okay, going forward, you know this is the beginning of the Renaissance so this is Jodo, and this is one of the first times that depth appears in an image, and the way he does it is kind of it's a nice trick he puts this person in green in front of Christ, and it
draws your attention inwards into the image, and you can notice you know people are not frontal here anymore this is kind of diverging style from from the Byzantine style.
And you can see motion in their faces so it's becoming more and more realistic.
And you can see how the objects get even better in the Renaissance here we already have linear perspective we have the little people in the back, you know are smaller and we have people in the front which are bigger and it kind of makes you feel like everything is going into the
scene and you can kind of walk into the scene. And, of course, going a little bit forward, you know we almost achieve perfection in painting so this is like, and this is a beautiful painting where he has here geometric orthogonal perspective.
And there's, you know you can really walk into this image. There's a lot of detail. If you blow up, for example the chandelier you can see how the light is bouncing on the metal, the little flames of the candles are very very nicely painted everything is very realistic.
And even better he puts a mirror behind those, those two people who are getting married here, and you can get even a double depth in the image so you can walk into the image and you can even see the backs of the people and the people who are looking at the people who are getting married.
And this is is really wonderful. And even you can see the, the little beads, the glass, the glass beads that are hanging up. It just is, is almost perfect.
Of course all of this this reflection basically ended with the invention of the camera, because then you could just walk out or look out of your window and take a snapshot and you're done right you know this is like, there you go, this is the world the world, you know, it is this is it.
Anything you want you could take a picture of, and it kind of made this, this break in the art world because suddenly, you know the people who would do your, your portrait for a lot of money we're out of, we're out of, we're out of work in a way.
Of course this raises different questions so once you have an image that you can take and the pixels are real, then the question can become about whether, whether is this a real picture is this fake you know, is this kiss that happened in Paris was it was it really
real or was it staged this one was staged by the way.
Our aliens actually descending on New York or is this a movie you know is this a conspiracy theory or that people actually land on the moon or you know can I believe the pixels of the camera or not.
Interestingly, and interestingly for us, artists didn't actually give up after the invention of the camera, they actually became more sophisticated.
So, here there is a nice example from Monet it's an impressionist painting, and you can see a parade in Paris there's a lot of people going and walking in the street.
And you know let's try, let's try to get you talking a little bit so like what do you notice in this image in this painting.
Anybody. What's interesting about it. There's a lot of flags. There's a lot of flags, yes. And what's happening to the flags.
Oh, they're waving. So it's like depicting motion.
And they're kind of blown in the wind.
Why do you think that they're blowing in the wind what makes you think that they're blurry.
That's true. They are blurry.
But are they really blurry, like is it is it fuzzy like a Gaussian filter on top of them were really right blow up a section here.
They're not like blurry in the in the normal sense but they're not rigidly defined so like the looseness of the brushstrokes and the way that they all kind of have that implies the wind and the motion.
Exactly, exactly. So, these flags are basically built out of strips of blue and yellow and red which is the French plan.
But they're not, it's spatially imprecise they're kind of jumbled strokes of paint even though each stroke of paint is not blurry, the juxtaposition of them is is kind of all over the place.
And the interesting thing about this is that this kind of matches really nicely to our peripheral vision. So in a peripheral vision.
We see things kind of in a spatially imprecise way it's not that we see things blurry. It's just that we don't really care where exactly they are in the image.
So, if you kind of look at this painting in a glance you kind of think to yourself oh everything is fine.
You actually look and take a very close look at it you see that everything is really really jumbled because you're using your, your actual, you know, attentive vision in the middle of your of your of your viewing field and that is more takes more into account the spatial arrangement of stuff.
When you get a sense of this painting you have to kind of look in every time you take a good look at it you get a different sense of what's going on because your periphery is giving you a different, a different sensation.
And that's what kind of gives this this vibrant motion. So, in a way, you know, you get an impression that's why they call it impressionist because you get an impression of what's going on at every glance but it's not really correct.
It's just how Monet is playing with your visual system and making you feel like there's something real here like motion which is really not in the pixels themselves.
Okay, so, you know, fast forward many many years and we have computer graphics.
I interrupt and be a little perfectionist I think that I really love these slides. There is this panel, build order from, you know, that maybe you want to close it.
Oh, no.
Oh, no, thank you.
You know this happened to me before and I should know this already.
Okay, thank you.
Oh, why don't you say something before. Okay, good to make a postman always always close the build order.
Okay, so we went to computer graphics right and suddenly oh no no I don't have my little timer. Okay, I'm going to have to look at the clock.
And suddenly, you could model the world physically right you can make a physical model of the light and the, and the objects in the scene and you can render them perfectly.
But you could almost render them to perfect.
This is, you know what you get out of of graphics, a lot of the time is super super real.
It kind of makes you feel and gives you a strange feeling it's kind of too realistic you know everything is a little bit too clean.
Everything is a little bit too robotic.
Do you do you know why that is why does it make you feel that way.
Okay, so I'll give you my idea my idea is that what's missing is is the noise what's missing is the junk is the dirt is the cracks in the sidewalk that are kind of random.
The suit on the buildings you know all the, all the, all the noise all the junk basically where our visual system is really used to seeing all this noise in in the outside world and there's a lot of beauty in this complexity that when you take it away because
you can't model it physically it's too complex to model, you know our visual system just jumps and is like oh something is really wrong with this image.
This is very important perceptually, and a lot of what I do is try to capture this complexity of the of the visual world.
Okay, so we talked a little bit of our perception.
I want to make one more note about it is that a lot of perception is really in your head. It's not really what you see. Okay, so here's an example.
If you look at this image and you look at the thing that's that's in a red box what is what do you see a player.
A player. Yes, soccer player. Yes, great soccer player. But do you really think that there's a soccer player there like if I take him and I magnify him.
Yeah, it's just a bunch of pixels right it's like there's white pixels there's black pixels you can't really see the head there's a little bit of legs there but there's really you know there's there's really no player here right do you agree.
Like, if you look at this this it's like, so a lot of what makes you think there's a soccer player here is actually the context you know there's a soccer field it's in situation you're kind of like filling in the details.
Here's another example. This is an image from a video that Antonio shot and it's kind of blurry but but your brain can fill in the details what are you what do you see here.
A man sitting at a computer with like a phone to his ear.
Exactly. But if I show you the details, you suddenly see that everything is wrong but he's not talking on the phone he's talking in the shoe, and he's not looking at a computer he's looking at a, at a garbage can.
Okay, and the mouse is actually a stapler and there's a toaster there so everything is wrong, but the spatial configuration of the scene is fine and if you blur it out your brain is just like oh this is fine you know I'm just going to fill in the details.
So, there are these loopholes of perception, and this is a great opportunity for design.
And this is how we're going to weave in things that are not real, but kind of make you think that they are, as long as we're careful to guard the important bits, there are some, there are some anchors of perception that we should care about.
And we have to make sure that they're there, and your brain is going to do the rest.
So, what I'm going to cover today is I'm going to look at multiple examples of this so we're going to look a little bit at work that looks at different senses and putting them together so here we're going to talk about audio and motion so vision and hearing.
We're going to talk about modeling these complexities so the very, very fine details of individual appearance, and we're going to talk about using time or using the passion, the passing of time as a creative material.
I'm going to stop here and ask if there's any questions or complaints and then I will continue.
So let's talk first about audio and emotion.
We're going to talk about how people move when they speak, and it's the, the, what I'm talking about is going to look something like this.
I know how was it the personality travels, maybe there'll be some sort of physical explanation for it.
So people move when they speak. And these are called conversational gestures. Okay, there's the kind of stuff that we do when, when we speak and we never do basically when we don't, when we don't talk.
And this type of gesture is not the only form of communicative gesture. There's a continuum from language that accompanies speech. Sorry, language that accompanies motion that accompanies language to motion that replaces language.
So, for example, sign language is a language of its own. It doesn't need speech to go with it.
And blends are like Italian items, all kinds of things that Italians do, which kind of have meanings that people agree upon. So it's almost like a language.
But we're not going to talk about these things. We're going to talk about the motion that accompanies speech when you do talk.
So what we want here is you want to learn about how people use gestures when they speak. And to do that, we're going to take in a raw audio signal of speech so literally the waveform.
And from that we want to directly predict hand and arm gestures. So it's going to look something like this.
So this is a really, really hard question. Okay, there's, it's an ill-defined problem. There's not a one-to-one correspondence between the audio and the motion because I can say something today and move in a particular way and do it a completely different tomorrow.
It's not synchronous. The motion is often not synchronous to the related utterance. And it's also a task that would be really hard for people to do or even to annotate for you. So getting supervised learning in this setup is really, really hard.
And what we want to do is we want to, you know, learn about this in a kind of in the wild setting. So what we did is we went and collected a large data set of people who are speaking and for each frame, we annotated it automatically using an out-of-the-box 2D pose detection.
So it kind of finds the pose of the arms of the hand and the hands of the people. And the data looks kind of like this.
Why are you waiting outside? Why are you telling me all this? And you're not going to believe what they said they want to do.
Isn't that disgusting? It's 2012. We're still not on the... and then report it to the police. Even Lauer's conversation...
So you can already see that people are very different in how they gesticulate. But within a person, there's a lot of repetition. And here I'm showing clusters in the rows of clusters of gestures.
And this is because people just tend to perform the same motions over and over again because they have their, you know, typical style.
And that's great because it gives us a learning signal. And so what we're going to do here is we're going to model each person individually.
And the way we're going to predict gestures from audio is we're literally going to take the raw audio as input. We're going to treat it like an image.
So we're going to think about it as a spectrogram. We're going to stick it into a neural network and we're going to output a temporal series of poses.
And what each one of these really is, is just a vector of numbers, but they represent the pose of the arms and hands of a person.
And the result looks like this.
So, you know, try and separate the two.
Okay, now the good news is the one thing to notice is, you know, for this given audio, we predict a stack of these poses, and we have two kinds of losses.
One is the regression loss to this pseudo ground truth of 2d poses that we have.
But what we really want is we want to generate motion according to the style of this particular person.
And so we add another adversarial loss that will tell us whether the motion is real or not with respect to this person.
And this makes a really big different perceptually, because I'm going to show you here on the left, you're going to see the result when you only have a regression.
So what happens when you do that is that you kind of get something that's very close to the mean so the motion is just very, very slow it's kind of like you're going through honey.
And when you add this adversarial loss you kind of snap to one mode of the output because you know I could have predicted different gestures but I'm going to pick just one and make it look real.
Okay, so that's going to be on the right hand side.
And it makes us feel a little bit better about what we're seeing.
And here by the way I'm pasting in the face I'm not predicting the face in this work but I'm pasting in the ground truth face to give you more perceptual context to see what to kind of understand what you're seeing.
Okay, so we can also look at prediction results for different people and just look what look at what it looks like.
Talking about the instantaneous rate, the rate just when the concentration and I putting the ground truth video on the bottom right even though you know I could really have predicted any realistic motion I don't really want to predict the real one but it's just for reference.
Energy to boil higher kinetic energy higher temperature.
So go in order to make it more modern.
Yeah.
Yeah, appropriate noise for that. Thank you.
So here this method fails. So we're only taking audio is input there's no, like, there's no new text there's no semantics. So the main limitation here is that even though we're using hours and of data for training.
So we really, there's really not enough data to capture the cement, you know, fine grain semantics in order to predict metaphoric testers so I'm going to show you a video here and I want you to notice what happens when he says the word random.
So we don't get this.
The more the circular motion that goes with random we just can't predict that. But we do predict the beat motion, which is these up and down repetitive motion that kind of cut the sentence temporally and make us feel like the person is actually moving while they're talking, which is which is nice.
And one thing that's that's interesting about this is that it's a little bit hard to do this kind of work the way that I described it. And that is because the most important part of the body when you're speaking with your hands basically is is the hands, right.
It's a very, very small in an image, and they're very articulated the fingers, and we're just not there yet in terms of computer vision 3D reconstruction techniques to get really good hand estimations.
So our entire everything we did depends on the fact that we have good, good, you know, detections of body pose and that's not the case so here's an example, just a randomly picked example from Ellen.
Ellen's videos and you can see what happens if you just use an image based system to get the hand reconstructions.
And that's for one item. And the receipt was so long that I couldn't even believe it I called it outrageous I called it mind boggling I called it long.
So this is, right, like if you try to request to this and everything would with fail miserably. So what do you do.
One thing that we noticed that is actually super interesting is that there is a really high correlation between the motion of the arms and the shape of the hand.
So we're going to do a trick, and we're going to take as input not only the audio, if, if we want to get better hands but you can look at just taking the arms as input, and only from the arms, the arm motion, you can predict a pretty good.
shape of the hands which is which is almost seems magical. And so if you do this body to hand thing, what you get looks like this.
Electrons around it.
The atomic oxygen would have six. So it has to more than it normally has. So there's no pick.
There's no audio going into here. There's literally just the motion of the arms and it's, it's, it's amazing that it even works.
Again, there's not enough information right so it's not enough to capture metaphoric so if you see here.
She's going to.
And this is the ground truth video okay.
To a CBS for one item. And you're not going to get that it looks like this to a CBS for one item, and the receipt was so long that I couldn't even believe an extra trick and you can say okay well I can take in the body but I can also just look at the input images because
that's what image based reconstruction does anyways, and together with this body prior, I can get a much better hand reconstruction.
And so here on the left is the is the body only input no audio no pixels. And on the right, I have the body and the image together.
To a CBS for one item, and the receipt was so long that I couldn't even believe it I called it outrageous I called it mind boggling I called it long.
So this is already better now this time like you know this is stuff that we can work with already and it looks much more realistic.
Are there any basic restrictions according to just like human anatomy that are also used within the model.
Sorry saying it.
Are there any just base restrictions on what movement is possible according to human anatomy like you can't complete leads. I don't know. Yeah.
And not.
Sorry, I'm speaking here from the garage so there's there's exciting background noise and not in my model. Okay, but there is in those.
When we use ground truth that's coming from 2D reconstruction of key points or 3D reconstruction of hands in this case, those models have a lot of, you know, human pose priors built into them, but we don't.
So, we're at we're kind of the goal of the, of the body to hands angle is to add an additional fire but one that is not based on, you know, physics that you might calculate from human bodies but it's based on a data driven prior.
So if you've seen enough bodies, you can infer this automatically. Does that answer your question.
Yes, thank you.
All right. So, let's see what time is it when do we stop at on the hour right.
Yeah, I mean we could, we could go a little further if you want. No, no, no, I'm just like I can I can I have a lot of stuff but I'm just going to cut accordingly. Okay, so I'm going to show you the more interesting.
I think that we said 2pm for students but it's. Yeah, that's fine. Okay.
So, okay. So, in this work, we really only care to predict 2D motion because we that's what we were, we were interested in. And these stick figures are a nice output representation.
But they don't really provide you enough perceptual context, as a viewer to actually see that what the result looks like it would do a good job. Okay.
And we can synthesize a video, an actual video of the speaker so we what are we going to do we're going to take a real video of the speaker, and for each frame, we can do the same trick where we get a 2d pose detection.
The goal is to learn the mapping between these 2d pose skeletons back to the real frame of the person and this is based on pics to pics you know you probably you know we saw a lot so this is this is based on his work, but we're going to do this for a video.
And if you do that, what you get is something like this, where again, I'm pasting in the ground truth face key points because I'm not predicting that but I need them to make a video right so it's going to look like this.
So, you know, try and separate the two. Now, now the good news is, these days, very few people will say they are completely.
Okay, so I just want to focus you on this. This is a completely fake video. Okay, it's completely synthesized. There is like, there's nothing real about it.
And it's actually being predicted from raw audio with this 2d.
You know system that gives us only the pose. And, and that's kind of amazing. It's, it's the coolest thing about this is that not only can we synthesize a realistically looking video, but we also managed to capture a very convincing
emotion of the person. Mostly what what makes you think that it's good is those beat gestures it's like okay I'm talking and I'm chopping up my sentence and there's kind of going with the same rhythm as my voice, and not as convincing enough for people to think
that that this is real.
Okay.
We're making fake videos, basically. Okay, so it's interesting to think about what can we do in order to decide whether a video was faked or not.
And it turns out that the same kind of idea can be applied to forensics as well. So, I'm going to show you an example let's look at Obama and like look at this is from his address to the nation which he used to do every week.
And he did say, to see what he does when he says you know hello everybody.
Hi everybody. Hi everybody. Hi everybody. Hi everybody. Hi everybody. Hi everybody. Hi everybody. Hi everybody. Did you say that.
Hi everybody. Hi everybody. Everybody. He has this like upward motion. And basically, if you have the right words, and you have the right motion together, then it's Obama.
try to do some deep fake of him, and you try to change his lips and make him say something
different, you wouldn't get the right motion to go with this. And this is like a great
signal to see that something is fake. And this is a really nice line of work that Shoti
Aramal did, you see, when Honey Ferreira, they look at exactly this kind of thing. So
it uses the, you know, the whole of the person, all of the details in multiple modalities
to detect things that are fake. Alright, I'm going to move a little bit to something else.
If there's any questions, this is a good time. Okay, so let's talk a little bit more about
this idea of learning on little details of a person, a person's appearance. And we're
going to consider a very special style of gesture, which is dance. And it's an art form.
It's been around since the beginning of time. But frankly, nobody's really figured out how
to capture the little subtleties of it. And what we're going to do is we're going to start,
we're just going to start directly by looking at a demo. Okay, so look at what what we can
do still feels the same. That's nothing. But tell me now, why were we thinking of everything
that we had? Well, it's gone to waste. If I start in the sunshine, all I see is rain.
So what we do here is we're taking this source video, we're detecting its pose, and we're
puppeting our target person to dance in the same way. And it looks really nice. But I'm
actually using a trick on you, I'm using a perceptual loophole. And this is really interesting,
we usually don't show this, but it turns out that the music is very helpful. Okay, I'm going to
show you exactly the same result without the audio. And I want you to notice if it looks different to
you.
Okay, anyone? Mostly just look super unnatural, where before it looked like it actually like was
dancing. It's awful, right? It's like everything is moving. It's kind of wobbly. And like, you know,
just, just not right, right? And that's really interesting. It's exactly the same pixels. Okay,
I just turn off the audio. But when I show it to you with audio, your perceptual system is like,
whoa, there's like music, and there's dancing, and it's together, and you're filling in the holes,
you really are. And it's very important, it turns out when you're showing a demo, if you guys go and
do things in design, or even in AI, to kind of set it up in a way that then makes people be in
their happy places, and use multiple senses when they're when they're looking at something, and it
actually makes it look better than it really is. And our goal here is actually not to make a perfect
video. I don't really want to be in the business of making people do things that they didn't do. Our
goal is to study, you know, the statistics and whatever. But, but it is very helpful to use this
different senses. Okay, so what are we doing here? In a way, it turns out that we can use this same
technology we talked about before to transfer dance motion from one person who knows how to dance,
to a different person who is a terrible dancer, like my co-author here, Ting Kuei. And this is
basically motion that's not conditioned on audio, it's conditioned on different people.
And training is the same. We train a cycle from you to your stick figure and back to you, and you
can think about it as trying to go from you to a full reconstruction of you through this tiny little
bottleneck, which is these 2D poses that are not learned. And the idea here is to get learned a
really good mapping from this stick figure back to the appearance of the person, because I really
want to model what he looks like. I want to capture all the little minute details of his
body and the way that he kind of, you know, looks when he's moving. And I want to get the stuff that
you can never really annotate. I want to get all the complexity, this, this beauty and the details.
This is what I want. So the goal here is not to start from a single image of a person and make
them dance. The goal is to actually model this person. And for that, I need a bunch of training
data of this person from different poses. And at test time, we unwrap the cycle and we put a
different person on the other hand. So here, test and training are not the same on purpose. And this
is different from a lot of other methods. And these stick figures are a nice representation in the
middle, because they're kind of agnostic for appearance, like you could be, you know, bigger or
smaller and width, but it doesn't really matter for your skeletal structure. And if we managed to
learn a good model of our target guide, we should be able to sample any new pose from this ballerina
and synthesize a new image of Ting Hui. And this is good because a good model of appearance to
generalize to new poses. So I'm using video synthesis kind of like at school. It's like show
your work. You know, the generated video is a test of whether our modeling or perception of
this guy really worked. Okay, so we do some tricks to make this not, you know, frame by frame and
achieve a temporal coherence, which is important perceptually. But I actually want to talk about
something else. I want to talk about the fact that the face is very important perceptually.
Okay, and we didn't invent this. This is, this is Renoir. And there's something really interesting
about this painting, which is basically that there's different resolution of painting between
different parts of the image, right? The face is very, very sharp. And the eyes are extremely
sharp. You can even see like the glint of the lighting reflecting on her pupil. But everything
else is fuzzy. And it's kind of like the flags we saw before. It's like not really in place.
And it does something very, very interesting. It makes you draw your gaze to her eyes, which are
the most expressive part of the face. It gives you an emotional reaction. And it makes you ignore
everything else because everything else is in your periphery, which is not really,
not really attuned to spatial relationships anyways. And it kind of mimics our own perceptual
experience when we gaze into someone else's eyes, like somebody that we really love or somebody
that we really want to listen to. It's always, you know, everything else about their body is in
the periphery. Okay, so, so this is kind of the trick that Renoir is doing is playing on you,
and we're going to play the same trick. Okay, so, so, you know, this is this was done before
amazing GANs were around. And so we didn't have the technology to make everything look perfect.
But we said, okay, the face is important. If we get the face right, people will say, oh,
this looks nice. So we actually devote a special GAN just to the face region in order to correct it
and more realistic. And it makes a big difference. So if this is the baseline, this is after temporal
smoothing, this is what happens when we add an additional GAN for the face itself. And it kind
of looks almost like Caroline, who used to be my undergrad and did this work. And now she's a full
ground PhD with you guys at MIT. And you can see her dancing in all of these videos, like this one.
Okay, and we can also do the same thing. We take a motion of one person and we can apply it to
many people. And there, when it's small, of course, the face matters less.
And we turn it into a controllable interactive application that got a lot of attention to the
field of image and video synthesis. It appeared in popular press, it exhibited at museums,
it was incorporated into stage performances. And now my co-author, the same guy who doesn't
know how to dance, even turned this into an app that you can download for free from the app store
and you can dance and TikTok and whatever you want with it. And this is interesting. But another
direction that we kind of have been discussing, a word to take this technology, is to provide a
platform for capturing performance as a form of intellectual property for choreography.
Because it turns out that unlike musical score that can be copyrighted, there's no way to copyright
dance. In fact, there wasn't even a way to capture dance in the West until the 20th century. So most
of the belays that we know of, like Swan Lake and, you know, all of these, even the famous ones,
haven't really survived in their original form. Until this day, there isn't an agreed upon notation
of dance. And this is just one example, which is called Le Bonnotation. But all of the forms of
notation, they don't have a way to accurately capture all the small details. They're not parametric,
they're not scalable. Every time you come up with a different move, you have to come up with a different
notation. And our idea is that capturing things the way that we do can try and get
to these issues and maybe offer a new solution. So we talked a little bit about the fact that
there is artifacts, but actually, it turns out that that is the interesting part about this
technology to a lot of artists. So here's one example where the same team plays Little Company
had a gig where they made this music video. It's really celebrating the problems in this technology
and making them into art.
And here's another example that I really love. This isn't actually using our
work, it's using Vint to Vint, but it's very, very similar. So you can ignore the differences
and just focus on what this person is doing with this.
And he's actually pushing it to the extreme extreme. So there's all this hair and the feathers,
which are really, really hard to capture for GANs, but he's just like, let's throw it all in and
just let it be artistic. And this is kind of what he's been looking for in using these technologies.
Okay, but there's also a limitation on how you can use this kind of stuff for art,
because essentially what we're doing when we generate images or we generate video or motion
using a GAN is that we're always using a training set, which is real. And we're trying to teach AI
how to make us more of this real thing. Okay, so we want to maybe generalize out of the
distribution of the training set, but not by much. We still want to keep things realistic.
That's our training signal. And this becomes a limitation when you want to do something like
this. So these are visuals from Bjork's Tabula Vasa music video. And this actually goes with a
large performance and show that she had a year ago. And what they wanted here, and they actually
came to us to ask for help in order to do this, and we couldn't help them. What they wanted to do
is they wanted to have this marriage between a human and an orchid. Okay, so they want the motion,
and they use mocap for this, but they want the motion to come from a human, but the visuals and
kind of the embellishment to come from a flower. And there's nothing that we can do to help them,
because we don't yet have this ability to do this compositionality between different realistic
things to make something, to make a new kind of life form, if you may, with AI. We just can't do,
we don't know how to do this. So this is something that is a limitation of training to do realism.
And it's a very interesting future direction from work, if you're looking for something really
cool to do. This kind of idea can give you a nice direction that we don't know how to solve yet.
Wait, so how did they do that? How did they? They did it by hand. They did it by hand. So
this is a really cool artist. He's also a professor, I think, in Hong Kong or Taiwan,
something like that. And they did motion capture on the people, and so they have the motion signatures,
but everything else is hand designed 3D models and motion that is basically
put together with it's dressed. It's dressed on top of the human motion.
So what they wanted, wouldn't it have been similar to people who were here in the class
the other day? There was some AI that could make pictures out of the texture of the skin of an
elephant or something. The texture of noodles would have been similar to that. You had a face,
and then you draw it with the texture of noodles, and then you change the noodles to make it look
like it's talking. Yeah, so that's one. So you mean the Geiger paper, the counterfactual paper.
And there's also, I know you guys looked yesterday at Dali. So there's different,
yeah, that's one approach to do that. It's a little bit, right now, that technology is not yet
very, if you take those noodles and you make them in the shape of a dog or whatever,
it's still noodles. And you just cut them into dog. There's nothing that really takes the texture
and applies it onto the 3D form. So you see here, if you look at the texture of the flower,
it really changes with the articulation of the flower. If you just take noodles and you
make the right mask for them, it's not yet realistic. And this is, again,
something we don't know how to do. We don't know if you look at the cycleGAN results, for example.
It doesn't conform to the 3D shape of the object. It doesn't actually respect that. So
yes, that's in the right direction, but it's not yet there. All of this is open to problems.
Answer your question?
Yeah, yeah, it's just stuff to think about. It's neat stuff.
Yeah, yeah, this is really cool. I don't know how to solve this, but this is a cool
idea, cool direction. Okay, I literally have three minutes. So I am going to have more stuff. So
the rest of the stuff I wanted to talk about would have been kind of designing with time,
using time as an interesting medium. And I guess what I can do is just tell you
two different highlights. Okay, so I'm not going to really walk you through all of the story here,
but let's see. If we think about time and we think about images, not from video,
but large collections of images, there's something really interesting about them.
And the interesting thing is that a lot of the time when you think about historical data,
you think about text, but you don't really write everything down. And those are things that
are really captured in images. And we're lucky, we're very, very lucky that we have now basically
more than 100 years of historical visual record. And the things that you get from that are things
that are, you know, you can look at this image and kind of think to yourself, you know, if 100
years from now, somebody wanted to explain in a history book, what are hipsters? It would be really,
really hard because, you know, here you can see the difference between nerds and hipsters. And,
you know, what is it that makes these guys look cool? Is it hats? Is it the scarves? Is it the
Instagram filter that was slapped on the image? Like, you know, what is it? It's really hard to
say in words. And in any case, we don't really bother to talk about these things when we write
stuff down. And so historical images kind of captured this for us. But of course, then we kind
of, you know, if you took a collection of historical images, you kind of end up with, you know, a bunch
of historical images that you need to sort through and you end up selling in the garage sale because
it's too much work. So if there is a way to automatically get, you know, how do things
change over time from images that would be really cool. And that's stuff that we've done a couple
of times. Here, this is work with historical yearbooks, high school yearbooks, which is a really
nice source of data because they're spaces and they always stay the same. But what changes is
fashions and social norms. And what you can do with a lot of data like this that has kind of a
consistent subject matter, but changes over time is something like Jason Sullivan here, who is an
artist, has done with his graduating class versus his mother. So he graduated in 1988 from Fort Worth
in Texas. And on the left, you see an average image of all the people in his classroom, the women
and the men, versus on the right, the ones that came from his mother's class of 67. And you can
already see that there's big differences, right? People used to look different. They used to dress
a little bit different. They used to treat the camera a little bit differently. And we did the
same thing with our data, where we took, you know, 100 years of photographs and we looked at
averages of men versus women over time. And you can notice that people, you know, look a little
bit different. The hair is different. They smile more than they used to. You can quantify
this kind of thing. You can look for other characteristic elements for different decades,
like different hair styles that are very distinctive. And this gives you tools for analysis
of creativity and fashion, but we're not going to look at this. But instead, what we're going to say
is, okay, this is interesting. You can do this with spaces. Everything is very aligned. You can
look at fashion. But what happens if you want to think about time in the real world, in the outdoor
world? How can you use time as a creative medium? And so one thing we did afterwards is we went and
looked at a lot of data coming from street view images. And we looked at whether we can say,
okay, let's say I want to travel in time. I'm stuck in a pandemic and I want to go visit New York,
but I want to make sure that I did it on a particular Sunday afternoon in 2011. Okay,
how can I do this? So maybe I can use flicker images if I want to go to Columbus Circle,
but if I want to go to some random corner, there's just not, there's just not, people just don't
take pictures there. So what we did is we went and looked at the Google time machine, which is
basically your normal street view interface, but they actually keep historical images of the
previous runs of the cars through the city. And that gives you a single location with different
lighting conditions and different weather conditions. And this is really cool because you can collect
this at a really large scale, like, you know, all of New York or basically the entire world.
And then for each location, you have multiple snapshots of that place,
only they're still very sparse. And to go to this place in a particular day and time,
you have to learn how to fill in the gaps. Okay, because, you know, the buildings stay the same,
but the weather conditions might change. So to travel in time, you want to take a particular image
and you want to be able to change the lighting and the, and the weather. So basically what you
want to do is you need to disentangle or learn to disentangle the things that are varying temporally
versus the things that are permanent. And if you can do that, then you can use time basically
to synthesize new things that are, you know, you never really captured, they might have existed,
but you don't know because you weren't there. And so for a particular scene, you can do things
like you can rotate the sun around. And this is completely synthetic, right? This is a result of
what we do. Or you can copy and paste buildings, for example, so you can modify the permanent factor.
And that would look something like this, right? Here's here's an inserted building,
it looks perfect, but, but it's completely fake. And I'm not gonna, I'm gonna, you know,
just not going to go into the technical detail of how it's done. But basically the thing that
helps us is that we've seen the same place over and over again. The one thing I do want to talk
about is that the nugget, the technical nugget that we use here is that we can use a decomposition
of the scene into two things that graphics tells us that are, you know,
physically correct, which is the difference between shading and reflectance, where shading kind of
captures the shadows and the effects of the illumination on the scene. And the reflectance
is the actual color. So I'm wearing an actual blue sweater, that would be the reflectance
of the sweater. And then there's the effects of the light on it that, that puts in the shadow.
And the interesting thing about, about these, this, this shadow representations, this, the
shading representation, is that we're, we, in our mind, we think about the fact that maybe
shadows are gray is like, if we think about them as grayscale, but they're actually not. Okay. And
this, this is the interesting thing for sexually here. And you can see this in this nice painting
by Monet. Okay, so, so Monet is painting a grain stack that is sitting in a set of snow
on the ground. So the snow should be white, the ground is white. But there's actually two colors
for the illumination. There's blue from the sky. And that is kind of an indirect diffuse light.
And there is a direct light that is yellow from the sun. And if you look at the shadow that's
being cast on the snow, in the shadow, it's blue, because the direct light doesn't hit,
doesn't hit the snow. And so you mostly get the indirect illumination from the sky.
And it's more blue than the surrounding light, which has the yellow mixed into it. And Monet is
doing a trick here where he's actually coloring with the yellow to complement the blue just in
the border to make it even more clear to your, to your, you know, your centers around cells in
your, in your eyes that this is what's happening. Okay, so, so the trick here to get everything to
look realistic was to say, okay, people before have kind of thought about shading as a grayscale
thing. And most of the color in their models have gone to the reflectance images. But we actually
use a two toned shading where we take separately into a cup, the blue and the yellow. And we are
trying to really capture a lot of the blue of the sky in the shading model and not in the
reflectance model. And that kind of makes everything come more together and look more realistic.
And then we can say, okay, and when you know, we generalize, we take an image from a completely
unseen place like Paris, we've never been to Paris, we didn't train on Paris, this is like a one
image example, and we can relate it and make it look like you've been there whenever you want,
basically. Okay, so now I am very much over time. So I am going to stop here, we've looked at
audio and motion, we've looked at details, we've looked at visual patterns over time.
There's, there's some food for thought you can take out of here, like, you know, for example,
AI and perception, we can use it to create tools for art and design. There's good and bad
implications, you have to think about what happens when you make synthetic or fake content. And
we've talked about modeling all kinds of complex things and multimodal stuff. But there's a lot
of stuff that's left to be done. For example, the example of the person in the orchid and
compositionality. And there's also the question of how do you not only provide tools that are
creative, but also create creative machines. And I think you're going to learn about that
more later this week. One final note is, if you want to learn more about perception and art,
this is a great book by Margaret Livingston from Harvard. And these are collaborators,
thank you all collaborators. And that's about it. Thank you. Thank you so much. That was very,
very interesting and intriguing and inspiring. Cool. I'm sorry, it went a little bit over time.
Just trying to like lower the details. I'm sure that there are many interesting things I personally
learned and also students hopefully inspired their thoughts and their future work.
Is there any question from students?
I think that most of them are thanking you and I see that in the chat.
Oh, chat. Okay. Yeah, I think that.
Okay, cool. Excellent. Thank you again. It was a great talk. Thank you. I'm very excited to
put this online so everyone can benefit from it. Cool. Awesome. Thank you so much. Bye.
Bye now. Bye.

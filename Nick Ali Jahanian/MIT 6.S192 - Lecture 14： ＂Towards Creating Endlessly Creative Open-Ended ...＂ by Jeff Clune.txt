Hello, everyone. Welcome to your course, Deep Learning for Art, Statistics and Creativity.
Today we have two specialist speakers. First, we serve is Dr. Jeff Klun, who is an associate
professor in computer science at the University of British Columbia and also a research team
leader at OpenAI. And he's going to talk about towards creating endlessly creative open-ended
innovation engines. I think this is a very exciting direction because so far we have
talked about the interaction between art and AI. We said that how AI can help us to create
and express ourselves and democratize the creativity in a sense. But also the other direction
is how our creativity can help us create better AI. For instance, how we learn by creating how we
define problems and find solutions for them and generalize to solve bigger problems and so on
and so forth. So today is one of those, I would say, a realization of such a great idea that you
will see as a gist of what Jeff has been working on. So please go ahead. And also another question
that we often ask in the class is that students are interested to know a little more about your
background because they always feel inspired by seeing great scientists and what for instance
got you to working on AI would be very interesting for them if you don't mind sharing.
Great. Thank you for the introduction. Let me share my screen here and make sure that that is working.
So are you able to see my screen? Yeah. And the presentation? Yes. Okay. And can you see my mouse,
cursor? Yes. Perfect. Okay. Hello everyone, my name is Jeff Klune. And I want to talk to you today
about trying to take on like an extremely big research challenge. I think it's a grand challenge
of AI. And that is trying to create what we call open-ended algorithms. I wasn't planning on telling
you a little bit about my background. I guess in brief, I started out on a quest just to understand
two twin questions, which is how did natural evolution produce all the complexity on Earth,
including the human brain? It's astounding. And we don't know how that process happened really.
We don't know how to recreate it. And you'll see a lot of work towards that today. And then also,
I'm interested in trying to figure out how does thinking happen and how can we create it in
machines? And I think in many ways, these questions are very intertwined, as you'll see also today.
So I started out in philosophy actually, because I thought they had the market cornered on thinking,
but really quickly kind of, well actually not quickly, slowly learned throughout the course of
my life that the best way to tackle these challenges is to try to build these systems
and recreate these systems computationally. Motivated by the wonderful quote by Richard
Feynman, which is that which I cannot build, I do not understand. So we understand by building.
And that has certainly been true in my life that I understand more and more by being forced to turn
speculation into code and into algorithms. So with that, I'm going to begin. So this talk is
really going to be in two parts. The main part is going to be the first part, and it's about
creating open-ended innovation engines. And if there's time, which I hope there will be,
I'm going to rush through a series of work that we've done that I call AI neuroscience at the end.
And then throughout all of this, what you're going to find is that this is a bit of a meandering
intellectual story, because throughout my career, different research has kind of unintentionally
produced different aesthetic artifacts of interest. And I kind of want to walk through some of the
things in touch as many of these places where I think our work has produced things that are
aesthetically interesting, as well as scientifically interesting. So the first thing I want to
motivate is, you know, the idea of open-ended algorithms. So these are things that endlessly
innovate. They just keep going forever. So if you think about natural evolution, look at the
tree of life there, and think about all of the marvelous engineering designs that nature has
brought and continues to create in an ever-going fashion, you know, jaguars, hawks, the human mind,
everything that we know on earth. You know, in most situations, we cannot rival these things with
engineering. And so what's fascinating is that, you know, a very simple algorithm, the Darwinian
evolutionary algorithm, plus the context it's been placed in, continues to innovate for billions
and billions of years. And I think it's really fruitful to think to yourself, you know, could you
create an algorithm that would you would want to run for billions and billions of years and come
back and check whether or not it's interesting. Currently, as scientists, we have zero ability
to produce things that are interesting even after a few months of running them on a computer,
let alone billions and billions of years. So natural evolution is what we, you know,
one of these open-ended algorithms. And another one is human culture, which just endlessly
innovates and produce innovation on innovation and innovation. That's both true in science and
technology, but it's also true in the arts, where you get, you know, impressionism after you get
the classical paintings, and then you get, you know, postmodernism or Jackson Pollock or all the
different kind of evolution of genres. So, you know, we started with the idea when we wanted to
try to work on this, is that natural evolution and human culture are what we call innovation engines.
And that is that there's kind of this simple recipe that they follow that allows them to be
creative. And that is that they start with a set of things, it could be an empty set, and then they
generate a new thing. And then if that's interesting, they keep it and add it to the set. And then they
take something out of that set, they change it a little bit, they permute it somehow, and they
see if that is interesting. And if that's interesting, they add that to the set. And you have this
growing set of things, this archive of things you've already produced that are interesting.
And then each one of those is a stepping stone to new potential innovations or solutions. And if
you think deeply about it, that's true both of human culture and natural evolution. And so the
question is, with that kind of mental framework, can we create algorithms that do that process
automatically? And so, you know, at the core, there's really kind of these two simple steps.
The first one is you have to have something that generates new things based on previous things.
That's the green box on the left. And then you have to evaluate whether or not those things
are interesting. And if they are, then you add them to the set and you just keep repeating this
process. So in the long run, what we'd love to do is take, you know, humans out of the loop, if
possible, label data out of the loop, and you just have some sort of generator like a neural network
that can generate new things like poems or codes or mathematical proofs or images, or, you know,
technological artifacts, something, maybe another deep neural net that is trained to recognize
what's interesting somehow. And then that process could just iterate. Now, for example,
you could imagine that you take like the orange box here is an autoencoder. And it looks at everything
that it's seen before, it compresses them down to a low dimensional bottleneck space, and then it has
to uncompress them. And then if you get some new latent vector that's new that you've never seen
before, you call that interesting. And that might be one thing that could kick off this problem.
And if you could do that, you would have an innovations arms race in any domain, you could
unleash this thing anywhere. And that would be amazing. And a lot of these ideas date back to
Schmidt-Huber ideas from the early 90s. However, the problem is that when you do that, you typically
do get new things forever, but you don't get new interesting things forever. You, for example,
might get white noise, just an endless stream of different patterns of white noise, because those
are uncompressible. So really, at its core, the biggest challenge in this field is kind of,
how do you avoid generating uninteresting novelty? And how do you only generate
interesting novelty? And here's one example by a friend of mine, Josh Auerbach, who tried to
basically take the same encoding that I'm going to tell you about later, and a similar system is
trying to automatically generate images and try to produce new interesting images forever. And
these images are interesting. They're pretty cool, but they're not nearly as interesting as they could
be, right? They're not like what artists would do over the course of centuries. You would expect
and hope that things would ultimately break out of these kind of abstract patterns. And that's
because these things are optimized to produce information and theoretic metrics like compression
or mutual information and things like that. So what we thought in this work is that one insight
you could have is that recognizing a new type of thing is like being able to recognize a new
class of thing. If you've never seen a palm tree before, that's a distinct kind of trees. And if
you've never seen a tree before, trees are distinct from dogs and roses and statues. And so
one way to think about being able to recognize an infinite number of new classes is to approximate
that by having a neural net just recognize a very large number of classes. And so if it could
recognize a million classes, for example, then as the generator produces new instances of those
classes, maybe the process could start going out and generating each of these classes. And that
allows us to then use supervised learning because we know how to recognize new classes of things.
So this is an approximation to the overall goal and to try to see if this system can work. So the
way that we wanted to approximate this, and this is all the way back in 2015 before image generation
really worked out well, is we said let's take a deep neural net that is trained on ImageNet,
which is relatively new around that time. It has a thousand different classes and it's really good
at recognizing the certain classes. And then we'll use that as our evaluator, which is the
generator's job is to generate instances of that class. And then the question is what are we going
to use for this, the green box here, the generator side. So what we need is an algorithm that can
recognize either an improvement on a current class or when a new class is generated. And so we decided
to use this algorithm that I'm excited to tell you about because it has a lot of really interesting
motivations behind it. It's called map elites. And it has one bin per ImageNet class. And I'll
tell you what map elites is right now. But to tell you about map elites, I kind of want to motivate
this whole field of a new kind of type of algorithm that my colleagues and I have been working on.
And it starts with this recognition, which is that there's a paradox in life, which is that if you
try too hard to solve a problem, you'll fail. However, if you ignore the objective, then you're
much more likely to succeed. So imagine that you're in this maze here, and you're starting here and
you're job to get here. And you might say, well, okay, make the robot who's here, make its objective,
getting as close as possible to the goal. Well, if you do that, and you get points here, these are
all the points that get generated by that search algorithm, because and most of them just go straight
north, because that lowers the distance to the goal. But then they just butt their head against
that wall forever. This is a classic local optimus, you're familiar with these things in search.
However, if you simply switch away from the paradigm of always try to optimize toward a goal,
and you just say, let's just go to new places, just seek novelty. That's what you get here. And
eventually this search stops focusing on just going north. It doesn't actually care more about
north than going east. And eventually it winds its way around and it solves the problem. And this
right here is a metaphor for every single hard thing we want to do in search. If there are local
optimum space, if we need to explore to discover this thing, then we probably should seek novelty
more than an objective. And it's even a metaphor for things beyond algorithmic search. It's also
a metaphor for human culture and even natural evolution. And the idea is that almost every
major scientific breakthrough, if you trace its lineage back, it's not a straight path to that
solution. Instead, it's a winding, circuitous route. So for example, if you went back in time,
centuries, and you said, I have this cooking way of cooking food. And what I want is a faster way
of cooking food that doesn't produce any smoke. Then you would never, if you only funded work into
improved cooking technology that can accomplish those goals of heating things faster, you would
never invent the microwave, which is a magical invention. Because to invent the microwave,
you had to have been working on radar technology and recognize the chocolate bar melted in your
pocket. Similarly, if you went back millennia to this abacus, and you said, that thing does
computation, I want more computation. And you only funded researchers who improved against
the objective of producing more computation, you might get abacuses with like longer rods,
more beads, something like that. But you would never invent the modern computer because to do
that, you had to work on things like electricity and vacuum tubes, which were decidedly not produced
because they improved computation, although they later proved instrumental to doing that.
The same is true for going from this kind of energy to clean energy,
where you have to be thinking about things like space and time that were not thought about
because they would produce new ways of producing clean energy. So the conjecture here is that the
only way to solve really hard problems may be to create problems while you solve them and
goals switch between them. And so goal switching is this idea that if you're trying to solve one
task, and you make progress on a different task, then you should also start optimizing and getting
better on that different task. So if this robot here, this scientist here wants to make a walking
robot, and all of a sudden during optimization, the robot starts crawling or starts balancing on
one leg, you shouldn't throw that out as a failure because it's not helping you walk
or making forward progress. Instead, you should start getting better at those skills to add those
to the set of things that you work on. And ultimately, those might be stepping stones
get to get you to this walking robot. So this my colleagues and I have been creating this new
subfield of algorithms of AI called quality diversity algorithms. And this family of algorithms
is trying not just to get the single best solution to a problem, it's trying to do something very
different. It's trying to get a large set of diverse solutions, but where every solution
is as good as possible for that type of solution. You know, you want the tallest in the giraffe
or the fastest ant, but you don't let an ant who's not that fast kind of get precluded by the fact
that it cheetah is faster, you still want the tall fastest ant and the best ant you can find.
So probably the most popular algorithm in this family at this point is this algorithm called
map elites, which was invented by Jean Baptiste Morel, a great colleague and friend of mine,
as well as myself in 2015. And it's very, very simple. And the idea here is if you're going to
solve a problem, want to first choose or learn, but we started off by choosing dimensions of
interest that you find that you yourself like. So imagine if you're trying to make a car, for
example, you might choose safety and fuel efficiency as two dimensions of interest. And then you just
discretize these, these dimensions, and you look for the best solution, according to some criteria,
like maybe it's the fastest car at each point in this grid. And what you want at the end of the day
is not just to get the fastest car possible, but the fastest car for every possible tradeoff
between safety and fuel efficiency. So here's an example problem we tried this on. This is
generating soft robot morphologies, which is like the bodies of robots. So we gave this optimization
algorithm those four materials there, they're kind of voxels that can pull said different times,
and some are soft and some are hard. And we said, you know, go fast. And, you know, first we did
this without map elites, we just did this with a canonical optimization algorithm or genetic
algorithm in this case, which is just trying to optimize for speed. And what you see here is this
kind of really interesting parade, this Noah's Ark of very different solutions and very different
creatures. And, you know, people got really excited when we put this online, and it's super fun.
But I think one of the things that people thought really interesting about this work, including
myself, is the huge diversity of designs that you see here. You know, it starts to evoke nature where
you see a lot of different designs. The problem is, there is a trick to this, and that is that all
of the designs that you just saw, each of those came from a different run of optimization. The
only way you got a diversity was by starting the run again and doing a massive search to find one
solution. But if you look within that population of creatures, they're all almost identical.
And that's not what we want. What we want is an algorithm that will generate a huge diversity of
things within one run so that you can run it for billions of years and it would continue to produce
interesting new stuff as opposed to converging to one type of solution and getting stuck on that
kind of local optimal. So we took the map elites algorithm that I just described to you and we
applied it to the same software of us problem. And what we did there, you know, we have to pick the
dimensions and we chose to pick the number of voxels and then amount of this dark blue material
because previously it hadn't been using this kind of bone like material and we wanted to see it play
with that resource more. And if you look at classic optimization, this could have been RL,
but in this case it's a genetic algorithm, any optimization, what you find is that it doesn't
actually search the space very well. And so it has low performing points and it didn't do a lot of
exploration. If you add diversity, which we know historically helps, you do get higher performing
points. You see these yellow points here, but it still did not explore a lot of the space,
even though it's incentivized to literally explore in these two dimensions. Map elites is a qualitatively
different algorithm. It's a sea change in terms of what happens within the algorithm.
If you look here, you see this rich exploration where it's fanned out and searched the entire
search space and it taught you more about this search space. It tells you, hey, there's not very
high performing points up here. There's a little bunch of optima over here. There's also this
separate little area here that you probably would never have normally found, et cetera, et cetera.
I've been doing these interesting points over here that you can go investigate.
And what's interesting is it often finds a better overall high performing solution than if you
just do direct optimization because it's doing such a better job of exploring the space of
possibilities. So if you look at any individual final point, you can trace back its lineage
through time to see where those solutions visited in the search space. And what you can see here
is that they don't just kind of mine one area of the space and get better and better and better
at that corner of the search space, that particular trade-off between these two dimensions.
But instead, the overall lineage takes these long, circuitous paths to their final destination.
Just as to get a human, you had to go through an intermediate stage of being a tapeworm and then
being like a tree dwelling. Actually, I don't know if we were a tree doubling, but kind of
something that looked more like an ape and all sorts of intermediate steps along the way.
So going back to the idea of an innovation engine, we now recognize the algorithm that
we're going to use here. There's one final thing I need to tell you about, which is how are we going
to encode the images we're going to search for. And I'm going to tell you what I mean by the word
encoding, because I think especially for people who are interested in aesthetics, this is one of
the most important choices you can make. And you'll see this show up in Joel's work later as well.
So I'm going to tell you about the encoding that we use, which is a CPPN. So first, I've been
throwing around these terms, genetic algorithm and evolutionary algorithms. You may not know what
they are. So I'm going to very briefly explain them. If you want to search for a problem, this is
also true in deep learning. The first choice you have to make is how to encode the problem. So
imagine if you wanted to search for tables. Well, you could decide I'm going to store the length of
each leg separately as a number on a parameter vector. We, in evolutionary algorithms, we call
this a genome, but in deep machine learning, it's often called a parameter vector. So you
store the length of each leg separately and the width and the length of the surface of the table
maybe on this string of numbers, this parameter vector. Once you've made that encoding choice,
you then can score the population. First, you create a population at random by generating random
strings of numbers. You score this population to see how good they are. You select which ones are
better according to some scoring function, which could be your reward function. And then you just
take these things here, take their parameter vectors and you perturb them in a little way
somehow. And then you get a new thing and then you repeat the process. In the gradient-based
method, this is kind of like where you take the learning step based on the gradient of the scoring
function. And then you repeat the problem. So when I talk about an encoding, it's this first
choice, which is how do we decide what is the search space that we will search in the parameter
vector and how does that map to the final solution. And that is in evolutionary language,
the process of going from a genotype to a phenotype, or a machine learning a parameter
vector to a final agent or policy or artifact. So there is this notion of a direct encoding
versus a generative encoding. And a direct encoding, you basically have one number on your parameter
vector for every single thing in your final artifact. So if you're searching for the weights
of a neural net, then you search separately for a number for each weight. Or for a table,
you search separately for the length of each leg. If you think about how perturbations affect
these parameter vectors, though, they are more mostly likely to produce non-regular phenotypes.
So most changes are not going to lead to a table that has to be flat and hold your coffee. And so
that makes kind of a local optimum between this solution and this solution. You have to go through
this intermediate thing unless you get lucky enough to generate a regular phenotype. If you have a
generative encoding, you reuse information in the parameter vector to produce the final thing.
So you might just specify the length of legs once and then reuse that for these four lengths
of tables. And now every single change to that parameter vector is going to produce a regular
flat table. However, you've lost something. You've lost the ability to express this type of table
up here. And so this is like a really, really essential choice when you go to produce any solution
with search. So generative encodings, you know, my colleagues and I and many others have been
focusing for a long time on why these types of encodings are really interesting. And some of
the desirable properties that we want is that you can get regularity, which means you can get patterns
in the final artifact. It might be the architecture for neural net, or here is the hands on your,
you know, in your body. And what you see is there's a repeating theme in your hands. That's the
regular pattern, but it also has variation. Each of your fingers is a variation on a concept or a
theme. And that's kind of one thing that you might want while you search. There are some others
benefits here, but I'm not going to get into those. So this is something that I just think is
really fascinating to think about, especially if you're interested in aesthetics. And it also ends
up being helpful algorithmically and it's going to factor into a lot of Joel's work, I assume,
depending on what he talks about. And this is this question of how does nature build the
astronomically elegant, complex creatures that you see in the natural world? Like a question
that I'm not sure if you've ever stopped and thought about, but it's a fascinating one to think about
is how does every cell in your body know what kind of cell to become? You have, you know,
the same software is being run in every one of your cells, the same DNA, yet some of your cells
turn into hair cells or spleen cells or liver cells or eye cells. How does it do that? How does
every cell know what kind of cell to become? Well, it turns out that nature is using a generative
encoding where it reuses information, where the cell fate, which is the type of cell, is a function
of its geometric location in the body. It's almost as if the body wanted to know the XYZ GPS
coordinates of each cell, so that it could tell you, oh, if you're like up here, left of the midline,
three quarters of the way up the y axis, then become a heart cell, for example. So if you look
through developmental biology textbooks, what you find is that these kinds of geometric patterns
are the lingua franca of developmental biology. So here's this beautiful cartoon by Sean Carroll.
So here's your DNA with has these genes on it. And in this developing embryo are currently
three different chemical patterns. They're called morphogens. They're literally some protein that's
sitting like diffused inside this embryo. And if this gene here says that protein A is present and
B and C are not present, then this gene expresses and produces a new protein only where that's true.
And so now you've combined these three preexisting patterns to produce this fourth new pattern,
and this might therefore tell the vertebrae and a spine that they should turn into vertebra cells.
You get this repeating theme down the middle, but only the left half of the embryo. And if you look
through, go ahead. Would I be able to interject real quick? Sure. My research is actually focusing
on exactly the same kind of problem, but in mammals. And so in mammals, the morphogen model
explains some stuff, but it's actually even more complex. It is much more complex. Everything in
nature is much more complex than we know. So I am simplifying here because I'm flying through
this material. And it's not to say that the only thing that's happening is geometric patterning,
but it is basically, I think it's the backbone of the way this stuff gets built.
And so by capturing that power and putting it into our search processes, we've gone a long way
towards the power of developmental biology. And you could argue that you've skipped out on a lot
of the extra complexity that would be very computationally difficult to simulate by doing
these things efficiently. Yeah. All right. That's a good point.
Cool. Thank you for the question. So getting to the issue I was just talking about, which is how can
we efficiently make this sort of a process happen? So what we don't want to do computationally is have
like diffusing chemicals in some chemical simulator, because that would be really, really expensive.
And so what Ken Stanley, my longtime friend and colleague figured out is that you can actually
abstract a lot of the power of this system without any of the underlying chemistry and
physics in these things that are called CPPNs or compositional pattern producing networks.
And the idea is, is just like in nature, we're going to encode phenotypic elements as a function
of their geometric location. So here's how it works. You take a thing that you want to
optimize. This could be a neural network. It could be a robot morphology. It could be a picture.
And you provide coordinates for everything in the artifact. So imagine it's easiest to think
about pictures. So imagine you give every pixel an x, y coordinate, then you literally pass the
number, then those numbers into this function. So first you put in one, one for this pixel,
and then one, two, and then one, three. And you ask the genome as a function of those two numbers
to spit out the value at that location. And if this is a random function, then you're going
to get a random picture. But if this function here has mathematical functions that, you know,
have regularities in them, then you're going to get a regular artifact. So for example,
if you want left, right symmetry, you can pass the x axis through a Gaussian here. And then
everything downstream of that Gaussian node will have left, right symmetry. Similarly,
you could have in the y axis, if you wanted a repeating theme, like segmentation, you could
pass the y through a sine function. And then everything downstream of that node will be
regular in that way. You can also add in linear things. So you could say, I want to follow the
sine, but only add in a linear component, so like shift it or warp it or bend it in certain ways.
So you can mix and match asymmetric and symmetric and repeating themes to produce
arbitrary complexity using these geometric functions. And kind of what was really amazing
at the time, because image generation wasn't working very well, was the kind of images that
would pop out of these systems. So all of these images here were produced on a website called
Pickbrier where humans manually choose which ones they find interesting. But the underlying encoding
is a CPPN. And Joel's going to tell you a lot more about like a modern version of this website.
So these images here are all encoded with CPPNs. And what you can see is very, very natural like
shapes, like things like left, right symmetry, repeating motifs, and the lineages as you kind
of permute and mutate these things, you go from a butterfly to a bat with these kind of beautiful
gradations and interpolations that are nice to see. Myself and my postdoc advisor, I took the
same exact idea and we just put it in three dimensions. And what you get are these nice
three dimensional shapes, which also show a lot of these regularities. And then we went off
and we built this website called endlessforms.com where you can go on, it's basically Pickbrier
but in 3D, you can take us an individual shape and you can say I want to further evolve or optimize
that shape. Let's see if this plays. Here, for example, you might take this lamp and you are
presented with a bunch of variants on the lamp. And then you pick the one that you like and you
see the next generation and you can kind of crawl through three dimensional lamp space.
And importantly, if you find one that you like, then you can publish it to the website and other
people can pick it up and branch off of that. I mean, this is how you get that growing archive of
stepping stones that allows us to produce kind of an interesting exploration of the space.
Here are some of the other designs that popped out of this system. And here's kind of repeating
segmentation, left, right symmetry, radial symmetry. And mostly a lot of the things just
look really natural and interesting. So this is kind of a fun aesthetic space to be playing and
using these CPB ends. Because we could, we 3D printed the objects and allowed users on the
website to 3D print them. So it's kind of fun to hold these things in your hand. And you can
therefore help people who have no knowledge of CAD and design to produce arbitrarily complex images
and then 3D print them for whatever they want, like a chessboard or something.
So when we put this out there, people really found this interesting, which I think just goes to the
fact that if you can automate the design, if you can help people produce really interesting
things that they're curious about and they find exciting, but eliminate all the technical
barriers to doing so, then people get really excited about those tools. And Joel's website
is a, you know, GAN breeder is a testament to that as well. So going back to the overall scientific
question here, which is, can we use this to create an open-ended algorithm? Now you know all the
pieces of the puzzles. So we're going to have Alex net, which is an early image network that was
quite good at the time, be able to recognize a thousand different classes. And then we're going
to have an optimization algorithm that's going to generate these little tiny CPB and networks
that are trying to produce images that light that the DNN, the deep neural net thinks represent,
you know, are classified as each one of the thousand bins in image net. So the idea, hopefully,
is that you'll get goal switching. So if one of the networks is the best dog we've ever seen,
or particular dog, and then a permutation on that produces the best fish we've ever seen,
then now that network can go to hop over to that bin and start optimizing to become a better fish.
And maybe that produces a better stepping stone for a cat and then a bird, etc. And the hypothesis
that we wanted to test is, is that better than separately optimizing for each one of the bins
in image net? So here is the performance over time. Time here, training goes from zero to
bottom to top. And the category of thousand image net classes are along the x-axis. What you can see
is that over time performance rises with training all the way up to one, you know, red in most places,
which means that the deep neural net is certain that this thing is a lion and this is a starfish
and this is a guitar. So my question to you is knowing that the deep neural net thinks that
each one of these things is in that category, you know, what do you think they look like?
And if you had asked this question in 2015, 2016, people would have said they look like electric,
you know, starfish and guitars, but you probably now, because you guys are, we've had the benefit
of a few years, you probably are used to the idea that what you do, what you get is not that,
but you get these things that are called fooling images or adversarial images,
which is to say that the deep neural net is absolutely certain that this is a starfish
and this is a peacock and this is a king penguin and this is an electric guitar,
even though they obviously are not those things. So at the time, this was this, we published this
paper, deep neural nets are easily fooled. And it was a really big wake up call to the community,
but AI sees the world differently. There are huge security concerns here.
And this generated a tremendous amount of discussion and awareness amongst the scientific
community, the machine learning community, also the broader public about the fact that these new
tools that we're building have a lot of deep flaws within them that we need to worry about.
Nowadays, everyone's very familiar with adversarial images. At the time, this was not very well known.
And so I thought that was interesting. However, I also think from an aesthetic perspective,
it's interesting that we were trying to generate innovation engines and generate images. We weren't
trying to study neural nets and whether they had flaws and then this just kind of popped out.
So I thought that was an interesting story. But while some of the images didn't look anything
like the categories of interest, another thing that we found interesting is that many of them
did. And from an aesthetic perspective, this is pretty cool because now you're getting an
automated art generator. So for example, matchstick, television and bagel, they pretty much do look
like those things. However, I also think from an aesthetic perspective that some of these really
kind of evokes like an artistic interpretation of what that abstract platonic concept represented
by that class is. Like for me, this is like a prison, like this image of a prison cell evokes
more than just a picture of a prison cell. It kind of seems to me like an artist decided to
represent like the bleakness, but also the hope or something about this prison cell. And so even
though there is no artist that was trying to capture that behind here, there's a neural network
that's trying to capture the platonic concept of a prison cell. And that somehow leads to its own
dialing in of what is central and essential about that concept, or at least evokes those
kind of reactions in us and allows us to explore potentially new types of artistic and aesthetic
connections to concepts. So if you look through the very diversity of images that were generated,
I do think this kind of really hit the mark in terms of a quality diversity algorithm.
You've got this huge set of images as all comes from, you know, one run.
And at least I'm not, I think that they are, they might have been pulled from a couple of
different runs in this case. But each one produces this giant, this diverse set of images,
and many of them I think are really aesthetically interesting, like I think this volcano or this
beacon, or this cup, I could actually imagine a coffee shop where this is this logo,
your comments on a mask and a banana, etc. So we were really, really thought it was cool to see
kind of this pop out of an automated system back in 2015. Scientifically, we're also really
interested in like whether or not goal switching was playing a huge role in these networks. And
so we have, if you optimize for a single class only, like the water tower class, what we see is
that you do indeed get stuck on a local optima. It lands on this particular pattern really early
in the run, and then it just does minor tweets on that idea and gets stuck on it until eventually
kind of maxes out what you can do in that corner of the search space. In contrast with map elites,
what you see is it early on, it locks on this half dome moon image, and it does okay, but then
it kind of gets stuck. And then from a totally different class, something that happened to have
been produced to, for the beacon class actually ends up looking like a better water tower and it
goal switches in, it invades this class. And then with further optimization to look like a water
tower ends up making the DNN think with 98% confidence that this is a water tower. And you
know, you can kind of see why. And we see this lesson over and over and over again, that there's
many goal switches happening within this population of networks. And we think that's a big reason why
performance is much higher than when you optimize for a single class. So what's really interesting
about goal switching is that it allows what biologists call adaptive radiations. So you come
up with a good idea, like maybe a more efficient way to metabolize oxygen in one lake in Africa.
And then that idea will spread to all of the surrounding lakes in Africa. And then on top of
that technological foundation, those fish will re-specialize to their particular niche and adapt
that, incorporate that innovation. The same thing happened with Darwin's finches, which radiated
out from one couple of finches to all of these diverse finches. And we see the same thing in
technology where computers, for example, were invented for one purpose and then kind of spread
throughout an ecosystem that are now embedded in all sorts of technological devices in our lives.
So what's really nice is you can see these adaptive radiations happen in these quality
diversity algorithms. So this is one of my favorite plots from all of those science I've done in my
entire career. Inside one of these innovation engine runs, you've got this early innovation,
which is this dome against a color background. And that thing, which lit up the abaya class then,
radiates out. And it's children because this is a population. So these literally are descendants
of each other. Its descendants kind of produce a phylogenetic tree, just like we see in nature.
And ultimately, this innovation turned into a volcano, a mosque, a water tower, a beacon, a
yurt, a church, a planetarium, an obelisk, and a dome. And it's just awesome to see an innovation
then get rid of that concept, get rift upon and kind of radiate out into a huge explosion of
diversity. So if you study the history of biology, you'll see that there were many moments in the
history of biology where something similar happened. We got like, you know, single multicellular
organisms or radar bilateral symmetry or the four legged body plan. And then you see this
explosion of diversity that descends from that central innovation. So I think it's beautiful
to see that happening inside of our algorithms. We ended up submitting the art that was produced
by this algorithm to a competition at the University of Wyoming, where I was a professor.
And every year, art students work for a year and they submit their best project to this competition.
And then there's a judges who decide which of them get hung on the wall and accepted into
the competition. So we did not tell them this was AI generated art, we just submitted it.
And not only was the art accepted, it was also given an award. So here you see people having
wine and cheese. And I was like eavesdropping as they're discussing the intent of the artist
behind producing all of these different images, not knowing that there was an AI algorithm behind
it, which I thought was pretty cool. So in some sense, this passed the artistic turning test.
Sample size one. FYI, in case you're interested, there is much more work on CPPNs that are
more modern. So nowadays, a lot of people are playing with differentiable CPPNs instead of
using evolution. I have to because it's so beautiful. Quickly look at the work of Alex here,
which I highly recommend you check out. All of these things here are different CPPN
represented networks that are doing deep visualization, which is the technique I'm
going to tell you about later. So I encourage you to check that out. There's also you can use
CPPNs to encode neural networks. I did that a lot in my dissertation, and now you can do that with
back prop. David Ha has been pushing that. And there's much more work in this vein.
Okay, so getting back to QD, I think that I hopefully have convinced you that it has all of
these nice properties, like a diverse set of high performing solutions that it produces,
it has goal switching, and it allows you to kind of illuminate the entire search space and learn
a lot about what's possible. Just quickly, I want to say that these ideas really have given us a lot
of leverage on hard technical problems. So in this paper that we had in nature, we used these ideas to
have robots that could adapt to damage within one to two minutes to get up and continue on with
their mission, even if they're extremely damaged. And then we also use these ideas behind the algorithm
Go Explorer, which you may have heard of, which completely solved the Atari benchmark suite,
including solving really hard exploration challenges like mono zoom as revenge and pitfall.
You can see all the previous attempts to solve this heart and his game, which became kind of its
own grand challenge of the field, do not perform very well. And then this is the difference once
you start adding in these ideas from quality diversity algorithms. Ultimately, we ended up
beating the human world record on this game. Oh, and as a quick little teaser, this paper was also
recently accepted to a really nice journal. I can't quite tell you which one, but if I'll
share that information on Twitter in the next couple of weeks, if you are interested to get
the final version and the updated version of this paper. So I think QD algorithms are really
interesting. I think the question that we should always ask though is what's missing where, you
know, they're not yet open ended algorithms. So the thing that I think is missing is that
while these things can produce a large diverse set of interesting solutions within one domain,
ultimately their ability to innovate is constrained because they're stuck in this one particular
setting that we put them in. But what we really want is these open ended algorithms that just
keep going and kind of generating wildly different solutions as they run. So traditionally in ML,
we pick a particular challenge like chess or grow or Dota or Starcraft and we bang away on it for a
while. But the intriguing possibility that I want all of you to consider today is could we create
an algorithm that generates its own challenges and solves them? Just as nature arguably created the
challenge or the opportunity of leaves on the top of trees and then the solution to that challenge,
which is giraffes or caterpillars that can eat them. So this kind of a thing might produce
something that's interesting after a billion years. So our most recent work on this is in this
algorithm called Poet, which is the paired open ended trailblazer. And the idea here is that we're
going to try to endlessly generate interesting, complex and diverse learning environments and
their solutions. So the idea is again quite simple and you'll recognize it. It's basically we want to
generate new learning environments and we're going to add them to this set of our population of
environments if they're not too easy and not too hard for the current population of agents
and if they're novel, there's something about them that's unique and different. And then we'll
optimize agents to better solve each of these challenges and we'll allow goal switching between
them. So the example task that we used here is obstacle courses. So this little creature here
has to run as fast as possible without falling over. And here's the general idea. You start with
an easy environment. So first you have to make that encoding choice. How are you going to encode
an environment on a parameter vector? Here we have things like the number of whether or not there
are gaps, whether or not there are stumps, the ruggedness of the terrain, etc. So you can start
with an easy one of those, which is maybe just flat terrain. And then you start having an agent
which has its own parameter vector. This is a neural network and is learning via RL to solve
this task. And once it gets good enough on that task that we copy phi one, the parameter vector of
the environment to make phi two. And then we'll try this agent via transfer and goal switching.
It goes and it starts optimizing here. Now we are simultaneously continuing to optimize this
parameter vector on this environment and this parameter vector on this environment. We keep
going. Maybe eventually this environment gets solved well enough by this parameter vector.
So we copy it and we now make phi three a new environment. It turns out that's too hard for
either theta one or theta two. So we throw that out. We generate, we try again, we get a new
environment and we test this one and this one. We take the better of those two on this new environment
to seed training. And in this case it was phi theta two. So it goes in there. This does not
have to be a linear chain. At any point, any one of the environments in the set can produce a new
environment. And then we'll try all of the current agents on that environment to see if they're the
best and if they are, they get to start. And the process can keep going like this. Now imagine
eventually we generate a really, really hard challenge like phi six here. And initially the
best parameter vector, we try all of them on this environment was theta five. It was the best
stepping stone. So we start optimizing a copy of theta five in this environment and it gets better
and better and better. But maybe it hits a local optimal and it can't break through and really,
really do well on this environment. But in the meantime, we're still optimizing theta four on
this environment. Maybe it has an innovation that makes it better on this environment. So it
invades this environment just like a species in nature could invade a new niche, kicks out that
parameter vector. And now we start building on the back of this innovation here. And then that
maybe with a little bit more optimization comes up with an innovation that then transfers in and
becomes the best thing we've ever seen on five six. And maybe that gets us off the local optimal
and solves that problem. So that is kind of the nature of goal switching. So here we use evolution
strategies, but any RL algorithm would work. And you can see this little agent here. And it is
traversing this course. And what you can see is at the beginning, all of the challenges are quite
simple. They're a little tiny stumps, little gaps, just a little ruggedness in the terrain. But over
time, the agent gets better and better. And the environments automatically start getting harder
and harder. So it's kind of like a natural curriculum generation. And you can still the
algorithm is here is kind of still pushing in separate dimensions, like taller gaps or more
ruggedness or wider gaps, sorry, given taller stumps. Later in time, with more training, the
algorithm starts to put together these challenges. Sorry, my dog is working. So you get things like
bigger gaps and stumps and ruggedness all put together. And ultimately, this these environments
get really, really, really difficult for this little robot to to each reverse. Here's another
challenge that was invented and solved by this algorithm. So I think from an aesthetic point of
view, it's kind of cool, because you can think about each one of these robots is its own little
creation. It's kind of a curiosity, just like animals in the world, we love to watch nature
shows and see different animals and how they're different and what they can accomplish and how
their bodies are different, etc. So you can kind of think of the agents produced by these things
as really interesting aesthetic artifacts. Scientifically, we wanted to see whether or not
goal switching in this domain was paying off. And so we did direct optimization in each one of these
environments and found that it failed miserably. That's down here. And with poet and the goal
switching, what you see is much, much better performance in each one of these environments.
This is the only way that we know of to go solve these hard problems. In the paper, there's more
of a detailed study about that claim, if you're interested. So I want to show you one anecdote
of what popped out in the system. So I think it's interesting. So here in the simplest possible
environment, a flat ground, you get this agent here that is optimized for a really long time,
and it's got this knee dragging behavior. And eventually, the system generates a permutation
of this environment, which is a harder challenge that has little tiny stumps. And this knee dragging
behavior is not very good, because it keeps tripping up on these little stumps. So with
some more optimization in that environment, the agent learns to stand up. And it gets faster at
that. Now, because the algorithm is always checking any solution to see whether or not it's better
at invading some other niche, this descendant actually goes back automatically and invades that
flat ground, replacing this knee dragger. Now that it knows how to stand up, as you can see here,
it gets much better performance in that new environment. And then with further optimization,
it ends up with much better performance. Now, because this is a computational system,
we could do the counterfactual. We went back to this original agent in the top left, and we
optimized it for an equal amount of computation in that flat ground environment. And it just never
learns to stand up. It's just on a local optimum, and it's stuck in its ways. It was only by going
into a harder environment and coming back, that it learned a better behavior and a better strategy.
And this is why I think that it's so hard to design curricula. You would never, as a human,
say that you're going to take something to a harder environment just to have it solve a simpler
environment. But in this case, that's exactly what was needed to solve this problem. So we go
through and we quantify in this algorithm that goal switching is essential to solve the hardest
challenges generated by this system. So in future work in this domain, I think there's all sorts of
stuff you could do. Obviously, you could just take it into more complex rich simulators.
So you could have more complex encodings as well. But here is like the world's
from deep mind. But I think it's really kind of pumps my intuition is to watch what's possible,
what will be possible in the future with more computation. Imagine what poet could do in a
world this complicated, where it has to do with flying creatures and climbing and talking to
other agents, maybe negotiating trades in a market. And if you were doing all of this, what
might pop out of the system? I think it's fascinating to consider both from an aesthetic
perspective and from a machine learning perspective. You also could optimize the bodies of the
creatures themselves. So in the right, you see, I showed you some work that we did in that vein
a while back, but not with poet. And David Ha has done that in particular environments that
are handcrafted. But imagine if you paired body optimization with environment generation,
then you could really get weird things like you see in nature where you have a particular kind of
like cave dwelling spider that's optimized to that environment, which is very different from birds
that are flying up in the Pacific Northwest. So another thing that I think would be interesting
would be to combine innovation engines with modern tools. So imagine if you took something
like Dolly, which is this amazing new thing produced by my colleagues here at OpenAI, and
not only did you have humans asking for particular innovations or particular images from Dolly,
but you have the algorithm invent the challenge and the solution. So the challenge could be,
you know, can you create this? Can you create this? Can you create this? Dolly would then
create them. And if they're interesting, you add it to a set. And then you have something
that looks at the set of things that are already produced and produces completely new types of
images. That would be awesome to see. And that doesn't have to be limited to images. You could
use then the same technology to do it in different modalities, such as videos and music and poetry
or algorithmic space. Again, the challenge that remains is how do you detect what's interestingly
new? I'll throw it out there that I think you probably with a lot of data could learn on function
of what humans consider interesting. In fact, if Joel remembers, I sent him a giant email saying
that I think we should do this with his website, GanReader. We haven't done it yet, but it'd be
a great project for a student to take on. So I want to quickly check the time here.
Yeah. So we started a little bit late. So I'm going to race through this because I think you'll
find it interesting, but I won't be able to go into any detail here. But part two of the talk,
which I'll do very quickly, is I wanted to tell you about this entire other arc of research that we
did called AI neuroscience, which is how much we want to study. Just like neuroscientists try to
study the human brain, we want to study how much the deep neural nets understand about the images
that they classify. So we're all familiar with deep neural nets, but they tend to be
a black box. We don't really know what each neuron in the deep neural net does. But one way
neuroscientists probe this question is they literally put probes into your neurons and they
look for which neurons light up in response to which images. For example, they found neurons that
light up in response to Kobe Bryant or Bill Clinton, for example. And people have called these things
like a Kobe Bryant neuron, for example, and they respond to very different modalities such as the
name Kobe Bryant, a line drawing him in the Lakers uniform. The question is you don't really know,
just because it responds to those images, if it's a Kobe Bryant neuron, it could be an LA Laker neuron
instead of a Kobe Bryant image neuron, for example. So we thought the ideal task would be to synthesize
the images that maximally activate that neuron. And if you did that and you got these images,
then you'd know, oh, that's a Laker neuron, not a Kobe Bryant neuron. But if you got these images,
you'd know it's a Kobe Bryant neuron. So this is actually possible with artificial neural networks.
What you can do is you can take a neural net, and then you could have like an artist, an AI
artist, that's trying to generate an image to activate this particular neuron here. And what
you can do is you can use backprop. So the artist generates an image, and then you just follow the
gradient to increase this neuron until you get an image that lights up that neuron. And it might
look like this. And you can do the same technique for all the intermediate neurons in the neural net.
We call this deep visualization. Our first attempt at this actually was that same paper,
deep neural nets are easily fooled when we did it, with CPPNs here, or a direct encoding on the
left here, or with backprop on the right. We got images that did not look at all like things
that they're supposed to, but the neural net was perfectly sure is a peacock or chimpanzee.
And you know what happened with that paper. We then went on and started asking questions like,
why are these neural nets easily fooled? And I don't have time to get into a lot of the details
here, but what we basically thought is that maybe deep neural nets do recognize the images they're
supposed to, like a lion or a dolphin, but maybe they recognize a whole lot of other things also
as in that class, unnatural images. So if we could stop the artist from generating unnatural images
and only stay to the space of natural images, then we might find out what that neuron really
is for and what it's been trained to see within the space of natural images.
So skipping over some of the details here, the fooling work started out saying maybe these
deep neural nets don't really understand at all what they're classifying. They're just locking
on the spurious correlations, like that there's an orange texture. If you see orange, you know,
this kind of orange texture next to blue, call it a starfish, but they never learned like what a
five-legged starfish is because they didn't need to to solve the problem. We wanted to see whether
or not there is that notion of like a five-legged starfish in the network. So in take two, what we
tried to do is we added more manual priors to try to constrain the image generator, the artist,
to generate only natural images. And when we add that extra constraint, then we get images, you
know, previously people had done that and they kind of looked like this. You start to see darbells
and sorry, dumbbells and dalmatians. These are the ones that we got with slightly better priors
and you can start to see that the network does actually kind of know what a flamingo is or a
beetle. It's an interesting historical side note. These images here and this work inspired deep dream
which is also done by Alex over at Google and that stuff is super cool if you haven't seen it.
And then third take, we tried to add even better priors, manual manually designed priors and what
you get are these images here. And I want to stop for a moment and kind of reflect on this from an
aesthetic perspective. We're trying to do better and better science. We're creating different algorithms
to or different hand coded priors to kind of accomplish the scientific quest. But if you look
at the different images, each one of them has a different style. And I think it's kind of interesting
that like slight tweaks to algorithms produce wildly different artistic styles. It's like all
these different artists are out there and you just kind of are searching through the space of artists
kind of accidentally while you're doing your science. So this style is very different from
this style. And I actually think this is just really beautiful. Like if I saw this in an art
museum, I would think that this is beautiful art even though it was produced purely for scientific
reasons and we had no intention of producing images in this style. We then went on for one more take
at this. We tried to say, okay, we're machine learning researchers instead of manually encoding
what characterizes a natural image. Let's learn it. And so we start learning the natural image
priors and our papers are full of lots of details on this. And the way that we do it is we have a
generator kind of like the generator and again, we hook it up to the target network we're interrogating
and then we try to search in a latent code to produce an image that activates a certain neuron
in question. And when we did that, we got these images, which at the time were some of the most
realistic images deep neural nets had ever produced. You were seeing realistic lawnmowers
and lemons and barns and candles. These images are not great by modern standards, but this is 2016.
Here are other images in this class. And for the first time ever, the images were starting
to look photorealistic. Like these are the synthetic images for this class and these are
the real images. And you know, I don't think that you would really be able to tell the difference
if I had swapped them, unless you look very carefully. So compared to the best work at the
time, which is on the left, these images were a big step up. And they helped us confirm this
hypothesis, which is basically if this is the space of natural images, these networks do understand
what it means to be a lawnmower. Like if this blue line here is the class of lawnmowers,
then they do stay to, if you keep them in the natural, if you only generate images in the
natural image space, then you do get a lawnmower. But if you let it generate images anywhere in
the space, like all the way out here, then it also the network will similarly say this garbage here
is in the class of what it means to be a lawnmower. And so if we want for aesthetic purposes to
have neural nets generate realistic stuff, we got to get it focused on something that both is
natural and activates the network's classification as opposed to way out here. And GANs do this also,
but they do it via a very different mechanism. So I told you, you could look at each individual
neuron within the network. I don't have time to go through this now. But if you're interested,
then I encourage you to kind of go into the paper and look, you could kind of fly around the neural
net and see that you get things like cargill grill detectors and buckets and bird heads.
And as you go up in the network, you get these really weird concepts like one eye turtles and
like arches over water until you eventually get the class neurons where we know what they are
because we've grounded them VR labels. The one final thing I mentioned here is that the one problem
with our technique is that it generates very little diversity. So these are synthetic images
produced by our network for this class. And they look a lot like the images that most highly
activate that neuron from the real world, from the real data set, but they don't represent the
diversity of images in that class. And so we did a lot of work, including adding with Yashua Benji
on these things called plug and play generative networks, where we wanted to add a lot more
diversity. And so you could take the same network and you can light up a bunch of different classes
that it's never even seen before. That's a bit of an aside like ballrooms and art galleries,
but mostly we were interested in getting more diversity. And the takeaway message is we were
able to accomplish that. So here is PPGNs, which is the one that has more diversity. And you can
see a much more diversity in this set of images versus DGN AMV1, which are the images over here.
And this diversity better represents kind of the diversity of the natural class. So with the original
attempt DGN, you got volcanoes that look like this, it kind of goes and finds one type of
volcano like a local optima and it sits on it. But the plug and play generative networks are
much more kind of like an open-ended algorithm, at least within this class, where it samples new
versions of volcanoes over and over again. And so you get all this big diversity of volcanoes out of
this new sampling technique. So to conclude this, the AI neuroscience part, I won't actually get into
these details, but it taught us a lot about what neural nets, you know, what's going on inside
neural nets, it taught us whether or not they really recognize and learn about the concepts
in our world. Like we did find, in the end, that they do know what a volcano is and you know the
five-legged nature of a starship and what a lawnmower is, even though they also are susceptible
to producing and recognizing these adversarial fooling images as being part of the class.
And it was cool to see the rapid progress just within my own team of collaborators from 2015
to 2017. And since then, I highly recommend the work of Chris Ola, who's continued to push in
this direction. And very, very soon, he, Gabriel Go, and Chris and others have new work coming out
of OpenAI that will blow your mind. So I encourage you to watch the OpenAI blog in the coming weeks
for this new result that you really like. You could do all of this stuff in different modes,
like speech and video, etc. I won't dive into this. I want to just highlight one thing. This is my
future work slide all the way back in 2016. And I thought it would be awesome to do this with real
animal brains. Since then, actually, somebody has done that. They applied it to a real monkey brain
and they synthetically are generating images that activate neurons within the monkey brain,
specifically within the face recognition part of the monkey brain. And you do in fact get a
synthetic monkey-looking face, which is pretty amazing. So to conclude my overall talk, I think
innovation engines are really interesting because they kind of push on this question of can we
automatically produce an open-ended creative process that in any sort of modality like art or
music or invention will just endlessly generate interesting new things. We've got a long way
to go to accomplish that goal, but my colleagues and I like Ken Stanley and Joe Layman and myself
are really, really focused on this goal and trying to pull that off, including now at OpenAI,
where all those people are. And I also showed you very, very quickly some of our work into AI
neuroscience, which we were doing for scientific reasons, but produce these interesting aesthetic
artifacts. And I'll just leave you with one final thought, which is that I find it surprising
how often science produces aesthetic artifacts. Almost none of the work that I was doing was
trying to do it just for aesthetic purposes, but along the way, it produced these things that I
think are beautiful and interesting and can be kind of aesthetic artifacts in their own right.
And so I think it's nice because you don't have to choose between being an artist and a scientist.
You kind of kind of can do both nowadays, especially with the modern tools and machine
learning. And I'm sure that's kind of a realization that is being reinforced over and over again with
all the different lectures in this wonderful class that you are participating in. So with that,
I want to say thank you. And I'll turn it over to either questions or Joel, depending on what
you want to do, Ali. Thank you so much. I appreciate it. It was really interesting
and inspiring to me. And I'm sure for many of us in this class, this comment that you also made
about the science and creating and art, I think that it also is very well aligned with
some of the other insight that we learned from other speakers. For instance,
Alyosha, of course, was mentioning that when I asked this question, he was mentioning that
he also thinks that creativity is a different tier over our evolution. So that really resonated
with me what you are talking today. And I think that this is very exciting for us.
One question that I have is if students want to, because this is a very interesting topic and
especially that type of poet or open-endedness area, if a student wants to join you in this
sort of mission, what do you recommend to them to work on?
Yeah. So one thing I would recommend is we had an ICML tutorial, think about a year ago,
that really covered a lot of this work in more depth. It's an ICML tutorial on population-based
methods. So then you can see, can Joel and myself, kind of going through, this is Joel Lehmann,
not Joel Simon, going through a lot of the work that we've done in this field.
And I recommend reading a lot of the work of both Ken and Joel as well as you can look into
some of the work that we've done in this area. And then in terms of what I recommend you work on,
there's so many things, there's so many options that it's fun. You could apply a lot of these
algorithms in a new domain, for example, that you find interesting, a new kind of art.
You could take more modern tools that work really well and weave them into these ideas,
or you could invent new ideas. I still think if people, if anyone out here can crack the question
of how can you automatically recognize newly interesting things, that I think is like a
Turing Award-winning innovation that will catalyze and propel so much algorithmic advance,
including potentially advancing our push to artificial general intelligence.
Like that might be one of the key stepping stones that gets us there. So I have this paper
called AI Generating Algorithms, which I recommend people check out if they're interested. And it
basically talks about how these sorts of ideas may do the fastest path to produce general AI.
So that's not an aesthetic quest. It's more of a scientific quest. But if you're interested in
that, I think that's fascinating. But I also just think, just literally take all these ideas and
do Poet, but do it in some totally wild and crazy different domain, or do an innovation engine in
like architecture or poetry and see what happens. You can use new tools like GPT-3 or
Dully, et cetera. So I think there's just a lot of low hanging fruit here to be explored.
Certainly. And that also reminds me of what you mentioned. We didn't optimize to create a microwave.
We explored different things. And I think that your advice is quite in that direction.
Also, Joseph has a question. Joseph, would you like to ask it yourself or?
Yeah, I wasn't able to get my mic working earlier. Let me just fix that. Hi, Jeff. I was just
wondering if you've explored anything on Poet in multi-agent settings to this point.
Yeah. The short answer there is that we have a lot of really exciting ideas
for how we want to take advantage of that. I can't share those specific ideas,
because we may or may not be working on them. But I also think, in the spirit of the talk,
that the best way to make advances is to have a community of people with different ideas pushing
different directions, because you never know what's going to unlock. So I almost don't want to give
you too many ideas either because I don't want to cause conversion thinking. I think it's almost
better if there's so many different ways you could apply the concepts of Poet to multi-agent
settings that I don't think you can go wrong. I think if many different people and groups push
on that, really good things will happen. Fair. We also may or may not be working on that.
The one thing I wanted to ask you specifically about that is whether you figured out one,
maybe you can't tell me, but any way to get around the problem where in multi-agent settings,
sometimes you don't have a single evaluation metric that correlates the environment difficulty
with agent performance, because you add more agents in, well, then the performance goes down
because they're more of them and they're all doing smarter things. So that sort of thing is
throwing a wrench in the whole annex measure. Yeah, that's right. So one of the things you could
switch to is a notion of agent versus agent, you know, like if an agent is as opposed to doing
better on that environment, it's that agent versus other agents or agents that I've commuted
before. Another thing you could do is you could switch to more of a learning progress metric,
which is if they're getting better, you know, are they learning? According to some measure,
like there's their value function, their prediction of how well they're going to do,
is that wrong? It's because they were either better or worse in that situation and versus
those opponents than they thought they were. And measures like that could really catalyze,
recognizing this is still an interesting environment because they're learning.
This is still an interesting matchup between this opponent and this opponent because they're
learning. I mean, we've been actually trying to do something very similar there. It still seems to
run into the same sort of issue though, right? If you can't measure absolute performance,
it can still be difficult to then measure relative performance because your reward
can be going down even if you are learning because so are all the other agents.
Sometimes you have to run as fast as you can just to remain in the same place.
Yeah, pretty much.
That's the red queen quote from Alice in Wonderland. Yeah, these are all challenges
and it's the kind of challenge that happens once you get into the multi-agent setting. So I think
this is just for a lot of experimentation and hard thinking has to happen. I don't think there's like
a really super, short, easy, obvious answer. It's just going to require research.
Well, I mean, I look forward to seeing what setting it is that you're trying that out in
whenever that gets published. Likewise, yeah, with your work.
Thanks. Excellent. Are there questions? Any more questions?
Guys, don't be shy. If you have questions, please go ahead. Of course, if Jeff has time.
I have time. I just also want to be cognizant of Joel and giving him his proper time.
Excellent. Okay, cool. Thank you. All right, then let's thank you again.
Jeff was really, really interesting and inspiring.

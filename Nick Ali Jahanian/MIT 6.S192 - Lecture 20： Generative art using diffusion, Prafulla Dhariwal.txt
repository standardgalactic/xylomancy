Hello, everyone. Welcome to your course, AI for Art, Aesthetics and Creativity. Today,
we have a very special speaker, Prof. from OpenAI, and he's going to talk about generating
art and artistic work and images in general, these diffusion models, and probably you have already
worked with the glide collab, so he's going to walk us through that as well. So let's get it started.
Prof, I always ask if please you can share with us what motivates you working in this space and
also giving us a little background about yourself. For sure. Thanks for having me here today.
By the way, this is really exciting. Yeah, so a background about me. I was an undergrad at MIT
in computer science and math, and then after that, I came to OpenAI to do AI research and
I've been here for five years doing research on unsupervised learning, generative models,
personal learning, all kinds of things, and what motivates me to do this research.
When I was in college, I was excited by the idea of just trying to understand what makes humans
intelligent, and I think I attended a few talks, which were really amazing, and I felt like there's
a lot of amazing progress happening in this field, and I just wanted to see what's happening,
see what I could contribute. And then one thing led to another, and here I am. I think so far,
wouldn't say we are very close to unraveling what makes humans intelligent, but we've made a lot
of progress, I think, in these years. So it's been pretty fun. Cool then. So let's get started.
And anyone, feel free to just ask any question at any point, pause me if anything feels confusing,
if any notation isn't understood. I don't see the chat window on my screen directly,
so if you could just directly tell your question, that would be easier, or if only if you see
something in the chat, just let me know. Sounds great. I'll get started. So I'll begin by just
showing a few examples of very powerful, creative ML models from the past few years.
The first one you all might have already seen in news GP3, the language model from OpenAI.
And one example I'm showcasing here is these language models. You could show a very few
examples of something pretty simple here. On the left, you see examples of poems by
Pacific Black writers. And on the right, then you can see once the model has seen
examples of this kind, what it can generate. And it's getting poetry from just a few
examples. This is pretty crazy. There's a second model I'll show you next.
Uh, is the audio playing?
Yeah, we can hear your audio.
Um, anyhow, so that was a sample from
a model for generating music called jukebox. And everything here was generated from the model,
the music, the singing, how to sing it, how to, you know, like pronounce the lyrics and everything.
All the model was given was the lyrics and an artist and it produced all of this by itself.
That's the generative model, like produce the music and like the sentence separately or together?
Everything together. Yeah, so, because kind of how you sing something kind of has to go with
the music that's provided vice versa, right? It's kind of hard to like generate them separately
from each other. Um, what was that your question?
So I can read from the chat that people are getting excited. Someone has allowed
another is saying this is freaking awesome. Thank you. And yeah, share or talk if you like.
Oh, yeah. Sorry, I can't see the chat windows. Feel free to just talk.
Yeah, when we heard samples from like that from these models, we were also amazed.
The next model I have here is the glide model you guys might have seen in the papers.
And here you have a model that given a text prompt is generating a visual representation
of it or whatever it imagines what that the text kind of signifies. So you could see,
and these are things that are go back to your point, Ali, you made earlier about composition.
These things involve a model really having to compose a lot of different concepts together,
like robots meditating in a vipassana retreat, but it is able to imagine this. So in the last
few years, I think like ML models for such very hard creative tasks have become really good.
And today we'll see like, what are some of the kind of concepts driving this progress?
And so before I even start down that route, like, why are we trying to, you know, from a research
perspective, like trying to train models that, you know, create things? Well, one concept here
is that, you know, as this code by Feynman says, what I cannot create, I cannot, I do not understand
training models that can, you know, create things, the images or whatever you own so on.
It's kind of one of the hardest tasks in those domains. And
if you really care about whether models can understand images, audio, video, or so on,
then one of the best ways to know if you're making progress out of these models are really learning
something advanced is to see if they can create really complex things and really hard to understand
things. And for people who care about representation learning or something, this is one, one way
you can know you're making progress on such stuff. And there has been a lot of progress
in this field of, you know, trying to create things from models or what we call generative
modeling. So here you see just in this very small domain of phase generation, things that
GANs could create in 2014, versus things that they can create in 2018, like, it's absolutely
astounding how much progress has happened in the past few years. So what is a generative model?
So what is a generative model? So you could think of what are inputs here in our dataset to look
like just a collection of examples, x1, y1, x2, y2, xn, yn, where x here represents,
let's say, an image and y some label or some other information describing this image. And you just
have the sample from some natural distribution of images, p of x comma y. So you could have,
like, images of corgis, ostriches, goldfishes, and so on. You want to train a model that can learn
this distribution. You want to train a model that then asks for a corgi, produces a corgi,
and asks for an ostrich, produces an ostrich, or so on. So it wants to learn p of x given y,
given some label y, corgi, ostrich, goldfish, or so on. Can I generate a real image or an image
that looks real from this distribution? And once such a model is trained, you can use it to generate
novel samples. So you can generate corgis, ostriches, goldfishes that are actually real,
haven't been seen before, but look like real images. One of the things, I guess, that matters is
how you evaluate such models, because if you don't have evaluation metric, you can't tell
you're making progress. And we won't go into too much detail here about these metrics,
but one of the metrics used was FID for measuring image generation. And what these
metrics are trying to capture is fidelity and diversity. Fidelity would mean how realistic
or how correct an image looks versus diversity would be how many different kinds of images
such a model can generate. And oh, so GANs were kind of like the state of the art for
difficult image generation benchmarks before diffusion models came along, which we'll not
talk about in our talk today. So the progress in diffusion models has been pretty recent. It's been
just like the last couple of years, there's been a lot of papers. And even in these people,
where you could see like things have been improving since 2020, and you could now generate realistic
faces, lots of different categories of images from ImageNet and so on from these models.
So it's a pretty exciting field. And these models, in one of our recent papers, we showed them to
be actually better than GANs at generating images. And so I'm pretty excited by these models. And
that's what I'm going to cover today. There's quick graphic here for how things look like when
diffusion models generate an image. Let me just play it again.
So let's go to what these models are. As you saw in that graphic, you could see that
that image was started out looking like noise and you finally got a real image out by like
this slow process of noise converting to an image. And what is actually happening behind
the scenes here is you have a fixed process that adds noise to a training image. So let's say you
start with X0 on the left here as an image. So that's just a dog ball on the left. They have a
fixed process that slowly adds Gaussian noise to this image. So at each step, you add a little
amount of Gaussian noise. And as you go from left to right by the end, at the last time step B,
you have just pure noise left. And what the model is trying to learn is to undo this process.
It's trying to reverse it. It's trying to take some noise damage and denoise it a little bit,
make it a little less noisy and so on. And how do you obtain a generative model out of this? Well,
if you train a model to reverse noise like this, then at test time, when you actually want to use
the model, you could start with pure noise at the end. You could start with XT. Then you could run it
step by step backwards to remove noise from it and try to produce an image from it.
Any questions on this diagram?
Okay, I'll just check the chat too. There's a question there.
Okay, no questions.
Okay, so let's remember the notation from your X0 is an example from the dataset, XT,
capital T, XT is noise and there's intermediate steps, X little t's, we call them,
where you have some slightly-noised image. And we can, to introduce more notation here,
we could represent one step of the forward-noising process with a distribution q of
X little t given X little t minus 1. And right now, I'm going to use Gaussian noise as a
noising process. So we're going to add a little bit of Gaussian noise to this.
All you want to think about here is, in this notation, there's some mean, which is centered
around the original noise-image XT minus 1 and there's some variance 1 minus alpha t here of how
much noise that is being added to this image. So this is the forward process. This is the process
that adds noise to images q of XT given XT minus 1. What we are learning is the reverse thing.
We want to denoise this image. So you are learning p of XT minus 1 given XT. And what you can show
is for very small, noising steps, where the amount of noise added is very tiny, the reverse
process kind of looks like the forward process. This is kind of a set of points. Maybe I'll go
in a little bit of an example here. Let's say, looking at a single pixel here, it had some real
value on the real line, let's say 0.8. And then you added a tiny amount of Gaussian noise to that
thing. So it became 0.81 or 0.79 or so on, depending on what noise you sampled. Now, you're given this
new value, 0.79. This is the noise value. And you want to predict the distribution of what could
have been the value it came from. If the amount of noise added that you added in your step was
very tiny amount of Gaussian noise, then the reverse prediction also looks kind of like a Gaussian.
So it looks like, oh, it's somewhere around 0.79, some distribution that where you came from.
I mean, 0.8 is pretty close to 0.7 in this situation. And so you could write that down
as a model that is predicting the mean of this reverse process and the variance of this reverse
process, mu theta being the mean and sigma theta here being the variance that you're trying to predict.
So far, so good. Any questions at this point?
Okay. So to summarize the notation again, x, x t minus 1 to x t is a process that's adding noise.
And the process we are trying to learn is x t to x t minus 1 to reverse this noise. And this looks
like a Gaussian in the forward direction and predicting a new Gaussian in the reverse direction.
A paper showed that you could do some forms of training tricks to make this process simpler.
You don't have to add noise little by little at every step. You could just directly sample
an intermediate x little t given your data example by just adding a lot of Gaussian noise to it.
And it also showed instead of trying to predict the mean of the reverse process,
you could just directly try to predict what noise was added to the image.
You could write the mean in terms of the noise that was added.
Trying to predict the variance can be just simplified to just using a fixed variance
or a learnt variance. We won't go into that today. All you should get from this is that
to predict the reverse process where you are trying to predict the mean and the variance of
the Gaussian to reverse that noising process, it's enough to try to predict what noise was added to
the image. So what does this look like when you are actually training these borders? You take an
image x0. You sample some random noise. You sample some Gaussian noise. And you just combine these
two to produce a noise x t. There's a formula here of how we've combined them. You can think of
this combination as something that is interpolating between the image and the noise.
So at t close to zero, you should just get the image x0. And t close to capital T,
you should get complete noise. And this kind of interpolation factor alpha t bar
kind of plotted it here. It goes from like one near t equals zero to zero near t equals capital T.
This kind of controls how you interpolate between a fully denoised image versus a fully
denoised thing. And at training time, you're just sampling all possible combinations of mixing
of noise and image. And you're trying to denoise all these combinations.
So what is the model trying to do now? It's trying to predict, as we said in the earlier
slide, it's trying to predict what noise was added into the image. So you take in the noise x t,
you take in what time step or kind of like an indication of where in the process you are,
you tell the network, hey, I'm at this step in the process. This is my noise image.
What noise was possibly added to this image? So it's trying to predict epsilon. And what it's
being used to train with is just like simple L2 loss, like just take the mean squared error
of the difference between the network's prediction and the actual noise that you trained with.
And you try to minimise this loss so that you can train the network to predict what noise was
added into the image. So intuitively, you can think of this as like, well, if I'm given a
noise image, and if I can predict what noise was added to it, I can kind of like subtract that
noise out, try to get a real image out. And this is kind of what is happening when you're training
a diffusion model. It's learning to denoise images. Any questions so far?
No questions? Okay. So what does the model that kind of does this denoising usually look like?
Kind of models that we have in our papers usually look like these convolutional unit style models,
where the U kind of signifies kind of like how the shape of the
model here in this picture is looking like. But to think of it is just like
a model that runs a bunch of convolution of images, it kind of like down samples the image down
into smaller and smaller spatial fields so that it can like learn features at different levels
of granularity and then kind of samples it back into something that looks like a prediction of a
noise. I don't have to go into the details of architectures, but just to give you an example
of what kind of neural nets are trained to perform this task, this is how they look like.
So okay, we have a model that is trying to denoise images. It's trying to predict the
noise that was added to an image. How do we go back to actually getting a generative model out of
this? As we talked about it in an earlier slide, it's equivalent to predicting the mean of the
reverse process. Like you can write down the mean of the reverse process in terms of the noise
prediction. So now that you have a network that can predict the noise that was added, you can
also write down a network that can predict the mean of the reverse process. And once you have
something that can tell you the mean of the reverse process, you can run the reverse process
backwards. So you can start with noise x t. You could run, you could sample from this reverse
process p of x t minus one given x t. You know the mean and we have fixed the variance to something
to do one step of sampling from this process. You do one more step, one more step and so on.
And as we talked about what we're doing was denoising, right? We're trying to like
learn a process that removes noise from images. So if you start from pure noise and you're
denoising it one step at a time, by the end of the process, you would have something that looks
like a real image. So what we've covered so far is how you train these models and how you sample
from these models. Any questions so far?
Okay.
It's okay. A bunch of theory there. But what you should remember is you train for denoising
and you can derive a sampler from it once you've trained for denoising. What do you do next? Well,
you could now make the model class conditional. You could provide labels at training time. So
you could provide, you know, let's say you're training on ImageNet or something, you could have
labels that say this image is a goldfincher, this image is a corgis or so on. And you could make the
model, the denoising model class conditional. You could provide these labels, the model so that
given this label, it tries to produce an image from p of x given y, like the distribution of
images that are represented by this label. And it's pretty simple. You just throw in a label
into the model at some point so that it now has this extra information when it's trying to denoise
images. You could also do something like upsampling. You could ask the model, given this low resolution
image, what would be kind of the high resolution image that could be generated from this? So again,
just like throwing in a label y, you can throw in a low resolution image as extra conditioning
information into the model as it tries to denoise. So we've now talked about models that are class
conditional. The thing is, if you just train models like this, where you give them a label for an image
and you train them for producing the image given the label, they're not very good at doing this
out of the box. They kind of produce very incoherent samples. And one of the tricks that we developed
to kind of fix this was the trick of guidance, where what you do is you train a model to look
at the images that are being generated, use a classifier to classify what is the label of this
image. So you kind of look at a noisy image and you're like, you know, whether you ask the model,
hey, does this look like a dog or not? So you train a classifier on these noisy images,
then you take a gradient of the classifier, you ask the classifier, hey, how can I increase
the likelihood to make this image look like a dog? Because you can run the classifier forward,
you can get a get a probability from the classifier of it being a dog, you can also differentiate
this function to get the gradient of how to change the image so that this probability increases. And
then you augment your diffusion model with such a classifier to kind of guide it towards
generating images that are more likely to be classified as a dog by the classifier.
So how do we end up doing this in practice? Okay, so you can train a classifier on noisy images,
you can just take your data set of images, noise them and train a classifier to predict the label
of the noisy images. And then how do you guide now your generative diffusion model
to use this classifier? Well, you run the classifier on the noisy images, you predict the
probability of, you know, the class label under the classifier, so whether something is a dog or
not, you take the gradient of this prediction to obtain kind of direction for which the model
should change its input to increase the probability of this image. You can add this direction into
the mean of the reverse process that you are already going towards. So in terms of the actual
formula, it just looks like adding an extra term to your mean prediction, which is the gradient
of the log probability of the prediction of the label given the noisy image. Questions on this,
this is important and this could be a little complicated.
It seems there are two questions before this. Can you read them?
Do you still need a classifier once the model is trained? So here by the model you mean the
diffusion model, right? Yes, you still need it because it is part of the sampling process.
You're using gradients directly from the classifier in the sampling process,
so you still have to keep the classifier around when you sample from the model.
Okay, let me look at the next question. This is the underlying representation of the classes for
the model. I'm not sure I followed the question, Ben. Could you explain?
Okay. I have another question. The s in the term, is that just a hyperparameter?
Yeah, we'll get to that in the next slide. But yes, that is just a hyperparameter.
Yeah, the main thing from this slide is like we previously had a reverse process
that looked like a Gaussian with some mean mu and some variance sigma. We now have a modified
reverse process where we've just modified the mean mu with an extra term which is scaled by
this hyperparameter s, has the variance in it for appropriate scaling as well, and it has this
gradient term, which is the gradient of a classifier on noise damages.
And we're kind of basically using this gradient to kind of guide the model towards directions
where the classifier would predict the higher likelihood of the label being correct for the
noise damage so that the conditional model produces an image that is more correct.
Another question. Why is the variance in the additional term also included?
That's just how it popped out from the derivation. I guess you can think of it as kind of like the
step size of these things is controlled by the variance. So if you have a Gaussian with a very
small mean in the reverse process, then you don't want to take a really large step with your gradient
because you'll pop out very far from where the reverse process should have taken you.
So if the reverse process is taking really small steps, which can be thought about by its variance,
then you also want to change that process only by that much amount. Does that make sense?
I see. So I guess the variance term is like a cap in step size. Right.
Select the maximum and the classifier gradient is maybe somewhere between 0 and 1 or something like
that? I don't know if it has any explicit range here, but I mean it's kind of mostly just direction.
And you're kind of scaling this direction by a step size. And the extra hyper parameter is if you
want to kind of like make these steps bigger or smaller than what is naturally there. So that's
the extra parameter s that we'll talk about in the next slide. Okay. Thank you. Yeah. Okay. So
the parameter s here. So what we found was if you just actually use the step size that pops out from
the derivation, so s being 1, no hyper parameter, it kind of doesn't do that much. So on the left,
you see the samples with s being 1. They don't look like any image from any particular class,
but it turned out when we added this extra hyper parameter and just bumped it up.
So you have scale 10 here on the right. They actually start looking like samples
from the distribution of a particular class. So you can think of this extra hyper parameter s
as kind of helping the model focus on the modes of the distribution because you're kind of narrowing
down the possible things that it produces. At least that's what we saw empirically. However,
the trade-off here is because you're narrowing it down well, they also kind of start looking
similar to each other, the images that are produced. But anyhow, the way to think about
the scale factor here is just that it's controlling how much guidance you're using, how much is the
classifier influencing the final outcome. And when you use a small value for s, it's not influencing
that much. When you use a large value, it's influencing a lot. And the effect of that influence
is you're kind of collapsing your distribution towards the modes of whatever the classifier
thinks is kind of the best representation of that label. So a very high scale will just collapse to
the thing that the classifier is most likely to classify as that label, which is not always
what you want. You want some kind of diversity in what you produce. So there's some
like intermediate value of scale that is kind of the best that you want to use.
And this is kind of how the process looks like in practice here. On the bottom here, you have a
usual diffusion process with scale. Let me see, I can't see the image. So scale zero, you're just
using no guidance. And then when you turn on guidance, now you're using the gradients of the
classifier, kind of nudge the process in the direction where it's more likely to produce that
butterfly. As you bump the scale up even higher, you're nudging it even further out from its original
reverse trajectory into this new trajectory, where it's now producing a very clear butterfly.
So the scale parameter is kind of controlling how much guidance is happening and how much the
model is being nudged out from its original distribution towards this new better distribution.
So you could do similar things instead of labels, you could now have text descriptions of images.
So same model class, you're still conditioning on something, but this conditioning thing, why,
instead of being a label is now a piece of text that's a robot's meditating in a past retreat.
And you could train basically the same kind of models, all you have to now change is, well,
you don't have a classifier now, right? There's no classifier that is predicting a label,
you have to predict the whole sentence if you try to do that. So how can we do guidance in this
situation? Well, first let me go in, wait, did I skip a slide? Oh, okay. Well, first let me go
into how you can even pass in conditioning information to diffusion models, which look like
text, you just, you can just simply run a transformer on the text and just attend to
the representations of the text in the model. This is not too important, it's just a pictorial
representation of how to deal with text being passed into these models. You can just run a
transformer model on this and just have your original convolutional unit architecture attend to
this model when it's trying to do the denoising. But back to guidance, how do you actually guide
when you have text as the kind of label information? One of the things you could do is
use clip. I think you guys covered clip in a previous lecture Ali, you said that, so you can
Yeah, it sounds good. Yeah, so I'll skip clip. Okay. So I'm assuming you guys are on the clip,
but basically clip is a model where you have an image encoder and a text encoder,
and it's trying to predict how close the representations of the image and the text.
And so you can use clip for guidance, you can ask, hey, I have this noise image, I have this text
description, run the image encoder, run the text encoder from clip, how close are these
representations? If they are close, then you're going to get a high dot product here, you can
take a gradient of this dot product and get a direction to increase this dot product. And
that's the gradient you're going to use for guidance. We're going to ask the model, hey,
can you increase this dot product so the image, the image that you're trying to produce from
the reverse process is close in representations to the representation of the text that you're
provided. So this is how clip guidance works. A different thing you can do, which we showed
which was showing a paper on classifier free guidance is you could skip the classifier completely.
And just train a usual diffusion model for reversing the process, but train it sometimes
without labels. So sometimes don't tell it what was the text that described the image. And then
at test time, you ask the model, which direction should it go, given the label, and which direction
should it go without the label. And then you move your predictions in the direction of
the model predictions when it was given the label. So in the formula here, if epsilon was the epsilon
theta x t given y was the prediction of the model with the label and epsilon theta x t given
the empty set phi was its prediction without the labels, you're kind of taking the difference
of these two and using that as your direction to kind of nudge the model. And again, you have
the scale factor s outside of this direction. It's telling the model to move in the direction
of the predictions with the label. And when you use s greater than one, you'll be moving a lot more
in the direction of the predictions with the label. The cool thing about this way of guiding
is that you don't need a separate classifier or a clip model or anything. You're just using
the diffusion model itself for guidance. You're directly just asking the model its own
kind of prediction of which way it should go to increase the probability of the generated image
being from the correct class. Any questions about classifier free guidance?
One question I have is, have you thought about implementing something like, okay, at each stage,
for instance, let's talk about the butterfly example. At each stage, I want to add
something to this image. And so the text can gradually form the shape or the image like,
okay, I want the butterfly and then on top of it, I want this flower and then
this, you know, gradually giving more idea of how your butterfly want to be depicted.
Because you are doing this in steps for image and then you are injecting the
tokens from the clip to your, you know, your network for image generation. And so what if
gradually adding things that you want to be in that image? Yeah, that's a great question. I haven't
done this, like we haven't done this directly, but you can kind of do this, right? You could run
your reverse process to some point with your text conditioning being just the simple thing,
hey, it's a butterfly. Then you could continue with a new text from guidance, hey, the butterfly
looks like this or so on and keep going. Maybe that works, not sure. You could do something
else, but you just run the whole process, first generate a butterfly. Then you take the butterfly,
you noise it to go back in the process and then rerun it, but now with a different prompt. So
you're kind of modifying this generated butterfly in a new direction. Then, you know, noise it again
and rerun it again with a slightly different prompt. You kind of be like slowly changing this
generation iteratively in these like kind of like iterative modifications.
In another slide later, we'll show how to do this with something like in painting,
but if you just wanted to do it for your direct image, this is maybe how you would do it. Does
that kind of answer? Yeah, yeah, I think that's a very interesting, you know,
thought and yeah, I appreciate your answer. Yeah, I see that in painting could be one way of thinking
about it. Yeah. Yeah, but what I was trying to say, there was like, yeah, you could also do it without
in painting by like kind of modifying the full image by like re-noising it and reproducing.
Yeah, yeah. Yeah, that also makes sense. I think that I was also referring to more like just the
way that by removing the noise, you are, you know, trying to somehow refine the image.
This also in steps could, you know, add more context to the image. And there might be different
ways of implementing it. Yeah, I think that's good. Okay, thanks. Next slide. Okay, so in our
guide paper, we kind of compare these two forms of like guidance for text conditional models,
clip guidance with classifying guidance. And here are a few samples, like representative
samples from the model. So on the left here is just samples without any guidance.
This is just a pure conditional diffusion model. There is no form of classifier clip guidance or
classifier free guidance. And kind of see, you know, it's kind of getting the prompts of the
pure esteemed glass window of a panda eating bamboo. It's kind of alright, but it's not very
coherent. Then you do clip guidance with scale two, and it starts getting better. But the classifier
free guidance once looked the best in all the tests we did. And I think part of this might be
just that it's not exactly the correct thing to use a clip. Part of it just might be that
it's kind of better inductive bias to use classifier free guidance. It could be a lot of
reasons, but at least empirically, this was working better in practice to generate more
realistic samples from these models. And you can see guidance does make a big difference in
generating more realistic things. But it also does kind of make, you know, it's kind of like
mode collapse effect. And all of these samples kind of start looking a little similar to each
other when you do a lot of guidance. So what have we done here? We've trained a model that,
you know, given a text prompt, can generate images, and we've done it for this diffusion
technique. And this was what we trained in the glide paper. And we then showed that this model
actually was beating the older open AI model, Dali, which was actually a bigger model, which was
trained in a very different fashion. The strain is using an autoregressive model on these like
discrete VA tokens. And the new diffusion model not only generated things that looked a lot more
realistic, it actually generated them faster and use fewer parameters. So this new model class
is actually a lot nicer to use for these tasks than the older class of models.
One cool advantage of these models is also because they're not doing this thing autoregressively
there, you know, just generating an image, you can do these things that are much harder to do
with these older autoregressive models, you can do things like in painting. So you could mask out
a portion of the image, and then ask the model to kind of fill in that portion. And how would you
do that? Well, just like we passed our conditioning labels in the past, you could just pass in kind
of this like half filled image as extra conditioning information to the model.
So you take this image and a mask on top of it, you provide this extra information
to the model when it's trying to run its generative process. And it's going to try to now
think of this as a label. Hey, like, this is the image, what are the possible images that
correspond to this label or this kind of masked image? What are the things that could complete
this image? And what you're providing is this kind of like image X0 with a little bit of region
masked and a mask M that tells what is the part of the image that has been masked.
You could also now do text conditioning in painting, you could provide an image with a mask
and you could also provide a text label to tell the model how it should impaint the region. So
these are examples from the paper. So on the left here, you have the text label being Zebra
is roaming in the field and you have this image with a green mask on it. So the masked region
was removed. And the model was asked to fill in this image conditioned on this product. So now
it's going to try to fill in not only something that kind of completes the image correctly,
like isn't the conditional distribution, but also kind of matches the product.
And on the right here, you see something with a girl hugging a corgi on a pedestal. And it's
kind of matching the style of the image very well here. If you can see, it kind of looks like
it's like painting and it's like kind of nicely like blended in. So this is a really cool thing,
which you can do very easily out of the box with diffusion models, but it's kind of much harder
to do with other classes of models. And you could take this idea iteratively, like you could
now erase a region of an image. So let's say we erased the region on the left here,
and you first filled it in with a cozy living room. Then you erased a different region
and asked for a painting of a corgi on the wall above the couch. Then you get a painting there.
Erase another region, put a coffee table, put a flower vase and so on. So this is one way of
doing the thing you talked about Ali, where you kind of like generate things iteratively.
But this is doing it through painting, erasing regions, erasing very specific regions,
then asking the model to fill that region in with the thing you want. So this doesn't cover all
kind of modifications that you want to do, but it does cover things that you can represent as like
adding things one by one into an image, if that makes sense. So like stuff that you maybe cannot
do with this is like, you know, change the style of an image completely, the full thing
cuts well. If you just erase the whole thing, you could, it wouldn't have anything to condition it
can't change the style. But things like this where you add things, you could do pretty easily
through iterative painting. Any questions so far on the impainting side of things?
So Linda is asking if the collab is available for impainting, I think I saw it on the website.
The collab shows impainting. Yes, the collab that we released in the Glidery
the third one is the second one that one does impainting. Basically, all you like to do there
is you'll have to provide this extra image, and you'll have to provide a mask or like mask out a
portion of the image and then provide a mask that tells what has been masked. And then you just run
the guided diffusion process as usual, but now with this extra information to try to
impaint this region. I'll go into kind of notebook later, but yes, the notebook's there.
Diego also has a question.
Can you remove objects using impainting? Yeah, I mean, so let me go back to the slide.
I mean, I guess, technically, in the very first one, we removed the thing, right? We just masked
out whatever was on the wall in the left. And here we ended up adding a painting of a colleague,
but you could just ask for nothing. And then it would just fill it with the wall. I don't know
if there's an example here. Yeah, in all of these things, we kind of changed something, modified
something, but if you just don't give any prompt, it's just going to try to fill it
without any extra information, just trying to make the best possible completion.
And that would be kind of like removing an object.
Does that answer your question?
Assuming yes, I'll move on.
Okay, well, you can take this idea further and you can do outpainting kind of. So like,
previously, we drew a mask that was inside the image, but you could also kind of move the
rectangle that the model is focusing on outside of the image. So now the mask looks like a strip of
things around the image that is masked. And you can ask the model to fill that thing. You could
keep moving this rectangle around to kind of expand out from an image. This is something
that Holly Herdendon did. She like took this central image here and then she kept moving
the square out, kind of expand out the canvas of the model and ask it to keep filling in
extra information outside of the region. And then at the day for the model, there is just
like some conditioning information, some mask, and it's going to just try to fill in
in that region, whatever it thinks is the best possible completion. It doesn't have to be inside,
it could be outside as well. So one other, I guess, important thing is like, we talked about
the release notebooks. The release notebooks is kind of the released model, which is the filtered
glide model. So in our paper, we talk about this where we looked at kind of the things you could
generate with the big original glide model. And there were a lot of like problematic things
that it could generate that made it unsafe to release the full big model to release a smaller
model on a filtered data set. And it cannot generate things that look as impressive as the
big model, but it still kind of can generate realistic looking images for like some of these
easier prompts. But yes, there was going to be a little bit of a performance gap between using
the filtered small model that has been released versus some of the best images you can see.
That said, you can still generate a lot of cool things with the small filtered model. These are
some of the things I found on Twitter that people have generated with the notebooks that we released.
So on the left here, I think what they did was they kind of did the outpainting thing,
but they just went kind of like in a panorama fashion left to right, and kind of kept asking
the model to fill these landscapes. And I think they turned up the guidance scale a lot to make
it very artsy. I think in the right, they've done this outpainting thing, but I don't know how
they got those structures. But I think a part of this, a part of the fun stuff here is kind of
these prompt search or prompt tuning things where you kind of find these prompts that generate
very specific kind of artistic styles. And if you find very cool prompts, then you can now use
these tricks of outpainting and so on to kind of like keep expanding it out to generate these cool
pieces of art. This is another thing I found on Twitter where they trained a classifier free
guidance model on conceptual captions. And I think this is like a flower, a space, flower with some
space art theme. It looks super cool. There is a question. You want to read it? Let's see.
What did it create that was dangerous? Or maybe there was a comment.
Oh, sorry. I think I, yeah, what did it create that was dangerous? Yeah, I guess for all details,
I would recommend just reading a paper. I mean, there were, and I wasn't the one who did the safety
analysis here was the people who work on safety at OpenAI and Alex. But I think it was stuff like
violence, it was stuff that could be used for like, for like misinformation and so on.
But I mean, these models are pretty powerful. So you could generate lots of things that you don't
want to be floating on the internet. I mean, the tradeoffs are hard here, right? Because like,
on the one hand, you do want to, you know, put these powerful models out there in the hands of
people to like, generate all these nice cool art and like, like lots of positive use cases, right?
But I think you want to also be conservative to not create a lot of like
stuff that you don't want floating around on the internet that's associated with your models.
But this is a tough tradeoff. I think it's nice that we can still release some safe
models that people can use. But making these models like fully safe, well, they never generate
something that is like, kind of like, not a, not a good thing. It's very hard problem in general.
I think you could find more like detailed examples in our paper, if you're looking for like specific
examples. But that was kind of our line of thinking on like releasing like the small filter version.
Maybe a slight tangent, but what does the process look like of let's say calling,
you know, the unsafe parts away from the model? Like, how do you go about that?
That involves usually, I guess, training. Training kind of like these classifiers to
filter out portions of the data set that could be like not safe. Like you could, you know,
train an NSFW classifier, you could train a classifier for like hate symbols, you could train a
classifier for like lots of different things. Then you, once you have labeled data on which
you can train these classifiers, labeled data for like, I don't know, real images that you consider
things that you don't want your model to generate. You could run these classifiers on your training
data set, filter it out, then train a model on the filter data set. So hopefully the model will
never go into regions where it can generate something like that because it was never part
of the training data. Awesome. Thank you. Okay. So just a quick look into the notebooks that
we've released. This is just like some useful knowledge. So like there's a few parameters
that you will have to like kind of like deal with when you're trying to generate stuff from
the notebooks that we released. Well, there's the two scales we've talked about and I talked
today, the classifier free guidance scale and the clip guidance scale. Small values for these
scales will generate, you know, more diverse, but not very coherent samples, larger values will
generate more coherent things. Very large values will generate like very artsy looking things.
So like for classifier free guidance scale, like I think three might be the default,
but you could try five, 10, 20 or so on to generate more artistic things.
Similarly for the clip guidance scale, time steps kind of controls like how many little steps you
take in the diffusion process. I think by default we use 100. There's 100 steps of like iterative
denoising that would happen. So if you use a higher value to look more sharp, but you'll also
spend more time generating a sample. So on it was a good like tradeoff that we used in the thing.
Finally, for the in-painting notebook, you would have to provide an extra thing which is like
what is the region of a given image that you want to impaint. So you would have to provide
let's say a 64 by 64 image that you want to impaint and some region in it that you've like
removed, that you kind of specify with a mask, which is like I think one in places where the
image is not masked and zero in the places where the image is masked. I could be wrong on the zero
versus one. So you should check the notebook for which direction. Basically it's a binary mask
that tells this is the portion of the image that is masked. This is the portion of the image that
is unmasked and the rest is like just a usual image with three channels that you provide as
extra information to the model. And do you just upload that as an image file? I think the way in
the notebook that works is so if this is on a co-lab you'll have to have the file on drive and
then you open it using pillowimage.open or something. I don't know if there's like a direct
upload button. But I guess like oh sorry I guess my question was like when you add the mask,
like the mask is just like removing part of like a regular image file or like there's something
more to it. Oh no yeah that's just removing parts of the program file. So like I think
if you want to do it programmatically just just zero out that region.
Does that answer the question? Yeah thank you. Yeah there's an example in the notebook. There's
a cell in the notebook that kind of masks an image that might be more clear where you can see like
you are loading an image from the disk then you are kind of like removing a region then you are
kind of writing down a mask that specifies what you removed and then you pass in all this information
into the model. I think that's it for the stuff you will need to kind of like apply this thing to
the notebook. And if you want more further reading or like what we talked about today I mean I try
to focus on mostly like things you will need to understand for like kind of generating art from
these things but you want to go into more detail about the theory of these models. I think the
best paper would probably be the denoising diffusion probabilistic models paper by Jonathan Ho
the DDPM paper. That has kind of like the basic theory of the models that we talked about today
and there was our paper on diffusion models began as an image and this is that kind of talks about
the guidance trick for classifier guidance and then the glide paper kind of talks about this for
text conditional models where we talk about clip guidance and classifier free guidance.
There was also the paper by Yang Song generative modeling by estimating gradients of the data
distribution which kind of like was before the Jonathan Ho paper which approached this problem
from a very different perspective of score matching and in the DDPM paper Jonathan Ho
and others showed how it's kind of equivalent to score matching so if you want to understand
diffusion models from a different lens I think I would strongly recommend that paper and these
two blogs as well by Lillian and Yang on diffusion models and score matching models
they're basically like two sides of the same coin and understanding from both perspectives is super
useful to see you know why these generative models work and that's it. Thank you so much it was very
interesting and fascinating and very inspiring. Are there questions? Go ahead. This is super cool.
I have a quick question. Did you notice if there was any relationship between say
like if you fed it two-dimensional noise and if you were to step through the X or Y coordinates
did you notice if there's any relationship between the coordinates used for two-dimensional noise and
the outputs you get? Say that again. What did you say at the end? If we use two-dimensional noise
is there a relationship between the coordinates that you use for the two-dimensional noise map
and the outputs of the model? Like is there a relationship that you observe between the
noise that you use and the output of the model? Okay there's a little bit so like one way you
can do this is you can like fix the noise, re-sample and change the label and you can see that
the generated images for the same noise but different labels kind of have similar like
perspective and spatial structure so like but they look kind of like images from different classes
so the noise definitely controls some aspect of you know how the final output looks like and
there's some kind of spatial like connection but it's not an exact direct connection. Does that
can answer your question? Yep got it. You can see examples of this in our I think
diffusion models beat GANs paper. I think that in the appendix we have an example where we like
do this specific thing. It's actually more directly connected when you use a different
sampling method so I'll show you the reverse process right where at each step you're doing this
reverse step with the Gaussian so at each step you're adding a little bit of noise when you
sample from the Gaussian but there's a different way of reverse sampling from these models which
is called ddim there's another paper on that where you just sample noise once at the start
and then you just run a deterministic reverse process to sample from the model. In that case
that there's kind of like this one-to-one correspondence between the noise and the
generated image and there it's more clear to see this. Got it thank you.
I can ask my earlier question again I guess if nobody else has another question.
Go for it. So I guess if you're using a diffusion model without clip right so I guess
what's being trained is the classifiers for the labels is that am I understanding that correctly?
It's a question for the denoiting process. Yeah I guess I'm trying to understand if
without clip how does it know what to denoise to without like some representation of the text
that you're feeding in? Yeah yeah so well yeah if you train a model or denoising a model without
text labels then it doesn't know where to go and the only way you can
generate a sample for a given text distribution it would be through like clip guidance or something
but we do have we do train these models to be text conditional the diffusion model
and in the classifier free guidance case you train it with or without the labels.
So maybe I can go back to one of our slides here the way the model the reverse process model
sees the text is through this kind of conditioning on the the representations output by a transformer
on the text. Ah okay got it yeah okay got it so this text conditioning here is without clip.
Yes so the point I got a point I lost over in guidance was you could use guidance on top of
unconditional models or conditional models so you could have a reverse diffusion model
that isn't conditioned on any labels then it wouldn't have any way of actually like
producing an image given a class but then you could use guidance on top to get it to
produce an image giving a class but you could also use guidance on top of conditional models
themselves so you could have your original model be able to produce an image given text like we did
here but also use guidance on top to make it even better at doing this. Got it okay yeah I think
I saw this one the kind of practice. Okay thank you.
Okay excellent if are there more questions?
Okay well I think that we can wrap up the session I again want to thank you a lot
Profil and it was it was great thank you so much. Thank you and thank you for having me
and feel free to just like email me any questions later or DM me on twitter with questions.
I think there's a lot of cool stuff out happening in this so like I would strongly recommend doing
some of the like reading some of the blogs are reading just like things that you can find
in other collab notebooks as well.

Okay, hello everyone. Welcome to this session of Deep Learning for Art,
Aesthetics and Creativity. Today we have our specialist speaker,
Junion Ju, and he is an assistant professor at CMU, School of Computer Science. He is going to
talk about efficient GANS. Junion is such a great researcher and scientist who has been working on
many, many interesting generative models, including he was in peaks-to-peaks and
psycho-consistency scans and Gauguin and many other interesting and intriguing work.
And I hope that we can see a gist of some of his work here today.
So, please, Junion, go ahead.
Yes, thanks for the nice introduction. I'm very happy to be here and talk about
some of our recent work on how to make GANS more efficient.
So, maybe some background that you probably already know about it.
Yes, GANS has been used for various content creation and creativity tasks.
For example, you can draw a sketch of a handbag or edge of a handbag and you can get
an output you made from the GANS.
And one recent model we have been working on is this Gauguin model led by Taishou Park and other
people. Here our artist is creating a semantic label that basically says this is a mountain,
this is also a log, this is the sea, and when the mountain, he can add some sand
and we can generate an image in real time. And he can add some more details like logs.
Yeah, and you can create a pretty nice image very quickly.
Yeah, so that's a very cool demonstration of the GANS and you can also apply a particular
like a style image from a training set and then apply it to stylize this image.
So, these all look very nice and kind of perfect, right? Those cool demo in front of
thousands of people. But if we close the door and the story here is really very expensive,
at least for this Gauguin project. Because to develop Gauguin project, Taishou has about hundreds
of GPUs in the summer intern and for six or nine months, he has access to several hundreds of GPUs,
very high-end media GPUs, which sometimes you cannot even buy it on the market, but it's
internal GPUs, we are not allowed to share the information. But he has access to lots of GPUs
during training and the training also requires lots of data, like tens of thousands of data.
After almost a year's development to learn this model, to do this demo, we actually bought a very
expensive laptop, like 6,000, 7,000 laptops with actually a high-end GPU, but that's still a laptop,
not a desktop, so we want to carry this laptop around and do the demo. So, in conclusion,
for this particular project, I realized GANS are very expensive.
Perhaps not for everyone now at this moment. I think it's expensive in three ways.
The first of all, to learn a game model, if you train a game model to learn the game model on a
device, you need a high-end GPUs for real-time performance, otherwise it will be very laggy,
you cannot have any real-time demo without a high-end GPU. The second to develop the
algorithm, to develop a new algorithm, you require hundreds of GPUs to train the model.
The computation is huge, maybe right now only the deep end of the media can afford this kind
of computation. You are for university lab, MIT or CMU, maybe you don't have so many GPUs,
it's very hard to compete with big companies. Third, to train a game model, it often requires
lots of data, and I will go through it one by one. The computation wise, it's very slow to learn a
CPU on a mobile device. It may take several seconds to learn on a modern CPU, and it's even
slower to learn on a mobile device like a tablet or like a phone. The algorithm wise, to train a
single model for a single train essentially takes several days, maybe one or two weeks,
maybe sometimes for a big game, maybe take a mouse. It's not like you just train one model
and publish a paper as a model training might take at least a dozen of training sessions,
even hundreds of training sessions. Each training session takes a mouse, you need lots of GPUs to
parallelize your experiments. So it's not like you have an idea, you train a model and you go
through the public paper, that's not the case. You have an idea, you try something to work,
you try something to modify the idea slightly, you have lots of iteration over the mouse.
Each iteration takes one week or one mouse on 8 GPUs. So if you put these numbers together,
if you want to actually do the research as quickly as possible, it requires lots of GPUs.
Sometimes even like a top university like top lab in MIT or CMU Berkeley cannot afford it,
not saying other labs. Lastly what people generalize is to treat this model like
requires tens of thousands of ASO images. You treat a model of faces, this requires 70,000
faces. These faces are very high quality images and you need to align the face before you treat
the model. So there are lots of pre-processing steps to get an image. To train an image model,
you need labels and you limit the data set first. So I think there are three things that can be used
for more users. If you are a content creator, if you are an artist, you may not have access to
the computation, to the algorithm and data. So in this talk, I would like to focus on
computation and the data. In this talk, we have some results on how to make gathers faster,
how to make a game model of maybe only hundreds of images. By that, we can maybe help more users,
content creators and artists to train and test their own game models without having access to
lots of data without having access to very high-end GPUs. So I will talk about the first part, which
is the model. GANS are very, very completionally expensive to run. If you compare GANS or conditional
like Psychogalgan to a typical image classification model, this is the computation.
Some people like to use Macs, some people like to use Flops, but the story is the same. It's quite
cheap to learn a mobile net for image classification purpose. It's very relatively cheap to learn
a restats network for classifying cation versus dogs versus other categories.
But somehow it's very expensive to run a Psychogalgan model. So this gap is almost 500 times.
If you compare the latest generative model versus the latest classifiers.
And that's a general trend. I think you may ask why, right? Why it's so much more expensive.
One reason for that is like in Psychogalgan, you take an image and you produce the image with
the same resolution. So the spatial dimension of the tensor remains the same more or less
through the process. When you classify, you take an image and you produce a number.
So the spatial resolution of this tensor becomes smaller and smaller.
So I think that's part of the reason. But also to generate an image, maybe you need more future.
That's the second reason. Anyways, it's quite expensive for GANs
that compare to the image classifiers. And in this work, I will briefly go through this work.
If we try to solve this problem, we propose a method called GAN compression.
GAN compression is a general purpose framework based on the distillation, channel protein,
and neural active search. So the idea here is,
so given a teacher model, teacher model is the original model.
For example, this Psychogalgan model would like to compress. It has lots of channels.
And you take how horse image is input in Psychogalgan case, it outputs a zebra.
And we also have a student, so the goal of this project is to try to find a student model
which with fewer filters, and it can produce the same kind of zebra as the teacher model.
So we have a loss function to try to make sure the output, the students output
zebra looks very similar to the teacher's output zebra. That's what loss we have.
We also try to make sure the student's intermediate feature representation
is very similar to teacher's representation. And lastly, we'd like to make sure we have a GAN loss
here. We'd like to make sure the student's output zebra looks like a zebra according to
our adversary loss. So we have three losses. We have the pixel loss. We try to make sure
the student zebra looks like a teacher zebra. And we have a feature loss which makes sure the
student's feature looks like a student's feature. We have a third loss which is GAN loss, just like
a typical GAN loss we apply to any GANs. We want to make sure the zebra looks like a zebra.
So this is our distillation part. Our channel protein part we would like to,
so our goal is to search for each layer, the optimal number of channels which can,
we want to find a smaller number of channels but can still satisfy all these loss functions
constraints. So we have a bunch of channels here. In the example, we have 60 channels,
you can choose 32 channels. You can choose maybe 48, 64 if you like. And I can choose 24 if you like.
So we have channel protein. So the idea is if you have fewer channels
for each layer, your model will be smaller and faster to run. Let's say if you reduce the channel
by half for every single layer, of course your model will be faster to run. But here the search
means we try to, we're not trying to reduce the channel uniformly. We try to maybe for some
layer we try to reduce it to 16 channels. For some layer maybe 24 channels are necessary or 32
channels are necessary. So we try to, it's kind of like a search problem. The search space is just
like channels per layer. You have eight layers. You have eight numbers you would like to search for.
If you have 11 layers, you have 11 numbers you would like to search for. So the idea is we try to
find this combination so that we can still reproduce a zebra or to look like a teacher zebra.
Okay. So how to do this search problem? The search seems very hard because for each configuration
you need to treat the model. This is a combinatorial number, right? There are lots of combinations. You
can, if there are four combinations, four choices per layer, that's lots of combinations. You don't
like a naive way is you can train a model for each configuration and then compare which model works
best. But that will take lots and lots of training time. So the idea here is we would like to have
all the network to share the weights. The idea is if you have 32 channels,
it's shared the same first 16 channels with the 60 channel model.
So by doing that, we don't have to treat them on each individual configuration
every single time. We can share the weights across different configurations. So we still
have only one model training. Each time we simple our configuration, it's like this configuration,
that configuration, and we try to treat this configuration with the loss. But we only have
one training session. But in each training iteration, we sample a different configuration
and try to treat the model with the loss function I just described.
Okay, so once we are done with the training, we can, then we can, then we can, for every
single configuration, we can try to evaluate these models based on a matrix such as FID
or such as other metrics you have and choose the best one. And then we can find that model.
Yeah, so there are a bunch of loss functions. We kind of, so there are several ideas. One
idea is there are several loss functions. We try to mimic the teacher model. But
actually such idea is we try to share the weights across different configurations
so that we can avoid treating each configuration multiple times, every single time.
Okay, so that's the idea of the method. I would like to share some results with you.
So here we, so for the harsh to zero, we can reduce the model size for 57,
not the model size, the, the, the, the, the cost for 57 max to 2.7.
7 to edge to shows, 7 to Gauguin.
So we can achieve nine times to 23rd, 20 times compression ratio in terms of the,
the model computational cost. Here is a demo is you will run the original cycle again
on this mobile device, Jason Xavier GPU. And we can measure the FPS. Here is the,
here's how it looks like. You want to transform the house to a zebra.
And here is we learn the same more compressed model on, on this hardware. So it's actually
three, four times kind of speed up and it gets real-time performance on this. This is a kind of
like a mobile, like on device chip you will use for like robots or cars. Yeah.
But here are models out here, the A port image, got a cycle output.
Here is a baseline in which you just reduced the, we'll call 0.25 cycle gain.
In this setting, you basically just reduce them all channels
per layer. And you lose them a channel to like by, by force, by four times. But we do it for
every single layer. So there are no, no, no, no, no, no search happening. So they're very nice
baseline. And you can see if you just reduce the number of channels uniformly every, for every
single layer, you will lose lots of details like zebra stripes, which is something you would like
to have. Okay. And here is the compressed model, 20 times compressed model with the outback search.
And it can preserve the output of the original teacher model, but why be 20 times faster?
So recently we applied this idea to not only to cycle gain, but also to style gain too.
This is ongoing submission. So, so, so the idea here is we would like to have a teacher model.
I would like to have, when we obtain a student model, just like what we did before with some
slightly modification. And during the, the idea is during the image projection, we try to
the idea is we project this image into the latent space of the gains. And then we add some
different like directions, like smiling directions, like glasses directions,
maybe different hair color. The idea here, here is we can, during the interacting editing,
we can use this low cost model, a 10 times fast model, which only takes three seconds.
It's time you adjust the slider. I will show you a demo in a minute. But, but when,
once we, the user, for example, you use someone to change the hair from black to, to, to, to white.
And you can do that, right? And you can change this one to this one. But once you, once you,
the user is done with the editing, you can use the original model to get the final output.
The idea is that you would like to make sure the smaller models output is consistent with
the big models output through this editing process. Otherwise, you will get, you learn
the smaller model and you get something, what you want, but, but once you press the button
off, you're learning a big model, if you get something completely different,
then this preview is not informative anymore. So I will show you a demo
as an animation in this video. So here that we have import image,
by to project it, and also by to, to, to, we would like to look, to represent it again.
And then we'll try to find the best latent code, which can produce this input image.
And so we have, once we have done that, we can, like, move this code around. Maybe we'll find
us a smiling, smiling directions. I think Philip, you saw, I might have already mentioned
is how, how, how you can find directions. Oh, and you can find the, like, you move here,
maybe you mix your face smiling and you move to a different direction, make, make your hair
a different color, like, like, see. And then the idea is, and then we can generate an image
after you change the directions. And here is the, the idea is, here's the original image.
And here is the, we used GANs to, to, to, to reconstruct this image, to represent this image.
This is the image, again, is the image of original image. And here are a few sliders.
Okay. And we would like to modify this image in various attributes.
And we'll learn it on, on the, on this CPUs, like just Intel CPUs. But if you click smiling,
here we click smiling, it took about three seconds to produce image with smiling face.
And it's, so you cannot drag the slider anymore, you can only click. And then you make younger or older.
It's just very laggy, right. And the idea is you would like to, if you, if you
click something or drag it to a slider, you should get the results immediately. And here is,
is our idea.
And you would like to choose our model, any cost GANs. And you can click something,
you only took 3.3, 3.4 seconds.
That's very, so you get very fast interactive feedback.
Okay, change it here.
And once you're done with your editing, you can still learn your original model and a finalized,
finalized output. So you guys very fast and, and, and interactive feedback when you are editing the
photo, and you, you, you get a high quality output will after you finalize your edits.
There's another example.
I'll make this, I look older.
I will try to remove the gossips because sometimes you mix some more, it will add gossips.
So there are some coloration between these attributes.
But it's very attractive. But once we finalize the edits, we can
generate the final rendering, very, very high quality, high resolution image. Okay.
Yeah, here's the last example.
Yeah, again, once you finalize the edit, you can click the button and learn the original model.
Yeah, so here's the one.
Yeah, so this is actually quite similar to how people is, is using like a regular, like
non-deplaning content creation software either in, in like rendering software like Blender or Maya,
you can choose to render an image with low quality preview.
It's very easy to do it in rendering, rendering algorithm, just sample fewer arrays,
or maybe you have fewer bounces if you know the rendering algorithm.
It's also in, in a bunch of Adobe software, like you can also generate a preview of your videos.
And there are a bunch of ways to make render rendering faster.
Once you, so that's just for your preview. Once you are done with your editing, you can export
the model, export the export your results to a much higher quality rendering, much higher quality
kind of just output results. So we're trying to separate the preview and the final rendering.
We're trying to make sure the preview looks similar to final rendering.
Otherwise, it's the preview does not provide enough information for editing.
Okay. Yeah, so what we, so far, what we have been talking about is how we can
make it faster to run on CPU. For example, maybe we, instead of three seconds, we can make sure it
runs about 0.3 seconds, 0.4 seconds, right? And maybe we can make sure it can run on a smartphone
or maybe within one second. Next, I would like to talk about data, which also prevents many of the
users and the community creators to choose their own models. So the idea here is to choose these models.
It requires lots of selects, like, like choosing images, right? When you set 7,000 faces, you need
to be high quality faces, and people actually align the face, crop the face from the original photo
and align them. The same idea here, if you choose a big game model, a bit of model from DeepBand,
the big game model requires the image net model, and it requires millions of images from
what's on the categories. Or even a true model like, one, you have seen the car model, the
bedroom model requires one or two million images to choose a car model, just to learn a model about
cars and bedrooms. So it's not very easy. So if you have a new idea, oh, I would like to have a
model of something, it's not like you can just train the model, like it takes a lot of time
to collect the data right in the first step, before you even train the model. Okay, so it took
months or even years to collect the data set, some training might require annotation if you
train a big game model which condition all the class labels. So for them, here, how about I just
train a model about myself, about my collaborator, here is actually Professor Song Han, assistant
professor at MIT. So often I just, I would just try to train a model on his portraits, right?
But of course, I don't have, the thing I don't have is I don't have 70,000 faces
of Professor Song Han, I don't, I probably don't know how to allocate them, but maybe I have 100
faces, maybe I have 50, maybe I have 200, but there are no way I have a million faces of, I mean,
maybe if you are like a celebrity or a petitioner, maybe you have, maybe you have a, I'm not sure
if you use that case, you have a million faces, but for your friends, for your family members,
you probably have like 50, so 100, so several hundred of them faces. And here, but the idea
case is we want to train a model of my, myself, my friends, my collaborators, but I don't have so
many images, so I would like to, so here, you know, I hear what I would like to train a model and try
to produce some new samples, and maybe I can use this model to edit Song Han's photo, right?
Instead of using a generic face model, I would like to have a customized face model for face
editing, but in reality, if you just train a model of Stargatan 2, we get very distorted images,
if you train a Stargatan 2, on the dataset I just mentioned, like hundreds of 50 faces.
So there are the huge cap between, if you train a model with 50 faces,
let's say you train a model with 70,000, a million faces,
the gain is really requires a lot of data to get good performance.
And this holds for a bunch of cases, not only for Professor Song Han's face, but also for Obama's
faces, if you train a model on 100 Obama images, it does not work very well, okay? And if you,
let's say, maybe not, maybe we have a very high standard for face, right? Maybe for other objects,
it's okay, that's not the case. If you train a model on cats, 160 cats, you get some distorted cats.
How about your dog friend? If you train a model on dogs, you still get distorted dogs,
even you have 390 images. So that highlights the issue is, if you want to use again
for your own purpose, for your own dataset, maybe for some paintings, you don't have,
if you train a model on paintings of particular painters or artists, you can also ask artists
to produce millions of paintings in the first place, right? You would like to train a model
on 100 or 1000 paintings of a particular style or a particular painter.
So we can, we can do this kind of experimental control setting,
is we can look at CIFAR-10, which is a standard dataset for against or for computer vision,
and we can measure it by FID, which is the lower and the better, which measures the distance between
the jerry hippie distribution versus the original training distribution. And if you train a model on
FID, sorry, you train a model on CIFAR-10, you get a very low FID for 100% training data,
but you get a much higher FID if you reduce the data by five times. And if you only train a model
on 10% of CIFAR images, you get a much higher FID. Now, that's why against having to rely on the
number of images, you are training that. But why is that? Why, why, what happened in the CIFAR-36
case, if you only have 10% of the data, right? Something happens if you, if you, for any kind
of opportunity model, if you don't have enough data, your model might start over 50. In this case,
we can look at the discriminator that's over 50 if you don't have enough data. So here the case is
the discriminator, we can, we have two plots. We have discriminators training accuracy.
Yes, how this is, how discriminator can classify real versus fake images for the training set
the model has been trained on. We can look at the discriminator's accuracy, hold our test,
our variation training, our variation real, real images, which discriminator hasn't seen
in the training set. So you can see that for 100% of data, the model, the discriminator starts over
kind of classifying images more and more accurately. But that's not necessarily me,
it can classify the test images as accurate as training images. So that's what happens a lot
if you just train the classifier, right? But the thing which makes this worse is if you only have
20% of training images, this kind of over 50 is much more severe. If you only have 20% of training
data, the discriminator will classify the training images very accurately, very quickly,
but it will, it cannot work for the test images anymore, the test real images, it will
drop the percentage, right? Reminded, this is a binary classification tasks, the accuracy below
0.5 is pretty bad. And if you have 10% of data, it's even worse. It can classify this 10% of data
very quickly, real or fake, but it cannot classify any kind of test images very quickly. And if
look at this model, this is when the gains start collapsing, the models start generating lots of
garbage images. So the issue here which we'll identify is that if you don't have enough images,
or if you only have 100 images, it's very easy for discriminator to just simply memorize
every single image of a training set. As a discriminator, it does not generalize well
to other real images. So if you train a territory with over 50 discriminator,
of course the generator cannot get all the signals about what makes Obama look like Obama,
what makes a professor so hard looks like professor so hard, right? If you're discriminating over 50.
So one idea to come back the over 50 in computer machine, machine is called data augmentation,
is for single real image, we can create multiple versions of this image.
If I create 10 versions of images, I kind of increase my data by 10 times.
Of course this information is redundant, but it's better than this one version, right? So the idea is
try to enlarge the data sets without collecting the new samples. As there are a bunch of things you
can do, you can follow this cat, you can rotate the cat, you can flip the cat, you can maybe change
the color a little bit, or you make maybe you can translate the cat, like make it shift left,
shift right, shift up, shift down, so you can you can move the cat around,
but the computer vision tells you it's still a cat, right? So if you train a model to classify a cat
and a dog, if you move the cat around and rotate the cat by 30 degrees, it's still a cat, right?
It doesn't become a dog. So you also know the labels. So the idea is you don't want to change the label
by what we equate your data, right? But how can we apply this idea to GANs training?
How can we use data augmentation for GANs and to combat the overfitting issue when we train a model
on very few images? So we have tried several ideas. The first idea which is very straightforward is
we can we can just apply this transformation or augmentation on the real images, right?
Just like we did for the training on image net classifier. And we can train a model on the augmented
images. That's so that's very straightforward. But the thing we're following is if you train a model
in this way, the GERI image will also has the effect of this transformation. If your transformation,
if you change the color or you translate the image around, or if you crop some patches,
your GAN will replicate its kind of artifacts, will mimic these artifacts. Because
your GAN does not know what the original image looks like. If you only feed the transformed
images, augmented images, try to mimic the augmentation as well. Why is that? Why is
not an issue in the classifier? Because if you train a classifier for cats versus dogs,
your label of cat and dog is the output, right? You want to output a cat or dog, but this label
is the same before or after augmentation. So you can still produce the cat. But here the output of
the generator is the image. If you change the output, your dataset, your output of your
generator will change. So you see the difference. The difference is whether you augment the input
or output. So here we try to augment the output. While in the classifier case, people try to
augment the input image. Why keep the output the same? So you cannot just augment the output directly.
It will mimic that augmentation. That's what normal charities kind of image is, right?
And so the second idea is how about we augment both real and fake images?
I hope we can cancel each other. And we'll only do it for the discriminator training.
So the generator training is the same. We still train it versus the generator of G of Z.
For a discriminator training, we augment both the real images X and generate images G of Z.
So Z is a latent code. We're simple. But the one thing we're following is since you are doing
slightly different things for the discriminator training, in which case you augment the X and
G of Z. But if you don't augment the data for a generator, you will see a gap. That is your
classifier discriminator works pretty well for the transform for the for the for the
augmented images, T, T is augmentation. But it does not work very well for the
generative for the images without augmentation for the G of Z, which is original images,
also which is original charity images without augmentation. Okay.
So there is a gap between the generator's objective and discriminator's objective.
And we found it does not work very well in practice because of this gap. So our approach
is called differential augmentation. The idea is we we can augment both the fake images and
fake images and real images and both training and sorry, in both generate training and discriminator
training. And here I will call it differential because if you if you augment the data here,
and if you want to get gradients from the discriminator to the generator, this transformation
T needs to be differentiable. Otherwise, you will stop the gradients from the discriminator to the
generator. So we implement a bunch of differential augmentations and apply it to here. So the same
impermanence we apply a current transformation, our transient image alone, we apply something
kind of cut out or kind of like augmentation. There are three kinds of operations.
And the idea is once we apply the augmentation, we can we can back up against the gradients
from the discriminator all the way to the generator. So that's how we can treat the generator.
And here are some results. So here's the original FID with respect to different amount of training
data. And here's our results. So we we get slightly better for 100% training data, but we'll get much
much better if you only have 10% or 20% of training data. And so this allows us to maybe
instead of training a model of 50,000 images, you only need 5,000 images. And here are more examples.
Yes, yes, if you only have 20% of the training data for image net, our model achieves much better
results compared to the baseline begin. We will use the same begin loss and same begin architecture,
but just add this augmentation, you can reduce the FID by half.
And now we can tell journalism, Obama or cats or dogs. Here are our results, we can produce
much higher quality results when you only have 100 images.
And now we can try to generate professor's face. It's not perfect, but much better than
the baseline. And only requires 100 faces of your friends, your family member or yourself.
And compare with fun tuning methods, there are a bunch of methods which you choose a model on
a large scale data set and a fun tune each on a smaller data set.
And our method is comparable performance wise, even when we don't require these kind of protruding
images. Here we compare with transfer again, which the idea is you train a model on
FFTQ faces and a fun tune each on Obama faces. And all methods are still slightly better than
their results. But of course, you can combine the best of the two words, you can train a model on
FFTQ and a fun tune it with our differentiable augmentation. And you can get slightly more
better results, you combine these two methods, they are complementary.
And here are one of the results we train a model and we try to traverse latent in the latent
space for different kind of, you can train a model for a particular person, you can train a model
for particular landmarks or cities or animals, just very long with some images and just hundreds
of images you can do the job. Yeah, anyway, so there are not so much take home messages.
If you only have your fortune again, do not forget they talk about Haitian, that's the message.
Okay, thank you for your attention. Yeah. Thank you so much, Junion. And it was such a great talk
and a lot of interesting directions and things to think about. I'm wondering if students have
questions. I would like to ask how some of the students can get these models and work with them
and can you please explain if what are the steps for them to get working with these models
and the code?
I think you are me. Yeah, yes, yes, I try to
crit my dual monitor setup. So yeah, yeah, I see that all the code are available on the
GitHub, I will send you the slides later. All the code are available on GitHub.
We have a step by step instruction on how to learn the model on
C file and how to learn the model, how to learn the model on your own data. If you just have an image
directly of hundreds of images, we have a command line that you can learn the model
directly. It can take like four hours to learn a model on hundreds of 100 photos
for a bubble of your own faces. It should be pretty straightforward.
And then are there tools that one could use for making some of these things more interactive
and use as a just at the beginning, you describe how it is interesting for designers
and, you know, art practitioners to use these models?
Yeah, I think David will talk about it. We have a bunch of tools on visualizing
and monitoring the internal units of this model if you would like to understand it better.
I think David has a bunch of online tools he will talk about maybe tomorrow or later.
Yeah, I think in general, it's also hard to use. It takes like four hours on 100 photos,
so if you have 20-80 time GPUs, yeah, so it's much faster than doing a big game model for
months or several weeks. Yeah, I think we're also working on maybe faster training. We are
still working on that. Hopefully, you can reduce the training time to several minutes or maybe half
an hour, so more people can use it. So oftentimes, you may not have the GPU resources, you may not
have so many images, so we are working on that and try to, yeah, but it's also good for us because
we also have limited resources compared to big companies. It's not like MIT and you have lots
of resources, but compared to big companies, we don't have so much resources in academia.
And then also, David is asking, how long does it take to compress a model?
Yeah, compress a model takes as much as time as training a model, maybe 50 times more.
It's slower because you want to, you are training up all kinds of configurations at the same time,
so it takes longer to train. Maybe it takes 50% more time. And while we are working on
improvement of that, try to make the training faster, it works for a bunch of models as well.
Yeah, the compressing model is slower because the idea is you have a bunch of models with
different configurations, they all work pretty well, so you can use configuration A for your CPU,
you can use a different configuration for your mobile device. So you can have different models
for different devices. Once you train the model, you have this kind of flexibility.
The idea is once you train the model, you only need to train once and you can deploy to
10 or 20 devices. That's quite essential for critical, for practical purpose, because if you
develop a product, you don't want to compress a model for iPhones, the next time you compress
for older iPhones, you would like to have this versatile ability to train once and
deploy to multiple devices. I think that this is very important because you can put the time
for the developer and then when the user wants to use it, the hope is that they spend
very much less time to get what they want, which is a very good idea. Thank you so much,
Junion. I appreciate that. Thank you for having me. It was such a pleasure for us.

All right. Hello, everyone. Welcome to your course, AI for Art, Aesthetics, and Design,
and Creativity. Today we have a very special lecturer, AJ. He has been at MIT just like
for his undergrad. I got to know him when he was here, and he's very active,
have been running at the ML groups, and sometimes chatting with me about these topics of creativity
and AI and art. I think that this is very exciting. He's going to tell us about his journey and
and his new work. I will let him to start the discussion. AJ, one of the things that I always
ask is that if you could please introduce yourself and tell us a little more about what
inspires you to work in this area. Sounds good? Yeah. Yeah, absolutely. I'd be happy to.
And thanks so much for having me. So today I'm going to be talking about some work I've done,
some work that's happening in the community around 3D content creation.
But first about my journey. Yeah, I was at MIT for my undergrad, and I was part of what is now
called the AI Club. And then we called it the Machine Intelligence Community.
In my undergrad, I did research in a couple of areas, but mostly actually in compilers.
So a little distant from what I do now, more on the high performance computing side and
performance engineering that had experience with self-driving cars and generative models for
self-driving applications during undergrad, and really fell in love with that topic. How do we
reason about uncertainty? How do we model complex data distributions and predict the future?
Like, for example, predicting the behavior of vehicles. And that led me down the path of
working on generative models in my PhD. And these generative models are, these days,
the state of the art generative models are parameterized by deep neural networks, which
try to fit large data sets, try to estimate correlations between different variables. And
these could be all types of different data modalities, like images, they could be trajectories
or behaviors like I worked on, audio, video. And today we're going to talk a little bit about
3D objects. And so that's kind of what inspired me. At the time, I was interested in uncertainty
estimation. But these days, I just really like the tangible results you can get out of generative
models, novel samples, and novel designs. It's very fun. I got to look at pretty pictures all day.
So my research interests, like I mentioned, is around generative models. We've done some work in
denoising auto encoders. How do you generate images with these denoising diffusion probabilistic
models? That's purely in the 2D setting, though it's been extended to other domains.
Over the past year and a half, I've also been doing a lot of work in 3D reconstruction and
inverse graphics. So how do we take images and try to infer a scene from them or generate in the
3D space? Sorry, Ajay, interrupting you. It seems that some of the students want the transcription
to be on. Is that okay? That's fine. Okay, excellent. Thanks. And building off of that
performance engineering background, I had from MIT, I also did a lot of work in the intersection
of machine learning and programming languages at the start of my graduate school. And I even
summarized my research interest as making it easier to create creative content, especially with AI tools.
And to provide some background for today, I'm going to discuss different types of scene representations.
What I mean by this is how do we encode the geometry and color of a scene in some
format that we can work with digitally? And there's this very long history of this,
particularly from the graphics literature. On this slide shows some different representations
of geometry that you'll be familiar with some of them. 2.5D might include RGBD images, like a photo
plus a depth scan. And they're point clouds, meshes. Meshes are the most common representation
used in graphics applications, but they can actually be challenging to work with in a learning context.
And so our focus in the learning context will be on the volumetric approaches. These can be very
easy to train. You can kind of think of at least a boss look greatest classifier, mapping each
point in space to whether it's part of the object or not, whether it's occupied or not.
More recently, there's been a significant amount of interest in neural scene representations,
sometimes called implicit neural representations that define the geometry of the object with a
function. That could be a distance function, so a network mapping from coordinates to the distance
to the nearest surface. And these can be a lot easier to optimize. These neural scene representations
can also compress the data significantly compared to explicitly representing the geometry of the
scene. So we'll be focusing on that direction. And in particular, we're going to be discussing
a model called neural radian fields I'll get to in a second. But they address this problem of
view synthesis. So how do we take some sparsely sampled input views of a scene and then construct
a representation of that scene's 3D geometry and color in a way that allows us to render it from
new perspectives? These are some example works. You can represent the scene as a multi-plane
image. So instead of a flat grid of RGB values represented as multiple planes,
and that allows very quick rendering from new perspectives. Neural volumes is an approach
from Facebook that has an encoder decoder structure. Take some input images, encode them into a
laden space, kind of a 3D laden space, and decode it out to images with volume rendering.
Neural radian fields have really been very popular over the last two years due to their simplicity
and quality of the results they can generate. So here's an example scene that's captured on
the Berkeley campus. Some photos of the scene are captured, for example, with an iPhone. I believe
these are captured with an iPhone. And then poses for each photo are estimated. And this neural scene
representation, called the neural radian field, is estimated off of those images. What's really nice
is once we estimate the representation of the scene, we can render it from novel viewpoints and
kind of smoothly interpolate these sparsely sampled views. The scene is only very sparsely
observed from discrete points. What if you as the user want to make a photo from a new perspective?
There's some interesting things to note about this rendering. Notice the specularities on the
surface. They're not just modeling the diffuse light. Also modeling how the light reflected
back at the user depends on the viewpoint of the camera. As you shift your head, the scene will
change. This is particularly visible on very shiny surfaces, like the glass or metal of the car.
A neural radian field, yeah, it really is amazing and really captured the attention of a lot of
people. And so this neural radian field and view synthesis field has grown very quickly and there's
still a lot of problems to be solved. One very interesting thing that these neural scene representations
bring to mind is that we're encoding a scene in the neural network's weights. So instead of
explicitly encoding the geometry of the scene via points or meshes, lists of the scene,
lists of triangles or voxel grids, it's encoded into this small multi-layer perceptron. So maybe
this is a half a million parameter network, just some stacks of dense layers. It's representing
a function mapping from 3D space coordinates, x, y, z. This is in the scene, x, y, z coordinates,
and a viewing direction. So what is the angle of the camera in order to model those view dependent
effects? The neural network then predicts at this coordinate what is the color of the scene,
and then its density, sigma. So this density is something like how solid is the object,
how much light will be absorbed. Rendering is done by ray tracing. This is not exactly what would be
done in most graphics applications, like real-time ray tracing, because we've kind of encoded
the light being reflected at any given point back towards the viewer into this function.
So we don't have to scatter light through the scene. The viewer will cast a ray from their camera
through the pixel. This is the image plane into the scene, and then query the neural network
along the ray. These are different 3D coordinates in the scene. The color of the rendered pixel
will then be some accumulation of the colors along that ray. And in order to determine the color and
the density along the ray, each of these coordinates is passed to the MLP as a very large batch,
and we get a color and density for each coordinate, and then can compose them with alpha compositing
into a color. There's some subtlety to this compositing. This is called the volume rendering
equation, because this equation is pretty simple. So this is the density predicted. This is the
camera origin, and it's displaced some steps along the ray. And so the neural network will predict
what is the density of the scene at that point, but it also predicts what is color,
and then we're integrating this color along the ray weighted by its density. But we also
have to weight it by transmittance, which is roughly speaking how much light is transmitted
from the viewer to that point along the ray. Because once we've accumulated enough density,
then objects forward or back in the scene will not be visible to be included.
This equation for color conditioned on coordinates is differentiable with respect to
the parameters of sigma and c. So sigma and c will be this neural network. Because this is
fully differentiable, it's relatively easy to optimize. Instead of optimizing scene geometry,
we'll instead optimize the weights of this neural network in order to get some desired colors.
So this might be the sparsely observed viewpoints, two viewpoints. Let's render the color
according to the neural scene representation, and then try to optimize the network so that it
matches the observed views, pixel by pixel. It might take a minute to wrap your head around,
but it's actually pretty simple. We have this one MLP that encodes a scene, lets us render
viewpoints differentially, and we'll optimize the scene so that it matches the input views.
And that's why it's inverse graphics. We're going from the 2D space to optimize
for the underlying 3D representation that will reconstruct those views.
I'll now talk about, are there any questions about that?
Can you, could you talk about like how the points are, let's say, used for the neural net? I can't,
let's say, see it like directly. I mean, I get the high-level idea, but
could you talk about like how those points are fed into the net?
Yeah, so you could consider constructing an MLP with five dimensions as input,
just five inputs, and then four outputs on the layers, and then intermediate features or whatever
you could imagine you want, so in nerf it's 256, that would be one approach. And it does work,
but then you get actually quite blurry reconstructions of the scene if you directly feed in input
coordinates, as these are just floating point numbers, 3D coordinates in doing direction.
But instead, what is used in this neural radiance field is assigned useoidal positional encoding,
so a frequency-based encoding. If you're familiar with the transformer positional encoding,
this is a common approach where continuous values like coordinates or time steps are encoded
using a Fourier representation, so you take sign of various scaled values of the input
coordinates, and that lets the network model high-frequency detail. So instead of kind of
memorizing a function from each spatial coordinate, it can model
frequencies of the underlying signal if you use the sign of soil embedding of it. So that kind
of just projects this five-dimensional input into some higher-dimensional space before feeding it
to MLP. Let's see, and there's no, let's say, I guess, filtering done before, I mean, applying it to
the net. So it's just transforming certain, I guess, components, but not doing some, I guess,
post-processing before putting it into, or let's say compression or something like that.
No, not really compression. There's some few, like coordinate transform because you'll
do this computation in a particular coordinate frame. There is some subsequent work which we
actually build upon that does a pre-filtering of the input coordinates. So instead of encoding all
the frequencies of the input coordinates, they'll be blurred, depending on how far away from the
camera you're querying. But that's, that's a subsequent work to nerf that reduces the aliasing.
Yeah, let's see, and the network is just like fully connected.
Yeah, just the, just the fully connected network, super simple.
Yeah, thank you. Yeah, one more thing that I wanted to mention here for a student is that
there is a difference between how you train this model versus the models that so far you
have seen for, for instance, classification. For instance, if you want to train a model for a truck,
what you do is you get a lot of images of different trucks in different lightings and different,
you know, models and things like that, and then fit it to your network. However, in this case,
you are taking lots of images of the single truck, single scene, and you are trying to
reconstruct that scene. So it's a big difference between, you know, what you are used to doing
and what we see in nerf. Yeah, absolutely. I kind of see it as instead of, the nerf is
representing a single scene. So instead of representing explicitly, we're representing it
within your own net with a function, but it doesn't generalize. It interpolates these input views.
And, you know, there's a catch to that, which is that in order to fit into the neural
radiance field to a single scene, it generally needs a lot of data. So while these views are
sampled sparsely, discreetly, and there'll be larger regions of space where we don't have
an image taken from that perspective, still to estimate a multi-view consistent
radiance field, experiments in the paper used a large number of images per scene. That's a
little bit impractical. So for these synthetic scenes, this is one synthetic scene that's rendered
in blunders. They were able to get 100 images of each scene and fit the neural radiance field on it.
For those outdoor scenes I showed earlier, like that red Toyota car, I think it's fewer,
maybe 20 images, but still that's a lot to capture with a handheld camera.
And in the first week work I'm going to talk about, we improved the data efficiency of the
neural radiance field training process. So instead of using, let's say, 100 images on this lego
scene, we used eight photos taken from randomly sampled viewpoints.
In the neural radiance field training process, we would know the pose of each image. This can be
estimated with a system like HallMap. It's really common in the 3D graphics and 3D computer vision
community. It's given some images, estimate their camera poses with correspondence, finding.
Then the neural radiance field renders an image or renders some rays from the same pose as the
input. Then it computes a mean squared error loss. So it's a pixel wise error.
The reason that this loss can be used is because we know the camera pose, we're able to render
out the scene from the exact same pose that the observer took the photo. If the camera pose is
shifted in the rendering process, then the reconstructed image and the true image won't
align pixel wise and we'll learn some inconsistent geometry. And so we're going to take a look at
the geometry. And so this is done at all of the observed camera poses. And this is why the neural
radiance field needs so many photos. If there's no observed photo, then it doesn't have the ability
to compute a loss from a given perspective, which means that it could overfit to the input views.
This representation mapping from coordinates to colors is very flexible. One possible degenerate
solution would be to put a billboard in front of each camera, just a poster board off of the
highway right in front of your camera containing the image that's observed,
rather than learning a consistency in geometry. And there's other ways you can get artifacts.
This is described as a shape radiance ambiguity in the Nerf++ paper. Essentially,
we could either reconstruct the shape correctly and then have relatively constant radiance from
different cameras, or we could encode each image into the view dependent coordinate of the network.
So because the network depends on the camera position, it's able to memorize potentially
the photo taken from each camera. When the neural range field is trained with 100 views,
it gets really crisp reconstructions. This is a hot dog scene, synthetic scene,
where we render out the views in Blender. Then the neural radiance field, when it's trained with
only eight views, only matches pictures close to the training data. When you move the camera
further away from the observed images to try to extrapolate, then there'll be a lot of artifacts.
If you regularize the neural radiance field a little bit and simplify it, it can learn
more consistent geometry, but there still are a bunch of artifacts in the reconstruction.
I'll skip over this. So in our work, we add an additional loss to the neural radiance field
training. We keep using the NERF mean squared error loss. It's called photometric loss
on the observed views that are sparse. But then our work diet NERF adds an additional loss at
unobserved positions. So because we have this neural radiance field at any iteration during
training, we're able to render out novel views even before the scene has converged.
It's a little silly that in NERF, we're not able to constrain these input views because
as a person looking at, you know, okay, let's say that our estimate of the scene's geometry
gives us these renderings. This is the observed rendering. We as people can still look at these
photos and derive some loss signal. Okay, I mean, the input view is a lot sharper than my current
estimate of the scene. There's a little red light at the top of the truck, but there's no light on
top of these reconstructions. Based on this principle that you can compare views
at different camera positions as a person by comparing their semantics,
like, you know, it's a bulldozer. A bulldozer is a bulldozer from any perspective.
We propose to add a loss in feature space. Using some visual encoder, each of the input views is
represented in a feature space, and then instead of computing the loss in pixel space,
Dierf will compute a loss in feature space, and that allows us to regularize the scene from
any perspective during training. We call this a semantic consistency loss since we're making
sure that these semantic features, things like object identity, object color, are consistent
across views. And over the course of training, this improves the results.
So the loss that Dierf used was this mean squared error loss, and then we're adding this
semantic consistency loss where some encoder thigh, some neural network encodes rendered images,
and then we compare them in a feature space.
We do have to sample camera poses in order to render this, so there's just some prior
distribution over camera poses. The choice of that feature thigh is really, really important
for the results because we want it to be consistent across viewpoints. So it should
really encode the object's identity and properties about the object rather than low-level details,
like the exact pixel colors. And motivated by that, we use a network called Clip. This is from
last year. It's a representation of images and text, so the representation of an images learn
such that it has an aligned representation with an associated caption. The data that
Clip is trained on is a very large data set of 400 million images that have associated captions
crawled from the web. And the Clip model has really led to an explosion of work in the AI art
community. It's really powerful. It's trained on such a large amount of data that we're able to
prompt it with topics that you wouldn't find in a narrow data set.
It also, by learning to match images to this text, we'd hope to learn some very useful features
about an image. For example, in captions, you can encode classes of objects, just like image net
labels. You can also encode a lot of other details, like the scene rather than just the foreground
object. You can encode things about pose of the underlying object, like a sitting person,
a standing person. And that should be encoded in the representation learned by the network,
if it's going to be able to match images against their associated caption. So the
training objective is encode a bunch of images, encode their captions, and then try to match
images with their true caption. Clip was originally used for discriminative tasks,
object classification in a prompting fashion. So if you want to classify photos of food,
the authors of clip constructed a bunch of captions, templatized with the desired object
category, a photo of guacamole, a photo of ceviche. And then the class label is given by the
caption with the best match with a given image.
The property we're particularly interested in in this 3D reconstruction context is whether the
representations of the images are consistent across views. That's what we call semantic
consistency in the work. What this plot is showing is that the cosine similarity of embeddings
from the image encoder of clip within a particular scene from different camera poses is highly
similar. So very high similarity in feature space within a scene across different perspectives,
within a scene across different perspectives, low similarity across scenes at different perspectives.
So because images are very similar in clip's feature space, very different in pixel space,
we're able to maximize feature space similarity of clip and get some useful loss.
Now what you've been waiting for are the results. This is nerve train on eight views
when it's simplified. And then here is it trained with our additional semantic consistency loss,
a bunch of near field artifacts in nerve. But when we add in this feature space loss,
it removes a lot of those artifacts.
Because those artifacts aren't plausible, they reduce the semantic consistency.
Cool. I'm going to go on to the next work. Before I do, anyone have questions?
I have one question, which is with regards to using clip. Are you able to access the text
as well that clip generates? Or are you able to decode it in some way and actually access
how the clip looks at the inputs? Just in terms of explainability, I thought it could be,
yeah, sounds really interesting. Yeah, that's a very good question. So in this work,
we're not actually using the text encoder. We'll see that in the next work. The text encoder is
just used for pre-training clip and in-dient nerve. So we're only using this image encoder.
Because then the motivation for that is that the neural radian students are motivated by the
view synthesis problem. So there's no text caption associated with the data.
They just have a couple of pictures. So we only need to use the image encoder.
That said, some artists have tried to take clip and use it to create a captioning model.
If you have a model that can match images against captions, can you actually synthesize
captions that best match a particular image? It's a challenging discrete optimization problem
because you're searching for a textual caption that will maximize some neural network's output
score. And that is basically a black box optimization problem. My impression is that
automatic captioning with clip doesn't work too well. It's really good at selecting an associated
caption out of a list of candidates. And that's how we're able to do object classification with clip.
So I think you'd be better served by learning a specific captioning model that will generate
a captioned condition on an image rather than trying to extract captions out of clip
just due to the difficulty of the optimization or the search. Thank you.
So like I said, we weren't using the text encoder in Dietner. In the next work, we try to
move in even more extreme direction of generating objects without any image data. So what if we
only have a caption and want to synthesize a 3D object from it? Is that possible? Can we remove
this mean squared error loss entirely and only use feature space losses? And these are some
examples of the results we're able to get with different captions like a render of a Jenga tower
produces this object. You can also engineer prompts, use hashtags because clip is trained on
web data. Our goal is to synthesize 3D objects from just the caption.
And to kind of refresh our memories, the neural radiance field is an inverse graphics approach
where we have densely sampled images, optimize the shared scene representation,
and then are able to render out new views. In the dream fields work, the second work in this line,
we do not have any observed images, only a caption written, for example, by a human artist.
We optimize something that will look fairly similar to Dietner with additional regularizers,
and then are able to render out new views. And any perspective is actually a new view because we
haven't observed this scene. This is an associated scene for the caption, an epic, wondrous fantasy
painting of an ocean. So the neural radius would use this mean squared error loss, and then Dietner
used feature space loss where the rendered image of the scene and an observed image of the scene
are encoded into a feature space that is optimized. Oops. Sorry. Okay. Now in dream fields,
we use the text encoder of clip. That wasn't being used before. We were just throwing it away
after trading. So instead of optimizing for a feature similarity in image feature space,
we now maximize similarity of image and text features. The reason we can swap between
the text encoder and the image encoder is because clip has learned an aligned representation.
It has tried to maximize the similarity of representations of images and their associated
captions, so those representation spaces overlap. And you can, in some sense, swap the encoders.
From text encoder to an image encoder, and hopefully still have that aligned representation.
But overall, the pipeline looks fairly similar. So it's randomly sample poses in the scene, render
an image, and then try to make sure that its semantic features match our features of our caption.
But if you apply that approach naively without any regularizer, then there are a bunch of artifacts.
Sorry. These are some example generations for different captions. I believe this one
had something to do with liquid in a blender. This one might have been a colorful bus with
graffiti on it. So without regularization, we are getting degenerate scenes. And it's not
surprising because there's really no data involved in this process. In Dietner, the scene was
regularized by having some input views. Here, the canvas is open, wide open.
So in our work, we added some regularization. The scenes are composited with randomly sample
backgrounds. And we regularize the scene to be highly transparent. So this transmittance loss
encourages varsity in the underlying scene. So instead of getting lots of low density,
wispy content, like you saw in the previous slide, with the transmittance loss and this
associated background, our motivation in Dreamfields is to create more of a consistent foreground
object, a single foreground object. And these are the renderings for that,
associate caption, washing blueberries. There's definitely a lot of room for improvement
because each of these blueberries is mashed together with the others. The general caption has
been encoded into this scene. And there's a consistent foreground object. This is the
visualization of the process of optimization. In response to the question Leandra asked,
so it's creating this from one image, there's actually no images observed. There's only a
caption fed to the system. So any images that I'm showing are rendered using our Neural Radiance
field. They're completely fixed and all. I mean, some intuitive explanation for this is
how can we learn a scene representation such that it could be captioned with a given caption
from any perspective. Maybe that's how a human sculpture went approach the problem. So given a
caption, give me a clay sculpture of a tower. Let's say a monocular sculptor. Good. Optimize
for a clay sculpture that is a tower from any perspective. Sorry, what happens if the
caption is something vague like just a dog? How would your optimizer know that it should
have the same dog even from different poses or camera poses? Yeah, excellent question. The
constraint that views should represent the same object from different perspectives just comes
from the shared three presentation. So we're optimizing the same MLP from any perspective.
Okay, thanks. We had to simplify. Well, we didn't have to. You're able to keep view dependence in
the Neural Radiance field. So this regularizer ends up being kind of important. Like I discussed,
we've died and if you're able to learn a lot of these near field artifacts.
Sharing the scene representation is important, but some of the other techniques in our paper,
like the regularizer are also important for making sure you get a clean result.
In this example, we experiment with different caption templates
to measure the compositional generalization of the model. So the base caption template
here is a teapot in the shape of a blank, a teapot imitating a blank. And then in the video,
the caption beneath each object is the word that's filled into the template caption.
So a teapot in the shape of an avocado produces this object, whereas the caption
of teapot in the shape of a glacier produces something more ice-styled.
And I'm sorry about these animations. If you switch the caption from an arm chair
to a teapot, you'll also notice some changes in the shape. So there's legs on this avocado chair,
but when it becomes a teapot, the legs are removed. There's a follow-up question about
whether the clip library is 2D. Yes, clip is trained only on 2D images, so just on 2D views.
The motivation for our using clip is that we can very scaleably acquire caption images from
the internet. If you, for example, look at Wikipedia and just look at the upper right image
associated with each article, it has a caption beneath it. And there's a data set out there
called Wikitext, which has about 11 million captioned images. The authors of clip were
able to collect even larger data set by scraping websites other than Wikipedia.
But if you look at data sets with 3D objects in them, they're very small. The largest might
be ShapeNet, which is entirely synthetic objects. And there's usually no caption associated,
so we'd have to have a human annotate. This is a general trend in the 3D reconstruction literature
that the availability of 3D data is quite limited. And so in dream fields, we're able to exploit this
pre-trained 2D image encoder and text encoder, and then kind of lift it up into 3D by using
a shared representation of the geometry. There's a bunch of different techniques that we use to
improve the quality of the results. I won't get too much into this, but the metric is a little
tricky to define because there's no reference object for each caption. We only have a data
set of captions provided to us by the user, and we want to measure how well our generations are
performing. In order to do that, we use a neural metric based off of matching generated 3D objects
against the input captions. This is something like precision of retrieving the correct caption,
given the generator objects. Some of the most important techniques that help us here are
regularizer for transmittance and data augmentations, those architecture we use for the MLP,
and then later on, what model we use for clip.
This is an example of the process of optimization from different iterations,
so it actually can converge quite quickly, but additional detail might be added over the course
of training. In order to run 20,000 iterations of optimization, it's an expensive process
because we need to render out these images during training, but back to the envelope
calculation, it's about three to four dollars to generate each model on TPU in Google Cloud
at an hour. It's in the realm where an artist could afford to do this.
We're working on some follow-up work, which will speed up this process and make it even less expensive.
That's all I've got on these works. The broad goal here is to make content creation
easier and generate assets that are useful. This 3D assets I see is particularly useful for
downstream applications because they could be plugged into a game or plugged into some other system.
We have code out for both of these projects. If you want to try out the text to 3D generation
in your browser, you can use a Colab notebook that I've put together. I've tested it on the
Pro version of Colab, which has higher memory GPUs, so you might need to play with some of the parameters.
Thanks. Thank you so much, Eje. This is really fascinating. I have a few questions
and then before letting everyone else ask questions, the first question is that
are you able to walk us through some of the Colab code today or should we do it on our time?
Let me see if I have it up. Also, before going to changing your screen, can you please go back to
the animations? Sorry, I have so many questions because this is really cool. Or maybe the one that
is armchair. Yeah, give me one sec. Thank you so much.
I think these are really cool. I think that for the students and I,
this kind of inspires us to think, you know, maybe one cool thing to do is that
we can generate these things and use them in some avatar or game or something and this will be really
cool. So this is something for students to think about for their future projects because the goal
of this course is to inspire us to think about what are the creative ways that we can use AI.
So this is really cool. And one question that I have is that do you have, can you share some
intuition of, you know, for instance, let's say the rubric. It looks like a rubric and it looks
like a chair. But then we see that there is some, we wish there was more of the structure. And it
might be because clip is the objective and or assessor and thinking that, okay, as long as I
have a patch of, you know, red and yellow and things like that that are appearing on rubric,
I'm happy, the rest, I don't care much. Or is there any better explanation of what's happening?
Yeah, so the 3D structure only emerges because of the shared representation and the easiest way to
satisfy clip from any perspective, having this Rubik's Cube chair from any perspective,
might actually be to learn some consistent geometry. That said, there's no prior,
other than sparsity, and some implicit regularization just in the structure of the MLP.
So there's no no prior on the 3D structure, learn from data. And so that's something that I think
is missing and definitely opportunity for future work is how do we learn some priors on 3D data
and integrate them into the system to try to improve the plausibility of the structure.
One, one example where this issue arises is that sometimes you'll get repeated structures on the
objects. Like if you optimize for a dog, maybe it will have eyes on multiple sides of its face
because they're not visible. So you only see two sets of eyes from any particular viewpoint.
That is all the discriminator clip ever sees are those two eyes, but the underlying geometry,
there's no constraint that the dog should only have two eyes.
Okay, excellent. Thank you so much. Are there questions before we go to the collab?
The outputs, are they like dot fbx files? Or do they still need to be let's say a little bit
prepared in a rendering software before they can be actually readily used in the game engine,
like Unity or Unreal? They do need to be post processed. So what you get out is a train neural
net. So this function mapping from coordinates, we don't use the v direction in these results,
just XYZ coordinates mapped to color and density. So there are a bunch of ways that you could convert
that. I don't know of off the shelf software that will be able to do that conversion for you,
it'd have to be coded up. But you can sample the scene on some grid, for example, and then you
will get out color and RGB, you could convert that to like a voxel representation. If you want to get
a mesh, there's an algorithm called marching cubes that is able to find a mesh in the scene.
And there's implementations on GitHub of marching cube for neural radiance fields
that we haven't integrated into our particular library. So you take a little bit of glue to
grab marching cubes and then plug it in. So what do you all use to turn the neural net into these
graphics? Sorry, could you repeat the question? What do you use to turn the neural net into the
graphics that we see here? Oh, yeah, so that's done by rendering. So you can render the neural
radiance field from any perspective in the code, but that just renders out a 2D image. It doesn't
give you, you know, like a mesh versus the game engine will have its own rendering algorithm based
on rasterization or ray tracing, given the underlying geometry and texture map, which might
be real time. So the rendering here is not real time. You have to go evaluate the neural network
at a bunch of different coordinates and accumulate its outputs into an image.
So that's implemented. If you want videos, we can do that, but you'll have to DIY the conversion.
Ellie had a question on strategies to reduce rendering costs.
So you can render images at low resolution. And in the collab notebook, the rendering is done at
very low resolution. So experiments, you render out 168 by 168 images or higher.
But collab only gives you a single low memory GPU. So we render out 88 by 88 images.
And that really significantly speeds up the process. So rendering takes maybe 300 milliseconds.
It's here to do about three iterations per second.
If you're using alpha transparent, okay. So Ben is asking, how do we handle with transparent objects?
So the neural radiance field, the volumetric representation is really amenable to transparent
objects because the density is this continuous value and we can observe objects. So accumulate
color from objects behind transparent objects. In optimization, you might decrease the
density of some object that should be transparent, like stained glass windows.
The background is composited at the end. So any ray, if there's some accumulated,
if the transmittance is not zero along the ray accumulated throughout the scene,
then there'll be some contribution from the background image.
So the reason that we've kind of encouraged coherent objects is that if the object is not
coherent, then the background will leak through the translucent objects. Oh, I see what you're
saying. That, yeah, if you want stained glass windows. So I mean, you would have to, the scene
would probably optimize so that the transparent object is blocked from behind by something.
Yeah, the next steps, I think they're exciting lots of next steps. Because this is an initial work
and there's things like speeding up the optimization. It's been a lot of recent work
and speeding up neural radiance field training from images. And I think a lot of that can be
plugged in. How do you synthesize deformable objects? How do you bring a human in the loop
so they can provide feedback partway through training? All kinds of stuff to tackle in making
this more of a practical system for 3D artists. And would you like me to share the collab? I
guess we're at time. Yeah, please go ahead. That would be great. Thank you so much.
So this is the collab notebook. You can find it from the project website.
It is a compact implementation.
The system will run faster on GPU than on TPU in the collab notebook. But for all of our experiments,
we use TPU. Some helpers are imported from our library. So if you want to hack on some of the
low level primitives, you can fork our library or kind of copy those helpers into the notebook.
But the main way you'll interface with this collab notebook is by adjusting the quality
settings here. So in particular, edit the query. Here I've filled in a high quality 3D
render of Jenga Tower. And you can select the clip checkpoint you want to use. Clipbit B16 is
used in most of our experiments. There's also an internal Google model that's not available here.
But you can scale down if you're running out of memory to either the B32 or ResNet50.
Choose the number optimization iterations. I think at least 1000 is necessary.
But more will add more detail to consider the rendering with and then this number of data
augmentations. And then run training. So here's an example of the training run I've already run
in the notebook for that prompt, a high quality 3D render of Jenga Tower.
It won't exactly match the result that was shown in the slides because the version,
the collab notebook could scale down. But over the course of 2000 iterations of optimization,
you can see the different learning curves. This is the total loss that's being optimized.
Clip's cosine similarity, negative cosine similarity is improving. So this means that
the renderings of the object are becoming more and more consistent with the given caption over time.
And the transmission is regularization here. This is showing what is the average transparency
of pixels in the scene. And in this plot at the bottom, the collab notebook will ride out renderings
periodically every, I believe, 100 iterations. So at the beginning, the scene is low density,
essentially empty. And then over time, some content will emerge from the optimization.
And then that's refined and sharpened over time. The camera is moving around. So the camera is
being randomly sampled around the object. And that's why the scene is rendered from different
perspectives. And then finally, the collab notebook renders out a video, 48 frames. And this is the
result. On the GPU that collab gave me here, a P100, the optimization I think took about
six, seven minutes. So hopefully you can get some cycles in.
In the run training section, it says if you run out of memory, tweak the configuration options
above. What do you recommend changing? Yeah, that's a good question. So I think you can change this
clip at B16. I would try to clip B32. There's also on the first import in NVIDIA SMI printout.
And so you can look at how much memory is available. Sometimes it's worth retrying multiple times to get
a larger GPU. This P160 gigabyte, I think you won't get without Club Premium, which is about
$10 a month. But you think you can get 15 gigabyte P4 GPUs for free. Sometimes the
collab will give you an 11 gigabyte GPU that might not be enough. If you can tweak the configuration
parameters, I would try reducing this number of samples. So this is the number of points along
each ray that is sampled. And that affects the batch size. So the render width, the batch size
scales quadratically with the render width because we're rendering all square images.
And then the num samples, the batch size to MLP scales linearly. So you could reduce this down
to 32 even at the lowest. B32 will use less memory than B16. So this relates to the patch size and
the vision transformer clip encoding. And then if you want to scale down even more, you can change
the number of data augmentations per iteration, maybe down to two.
Oh, Ben says that you can't retry for a better GPU.
That's unfortunate. I mean, I don't know whether MIT has like a shared GPU cloud, but you can
also just download this iPIND and run it on your like Jupyter notebooks, post it on some MIT compute.
And it will paralyze across multiple GPUs.
Cool. And I'm happy to take any more questions that you have.
Excellent. Thank you so much. Maybe at this point I'll stop recording and if students have more
questions, we can...

Hello, everyone.
Welcome back to our course, Deep Learning
for Art, Aesthetics, and Creativity.
Today, it is our pleasure to have very a specialist speaker,
David Bao.
And I just let him to introduce him a little more,
because I think it's very inspiring for many students.
The past that he has come to this point and for future.
Please go ahead, David.
So I want to give a little background.
Since I am a post-industry academic,
I spent a bunch of years as a software engineer at Google
before coming back to MIT.
And I want to give a little bit of insight
in my thinking there.
So the reason it's really interesting to be in computer
science right now is because the field is changing.
The dream of having self-programmed computers
is one of the oldest dreams in computer science,
but it's never been a reality.
Even though we've studied machine learning for a long time,
I think that until just a few years ago,
machine learning was really more accurately called.
It would have been more accurately called the art
of accurate counting, statistics, understanding
the statistics of how frequent words are and diagrams,
or certain image statistics, or something like that.
And if you understand statistics well,
then you could do some nice tricks.
But I think that until recently, really calling
these things sort of self-programmed systems
would have been an overstatement.
But I don't think it's really an overstatement anymore.
I think that these machine learning models are really
learning non-trivial things.
And it leads to all sorts of questions
about what should we be doing as programmers?
What does it mean to do software engineering?
And so I thought it was a very interesting time
to come back to academia.
That's why I'm here.
And I actually think that that's one of the choices you face
when you're trying to decide between industry and academia.
And I think in industry, you will have lots of resources
to make things work, to make the next widget or the application.
And there are great places.
Google is a great place where you can really push state-of-the-art
in that and do really neat stuff.
I think that there's less of a push in industry
to ask the question, why?
Why do things work?
Why are we doing what we're doing?
Where is it going to lead in other unintended consequences
and things like that?
We tend not to ask those questions too much in industry
because there's so much to emphasize on the how of how
to get it to work.
And so I thought it was a time to switch tracks
and start asking why because the field is changing
so dramatically.
And I think that I'd encourage people
who have an interest in these type of questions
to realize you can really make a real contribution taking
the academic track as well.
So OK, so let me introduce my talk.
So it's about painting with neurons of general adversarial
networks.
It comes out of work from asking why.
Why do these networks do what they do?
And so let me advance here.
Am I in full screen?
So do you see the full screen slideshow?
I can't see what I'm projecting.
Or do you see all my notes and all that stuff?
Yeah, I can see it.
But also maybe a student can tell us.
Yeah, OK.
Is everything looking this OK?
Yeah, it's a full screen slide.
Hopefully it's OK.
So OK.
So the main problem that we're looking at here,
and I'm not sure why the images are
overlapped in the right way.
Hopefully the layout will get fixed as we go on to next slides.
But the main problem surrounding my talk
is image generation.
And so for the last few years, there's
been this question, how do you make a state-of-the-art program
to generate realistic images?
And the general process is, first,
you want to collect the data set of real images,
like these pictures of buildings on the right.
And then you want to train some sort of program,
some sort of generator network to generate those programs.
And so it's been a puzzle.
There's a lot of different ways you could imagine doing this.
And so people have been puzzling,
how do you train such a thing?
How do you even supervise it?
What should the inputs and the outputs of the network be?
And the thing that has really been working the best
in recent years is an architecture
you guys have all heard of called GANs,
Generative Aversarial Networks.
And the trick for GANs is to reduce it down
to a simpler problem that we know what we're doing.
And so the simpler problem that they recognized
when designing GANs was that generating images,
we don't really know how to do.
But classifying images, gosh, that is an easy problem.
We can classify images.
And so what we could do is we could train a classifier
on this really easy task, which is given two sets of pixels,
which image is real and which image is not a real photograph.
And it turns out that for most arrangements of pixels,
this is a very easy task to train a discriminator on.
It gets very good very quickly.
We'll start getting 100% accurately on that.
And so but the neat thing is that once we have a discriminator
that can tell the difference between a fake image and a real image,
then we can hook it up to our generator and we can say,
all right, we didn't know how to tell you, generator,
how to make a real image.
But you know what this discriminator can tell you?
Because all you have to do is generate patterns of pixels
that fool the discriminator.
If you can make the discriminator think it's real,
then it must be better than random.
Now, the problem is that even though the discriminator can
get very accurate in telling what's real,
the generator will also be very good at learning how to fool
the discriminator without working very hard.
It'll realize that, aha, the only thing I need to do
to make the discriminator think it's real is put some blue sky
in there and put some texture that kind of looks like building texture.
And the discriminator will say,
whoa, that totally looks real.
There's a sky, there's the right colors for buildings
and some vertical lines and things.
Ah, that's totally real.
But as a human, we look at that and we think,
oh, that's not a very realistic image at all.
So the trick is to iterate this process, to go back and forth.
After the generator can generate sort of halfway looking real images,
then have the discriminator say, ah, well, that's actually fake.
And we're going to tell the difference between those new fakes,
those better fakes, and actual real photographs.
And the discriminator has to now work harder at getting better.
And so if you alternate these processes,
then you end up conversion to very, very good generators
that can generate very realistic images.
And they, you know, the typical learning process
is actually just to do only one step of iteration
between the discriminator and generator and just alternate that.
So by the time you're done, you've played this game,
you know, millions and millions of times back and forth
between the generator and the discriminator.
But the neat thing that's happening here is that
it can generate these images that look very realistic in the end.
But let's see.
So, oh, here's another picture.
So we'll get these images out that look very realistic in the end.
And we'll get this generator, which is just a deterministic function
that takes actually the input of the generator
is actually just a random vector.
So we'll take these relatively small random vectors,
like a 512 dimensional random vector,
and we'll put it into this thing.
And it's been trained so that no matter what it outputs,
it will look very realistic, like this example image here.
Or if I change a vector, I'll get a different image out,
and it will again look very realistic,
even if it looks completely different.
And so it's just a deterministic function
that really wants to make realistic images.
And so here's like a sample of like output from a generator.
And you can see that after millions of these sort of
generative training steps where it's pitted against a discriminator,
it actually gets to be pretty good.
And so this is StyleGAN V2.
It's a model that was published last year.
And it's currently the state of the art
in generating realistic images of certain types of image
distributions.
And so when you look at a collection of images like this,
you might think, actually, the first time
I looked at the output of some of these state-of-the-art GANs,
I was confused between the training set and the generated output.
This is not the training set.
This is actually what the generator is producing.
And so you see all sorts of interesting effects here.
And so one of the questions to ask
is what the heck is the model doing inside?
Can we understand the underlying algorithm
and what the characteristics of that algorithm is?
Like why does this work?
And so one of the funny things that you'll notice
is that some of the images have these strange artifacts.
Like take a look at this one here.
So this GAN is pretty good.
This generator is so good that it actually
has noticed that the training distribution that is imitating
has some percentage of images that were stolen off of Shutter
stock, and they still have the watermark on them.
And the generator says, well, if I
want to make things look realistic,
I better put watermarks on some percentage of my images, too.
It learns it's got to protect its own copyright.
So it does that.
And so something like 6% of the outputted images
from state-of-the-art style GAN will
have these artifacts that show the same type of watermarks
that were on the training set.
This is the Elson Church training set.
And so these kind of watermarks look like this.
But the reason I thought this was cool
was that it's this very clear thing
that the image generator does, but it doesn't always do it.
Like most of the time when it generates images,
it generates images without a watermark.
But sometimes you get these watermarks.
And so it's almost like this binary decision.
There must be a switch that the network has, at some point,
where it decides whether it's going
to put a watermark on an image or not.
And so we can kind of ask the question, where is that switch?
Is there a neuron somewhere in this network,
which is controlling the watermarkness?
So I went on a hunt for this.
This particular network has about 30 million parameters,
which sounds like a lot.
But it's just a deterministic computer program in the end.
And it's not that hard to go hunting for things like this.
You can make an algorithm that has a heuristic that
determines whether it's a watermark or not.
Just go hunting for things that correlate with that.
And so I'll show you what I found.
So at layer 5, I found this very interesting neuron
that did correlate with watermarks a lot.
It was activating whenever images look like this in the end.
And not only that, but because it's at layer 5,
it has a location for where in the image it activates.
And I'll show you where it's activating.
So this neuron is activating at these middle parts of images
whenever the image is showing a watermark.
And there are other neurons that have similar behavior.
Like so, for example, there's this neuron 234 at the same layer.
And it activates in regions like this,
both in the middle watermark and the bottom bar that shows up.
And there's about, if you hunt through the neural network,
you find about 30 neurons that are similar and behave like this.
And so that's pretty cool.
So then the question is, well, do these things really
act like a switch?
What if we've removed these neurons from the network?
What if we force them all off?
What if we force these neurons to be off all the time?
That will all happen.
So normally, we think of these neural networks
as completely opaque systems.
We train them end to end.
They're just these big black box functions.
And we normally think of the functions
as computing things where everything depends on everything.
And so if you randomly rip through the function
and remove some of its operations,
then maybe you expect to get total nonsense out,
just garbage or noise.
But we found these particular neurons that really
correlate to this thing.
So let's see what happens when we turn them off.
Do we get anything intelligible at all?
So this is what the network generated before these
are the watermark images I showed you before.
And I'll show you what happens if I turn off
these 30 watermark neurons.
So I'm going to give the network the same input,
but turn off these neurons during its computation.
And you can see what the output looks like.
So you can see before forcing these neurons off
and after forcing those neurons off,
the images are still very intelligible.
They look realistic still.
But now the watermarks are gone.
So I thought when I saw this, I was pretty excited
because it's like, oh, there are switches inside the networks.
And these networks are doing all sorts of amazing things,
not just like showing watermarks.
So when I first found this, it was on ProgressiveGAN,
which is a year earlier, a couple years earlier.
The images didn't look quite as good.
But still in ProgressiveGAN, they
do all sorts of amazing things.
Like they will arrange a scene with a river and trees
and grass and building architectures with all sorts
of different features.
And you can ask, is there a switch
to turn on and off clouds in the skies?
Is there a switch to turn on and off trees or windows
and buildings?
And so I went hunting for that.
And the way I went hunting is I tested every neuron one
time I inverted the test.
So basically, I look at each neuron
and I say, where is it activating?
And I ask the question, is it activating
an interesting part of different images?
So for example, if I took this one neuron here
and I see where it's activating when
it's generating this image, you can see it's very hot on the right
and on the left, but not much up in the sky.
And on this very same neuron, when
we generate a different image with a different input,
this very same neuron is not activating very much
anywhere in this image.
But if we generate another image,
then it will activate in a specific area here,
mostly on the lower left part of this image.
And you can see where it's on the lower left.
There's a tree there.
And so it kind of gives you the hypothesis
that maybe this neuron is correlated with trees somehow.
So obviously, we can do this.
We can collect this information over thousands of examples
of generated images by looking at where
the neuron is activating.
We can ask what kind of thing is in the image,
what kind of objects, what are the semantics of the image
in the location that the neurons are in.
And we can just repeat that test process thousands of times
to see if the neuron is agreeing with any particular kind
of semantics that are in the images.
So if the neurons are showing up where the trees are all the time,
we can just count and see if that's true in general.
And we can also look for correlations with other things.
So what I did is I searched for correlations
with thousands of different, hundreds of different types
of semantics and object classes, different parts
of buildings or objects or other things
that can show up in a scene.
And so what do we find?
Well, we do find there's a neuron that correlates with trees,
just like the one I was showing you.
There's actually a few that are like that.
And there's also neurons that correlate with other things,
like domes or other building parts, like windows and doors.
And if you change the model to look at other things,
then you can find neurons that correlate with things
like windows or chairs or other things
that show up in the scene.
And so this is actually pretty neat
because this model was trained unsupervised by any labels.
All we did is we told it generate realistic looking scenes,
realistic looking photos.
And we did not train it with any labels.
We didn't tell it that these are photos of scenes
that have big windows and these are photos of scenes
that have little windows or anything like that.
Or here's where the windows are.
But what happened was the network discovered
that it had to learn a representation where windows are
represented differently from the way chairs are represented.
But somehow, even though windows can look very different
from each other and chairs can look very different from each other,
the network has this component, this representation,
this neuron that activates on all these chairs,
despite the amazing amount of diversity that it shows.
Like none of these chairs really look similar to each other.
They have different colors and different textures
and they're oriented in different ways.
And yet, the same neuron is activating on all of them.
The same thing goes for other things that show up in these images.
So does anybody have any questions about this?
I'd love for this to be a little bit more interactive
than the way I'm giving the talk.
So let me open the floor for a question for a minute.
Has anybody tried playing with the internals of GANs yet?
I'd love to see if has anybody like generated images
using a GAN before?
No, but I do have a question.
Yes.
So what was the end goal or the larger reason behind finding all of these neurons
that correspond to different objects or features?
Well, when I was originally looking at it,
my original goal was just to understand how these models did their computation.
So asking the question, why?
But the neat thing is that after I found the structure,
then it became clear that there are new applications that you can build on top of it.
And I think that's one of the cool things that comes out of this sort of academic style inquiry is,
you know, originally I was just looking to make catalogs like this.
This is a catalog of all the different types of correlations
that I found with neurons inside a model for generating kitchens
and the kinds of, you know, the patterns you see.
And, you know, I've done this before for classifiers.
And, you know, when you do it for generators, you get different patterns.
And so I was just really interested in making these maps of seeing what is computed at what layer,
you know, where and how accurately.
So this is, you know, the progressive GAN has, depending on the resolution, has about 15 layers.
And if you sort of chart what you see in different layers,
you can see this really interesting thing phenomenon where it's in the middle layers
that you get these highly semantic correlated neurons.
But then as you get to the later layers, then they tend to be more physical
and there's not as many semantic objects.
So it's like in layer five, we have things that really correlate with ovens and chairs
and windows and doors, even though like a window kind of looks like an oven.
The model clearly has different neurons that correlate with windows from ones that look like ovens.
And so I was originally interested in just mapping things out.
But the correlations were so striking that it leads to these interesting applications that you can build.
And I'll show you some in the next step.
Let me, before I do that, let me see if anybody else has a question as well.
Yeah, David, I was hoping that you could also show us the application at some point,
which I think is very good to see why you asked this question.
I mean, that's more.
That's great. Let me let me zoom on to the application.
So so the the new thing is that just like we could turn off watermarks,
we can turn on and off things in image generation.
So for example, if I find all the neurons that correlate with trees and I turn them off,
you can see what happens.
I'm going to turn them off sort of one at a time here.
And so originally, the image would just be generated this.
But if I turn off some tree neurons, you can see that we can actually remove the trees from the scene.
And the cool thing is that this is different from Photoshop.
If you went and you tried to erase trees from an image, then you'd have this puzzle of what would happen
about stuff that was occluded by the trees, like what's going on behind there.
And so this image generator is actually it's got this latent model that has an understanding of what the scene is.
And so and even has an understanding of things that it's not explicitly drawing.
So if you remove the trees from the scene, then it'll come up with a reasonable looking, you know,
image to draw what was behind the trees.
Or you can do the opposite, which is you can take neurons that were not originally on in a generated scene and turn them on.
So if I take a set of neurons that correlate with doors and I turn them on in a certain location,
and you can see what happens in the generated image, I'll get this door in the scene.
Not only will it just be a door, but it'll have an appropriate size and orientation and style for the building that it's in.
And so if I take exactly the same neurons, and I activate them in a different location like here in this building,
then even though it's exactly the same neurons exactly the same activation that I've done, I get a different door that is like a much smaller has a different style and so on.
It's appropriate to the building that it's in.
If I if I try to put a door in a place that would make sense, like by turning on neurons up in the sky, then it like will like not do anything.
This is this is the actual output of what happens if I turn on the exact same neurons up in this location.
So there's a lot of interesting context sensitivity that you can measure.
But one of the cool things that you can do is you can actually hook this up to a paintbrush user interface.
Like I can find neurons that correlate with domes or doors or things that if I want to add doors to a building,
I can just sort of paint them on and the doors will show up and you can see the orientation of the doors is appropriate to the wall that you put.
And if I just say I want trees, it'll put trunks and leaves, you know, in the right place in the trees of plants to plant plant them on the ground.
If I take grass and I can turn the grass neurons off and remove grass from the scene and it'll come up with what the scene should look like instead.
And so I can kind of do these semantic manipulations directly.
Oh, here we're turning on domes and you can see it will turn the top of the church from a spy to a dome,
but it also sort of stitched the dome into place to make it look good here.
I'm removing grass again.
We can like put a door in the scene and if I put, you know, sort of put a door in the wall,
then it'll come up with like the appropriate location and style orientation for the door,
even if I draw very roughly.
So when I'm drawing, every time I touch the surface here, what I'm really doing is I'm just turning on a few neurons
and I'm letting the math of the GAN generator deal with all of the details of how to arrange the actual pixels.
So does that sort of give you a sense?
Does that answer your question for like, you know, what kinds of things you can do with this by understanding what's going on in the interior of the model?
Maybe now I should stop it.
Oh, yes, go ahead.
Are these different neurons for like doors in different areas?
No, no.
So when I, when you click on the door button on the left, I am picking 20 neurons that are the door neurons.
So by doing the statistical analysis ahead of time that I showed you earlier,
I've identified 20 neurons that correlate very strongly with the presence of doors.
And when you click on the button on the left, I am picking those neurons.
Now it's a convolutional network, so there's this translation, it depends.
Those neurons appear at every pixel.
And so what you can do is you can just turn on those neurons in random pixels that you touch.
The pixels that you're changing where the neurons are, that was what I didn't understand.
Does that make sense?
So, so because it's a convolutional network, so it's actually, it's, it's like the neural network is cloned at every location.
It's the same neural network that's being used to process every, you know, patch or patch of pixels in the image.
And, and so if I asked for a door in a place that wouldn't really make sense, then it won't put a door there.
If I asked for a door in a place that makes sense, and it'll make a big intervention, it'll stick a big door there, which you can see.
So I could be very rough about where I put a door and it'll, like, put it in the right place.
So that's, that's the idea.
So let me, let me zoom around here and I'll show you a couple of things that you can do.
So now there's some limitations to this.
And I'll just show you some of the techniques that you can use to get around the limitation.
So one of the problems is that, you know, we can do all this cool editing, but we can do this editing of a randomly generated image.
And, and so, so when I posted this demo on, on the web, you know, an artist called me and said, Hey, you know, I love how you can edit images.
I can edit this image of a kitchen here.
But that's not the kitchen I want to edit.
I want to edit my own kitchen, right?
Like, here's a photo of my kitchen.
And, and I want to edit that one.
And, and I had to explain to them, you know, they said, well, can you just load into your demo, my, my kitchen instead of yours?
And I had to explain, no, no, no, that's not how GANs work.
They're unconditional generators.
You know, you give it a random vector of 512 numbers.
And it decides what image to make.
And then once it decides what image to make, then you can edit it.
And so I am sorry, I can't edit your kitchen.
And so they were very disappointed by that because they had all sorts of art ideas of things they wanted to do.
And so now the problem is that, you know, the problem could be solved if we could find the random vector, some random vector that that output the kitchen image
or the specific real photo that I wanted.
The problem is how do I find it 512 dimensional vectors as pretty big vector space.
And, and so I don't know if my GAN can actually generate this image or not.
So one of the things you can do is you can just treat this as a, as an inversion problem.
You can take the neural network and you can learn how to run it backwards.
Basically, you know, think of the neural network as a function G and you want to learn G inverse.
So you can treat that as another training problem.
And there's a bunch of tricks and I won't go into all the tricks here.
But, but basically the idea is that you can actually find a Z that comes closest to generating your image by, by training and doing a couple of other tricks.
And you can actually get a Z that will generate your image pretty closely.
But the, the, the thing that's a little bit sad is it also reveals things that the network cannot do.
So, so this network is capable of generating this image that I'm showing you here.
But the original kitchen that I started with looked like this.
So you can see what the differences are.
I've lost a lot of stuff, right?
So, you know, I can use the GAN to edit this image, but this image is not exactly what I started with.
And so, so one of the pieces of science that I did is I asked the question, you know, is there some way that we can actually make this work?
Can we actually, you know, get the network to output a real photo that, that the user gave us?
Well, we can get the network to output this sort of simplified version of it.
And it turns out that if I modify the weights of the network, I can actually fine tune the network to get it so that a very, very nearby network with weights that are almost the same as the original actually hits this target image.
Exactly.
And so, so there's a bunch of details in the right way of doing this, but it turns out that, you know, you don't actually have to change much.
If you change the fine grained weights of a network, you can, you can change a lot of the details of what images actually get generated.
And, and, and if you are given a target image to get, you can actually tweak, tweak any network to generate exactly that target image if we want.
And so, so, you know, so yeah, we can get all the objects back.
And the key thing is we haven't really changed the network much so we can still do editing. So like if we take the window correlated neurons, we can take our modified network we can turn them on.
And, and then now we can like add a window.
Let's see if we show that. Yeah, so this here's outlook so we get this nice window here. And the scene is began is doing its cool tricks of orienting the window properly and doing some reasonable things.
And it has some really interesting effects that are non trivial here. Some of them are good and some are bad. So for example, all I did was turn on the neurons in this location saying I want windows, and it did it.
But look what else it did. It also added these reflections right here on the counter and so this, this kitchen can does this a lot like adds adds non local reflections where it thinks that there's a shiny table and so the cool thing here is that after I did all the inversion and stuff this
can actually thinks that there's a shiny table here and it's right and it thinks that if I add a window here they should add reflection that's right also.
But look what else happened up here that see this lamp up here.
When I first lifted this in low resolution I thought oh maybe it turned off the lamp because once you have windows you don't need the light on.
But no it didn't do that it just messed up the lamp is just total it took this whole area up here, and just, and just distorted it badly.
So that that's a little dissatisfying it means that this fine tuning thing where we get again to you know target a specific user image.
When I do when I try to teach it all the details I'm not really teaching it what the lamp was I was just sort of showing it how to arrange the pixels and again made its best guess on how to generalize
how the image should look differently. If I change something like out of window, but with only one example of a lamp that looks like this it generalized wrong it has no idea what should happen to that lamp when I when I add a window so this is this question of like how to make
changes in a network with with with achieving good generalization is which is a good question and it was there was something that puzzled me for a year after doing this work.
But, but the work is still pretty cool you can still use it for modifying real photos so here's like a photo of I got off of Wikipedia of like some real locations.
And you can you can edit them I can add grass I can add doors I can add domes, you know just like, just like the, the other game paint app except I can actually start with a real photo that you give me, and I can invert that photo through the network.
I can get a good starting image fine tune the network to make it, make it output, you know the target image and edit that image, add bigger domes and it'll sort of match the architectural style, and, and, and, you know, do different things like that, I can add domes remove
domes add doors, you know, things like that.
Let me see if I can get this video here to show.
So this is status center. Let's add some doors here.
So you get the idea I'm doing exactly the same intervention that I did before.
And it's, it's opinion just like before, it will not add doors in places that it doesn't think are not good places for door, it has some opinions about where doors are allowed it likes to put them in brick walls.
It thinks it's okay to put a door in a tower, like that architectural detail oh I put domes here. It's happy to put domes on top of buildings, it's not happy to put a dome like in the middle of the sky.
It's not happy to put a door in the middle of the sky, but you know it put trees in different places.
And, and so there are things that understand there are things that it doesn't understand very well, it's sort of making a guess of what the structure of the images.
It doesn't know what to make of my advisor, you know, sort of planting grass in front of him.
And that's not very realistic, but you can kind of get a feel for what the structure and knowledge of the model is by doing these kind of interventions.
So this was really cool, I think it got a lot of people's attention.
Adobe noticed this stuff, and has been busy trying to make different painting applications using, you know, again technology that are I think partially inspired by by by this kind of discovery.
So, David I have a question.
Yep.
This is really cool.
The question is, when you modify for instance churches, I assume you have trained your again on a church data set.
Yes, that's correct. What about when you do it on the real images for instance in this case, you know, your advisor.
Yes. So actually both of these are using the church data set as well.
I think it's interesting that even you have trained again on church, you can depict a person.
Yes, so this is so the game.
Now you have to keep in mind that what I've done here is a fine tuned again. So, you can actually, you know, you can actually get you can actually get again to do a lot of things by fine tuning it.
So I've, I've told the game, please basically overfit on this target image. So the game, you know, has 30 million parameters.
And, and, you know, an image only has, you know, 10,000 pixels, and it has plenty of excess capacity to memorize the details that I might want to do.
And so what I've done is this as I've taken the image. I've asked again, through my inversion techniques, what is the closest church image that you can generate that looks like my thing and you get a different image it.
I don't have the image to show you here but you get an image that looks kind of more church like it's a little bit it'll be architectural have the right kind of shape the kind of right textures.
But, you know, it won't show my advisor here and things like that it'll be, it'll be this rough approximation for that my my image, but that is in the domain of what the game can actually generate.
Then I say, Okay, that's not what I want to do I want to actually edit this photo. So let's fine tune that network so that, so that,
you know, that same Z instead of generating the church that you would normally generate I want you to generate this image.
Change the weight slightly get it so that that Z targets this. And, and so that's what I've done here but I've tried I've done that in a way where I try not to change the weights too much I just try to change the weights.
I change the fine grained layers and I don't change the course grain layers and I, and I have a regularizer to make sure the weights don't change too much.
I mean that you are changing the pre trained weights or you are putting some extra weights and then you place them.
Oh, here I'm actually changing the pre trained weights. So the network has 15 layers I'm actually going and I'm changing some of those layers.
I'm not adding anything new to the network. I'm just changing the weights in the network itself.
Now what I've done here is I've overfit the network to this one image. The network is not generalizing this knowledge so for example,
you know it can draw Antonio in this one image. But if I look in the network, if I probe it a lot and see can it ever generate Antonio in a different setting in a different image.
It cannot. In fact, you know, as much as we probe things, it really doesn't look like we've changed the output of the network in any meaningful way for any image, except for this one.
It's almost like, you know, the network generates is really complicated manifold of realistic images, and we've told we've picked up one point of the manifold and we've dragged it over to pass to this point, but we've done it in a very local way.
So it's really not affected any other points of what the GAN is generating.
But for the purposes of doing this kind of application, it doesn't matter that it's not generalizing because the user doesn't care about a different photo, they just care about their own photo.
So it's a pretty cool technique anyway, even though it's sort of not the classical goal of machine learning. Does that make sense?
Yeah, it does. And I wonder if the user has more images of themselves with that over time and make the network even better in generation.
Yes, this is the big question. And I played with this for many months and I haven't got it to work. And if anybody can figure out how to get to work, I feel like it's one of the holy grails of like how to add a new thing to a generator.
The generator knows about all these things that knows about trees and knows about all these architectural pieces.
But what if I came along with something new? What if I work for GM and I want to sell Cadillacs, then I might come to one of these models and say, you know what, you should draw cars.
In fact, I want you to draw specific cars. I want you to draw Cadillacs in front of all these buildings. How would I add Cadillacs to my model or add Antonio to my model or something like that?
And we don't know how to do that yet, although I'm going to show you a little bit of work where we can do something that's very similar.
And if I don't know if I have time to go over this, but I'm going to zoom through this because I'm so excited by this work.
So it's motivated by this sort of question, which is, you know, we have a model of like drawing towers, let's say, right?
But there are things in the world that we might want to model that we don't have a data set for. For example, you know, in Decatur County, Illinois, there's this courthouse that has a tree growing out the top of the tower.
It started growing out there by accident, but the people in the town love it.
And so, but it's, but there's no, so like if I want to get a generated model to draw trees growing out of tops of towers, I can't do that in a classical way because I can't create a big data set of a million buildings that have trees growing on the top of the towers because they don't exist.
It's just this one.
And so, now if the point is I want to generate images of this type, you know, well, I could use a regular image editor I can take any building of tower and I of course I can stick a tree on it.
Right. I could use my, you know, again, painting method to, you know, activate tree neurons or something like that. But no, no, no, that's not what I'm asking. I'm asking this other question of like, how can we stick tree towers into my model?
How do I modify the model to have this new concept in it? Like I start with this model that has all these weights that encode all these rules for how buildings look and things like that.
And I want to create a new model that has new weights that encode new rules. So for example, the old model could generate all these buildings of towers that look normal have spires, you know, pointy tops.
And I want to make a new model that has weights that encode a different rule so that like they have trees growing out the top. Right. Or any rule that I choose.
Right. And it turns out that this is actually possible. So this is different from the team techniques that I showed you before, because in this technique is actually generalizing.
This is, you know, if you use this technique, not only you change the output of one image of the GAN to have like some effect, but we can actually change the outputs for a whole class of, you know, a large subset of the outputs of the GANs to follow a different rule.
Like any pointy tower output will have trees instead of pointy towers.
And so, so I'll just show you a little bit of like the interaction here of what it looks like when you get our method into an application. So I, let's see if I can get this to work. So here, what I'm showing you is the output of a style can be to generate in churches, you can kind of.
And there are three parts of this UI. There's an image viewer. Then what you do is you can select a rule that you want to change and then you can specify how you want to change the rule. So there's three parts of this little user interface.
And I'll just show you sort of how the effect looks by showing you one of the interactions. So you can kind of use the image viewer to scroll through lots of examples of what the generator is capable of generating.
And then we can go to these examples and we can say, hey, you know what I'm really interested in? I'm interested in this rule of how to generate pointy towers.
And so I can select a few pointy towers. And you can think of this is what I'm looking for is the neurons that are responsible for the shape.
And so I can select a few examples and I can say, hey, what other, what other outputs of the game share the same representation.
And it'll show me, oh yes, the GAN is generalizing this way. These other pointy towers are represented the same way as the ones that you chose. And then I can go and I can say, all right, I want to redefine how these pointy towers are rendered by this generator.
I want them to be rendered like this tree here so I can copy the tree from one output of the generator and I can paste it into where I would like that tree to show up.
I wanted to show up instead of pointy towers. And then I can say, okay, now insert this new rule into the model, compute what the right changes to change the model.
And then after I do that, that takes about eight seconds to do the math to figure out how to change a rule. And then after I do that, then I get the GAN to generate new images and, and they look like this, you know, like the tops of the towers now have trees on them instead.
You can see how that looks. And it's not just affecting that one image, it's affecting all the pointy tower images. I can do a little search for more pointy tower images and, and do I have that here in my thing. Yeah, so here's a search for more pointy tower images and you can see they, you know, they all have gotten these trees sprouting out the top of it, like some sort of dystopian tree world where vegetation is taking over the planet.
And, and so you can do this in a bunch of things. I'm going to skip over some of the technical things here or some of the other examples of what you can do here.
You can edit reflections and things like that. I've got other videos that you can look for on the internet. But I wanted to show you a sense for what we're doing inside when we do this kind of thing.
Like I showed you before that again has is like, got all these convolutional layers stacked up it's about 15 layers. And what what what the discovery was that led to this application was that each one of those layers can be thought of as solving a very simple,
separate problem from the other layers. And what is that simple problem.
It can be treated like a memory where the layer is solving this problem of matching key value pairs that it's memorized. So every location has a feature vector that you can think of as a key.
And what, and the key, each key like, you know, represents a certain type of context like, you know, the middles of towers or the tops of towers or something like that.
And what you can think of the, the map as, as, as storing is what should be, what is like the pattern of features that should be rendered whenever that context comes up.
Right, so you can think of it as just basically key value store. And, and so, so this whole idea of using a matrix as a key value stores and it's like the oldest idea in neural networks.
People observe back in the 1970s that if you have a single layer neural network, you can treat it as a as an approximate key value store that remembers keys with minimal error.
And, and so, if you had a set of keys and a set of values you want to store, and you ask what is the optimal single layer neural network that you'd use to store it.
It's actually, you know, classical linear algebra, it's like the solution to at least squares problem.
So what we can hypothesize is that in these very, very fancy, you know, 2020, you know, 50 years later deep neural networks, actually each layer is just acting as one of these now which keys are being stored and what values were being stored.
We don't know, but we could hypothesize that there is some set of things that are being memorized some set of keys and values.
And so that, that maybe this weight matrix that we have is the solution to some of these squares problems. So the cool thing that we can do is, we can say we can ask the question, what would the weight matrix look like if we changed one of the rules.
If we had one new key value pair that we want to change, then what would the weight matrix be. Instead, we want all the other things that the network is memorized to still be memorized with minimal error, just as before, except we're going to give this new constraint
we want to write a new key value pair into it. And it turns out that that's also the squares problems and constrained the squares problem we can write down the solution in this form.
What's great about these two, the squares problems is that they cancel each other out. Most of the terms are the same. And, and, and we can actually ask the question.
How would the weights have to change if we added a new key value pair, without knowing which values were written into the network before we don't actually have to know what the old key value pairs were.
We can just assume that the network was optimal as storing all these key value pairs, and, and the math for like how to write a new key value pair comes out the same anyway so so that's there's there's a little bit of a mathematical insight and trick here.
But what it allows us to do is it allows us to find exactly what to do to change one thing that the network is memorized you do this rank one update in a specific direction.
And, and you can take a key and change it to any value you want. And that will, you know, the same form will minimize error for for other keys regardless of what value we write it's almost like it really is a form of memory that we're changing.
So our method is basically you find a key by asking the user to select a few contacts that look the same we average them to get a good key.
Then we ask for a copy paste example to get a goal. That's the new value that we want to write into the key of the memory and then we do this math to to find how to change w in the direction of the key only we find a rank one update that does this.
And so, and so that avoids changing other rules so we can do this on a bunch of different game models and.
And so you can see like, you know, people like to change people's expressions here so what we're doing is a little different from what you normally do to change people's expressions and again what we're doing is we're actually going to rewrite the game.
So it only outputs people who are smiling we're going to take all the frowns, we're saying okay there's there's a rule for frowns we're going to change that to a rule for smiles by showing an example.
And so by patching frowns to smiles. Now we have a model that just outputs people who are smiling. We live in a happy world.
So that's, that's, that's pretty cool.
And of course we could have done that by, you know, changing the training set by collecting only training data of people who are smiling. But the neat thing is that you can also do this for things where that you don't have a training set that looks like it so for example, there's a there's a rule in the model for
the eyebrows should look on kids. So you can see that kids have these very wispy light eyebrows that don't have much air. So we can find that rule by identifying a few examples that gives us a rank one direction in the weight matrix.
And then we can redefine it we can write a new thing into it and say, you know what we want the eyebrows to look like this like that's very pushy much sash.
You know, paste into one example do the math. And then now we can change the weights in a way that generalizes so now all the kids had these very pushy, you know, eyebrows, and it's something that we wouldn't have been able to get by collecting training set because we
don't have kids that look like this in the real world. It's something that just comes out of our imagination. So this is kind of the thing. I kind of feel like this is the big reason why, why, why be interested in how these models are working.
Inside. And the reason to be so interested in it is because, as long as we don't look inside our models, then we're really constrained.
Because the only thing that our models can really do is imitate the real world, we can collect huge amounts of data, and the models that we create will just get better and better at imitating the way that the data is the way the world is today.
And I kind of feel like it goes a little bit against why I was interested in computer science years ago when I entered it in the first place, because amazing thing about computer science is that you can use it to create algorithms that represent things in the world that
don't exist yet things that you can only imagine. And so machine learning is sort of on this path right now, where we're getting very, very good at replicating the way the world is.
And we're going to be confronted with this question of how do we use these techniques to actually create new worlds that don't exist yet that are the way that we want them to be. And I think that this really going to require us to not just get models that are, you know, just really
good at imitating, but also models that are understandable to people so that we can change their rules inside.
And, and then use them to create things that that are based on imagination instead of, instead of just just the training data.
And so, so here's, here's a fun thing here I think if I want to be fair to the horses.
You notice that none of the horses in this horse generating GAN, get to wear hats, even though all the people get to wear hats. So we can change that by taking a hat from a person and inserting it into our GANs model of what a horse's head should look like.
And now horses get to wear hats. Right. And so, so let's build a better world.
And, and allow people to change the rules of the world by making the rules more visible and and manipulatable by humans. That's, that's sort of the goal of the whole thing.
So, any any questions, any questions.
I have a question. Yes, does this method work with multiple different models or is it only successful when, like, taking a hat from within this model and put it on a horn.
So right now this, this method is only able to take it.
It's, it's only able to rewire one model. So I can take one part of a model and rewire it to a different model you're sort of asking the transplant question.
And at the point where, you know, it's like a surgeon, I can like connect one blood vessel to another blood vessel in the same human right and you're sort of asking the question, well, can I do a heart transplant, can I take a heart out of one person put in another one.
And I cannot do that yet. It turns out to be harder.
But I, but it is a, it is an obvious goal. And I, and I feel confident that if we understand well enough, all the things that make these computations work, what is needed for the care and feeding of a computational module what is a computational module inside a big learning system.
Then we should, you know, it should be a goal to be able to move a piece of computation from one neural network to another one.
Does that make sense.
Thank you.
Yep.
That's a really great question, by the way, I think it's, I think it's fundamental.
Any other questions.
This is not too well articulated question.
I guess what you, what you order you thought about this.
I think this is this like neural nets have tend to see to like avoid the responsibility of the results like everything is done in the hidden layers and sort of shrug off shrug off the responsibility about the results.
I thought it was like interesting how you set the objective towards something as abstract as realistic and here like how you define the concept of being realistic is based on the big data you collected from the web but but oftentimes some like fake
images sometimes look even more realistic than real images.
I don't know like tree growing on top of the building may look fairly realistic for some people but maybe for plant experts, maybe it would not.
Right. So I don't know like, I think this might result in like the blurring between the making us hard to distinguish between the real and the fake or something like that.
I don't know.
Yes, yes.
So, so the, we're, we're unaccustomed to making it easy for making programs that make such realistic renderings of the world and it's actually a concern.
I think that, you know, people have misused this technology already that we, you know, we use, we, you know, there's the whole deep fakes phenomenon but even without like faking videos.
People, people have, you know, used face generators to make lots of fake Facebook profiles and things like that, you know, pretending there are millions of people that exists that don't actually exist and things like that.
So, so even before you sort of do manipulations of the world, I think that there's already this problem of, of, of, you know, pretending that there's a lot of data that there actually isn't by using these generator models.
And so I think that there's, you know, the whole, the whole question of fakes is a very serious question like how do we, how do we function society if we don't know what's real and what's fake.
Now, it's, it's not a totally new issue. You don't need a state of the art deep learning model to make fake, you know, people have made fake photo shops by hand forever people write can write text that has all sorts of lies forever.
In fact, that's probably more effective than you know, trying to train a deep learning model and, you know, sort of make it work.
But I think it's, I think it's a, you know, it's still an important question because the easier we make it to make fakes you start to get issues like a scalable fakes where it's not just one.
One photo that is a lie or one article is lie you could generate millions.
I think that there are serious issues with that so there's some pretty interesting work in forensics for detecting fakes and things like that.
That I think is important to invest in as well as we as we advance the state of the art and this kind of thing. So I so, so I don't want to minimize the implications of this type of thing.
I think that for the type of work that I'm doing I think that you observed that the tree kind of looks realistic it's not super realistic. You know, if you're a plant expert it's just sort of, you know, sort of there.
I think the same thing with hats they don't really super look like hats. And so I think that we're, we're sort of the stage where they're really exciting work, the implication of what I've done here I think is the idea that the, you know, learning
how these models are working inside by understanding what the internal structure of the models is is really the, the, the, the exciting part that that it's starting to give a little insight on how we might untangle and disassemble what the internal logic is that is being
being learned by these, these deep networks, and, and I'm actually, I feel like this is, I feel like there's a different issue other than fakes which is actually has some ethical implications, which is transparency of deep networks.
Because one thing that they're not really good at doing is when you have a deep network do something amazing. They're really not good at answering the question why, why did you do that. Why did you choose to render it this way why did you choose to pick these objects to put in the scene or
why did you choose to deny me some credit or to, you know, to make some other decision that we were at, you know, you know, depending on neural networks to do. And I think that if we can understand how to disassemble the rules that are being applied inside the network for to make this decision then I think that will, will be
we'll have a way of asking why, and by looking at the computation directly. So that's my, that's one of my goals and one of my hopes in doing this kind of work.
Definitely, I can see some of the worst of you about, like, about the transparency of the neural network, especially when you, when you show the example where you detected a single neuron that contributes to the watermark thing.
It was really interesting.
Yep.
Yeah, I think so too. I was surprised that it worked.
Because we normally think of neural X is very, very, very opaque.
I also have a small question regarding artifacts. So I think in the beginning you talked about how you segmented the network with like masks that were classified before by mapping neurons and beginning layers which create things.
But could like, couldn't that be also used to figure out where artifacts or anomalies are generated to make guns better.
Yeah, actually, I don't have a picture of it here. But in, in my work where I was looking for neurons.
It's called, the paper is called GAN dissection. You can, you can Google for it. And, and, and I, I showed that in that paper, we analyze some of the pre trained GANs that came from a previous work from NVIDIA called progressive GAN.
We analyze some of the pre trained models and we found that they actually are neurons that correlate with bad looking artifacts in a scene. And if you turn those neurons off, you can actually not only improve the quality of the output of the GAN, just qualitatively
like, you know, you can get these artifacts to, to not show up but using standard measures of GAN, you know, statistical measures of GAN image fidelity at large scale.
By removing these neurons, you can actually improve that what we call the FID scores of these GANs when when tested on like 50,000 images. And so, so that's actually very weird to me that's it was a big surprise, because, because we, we train these things using, you know, powerful
optimization techniques, using, you know, billions of floating point operations, you know, training these things on big expensive GPUs for a long period of time.
And the idea that a human can come along and do a simple looking visualization, pick out a few neurons based on things that don't look good.
And improve the model by check training those neurons off was like it shouldn't be possible right if it was so easy to improve the model that way, why couldn't the optimizer find it.
And so, so I think that was, that was, that was pretty interesting.
I, I have not repeated that experiment on the latest GANs which are actually much better the style GANs to architecture.
They went back and they analyzed a bunch of the artifacts that show up in this, this family of GANs. And they found that there are certain learning methods that they can do to remove the artifacts or reduce them somewhat.
And so I don't know if a human can still beat the current generation of GANs it'd be worth going back and seeing that phenomenon is still there.
That's pretty cool. Thank you.
Yep.
Hey, excellent. Thank you so much, David. It was really fascinating topic and talk, and more interesting to me, asking the right questions, asking questions and learning to ask the right questions.
It's really interesting. And I think that it opened path to many of us.
But hey, thank you for the opportunity to talk to the group here today. I always enjoy the, the chance to interact with folks about this. If anybody wants to send other questions about it, of course, you can always send me a note.
And, you know, I love this stuff. Yeah, definitely. I think that it would be great to follow your work on your eat hub and your website and especially for students who play with the tools that you have so they have them get an understanding of how these two work and make them curious about the work.
Cool.
Excellent. Thank you so much.
Thank you. Thank you everybody.
Thank you.
Fine.

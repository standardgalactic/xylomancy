in this episode.
There's a long history in 20th century
philosophy of science, of investigating
the possibility of something like an automated science
or a formalized science or a algorithmically implemented
science, all aspects of science, including sort
of uncovering theory.
And this has been a rich tradition,
but it's been strictly abstract about hypothetical machines
and hypothetical algorithms and hypothetical forms
of automation of scientific labor, right?
And all of a sudden, sort of perhaps largely
unanticipated by philosophers, we reach a point
where boom, deep learning revolution.
And suddenly, there is a kind of massive shift
towards taking aspects of scientific discovery
and passing them off to a intelligent computational
system, right?
And I think there's been kind of a tendency
to continue to view that within the lens of this automation
of scientific discovery debate as it's existed,
without really attending to the details of how
these technologies are, in fact, being implemented
in scientific practice.
And by the way, I think when what we are doing
is collecting data from some system in the world
and using some machine learning model
to extract statistical patterns from that system
in order to learn about that system, what we're doing
is effectively science.
It follows the model of science.
So in fact, most applications of machine learning
ought to be considered a kind of science-adjacent activity.
And in my opinion, it held to the standards of good science.
Hey, everyone.
Welcome to my conversation with the philosopher of science,
Mel Andrews, who's doing work in machine learning
and other adjacent fields such as the philosophy
of mathematics, epistemology, and of course, AI ethics.
It was a fantastic, cordial, and riveting conversation for me.
And I should first say, it's good to be back.
It's good to be back after my short holiday,
back doing podcasts, having these fascinating conversations
with these superlative guests.
And I couldn't think of a better person to start off
with other than Mel.
They is fascinating.
They also has this way of kind of elucidating certain points
that for a while, I did struggle to understand.
And of course, probably the best example would be this paper,
The Math is Not a Territory, Navigating the Free Energy
Principle.
As someone who's deeply interested in the kind
of philosophy of FEP, this paper really
helped me understand what the ontology of the free energy
principle is and what really is the proper way
to view it as a kind of a mathematical, formalistic,
conceptual framework that can be applied in many areas,
such as machine learning and, of course, cognitive science.
But we did start the podcast discussing Mel's recent paper
regarding the epistemic status of machine learning.
And they provocatively claimed that machine learning has
a pseudo-science problem, which I vehemently agree with,
and even kind of brought up this resurgence of physiognomy
kind of provocatively to make this point.
It's a preprint, in fact.
And we discussed a bit of that paper
and kind of extrapolated some of those ideas
into the broader sociopolitical discussion
around AI ethics, which, again, is very interesting.
And I do really appreciate that Mel
is a very critical, almost fecodial analysis
of these different disciplines that
are getting a lot of attention these days, such as AI
and machine learning, of course.
I also mentioned to Mel that this credit feed is a place
that I visit quite often online.
I hate Twitter.
I think Twitter is a hellscape.
However, there are a few accounts that I've bookmarked,
and I do frequently visit simply for entertainment,
but also to learn.
And Mel's Twitter feed is certainly
one of those that I've bookmarked.
Having said that, before I get carried away talking about Twitter
and all of that, a bit of a formal introduction
to Mel Andrews.
Mel Andrews is a philosopher of science,
working on the role of mathematical and computational
methods in science, and particular machine
learning-based methods.
Mel is currently a pre-doctoral research
associate at the Department of Machine Learning
at the Carnegie Mellon University
and doing a PhD in philosophy of science
at the University of Cincinnati.
They are also a visiting scholar at the Australian National
University and the University of Pittsburgh.
Having said that, here's my conversation with Mel Andrews.
Also, on a housekeeping note, I should mention that I'll leave
links to everything we've discussed in the show notes.
OK, now to the podcast.
The sort of conclusion I reached eventually was it just,
I mean, in academia, there's a certain portion of what
you do that feels like running on a wheel.
Like, it's not, it's a performance.
It's not actually to accomplish anything.
But it just feels like such a high percentage of that
in industry is just sort of, there's
so many levels of removal.
The person who's passing down the orders
is so many levels removed from the people actually
implementing solutions that you're almost
not accomplishing anything.
Yeah, also like, I mean, look, it's, I think, France Kafka.
He captured it best in his novel.
You know, it's Kafka-esque in the sense
that it seems like people are just doing things
for the sake of doing things without really going anywhere.
And you're having all these, you know,
wonky meetings.
But to be fair, though, like, I agree with you
because I'm still outside of academia.
I've kind of idealized academia.
But after talking to a lot of people like yourself
and on the podcast, I'm getting a more realistic picture
of what it is.
Because at the end of the day, the university is still
in our society.
It's a part of our culture.
And it's beholden to a capitalist economic system.
Exactly. Yeah, yeah.
And it kind of shows how much the, let's call it,
the capitalist tentacles reach into every corner
of our social existence.
I mean, although I even go a step further and I say,
I mean, I'm a big fan of Gizek.
It even changes our psyche, our subjective state in this world.
And, like, fundamentally, who we are as, let's say,
beings in this world, you know, and that, of course,
affects academics and, quote unquote, lay people
and everyone really.
But I see what you mean.
Yeah, yeah.
So I've got to ask now, though.
Sorry.
In terms of even, like, the AI ethics communities,
you have, like, the AI risk, AI safety people,
like existential risk, kind of the effective,
altruist-oriented community.
And you've got, then, you know, academic,
fairness, safety, regulation communities.
And they're both sort of pointing at each other
and accusing each other of, like, corporate capture.
But of course, the irony is that these communities
are both about as corporately captured as you can generally.
Yeah, yeah, yeah.
I mean, every entity really, you know,
neoliberal society is corporate captured in some sense.
You know, after the 70s and 80s, it fundamentally changed.
I completely agree with you.
No ethical work, no ethical consumption.
I mean, there's only really a handful of, I'd say,
at this point, as in some sense, I'm like,
ah, well, I am in your fight.
But I'd say there's only a handful of researchers
who I follow quite closely.
You certainly being one of them.
And then NS and a few others.
Because as in for me, there's this whole question
about AI ethics and, yeah, AI safety.
I only find it to be useful when we kind of go at it more
from the, let's call it the ontological level of,
like, what is AI?
What is machine learning as we have it now?
And kind of starting at, like, that very fundamental level,
ontological level, as you've done, you know,
in some of your papers.
Because for me, that gives a lot more,
I hate the use of stone because it's kind of cheap,
but like a realistic view of where we are with machine
learning and AI and where we can go.
Yeah, or even sticking to, you know,
sticking to the realm of not what is conceivable.
I mean, I think there's a lot of discourse
happening at the level of what is conceivable,
what sorts of technologies are conceivable,
or what sorts of technologies are metaphysically possible,
or what have you, and it's like, well,
let's think about what might come into being
in the reality we occupy.
Because what comes into being in the reality we occupy
is governed by market forces.
Correct.
And we should think about technologies
that some person might conceivably be incentivized
and capacitated to build.
If there are technologies that no one would ever
be incentivized and capacitated to build,
realistically speaking, I don't think
there's much point in debating what their capabilities are
or what the danger is emerging from these technologies would be.
And I think there's a lot of debate that's happening at the level
without considering, you know, incentives.
And truthfully, I think this is, again,
the primary sort of access of opposition
in the AI ethics communities or community
is between the risk safety people
and the sort of fairness responsible AI people,
fact community, et cetera.
And maybe the failing point of both of these communities
is, in their scholarship, a total failure,
at least in the major part of the work,
to really, really consider incentive structures.
So the interventions we're suggesting
have to be able to work within incentive structures
as they exist or they might conceivably exist.
They have to consider how we might,
if the point is to manipulate incentive structures,
we have to have that conversation explicitly.
How are we intending that the intervention we suggest
would nudge incentive structures as they exist?
And I think there's just sort of a failure
to consider that, in large part,
in a lot of the work falling under the ages of AI ethics
writ as largely and abstractly as we possibly can, right?
Undoubtedly, undoubtedly.
Yeah, I mean, it sounds like a very straightforward thing,
but AI doesn't sit in a silo.
It's always within a cycle.
It sits within a psychosocial reality.
I couldn't agree more.
Which is deeply, deeply complex to the point
that it having to think through incentive structures
feels like it makes any problem in this realm
completely intractable.
So I get exploits.
Yeah, although I want to add, which
is where I, again, like what you're doing,
and we'll certainly get to this in a bit,
kind of trying to understand, which is why, for me,
I think only a philosopher of science,
and perhaps even someone with a bit of a background
in sociology, can kind of explore
the kind of epistemic status of what is machine learning
or what is AI in contemporary times,
and then maybe speculate and theorize
on where it could go and develop in the future.
Because I'm going to be honest, as more of an outsider,
some of this sounds a lot like science fiction to me,
and I'm kind of like, but are we really even discussing
as to what the current models are?
What are large language models?
Like what do they really do without all of this speculation?
Which, again, the speculation seems
just like it's just a bunch of blokes having fun
without considering all the factors that you mentioned.
The our psychosocial reality, ways, AI, incentive structure,
kind of the symbolic network that all comes together,
which it seems like an intractable problem, I agree.
Yeah.
But I mean, I think a lot of philosophy
is self-gratifying, in a sense.
Oh, for sure.
Oh, my God.
I mean, I would say a lot of thinking in general.
Yeah, yeah.
I'd say a big part of it is for that,
because it's self-gratifying, not for any ethical.
Yeah.
I mean, I think I have a very sort
of like pragmatic embodied view of thinking.
And I sort of a very, I think that goes into a sort
of pragmatic ethical or political stance as well,
where I think, I mean, I think we think in order
to affect action in the world.
Thinking, I don't think thought takes place.
I don't think we can say cognition has really taken place
unless we see the hallmarks of it in behavior.
So I mean, I actually think I'm a sort of weird behaviorist,
in the sense that I think that consciousness and sentience
and agency and cognition and a lot of sort
of what's been considered unobservable mental characteristics
are actually readable from behavioral dynamics.
And I think that we should not be thinking of philosophy
as an Iowa pursuit.
It impacts the way we go about the world,
or it ought to be approached as though it's
going to have immediate impact on how we interact
with the phenomena it treats as its subject matter.
I'm just saying I'm fully a pragmatist with respect.
Yeah, I get you.
But also, I mean, not only as a pragmatist,
but I would even say, I've been deeply interested
in psychoanalysis, and I've spoken to heaps of psychoanalysts.
And one misunderstanding I think a lot of people
have coming from like the Carl Jung type is like the idea
of depth psychology, where they think the truth of a person's
desires and motives are somewhere deep within,
and you need like a psychoanalyst to find it.
But the flip is the Freudian more Lacanian idea is no,
no, no, that the truth lies in your actions.
The truth is never like deep within.
Rather, it's very conspicuous.
It's out there, and you can see it in how people act,
and how people engage in their social world.
So I can say nothing is really hidden with that.
Yeah, that's the ultimate.
You come to realize nothing is really hidden.
Yeah, yeah.
So you wrote, you've been working on a few papers,
kind of working on the epistemic status of machine learning,
which I've thoroughly enjoyed reading and learned a lot.
One of them was this paper I read.
This was I read this a while back, which is the machine
learning and the theory-free ideal.
I'll leave a link to that in the show notes.
But also the recent preprint you shared with me,
which I thought was a very good provocative piece, in fact,
titled Ghosts in the Machine Learning,
the Reanimation of PseudoScience, and its ethical repercussions.
So to perhaps to orient the listeners,
if you could kind of give us an introduction as to, yeah,
what is this work you're doing, researching the epistemic status
of machine learning, apropos philosophy of science,
and then perhaps, as carrying on from our AI ethics
conversation or chat, how those two ideas are connected,
the epistemic status of machine learning,
and then what's its relationship to AI ethics?
Yeah.
So there's a long history in 20th century philosophy of science
of investigating the possibility of something
like an automated science or a formalized science
or a algorithmically implemented science,
all aspects of science, including sort of uncovering theory.
And this has been a rich tradition,
but it's been strictly abstract about hypothetical machines
and hypothetical algorithms and hypothetical forms
of automation of scientific labor, right?
And all of a sudden, sort of perhaps largely
unanticipated by philosophers, we reach a point
where boom, deep learning revolution.
And suddenly, there is a kind of massive shift
towards taking aspects of scientific discovery
and passing them off to a intelligent computational
system, right?
And I think there's been kind of a tendency
to continue to view that within the lens of this automation
of scientific discovery debate as it's existed,
without really attending to the details of how these technologies
are, in fact, being implemented in scientific practice.
And by the way, I think when what we are doing
is collecting data from some system in the world
and using some machine learning model
to extract statistical patterns from that system
in order to learn about that system,
what we're doing is effectively science.
It follows the model of science.
So in fact, most applications of machine learning
ought to be considered a kind of science-adjacent activity.
And in my opinion, it held to the standards of good science.
Now, I think that there's a lot of good work being done
with machine learning and science.
There's a lot of really epistendically careful work.
And I think there's also, like with most things,
a glut of garbage.
Pick a genre of anything.
I don't care if it's films or psychological studies
or books on the history of hip-hop or like whatever it is.
Or perhaps even people, for that matter.
Yeah, most of it's crap.
Yeah.
And there's a very small percentage of it that's quite good.
Machine learning is no different.
There's a lot of good work being done
and 1,000-fold more bad work being done.
The issue is that it's being adopted so rapidly.
These methods are being adopted so rapidly
in so many contexts across society.
Most people don't understand how the technology works.
There is a lot of AI hype.
As long as there has been AI, there has been AI hype.
And what hype is, I think it should be made explicit,
that what hype is, is it's a targeted disinformation campaign.
It is.
Yeah, perhaps you could elaborate on what you mean by that.
Yeah, so AI hype refers to people when Sam Altman says,
we'll have AGI by X or when Sam Altman says,
well, GPT-3 could take a 20-minute activity
and reduce it to a five-minute activity,
but GPT-next will reduce a week-long activity
to five minutes or something like that.
That's AI hype.
Now, I saw a recent statement by, I believe, anyway,
I don't want to mention any names that the next GPT model
will be able to replace PhD researchers.
And we won't need any PhD researchers anymore.
So yeah, I'm in shock.
That's AI hype.
Scientists and science popularizers
saying the theorist will be replaced,
the physicist will be replaced, the doctor will be replaced.
You know, in medical context, saying, oh, we won't need,
you know, we won't need secretaries,
or we won't need nurses anymore.
Any of that drama.
But it also includes the doomerism about AI.
Like, soon AI will surpass human intelligence,
and it will, you know, hurt us somehow.
Yeah, that's the part which I, for me,
at least feels a lot like science fiction,
if I'm being honest.
The doomer.
Yeah.
There's something even, it's interesting to even say.
Oh, that's science fiction.
I mean.
Yeah.
And I have nothing against, I love science fiction,
but it's got its own status in our dialogue, right?
I mean, science fiction, it's called fiction.
But saying that's some particular tech breakthrough
is going to replace cardiologists even is absolutely.
Will it, you know, have some impact
on the role of cardiologists in specifically
how they approach analyzing the results of imaging, right?
Totally.
Is it going to replace cardiologists?
No, that's science fiction, right?
And it probably should, given all technology effects,
how we do our jobs, how we live our lives.
And that's completely fine.
It probably should affect how they do their job.
If it works, it should.
That's that's true.
It doesn't work if it can't.
But if it works, it should have an impact.
It's just that there's, in anything that's marketable,
there's an incentive to misrepresent what it does.
This is true of, you can pull out a magazine from 1952
and how it advertises some cooking implement
to stay-at-home moms.
And it misrepresent, you know, it's like,
this is a life changing.
It's going to lie to you about what the thing does, right?
But with AI specifically, you know,
I think there's to some extent, even the average housewife
in the 1950s, it was like, yeah, probably,
probably not all of this about the oven
is actually the ground truth, you know?
Whereas with AI, people seem to be
willing to believe really radically untrue things,
you know, just things that are actually
to anyone in the know, blatant lies.
But there is a culture of really leaning
into highly fabulous lies, propagating them to no end.
And no one seems to be doing fact checking.
And the worst part to me is, I would think that's in,
so I'm coming from this position of history
and philosophy of science, where it's like,
we've studied hundreds and hundreds of years
of science and technology and how people misrepresent
what it does and what it actually does, right?
So we would, I would hope that we would have
a critical lens on this.
And it strikes me that there's a lot of philosophy
that's simply parroting or lending kind
of philosophical justification to these hype narratives
that, you know, deep learning will radically change
the face of particle physics or something like that.
It's like, really?
Really, though?
Yeah.
OK, perhaps this is a good place to go to.
So one term, which I really liked in the paper,
which you used was this idea of the theory-free ideal.
I really liked that term, because I
think for me that captures what apropos philosophy of science
or let's say apropos the scientific method, what
the hope or the dream with ML models are.
So if you could, again, to flesh this out a bit more,
as to where we are right now with the current paradigm
of machine learning, what is the epistemic status
of machine learning?
And then could you then probably connect that
to what is this theory-free ideal?
And then as you point out in your papers,
what are the mistakes that people make
by having this ideal of science being theory-free when done
with a, let's say, an ML model, for instance?
Yeah, so I think machine learning models
are statistical models that are computationally instantiated.
There's nothing that would, in principle,
make the epistemic status of these technologies any different
from any kind of other statistical model.
Now, you're pushing yourself into really high-dimensional spaces
in which data is transposed, and so there's
intrinsically the dimensionality of the patterns you're finding
is much higher than what you're doing with classical statistics.
But I guess I doubt that when people are doing multiple regression,
they are kind of holding all the dimensions of the data.
Holding all the dimensions in their head in the way
that they would have to be in order for the contrast that's
typically drawn between deep learning and classical statistics
to make sense.
Like, there's meant to be a kind of opacity,
a kind of intrinsic, deep, unknowability of the kinds
of patterns that these statistical methods, that
is deep learning methods, are finding relative
to classical statistical methods.
And I just don't see that distinction
being substantive and absolute in the way that it's proposed to be.
I think these are, they're hard statistical methods
like other statistical methods.
And if there's a difference, it's a sociological difference.
OK, I think I've followed you all the way except the last bit,
the sociological, but if you could probably flesh that out.
Yeah, so it's the corporatization of these technologies.
I see, OK.
It's the hype narrative.
So even in a research context, there's because machine learning,
I mean, because machine learning is this place where stats met up
with AI, and AI is something that's always,
AI refers to a lot of different research traditions
that have had historically very little to do with each other
in terms of their substance, in terms of their subject matter,
in terms of the methods.
They mostly have to do with where funding is being targeted
and the kinds of narratives spun around these research methods.
So there's very little substantive that
holds everything that's historically been called AI,
going back to cybernetics, going back to McCarthy, going back to,
you know, old-fashioned pits.
And, you know, like, going back all through the history
of things being called artificial intelligence,
there's very little that connects all of these,
but who they're targeting for funding and the kinds of narratives
they're using in convincing the public and funding bodies
of what they're doing.
So there are methods and statistics that have been,
you know, approaching something like machine learning,
going back 80s, 90s, whatever, right?
But where that meets up with the AI narrative,
you get this hype and disinformation
and overselling of competence, right?
And so there's an attitude and a meta-narrative
surrounding machine learning that is, I think,
more what sets it apart from classical statistics
than anything else.
This is a spicy take, you know, kind of with a grain of salt.
But this is my, my challenge is really like,
tell me what is so radically, epistemically different
about these technologies that they should be placed
in some category that's discrete from classical statistics.
I have not seen it, you know?
And you point this out, yeah, and also,
I guess it's this meta-narrative that drives,
it's the impetus behind this theory-free ideal
when it comes to science.
Yeah, and there's been, so philosophers,
natural philosophers, scientists have debated
what theory is and what its proper role in science
is since the insipience of what we call modern science,
right, since Bacon and Newton and Gallaudet,
you know, modern science, right?
Even going back to Bacon, there's a kind of push
for a kind of radical empiricism
that pushes away as much as possible the role of theory
about just tabulating as much data as possible
and sifting through it for patterns
and not bringing our kind of conceptual infrastructure
to bear on it.
But then since Hume, since Hume brought his sort of problem
of induction to the table in epistemology,
there's this widespread recognition
that, well, all knowledge of the natural world
is knowledge by induction.
You do not get deductive certainty
about empirical matters.
Yeah, so for instance, the sunrising.
Yeah, so the typical example is the sunrising
just because it rose yesterday.
We can't say necessarily it'll-
Every day of our lives, the sun has risen in the morning.
Yeah, yeah.
And yet give me a deductive proof,
give me logical certainty that it will rise tomorrow.
There is none.
There's only the remit of our experience to tell us
that it will rise tomorrow.
There's no logical necessity
that it will rise again tomorrow.
So knowledge of nature, scientific knowledge is
and all our kind of day-to-day practical knowledge,
like I can eat this bread and it won't poison me
because the bread I ate yesterday
that I got from the same baker didn't poison me.
This is inductive knowledge,
which means we don't get deductive certainty.
And it also means that to get that kind of knowledge,
you need to start off with rich conceptual infrastructure,
which I'm calling theory.
I think when, so I think there are lots of ways
that philosophers have traditionally
cashed out what we mean by theory.
I think that when we talk about theory-free science,
we mean a powerful influence at the beginning of inquiry,
at the beginning of the investigatory procedure
of our prior conceptual resources,
our prior conceptual acquaintance
with the target phenomena, right?
We do not get inductive inference off the ground
without bringing to bear prior theory or conceptual.
Philosopher and historian of science, John Norton calls it,
bring to bear material facts, right?
But there are lots of ways of putting it,
but you need theory to get empirical knowledge
off the ground.
And so in this sense, you cannot have theory-free knowledge
of natural systems.
And I think that you can look back for hundreds of years,
and there's always this dialogue between,
no, we should get rid of as much as much as we can push away
the influence of prior conceptualization,
we should do that, and that's scientific objectivity.
This is, I think, one notion of scientific objectivity,
right, that has been kind of implicitly in the background
of a lot of discourse in philosophy,
natural philosophy, science for hundreds of years.
And then another stream that says,
well, you can't actually have knowledge of nature
without bringing conceptual resources to bear.
So it's about documenting them, it's about recognizing them,
it's about, to some extent, working backwards
from those assumptions and saying,
which of those assumptions are, in fact,
substantiated by what we've then been able to do
from what we've measured or observed in nature, right?
And what is, in fact, just arbitrary or unknown, right?
And I think since the rise of domain generic statistical methods
in the 20th century, in particular,
stats really gets off the ground
after the eximatization of probability theory
with Komagorov, statistical reasoning probably
statistical reasoning, probabilistic reasoning is,
to the extent that's so widespread in science,
it's relatively new, it's really a kind of 20th century,
like statistical reasoning is kind of a 20th century thing.
I mean, it sort of got off the ground with, you know,
gambling and stuff in the 17th century,
but, you know, as a scientific method, it's new.
And really, I think, since the mid-20th century,
you get a lot of this, what I call a theory free ideal.
And it's in the kind of fundamental,
I don't know if I believe in this designation,
but in the more fundamental sciences
who know how to theorize
because they've been doing it for hundreds of years
and know how to mathematically represent their phenomena
because they've been doing it for hundreds of years,
you get less of this,
but in the younger sciences,
like the quantitative social sciences,
like social psychology, like population genetics,
what have you, economics,
there's this belief that the more things
are theory-free, the more data-driven the methods are,
the more objective they are and the more science-y.
Yeah, so when you mean the more fundamental,
you mean like physics, for instance, right?
Yeah, areas of, but not all areas of physics, right?
The areas of physics that are really established.
Yeah, although this beautifully connects
with your sociological point,
because I'm sure you're aware of the whole
Bo Einstein debate and the, you know, shut up and calculate,
there's like a lot of in the history of 20th century physics,
it is the idea that even within physics,
you shouldn't ask, but given that physics
is the foundational discipline,
the ontology of the physical world,
there was a time, especially because of, you know,
World War II and the nukes and all that,
just don't ask the ontological questions,
just shut up and calculate,
that there's a ruthless pragmatist
and ruthless just create.
And so physics kind of sometimes,
and again, I again say this as a bit of an outsider,
it becomes a bit more like engineering
over a foundational discipline where you're asking,
well, what exists, what is reality?
That was sort of the Los Alamos attitude, right?
As in the idea that physics is engineering?
The thought I've been calculating, attitudes.
Yeah, yeah, I mean, I know more from the Copenhagen school,
I mean, I thought that I think Tim Modellin,
he speaks quite well about that.
And just, it's like a interesting peculiarity
in the history of physics,
especially because if you look at the big figures,
like the Einstein's or like the Maxwell's,
they were deeply interested
in these philosophical questions,
it's like, what exists, you know, it's not...
Einstein was a philosopher.
He certainly was, he certainly was, you know,
without a doubt, yeah.
I mean, probably after Newton,
it's probably the quintessential natural philosopher.
I couldn't agree more.
And yeah, so I mean, just on this note,
I couldn't read the paper too carefully,
but I do love the, perhaps it's worth mentioning,
because it's rather provocative and I like that a bit,
where you say, yeah, machine learning
has a pseudoscience problem,
and then you bring up the,
well, you and the other writers,
Andrew and Beiber, is it?
Yeah, bring up Beiber.
Yeah, yeah.
They bring up the idea of how there's a resurgence
of physiognomy in kind of these ML communities.
So just, if you could just humor me with that for a bit,
just kind of what all that's about,
why you claim, you know,
machine learning has a pseudoscience problem,
which I think you already kind of did outline
quite in detail,
but then this little example you use on physiognomy.
Yeah, well, part of it is this sort of runaway idea
that we can do science without theory
that picks up steam in the mid-century,
and then with the rise of machine learning techniques,
and these being adopted widely,
it just becomes, it becomes,
it gets bundled into this hype narrative
about how these technologies work,
and then everyone sort of believes
that these technologies are capable
of extracting true knowledge of some natural system
in virtue of having achieved high classifier accuracy
on some natural data set, right?
And what's actually happening is researchers
are interpreting that pattern
as having discovered precisely whatever
their intuitive idea of what they were gonna discover
was beforehand, because if they're not explicitly
doing the theory, if they're not explicitly theorizing,
then they're implicitly theorizing,
which means that they're effectively trying to con you
into believing whatever their intuitions were
at the start of the research procedure
without effectively having furnished evidence of that,
besides having told you that they trained some model
and there's some pattern in the data
that satisfied some criteria, right, for success.
But I think because of all these hype narratives
surrounding machine learning,
and again, not for substantive epistemic differences
in how these statistical methods work,
but rather for sociological reasons,
you get a lot of bad, bad, bad science.
So if I want to do a quantitative social science study
and I'm in a sociology department
or an economics department,
my advisors won't let me do that until I've read up
on the 100 years long history of scientists in my field
having studied that exact problem, right.
Machine learners on the other hand,
machine learners don't even read
the history of their own work.
Machine, like, it is not typical for someone
in machine learning to have even a five years deep
understanding of the history of their own field, right.
Which is, there are of course exceptions,
but the general rule is people in machine learning
do not read.
Yeah, and so, but you're talking,
are you talking about more on the scientific side
or do you just mean general, commercial ML engineers?
Practitioners, but also in academics.
Academics, okay, that's unfortunate, yeah.
And so when you go to apply ML,
everything that's submitted to Art Tripoli
or NeurIPS or what have you,
ACM, you get just this glut of work
of people applying the methods of ML, especially DL,
to some problem that scientists have spent
maybe hundreds of years working on
and there's no acknowledgement
that what they're tackling,
what they're attempting to tackle is a scientific problem
that some very specific field of molecular biomechanicists
or whatever the field is, right,
have been working on for hundreds of years.
And then there's this attitude that's,
while deep learning will solve the problem
and I don't have to pay my dues
and read about the methods in this field.
And then the reviewing practices at,
well, part of it's like we got rid of,
we got rid of traditional peer review and machine learning,
which is like, was traditional peer review hopelessly broken?
Yes, did we introduce new problems
by getting rid of it wholesale?
Also, yes, right?
And so then you've got,
so there aren't standard journals in machine learning
the way they are in biomechanics or biochemistry
or socioeconomics or whatever, right?
You are submitting to the big name
machine learning conferences, but peer review there is,
I mean, I have to say it's pretty radically incumbent.
I don't know that,
I don't think anyone who reviews for
or submits to machine learning venues would,
would try to fight me on that.
I mean, I think the consensus is that
the peer review process for these venues
is wildly inadequate.
Inadequate, yeah.
And it's because when you apply machine learning,
right, you're applying it to some domain,
you're applying it to some domain
where there is a vast history
of people trying to solve some problem.
And when you submit your little deep learning thing
to IEEE and you're trying to tackle some problem
in social science,
they're not asking social scientists to review that,
God know, right?
They're asking other people who trained a transformer
to, with data of that shape, right?
But they're not asking people
who have the disciplinary knowledge
to review the methods for what actually matters
to doing science, right?
Yeah, yeah.
And when do you think this changed?
Well, is this imminent to the practice itself
or when do you think this change took place
where the peer reviewing method became a bit lax
or inadequate as you pointed out?
Well,
it was just sort of an organic thing, right?
You have on the one hand,
I mean, it was never the case that there were
people applying machine learning,
as far as I'm aware,
it was never the case that people were applying machine learning
to some problem in biology
and then submitting that to a biojournal.
That's not the, I mean, occasionally that happens, right?
But that's not standard practice
and I don't think it ever was.
It was also not the case
that there were kind of standard journals,
there were standard journals for stats, right?
But there were never kind of standardized
deep learning journals, right?
There were computing or stats conferences
that got kind of evolved into ML specific conferences
or new ML specific conferences emerged.
And like, a lot of the main ones are actually,
they didn't start out as ML conferences
and they evolved to be ML conferences,
but also you have at the same time
the emergence of pre-printing servers.
And so you get the emergence of a new kind of,
like machine learning has been machine learning
the methods that we call machine learning
have been around since like the 80s, right?
You could trace it back earlier
to kind of proto machine learning methods.
Those go back much, I mean, again,
like it was out of World War II,
it was out of the research at Los Alamos
that you got like MCMC sampling, right?
Like Mark Alchein, Monte Carlo sampling,
like Metropolis Hastings sampling out, right?
I didn't know that, yeah.
Yeah, yeah, it goes back to like the early 50s,
late 40s, early 50s.
So even before-
That's kind of machine learning, right?
Yeah, yeah.
But, you know, machine learning as a,
it's the early 2000s that machine learning
kind of goes like, hey, we're a scientific field
or hey, we're an engineering discipline,
like we're a discipline now, right?
And it's at the same time
that you're getting pre-printing servers
as the sort of way that stuff is disseminated.
So there's, effectively, there's no incentive
to start journals and there's incentive
against starting journals, I would say.
This is my, I'm making this up on the spot,
but that's kind of how I would reconstruct that history.
No, that makes sense, yeah.
I mean, it's partially contingent,
it's just historically how things have-
Yeah, and also there's this widespread recognition
that there's something deeply broken
about traditional peer review, which is true.
Which, we're sure that I believe all,
every academic I've spoken to has said that.
So unequivocally, I think it's just a general consensus.
Yeah, yeah, excellent, excellent.
Now, that's great, Mel.
I wanna be cognizant of the time,
which is why I wanna get to this,
and I'm sorry if this sounds like I'm flattering you,
but your paper on the free energy principle,
it is, I've probably read it like three or four times
and I think I can probably parrot out certain parts of it,
I've obeyed them because I've read it so many times,
especially because-
Yeah, I hate it, it's actually, it's a really good paper.
It's a fantastic paper, it's a fantastic-
And so now I'm trying to like, I'm like, okay,
what, you know, I'm trying to write a subsequent paper
and it's like, it's not that good.
I mean, because like, it's probably
your most cited paper, right?
I mean, I found you through this, in fact,
like I didn't even know who you are
until I came across this work.
Yeah, so just for the listener,
it's called the math is not the territory,
navigating the free energy principle.
Yeah, I mean, it's just, it's philosophically interesting.
It's got so much into like the history of science
and then, you know, like, what is formalization?
You speak about the structure like,
yeah, he keeps a lot to discuss here.
Although, although before we get to the,
let's say the nitty-gritty and one thing I want to mention
is, so I've been trying to get through this,
this book on active inference.
And I've got to say, because when I'm reading this,
every page, I'm kind of reading it in a way
through the kind of the lens of what you put in me
through this paper, you know?
It's like, I've got a bias now.
I've got the math is not the territory bias.
Because I've got to actually do it.
It really helped me understand,
yeah, it really did help me understand the ontology
of what really is the free energy principle,
because it's got so much interest.
So many people talk about it.
And, you know, Carl Friston, he's fantastic.
I've learned so much, but he sometimes
isn't the best elucidator, you know?
Like he, when it comes to a, you know,
he's not a philosopher.
Correct. I think that could be the reason.
Einstein was a philosopher.
Carl Friston is like Isaac Newton in that he's kind of like,
I don't care what's like, he'll assent to any metaphysics.
Newton would have sent to anyone.
Newton was like, I'm not doing metaphysics.
I'm associating relationships I see in data.
Right. But don't tell me about the physical seat of gravity.
I'm not talking about that.
Yeah. Oh yeah.
I mean, that certainly wasn't a dig at Professor Friston,
because he himself says, he says, yeah,
I'm not a philosopher, I'm a scientist.
And I don't really even, he doesn't even really take
a philosophical position or metaphysical position
pertaining to the FEP.
But, but having said that, Mel, so,
oh, sorry, be that as it may, I mean, what, what, yeah,
why do you think there's such deep philosophical interest
in the free energy principle of like every,
any philosopher who works in the philosophy of biology
or the cognitive scientist or ML,
but it's such deep philosophical interest.
So, yeah, why do you think the reason for that is?
I think there are a lot of reasons.
One of them is,
okay, so I think, I think of all math,
all applied mathematics or scientific math or models
as a kind of thinking tool.
Math is a thinking tool for science, right?
But the free energy principle is a thinking tool
in a different way in that it's not actually meant
to be placed in contact with empirical data, right?
It's just about enabling us to conceptualize
of target phenomena in new ways.
And it happens to be new ways
that are actually really philosophically generative
and novel to suck degree.
Not exactly novel, but overlooked.
I mean, you get some of like,
some of what's being said about life.
It's, if you look hard enough,
it's really in Schrodinger's what is life.
You look at that text,
there's a lot of the ideas that are being brought out
in the FEP in that text originally.
Yeah, he brings up new ideas.
One of the ideas go back, but are largely ignored
because they're very much opposed
to the kind of neo-Darwinian canon
that we have now in biology,
where we're really looking at population level analysis
and you're not looking at physical exigencies
or structural exigencies of biology
at the physical systems involved in biology
and what kinds of necessities need to be there
for life to exist.
And then connecting that up to a view of cognition
as fundamentally oriented towards action
and interaction with the environment too.
I mean, that's also there.
So there's a lot that's philosophically rich
that is associated with the FEP
in how the FEP is discussed.
It's not necessitated by the FEP, the FEP is just math,
but it's math that allows us to conceptualize
of things that we're taking away.
It's also cool math, it's fun math.
Part of it is there's a lot of flourished that math
that doesn't need to be there.
That's just it, just if you like math, it's cool.
And you can keep pushing it, new cooler.
It's kind of, it's like this magpied,
you just, it's like a bunch of shiny math taken
from 18 difference distinct field.
You know, you've got some of it's machine learning math.
Like some of it just is, it just is elbow, right?
To minimize free energy, the quantity known as free energy
as a kind of information theoretic construct
to minimize free energy is to just optimize
the evidence lower bound, which is elbow,
which is machine learning technique, right?
It's also Fokker-Plunk or the master equation
or the Kolmogorov forward equation,
which is, you know, a principle of stat mech.
It's also, I connected it up to Max Ent,
maximum entropy principle, which is a sort of
James put forward this idea that we can view,
basically the core principles of thermodynamics
expressed statistically can also be reoriented
as a kind of
epistemic principle for like keeping your priors flat, basically,
except for when the evidence, does that make sense?
No, I don't think I got the latter bit of it
regarding the epistemic principles, yeah.
Yeah, so James in the, is this 50s?
Max Ent, James, when's the first Max Ent paper?
50s, right?
Yeah, it's a 50, it's 57, it's 57.
Yeah, so the idea is that-
Max Ent, yeah.
Yeah, 57, that's right, yeah, yeah,
BT James, two papers, anything like that.
So it's like a closed thermodynamic system.
The principles of thermodynamics tell us
that a closed thermodynamic system
will max out its entropy, right?
Like that's, this is what thermodynamics tells us.
At a second point, yeah, of course.
Right, ultimately, in the limit.
You can give this a gloss as a principle for best inference.
Like it's already probabilistically formulated,
but then view that as a rule for governing
the probability distribution over some belief.
And it's again, Max Ent, James,
Max Ent, because your priors need to be as flat
as they, the probability distribution is as flat
as it can be, given the evidence, right?
So there's-
And you're saying the FAP does bring this into its,
it's theorizing too, like-
But they did, I don't think they did.
I think I pointed it out, and then they started to,
so there's, their recent work is actually explicitly
incorporating the James stuff.
Okay, fascinating.
Yeah.
I think it was actually, because I was like,
so James, right?
That's fascinating, I didn't know that,
cause I mean, I mostly view it through,
you know, predictive coding and Bayesian reasoning.
I mean, really, that's-
I actually doubted that lineage.
Oh, okay.
I actually doubted that lineage,
but I thought that they've made that connection
explicitly beforehand, but the people who are now
explicitly doing the Max Ent incorporating with FAP
are attributing it to me.
So I'm like, you know, it's if he wants-
Okay.
Sure, I'll take it.
Are there any, cause like, I mean,
I'd love to read up on this.
Are there any papers?
Cause I mean, I've only just started on the book.
This is like-
I think the best, okay, so the bit,
Tom Parr is fucking excellent, but-
And Joe-
All of those people who wrote that book are excellent,
but I think-
They're very good writers too, yeah.
I think the most competent mathematician
who's working within the free energy framework
is Dalton Saktivarivo.
Okay.
I gotta, I don't know who I spell that, Dalton-
Do you know how to spell Tamil names?
Yeah.
Dalton.
The first name's very Scottish, which is-
Dalton, okay, there we go.
Yeah.
So actually, they took it from,
so they took the max and like,
so I proposed like, hey, you know,
well, what the FEP is kind of is,
you're taking principles of statistical mechanics
and you're epistemicizing them.
You're giving them an inferential reading.
You're taking laws of stats and statement.
Yeah, yeah.
Reading them as principles for-
Cognition.
Yeah.
Inferencing.
Yeah.
I mean, I'd say for me initially,
when I came across the FEP,
that's what I found quite interesting
where, you know, Carthus and his background's
even in physics and like you take
from statistical mechanics, which is in physics.
I think he did a bachelor's in physics in like the 60s.
Yeah.
I shouldn't say a background, you're right.
I think he studied physics.
He certainly is a neuroscientist, but like,
I think he does-
Because like the physics he's getting is largely-
I've dated, oh.
Well, I think he's just drawing
from a lot of areas of physics.
And really it's like he's in clinical neuro, right?
He's spent a career in clinical neuroscience.
Well, I mean, generally what interested me
was that the fact that you take these theories
and principles from physics, like,
you know, all from statistical mechanics
or as you point out, like, you know, Max and
and then apply them to move to like cognition,
reasoning, Bayesian inference and the likes.
I just, I don't know, I find that fascinating.
And I don't know, I just feel like I, again,
as a neophyte, it just excites me to see where
the developments that goes on the FEP
and the, you know, the concomitant,
more engineering work that comes along with it.
Having said that, Mel, one thing is this,
in your paper, you say the math is not the territory.
And I think here's where we get to the, really,
the crux of the argument, the philosophy of this paper.
And I'm just gonna read out a bit of an excerpt.
I think it's valuable for the listener
in case they haven't read it already,
which I, if you're interested in the FEP
or ML for that matter, I highly recommend reading this.
So you just, you claim here,
I think this is from the abstract, I'm not sure.
Anyway, I've got this excerpt here.
Conceptual reification is a common ailment
of scientific modeling.
It is particularly likely to occur in cases
in which models have somewhat convoluted histories.
Reification involves, and here's the important bit, in fact,
reification involves mistaking an aspect of a model,
its structure, its construal or the union of both
for an aspect of the empirical of the natural world,
mistaking the math for the territory, so to speak.
So yeah, could you please elaborate on that little statement
and then connect that to the FEP?
Yeah, so I think there's a tendency
to conflate scientific realism
with realism about the conceptual tools
that we use in science.
So I'm a full-blooded scientific realist.
I think, albeit a pragmatic realist.
Yeah, I'm somewhat of a content.
I think I'm more of a realist than most people
who call themselves pragmatic realists.
I think people who call themselves pragmatic realists,
I'm like, you're not really a realist.
I'm actually a realist, just pragmatically so.
No, I mean, I would say science without a doubt,
it does give us truths about reality.
I mean, I think I'm a realist in the Kantian sense,
let's say.
Yeah, whatever the truth knowledge,
whatever the epistemic goods of science are,
I wanna say it's truth,
sub-pragmatic or knowledge, sub-pragmatic, right?
I think there's, it's not absolute
all-encompassing omniscient truth about nature.
It's pragmatic truth, it has its limits.
And it's oriented towards us as the kinds of beings
that we are having to occupy the world
and navigate it the way that we do, right?
But I think it is true.
I mean, if anything is truth or knowledge,
that's what that is.
Exactly, with all those caveats,
it gives us truths, let's say.
Yeah, yeah, yeah.
But then we introduce all this conceptual machinery
into science to be able to do science.
Unlike the idea of a Hamiltonian
or the idea of a gravitational field
or the idea of a fermion
or the idea of, you know, a force function.
And that includes all the, you know,
the Fokker-Planck equation
that includes all the sort of mathematical infrastructure too.
These are all the conceptual tools of science.
And,
I think that there's a lot of slippage
that's like, well, science delivers truth.
Therefore, all the conceptual tools we introduce
in the process of doing science and getting to that truth
are also true and real in some sense.
And I'm like, no, that requires a lot of careful work
at the end of the day.
At the final kind of interpretive stage
of a scientific procedure, you ask yourself,
do we think strings are real?
Do we think that quantum fields are real?
Do we think that Hamiltonians are real?
Or was this just sort of a calculational device?
And I think there's a lot of in between,
between it's real and it's a calculational device.
But I don't think at the end of the day
that's a distinction that can be fully upheld.
But there's a tendency,
and I think it's true even among careful philosophers
to attribute realism or truth to try to make
the conceptual tools of science truth apt
when they ought not to be read as truth apt.
Yeah, when it's a thing itself.
I mean, it depends, right?
For instance, I've spoken to a few mathematicians
on this podcast, one person who I,
that comes to mind is Jolder with Hamkins.
He's a Platonist.
So for him, of course, yes, FEP,
if it's a mathematical structure,
it could really exist in like a platonic sense.
But in your paper, you're talking more in regards
to our physical world, what science studies.
You're not talking about like a platonic realm.
I mean, I don't know what your views are on Platonism.
With math, I think there's a lot of accounts
of how math works in science that are,
whether explicitly or not,
deeply committed to Platonism or something akin to it.
And for me, I'm like, if your account
of how natural science works depends on assuming
the most kind of industrial strength metaphysics possible.
It's not a good account of natural science.
Like I believe in naturalism.
I'm a naturalist.
And a lot of naturalists,
a lot of proclaimed naturalists happen to also be Platonists.
And I'm like, that's not real naturalism, right?
You need an account of science that assumes as little
in the way of metaphysics as possible.
And is cognizant of what metaphysics it does assume
to the extent that you can't get away from that?
Yeah, that's why you mentioned all the presuppositions
that your realism is based on, right?
Right, yeah.
For me, it's like the word dog or the word mitochondrion,
right?
Do we think the word dog exists?
It's like,
I think dogs exist.
Yeah, yeah.
I think that there's a class of natural entities
that is well-carved out by our best science.
And we refer to it via this concept
that has this label attached called dog.
Yeah.
But I don't think, I think the word exists
as a cultural mental artifact,
but I don't think it exists mind-independently.
And I feel the same about math and science.
It's like...
Yeah, sort of interrupt math,
but I think I agree with you.
I mean, I'm still...
I hate to use this like cheap agnosticism,
but I'm kind of agnostic as to mathematical Platonism,
although, because I kind of view it in the...
I think Jean-Pierre Jai said that,
and if you think about it all of mathematics,
it's just, it's semiotics and linguistics.
The thing though is it kind of does...
It has to blow our minds as to how consistently it's worked.
I mean, at a pure pragmatic level,
it is rather bizarre that there is this internal consistency
in mathematics.
So I'm still...
Unreasonable efficacy.
Exactly, yeah, yeah.
I mean, you can't necessarily say that, therefore,
that can be true in some fundamental metaphysical sense,
but it just makes one wonder.
Yeah.
So back to the idea in the paper,
I kind of want to also discuss...
In this paper also, you do...
Oh, I think I made a mistake there.
I'm confusing papers.
In this, in the math, it's not the territory.
Do you discuss the natural selection,
like you kind of contrast or juxtapose the FEP
with natural selection,
or could it be the other paper you wrote,
which was more like an introduction to the paper,
the free-range principle,
an accessible introduction to its derivations,
implications, and applications?
I don't know, good question.
Yeah, because regardless, what I also liked was...
And this probably connects to my previous,
my initial question,
why a lot of philosophical interest in the FEP,
it's just...
Yeah, so like the FEP,
is it like a meta-narrative or like a meta-theory,
or it's called a super-theory,
not a meta-theory, a super-theory,
something like natural selection?
Yeah, or like almost an intuition pump?
Yeah, I think I prefer that,
because you can't really...
It doesn't really fall into the Popperian falsifiability
in that rigorous sense, right?
I should say maybe that was sort of a...
I'm sort of adapting the Dennett concept.
That's not exactly...
That's not how he uses it,
but one could say it's an intuition pump
in a stretch of that terminology.
I see.
It's a kind of conceptual framework
that enables us to think about
things very differently in science.
But it's not actually...
It's not a method.
It's not a formal model.
It's not a theory or a hypothesis or a...
There are all these conceptual tools in science,
and it's not any of the kind of lower-order conceptual tools
of science in the same way that natural selection is not...
I don't view it as a theory.
Natural selection isn't a theory or a hypothesis
or a model or a...
I mean, it's really sort of a meta conceptual framework,
I would say.
In that sense, it's very far removed from the actual
day-to-day work of science.
Of scientists, yeah.
And experimental work, for instance.
And I think in part 1.5,
you essentially say,
I propose here that we reserve the free energy principle
to denote only the maths of which the FEP is composed,
models utilizing this formalism to study natural systems,
bear also an interpretation,
lending a means of interpreting the maths as
about the systems in nature.
So, I mean, that idea of...
Yeah, it's kind of a formalistic conceptual structure.
I like thinking of it that way.
Excellent.
So, okay, let's see.
In terms of time, yeah, we've got a few...
Is it okay if you go for about 10 more minutes?
Fine, all right.
Fantastic, thanks.
Thank you, Mel.
I kind of...
I'm happy to chat again at some point if that's...
Yes, yes.
I mean, in fact...
I just have to run at like, you know...
I have to run in 15 minutes, but...
Yeah, just 10 more minutes of your time,
only because one thing I really try to do with this podcast
is...
I don't want to say a bridge,
but I try to bring into dialogue
so-called continental philosophy
and more of the analytic philosophy
that kind of you work through.
I should say I studied with Dennett
in my undergraduate
and it was forbidden to me to touch
continental philosophy.
Yeah, you see, this is, you know, a kiss and point
and I don't like that, to be honest.
I don't even like creating this demarcation, really.
It's just philosophy.
It's just thinking.
Philosophy is a philosophy.
As far as I'm concerned...
But see, he knew my mind
and he knew what reading Foucault
would have done to me
and it was nothing good.
Yeah, there you go.
You know, you start talking about the epistemes
and all of that.
Yeah, yeah, yeah.
But, you know, what's fascinating is,
like, look at how much of, you know,
fruitful, productive output you've had,
if I may,
because you do take in the sociological aspects
that Foucault, perhaps above all,
will point out very carefully
when it pertains to our knowledge
and epistemology.
But, yeah, on that note,
so let me just ask a very, very broad
general question, Mel.
As a philosopher of AI,
as a philosopher of science,
what insights do you think
us in the, let's say, Anglo-American,
you know, I'm in Australia,
you're in the...
Are you Canadian or are you in the U.S.?
U.S.
U.S., right?
So, yeah, the Anglo-American kind of...
Yeah, there you go.
The more analytic, you know, oriented philosophy,
which, you know,
I spoke to this philosopher called Simon Critchley.
He said it's more for just a historical phenomena.
It's got...
It has nothing to do with, really,
with ideas per se.
It was just, you know,
how things changed with World War II
and how certain analytic philosophers
at certain political...
It was more of a political phenomena,
more than a philosophical phenomena in history.
But, you know, be that as it may.
Yeah, as a philosopher of AI and a philosopher of ML,
what insights do you think we can gain
from kind of discos within continental philosophy
with thinkers all the way from thinkers like Heidegger
to people like Zizek
and to, you know, sociologists like Foucault?
I don't delve into that work very much,
but I know people who do,
and they, I think, make very interesting work of it.
Elmer Feiden is doing...
is more...
I don't know.
The philosophy of AI is more...
Yeah, thank you very much.
...is more continentally influenced.
Okay.
I haven't heard of that person.
Fantastic.
Okay, thanks for that.
He will know other people.
Yeah, yeah.
There's also some...
I mean, I think there's a way in which sort of STS scholarship
on technology, AI, ML,
is also more continentally leaning in some ways.
Well, this is a book that I've just started on
called The Critical Theory and AI,
which I'm hoping to speak to its by Simon Lenegrin.
I just started on the books.
I can't really say much on it.
It was just published last year.
Yeah, we just put out this...
the pseudoscience paper.
And a lot of the reviewers were suggesting
some sort of like critical theory.
Oh, you mean the one that goes in the machine learning paper?
Yeah, our reviewers were suggesting some of that stuff.
Fascinating.
Yeah, yeah.
I mean, yeah, which is like for me,
and, you know, leaving aside the names,
like just generally for you, as you pointed out,
you know, then it said, you know,
no touching Corneola philosophers,
but just I'm curious,
well, why did you rebel against your...
I'm joking, of course,
but your master's commands
and kind of get interested in...
because I've seen through your Twitter feed,
for instance, that you do,
you do retweet stuff by some Corneola philosopher,
by Zizek,
you want totally a verse to it,
or a post to it.
Yeah, I mean, I think the divide is...
We have in the discourse, you know,
in philosophy.
I think, again, the divide is pretty superficial.
I couldn't ever...
Yeah, absolutely.
As you said.
And I think, as I said,
like with everything,
most of everything is...
This is, again,
a denitism,
but 99% of everything is crap,
was his life.
And I think that's true of both analytic
and continental philosophy.
There's a dominant tradition.
There are aspects of,
like if we think of analytic philosophy,
continental philosophy as methods,
I think there are really problematic
features of both of those methodologies
as methodologies for philosophy.
I think there are really sort of stultifying
features of both of those methodologies.
And there's brilliant work.
There's brilliant ideas coming out of both traditions.
But in a way,
it's working against some of the kind of primary tendencies
of those traditions, in a sense.
And so in my work,
I'm just trying to say something
interesting that matters for the reality we live in,
and I'm trying as much as possible
not to be operating within a particular conception
of what good thought is.
I mean, I think,
thinking according to some conception
of what is the right way to think
is not a right way to think.
I think it keeps us short of discovering truths.
So I try to not adhere as much as possible
to principles of particular genres of philosophy
in so much as I can do that
and still get published enough in philosophy venues
to get a job.
That's a whole other conversation.
Just on a side note, Mel,
are you familiar with the think of Bianchel Han?
No.
Yeah, so he wrote this book Psychopolitics.
Where is it?
He's quite big in Germany.
He's a German philosopher.
In fact, no, he's originally from South Korea,
but now he works in Germany.
And when I read your paper,
in fact, when I read you the theory-free paper,
the theory-free ideal paper,
and then also the recent one, the pseudoscience one,
he has one chapter in this book Psychopolitics
on big data.
And he wrote this in like the early 2000s.
A lot of this, I'd love to, in fact,
as you said, you'll be open to another conversation,
I'd love to kind of discuss.
It's just like 15 pages that chapter
that just has his ideas on it.
Because I think you'd be the perfect person
to comment on it with your kind of analytic background.
Yeah, with the work you've done,
because it really did remind me of his ideas
when I read that, when I read your work, in any case.
But yeah, again, I want to be cognizant of your time.
So just one last question, Mel.
What would you say, currently, I just like asking this
because it kind of excites me as to where,
what kind of work I should do
in this space of philosophy of ML
and philosophy of AI.
Apart from what we've discussed so far,
what other avenues for research do you think
people should look into and take into consideration
and start thinking about?
Oh, God, there are a million.
I think there are a million avenues that are pursued
or could be pursued that I think are kind of fruitless.
But there are a million avenues that are
really fascinating.
So I mean, I think the way machine learning technologies
are being adopted and changing labor
and changing the way these technologies are changing
our relationship to the sort of resource consumption
to the means of production, to the output of labor,
to the output of intellectual labor
or epistemic labor, one might say,
which are, I think, much more subtle than we've given
credit to so far.
The way...
There's a problem of opacity or explainability
or what have you, I think that's...
It's clear that the sort of research avenues
that we've been on with that are deeply flawed
and the kind of technologies we've invented
to tackle those problems
and so much as their problems are deeply flawed,
but figuring that out is hugely important.
How these technologies are bringing about harms
in the real world is a tremendous issue
and tackling that in a way that is deeply cognizant
of incentive structures, right?
And investing in interventions
that those in the positions of power
to create and deploy these technologies
would actually be incentivized to adopt, right?
And working on that incentivization.
So this requires a vastly interdisciplinary perspective.
This requires having whatever the domain of application is,
that's a hospital, if that is in biological science,
if that is in building cell phone applications,
if that is whatever the application is,
having deep domain knowledge of that application,
having knowledge of the regulatory structures
or lack thereof, having knowledge of the incentives
of those who are creating these technologies
and deploying them and, you know,
having knowledge of the knowledge level of those
who are creating these technologies and deploying them,
et cetera, et cetera, et cetera.
And so I think it's like now is the time
when we need to stop messing around
and start doing really, really good,
really dialed in interdisciplinary work
and supporting that
because you cannot effectively prevent
or remediate the ill usage of these technologies
from a single disciplinary background.
You need teams of people with expertise
in a lot of different domains
who know how to talk to each other.
Yeah, oh, that's for sure.
Yeah, so I think, sorry, I lied.
That was the penultimate question.
This is the last question.
And then I'll let you go, I promise.
Because you spoke about kind of social harms.
One other question I wanted to ask from you was,
you know, we spoke about science fiction
and, you know, kind of the dooms kind of mentality.
But would you compare, would you, like,
would you say the current paradigm of AI,
the current technologies we have,
that it can be compared to something like
the nuclear bomb in the nukes in the 20th century
or do you think that's an exaggeration
or I'm just being dramatic?
And there yet it's not that grave?
Well, the impact of nuclear bombs,
the impact of nuclear technology,
was in one part the reality
of this groundbreaking technology
that made killing and destruction
possible at only levels.
And in another part, it was a psychological phenomenon.
It was almost more radically a psychological shift, right?
I think the thing with machine learning technologies
and any possible future development of these technologies,
in fact, not just what we have right now in front of us,
is there's that almost that level of psychological reaction
to what these technologies mean.
But the disruptive force is not owing to
a really radical scientific or technological breakthrough.
It's more the psychological and social time.
It has the potential to really radically disrupt labor markets
and it already is.
But that was something that was already happening.
Workers were already being pushed out.
That was happening since the Industrial Revolution.
That's capitalism.
It's just that now there's an even more...
It's perpetually a more and more urgent need
to shift away from a mode of production
in which if you don't work, you die.
The world in which if you do not labor,
you starve to death is not one that works
in which most people are being automated
out of their jobs.
Regardless of the technology.
On that grim note, thank you very much for your time, Mel.
I've thoroughly enjoyed...
I've learned a lot from you,
but also just your Twitter feed is fantastic.
I frequent it often.
I hate Twitter.
I think it's a terrible place, but I go to yours
and I've got five people bookmarked.
I just visit their feeds.
But yeah, thank you very much for your time, Mel.
And I hope we can chat again soon.
Yeah, thanks.
I'm happy to follow up.
I have a Korean-German author.
Yes, the one by Berchel Hahn.
Yeah, I'll send you...
I've got a PDF.
I'll send you the PDF and all that
because I love to kind of, you know,
follow up on that and see what ideas you have.
But yeah, thank you, Mel.
Yeah, thanks.

I want to discuss explanatory coherence.
You've done a lot of work on that,
and I'm wondering if you could kind of give a brief introduction
to what explanatory coherence is to a lay audience,
while at the same time, why you think
what are the implications in fields such as AI and psychology?
That's a big question, but it's a really interesting one.
So let's go back to Darwin.
So I mentioned one of my favorite books of all time
is the Origin of Species.
So what was he doing in that?
Well, in my view, what he was trying to do
is to give a coherent explanation of all sorts
of things that he observed.
So he went on this voyage around the world,
and he collected all sorts of other kinds
of biological information.
And he gradually seemed to him that it seemed, in fact,
that the species had evolved.
I mean, now everybody knows that.
Kids get that probably in grade four.
But it was a very controversial idea.
Some people had maintained it, but it went up
against religious doctrines.
And so he gradually started to amass more and more evidence
that species had evolved.
But then, from reading a crazy economist named Malthus,
he suddenly got the idea of how they evolved.
And that's how he came up with the idea of natural selection.
So now we had not only a bunch of observations
and the idea that evolution probably
had occurred that would explain it,
but an idea of how evolution had occurred.
That is, that natural selection was the mechanism
behind evolution.
So what he did in that book was an incredibly beautiful
argument for his view, as opposed to the view that
was dominant at the time, which was divine creation.
So what he was trying to show is that his view was a better
explanation, but then divine creation,
because it was more coherent with the evidence.
So this is an account that philosophers
call inference to the best explanation.
You can argue that something is the best explanation
because it's more coherent with the evidence.
But now you have to say what coherence is.
So I had these early ideas coming out
of my philosophy of science background.
But then, in 1987, I got one of the best ideas I've ever
had, which was how to turn coherence
from a sort of vague philosophical idea
into a precise computational one.
That is, how can you compute coherence?
So I've been working on neural networks
in collaboration with my colleague, Keith Holyoke.
And he came up with an idea that you could use neural networks
to explain analogy.
These neural networks, of course,
are now absolutely central to artificial intelligence.
It's really taken off.
That's a whole fascinating topic in itself.
But he figured out a way of doing that.
And by that time, I'd done my master's in computer science,
so I was a pretty good programmer.
And so I programmed up a program to use neural networks
to do analogies.
So that was nice.
And then I thought, what else could apply to?
And then I thought back to the problem
that was part of my doctoral dissertation, which
was inference to the best explanation.
How do you pick up the best theory?
And so then I realized that that kind of coherence
can be understood using the same kind of neural network
technique that Keith and I had done for analogy.
So it was a really powerful method, both computationally
but also psychologically.
Because since then, lots of psychological experiments
have backed up this idea of coherence.
So I think of the mind, the brain,
as essentially a coherence agent.
There's some people who think that it's primarily
a predictive engine.
But I don't think that's true.
I think it's primarily a coherence engine.
We're trying to make sense of things,
whether we're making sense of the past, which
is what explanations do, or making sense of the future.
We're making coherent predictions,
or we're trying to identify things.
Is that a rabbit or a squirrel?
Those are different kinds of thought.
Everything we do can be understood
as having coherence behind it.
But coherence now isn't just a sort of vague metaphor
that it was for philosophers.
It's not just a matter of consistency.
It's rather of taking a whole bunch of different things
and putting them into a good package.
But what's a good package?
Well, here there's an idea that came out
of the neural network world called constraint satisfaction.
So we're trying to satisfy a bunch of constraints.
What constraints did Darwin face?
Well, he was trying to explain as much as possible
about what he'd seen in the biological world.
That's the positive constraints.
But he also had a negative constraint.
He had to show that he could do that better than the theory
that was the competitor at the time, which is divine creation.
So that's a negative constraint.
So what you're doing in all of these things,
whether it's decision making, or pattern recognition,
or even emotion, you're putting together
different sorts of constraints to evaluate
what's the most coherent view.
So that's how I came to see coherence,
not just as a vague philosophical idea,
but as a quite precise computational one
that can be used to explain the mechanisms
that underlie a vast amount of human thinking.
So that's why I think coherence is really a fundamental idea
to psychology and cognitive science
and to these philosophical projects as well.
Yeah, excellent, excellent.
I'm glad he brought up predictive coding
or predictive processing,
because I'll be talking to a cognitive scientist
next month, in fact, based here in Melbourne.
And I want to ask her about your theory
of explanatory coherence,
because I believe you do have certain critiques
of predictive processing,
but also in your book, you are critical of,
in fact, no, I think you wrote an article on this,
a paper on this, pardon me,
you're also critical of functionalism,
kind of what Hillary Putnam and the likes put forward.
So from your kind of theory of mind,
what would you say are your critiques
of one predictive processing and then a functionalism?
Those are two different views.
I don't think they have anything to do with each other.
I'm just curious,
like what would be a critiques?
Okay, let's see one at a time.
Predictive processing definitely
is an influential view right now,
but I think it's just not right.
It says that the brain is a predictive engine,
as if everything is prediction,
but the mind doesn't just do prediction,
it does at least five other things
that are just as important.
It does explanation, which involves explaining the past,
that's not prediction, that's the past,
prediction is about the future,
or even pattern recognition.
I mentioned, I see an animal in my backyard, what is it?
Is that a squirrel or a rabbit?
Well, that's pattern recognition,
that's not necessarily a prediction,
I want to know what it is.
We also want to do, and this is really important,
evaluation, is this good or bad for me?
Is this a threat to me?
Or is this something I can eat?
How do we do evaluation?
Well, in humans, that comes from emotion.
The predictive processing approach has said nothing interesting
to say about emotion at all,
but that's absolutely fundamental
to many areas of human thinking.
I've got a whole theory of emotion,
of which coherence is part of it,
but it's only part of it.
So you have to have evaluation going on.
Communication, we sometimes predict
in order to communicate with other people,
but there's lots of other things going on
where we want to be able to get our ideas across to others.
So that's just at least five things
that are part of the human mind
other than predictive processing.
So that's my first critique.
My second critique is the way that people in that world
think that predictive processing work
doesn't correspond to how the brain works very well.
They're basiants, they say that the brain uses
probability theory in accord with Bayes' theorem
to predict the next thing.
While this is crazy computationally,
Bayesian processing is well-known
in artificial intelligence to be extremely inefficient.
You can prove that it's computationally intractable.
You can show that it causes all sorts of problems.
Bayesians have to jump through all sorts of hoops
to try to deal with anything larger than that.
I've done a little bit of Bayesian modeling
because I wanted to, I drew a couple of papers
where I compared a Bayesian model of legal reasoning
to my explanatory coherence one.
But the Bayesian models are crazy
because you have to generate all sorts of conditional
probabilities that nobody has an idea what they are.
So if you actually do Bayesian modeling seriously,
you'll find, first of all,
you don't know any of the probabilities.
You don't know any of the conditional probabilities.
You don't have the computational or neural resources
to actually commute the probabilities.
So the way that predictive processing
with its too narrow view of how the brain works
fills it out is by making brains Bayesian when they're not.
A really good contrast here
is with the new generative AI models,
which are actually incredibly good at predicting.
Have you used chat GPT or any of the others?
They're astonishing.
They're astonishing at how good they are
at predicting the next word to say.
And they end up producing really coherent stuff.
And they get things really badly wrong sometimes,
but often they're really good.
But they don't use Bayesian predictions.
It's they've got all different kinds of algorithms that they use,
the tension mechanism and sorts of things.
So they realize that the Bayesian approach
is not going to work for them.
So those are my two major criticisms
of the predictive processing.
The brain is a multi-fast,
it's a coherence engine doing six things
as well as prediction.
And it's not doing it using Bayesian probability calculations.
Okay, so is that good enough for predictive processing?
Do you want to reply?
No, I think that's perfect.
I want to ask you about the free energy principle,
but probably we'll get to the functionalism
and then maybe come back to the free energy principle.
Yeah, okay.
So functionalism is a view in the philosophy of mind.
It's actually a really bad term
because functionalism is a term that operates
in about six different fields or six different meanings.
So we need to pin it down a bit.
Let's call it computational functionalism.
Because it came in the 60s
when computers started to become aware.
And Hilary Putnam knew about the advances in computing.
And computers were really primitive then.
I've got a watch now that was better than all the computers,
way better than anything that came along for decades.
But still, people were starting to think that
with computers and the possibility
of artificial intelligence,
we've got this abstract way of thinking of thinking
as a kind of computation.
Now, one thing that's really true,
or seems to be true about computation,
is that it doesn't really matter what you run it on.
So here I'm using a Macintosh.
I don't know what kind of computer you've got.
It could be a PC or running a different kind
of hardware altogether.
Doesn't matter.
We can all run the same software.
And so the analogy that Putnam hit on was,
mind is software rather than hardware.
You can take the same software
and run it on a bunch of people of hardware.
All that matters is it has the appropriate
computational functions.
That's where the word functionalism comes from.
So if you have inputs and outputs
and you have the functions in between,
you want to be able to make thinking work.
And so forget about the hardware.
Forget about the brain, for example.
The psychologists have been studying the brain
at that point for, I guess, 60 or 70 years seriously.
But the functionalists in the 60s said,
let's forget about the brain.
It's all computation.
It's just like AI.
Anything that runs in the mind can run on a computer.
Okay.
And actually in the 1960s and 70s,
that was a pretty reasonable idea.
And this is why a lot of philosophers
move considered themselves functionalists.
It became the dominant view in the philosophy of mind.
So I think that was a pretty good idea
in the 60s and 70s because AI had become a real field.
Computers had become at least rudimentarily powerful.
And so not a bad idea then.
But things changed in the 80s.
In the 80s, a bunch of things changed.
First of all, brain scanning came along.
It had been really hard to study the brain before
because you had to do things like poke electrodes
into brains that had been exposed.
And so it was really hard to study the brain.
But in the 80s, brain scans came along.
First of all, I forget what they were called,
and then eventually fMRI.
But suddenly you could actually study the brain
in a much more detailed way.
And then you could start to test
some of the claims that had been made.
So when people started doing fMRI studies,
they thought, oh, we're gonna be able to show
that the mind really is module, that is modular.
That is different parts of the brain
are doing very specific things.
And so we should be able to find that
this part of the brain does high-level thinking,
this part of the brain does emotion,
and that part of the brain does vision,
and we could localize it.
But once people had this new tool,
they started to realize, hey, it's not like that at all.
Lots of what goes on in the brain
involves interactions of lots of different areas.
So suddenly the brain became much more interesting.
It didn't look like just some other kind of hardware
you might run thoughts on.
It looked like you could study on its own.
So there were these empirical findings coming out
of the new tools available for studying the brain
that suggested that, well, maybe the structure
of the brain really does matter.
So that was an incredibly important empirical basis
for starting to question functionalism.
But there was also a really interesting theoretical basis
coming out of the ideas about neural networks.
So the ideas of neural networks had been around
really back since the 50s, but it didn't work very well.
And people like Marvin Minsky had argued that,
no, these ideas about neural networks
are not going to work very well.
They're just simply not theoretically strong enough.
But in the 1980s, people greatly expanded
the possibilities of what neural networks could do.
They invented a new algorithm called back propagation
that does learning.
A whole movement got started called connectionism,
which said that knowledge isn't a matter of the words
you've got or the symbols,
which is what artificial intelligence is.
It's rather, it's the connections.
It's the neural connections.
So suddenly people were modeling their computer models
because these are being done
with computerized neural networks.
They were modeling on ideas about the brain,
how you can have different neurons working in parallel
with simple connections with them
and nevertheless doing that.
So in the 1980s, suddenly functionalism was in trouble.
Not many people noticed
because they weren't tracking what was happening
in neuroscience and in neural network theory.
But it was, and by the 1990s,
I think it really had completely turned around.
I think by the 1990s, functionalism was no longer plausible.
You needed to take the brain seriously
if you wanted to understand.
And the whole field of cognitive psychology changed.
It went from being completely abstract in computation
all to doing almost everything it did
in relation to what happened in the brain.
So cognitive psychology is now completely connected
with neuroscience in the field of cognitive neuroscience.
Other areas of psychology, developmental, social
also became intensely tied in with the brain.
So the idea that the hardware doesn't matter,
which was what was behind Putnam's functionalism,
just by the 90s, didn't seem plausible at all.
So that's why I think functionalism is a defunct view
in the philosophy of mind,
even though there are people who seem to assume that it's right.
Sometimes it goes under other names.
So it's another name that people use.
It's called substrate independence.
The idea is the substrate is the physical,
and so that doesn't matter.
And there are people who use that
because it suits some of their views,
such as the idea that we're all living in a simulation,
which I think is a really dumb view.
But in order to believe that,
you have to believe that substrate independence is true,
which is another word for functionalism,
which says the hardware doesn't matter
because a computer, the idea of a simulation
is some computer in the future
is basically simulating our thoughts now.
Well, that assumes that a computer
can simulate all our thoughts,
which assumes functionalism or some straight dependence,
independence, which I think is wrong.
And I've actually just published a paper
in philosophy of science two years ago
that gives a whole bunch of arguments
based on energy about why it's wrong.
But there are other reasons as well for thinking
that functionalism or substrate independence is wrong.
Okay, that's enough.
Functionalism.
That's really fun, but go ahead.
I didn't mean to interrupt you there, but please.
I just wanted to summarize.
So it was a great idea in the philosophy of mind
that no longer should be taken very seriously,
given what we know about brains and energy.
And so even look at
the way AI is going right now.
So the generative AI models,
the large language models are incredible,
but they're really energy pigs.
It takes vast amounts of energy
to train these things and to answer questions.
Our brains are astonishing.
Our brains work on basically 40 watts,
like a small light bulb,
very small amounts of energy, very efficient.
And yet we're still smarter than any computer
with all these resources.
So there's a fall field called neuromorphic AI,
which is trying to make computers more like the brain
to get these advantages energy and efficiency
and working in real time.
So I think these are really interesting research areas
that show that functionalism just wasn't,
is no longer a plausible view in the philosophy of mind.
Now that's an astute point, Professor,
because I was, well, two points on there.
Firstly, I always found functionalists
to be good old Cartesians,
where they had the mind matter.
They think mind is independent to matter,
which for me never made any sense,
given we have physical embodied beings.
And secondly, you are 100% right
that I was listened to a talk by Scott Aronson,
the American computer scientist,
and he is now a researcher at OpenAI.
And OpenAI is heavily investing
in quantum computing and even in nuclear energy,
because they've understood that if they are to grow
their LLMs, they need infinite amounts of energy
because the compute power for LLMs are so,
they're so high compared to like our puny little brains,
which is a fascinating conversation.

Last week, we learned about the support vector machine, which is a powerful algorithm with a science-fiction feel.
But despite its usefulness, it can be difficult to understand and even more difficult to explain.
What if I told you that there's another supervised machine learning algorithm that's also powerful and useful,
but is grounded in real life so that it's much easier to understand and much easier to explain?
That algorithm is the decision tree.
A decision tree algorithm is one of the most popular machine learning algorithms in use today.
But it's different than any of the algorithms that we've studied so far.
What makes the decision tree algorithm so different?
Well, let's find out.
Welcome to Julia for Talented Amateurs, where I may call some Julia tutorials for Talented Amateurs everywhere.
I am your host, the Dabbling Doggo.
I dabble.
For those of you who are visual learners, you're going to love the decision tree.
While other machine learning algorithms can be highly abstract and difficult to visualize, the decision tree is just the opposite.
The way a decision tree works is similar to how you might make decisions every day.
The idea is that you can make a big decision based on a collection of little decisions.
For example, if you're about to go outside during the rainy season, you might wonder, should I bring an umbrella?
Before answering that question, your brain may ask a collection of smaller questions first.
For example, it might ask a simple yes or no question, like, is it raining now?
If the answer is yes, then it tells you to bring an umbrella.
If the answer is no, then it might ask another simple question, like, is it forecasted to rain later today?
If the answer is yes, it tells you to bring an umbrella.
If the answer is no, then it tells you that you don't need to bring an umbrella.
Whether you're conscious of these micro-decisions or not, your brain is going through a similar thought process as you make countless decisions throughout your life.
What if you could build a computer model that followed that thought process to help your computer make decisions?
That's the idea behind the decision tree algorithm.
The decision tree is used in several different academic disciplines, but in machine learning, it's typically used for supervised machine learning.
The decision tree may be used for either classification or regression, but in this tutorial, I will only be covering decision trees for classification.
As an academic subject, decision trees have been around for over 100 years, but more recently, decision trees have been adapted for the computer age.
There are several different decision tree algorithms used in machine learning.
But the version that we will be using is called classification and regression trees, or CART for short.
The first version of the CART algorithm was completed in 1977 by Leo Breiman and Charles Stone from UC Berkeley and Jerome Friedman and Richard Olsson from Stanford University.
In 1984, they published a book appropriately named Classification and Regression Trees, which detailed both the concepts and the mechanics of constructing decision trees.
Decision trees are typically visualized using some form of graph theory representation, meaning a visual combination of nodes and edges.
Decision trees are inverted trees with a root node at the top.
A root node contains all of the data in the training data sets.
For a binary decision, there are two edges coming out of the root node, typically representing a yes-no or true-false decision.
These edges are also called branches.
Connected to those branches will be another node.
This time, a node may either be a leaf node or a decision node.
A leaf node contains a subset of the training data and represents a final classification, so there are no further decisions to be made for that subset.
A decision node also contains a subset of the training data set, but requires additional decisions to be made.
Like the root node, the decision node has two edges coming out of it to represent a different yes-no or true-false question.
Unless given constraints, the decision tree algorithm will continue this process of interrogating the data and growing the tree until every sample of data has been classified.
Since the objective of using a decision tree algorithm is to create a model that can be used to make predictions on unseen data,
it's a good practice to place some constraints on the decision tree algorithm so that it stops going through your data at some point.
That way, you can have a more generalized model that you can use to classify new unseen data.
Now that you know what a decision tree looks like, let's build one using the DecisionTree.jl package.
While the DecisionTree is easy to visualize, it is surprisingly difficult to implement the code from scratch, so we'll be using the DecisionTree.jl package.
DecisionTree.jl was created by Ben Sadeghi.
On the surface, DecisionTree.jl seems like a simple package, but the more you dig into it, the more you'll understand how sophisticated it really is.
DecisionTree.jl is not a wrapper to a library written in a different language.
Instead, DecisionTree.jl is a pure Julia implementation of the CART algorithm.
Let's start by setting up our programming environments.
For today's tutorial, knowledge of Julia and VS Code is required.
I'm also assuming that you're watching this entire machine learning playlist, so episodes 501 through 505 are required.
In your VS Code Explorer panel, create a new folder for this tutorial.
In the tutorial folder, create a new file called SL underscore trees.jl.
Launch the Julia REPL by using ALT J, then ALT O.
For this tutorial, I think it's useful to dock the REPL panel to the right side panel.
This is optional, but we will be referring to the REPL throughout this tutorial.
In order to dock the REPL to the right side panel, you can click and drag the REPL panel header until your mouse reaches the right edge of your window.
A panel highlight should appear, indicating that you can dock it there.
Once it's docked, you can adjust the panel size to your liking.
For those of you who use Julia's workspace panel, you can also dock the workspace panel in the right side panel if you wish.
In the REPL, change the present working directory to your tutorial directory.
Enter the package REPL by hitting the closing square bracket.
Activate your tutorial directory.
Add the decision tree.jl package.
Type in status to confirm the version number.
Exit the package REPL by hitting Backspace.
Adjust the panels in VS Code so that you can see both your text editor as well as the REPL panel.
Okay, let's get started.
Let's start by loading some packages.
Next, let's load some data.
We'll be using the iris dataset as our motivating example to learn how to make predictions using decision trees.
The decision tree.jl package comes included with its own version of the iris flower dataset.
For some reason, there are some slight differences between this dataset and the one we used last week from the rdatasets package.
It's not a material difference and it won't change the outcome of our models, but I did want to let you know that these are not identical datasets.
The load underscore data function is from the decision tree.jl package and it splits the data for you between inputs and outputs.
But if you view the data, you'll see that there are no Julia data types assigned to that data.
As you can see, both the inputs and the outputs have a data type of any.
In order to improve performance, the decision tree.jl package recommends assigning data types to your data before building any models.
This dataset should look familiar to you.
In this particular dataset, there is no header row, but as a reminder, the first four columns are the features, which are the various measurements in centimeters.
And the fifth column contains the labeled class data, which is one of three classes.
Satosa, Versicolor, or Virginica, which are the three different species of the iris flower.
Next, we will need to split this data between training and testing.
In order to do this, we'll be using Houdin SR's code from last week.
I am simply going to copy and paste this code from last week's tutorial so that you won't have to watch me retype this code.
I'm going to go through the next few steps fairly quickly, since it's the same workflow that we used last week.
I'm using the same random seed that I used last week, so the index numbers we used this week for training and testing should be the same as we used last week.
Next, we need to split the features between training and testing.
And then, we need to split the classes between training and testing.
Unlike last week, we do not need to transpose our data, since the DecisionTree.jl package does not require it.
Okay, we are now ready to build our first DecisionTree.
In order to create a DecisionTree, all you need to do is use the DecisionTree classifier constructor.
There are a lot of different keyword arguments that you can include in this constructor, but I'm only using the max underscore depth keyword argument, which will stop the tree from growing too large.
You can find all of the available keyword arguments in the DecisionTree.jl documentation.
The DecisionTree.jl package also includes a DecisionTree regressor constructor, which is used for regression problems, but I will only be covering the classifier in this tutorial.
The rest of the workflow is very similar to the workflow that we use for SVM.
You can fit your training data to the model by using the FitBang function.
And that's it.
You can see your DecisionTree in the REPL by using the print underscore tree function.
I know it doesn't look like much, but there's a lot of information in this compact output.
Here's the visualization of that output.
The top line of the output is the root node.
The algorithm has scanned all of the features and has determined that splitting feature number four, or the fourth column, will provide the most useful information.
The algorithm has selected a value of 0.8 as the threshold.
Samples that have a feature four value of less than 0.8 go into the left node, and everything else goes into the right node.
With the left node, the algorithm has determined that no further splits are required.
This left node is a leaf node, and the classification is Satosa.
The 35 slash 35 means that 35 samples have a class label of Satosa out of 35 total samples.
The right node is a Decision node, since the algorithm has determined that more information may be gained by splitting the data further.
For this subset of data, the algorithm has determined that splitting feature number three, or the third column, will yield the most useful information, and it has selected a value of 4.95 as the threshold.
Samples with a feature three value of less than 4.95 go into the left node, and everything else goes into the right node.
At this point, the tree stops growing because we use the keyword argument max underscore depth equals two.
If we didn't use that keyword argument, the tree would continue to grow until the algorithm determined its own stopping point.
As a result, the last two nodes are leaf nodes by default.
For the left node, the algorithm is classifying that data subset as VersaColor, and for the right node, the algorithm is classifying that data subset as Virginia.
But the algorithm knows that there are some misclassifications.
In the left node, the 25 slash 26 means that 25 samples have a class label of VersaColor out of 26 total samples, meaning there's one sample that has been misclassified.
In the right node, that 32 slash 33 means that 32 samples have a class label of Virginia out of 33 total samples, meaning there's one sample that has been misclassified.
In order to get a better understanding of what the algorithm is looking at, let's take a look at the actual data subsets.
Let's start by looking at the training data, which is what the root node is looking at.
This data subset contains 94 samples.
If you add up all the numbers in the decision tree nodes, you should get 94.
If you sort column 4, you can see why the algorithm selected 0.8 as the threshold for feature number 4.
It doesn't matter if it's less than or less than or equal to 0.8.
As you can see, none of the samples have a feature 4 value of 0.8.
Instead, there's a clear dividing line between the values of 0.6 and 1.
0.8 is just the average between those two values.
There are 35 samples above the line, and all of them have a class label of Satosa.
That's where the 35 slash 35 comes from.
Now, let's take a closer look at the data subset that was sent to the decision node on the right.
This subset contains 59 samples, and the class labels are either Versicolor or Virginica.
If you sort on column 3, you'll see why the algorithm selected 4.95 as the threshold.
There is no sample with a feature 3 value of 4.95.
Instead, there's a clear dividing line between 4.9 and 5, so 4.95 is just the average.
There are 26 samples above the line and 33 samples below the line.
Out the samples above the line, 25 have a class label of Versicolor, which is where the 25 slash 26 comes from.
And of the samples below the line, 32 have a class label of Virginica, which is where the 32 slash 33 comes from.
So in each class, there's one sample that has been misclassified.
You can see the misclassified samples in this data view.
Being able to visualize this much detail is unlike any machine learning algorithm that we've used in the past,
which is one of the reasons why decision trees are so popular.
But we already know that this model has some mistakes in it, so let's see how well it does at making predictions.
Like last week, you can make predictions by using the Predict function.
Now that we have a prediction, we can check the accuracy of that prediction.
So this accuracy score of 89% is not very good.
You'll recall that our SVM model got an accuracy score of 95% right out of the box.
It seems like all of this transparency came at a price.
Although decision trees are easy to understand and easy to explain, they are often not very good at making predictions,
which of course is the primary reason for building machine learning classification models in the first place.
Let's take a deeper dive into the misclassifications to gain a better understanding of what went wrong.
A common analytic tool used in machine learning is called a confusion matrix.
A confusion matrix provides a simple way to visualize where your model got confused while making predictions.
The DecisionTree.jl package comes included with a confusion matrix constructor.
The way to read this confusion matrix is that the classes from the first arguments are in the rows and the classes from the second arguments are in the columns.
So since we used y underscore test as the first argument, the rows show the counts for the actual labeled data that we use for testing purposes.
Since we used y underscore hat as the second argument, the columns show the classes predicted by our DecisionTree model.
The numbers shown along the diagonal are the correct predictions, and any other number represents an incorrect prediction.
For example, that one indicates that our model predicted virginica once when the actual labeled class was versicolor.
That five indicates that our model predicted versicolor five times when the actual labeled class was virginica.
The accuracy score shown here matches the accuracy score that we calculated manually.
A kappa coefficient is a metric that you can derive from this confusion matrix.
I won't go into the math, but the kappa coefficient offers a more nuanced assessment of your model's predictive abilities.
When making predictions, there's always a possibility that your model made a correct prediction purely by chance, thus artificially inflating the accuracy score.
The kappa coefficient attempts to remove the probability of random positive predictions in order to present a more conservative way to assess your model's performance.
As you can see, after factoring out the probability of making a correct prediction just by getting lucky, the accuracy score is closer to 83%.
We can see the incorrect predictions using the same code that we used last week to display the results.
As a reminder, the first column contains the predictions, and the second column contains the class from the labeled test data, and the third column indicates whether or not the prediction was correct.
If you sort on column 3, you can see there's one sample where our model predicted virginica when the actual species was versicolor.
And there are five samples where our model predicted versicolor when the actual species was virginica, which is consistent with what the confusion matrix showed.
You'll recall from last week that the SVM model is non-probabilistic, meaning that it makes predictions without assigning probabilities.
The decision tree algorithm is different.
You can see the probabilities determined by the decision tree model by using the predict underscore proper function.
So this display shows the 56 samples in the test data set.
Column 1 is the probability that the test sample has a class label of Satosa.
Column 2 is the probability that the test sample has a class label of versicolor.
And column 3 is the probability that the test sample has a class label of virginica.
As you can see, our model is 100% confident that the first 15 samples have a class label of Satosa.
But for every other sample, our model is not as confident, since it knows that there's some probability of misclassification.
All of the other samples show a 96% probability when predicting either versicolor or virginica.
Okay, so now what do we do?
The decision tree is great since it's a wealth of information.
But it's not so great at making predictions, which is really what we need.
The good news is that there are a couple of ways to improve this accuracy score.
But like many things in life, those improvements will come with some tradeoffs.
One of those tradeoffs is a fundamental concept in machine learning called the bias variance tradeoff.
On the surface, the bias variance tradeoff may seem like a relatively easy concept to understand.
But upon closer inspection, one learns that it is very nuanced, so it's surprisingly difficult to implement in real life.
The bias variance tradeoff is typically visualized by these three diagrams.
The diagram on the left is an example of underfitting the data, meaning that it hasn't captured enough of the relevant relationships between features and outputs.
The diagram on the right is an example of overfitting the data, meaning that it has learned the training data too well, so that it will make a lot of mistakes when trying to make predictions using new data.
The diagram in the middle is an example of a model being just right, meaning that it has captured enough of the relevant relationships between features and outputs while still being able to be generalized so that it can be used to make predictions using unseen data.
So these diagrams are really from the data's perspective, so they aren't necessarily helpful when you're trying to design a model.
A different way to visualize the bias variance tradeoff is by looking at these four diagrams, which is from the model's perspective.
The lower left diagram shows a model with low bias and low variance, which is the best case scenario.
The upper right diagram shows a model with high bias and high variance, which is the worst case scenario.
In reality, most models are neither best case nor worst case.
Instead, most models fall along that diagonal region between the upper left diagram and the lower right diagram, meaning that your model will most likely have a high bias and low variance, or low bias and high variance.
The best that you can hope to achieve is some result near the middle of all four diagrams, resulting in a model with some bias and some variance.
The model generated by the decision tree algorithm is an example of a model with high variance and low bias, meaning that it should be placed in that lower right box.
Decision trees have a natural tendency to overfit the data, since in theory, it can continue to split your data until it has correctly classified every sample in the training data sets.
But that's no good, since you can't generalize such a model, since new unseen data will most likely be different than the training data.
You can improve the predictive accuracy of decision trees by using something called ensemble learning.
There are two main ensemble learning techniques that are used to improve the performance of decision trees.
One is called bagging, and the other is called boosting.
We'll cover bagging first.
An example of a bagging algorithm is known as random forest.
Bagging is a shortened form of the term bootstrap aggregating.
Without getting into the details, the concept behind bagging is that you can reduce the variance of your machine learning model by doing two things.
One, you can increase the independence of your features.
And two, you can increase the number of models.
You then take this increased number of models with features that are more independent, then you aggregate them into one metamodel.
The result is a model with lower variance, but with a slightly higher bias.
This is the bias variance tradeoff in action.
One of the best known examples of a bagging algorithm is random forest.
The first algorithm for random decision forests was created in 1995 by Tin Cam Ho.
The way that random forest works is that it introduces independence between the features and then generates multiple decision tree models.
But the random forest introduces even more randomness into the algorithm by only considering a fraction of the features at every split.
This results in even more independence between the features, which results in even lower variance.
I realize that this is a lot of hand-waving, so let's see random forest in action.
Fortunately, we can use the decision-tree.jl package to generate a random forest model.
All we need to do is use the random forest classifier constructor to generate our model.
By default, this constructor will generate 10 decision trees.
You can set the number of decision trees manually by using the n-underscore-trees keyword argument.
After that, the rest of the code is similar to the code that we used for the decision tree model.
So, we created a random forest model using the exact same training data that we used earlier and the exact same testing data.
This time, we achieved a predictive accuracy score of around 95%, which is a significant improvement over the 89% that we scored earlier and comparable to the SVM model from last week.
Just like before, we can view our results on a confusion matrix.
As you can see, our random forest model only made 3 misclassifications versus the 6 misclassifications made by the decision tree model.
And just like before, we can view the actual mistakes.
If you click on the header of column 3, you'll see the 3 misclassifications.
It predicted virginica twice when the actual class was versicolor, and it predicted versicolor once when the actual class was virginica.
You can also view the probabilities of those predictions.
This time, the probabilities look a little different.
This random forest model is not as confident as before when it comes to the Satosa class.
There are several samples that have a 10% probability of being Satosa.
On the flip side, this random forest model is much more confident about the versicolor and virginica predictions, with many samples having 100% probabilities.
There are also more probability splits like 90-10, 85-15, 70-30, and so on.
Although we can still view some interesting information, unfortunately, we do not have as much access to information as we had with the decision tree model.
For example, even though I know that this random forest model generated 20 decision trees, I do not know what those trees look like.
Nor do I know what features they used, or what threshold levels were determined.
I can no longer present a visualization of what's going on under the hood.
So the random forest is kind of like a black box, just like our SVM model from last week.
So even though this random forest model is more accurate, it's more difficult to explain.
As you are no doubt concluding, creating machine learning models is a series of trade-offs.
Before we conclude for today, let's go through an example of boosting, which is another ensemble learning technique that will reinforce this concept of trade-offs when creating machine learning models.
While backing is used to lower variance, boosting is generally used to lower bias.
While it makes sense to use bagging with decision trees, since decision trees naturally tend to have high variance, it's not immediately clear why using boosting would help improve the performance of decision trees.
The motivation behind boosting is to answer the question, can a set of weak learners create a single, strong learner?
An example of a weak learner is a classifier that is only slightly better at making predictions compared to a random guess.
An example of a boosting algorithm is Adaboost, which is short for adaptive boosting.
It was first introduced by Joachim Freund and Robert Shapur in 1997.
Adaboost may be used with various learning algorithms, but it seems to pair particularly well with decision trees.
Unlike random forests that grow multiple decision trees and then aggregate them to form a new metamodel, Adaboost starts with what is known as a stump, meaning that it's a decision tree with only a root node and two leaf nodes.
It's not clear to me how this works for classifications with three classes, like the IRIS dataset, but Adaboost does work for multi-class classification problems.
Based on the information gathered by the Adaboost algorithm from that single stump, the algorithm will increase the weights given to the misclassified data so that it can focus on the misclassifications in the next round.
The algorithm will then use that new weighted dataset and pass it through another stump and make another set of predictions.
Again, the algorithm will increase the weight given to the misclassified data so that it can focus on the remaining misclassified data in the next round.
The theory is that by doing this, the algorithm will focus on the samples that are hardest to classify as it continues going through this process.
It will repeat this process and add the knowledge it gains from all of these weak learners to form a single, strong metamodel that can be used to make predictions.
I realize that this is even more abstract than the random forest, so let's take a look at Adaboost in action.
Fortunately, we can use the DecisionTree.jl package to generate an Adaboost model.
All we need to do is use the Adaboost Stump classifier constructor to generate our model.
This model does not have a default setting for the number of iterations, so you will need to enter that number manually by using the N underscore iterations keyword argument.
Setting the N iterations to 20 means that our model will generate 20 stumps to create one tree, as opposed to creating 20 full trees used in the random forest model.
After that, all of the code is exactly the same as the code used in the random forest example, so I'm just going to copy and paste the code from above.
So, we created an Adaboost model using the exact same training data and testing data that we used for both the DecisionTree and the random forest.
This time, we achieved a predictive accuracy score of around 95%, which is a significant improvement over the 89% of the DecisionTree and comparable to the random forest and SVM models.
Like the random forest model, this Adaboost model is sort of a black box.
We do not have access to as much information as the DecisionTree model, but we still have access to some information.
Just like before, we can view our results on a confusion matrix.
The confusion matrix looks exactly the same as the confusion matrix on the random forest model.
And the prediction versus actual data looks identical to the random forest, so you may be wondering if anything is different.
Let's take a look at the probability distribution of the predictions.
So, this looks very different from the DecisionTree and the random forest.
Although the Adaboost model had the same predictive accuracy as the random forest model, it is a lot less confident about its predictions.
Most of the predictions have a 65-35 split, or a 70-30 split.
There are even some examples with a 52-48 split, which is barely better than a coin flip.
There's not a single sample with a 100% probability, although there are plenty of samples with a 0% probability.
So that's really interesting, isn't it?
Both random forest and Adaboost offer the potential for enhanced predictive accuracy, but they approach the problem using very different techniques.
We covered a lot of material today, so let's go through a quick summary.
While DecisionTrees are relatively easy to visualize and easy to explain, they are surprisingly difficult to implement.
They also have some flaws, namely that they are not known for their predictive accuracy and they are prone to overfitting.
This led to a discussion about the bias variance trade-off, which will be an ongoing subject of conversation throughout this machine learning series.
Through that discussion, we learned that we could potentially enhance the performance of DecisionTrees by using ensemble learning methods, namely bagging and boosting.
Bagging is short for bootstrap aggregating, and Random Forest is one of the best known implementations of that method.
Random Forest works by using various techniques to increase the independence of features and by increasing the number of DecisionTrees to be used in a metamodel.
Boosting, on the other hand, uses an additive approach by using a collection of so-called weak learners to combine into a single strong learner.
Adaboost, which is short for adaptive boosting, is a well-known implementation of boosting.
Along the way, we were introduced to another analytic tool, the confusion matrix, along with the CAPA coefficients, that we can use to better understand our results.
That's a lot to cover in one tutorial, and that's a lot to include in a Julia package.
Like I said at the beginning, the DecisionTree.jl package is surprisingly sophisticated and is full of Easter eggs.
Please show your appreciation to the developer and the contributors for sharing their knowledge with the rest of us by going to the GitHub page of DecisionTree.jl and leave them a star.
Well, that's all for today. If you made it this far, congratulations.
If you enjoyed this video and you feel like you learned something new, please give it a thumbs up.
For more wholesome Julia tutorials, please be sure to subscribe and hit that bell.
If you like what I do, then please consider joining and becoming a channel member.
New tutorials are posted on Sundays slash Mondays.
Thanks for watching, and I'll see you in the next video.

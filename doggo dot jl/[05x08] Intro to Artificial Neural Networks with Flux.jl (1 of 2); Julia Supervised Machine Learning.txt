Today, we tackle the famous MNIST classification problem, which has become the Hello World
of deep learning.
I introduced the MNIST database way back in the first episode of this series.
As a reminder, it's a dataset containing a collection of handwritten numbers from 0
to 9.
It was created in 1998 to answer the question, is it possible to teach a computer to recognize
handwritten digits?
So, is it possible to teach your computer how to recognize handwritten digits?
Well, let's find out.
Welcome to Julia for Talented Amateurs, where I may call some Julia tutorials for talented
amateurs everywhere.
I am your host, the Dabbling Doggo.
In today's tutorial, we'll get an introduction to Artificial Neural Networks, or simply known
as Neural Networks.
If you look on the Wikipedia page for machine learning and expand the supervised learning
section, you'll see Artificial Neural Networks listed there.
But you'll also notice that there's a separate section further down called Artificial Neural
Networks.
If you expand that section, you'll see a lot of different subjects listed there.
Artificial Neural Networks are part of a very large field of study that goes well beyond
the scope of the series.
There is no way I'm going to be able to cover all of those subjects now, so I will try to
cover them in a future 13-part series.
For this series, I will use this tutorial and the next tutorial to provide a high-level
introduction to Artificial Neural Networks, which is the gateway to deep learning.
There are many different ways to approach the MNIST classification problem using various
programming languages.
In today's tutorial, we'll go through a pure Julia approach by building an Artificial Neural
Network using the flux.jl package.
In the next tutorial, we'll take the code that we developed today and go through it
in more detail to understand the concepts behind the code.
So the game plan for today is to get through all of the code and to get our model up and
running.
As a result, I will not be providing a lot of explanations today.
For today's tutorial, knowledge of Julia and VS Code is required.
I'm also assuming that you're watching this entire machine learning playlist.
In your VS Code Explorer panel, create a new folder for this tutorial.
In the tutorial folder, create a new file called SL underscore ANN.jl.
Launch the Julia REPL by using ALT J then ALT O.
Maximize the REPL panel.
Change the present working directory to your tutorial directory.
Enter the package REPL by hitting the closing square bracket.
Create your tutorial directory.
Add the following packages, flux, images, ML data sets, and plots.
Type in status to confirm the version numbers.
Exit the package REPL by hitting Backspace.
Maximize the REPL panel.
Okay, let's get started.
Let's start by loading some packages.
One of the quirks of the flux package is that you also need to load some of the specific
functions that you want to use in your session.
And finally, we need to use some of Julia's standard libraries.
Using a random seed is not required, but I'm using it so I can reproduce my results.
The ML data sets package contains several data sets that are commonly used in machine
learning.
One of those data sets is the MNIST data set.
The ML data sets package also contains a convenient way to load those data sets into memory.
If this is your first time loading the MNIST data set onto your computer, you should be
prompted to confirm whether or not you want to download the data set, since it's quite
large.
Select Yes to begin the download.
Depending on your internet connection, it may take a few minutes.
Since I already have the MNIST data set on my computer, the data set was loaded right
away.
Now that we have the data, let's take a look at it.
This is the input training data.
So it looks like a tensor of floating point numbers that has 28 rows and 28 columns and
is 60,000 layers deep.
This is by far the largest data set that I've used on my channel to date.
Unfortunately, there's not a great way to view it in the REPL.
The 28 rows and 28 columns contain numbers from 0 to 1 that represent a black and white
image of a handwritten digit.
We can use the images package to view one of the images.
For some reason, the images are loaded horizontally.
You need to transpose the image in order to view it vertically, so don't forget that
apostrophe.
You should see a white handwritten number on a black background.
No offense, but that's not exactly the nicest handwriting.
But it's a useful sample since everyone has a slightly different handwriting style, and
it's important for your computer to be able to read different handwritten versions of
the same number.
Now the question is, what number is it?
Like any other data set used for classification problems, every sample in the MNIST data set
includes a label with a correct number.
Let's take a look at the label attached to this image.
So the labels are contained in a column vector containing 60,000 integers.
These labels are integers from 0 to 9.
The label for the first sample is 5.
If you look at the image, it sort of looks like a 5, and it sort of doesn't.
If it's this hard for humans to figure out, you can imagine how challenging this will
be for our computer to try to figure this out.
Now let's take a look at the test data.
In past tutorials, we took our data set and split it between training and testing.
The MNIST data set contains 60,000 samples for training, and another 10,000 samples for
testing.
So there's no need to split the data, since it's already been split for you.
Let's take a look at a testing sample.
So it's the same 28x28 matrix for the image, but it's only 10,000 layers deep.
So this image is easier to read.
It looks like a 7, but let's check the label to be sure.
So the label is indeed 7, which is comforting.
Now that we have our data loaded into memory, and have it split between inputs and outputs,
as well as split between training and testing, we should be ready to build our model, right?
Well no, not exactly.
There are a couple of additional preprocessing steps that we need to do.
Fortunately, the Flux.jl package comes included with utilities to make these preprocessing
steps easy.
For the input data, we need to, quote unquote, flatten the 3-dimensional tensor into a 2-dimensional
matrix.
Normally, that would mean reshaping our Julia array, but the Flux.jl package has a utility
function called flatten that will do that for us.
You can see that our 28x28x60,000 tensor is now a 784x60,000 matrix.
All it did was take the 28x28 image and convert it into a column vector with 784 elements,
so each column contains the floating point numbers associated with each image.
We also need to flatten the inputs for the testing data as well.
For the labels, we need to do something called one-hot encoding.
If you look in the REPL, you'll see what it did.
For each label, it replaced the integer with a column vector with a 1 at the index representing
the integer.
So in the first column, that 1 is in the 6th row.
While you'll recall that the first label is 5, not 6, that's because this column vector
starts at 0 and goes to 9.
So the 6th row represents the label for the number 5.
The one-hot batch function also concatenates all of these column vectors horizontally, so
the result is a 10x60,000 one-hot matrix made up of Boolean values.
All of those dots are zeros.
We need to do the same thing for the testing labels.
In the REPL, you can see that it's a similar result, except the testing data only has 10,000
labels.
Okay, we are now ready to define our model.
Unlike other machine learning packages that we've seen so far, the flux.jl package is
not an algorithm.
Instead, it's a deep learning toolkit that provides building blocks that you can use to
create your own custom deep learning models.
We'll go through the flux package in more detail in the next tutorial, but for now, let's
just use those building blocks to create our first artificial neural network.
There's a lot going on in this deceptively simple code.
Here's a visualization of what we just built.
This is a diagram of a neural network.
Specifically, this is an example of a multi-layer perceptron, or MLP, which is a type of artificial
neural network.
Although this model is more complex than any model that we've seen so far, the MLP is
considered relatively simple in the deep learning world.
We'll go through this diagram in more detail in the next tutorial, but at a very high level,
this diagram represents a model containing a lot of parameters.
We're going to feed our training inputs into this model, and then the model will try to
learn the parameters necessary to predict the training labels, just like any other machine
learning model.
In other words, the model will digest the data and calculate a loss.
Based on the result after a single epoch, the model will then update the parameter slightly
in order to reduce the loss by using an optimization algorithm.
Your model will repeat this process until you tell it to stop.
I'll cover all of those other coding terms like chain, dense, reilu, and softmax in the
next tutorial.
Let's move on to defining the loss function.
There are many different loss functions used in deep learning, and the flux.jl package
supports all of the major loss functions.
Today, we're using a loss function called cross entropy.
Again, more on this in the next tutorial.
Our model contains a lot of different parameters.
The flux.jl package initializes all of those parameters using random values.
Next, we need to select an optimization algorithm that will determine how our computer will
quote unquote, learn the data.
There are several different optimizers used in deep learning, and the flux.jl package
supports all of the commonly used optimizers.
Today, we're going to use an optimizer called Adam, which is short for adaptive moment estimation.
Okay, we are now ready to train our model.
The flux.jl package comes included with a handy utility that makes it easy to train
our model.
All we need to do is provide a for loop to repeat the training process over multiple
epochs.
Just a warning, depending on your computer's CPU, this may take several minutes to train.
I'm going to fast forward through those parts.
After an initial delay, you should see some outputs in the REPL, and if everything went
okay, the training loss should be decreasing over time.
We now have a trained model with lots of updated parameters.
So how do we use this model in order to make predictions?
Well, we can just run our test data through our newly trained model in order to get the
predictions.
So running our test data through our model results in a 10 by 10,000 matrix, which you
can see in the REPL.
It's a little difficult to read, but those crazy looking numbers are really small values
close to zero.
In each column, you should see a single value close to one.
The sum of each column adds up to 100%, and each row contains the probability of the
prediction.
Remember that the index numbers for the rows go from 1 to 10, but our labels go from 0
to 9.
So the first row is the probability that the image is a 0, and the second row is the probability
that the image is a 1, and so on.
In order to make it easier to work with these predictions, we can use the one cold utility
function from the flux.jl package, which is like the opposite of the one hot batch utility
function.
A one cold function converts a matrix into a column vector containing the index number
that has the highest probability value.
In order to convert index numbers into labels, we need to subtract one from each of the index
numbers.
You can see that our predictions are now contained in a column vector with 10,000 elements.
Now all we need to do is compare the predicted labels with the actual labels for the test
data.
So our little artificial neural network model achieved an accuracy score of 96.24%, which
is pretty amazing considering how difficult the challenge is.
Now before we go out and celebrate, let's take a look at the current best in class accuracy
score.
According to the Wikipedia article on the MNIST database, the highest average accuracy
score for any machine learning model is 99.83%, which was achieved in 2020 using something
called a convolutional neural network.
In any event, let's take a look at some of our misclassifications to see if we can gain
any insights.
So this table contains 10,000 rows.
The first column is the index number for both the predictions as well as the actual
label data.
The second column contains the predicted labels, and the third column contains the actual labels
from the test data sets.
The fourth column contains one for true if the predicted label has been classified correctly,
and a zero for false if the predicted label has been misclassified.
If you sort on column 4, you can see all of the misclassifications.
So there are 376 misclassifications, which sounds like a lot, but remember there are
10,000 test samples.
Let's take a look at the first misclassification, which has an index number of 9.
So I don't know about you, but I cannot read this handwriting.
I mean, what is that?
It's labeled as a 5, but does that look like a 5 to you?
Our model guessed 4, and you can kind of forgive it since I don't think I could have classified
this image correctly as a 5 just by looking at it.
Anyways, you get my point.
This is not a trivial classification problem, and it's amazing that any model can achieve
accuracy levels that are comparable to human beings, given the wide range in handwriting
styles.
So that was fun, right?
Before we go, let's plot a learning curve to see how our model did while training.
So this learning curve looks similar to other learning curves that we've seen in past tutorials.
Even though the code for this neural network looks very different, the overall learning
workflow is very similar to other machine learning algorithms.
Let's save this plot and recap what we just witnessed.
Today we became very familiar with the MNIST data sets, and we got a quick introduction
to the Flux.jl package.
After doing a little preprocessing work on the data, we immediately built an artificial
neural network model using the tools provided by the Flux.jl package.
Then we followed a similar workflow that we used with other machine learning algorithms
by defining a loss function, by initializing parameters, by selecting an optimizer, and
by training our model using a for loop.
But the actual details of that workflow are very different with a lot of new terms.
In the end, we were able to use this trained model in order to make predictions, like we
did with other machine learning models.
We were also able to calculate an accuracy score and plot a learning curve.
Hopefully, today's tutorial has left you both excited and confused.
After seeing what's possible with artificial neural networks, you probably want to use
it right away, but may be hesitant since you may be wondering what's going on under the
hood.
In the next tutorial, we'll revisit this code in order to gain a better understanding of
the concepts that make these artificial neural networks such a modern marvel.
So stay tuned for that.
Well, that's all for today.
If you made it this far, congratulations!
If you enjoyed this video and you feel like you learned something new, please give it
a thumbs up.
For more wholesome Julia tutorials, please be sure to subscribe and hit that bell.
If you like what I do, then please consider joining and becoming a channel member.
New tutorials are posted on Sundays slash Mondays.
Thanks for watching, and I'll see you in the next video.

Okay, great, welcome everyone and thank you for joining us.
This is the last distinguished lecture series for the Institute of Expansion AI for the
academic year.
We resumed again in September with a full program for the year.
As you also know, in parallel every two weeks we run the Expeditions in Expansion AI series,
which is designed to feature a lot of our Northeastern University experts and faculty
and so forth.
In two weeks, definitely join us for a talk by Silvio Amir on a super interesting topic
who's at the Curie College.
My name is Osama Fayad, I'm the Executive Director of the Institute for Experiential
AI and also Professor of the Practice in the Curie College for Computer Sciences and it
is my pleasure today to introduce Yan Lokhan.
Yan is a very well-known name in the field.
I've known him for many years, I think at one point in my life I interviewed at Bell
Labs or AT&T Labs and that's when he was there.
He is VP and Chief AI Scientist at META, also known as Facebook, and Silver Professor at
NYU affiliated with the Cuon Institute of Mathematical Sciences and the Center for Data
Science, which he actually founded.
He was the founding director of FAIR.
I learned this morning that FAIR used to stand for Facebook AI research, now it's changed
to META FAIR for Fundamental AI Research and of course he founded the NYU Center for
Data Science, received an engineering diploma from SEA in Paris and a Ph.D. from the Sorbonne
University.
After a post-doc in Toronto, he joined AT&T Bell Labs, which got renamed to AT&T Labs
in 1996 as Head of Image Processing Research.
He joined NYU as Professor in 2003 and META or Facebook in 2013.
He is the recipient of the 2018 ACM Touring Award along with Jeffrey Hinton and Yashua
Benjio and for those of you who don't know the Touring Award, it's essentially the equivalent
of the Nobel Prize for Computer Science, the toughest award to get from the ACM.
The award was for conceptual and engineering breakthroughs that have made deep neural networks
a critical component of computing.
He is a member of the National Academy of Sciences and the National Academy of Engineering
amongst many others.
His interests include AI, machine learning, computer perception, robotics and computational
neuroscience and I'm sure you're all eager to hear from Jan on what's been happening
with generative AI and what all the buzz is about, hopefully we'll get into the technical
details and immediately following his talk, we will do a fireside chat where I will try
to ask him some tough questions and then we'll also get questions from the audience.
By the way, we did get online questions from the audience.
We got 150 questions, so there's no way we're going to walk you through all of those.
So we'll see how much time allows us to answer.
Thank you and please join me in welcoming Jan to Northeastern University.
Thank you so much.
It's a real pleasure to be here and thanks for coming here so numerous or for listening
in online.
So I'm going to talk a bit about the state of the art in AI but also about the next step
because I'm always interested in the next step and how we can make machines more intelligent
and we need to figure out how to get machines that can not just learn but also can reason
and plan and current AI really does not allow current systems to do this.
So I'll try to kind of sketch a potential pathway towards such systems.
I can't say that we built it completely but we built some components and I'll go through
this.
So AI is in the news, everybody is playing with it at the moment, it's pretty amazing
how it works.
There's a lot of success, it's been very widely deployed, very much in many applications
that are behind the curtain but in some of them much more visible.
So LLMs have the advantage of being visible but for the last 10 years or so there's massive
use of AI and the latest development of AI for such thing as ranking for search engine
and social networks or for content moderation, things like that.
But overall machine learning requires a lot of data and the machines that we have are
somewhat brittle, specialized, they don't have human-level intelligence despite what
we may be led to believe.
So in short, machine learning sucks, at least compared to humans and animals.
We've been using supervised learning which really was the workhorse of machine learning
and AI systems until very recently.
Reinforcement learning is insanely inefficient but it works really well for games but not
many other things.
So one thing that has taken over the AI world in the last few years is something called
self-supervised learning which I will talk about at length.
But current AI systems are specialized and brittle, they make stupid mistakes, they don't
really reason and plan with a few exceptions for a game playing for example.
Compared to humans and animals they can learn new tasks extremely quickly, understand how
the world works, can reason and plan, have some level of common sense, machines still
don't have common sense.
So how do we get machines to reason and plan like animals and humans learn as fast as animals
and humans and we'll need machines that can understand how the world works, can predict
the consequences of their actions, can perform chains of reasoning with unlimited number
of steps, can plan complex tasks by decomposing them into simpler tasks.
So let's start with this idea of self-supervised learning, it's really taken over the world.
Every sort of top machine learning system today uses some form of self-supervised learning
as a first step to pre-train the system and it's used everywhere.
What does it consist of?
It's really the idea that instead of having, of training a system with an input and an
output which is the case in supervised learning or with an input and a reward which is the
case for reinforcement learning, you train the system to basically model its input.
You don't train it for any particular task other than capture the dependency between
different parts of its input.
So one thing you might do is for example take a piece of video or a piece of text, show
a piece of the video to the system and ask it to predict the missing piece like the continuation
of that video and after a while you reveal the rest of the video and you adjust the system
so that it does a better job at predicting.
So prediction really is kind of the essence of intelligence and to some extent by training
a system to predict, it doesn't have to be predicting the future, it could be predicting
the past or the left from the right, you're training the system to represent data essentially.
And that's been nothing short of astonishingly successful in the domain of natural language
understanding.
So every top performing NLP system today is pre-train the following way or with some form
of the following way which is a special case of an old idea called denoising autoencoder.
And the idea is that you take a piece of text, sequence of words from a corpus, typically
it would be a few hundred or a few thousand words long.
Those words immediately get turned into vectors but let me not talk about this for just now.
So the first thing you do is you corrupt this text, you remove some of the words and replace
them by blank markers or you substitute them for another word and then you train some gigantic
neural net to predict the words that are missing.
In the process of doing so the system has to basically develop some sort of understanding
of the text because if you want to be able to predict what word comes here you have to
understand the role of the word in the sentence, the type of word that comes here and the whole
meaning of the sentence.
So the system basically learns to represent text and the amazing thing is that just by
doing this you can train a system to represent the meaning of text in pretty much any language
as long as you have data.
With a single system you can have a system that represents the meaning of a piece of
text in any language.
So pretty cool, you can use this to build translation systems, systems that detect hate speech on
social networks or figure out what something talks about and the way you do this is that
you chop off the last few layers of that gigantic neural net and you use the representation,
the internal representation learned by the system as input to a subsequent downstream
task that you train supervised like say translation.
And it's really astonishing how well this works.
So from this to a generative AI system there's a small step particularly for text generation.
Text generation is a completely different thing which I'm not going to talk about but
although some systems use the same technique.
So what is a generative text generation system, a large language model?
It's a system of the type I just described except that when you train it you don't remove
random words in the text that you show at the input, you only remove the last one.
So you train the system to predict the last word in a sequence of words.
So show a sequence of words and then show the last word and train some gigantic neural
net perhaps with billions or hundreds of billions of parameters to predict the next word.
And you have to train this on trillions of text snippets, typically one to two trillion
for the biggest models.
Once you have that system you can use it to generate text using what's called autoregressive
prediction which is a very classical thing to do in signal processing.
So you take a piece of text called a prompt, you enter it into the system, you have it
predict the next word and then you shift that word into the input.
So now it becomes part of the input to the system and now you can predict the next word,
shift it in, predict the third word, shift it in.
That's autoregressive prediction.
And that's how all the big LLM's that everybody has played with work.
That's how they've been trained, that's how they generate text.
So those LLM's are kind of amazing in terms of the performance that they produce.
So again they're trained on something like one to two trillion tokens, a token is like
a word or a subword unit.
And there's a whole bunch of those models, most of which you probably haven't heard of
but there's a few that have become household names.
So we've heard of ChagGBT and GPT-4 from OpenAI which are kind of usable, barred from Google
and derivative of ChagGBT and GPT-4 from Microsoft, married with Bing.
But there's a long history of those things that goes back several years.
Time from Fer, Blenderbot and Galactica, Galactica was trained on the scientific literature
and is designed to help scientists write papers.
And a more recent one called LANA which is the code is open source, the model you can
get it on request if you are using it for research purpose.
And it's the same level of performance as things like ChagGBT but it's not fine-tuned,
you have to fine-tune it for application.
And in fact people have done this, Alpaca is a model which basically is a fine-tuned
version of LANA that was built by people at Stanford for answering questions and things
like that, instruction.
So they're pretty amazing, they surprised a lot of people in how well they work but
they make a lot of factual errors, logical errors, inconsistencies, limited reasoning
abilities, things like that and they are easy to, they're pretty gullible.
So you tell them, you know, what is 2 plus 2 and the system will say 4, you say no, actually
2 plus 2 equals 5, oh yeah, you're right, I made a mistake.
So they, you know, they kind of, they, they predict answers that would sound like someone
could produce these answers but the details might be wrong, okay?
So you can't really use them for factual answers but you can use them certainly for writing
aids and particularly it works really well for, for text or for, you know, standard sort
of templatized text that you need to, to write like, I don't know, there's a bunch of professors
here that have to spend quite a bit of time writing recommendation letters for students.
Very useful for that.
And very useful for code generation.
So the software industry is probably going to be revolutionized by such tools.
So this is an example of code generated from a prompt by the, the Lama 65 billion model,
the open source one.
So, you know, ask it, you know, find real roots of AX square plus BX plus C and the thing
just writes a function in Python or whatever, whatever you want or Reg X or whatever.
Who remembers the syntax of Reg X?
Like.
He kind of, you know, hallucinate text that might sound plausible or completely implausible
like this.
Did you know that Janne couldn't drop a rap album last year?
We listened to it and here is what we thought.
And the thing writes a review of my alleged rap album.
I'm not much of a rap person.
I'm more of a jazz person.
So when my colleagues showed me, showed this to me, I told them like, can you do the same
for like a jazz album that would be kind of more appropriate?
I mean, I'm a terrible performer, but, and I said, yeah, we tried already, but it didn't
work very well because there's not enough training data on the web of reviews or jazz
albums.
I found that incredibly sad, I cried.
So you need a lot of data to train those things, right?
In fact, the amount of data, like something like 1.5 trillion token that Lama is trained
on, it would take about 22,000 years for a human reading eight hours a day at every
speed to read the whole material.
So obviously those things can accumulate a lot of knowledge, at least approximately.
So yeah, writing assistants, code generation, first draft of a lot of stuff, they're not
good for producing factual and consistent answers, at least not yet.
So a lot of LLMs are being augmented or extended so that they can use tools like calculators
or database engines or whatever to search for information and then refer to the source.
They're not good at all for reasoning, planning, or even for arithmetic.
So but we are easily fooled by their language fluency into thinking that they are intelligent.
They're not that intelligent.
And they really have no understanding of the physical world because they're trained with
text.
And there's another flaw, which is a huge problem.
It's the fact that if you imagine that there is the set of all possible answers represented
by this sphere, disk, which is really a tree, right?
Every token you output, you have a certain number of options for what the token should
be, what the word is.
So it's a tree of all possible answers.
Within this tree, there is a small subtree that corresponds to correct answers for the
question being asked.
And imagine that there is a probability E for any token that is produced by the system
to be outside, to take you outside that tree of correct answers.
Once you go outside that tree, you can't come back because it's a tree.
So let's imagine that the probability per token is E. So the probability that a sequence
of N tokens would be correct is 1 minus E to the power N, making the assumption that
the errors are independent, which of course they're not, but that's kind of a crude assumption.
And so the problem with this is that it's an exponentially divergent process, this
autoregressive prediction.
Errors accumulate.
And if you produce too many tokens, the thing will sort of diverge away from the set of
correct answers, exponentially.
And that's not fixable with the current architecture.
You can fine tune those systems a lot to reduce E, but you're not going to make it
go away.
So I have a bold prediction, which is that the shelf life of autoregressive LLM is very
short.
My prediction is that five years from now, nobody in their right mind would use them.
So enjoy it while it lasts.
You'll be replaced by things that are better, OK?
And I'll hint about directions to kind of perhaps fix up those problems.
So this is a paper that Jake Browning, who's a philosopher, and I published in the Noema
magazine, which is a philosophy magazine, about the fact that a system that is purely
trained from text, from language, cannot possibly attain human level intelligence because much
of what humans know is actually derived from experience of the physical world.
This is true for a lot of human knowledge, but it's true certainly for almost the totality
of animal knowledge.
It's all about the world.
There's no linguistic related, no language related.
So linguistic abilities and fluency are not related to the ability to think.
Those are two different things.
There are some criticisms of autoregressive LLMs from people coming from the cognitive
science realm who say, like, this is not at all the way the human mind works.
There is essential missing pieces.
Other criticism for people who come from sort of more classical AI, pre-deep learning,
they say, like, you know, AI systems are supposed to be able to plan and reason, and
those LLMs can do it.
Or at least not, you know, they can do it maybe in very sort of primitive forms.
Perhaps they can plan things in situations that correspond to a template that they've
been trained on, but they're not so innovative.
So we should ask, how is it that humans and animals can learn so quickly?
And I've been using this diagram for quite a while now, several, many years, from Emmanuel
Dupu, who's a cognitive scientist in Paris.
And we tried to sort of make a chart of at what age babies learn basic concepts about
the world, so things like distinguishing between animate objects and inanimate objects, learning
the notion of object permanence, the fact that when an object is hidden behind another
one, it still exists.
Notion of rigidity, solidity, things like natural categories, babies don't need to
know the name of an object to actually know that there are different categories of objects
around four months or so.
And then it takes about nine months for babies to really understand that sort of intuitive
physics that objects that are not supported would fall, that, you know, objects have a
momentum, weight, friction, you know, knowing that if I push on this object, you know, light
objects like this, they're going to move, but if I push on an object that's heavier,
it's not going to move unless I push harder.
So things like that.
So if you show a six-month-old baby or a five-month-old baby, the scenario here on the left where
you have a little car on the platform, you push the car off the platform, it appears
to float in the air, a five-month-old baby will pay attention, a ten-month-old baby will
go like this because she understood that by then that objects are not supported as supposed
to fall and this object appears to be floating in the air.
So we can determine that her mental model of the world is being violated, okay?
That's how this chart was built.
So we accumulate as babies an enormous amount of background knowledge about how the world
works, mostly by observation, a little bit by interaction, when we start being able to
kind of grab things, but in the first few months, it's mostly just observation.
And we don't know how to reproduce this with this type of learning with machines.
Once we accumulate all this background knowledge, you know, in a number of years, learning a
new task like driving is very fast.
So any teenager can learn to drive in about 20 hours of practice, mostly without causing
any accident.
So the teenager doesn't have to run off a cliff to figure out that the car, that nothing
good is going to happen if you run off a cliff.
The mental model of the world is already there, okay?
We still won't have level five cell-loving cars.
So obviously we're missing something pretty big.
Any ten-year-old can clear up the dinner table and fill up the dishwasher.
We're nowhere near having robots that can do this and it's not because of mechanical
design.
It's because we don't know how to build the minds behind it.
So we're missing something big, right?
The past towards human-level AI is not just making LLM's bigger.
That's just not going to get us there.
It's been a common recurring error by AI scientists and engineers over the last six decades to
imagine that the one thing that they just discovered was the solution to human-level
AI, only to discover a few years later that no, there was actually a big obstacle, another
obstacle they had to clear.
It's a recurring history story in AI.
So common sense will probably emerge from the ability of machines to learn how the world
works by observation, the way babies and animals do it.
So I see three challenges for AI research over the next decade also.
Learning representations of the world and predictive models of the world, I'll say why
in a minute, and self-supervised learning is going to be the key component of that.
Learning to reason.
So psychologists talk about system one and system two.
System one is the type of control that our brains use to kind of react to something without
really having to think about it, like subconscious action.
So if you're an experienced driver, you don't have to think about driving.
You can just drive and you can talk to someone at the same time and barely pay attention.
So that's system one.
But then when you are learning to drive, you pay attention to absolutely everything.
You use your entire focus, consciousness, attention to drive, and that's system two.
And then the last thing is learning to plan complex action sequences, decomposing them
into simpler ones.
So I wrote a sort of vision paper about a year ago, which I posted on open review for
comments.
So you're welcome to comment on it.
I give a bunch of technical talks about it.
One of the earliest one was at Berkeley, but you are having a more recent version of it
right now, so you don't need to look at that one.
And it's based on what's called a cognitive architecture.
So basically, how can we sort of design a system with different modules so that those
modules may implement all the properties that I was telling you about so systems can perceive,
reason, predict, in particular predict the consequences of their actions, and then plan
a sequence of actions to satisfy a particular objective.
So the main components of the system is the key component, I would say, is the world model.
And the world model is what allows the system to predict ahead, imagine what's going to
happen.
This is, to some extent, what current AI systems don't really have.
Perception system basically gets an estimate of the state of the world and initializes
the world model with it.
The cost here is a really important module, so basically the entire purpose of the agent
is to minimize this cost.
So the cost is something that uses a measurement of the state of the agent, particularly the
prediction from the world model, and predicts whether an actuum is going to be good or bad.
And the entire purpose of the agent here is to figure out a sequence of actions, so this
is taking place in the actor, figure out a sequence of actions, such that when I predict
what's going to happen as a consequence of those actions using my world model, my objective,
my cost function, will be minimized.
So if my cost function is, so the cost function is basically our measures of discomfort of
the agent.
Biological brains have things like that in the basal ganglia, so this is the thing that
tells you when you're hungry, for example, or you're hurting.
So nature tells you you're hungry, nature doesn't tell you how to feed, you have to
figure that out by yourself, perhaps using your world model and your planning abilities.
So this is the same thing here, imagine this is a robot and the robot battery are kind
of starting to get drained, so there's a cost function here that says be careful, you're
running out of power.
And so the system, according to this world model, would say, well, I can recharge my
battery by plugging myself into a socket, so it figures out the sequence of actions
to plug itself into a socket and that will eventually minimize the cost function that
just appeared.
So in fact, there's two ways to operate that system one is the kind of system one where
the system makes an estimate of the state of the world, run this to a perception system
called an encoder here, produces an estimate of the state of the world called a zero and
that runs into a neural net called a policy network that just produces an action and the
action goes into the world.
LLMs are like this, they are system one, you give them a pump, that's X, they produce
an action, that's the token they predict, that goes back into the world and the world
is very simplistic here, it's just you shift in the input.
So no reasoning necessary, here is system two.
So you use the same system here and this is a sort of time-enrolled version of the system.
So we have the world model, the world model is this green module and the different instances
of that green module are the state of the system at different time steps, so think of
it as like a recurrent net that you unfolded, so it's really the same module at different
time steps.
What the world model is supposed to be able to predict is given the representation of
the state of the world at time t and given an action that I'm imagining taking, what
is going to be the predicted state of the world at time t plus one.
So I can imagine a sequence of actions that I might take, imagine the effect on the world
using my world model and then I can plug the state of the world over this trajectory
through my cost and measure whether my cost is going to be minimized by this action sequence,
my objectives.
So what I should do is run some sort of optimization procedure that will try to search for a sequence
of actions that minimizes the cost given the prediction given to me by the produced by
the world model.
This type of planning is very classical in optimal control.
It's called model-productive control.
In classical optimal control, the model is not learned usually, it's handcrafted.
Here we are thinking about a situation where the world model is learned by, for example,
watching the world go by, by video, but also by observing actions being taken in the world
and seeing the effect.
So to get a good accurate model here, I'm going to have to observe the state of the
world, observe, like, take an action and observe the effect or observe someone else
take an action and observe the effect.
Let me skip this for now.
Ultimately what we want is a hierarchical version of this because if you want the system
to be able to plan complex actions, we can't plan it at the lowest level.
So for example, if I want to plan to go from here to New York City, I would have to basically
plan every millisecond exactly what muscle actions I should take, okay?
And it's impossible, right?
You cannot plan an entire trip from here to New York City millisecond by millisecond,
partly because you don't have a perfect model of the environment, like you don't know if,
when you're going to walk up the room here, whether someone is going to be on the way,
in the way and you're going to have to go around.
So you can't completely plan in advance, right?
So what we do is we plan hierarchically and say like, okay, I want to go to New York City,
so the cost function at the top here measures my distance to New York City.
And the first thing I have to do is go to the airport and catch a train, or go to the train
station and catch a train.
Go to the airport, catch a plane.
So the top predictors are predictors that are high level that says, oh, okay, if I catch
a taxi, it might take me to the airport.
If I catch, or to the train station, then if I catch a train, it will take me to New
York City.
Okay, so you have those two hidden actions, those Z variables here.
And they define a cost function for the next level down.
So if the first action is I'm taking a taxi to the train station, the lower level is how
do I catch a taxi here?
I go down in the street and hail a taxi.
No, this is Boston.
I need to call it Uber or something, okay?
So I go on the street and I call it Uber.
How do I go in the street?
There's going to be lower levels.
I have to get out of this building.
How do we get out of this building?
Have to walk through the door.
How do I work through the door?
I have to put one leg in front of the other, over obstacles, and all the way down to millisecond.
I also control for a short period, which is replanned as we go, okay?
No AI systems today can do any of this.
This is completely virgin territory, okay?
There's a lot of people who have worked on hierarchical planning, but in situations where
the representations at every level are hardwired, they're known in advance.
They're predetermined.
It's sort of like the equivalent of a vision system where the features at every level are
hardwired or designed by hand.
There's no system today that can learn hierarchical representations for action plans.
So that's a big challenge.
The cost function, so here's what's important here.
A lot of people today are talking about the fact that AI systems are difficult to control
and that's terrible, maybe toxic, you know, various things.
The system I describe cannot produce outputs that do not minimize the objectives.
And so if you have terms in the objective that guarantee certain conditions, that system
will have no choice but obeying those conditions, okay?
So having a system that is designed like this that whose output is produced by minimizing
a set of objectives according to a model will basically help guarantee the safety of that
system because you can hardwire intrinsic objectives on the left here that basically
guarantee the safety and the system cannot escape the satisfaction of those constraints.
So let me take a very simple example.
Let's say someone figures out how to build a domestic robot that can cook.
This robot, you know, will have to be able to kind of handle a kitchen knife and you
might put a cost function that says don't flail your arm if you have a kitchen knife
in your arm and there is people around, okay, because it's dangerous.
So you can imagine putting a lot of kind of safety conditions in those systems to make
them steerable.
So I don't think the problem of making AI systems safe is such a huge problem that some
people who are very vocal are saying it is that, you know, AI is going to kill us all.
It's not going to kill us all.
We would have to screw up really badly for that to happen.
Okay, now here's the thing.
How do we build the world model?
And that's basically the biggest challenge that we have at the moment.
How do we build a system that can predict what's going to happen in the world?
For example, by training itself to predict videos.
Now, the problem with predicting videos is that the world is not entirely predictable.
It may not be deterministic, but even if it were deterministic, it wouldn't be completely
predictable.
So in fact, here is an example here.
If you take a video, this is a top-down video of a highway that looks like cars driving
around just following the blue car, and you train a neural net to predict what's going
to happen in the video after the first few frames, it produces blurry, it makes blurry
prediction because it can't predict if the car that's behind you is going to accelerate
or brake or change lane or whatever.
So it makes an average of all the possible future, and that's a blurry image.
Same with...
This is an old paper where we attempted to do video prediction using neural nets, and
the predictions are blurry because there's too many things that can plausibly happen
and the system can only predict one thing, so it predicts the average.
So that's no good.
The solution to this is what I call a joint evading predictive architecture, and this
is really the most important slide of the talk.
So the normal way to make predictions is through a generative model.
What's a generative model?
It's a model where you have a bunch of variables you observe, let's say the initial segment
of a video.
You run it through an encoder and through a predictor, and the predictor predicts why,
which is, let's say, the continuation of that video.
And you have some cost function that measures the discrepancy divergence between the predicted
why and the actual why you observe.
This is when you train your world model.
It could be that a predictor has an action variable that comes in, but in this example
there isn't.
So examples of this are things like variational autoencoders, mass autoencoders, or denoising
autoencoders, which is a more general concept.
And so basically all NLP systems, including at LMS, are of this type, the generative models.
But here is the thing, you don't want to be predicting every detail about the world.
Here you have to predict every single detail about the world.
So it's easy if it's text, because text is discrete, so predicting the next word.
I cannot predict the next word from a text, but I can predict within 10 possible words
some probability distribution of the, over all the words in the dictionary of which word
comes next, right?
They can represent distributions over discrete variables.
I cannot do this over the set of all possible video frames.
I cannot usefully represent a distribution over the set of all possible video frames.
So I can't use the same trick for a video that is used for language.
The reason why we have LMS that works so well is because text is easy.
Language is simple.
It only popped up in the last few hundred thousand years anyway, so it can be that complicated.
And it's also processed in the brain by two tiny areas called the Wernicke area for understanding
and the Wernicke area for production.
What about the rest of the brain, the prefrontal cortex?
That's where we think, okay?
That's not part of LMS.
The LMS are perhaps good models of Wernicke and Wernicke, but that's it.
So what I'm proposing here is to replace this generative architecture by a joint invading
architecture.
And the essential characteristic of it is that the variable that you want to capture the
dependency of with respect to X goes itself through an encoder.
And the encoder eliminates the relevant information that is not useful for anything.
Okay?
So for example, if I had a video of this, if I was shooting a video of the room here and
then panning the camera and asking the system to predict what's the rest of the room, it
would probably predict that the rest of the room looks like the initial part that there
would be a lot of people in different seats, but it couldn't predict your age, gender,
hairstyle, clothing, or the precise texture of the floor or things like that.
So there's details that cannot possibly be predicted, and one way to avoid predicting
them is to basically eliminate that information from the variable to be predicted through
an encoder.
So that's the joint invading architecture or predictive architecture because it has
a predictor.
Now, there's an issue with this thing, which is that if you train a system with, let's
say, a piece of video and the following piece of video, and you just train it to minimize
the prediction error, you train the whole thing, it collapses.
It collapses.
Basically, the encoders ignore the inputs, they produce constant vectors for Sx and Sy,
and the predictor just needs to map Sx to Sy, and it's a constant, so it's super easy.
Okay?
And so the question now is, how do we prevent this from happening?
How do we prevent it collapse?
It doesn't happen with generative models because they can't collapse.
So there are three flavors of those joint invading architectures, a simple one where
you're basically trying to make the two representation of Sx and Sy identical, so, for example, X and
Y are two different views of the same scene, and you want Sx to represent the content of
the scene, so it doesn't matter where you look it from, you just want to make the representations
equal.
When the encoders are identical, this is called a Siamese network, this is another idea that
goes back to the early 90s.
You have deterministic joint invading architectures, and then you have joint predictive architectures
that may be nondeterministic where the predictor function has a latent variable that could
be drawn from a distribution or taken within a set that would allow that system to make
multiple predictions if necessary.
Now we have to ask ourselves the question of how do we train those things, and I'm going
to use a symbolism here where I've used the rectangles and squares of cost functions energy
terms, the circles are variables, observed or not, and those symbols here are deterministic
functions, imagine a neural net, okay, trainable.
You may have to hardwire some cost functions in the system to have it, to drive it to focus
on aspects of the input that are important, so that's the purpose of that C cost function
at the top.
But to explain how to train those things, I'm going to have to explain a little bit
what energy based models is about, because the classical kind of probabilistic modeling
in machine learning kind of goes out the window when we use the joint invading architectures.
So what's an energy based model?
An energy based model is a learning system that captures the dependency between two sets
of variables x and y through an energy function that is supposed to take low values, low energies
around data, training samples, so imagine those black dots are training samples.
You want that energy function f of x, y to take low values around the training samples
and then higher values outside.
And that system will capture the dependencies between x and y.
If I give you a value of x and I ask you what can be the possible values for y, you're
going to tell me, well, it's either this or that or maybe that other thing at the top.
So it's not a mapping from x to y.
It's an implicit function.
And by figuring out what value of y minimizes the energy function, you can do inference.
You can infer y, possibly, but you don't necessarily have to do that.
So that's energy based model.
It's kind of a weaker form of modeling than probabilistic modeling.
And so now the learning problem becomes how do you train this energy function, which is
going to be some big neural net, so that the energy takes low value around the training
samples and high values outside.
If you're not careful, you're going to get a collapse so that the same type of collapse
I was telling you about before, if you just pull down the energy of the training samples,
minimize the prediction error in this joint-eventing architecture, you're going to get zero energy
for everything.
That's not a good way to capture the dependencies.
You have two classes of methods, contrastive methods.
So contrastive methods consist in generating those green points which are outside the region
of data and then push their energy up while you push down on the energy of the data points.
So that's going to create a groove in the energy surface and the system will have captured
the dependency between x and y.
But there's an alternative here which is regularized methods where the point of those
methods is to minimize the volume of space that can take low energy so that when you
push down on the energy of data points, the rest of the space takes higher energy because
there is only a small amount of, a small region of low energy to go around.
So those are the two classes of methods.
Every method you ever heard of in machine learning can be viewed as one of those two.
Most probabilistic methods actually belong to the contrastive category.
Anything that uses Monte Carlo sampling, for example, is contrastive.
And then things like sparse coding and k-means and things like that are more on the regularized
method side of things.
Okay, so I'm asking you to do four things.
Abandoned generative models in favor of the joint embedding architectures.
So generative models are the most popular thing at the moment.
Forget about it.
At least if you're interested in getting to the next step in AI.
Abandoned probabilistic models because if you have those joint embedding architectures,
you cannot actually use it to derive a p of y given x.
The only thing you can use is this sort of energy-based view.
Abandoned contrastive methods in favor of those regularized methods, which I'll talk
about a bit more.
And then something I've said for many years now, abandoned reinforcement learning because
it's too inefficient.
So those are some of the pillars of machine learning.
And I realize this is not a very popular opinion here, but okay.
So what about those regularized methods?
I'm just going to give you one example.
There's a whole bunch of them.
There's like a dozen of them, but I'm just going to give you one called Vicreg.
And the basic idea of it is to prevent those representations from collapsing, we're going
to use a criterion that attempts to maximize the information content that comes out of
those representations.
Okay, so we're going to measure the information content in some way and then maximize the
information content or minimize the negative information content.
We're going to do this for both SX and SY.
We're also going to minimize the prediction error.
And if we have a latent variable, we're going to have to minimize the information content
of that latent variable.
I can't explain why because it would take too long, but you have to do that also to
prevent another type of collapse.
I'm going to focus on how you do that.
So the sad news is we don't have good ways to measure information content or we don't
have any good ways to estimate lower bounds on information content so that if we push
up on this lower bound, the information content will go up.
We only have upper bounds for information content.
So we're going to do a very stupid thing which is push up on the upper bound of information
content and hope the actual information content will follow.
And it works.
So there's a simple way to prevent the encoder from completely collapsing, which is to insist
that every variable in SX, SX is a vector, and you insist that every variable as measured
over a batch has a standard deviation that is at least one.
So this is the cost that you see at the top here.
Measure the standard deviation of each component of SX and put it in a hinge loss so that the
standard deviation is at least one.
So that prevents the system from completely collapsing, but it's still cheap by making
all the components of SX equal or correlated.
So the second term says, I want to minimize the off-diagonal terms of the covariance matrix
of those vectors measured over a batch.
So I want pairs of variables to be uncorrelated.
So basically, the collection of those two criterion says, if I measure the covariance
matrix of those vectors, SX and SY, coming out over a batch, I want the covariance matrix
to be as close to the identity as possible.
There's a number of different methods that have been proposed that are kind of similar
to this, Barlow-Twins.
So this one is called Vicreg from my group at META in collaboration with Jean-Ponce.
And then variations of it, but similar methods from Berkeley in Yimai's group at Berkeley,
called NCR squared.
Yeah, maybe one minute.
Yeah.
So this works pretty well.
And I'm going to not bore you with tables of result that show you how well it works.
Only to mention something else, which is another method to do this kind of self-supervised running,
which is closer to this JEPA architecture, called iJEPA.
So this is for learning features for images without having to do the documentation, but
basically through masking.
And this works amazingly well.
It's very fast.
It's a new method.
The paper is on archive.
I don't have time to explain how it works, but basically you run an image through two
encoders.
One is the full image, and the other one is sort of a masked image, partially masked image.
You run them through the same encoder, or very similar encoders, and then you train
a predictor to predict the full feature representation of the full image from the representation obtained
from the partial image.
And just doing this produces really good features for images.
You get really good performance on object recognition in images and stuff like that.
Again, tables that show you that's true.
But I'm coming to the end.
So the reason for training those JEPA is to build world models.
So architectures are this type.
So this is a JEPA, but it's also a world model that, given an observation about the state
of the world, is going to be able to end an action or imagined action in latent variables.
It's going to predict what's going to happen next in the world.
And once the time passes by, we're going to observe what happens and then perhaps adjust
our system to train.
But we want to use a hierarchical version of this where we can have a higher level of
abstraction representation that will allow us to make predictions further in the future.
I can't tell you the details of how I'm going to get to the train station, but I know I'm
going to have to be at the train station around 4 p.m.
So that's the high level.
And we have early experiments with various complicated neural net architectures, which
I'm not going to detail, to train from video, try to predict basically what's going to happen
in a video using warping and stuff like that, and it works really well.
But in the end, what we'll have is a hierarchical system from which we can do hierarchical planning
and then we'll have been trained to predict what's going to happen in the world as a consequence
of actions or latent variables that we can observe, that we can infer.
And those systems will be able to plan and reason and will be controllable because their
behavior is entirely controlled by the cost functions we ask you to minimize.
And so much more controllable than current LNMs.
And that's pretty much the end.
So self-supervised learning is really the ticket.
Learning and certainty can be done with this energy-based model method and using the joint
embedding architecture that allows us to avoid predicting all the details that are irrelevant
about the world.
Learning world models from observation and interaction and then reasoning and planning
is done by basically gradient-based minimization with respect to actions.
And that's it.
Thank you very much.
Thank you, John.
Thank you, John, for the great talk.
Now we'll have the second part, which is the FASA chat between John and Osama.
So please.
Okay.
Bye.
Thank you very much, John.
It was actually truly inspirational because it is definitely different than your typical
machine learning talk.
So I enjoyed that.
Well, I told you to throw away all the digit pillars of machine learning, so yes.
So I've collected a bunch of questions, some coming from the audience, some coming from
our institute and our faculty.
And we'll try to go through them in 20 minutes or whatever we can cover.
Actually I would commit to answering every question on social media, but because we got
150 questions, I'm afraid to commit my time or yours to this at this point, but we'll
try our best.
So I'll start with my first question.
It's been a long-standing wisdom in statistical inference and probabilistic reasoning that
when the number of parameters of a model gets large enough, you kind of lose your ability
to generalize and you start just memorizing data, and we all know that that's no good.
That's just too detailed, the bias variance trade-off.
But somehow deep learning seems to have broken through this barrier when we went from regular
neural nets to the deep nets.
And is there an intuition or understanding today as to why this is working in LLMs with
hundreds of billions and now trillions of parameters?
Right.
Well, the fact that it is working, that you can train ridiculously over-sized neural net
and it will still work reasonably and generalize, is dumb-founding so much that it contradicts
every single thing that has been written in every statistical textbook that you should
never have more parameters than you have training samples, right, if you're fitting a polynomial
or something like this.
But we knew experimentally, even in the late 80s and early 90s, that you could make those
neural nets pretty big and even if you didn't have a huge amount of training data, it would
still work pretty well.
There was just no theoretical explanation.
So the theorists told us, you're wrong, you're stupid, this cannot possibly work, so I'm
not going to believe your results.
And that's in part what made it very difficult to get neural nets accepted in the late 90s
to the 2000s.
But it turns out there is a phenomenon that has since been named double descent, which
is that if you increase the number of parameters in a model for a constant size training set,
your training error, of course, is going to go down, right, to zero, probably.
But your test error is first going to go down, go through a minimum, and then go up.
And you start having parameters, a number of parameters that is commensurate with the
number of samples that you have.
So that's when the model starts to be over-parameterized.
And it goes up, but here is the thing, if you keep going up, if you keep making the
model more complex, the tester will go down again, it will go through a maximum and then
go down again.
It's called the double descent phenomenon nowadays.
And it will do this if you regularize the parameters somehow.
You don't necessarily need to regularize explicitly, because neural nets have some sort of implicit
regularization in them, but you see this phenomenon.
Even works if you fit a polynomial, right, fit a 10-degree polynomial with 11 data points.
And your fit will be horrible, right, because the polynomial has to go through every single
point and it's going to go wild in between.
But if you increase the degree of the polynomial to something like 20 or 30, and you regularize
the coefficient, your error goes down again, your tester goes down again, the fitted polynomial
goes through every point, but it's less irregular than with just a degree 10.
So this existed all along.
It's just that people didn't realize that it was a thing, or at least people who were
not practitioners of neural nets who had realized this was a thing.
But do we have any explanation why this is a thing?
So there's a lot of conjectures.
There is some theoretical work.
Some people claim it's about the dynamics of gradient descent.
There is some sort of implicit self-regularization in neural nets that occurs, whereby the system
kind of recruits just a number of virtual parameters that it needs somehow.
Some say it's regularization due to stochastic gradient.
Stochastic gradient descent is noisy, and so perhaps that forces the system to find
robust minima in the objective, in the loss, that generalize better.
It's not clear.
There's a bunch of different things.
Definitely one of the mysteries that keep us interested.
This question comes from Raman Chandrasekharan, or Chandra, who's one of our senior research
scientists in Seattle.
How long before LLM, and maybe, I don't know, models in general, can genuinely start saying,
I don't know the answer to this question, as opposed to attempting to guess the right
autocomplete anyway, because that's what it's programmed to do?
Yeah, so I don't think current LLMs can really do this at the moment.
I think it's probably possible with architectural attack that I show, because if there are no
good minima to the objective that the system is attempting to minimize to produce its output,
it's going to say, well, I found this thing, it seems to be minimizing this objective,
but not very well, so probably it's not the right answer you were looking for.
Or by the shape of the minimum, of this energy minimum, perhaps you could say, if it's really
a sharp minimum, then that's the one answer that corresponds to the question.
If it's kind of a shadow minimum, maybe there are multiple answers that are possible, so
you might be able to attribute, to map energy levels to, of different answers to confidence
levels.
Yeah, two, this is a question from me, I guess, two, two aspects of critical importance to
say GPT or large language models that are not talked about a lot by the companies who do them
are data curation, so getting that clean data, that balanced data, that representative data,
which by the way, counter to popular belief, open AI spent a lot of its money on, on curating
just that right corpus so that they can do the training reliably, and the, the second part,
which is something we're big believers in at the Institute for experiential AI, experiential AI
stands for AI with the human in the loop, having that human intervention through relevance feedback
that we know now open AI is doing and has been doing, and some of the queries that are actually
taken over by humans at some point when they make enough errors to come back, but the good thing
is they learn from them, and we think that's a great practice. Why do you think the companies
don't want to talk about the importance of the data and the importance of the human in the loop?
I don't know if they don't want to talk about it. I mean, it's clearly very expensive to
collect, curate data to produce a good, a good data line, but in my opinion, it's doomed to failure
in the, in the long run for two reasons. The first one is the curation requires going through this,
you know, enormous amount of, of data that you want to train the system on, and, and any data
you eliminate, you know, it's less training data for your model. But the second thing is
even with human feedback, human feedback that rate, you know, different answers or, or, or fine
tune the system for certain question and answers, sort of manually curated. If you want those systems
ultimately to be the repository of all human knowledge, the dimension of that space of all
human knowledge is enormous, and you're not going to do it by, you know, paying a few thousand people
in Kenya or in India rating answers. You're going to have to do it with millions of volunteers
that, you know, fine tune the system for all possible questions that might possibly be asked,
and those volunteers will have to be vetted in the way Wikipedia is being done, right? So think of
LLMs in the long run as a version of Wikipedia plus your favorite newspapers plus the scientific
literature plus everything. But you can talk to it. You don't have to read articles, you can just
talk to it. And so if it, if it's supposed to become the repository of all human knowledge,
the, the thing has been trained to do will have to be curated by, by quite sourcing the way
Wikipedia is to cover all the possible things that may be, may be covered. This is a very strong
argument for having open source base models for, for, for LLMs. So in my opinion, the future is
inevitably going to be that you're going to be, you're going to have a small number of open source
base LLMs that are not trained for any particular application. They just are, you know, they train
on like enormous amounts of data that requires a lot of money. So you're not going to have 25 of
them. You're going to have two or three. And then actual applications are going to be built on top
of it by fine tuning those systems for particular vertical applications. That's the future.
Sadly, in the industry, there are people who are lobbying governments to actually make the
open sourcing of large, large scale LLMs illegal. What they're worried about is,
you know, potential misuse of LLMs by bad actors, potential users. So some people in,
in the U.S., for example, are worried, oh, if we open source our LLMs, you know,
China and North Korea and Iran will put their hands on it. And that's going to be bad.
And then some people are worried that, you know, the real powerful LLMs are going to be
super intelligent and destroy humanity, which I think is preposterous, even though some of my
friends that I respect actually believe this. So I think it would be really, really bad if those
lobbying attempts succeed. I'm very much in favor of a future with open base models.
And there's going to be bad actors, but there's going to be countermeasures against them.
It's going to be, you know, or powerful, good AI cop against their nefarious AI, essentially.
So let's shift to this trend. And this is, I've merged a question from Jimmy Shanahan
from our AI Solutions Hub with questions from Tomo Lasovic and Ken Church at EAI.
The trend nowadays seems to be heading towards bigger is better, more compute, more parameters.
There's been some studies even suggesting that by open AI themselves that they're,
they're moving at a pace, at a pace faster than Moore's law, even though now they seem to be
normalizing towards it, although Moore's law itself is slowing down. So the real question here is,
how long can this go on? And will we ask them, what do you think? I know that we may not have
the final answer here, but it seems crazy. Like all you have to do is wait a few weeks and you
hear about the next big model. Well, so actually in the last few months, you've seen a decrease in
the size. So Lamma, for example, the 13 billion version of Lamma in terms of raw performance
on standard benchmarks is actually better than GPT-3, which has 175 billion parameters. And so
it's not clear that bigger is better. With the architecture I proposed, I think you can get
away with smaller, smaller systems that perform at least as well. The reason being that when you
train in a current autoregressive LLM, you have to train it to not just accumulate knowledge,
not just predict the next word, but also solve a lot of problems. So basically, you know,
know how to produce the right answer when you specify the question in the prompt. And so everything
is wrapped into the weights of that single model. Whereas in the model I proposed here, the architecture
I proposed, the word model is just a word model. The task is specified by the objective function,
okay, which may include the prompt. So it may include the representation of the prompt.
And so you're separating different things. You're separating the inference
procedure that produces the output from the word model, the sort of the mental model of
the world that the system uses from the task itself, which is specified by the objective.
And you can probably get away with smaller networks for the same performance.
But yes, I mean, there were a few years ago, you know, models by Google that had like a trillion
parameters. There were basically multiple models that were stuck together with some sort of, you
know, the palm gating, yeah, between them, they've kind of backpedaled on this a little bit.
If you want the system to be practical, like to be used by everyone, you can't make them
like a trillion parameters. Right now, it'd be just too expensive. So you have to minimize their
size. Now you can run things like Lama 7 billion on a Mac. You know, you can run on a laptop.
You can't train it on a laptop, but you can run it. Yes. So clearly, you believe you're advocating
for a different view what the machine learning and AI community should be, should be doing,
as opposed to what they are doing today. That's the story of my career. Yes. And this question
is coming from Ken Church. A former colleague from AT&T. Yes, from AT&T. He is at the Institute
for AI in Silicon Valley. Do you believe, well, I guess the question is, how long do you think
it will take to pivot the field from where it is to where you would like it to be?
Well, last time I tried, it took 15 years.
If not more, actually, depending on how you count, might have been 20.
So I don't know. I think, I see a phenomenon in the kind of, this is a sociology of science
question. When there is something that seems to work, everybody gets excited about it and is a
fashion trend type phenomenon where, you know, every paper, you know, written is about this
trend. I saw this in computer vision, you know, back in the early to mid 2000, everybody was
working on boosting, you know, that was the thing, like, you know, you had to work on boosting for
computer vision. And then, you know, someone in 2006 and 2005 came up with a different way of
doing vision using dense features like sift and stuff like that, using unsupervised running for
a middle layer and then an SVM on top. And all of a sudden, everybody was doing this. Okay. And then
starting in 2013, everybody started using convolutional nets.
That came from results. So now we are in a phase where everybody is focused on LLMs.
And if you don't work on LLMs, nobody wants to talk to you. But it will change.
So you think it's 15 years? No, I think it's more like five. I made that prediction that
autoregressive LLMs will probably... Five years, that's true. They're doomed. Yeah. I mean,
I might be wrong, obviously. We will hold you to that. I'll come back and revisit in five years.
Maybe it's a wishful thinking. Self-fulfilling prophecy, perhaps.
A question for something different here from Sam Scarpino, director of AI and Life Sciences
at the Institute for Experiential AI. What are the biggest gaps on the education side for
graduates of higher education in AI and, in particular, the new directions AI is taking?
What do you think is missing? So I think what's missing... So it depends which
major you're following. Most computer science curricula in the U.S. are very weak in mathematics.
The requirements for mathematics in a typical CS degree, the minimum requirement is very,
very small. It's one course in discrete math and perhaps in algebra, if you're lucky.
Maybe a probability if you are courageous. But what about optimization? That would be
kind of something that would be very useful. And then there is courses in physics because
the mathematics of inference and variational auto-encoder and stuff like that. Graphical models,
et cetera. The mathematics of this is from statistical physics. And so if you have a choice
between taking your course in, I don't know, mobile app programming or quantum mechanics,
take quantum mechanics. I'm not kidding.
This is a question that came from the audience and a few of the people at the Institute.
Your thoughts on the current, you know, these recent congressional hearings where
certainly seems like much of the testimony by some ultimate was understandably self-serving.
You know, they need to be allowed to compete and have their way of working protected. At the
same time, he's encouraging the rest of the community to be open source.
What would you have said to Congress? Have you been on those hearings?
I was not invited. I was not invited to the White House either before that.
So what I would have recommended is that if you want a vibrant ecosystem on top of current
AI technology, you need to have sort of open source based models on top of which an industry
can be built. And that industry will build vertical applications for particular domains
on top of a base model. You don't want to have 25 companies selling 25 different base models
and keep them closed source. If you want an industry to be built on top of it,
the infrastructure has to be open. Because that's the only way to really sort of know what you're
doing essentially. And to have some control about your future. You can't just go like this and
Unix versus Windows. Right. So if you go back to the history of the Internet, there was a similar
story where back in 1992 when Bill Clinton and Al Gore started to figure out like what,
how do we build the information superhighway? They went to see the big communication companies
like AT&T and AT&T told them, oh, leave it to us. We'll build the stuff. It's going to be
ATM and ISD enter the home and blah, blah, blah. It'll be wonderful. And you'll have to pay $5 per
hour. And Al Gore said no. He said we're going to make what was an ARPANET that became the Internet
basically available to the public and delocalized and self basically open in terms of standard.
And no company is going to control it. And that was a really, really good idea. We can thank
Al Gore for this. The world can thank Al Gore, not just the U.S. He did invent the Internet.
And then a similar story happened several years later when people started to realize that you
could use, you know, graphic browsers like Mosaic and Netscape and stuff like that. Right.
When the World Wide Web became popular. So there was a war between Sun Microsystems
and Microsoft. Sun Microsystems said, oh, we're going to sell you servers running Solaris,
a version of Unix with, you know, a web server infrastructure and Java. And you're going to
be able to build like anything you want. Microsoft said, no, it's going to be Windows NT with the
IIT web server and the ASP website, you know, server side protocol framework, whatever. They
both lost. Sun Microsystems went bankrupt, was sold for parts to Oracle. And Microsoft essentially
exited the market. One was Linux and Apache open source. And the reason is because it's such a
essential basic infrastructure that it has to be open. It progresses faster if it's open.
And it's more reliable. It's more secure. I mean, there's all the advantages. And, you know,
it's easier for startups to build on top of it. So in the future, we're going to see AI systems as
basic infrastructure. All of our interaction ten years from now with the digital world will be
through an intelligent virtual agent that will be with us all the time. It's like every one of us
will have a staff of intelligent people working for us. Okay? We shouldn't be threatened by the
fact that those things would be smarter than us. Like everybody that, you know, is working with me
at fair is smarter than me. So I don't feel threatened by that. You're not a very good
manager if you're threatened by people who are smarter than you. So your purpose actually should
be to hire people, only people who are smarter than you. But anyway, so we're going to have
those intelligent systems that are going to be under control that are going to help us, you know,
daily lives. And we need those systems to be open because if it's kind of a closed system controlled
by some company in California, it's going to be able to control our entire knowledge and data
diet. And that's just too dangerous. And it's not necessary. It's necessary for a search engine or
a social network because it has to be centralized for various reasons. But for an agent like this,
it could run on your local device. It could run on your laptop. You don't have to talk necessarily
with big servers in California. You don't want to give all your, you know, deeper secret
secrets to that. So it's going to have to be an open platform for that reason.
If nothing else, governments around the world are going to insist that this is the case.
So that's why I would tell Congress, make it so that like, don't ban open source
edit ends. They're not going to destroy humanity. Yeah, they're going to be bad actors, but, you
know, you can have countermeasures and make it open. It's the only way to make it safe.
I'll ask, we'll make this a quick question with a quick answer. And then I know we have some
questions live, so we'll switch to those. In a way, you kind of answer this question when you
said LLMs are doomed. But if LLMs were to become perfect, at least in language, would that ever
give us insight into how language and natural language understanding works? The language model
today is distributed over these billions of parameters. And do you think we'll ever have
an understandable LLM, like, for example, we use PCA to understand what regression is doing?
Or is that hopeless? At some point, I think it's going to be right to be hopeless. I mean, we'll,
we'll probably learn a lot about, you know, how the systems represent data and like how they manipulate
it and stuff like that. So this is not opaque, right? We can completely kind of, there's complete
visibility on how those systems operate. Now, the question is understanding really how
those decisions are being made. So I'm actually not particularly interested in those questions,
like, you know, as long as they work properly. The same way, I'm not particularly interested
in figuring out exactly how the brain works. I'm more interested in figuring out how the brain
builds itself so that it works, right? So I'm more interested in learning than in
studying the result of learning if you want. So it's the same for, for those, for those systems,
more interested in how you get them to learn what you want and how to solve the problem in the end
is kind of considerably less interesting in my opinion. But at some point, they're going to be,
you know, super intelligent, repository of all human knowledge. You know, it's going to be too
big for us to kind of comprehend at a deep level. Fair. And by the way, I failed to acknowledge
that question came from Walid Saba, who's one of our senior research scientists at the EAI up in
Portland, Maine. The next question I'll use and then I'll switch over to audience questions.
comes from Gene Tunic, the director of AI plus health at the Institute for
experiential AI. You believe that deep learning can eventually lead to human like understanding.
And you have said that self supervised learning from unlabeled data can be a powerful tool,
although it seems like in human learning, as I was watching your examples, for example,
a lot of that data is in a way supervised or tied to some kind of reinforcement feedback
around what to expect. Is it good? Is it bad? Yeah. Et cetera. So how, where do you draw that line
between, you know, can we really truly go towards unsupervised or there's a huge dependence on
supervised and on those labels to get it right? Because the world is in a way is telling us
indirectly through supervision. So self supervised learning, I mean, the reason it's called self
supervised is that deep down it's actually supervised learning. It's just supervised learning
where the supervision signal is the input itself, right? So, so in a way that's kind of, you know,
a kind of a simple answer to that to that question is still supervised learning in the end, but
with particular architectures to handle uncertainty and things and dimensionality and things like that.
Regarding reinforcement learning, there is a point at which you need some form of reinforcement
learning. And you need you need it for in two situations or at least techniques that have
been developed in the context of reinforcement learning. The first situation is if the objective
function that is optimized by your system does not reflect the ultimate objective function,
you actually want to optimize. So for example, you're learning to ride a bike. Your objective
function is the, you know, time to the next fall or something. And or the inverse time to
next fall, you want to minimize that, right? But you don't know how to compute this from
the internal state of your system. And so you need to train an objective function to approximate
this real cost, which in the context of reinforcement learning is called a critic. So that's when
you need one of those things. The other situation where you need it is when your word model is not
accurate because it's not been trained in all corners of the state space and you happen to be
in a part of the state space that it wasn't trained on. Your word model is going to be bad
and your predictions are going to be bad. Your planning is going to be bad. So to prevent this,
you need to train your word model using things that are called curiosity or exploration. And
that's another concept that comes from reinforcement learning. So don't completely abandon reinforcement
learning, but minimize its use. As we switch over to the live questions, let me, I can't
help but ask you this question. It comes from several anonymous people as well as Ken Church,
your former colleague. Did you actually say the revolution will not be supervised? I did, yeah.
Okay. But actually I stole it from Aliyah Afros from Berkeley. He had a magnificent slide that
was a picture of a wall painting in Chile someplace, which was one of those kind of
revolutionary thing. And I took that picture and overlaid on it.
The revolution will not be supervised. Yes. Okay. So I stole that from him. I deserve no credit.
Shall we switch over to a question from the audience? Yeah. So first question from Glenn
Jenkinson is, what two questions about AI do you wish you were asked more often?
Two questions. I don't know. I get asked a lot of questions. I can't imagine a question
have not been asked. That's relevant. I mean, I think the important questions are the ones that
I'm asking myself and I wish other people would sort of frame the problems in the same way. So
big question. How is it that any teenager can learn to drive a car in 20 hours and we still
don't have level five autonomous driving? That was the first question. So second question is,
what are we missing? That's the answer I want. Joe. Next question from Juan Leylanda.
Do you think quantum computing will have a significant role in the future of AI? No.
Or at least not anytime soon.
But the time this happens, I probably won't be alive anymore. So I'm not taking a big risk.
No, I don't think so. I mean, there's precious few situations today where quantum computing
could be useful. There's no situation where it actually is useful because the quantum computers
are not big enough at the moment. So it's a huge bet. I think scientifically it's fascinating. I'm
really fascinated by quantum computing at the conceptual level. I have one or two papers with
Seth Lloyd on connections between neural nets and quantum computing. I think it's a very interesting
topic, but I don't think it has any practical value in the short term. Joe. One last question
from Antoine de Bourre. To what extent do you see ML models being used for problems that we
already have pretty good algorithms to solve, such as sorting shortest path, linear
integer programming, and so on? How would you characterize the boundary, if any?
So there's a lot of problems that we can currently solve that are NP-complete or NP-hard.
And so we can solve them within limits. What we need very often are approximate algorithms,
so methods that give us approximate solutions to complex problems that, you know,
in theory are NP-hard, NP-complete, whatever. But if you reduce yourself to accepting
approximate solutions, might become solvable. So I think there is a lot to be said for ML
methods that do something that has become to be known as amortized inference. So amortized
inference is this idea that you might have a problem that is formulated as an optimization
problem. Every computing problem can be formulated as an optimization problem.
And what you might be able to do is solve that problem in certain cases, give a solution.
And now what you do with this is that you train, in your own net of some kind, to predict,
to approximate the solution to that optimization problem from the specification of the problem,
from the inputs. So that system will not be able to completely solve the problem in all situations,
but for the type of problem that you train it on, it's going to be able to give you an
approximate solution really quickly. Amortized inference. There is a tutorial on this that is,
was written and given at a recent conference by one of my colleagues at fair called Brendan Amos,
AMOS. Very interesting concept. I will close my questions with one last question then we'll take
a real live one and call it the end. I have to use this. It comes from one of our faculty who
wanted to remain anonymous. I don't know why, but given the big excitement around LLMs and not
without a reason, what are some of the research directions that are possible to tackle for non-
Google slash Facebook type sized institutions that are under studies? Space for foundational
research, big open questions in need of creative solutions. Thus, if you were a young investigator
today, like a starting assistant professor, what would you do in this environment?
I mean, that's a problem I have to face when I have, you know, PhD students at NYU that don't
have access to, you know, 16,000 GPUs, right? Unlike people at fair. So I think a lot of,
like most good ideas still come from academia. So you're not going to be, you know,
Google or Meta or Microsoft on, you know, beating the record on translation or something like that.
You don't want to do this in universities. But coming up with new ideas, for example,
the problem I mentioned of how do you do hierarchical planning? How do you train a system
to figure out, you know, how to represent the world and action spaces so that you can do
hierarchical planning? It's completely unsolved. You can do this with, like, toy problems. If you
have any idea of how you might approach that problem on toy problems, you don't require,
you know, you don't have to have, like, tons of GPUs for that. You will have an idea that might have
a huge impact. So if you have, like, a good architecture that you can show, can learn,
you know, some simple word model from video, it's the same. You don't have to train on, like,
all of YouTube. You can train on, you know, artificial environments and stuff like that
and sort of demonstrate that it works. It doesn't have to be, like, large scale. So this is the
kind of stuff you want to do. And then there is a new domain which is building on top of
open source base models. So unfortunately, right now, the best base models, LLMs, are the LAMA
class of models from $7 billion to $65 billion. They're not usable for commercial use. They are
distributed with a license for non-commercial use, so only for research, which you can, of course,
use in the university. And there's a lot of work to be done to kind of figure out how to make those
things, you know, safe, factual, et cetera, and you can, you know, work from those base models.
You don't have to retrain them from scratch. So you don't need to have, you know, room full,
you know, rooms full of GPUs. We'll try for one last question, maybe two. Go ahead, please, with
your question. Hi. So my question really dwells from the side of, or we'd love to hear your thoughts
on impact and control of these large language models or any of these models, the fancy models
that you showed with billions of trillions of parameters. So the impact side is do you really
give or how much thought do you give to the impact that would have on the community or on the people
in general based on what that model does? And control is once that model is out there,
how do I make sure that it doesn't do a certain things? It's not supposed to do with regular,
the way people used to use internet before those models. It used to be very controlled
environment where you could have, in a way, regulate those environments. But now with models,
it's getting increasingly difficult and a slow process to have or to not have certain things
in those models. Okay, so there is a long view, a very positive one, which is imagine that all of
us have those assistants with superhuman intelligence. So it's like every one of us has kind of a staff
of people working for us, but like super people working for us. This is going to create a new
renaissance for humanity, right? It's going to increase humanity's intelligence, however you
want to measure it. That has to be intrinsically good. It's been the case in the past that
anytime a new medium was invented or a new way of communication was invented like the printing
press. Humanity kind of went to the next step, right? The printing press let the dissemination of
you know, philosophy, science, secularism, democracy, all that stuff. The US would not exist
without, you know, the French philosophers of the 18th century. So either with the French
Revolution. And so I think, you know, same for the internet, right, that gave people
instant access to an enormous wealth of knowledge. Also disinformation, but okay, I mean,
we have to have countermeasures for, you know, every technology can be used for good and bad.
We need to have countermeasures for the worst aspects. But ultimately, I think
we need, you know, widest possible access to those AI systems by everyone.
Now, how do we make sure those systems don't lie to us? How do we make sure that the information
they give us is not under the control of someone that has nefarious purpose, you know, things like
that, which is I think a good reason for them to be open as I stated earlier. But I think it's a
bright future for humanity, you know, contrary to some people who tell young people don't expect
to live long, which is nuts. I think it's a very bright future. I know you've been waiting for the
next question, but we are five minutes over our time limit. And I know we have to grab a bite and
deliver you to the train station on time, according to the hierarchical plan. So with that, please
join me in thanking Jan for an amazing session today. Thank you.

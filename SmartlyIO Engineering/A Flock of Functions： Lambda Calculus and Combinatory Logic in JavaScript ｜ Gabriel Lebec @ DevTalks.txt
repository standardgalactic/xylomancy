Okay. Hello, everybody. Thank you. Really excited to be here. Quick little about me.
I made this talk while I was working at Fullstack Academy as an instructor, so it's going to
hopefully be a little educational. I currently work at Google, but this talk isn't about
that. I'm not representing Google or anything. My thoughts and opinions are my own, et cetera.
And this is the smartly version of this talk, so it's been slightly tweaked and improved
in various ways. And I've got social media accounts and things like that. There are slides
posted online. This will be in the video, slash we'll post it later. You can go find
them. And let's get started. So I'm going to start off with this combinator called identity.
And it looks like this. It is a function that takes an input A and returns A. It's pretty
straightforward. So let's try it out. So the identity of one is... Yeah, it's interactive.
And the identity of identity is... Identity. Okay. So right off the bat, we have something
cool, which is functions can be arguments to other functions. These are first class functions.
And that is a defining characteristic of the lambda calculus slash combinator logic. So
the identity of any X is X. And we can write that a whole bunch of different ways. You've
got JavaScript arrow functions on the board. We have this I variable that has an equation.
That's called combinator logic. And we have this shaded out lambda thing below it. You'll
be able to go see these notes in the slides. You don't necessarily have to try and read
all the actual lambda calculus syntax for this talk. So I'm not going to be showing
it all the time. I'm going to focus on the very high level concepts. And we saw that
the identity of identity is itself. So yes, functions can be arguments to functions and
return functions and do all that stuff. In Haskell, this is built into the base common
library that everybody imports usually as the ID or identity function. And it behaves
exactly as shown. The identity of five is five. So what is up with this lambda notation?
It literally just means function. If you were writing JavaScript the long way and not with
an arrow function, it's the word function. If you're writing Python, it's like def function
or something like that. I don't know. And it takes a parameter. It takes an input. All
functions in the actual strict bona fide lambda calculus take only one argument. We'll see
how that's useful later. And then finally, it returns some expression. In this case,
the identity function returns its input. And this whole thing together is sometimes called
a lambda abstraction. Abstraction because it abstracts away through a parameter concrete
use cases. So one concrete use case might be if A was one, this would return one. If
A was hello, this would return hello. So by parameterizing those concrete use cases, we
end up with an abstraction. But it's really just a fancy way of saying it's a function.
There's all sorts of syntaxes across programming languages for these things. And you'll probably
have heard about them. It's like, oh, now Java has lambdas. And why is that exciting?
There's a lot of different ways of writing these. Java is the worst, unfortunately. I mean, just
compare and contrast, right? But they are fun. They are exciting. And there's a reason they
show up a lot in functional programming. If you're familiar with sort of the compside way of
showing syntax, this is the grammar for the lambda calculus. This is a very tiny language. It
only has really four things. It has variables, which are just plain old symbols. It has a function
being applied to its argument. That's application. And that's just by space. A space between two
things means function applied to argument. It has function definitions, which we just saw and
they're called abstractions. And then it's got parentheses to control what order things are
evaluated in. So this whole talk, I'm going to be comparing and contrasting the way these things
look, but they're really the same concepts. Variables can hold a value. A very important
difference here. In the lambda calculus, there's no such thing as a mutable variable. All variables
are immutable. They have a definition and you might know that definition or you might not, but it
never changes. You never reassign. All right. I talked about function applications. Again, just a
space means function called on argument. We tend to say function applied to argument or function
invoked with argument. And there's little details here. For example, it's left associative
application. So that FAB line right there means first call F with A and that returns a function
and call that with B, which we can disambiguate using those ghosted out parens and say like,
okay, to make this really clear, we're passing A into F and then we're passing B into the result
of that. But since it is left associative, we typically just don't write that down. So you won't
see those parens. In contrast, you'll notice that the last line there, those parens around AB,
that does change the expression. That is now a different expression. It means first call the A
function with the B argument and then pass that result into F. So now we've muddled up the order
of things. Abstractions, these are function definitions. They're just JavaScript arrows. That's
all they are. How do they work? Well, again, you got lambda means function definition. Dot means
return. So lambda A dot B means taken in A and return B. What do A and B have to do with each
other in this particular example? Absolutely nothing. So this throws out the A argument and
returns whatever B is. What is B in this slide? The answer is we don't know. We don't know what B
is. It's a free variable. It's not defined anywhere. So if you were doing lambda calculus by hand,
you would simplify this. If you called this function on an argument, you'd say it returns B
whatever that is. And that's fine. You don't have reference errors in lambda calculus. You end up
with a symbol and that symbol might stand for something or might not. Anything else of interest
on this slide? Not really. One, maybe perhaps the last thing there. You'll notice that you can have
nested lambdas. So A returns B returns A is just like if you had serial arrows in a JavaScript
expression. And this is actually right associative. So associativity goes all over the place of
lambda calculus. That could be one of the confusing things at first. But that means that bottom
function there, if you pass an argument in as A, it doesn't return the final thing. It returns a new
function that takes a B. And when you call that second function, finally it returns an A. This is,
by the way, known very popularly as curing. So let's see that again. So that bottom function there,
we can see in JavaScript is defined as A takes that, returns that, blah, blah, blah. I didn't bind it as
anything so I can't use it. But we'll call this example. That's a classic, right? So if I call
example with some argument, I get back a function, a plain old function. If I try to call it with two
things, this is not going to work exactly the way I expect. I still get a function back. So how do I
pass in A and B? Well, since passing in A returns a function, I can immediately invoke that function
expression. Oh, my God, if he's there back. With the second argument. And now I actually get a result.
So when I say in the lambda calculus every lambda only has one input, that doesn't mean we can't
think in terms of multiple inputs. We just do so in a kind of funny way where we split up all the
inputs into a bunch of nested return functions. And that means to actually call a function with
multiple inputs, we pass them in successively, not all in the same prems, just one after the other.
This gives us all the power that we normally get with JavaScript. We can pass in both things
simultaneously, which I just did. But it also gives us new capabilities that normal JavaScript
function signatures don't have. Because I can actually, let's do this. You know what I'm going to
do on the next slide. I can actually not pass in all my arguments. And I get back, instead of nan or
garbage or something, I actually get back like the next step, which can await further action. This
is the most mathy complex part of the entire talk. So get ready. It's just function evaluation. What
is function evaluation in lambda calculus? It's symbol replacement. So it's called beta reduction,
but that just literally means we're going to replace inputs with their arguments and then rewrite
the body. Example, here's a function in red. We're going to underline its argument to make this all
very clear. It comes into this function as its parameter A. And then we go look inside that
function body, find every A parameter, and replace it with the value we're binding. So this just gets
rewritten as its argument. We just keep doing this. So we have a function and an argument. The
argument gets bound to the B value here. So what do we do? We look at this function body, replace
all the B's with X's. And that symbol replacement, which is all the lambda calculus is, gives us
this expression. Once again, we have a function and an argument. This is going to replace the C
parameter. So we go look in the function body for every C value, replace it. There are no C's here,
so it's easy. And we end up with a final value. Now there's no more reducible expressions or red
X's, as they're called. All these names are ridiculous. I'm sorry. And so this is in beta
normal form, which just means simplified. There are little caveats here. Like, if you could do
multiple evaluations in different orders, they don't give you different answers exactly, but one way
might give you an answer and one might be an infinite loop. So there's like little tricks to
that. Another thing is that if you happen to have different functions that coincidentally share
the same variable names, you have to be very careful not to accidentally conflate those variable
names. And there's algorithms to deal with that. We're not going to go into them. Okay, so I promise
that I would show, where'd the K combinator go? I guess I'll do that in a second. I'm going to show
you something after this. We'll come back to it. So this is the mockingbird. What does it do? It's
interesting. It takes in a value and then it calls that value on itself. So if the value is going
to be called on itself, it probably should be a function. So I'm labeling it as F. Let's try this
out. Takes a function, calls the function on itself. Weird. What would the mockingbird of the
identity function be? Well, it takes I. It duplicates I. So this gets simplified to I, I. And the
identity of identity we already established is... Yeah, identity. So it looks immediately very
strange, but then we immediately see a really trivial example of how it can actually work. The
mockingbird of identity is the identity of identity. And the identity of identity is identity. So
let's do that three times fast. What's the mockingbird of mockingbird? Whoa. Let's try it out. Any
predictions? It's a stack overflow. Why is it a stack overflow? Well, the mockingbird duplicates
things. So if you duplicate M, we get the mockingbird of mockingbird. But we start it out there. So
this is just an infinite loop. Yeah. Too bad. This actually is too bad because this is what, like,
the problem is with Turing completeness as a concept. In Turing complete systems, which this is
one, you can have infinite loops. And it gets even worse because you can't even tell if your
expression will be an infinite loop. You just have to try it and see. Now, sometimes you can prove
it for an individual expression. You can say, like, I have a proof that this will loop indefinitely. But
there's no machine that I can feed this lambda calculus string into that will say, oh, be careful.
That's an infinite loop. You can't do it without running it. Yeah. So that's a problem. This particular
expression in lambda calculus is known as omega. And here's the beautiful thing about doing math and
pen and paper instead of with a computer is that I can recognize I'm entering an infinite loop and
say, well, just name this thing omega and not actually keep trying to calculate it. One thing
that when I was learning about all this stuff and getting excited about it that confused me a lot was
that there are so many different names for these things. So here I'm looking at the mockingbird,
which is the omega combinator when applied to itself, which is also written in lambda form. And
it's like, I was trying to sort all this out. And that's when I started writing this talk. One little
last thing about syntax, and then we get to more juicy stuff, I promise. We can write things in
this curried form, these nested lambdas. But it's hard to read, right? Like a returns a function that
takes a b returns a function that takes a c there returns b. There's a shorthand. The shorthand is
any nested lambdas, we just squish them all together and put them to the left of the dot. So we see
that lambda ABC. It's the exact same thing. We haven't changed the meaning of it. We're just writing
it in a shorthand. And then when you go and do this simplification and evaluation step, you just
have to be very careful to notice that. So here we have a function that takes a b and c and returns
a b. Or is it? Remember, it's that syntax shorthand. So really, this is a function that takes b and
returns a new lambda that takes c. So anytime we've got multiple parameters between that lambda and
that dot, just remember, there's a bunch of lambdas hidden sandwiched between them. And when you do
this simplification, they have to pop back out. Congratulations, you know the lambda calculus.
Talks over. No. All right, here's the part I was trying to get to earlier. The kestrel, this is one
of my favorite combinators. How does it work? It takes an a and b and it returns a. So it takes two
things and returns the first one. Wait, I already did that. But I'll rewrite it. The k
combinator. Let's try it out. So I'm going to pass in two things, two symbols, arbitrary symbols.
They don't mean anything. You've never seen them before. And look, it gave me back the first symbol,
whatever it was. I could do this if I really wanted. So this is JavaScript really being symbolic. And
of course, I can pass in whatever. And wait, I'll pass in k. And what should this give me back? So
the k combinator takes two things, returns the first one. So I get back my first symbol. And
there actually are symbols in JavaScript now, which makes this even better. But why do I care
about defining it in this like one argument only nested lambda way? This is the reason. The k5
function is if I call k with just one of its two arguments, not both of them. Oh, it's a function.
What the heck is the use of that function? Well, if I call it on something else, it gives me back
five. What if I call it back on something completely different? It gives me back five. What if I call
it on undefined? It gives me back five. The k5 function, or the constant five function, is a
function that is now stuck on and obsessed with this value. I can't get this function to return
anything else. Turns out that's useful for the same reason things like zero in a number system is
useful. Sometimes you need a function that does very little because you have an API that's like,
you must give me a function here. And like, I don't want to give you a function. You're already
doing everything I want. So the identity function and the k combinator, these things start to form
sort of very primitive, almost trivial building blocks that slot into larger, more complicated
systems. So k of two things returns the first one. If you swap them around, you still get the first
one, whatever that is. And in Haskell, this is called const, which isn't confusing for mainstream
imperative programmers at all. It is a function. It is the k combinator. Takes two things, returns
the first one. Let's expand on this. If we've got this k combinator and we pass in two things, we
get the first one. You're getting it. But wait a second. This is math. This isn't really programming.
Quite. So this is a quality sign. It's not an assignment operator. This is a quality in the
truest sense of whatever that means in math, which by the way is a very deep topic that we
won't touch on. What is the nature of equality? People don't agree. Anyway, so the thing on the
left and the thing on the right are in theory the same thing. But the thing on the right is a
function. It's the identity function. That means I can call that function on a value. Interesting.
But we know the identity of y, right? We already know what that actually evaluates to. That's
y. Huh. So this function on the left, k, that I talked about taking two things, I've kind of passed
three things to. I, x, and y. And I actually get a result out of that. That's weird. I thought I
defined this function as only taking two inputs. But now I'm using it with three. This is the kind
of weird flexibility and power that Kerr-Ying gives you. When your functions are constantly
returning other functions, that return other functions, that return other functions. Like,
you can kind of chop things off early or extend them far beyond what they were originally being
used for. And they still do things. And those things might end up being useful. Why is this
useful? Because check this out. The first two values there, ki, are a function being called on
x and y. And they return the second of those two things. I start with k, returns the first of two
things. Ki is a new function out of the ether that returns the second of two things. I'm not
hearing all the amazement I expected out of that. Yeah, thank you. And this is called the kite.
So it's also easy to define. I could do it the way I've been doing it and say ki of like two things
returns the second. There's the proof. I could also define this as calling k on i. And now I can
rename it. And it just looks like a normal function. And it passes two things and it gives me the
second. But what's really amazing about this is I never sat down and wrote out, look, ki takes
a and it takes b and it returns b. I took two existing functions, neither of which talks about
the second value of any two inputs. And just by smashing them together in this sort of like
atom to molecule way, it gave birth to a new function with new behavior that I didn't manually
specify. So ki of m and k returns the second thing. Ki of k and m returns the second thing.
And things are behaving more or less as we expect at this point.
What's with all the bird names? Mockingbird, Kestrel, kite. Where's this coming from? So
there is a mathematician who had a brilliant short career, very sad, very sad ending to it,
named Schoenfinkel. I can't pronounce German. I'm sure many people in this room can pronounce
German much better than I am. But he named these things very long German names like
SÃ¼tze mit Setsungsfunktion. And then he shortened them because it was annoying to write those down
into things like i and k. And then an American mathematician, Haskell B. Curry, studying this
stuff, used many of the same letters, switched some of them around just to confuse everyone who
came later. And then a logician and puzzle author whose books I really enjoy called Raymond
Smollion wrote this book called To Mock a Mockingbird. And in To Mock a Mockingbird, it is all a
extended metaphor of birds in a forest that call the names of other birds, which give birth to
birds and you get where this is going. It's the same thing. He just took all those letters and he
expanded them out into bird names for his metaphor. By the way, if you try to learn
commentary logic through Smollion, I mean you might make some progress, but it is a puzzle book.
It's meant to be confusing. So just fair warning for anyone who's like, I'm going to go buy the
book now. It is a good book. You should check it out. But it's not meant to sort of be easy to follow.
Also, he called it the idiot bird. I don't know. It should have been Ibis.
Anyway, this was done in tribute to Curry, the other mathematician, because Curry was a bird
watcher. And Haskell B. Curry, why do we care about this guy? And oh my god, okay, let's kind of
talk about history. This is going to be the super fast version of this because I'm trying to stay
under time. But in the early 20th century, actually even late 19th century, mathematicians
are trying to figure out what is math? And there are all these different systems that proceed from
different axioms. So what's the one true axiomatic system of math from which all of the things can
be proved? And it's nice and simple and intuitive looking and clean. So Peano comes up with these
things called Peano arithmetic, which is like, I'm going to define the numbers. There's a thing called
zero. And one is the thing that comes after zero. And two is the thing that comes after the thing
that comes after zero. And this sounds super trivial, but it actually forms a really sound
foundation for very large and complicated branches of math. Frege, who's unfortunately not as well
known as he should be, does amazing work in developing concepts of combinatorial logic and
functions and invent its own function notation, which is in an abstract syntax tree, which by the
way would have been so much easier to simplify for people. But it doesn't type well, obviously,
right? Like if you type setting, you need a linear system. And his was two dimensional.
And he uses currying, although he doesn't draw attention to it. And he develops a ton of work
in axiomatic quantified logic, things like for all x's in the real numbers, there exists a y such
that y is greater than x. Those things like for all, and there exists, a lot of that originates
with Frege. 1910, Russell and Whitehead famously published Principia Mathematica, which is another
big attempt at unifying all of math, a great grand unified theory of math. Unfortunately, along the
way, Russell discovers a paradox, an underlying flaw in set theory, as it was known at the time,
where the question of, what about the set that contains all sets that do not contain themselves
or something like that? I can't remember the details. He's like, wait a second. Does that set
contain itself or not? And you can't answer the question. It's a nonsensical question.
This was a giant problem because tons of preexisting work was based on that version of set theory,
which is now shown to be inconsistent. It didn't actually hold together.
So this was like the first big crack in the foundations of math, where people started
saying like, maybe this stuff is harder than we thought it was, because now we're trying to be
extra super duper careful. Schoenfinkel, 1920, does tons of work on combinatorial logic. Now,
I'm talking about combinatorial logic, lambda calculus, javascript arrow functions, what's
the difference between all these things? Well, we'll clarify that more throughout,
but anytime I'm using those letters like k and s and i and m, that's combinatorial logic. It is
the study of functions that act on each other. And anytime I'm showing you lambdas with lambda a
dot b, et cetera, that is lambda calculus. Turns out they overlap by like 99%. They study functions,
first class, higher order, anonymous functions. Actually, the combinatorial logic ones are not
anonymous by definition. But that's almost all the difference. It's like one of them,
you give them names and one of them, they're anonymous. Van Neumann also does some work on this,
I'm going to skip it, combinatorial logic. Curry comes along and does a ton of this work,
not knowing about Schoenfinkel. And then he discovers all of Schoenfinkel's work. He's like,
oops, this guy already did it all. But he keeps going and he comes up with a lot of really great
results as well. By the way, now we know this thing that you split up the parameters as currying.
Okay, yeah. But it really should be called Schoenfinkelization.
In 1931, Kurt Gerdel, or maybe fragification, I don't know, Gerdel now really like upends all
math and just destroys all math forever with his incompleteness theorems, which proved that any
system that's sufficiently complicated and interesting is either inconsistent, meaning it's
flawed and actually nonsensical, or it is incomplete, which is the lesser of two evils,
but still very disturbing for a mathematician. Incompleteness means you can talk about some
possible thing that might be true or false in math, but there might not ever be a proof of whether
it is true or false. But it is true or false, you just can't find out. It sounds like the most
horrifyingly impossible thing to a mathematician, especially of that time period where math was
seen as the language and nature of reality, that if you were working on an unsolved problem,
surely if you were just smart enough or determined enough or had good enough teammates,
you could all figure it out together. And now Gerdel was coming along and saying,
nope, you might never find the answer because it might be impossible to find out the answer.
That was terrible. It was really terrible. By the way, a fun philosophy point. People use this
along with Einstein and general relativity in the 20th century to say that the nature of
reality is fluid and abstract and unknowable and things like that. This really upset Gerdel because
he viewed the incompleteness theorem as a universally incontrovertible fact about the universe.
So he was really excited about this result because he's like, I've finally found something that is
always true. And everyone else is like, oh my god, you've destroyed math.
Another thing. He works in general recursion theory, which is another big system trying to
unify all of mathematics. Around the same time period, a Hungarian mathematician much less
well known, unfortunately, than Gerdel is Rosa Pater. And she is known as the mother of recursion
theory. She does a huge amount of recursive function theory development and work in papers
for decades. So I'm glad to be able to point that out. Now we get to really what's at the
subject of this. So the thread here has been all these people very smart are trying to figure out
what is math, what is the nature of math, how do we define it? And now Alonzo Church comes along
and defines this thing we've been looking at called lambda calculus. This little tiny language.
That's it. It's just a notation system for symbol replacement. That's what a calculus is,
by the way. A calculus is any formal system of rewriting symbols on a page. And this is one for
functions. And it turns out that that tiny little language is beautiful because of its simplicity,
which makes it easier to prove things about it than competing theories at the time, like general
recursive theory, which is more confusing to read, in my opinion, my very own, like don't take that
as gospel. He's got some grad students. Steven Cleaney goes on to invent regular expressions
later, but also these grad students help like prove various things about lambda calc. And then
this is the brilliant shining moment where lambda calculus like proved its worth. These tiny little
four lines of grammar and syntax were used to solve a giant unsolved problem of the day called
the decision problem. David Hilbert's decision problem, which was is there a machine that you
could write, an algorithm you could design, that I could feed into it some math expression
and it will tell me if it'll run into an infinite loop. And the answer to that is no. There is no
such machine. We already talked about that. But church, what's interesting about this is that
that there is no such machine, but rather that church used lambda calculus, which seems so tiny
and trivial, to prove that result. And he published a paper and about a month later, Turing published
another paper solving the same thing with something much more famous called Turing machines. And
Turing was a little upset that it turned out that the idea that he had been working on for years,
I think, he just never got around to publishing. He finally did it and someone else had published
a month earlier. And yeah, sometimes that's the way it goes. But he goes and looks at this and
he's like, wait a second. My Turing machines, which is this like stateful, imperative, sequential
program instructions. And this lambda calculus thing, which is an expression that you just do
simple replacement and it simplifies. Even though they look very different, I can write a lambda
calculus interpreter in my Turing machine. And I can take a lambda calculus and use it to implement
a Turing machine simulation. So wait a second. If there's something in a Turing machine, I can do
it in lambda calculus because I just use lambda calculus to simulate Turing machines. And vice
versa. Therefore, these two systems are really just as powerful. Anything one can do, the other
can. So they seem just like amazingly, even though they're completely independent schools of thought
and they're both trying to define what math is and all this business,
they seem to have accidentally discovered equally powerful systems for computing things.
So the church Turing thesis or hypothesis is not a fact. The hypothesis is that these systems
define computation in some deep philosophical way. That's undefinable. So you can never prove it
because it's not well defined. The fact that lambda calculus and Turing machines are equally
powerful is not the church Turing thesis. That's a common mistake. So he's really excited about
this. He actually comes over to Princeton, gets a PhD under Alonzo Church, helps define like new
fixed point combinators and things like that. What is a combinator? I've said this word a couple
times. Again, it's like all these different names for things. Combinators are just functions with no
free variables. Remember I mentioned a free variable is one where you don't know what it's bound to.
It's not bound to anything. It's just kind of in space. You end up with it and it's like,
what does that mean? I don't know. It's the symbol B. Combinators don't have those. So all of their
outputs are bound as inputs somewhere in the expression. So the definition of B will come
from somewhere when you use it. It will not be undefined. In other words, combinators can never
have reference errors because their values are all based on their inputs. That's all they can do
is remix their inputs. And combinatorial logic, full circle now, is the study of those kinds of
functions. Functions which act on each other in surprising ways. We've seen a bunch. I, M, K,
K, I, et cetera. And again, this gets it like there's all these different names. There's the
mockingbird versus M versus lambda F to FF. You can't actually define that one in Haskell, by the way.
Not simply because Haskell's type system doesn't like infinite types and the mockingbird is all
about infinity. There's a trick. You can do it in Haskell through like a little type hack,
but it's not beautiful. Let's look at this one. The cardinal. The cardinal is a very lovely
combinator. It takes three things, spits them back out in different order. So let's say the first
thing's a function and the second two things are inputs. C combinator takes a function and A and B
inputs and it calls the function with the inputs reversed. It flips the inputs around.
That's interesting. What will we use or use that for? Well, let's see examples of it just to make
sure we're on the same page. If I feed in a function like K and two arguments like I and M,
the K combinator returns the first thing. So the flip of the K combinator returns the second thing.
Well, that's kind of cool. Sometimes, because of this currying business, your inputs and your
arguments don't have quite the order you wish they did to make some of these tricks easier.
And so this flip combinator or cardinal or C combinator can rewire things for you so that
they take arguments in different orders. And C of K takes two things and returns the second.
That sounds super familiar, right? Flipping K gives you Ki. They are the same function.
C of K and K of I are the same function. They behave identically. For every input,
they give you the same output. And when two functions that have been defined separately
have nonetheless identical input output behavior, they're called extensionally equal.
Extentional in the sense that from the outside, you can't see the guts of the function or how it
came into being. All you can do is throw things in and see what comes out. There's no way to
distinguish those functions. There's another kind of equality called intentional equality,
which is more squirrely and it has to do more with the internal guts. We're not going to care about
it. So there's another beautiful combinator, the C combinator. And in Haskell, it's called flip.
And if you've ever played around with Haskell and talked about point-free programming,
flip shows up a lot generally in a way that makes things much more confusing than they have to be.
There's a cool site out there called pointfree.io that simplifies Haskell expressions,
it really doesn't. It shortens Haskell expressions to the minimal version that uses the fewest
ingredients and you'll just see tons of flip and ID and const and it's completely unreadable
for humans. Sometimes you get lucky and you get something that actually is simpler and easier,
but flip is one of these things that is very heavy in the rewiring that's possible.
Okay, I talked about this for like 35 minutes. Who cares? What is the point?
Why do we need to care about any of this? Well, let's find out.
I talked about lambda calculus and Turing machines. Why are Turing machines exciting?
What? Simple. They are simple. So is lambda calculus. They're both simple. They're both so
simple that it's almost hard to do anything with either of them, right? Like if you've ever hand
coded a Turing machine to do like anything interesting, it's very complicated. You've got
write a one, write a zero, read something, go to memory address. Like that's almost assembly.
And that's really why Turing machines are cool is because if a hypothetical machine can calculate
whatever you want it to, well, why not build real physical machines that do the same thing?
So from Turing machines, we get the idea of let's make actual computers that can do this.
And those computers have to be programmed in this very low level on off switch state
version of programming called machine code. But that's really hard for humans. So now let's make
a text representation called assembly of those very low level machine instructions. But thinking
in terms of machine instructions and assembly is still hard. So now let's make high level textual
languages that get translated into machine code and aren't one to one correspondence and give you
beautiful new human centric concepts, things like loops and variable assignment and stuff like that,
which you don't get in the same way for this low level stuff. But those languages are still
maybe saying like, you tell me what memory to reserve and what blocks and how much of it and
how to index into it and things like that. And someone out there is like, I'm sick of saying,
you know, pointer to something. Can't we just have a language where I say var x equals object and
it'll go figure out all that memory stuff for me. So now we're going even further away from the machine
and more towards the concepts humans care about, which are, I've got a domain, I'm modeling something,
I have ideas that I want to bandy about. And then we go even further and we say like, wait,
why do we even have state? Why have variable mutation and reassignment? That has to do with
the way the machine physically works. But we don't need that. And if we take it out,
some of our programs get a lot more type safe. And this whole time, the layers of languages
that have been building up and people have been inventing have been gradually moving away from
the super imperative machine based way of thinking toward a more pure conceptual way of thinking.
And someone out there says, wait a second, if lambda calculus and Turing machines are
equivalent, what if we went the opposite direction? What if we started with something that has
absolutely no concept of a machine, but is still able to compute things and try to make it useful,
try to make it work on a Turing machine? We know that it's possible. Now the question is,
can we optimize it? Right? Like, can we take this pure stateless functional way of thinking and
make it perform well in terms of memory and speed and stuff like that? So we'll write these
pure functional languages and we'll have compilers that have been very cleverly written to develop
good machine code from them. These things are equivalent, which means everything can be functions,
literally everything. But not everything should be. But more than you might think.
Such as Booleans. We're going to invent Booleans from scratch now. Let's do it. Well, it's problem
because we don't have any of those things. We just have parentheses. So far, that's all we've got in
our language, our parentheses. We don't have equal signs. We don't have the not operator. We don't
have ors. We do have kind of variables, but the variables aren't Booleans. Like, there's no concept
of true, false. Okay. What are Booleans used for? A lot of things. One thing they're used for is
selection. If some condition, then result. Else, different result. Well, I've got two possible
results and I'm selecting which one I want to use. So if it's the true Boolean, give me back the
first expression. And if it's the false one, give me back the second expression. Huh. Well, we don't
have this syntax. You don't have question marks and colon. So we'll just knock those out. And then
we have space. So that means function application, right? So whatever this thing, it has to be a
function. And I need two functions, one for true and one for false. The true one will select the
first expression and the false function will select the second expression. We've already seen this.
It's the KNKI functions. Surprise. I already did it. So we'll just go up in here. And we'll now say,
so if I log out the K function, by the way, it says function KN node,
I cheated ahead of time and I did load one tiny line of functional stuff in here, not functional,
but one tiny line of code that's not limited to KACO. That's to rewrite function names in node,
which is slightly annoying in node version 12. But let's label this thing K slash true.
Ah, reference error. Inspect is not defined. Oh my God. Okay. I reloaded this. You know what?
We'll just not do it. You'll all just remember that K is true.
See, this is what happens when you try to go outside of pure math, right? This is just a mess.
The KI function, unfortunately, this function is anonymous. It doesn't have a name because I
developed it by smushing together KNI. So I'm going to quickly rewrite it manually, even though
that's sad. Great. So anytime you see K, you can now think to yourself, oh, that's true. And
every time you see KI, you can think to yourself, that's false. Just like if you see one in zero,
you think true or false or on and off, you see true or false or TRUE is true and FALSE is false.
Now the K function is true and KI is false. It's just a representation.
But if I have some condition, like unknown bool, is quick, don't look. No one saw that.
Well, which one is it? I can find out by passing it two things. If it was true, I get back the
first thing. Oh, I guess unknown bool was the true function. So I can do everything I wanted,
just like already out of the box. All right, but that's not that interesting, right? What's
another thing we do with Boolean logic all the time? We negate. We actually, like, transform
things around. The NOT operator. Well, again, we don't have exclamation points, so we're going
to get rid of that. It becomes a function called NOT. How do we define it? If P is true, return
false. And if P is false, return true. Well, we need to select between two possibilities.
The flip combinator is going to do this for us. I've already defined NOT. You didn't even realize
at the time. The flip combinator, the flip of K is KI and the flip of KI is K, which means the
flip of true is false and the flip of false is true. So NOT is just the C combinator. Let's try
it. The flip of K is some function, but which one is it? It's the false function. It returns
the second thing. And the flip of KI, or false, is the true function, so it returns the first thing.
We can also rename these like T is that and F is this. So now we can say things like NOT
true. Which one is that? Is that true or is it false? It turns out NOT true is false,
and it turns out NOT false is true. And we just invented this out of the ether.
These are called church encodings. Alonzo Church was one of the people who helped invent them.
And it's like, okay, we'll represent these things as functions, and then we can do functions on them
that transform them and move them around. There's other ways of doing them. I've written one up
there, like this lambda expression is another way of defining the NOT function. But let's move on
and do AND in the interest of time, of which there is very little left. The AND function takes two
booleans. Okay, so how does this work? If both P and Q are true, return true. For anything else,
the AND function return false. So we're taking in two booleans, P and Q, and remember these are
functions, P and Q are both functions. So what can we do with those functions? Well, we can use
them to select things. The first boolean input is going to select between two expressions.
If P is false, it's going to return the second thing. That's what the Ki function does.
But if P is false, what should AND return?
One of my booles is false. So AND should return false. So P should return false.
And if P is true, what should AND return? Q. Exactly. Because it depends what Q is, right? If Q
is false, well, we should return false. But if Q is true, P is true and Q is true. So return Q,
which is true. And hey, it all works. In fact, you could simplify that too. If P is false, return
P, which is false. Let's try it. It takes two unknown booleans. And then if P is true, return
whatever Q is, because we have to check it as well. If P is false, well, we know P is false. So just
return P because they're not both true. And guess what? I have to prove that it's actually,
it's some boolean. Oh, it actually worked that time. So AND true, true is that. AND true, false
is false. AND false, false is false. AND false, true is false. Hey, we invented AND boolean
logic. You can do the same thing with OR. It's just like slightly different order. You say P
returns P and Q and things like that. What is this? This one's a little bit more dense. And by the way,
functional languages, that function application has just a space. Look how much easier it is to read
when you don't have all these parens for function invocation. Like, I think that JavaScript is
actually harder to understand than the lambda calculus. But if you want a little help, P selects
between two different things, depending on whether it's true or false. Q selects between two different
things depending on if it's true or false. And the behavior of this unknown function, this anonymous
lambda, is if both P and Q are true or if both P and Q are false, return true. But if they're
different from each other, return false. So this is boolean equality. And we can simplify it
through some steps that you can go review later in the slides. It actually turns into this,
which I'll do right now. Boolean equality takes a P and a Q and it calls P on Q and not Q.
And now I can test, hey, do true and false equal each other? No. Do false and false equal each
other? Maybe. That's some function that just got birthed out of nothingness. So let's check.
Yeah, it's the true function. It's the one that returns the first thing. And I'm going to keep
moving because I'm almost out of time. So we've got these church encodings and they're lovely.
Hey, here's De Morgan's law. It's a thing from logic. It says not P and Q is equal to not P
or not Q. And now using just a lambda syntax anonymous functions, we have this beautiful,
concise, easy to read way of expressing this logic. The equality of not and PQ or not P,
not Q, is always going to be what you expect it to be. Feed two bullions into this thing,
and by bullions I mean either K or KI, and you get out what you would expect from Boolean logic.
What else can we do with this kind of fun stuff? We can invent numbers. And if you've got numbers,
actually specifically positive integers, actually specifically natural numbers, zero and up.
If we've got those, well, a pair of numbers is fractions, but we don't have pairs yet. So
we could do arithmetic and we're going to need some data structures. So we'll invent those from
scratch using just functions. And once we have data structures and arithmetic and things like
that, we can do data types and type systems. And once we've invented some of that stuff,
we can also invent recursion from scratch. Lambda calculus does not have named functions.
The C and K and I, that's combinatorial logic. We often refer to these functions by their names
because it's way easier than saying lambda, AF, etc. But lambda calculus itself does not have
named functions. In fact, it's deliberately designed without them to make it easier to
prove things about math and the way things work. So how do you do recursion in lambda calculus?
Well, sorry, you're going to have to see part two in the video online.
Yeah. We're almost done. Question. So some fun little things at the end here. How many
combinators, like C and K and stuff, do you think you need to cover every possible computation to
be able to do anything? Because in combinatorial logic, you have to define all those functions
ahead of time. In lambda calculus, you could define a function manually using syntax whenever
you want. But in combinatorial logic, you need to pre-define all these named functions.
So do you need 20 of them? 10? 5? Is it even possible? Maybe you need an infinite number.
Well, here are two. The S and K combinator. They behave together in all sorts of very clever ways.
And for instance, the I combinator, you can invent it by calling S on K of K.
And the V combinator, which you didn't see, it's in part two, it's a data structure,
it's a two-tubble. You can do by this nonsense. But it gets even smaller. You can do everything
with a single function. So if you really wanted to go crazy, you could take this iota function
and write any other function in existence using just this and parentheses. But why? So what is
the point of this entire talk? Like, why am I here talking about this? The short answer is I just
really like it. And I hope that you've enjoyed it. It's kind of a game, like I read about this stuff
in Smollion's book, which is deliberately supposed to be entertaining. It's logic puzzles and games
and things to exercise your mind and think about. It's also really great practice for thinking
functionally. Not because in a functional programming language, you have to invent booleans
from scratch, although often you do. Not quite in that way, though. But, like, just the act of
doing all these function evaluations with symbol manipulation and substitution and evaluating
things and seeing the power of functions operating on each other makes that sort of
foundational knowledge of working in a functional language just feel much more comfortable.
It's a form of exercise of calisthenics. Programming languages like Haskell and PureScript
and Agda, I guess, I don't know, and Elm and other things. Their basis and their core are
Lambda Calculi, slightly souped up ones like System F and System FC and things which add
types and all sorts of stuff. But you know how Haskell actually works? It's great. You write
Haskell. The compiler takes that and simplifies it down to a slightly souped up Lambda Calculus.
And then it pairs that Lambda Calculus program that was from what you originally wrote as like a
same thing with a runtime written in C that's a Lambda Calculus interpreter. And that's how Haskell
works. That's why the minimum size for any Haskell program is like eight megabytes or something
because it ships with this Lambda Calculi interpreter that runs your code. I think,
personally, though, at the end of the day, like, I don't want to have to defend or justify and say,
like, you should all learn Lambda Calculi because it'll let you do this thing in your job.
It might be able to. There's a couple cool examples I can think of that I've now thought of
after the fact. But for me, it's like pure art for art's sake. And I just think that's like a
lovely thing to find in programming and computer science. And yeah, there's the Y Combinator,
that invents recursion. You can go see about it later. So there's a whole bunch of slides and
stuff in here for reference. These are all part two. And then, yeah, there's resources. And I guess
seven minutes over time, that's not too bad. Let's do questions.

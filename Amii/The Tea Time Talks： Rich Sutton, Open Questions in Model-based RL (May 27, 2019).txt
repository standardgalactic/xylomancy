Okay, it's not really quite 4.15 yet, you're two minutes, but let me just start with just
have a welcome you to tea.
We do this each year and it's very informal and I welcome you all to sign up and participate
and to ask questions and you know we're here just to talk about science and ideas and exchange
points of view.
So think of it as a community, enjoy the tea, have a cookie, it's all good.
The other thing is Thursday, Thursday I think we still have an open slot and we could have
Martha, Martha may take it, I was hoping Joseph would be here and he would take it, but that
will work out, I'm pretty sure.
Okay, so the topics are really intelligence and how do we understand how the mind might
work and could work and today my topic, really the topic is model-based reinforcement learning.
Model-based reinforcement learning is really it's sort of the whole enchilada.
We try to get all the pieces present, model-based, model-free, make understanding of the world,
be able to reason or plan with it and how many people here have just raised your hand
if you've heard of the phrase model-based reinforcement learning.
How many haven't, just how many actually, let me say it a little differently, how many
of you just think it's the model-based reinforcement like the most important thing in AI like I
do, or should I give a little bit of an argument from authority for it?
Okay, I spent that long talking about it, I might as well give my, I just threw in a
slide where I say lots of people are talking about this, it's sort of the thing, Jan LeCun
talking about predictive learning, understanding the world, Yashio Benjo talks about the most
important step in AI is to make predictive causal model of the world.
This is a thing that's coming around at last.
So this is our topic, our topic is model-based reinforcement learning and it means making
a model of the world, I can use this word model to mean model of the world, how the
world works.
I'll only use the word model as a world model, model of the MDP, the Markov decision process,
the transition dynamics of the world and model-based reinforcement learning, we learn the model,
we learn the model and we also then use the model to plan or reason about what to do.
So this is, we call this planning and planning proceeds by, you imagine states, you look
ahead from the individual states to see what might happen and you back up to improve your
policy or your value with those states that you're imagining might happen and then this
is how you figure out, decide what to do.
Okay, so it's decided this idea has been around for a while, I'm going to talk briefly about
the Dynar architecture, something I did in 1990, almost 30 years ago now, where I first
proposed or first explicitly proposed that planning and learning would be radically similar.
So there's some reinforcement learning algorithm that's interacting with the world and then
it's also learning the world model which is another box that you can plug in place of
the world and instead interact with the model for a while, in that sense, and so you're
interacting with the model and you're saying, what if I do this, the model says that would
happen, you get this reward and you learn from that just as if it had really happened
and so then this way the planning or the logic and the reasoning, certainly the planning is
radically similar to just acting in the world, just acting in imagination.
So that's the first half of the idea, the second half of the idea is that these things
are all done simultaneously, so planning and learning and executing, they're all going
on all the time, so you're always interacting with the world, getting new experience, you
can do a model for your reinforcement learning with that bit, but you also use that experience
to make the model and the model through planning and that also affects your value function
and your policy and makes you do better.
Those are the two ideas of Dynna, it's kind of very simple and we make it into an algorithm
that's kind of useful to complement those diagrams, those visual things with a little
bit of a diagram, make it very concrete, so this is a diagram, this is an algorithm from
the book, so basically your Q is your action value function and model SA is your prediction
about what would happen if you were in state S and did action A and you initialize that
and then as you go through life, you're just doing this loop over and over again, you're
looking at the state you're in, you're picking an action in it and set B somehow, then you
take that action, send it to the world, the world gives you back a result, R, the reward
and the next state S prime and so then you do model free learning with that little four
tuple of experience, state action, reward, next state, so here we're doing Q learning
in step D to update the action values and then we update the model because we've seen
what will happen for that state and that action, that this will happen and this is where we're
assuming a deterministic world because really many things might happen if you did that action
in this state and we only saw one thing and now we're going to just overwrite the model
so that I'll predict always that that would happen, so the thing about Dynna, particularly
the first instance of it in 1990, it's a very simple and almost a toy case, it's very clear
but it's only expressed, it doesn't fully express all the possibilities, we're going to talk about
today's, how you can go and consider more possibilities, anyway after you do, now we've
got as far as learning the model and then step F is where you do the plan, so by plan you just
imagine some state, maybe randomly from something you've seen before, imagine an action, you ask
the model what would happen if you were there and then you do that same step, same it's the same
as D, it's Q learning but here now we're doing Q learning on these imagined things and there
we're doing Q learning on the real thing, so that's the basic idea of Dynna, I'm sure many of you
have seen that before, now maybe you've seen the demos of it before, so here's the standard demo
where we have a grid world, we have a start state down in that corner and we have a goal
state where you get a plus one reward and the actions are to move up, down, right and left,
so four actions from each state, the states are all represented tabularly, so like this is
state 34 and this is state 92 and you just have, the model says oh when I go action four in state
34 I end up in state 92, so that's what the model is learning and because it has the model it is
very quickly learning how to get from start to goal efficiently and it's actually able to learn
about states that's not in and this becomes most apparent if we pick up the goal and move it to
a different place, so of course once we've moved it the agent will go back to it'll go here again
where it used to be and be disappointed so to speak and then it has no idea where it is,
it's built a model but the model says that there's no goal up there as the last time it was there
there was no goal, okay but eventually it will stumble upon the goal again and then it will be
very quickly able to plot a path, so watch when it first finds it now even here it will eventually
see it's learning the right way to go and it's learning a good path very quickly because it has
a model, okay so that's like tabular dyna, okay and today we want to talk about the extensions
of this, open questions in it, yeah I guess I didn't even say it, it's open questions in planning,
I'm not going to tell you the answers, I'm going to try to set the questions,
so dynamic architecture extends naturally to stochastic dynamics, what you saw was just
deterministic dynamics, we assume that the world always went the same way but you could instead
of overwriting what the model says in the state in action pair you could start to collect a list
of all the things that might happen in their probabilities and then you could sample that
and you could do exactly the same thing, you could add function approximation, now function
approximation I'm going to talk about it but there's a there's a there's a it's really a spectrum,
a range of degrees of function approximation, so that's what we saw was tabular, I call it tabular
and that means every state action pair is treated totally different from every other state action
pair, there's no similarity between them there's no generalization and so there's just a big table
and I store things in that state action pair and really in real life certainly in computer
go and in Atari games and in any robotics application you have to generalize from one
state to another and that's you know you never see the same state twice okay but we start with
the tabular and you think you're used to your deep learning that'd be a non-linear system
you could also have linear things turn out to be quite important and even the aggregate case
state aggregate means you still have a table but there there could be many many states fall
into the same table entry okay so you're aggregating states and treating them all the same this is
this is a nice case actually we are we can get theoretical results for it that we can't get
for the other cases okay so there's function approximation we want to do that in some sense
that's our bread and butter we just generalize the table to a
function approximator like supervised learning system but let's go on I want to extend it
quite far so let's list the things and the next big thing is partial observability because
really the world doesn't even give us states it gives us observations gives us things that happened
things it's our senses it doesn't tell us we don't know the full state of the world
we just get an observation and now and we have a little trick okay now ignore that the trick is
there is the red box but if you look at the rest the rest of it is basically the kind of thing
we've talked about so far we have the world we have our policy and our value function and we're
interacting with the world getting rewards and we're getting some observations and then that
red box is turning it into a state and so so once once we get past the red box it's just like
before we had a state and we can make the make send that state up to the model to be learned
and we can send that state up to the planner to to to propose things and the planner will
will will do some adjustments to the policy and value function just like the reward does
but it will come from the planner and this will be the common path between model free learning
and model based learning okay so the thing in the in the red box this is this is this is the
state update function which just says that the agent has to take responsibility for learning
some mapping from the observation the last state and its action to what it's going to use as its
state it stays as a summary of the past it's good for making decisions and predicting the future
and so the state update function is called you it's just this it's it's it's exactly this thing
and it's got to be learned okay but in this talk i'm going to assume that the state is given
and the u box is given and i'm gonna mostly assume anyway when you kind of talk about changing the
state feature vector or the the state representation that will be the state update function the state
update function okay that's that's a major extension and at the same time it's almost done
because i've got i've got some kind of a box and so i've got some kind of a box produces some kind
of a state representation and my methods always at least once i did the second step a function
approximation they always were able to accept a representation that wasn't necessarily perfect
and so whatever you give me however imperfect it is i will be able to do a certain well with it
just as i would be able to do certain well with a with a certain feature vector of representing
the state okay another big step is that that if we do it right it doesn't we can we can we can
separate it from all the other issues just like we have here which is to do temple abstraction
really if you take your model of the world your model of the world is not if i'm in this state
i do this action one step later i'll be in this other state it's really more like oh if i uh go
to the talk i'll learn something or if i if i run home i could i could eat a sandwich or i can i can
take a plane and travel to serrano okay so those are obviously all big multi-step events and we're
actually the kind of learning and kind of reasoning and planning we want to include should include
all those sorts of things so there is a theory of options which enables us to to treat those
surprisingly so but we can treat all those as exactly in the same cases okay and and last what
the average reward setting the average reward say i'll talk about that in a little bit um so
rushing along uh i'm gonna i'm talking about open questions
in model based reinforcement so i have to say a little bit what's closed you know what's not
what i'm not going to consider open so these are my my settings these are my presumptions
and i say close this because like lots of people will disagree with me or they would
disagree with me if i gave them a chance okay um uh i think planning should be online incremental
like asynchronous dynamic programming and like the dynasy system you you just seen
i think that models and planning they should be state to state so you so many people in the literature
make models and the planning where they they include the observations in the plan you're like i
if i did this then i would see that and then i would no no it should just be state to state
and if you think about just a little bit longer uh really it's obviously you've got to be state
to state you don't want to have your observations which are which are tied to the single time step
and tied to state update you want all those to be separated okay um now of course it's not really
state to state the state feature to state feature vector state feature vector to state
feature vector and that will be the where the state the feature vectors are coming from the
learn state update function that we mentioned earlier okay closed models planning they should
be temporarily abstract you should not be one step they should be used as uh based on options um
also search control search control is how you decide which states to think about an imagination
and that's essential for your plan to be efficient if you think about stupid states
you'll just learn stupid things but if you can just select the crease the key states to think
about to form your plan then you can be efficient and effective in your plan and the last thing is
that so these are sort of like saying i i need this i acknowledge i need this even though i'm
not going to deal with it directly and similarly we need some problems to in order to structure
the learning of the options and the option models okay but let's go down to the open questions
the open questions number one should the model what is this model should it generate sample
states which i suggested or should generate expected states okay um there's a bunch of things
under that and i'm going to go through it in detail but how should planning be done with
average war this is the other big thing that i hope to cover today um average war and then
all the other things i won't i won't probably won't get to um but let's look at so let's let's go
to how we put function approximation in here and and what is the content of the model so just a
little bit of um terminology of course planning proceeds by using the model to look ahead imagining
something that might happen each one of these imaginings of the future from a state action pair
is called a projection i'm going to use this word projection this is where we imagine a future
okay and then after one or more projections we compute something and then we back it up
and that's called a backup and this goes on forever okay so now from this so this diagram
is a typical backup i'm i'm thinking i'm thinking about this state and i'm i'm looking at these
state action pairs and imagining might happen so what would be the projections in this picture uh
uh shabanch where is the projections in this picture
at the top good that's totally wrong and since he's he got it totally wrong then everyone can
can just do it where the projections the projections are where you're imagining the future from a
state action pair this is my test to see if you're actually following my definitions
starting from state action pair you imagine the future okay these this is a state
this is a state action pair because you can tell because it has an action on it and it comes from
a state so it's a state action pair and then you imagine the future the projections are here
there are three projections we're looking ahead all the actions i might make and i project what
would happen and i and i figure out how good they would be i take the max and i back it up
okay so the backup then goes from the leaves to the top of the of the process okay okay dylan quick
well it's it's from from the state action pair to where it goes this this part is the projection
right okay good and what about this picture dylan
where are the projections here
say that again
you should have you should be sure by now
so there are the projections so this is this is a long skinny sequence there's a skinny backup
so uh we're imagine we're probably sampling instead of doing all possible actions we're
sampling an action we're sampling a next state we're sampling an action after that and we're
sampling a next state after that but these two are the are the projection parts the other parts
are parts that the agent is doing the agent says suppose i do this action oh and then ask the model
what would happen the projection okay and so what about the backup here okay so the backup here goes
from from the from the leaves always goes from the leaves to the top uh no no no no not the way
i'm going to use the word okay and this this is this part anyway is definitely your choice
it's not an imagination about what the world might do okay i'm going to use the world i'm just
gonna okay so um and then the last one the projections are here um now these two states
they might be the same state maybe i imagined this one and i said huh now what what if i was there
and i did this one so that um so that uh if they were the same state you might imagine doing the
same thing but in fact by definition as a backup they are separate backups and you'd get these two
two together and if you did this one first and then you did that one then it might be
a similar effect because you would change the value the estimated value of this state
and then you change use that to change just maybe that one that one okay okay i have to keep going um
uh uh good so
uh this is the biggest question what is the output of a projection
okay intuitively it's it's clear enough but once we get serious we have to decide what is it really
because we're using a function approximation and uh our states are probably real valued
feature vectors and so uh what do i need what what is the output of a model like i'm in this
i'm imagining being in the state doing an action but the world is stochastic many things could happen
so one thing i could do is i could represent the whole distribution of all the things that could
happen okay this isn't totally crazy people are doing this uh the system called pilco by
mark disson rothk and he's doing that uh but it's problematic because uh distributions are
are large of of real value feature vectors it's a it's a it's it's they're large they're complicated
they're they're gonna we want methods that are general and scalable and approximate and so that
we can we do this without committing to a very specific form for the function approximator
i am skeptical that we can do this okay this is the first question the first open question i'm
going to say i'm skeptical but i'm really saying it's open maybe you can do it as a distribution
but if if you did this then how could you roll without how could you iterate it how could you
go to another step because you'd go from a state action pair to a distribution here's a messy
distribution thing and then how could you go from there how could you roll on to the next uh
uh projection um you would be it's it's it's it's a little bit problematic now of course you
could always sample that distribution and then you have a sample model so you get the state action
pair and you get a sample of the next state and then you could roll it out then you say okay there's
the next state i could say okay now suppose i was there what i could what could i do there and you
can go on um but you really have many of the same problems because you have to learn the distribution
because you have to you have to generate a sample from that distribution you have to represent it
and uh so so anyway this is this is this is this is a real possibility but it's um
but it's it's it's potentially difficult to make that to learn the distribution from which you sample
and you notice that now planning has become stochastic because you'd have to do many samples
like in Monte Carlo tree search of that next state in order to average over them and get
expectation whereas up here it was deterministic i get you the whole deterministic distribution
okay and then there's the third case which i like the best which is that you learn the the
the output of a projection is an expectation an expected feature vector call us an expectation model
and um uh so this is also deterministic but maybe it can't be rolled out uh because you get this
you know this average of feature vectors for the next state and um uh it's straightforward to learn
this expectation models because that's what all of our algorithms do they learn expectations
and um but in general we've lost information if you only have the expected next feature vector
instead of the whole distribution you lose special things but uh but there's this important fact
mathematical fact that in the special case of linear value functions you actually don't lose
anything so um i i i guess i don't have time to do this equation so i'll just say that the
point it's just a math equation that doesn't matter anyway but basically we can show that
if you do you're doing the update with the distribution model and you can write up mathematically
this is what it is and then just through a few steps uh you can prove that you get exactly the
same thing if you uh if you use an expectation model so here this is this is the probability
distribution of the next states here we have the expected next feature vector for the state
and the the action or option o and you can show that these are equal in the special case
where um the value functions are linear okay um so this is open questions open questions so
um this is just a proposed strategy is that with linear value functions and expectation models
and so uh you know i just want to talk a little bit about this question should the value function
be linear uh it allows us to do this and doesn't really lose anything but really it's a question
of moving the the work around whatever you do you have to learn the uh a non-linear relationship
and the strategy of of an expectation model is that the non-linear work is done in the state
update function so it puts the burden on the state update function and um so here i want to talk
about zack's term is zack here oh good i can claim it was mine and uh and and um we have this this
this picture from the book of the shape of all the backups now these are the shapes of the backup
really uh this side is planning that side was seen as not planning you know just teedy and
Monte Carlo learning but now i want you to think that really we can do both sides as planning
planning could could could involve a a short not just the wide backups of of dynamic programming
and and tree search or exhaustive search but um you can do the skinny backups um and so my my
long short start is that i'm going to uh those in the those in the projections is that i'm gonna
argue that really everything can be done with the the smallest uh the smallest backup just looking
ahead from sample one action and sample one expectation outcome and that's that's that's
i think is a cool way to do planning and you you can do that without losing anything because
if you want to assemble a bunch of these tiny backups in the right order or in just over and
over again you can learn a long plan okay um so i have one more slide uh just going to briefly
talk about the average ward setting i'm just some of you know what it means um some of you you don't
but if you do uh really when we use function approximation we have to go to the average
ward setting we have to give up discounting and i just want to make the observation in front of
you all that that uh this planning with average ward it's still a totally open question i thought
it was easy but i was thinking about the other day with with zack and it's really an open open
question it's even open for the tabular case you just take one step dyna and try to make an average
reward version that would be a paper because it's it it's not at all clear how to do it
uh so there if you're looking for a thesis topic for your to do this summer to get your
master's done just in time for september uh that would be a good one if you don't have one already
okay and there's also questions whether the model should should give us the the the expected reward
or the expected difference between the reward and the average reward okay five minutes i got
less than that don't i i give you a bit more okay no i need you we're gonna i'm not gonna i'm not
gonna be that bad but thank you for being so generous okay so um i think i'm done and um
these are these are the questions that we started with the open questions should the model generate
sample states or expectations and if it's going to give us expected states should the value function
be linear we've seen how those fit together nice and and the question is a further question is
can state update support that can it learn a good enough state features so that the value
function can be linear without losing something important and then there are other questions
about whether the model whether this suggests the model should be linear as well or whether it can be
semi-linear which means like a squashing function applied to a linear function we talked about how
planning should you know once we combine average reward with planning unsolved problem we should
work on that uh we should also worry about how should planning affect the actual actions and
what problems should direct the construction of the option models and uh and i can't i shouldn't
try to explain the last one it can ask me about it if you'd like oh and i sort of said my answers
is that we want maybe we want to expect the states maybe want the value function to be linear
maybe we can support this i don't know i don't know and this is the question of feature acquisition
that should be the sub problems maybe and uh maybe we can describe them by the features okay so um
i'm done thank you for your attention
okay we do a little bit of time for questions i ended abruptly there but that's the story open
questions model-based reinforcement please okay so uh i that's probably should have been one of my
my closed questions because we definitely need off-policy learning in order to learn the models
in order to be do it efficiently and so the part of the premise is that we're doing off-policy
learning and we have we have a suite of a few methods that will work on that nowadays
yeah off-policy learning is essential
so i would assume that you would want to learn lots of value functions and not just one
if you want all of them to be linear in your representation then that's a lot of burden
on your representation yes so if all the complexity is in the state representation then what is the
model really giving you well it's giving you the dynamics which is it's not in the state the state
doesn't give you the dynamics uh it is a lot of work on the on the state update function
and but more importantly i'm just realizing that i forgot to thank my new collaborators which are
mohammed or zaheer and yi wan who uh we we are been working on this and they should be up here
and um and we had we submitted a paper on on expectation models to uh to itch kai itch kai
and we accepted itch kai so that part's been written up and uh we're working that out
yeah so so a lot of work is going into state update
that's that's the strategy i think i think it's might be appropriate good
um i have a couple of observations i don't know even whether i'm on the
but i am interested in applied intelligence uh the one observation is wall street they seem to
have a numerical model of the world you know so i mean that's one world or one model that you
can look at um i it seems to me they're far more numerical than other types of means the other one
is a situation of a duck hunting sorry the eagle hunting ducks and there it's not linear like the
ducks the possibility of the duck movement is linear but it radiates you know so it's each
duck in the plot can radiate any number of different directions so i'm not sure whether
your model could cover the eagle catching the ducks or you want to give that to us
ha ha ha thank you that's good so this linearity thing um it's very important that it's linear
in the in the features okay and and this is this is the trick it's sort of already known
it's well known that anything can be linear if you arrange the right features uh so you could
do the duck uh you you could you could presumably capture the higher order the uh the interactions
between the so what do you lose when you get linearity you know you have two features a and b
well if the right choice or the right value depends upon both of them being present at the same time
and then then then then you can't do that linearly if the linear function you can only say oh this has
it has a certain effect this has a certain effect and uh there can't be a special thing that if
they're on together okay and so what what you do with that none and then when there's an interaction
between variables um you know maybe it's it's uh they're both bad but if they're on together
then it's good okay so it's exclusive or that's the kind of thing that you can't do but but we've
known since the beginning of neural networks that that um if you then add a third variable for the
conjunction of the two original features then you can learn the nonlinear function in the original in
the first two okay and so the same same is true so it's it's not a principal limitation in any way
so think again about a nonlinear network like like um you know in alpha go it learns this
complicated function many many layers it's nonlinear but it's linear in the last layer
okay so if you had some way of finding the features in the last layer then you could be linear
and so in some sense what we're just saying take the that thing that you worked out in the last
layer and make that your state and so as your state then you would have to your models would have to
produce it you see it's it's it's it's it's a strategy it's it's it's I think I so why am I
advocating I'm advocating because even if you don't if you do if you were going to try to learn a
nonlinear map uh that that nonlinear map would have to figure out that that a that these two variables
are are need to be treated especially and they would have to create the conjunction term inside
that that nonlinear mapping so it's like we're doing the same work but we're pushing it into a
different place we're pushing it into the state update yeah other questions
okay thank you very much

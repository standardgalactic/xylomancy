Thank you, everybody. It's been so nice, you know, having this time to get to know all you and, you know, I feel I've gotten a little bit of time with all you going to various events and, and you're all just lovely people.
And I've told you how lucky you are, and, but now we're coming down to the end, okay? Your minds are like folds to the brim. You can't take any more ideas, any more equations, any more, even important things.
But more importantly, you've learned a lot, and yet, you know, it must be striking you that there's so much that you don't know, so much more to learn. And, you know, there's, there's so much reinforcement learning to learn, that whole thick books, somebody's got, and that's just the introduction, you know?
And, and then there's all of deep learning, and this is not enough. Like, you have to, you're trying to, we're trying to understand how the mind might work, okay? So, you have to know something about psychology and neuroscience and philosophy and anthropology, maybe, and linguistics, control theory, all the statistics.
You have to learn so much. It's like it's impossible, okay? And it must, must be kind of scary to you. I'm, I suspect, is this scary a little bit to you? How much there is to learn?
Yeah.
Yeah. So, the good news is it's impossible. Because it's impossible, you don't have to, you know, ask, ask forgiveness or permission because you're not, that you're not going to learn all of it, okay? You're only going to learn part of it.
Because you have to, you have to leave some time so you can do some of your own work that contributes, not just reading and absorbing. You want to contribute, okay? So, you have to pick a balance between what you learn and, and what you, what you work on yourself and try to eventually give back to your community.
So, this balance is, is something you're always going to have to work on. And so, today what I want to do, I want to say, I probably want to do too much today. I want to talk about how you can develop your own thoughts and, and how you can, can participate and contribute.
And I want to tell you a little bit about how you might do like a first contribution to reinforcement learning. The simple trick I call completing the square. I'll tell you a little bit about what I'm doing. And this is too much. It already is too much.
And I'm going to tell you, I want to talk a little bit about AI and society, which is also a different kind of research opportunity.
So, there's all this, there's all this stuff to learn. And first is a question of how do you learn some more, okay? And I think you know these things. You know about the online course from the Whites that just came out last Thursday. How many of you have, have, have registered for that?
Okay. And you know, you know about my book with Andy Bartow. How many of you have that? Okay, that's good.
Now, as you learn about all this stuff, there's a problem because, because there are no authorities in science. Now you might be thinking I'm an authority, but there are no authorities. And you, you, number one rule is you can't be impressed by the part of all that stuff and all those different fields that you don't understand.
You have to be respectful, okay? You say, I don't know that. Maybe it's good. Maybe it's not. You don't know. And the fact is, when they, when they look at science, like from 100 years ago, most of it is not very good.
Most of it is, yeah, doing some screwy thing that seemed important then and is not really important. So, I think we have to assume that that's true today.
Okay? So, you, you cannot say you have to learn everything and you have to, cannot say this is really important because other people understand it and I don't. You should, you have to, you have to work from your own mind. What do you understand and, and work from there because you want to make a contribution so you, you've got to work from things that you understand.
So, don't be impressed by what you don't understand and there's a flip side to this. When you come back to give a talk, you don't try to impress others by what they don't understand. It's really tempting to do whenever you give me a talk, you know, present a bunch of crazy stuff that's really hard to understand and, and then people hopefully will be impressed.
You don't, don't do that. That's just a waste of time. There's all these different fields and all kinds of different people. Other people know more than you do about the other fields and so no one knows everything and you want to benefit from others and, but you can't, you can't put up barriers.
You've got to encourage communication between all the different disciplines. Okay, so no authorities. What do you do? Well, you have to be brave and ambitious. You can't just be, I'm going to do some tiny little thing and maybe no one will bother me. No, you've got to be brave and ambitious, but you also have to be humble and transparent.
Humble because it's a great task we're looking at. The task of understanding the mind and the problem, the problem is huge. It's, it's subtle, but it's not devious. It's not hiding from you. It's waiting to be discovered. It's up to you to see it.
And so I think the bottom line that I'm trying to express with this slide is that your thoughts are potentially a great value. I mean, you're going to work in, you're going to work in one of these fields and so you want to contribute and you have to, you have to feel that your, your thoughts are potentially a great value.
And, you know, I'll tell you as an authority that they are, but too bad there are no authorities.
So how can you train yourself to think well? So I think I mentioned that the key thing is to write. And so I made this slide.
The best way, the best way is to write and discuss with other people. And so maybe it takes 10,000 hours to become an expert at anything. People say this. I have no idea if it's true, but, but it does take a lot of work to become an expert in something and to have something to share back with people.
So this could well be true for thinking about thinking. And this pile of notebooks is my, my history of 45 years of writing about my, my thoughts. And you can see the one in the middle is from year zero 1974.
And it's all just a bunch of loose leaf papers that one day I kind of folded together in that notebook.
And then as time went on, I started to take my thoughts more seriously and then saying I should keep them in a good place and refer back to them sometimes and just sort of, you know, respect my own attempt to think.
And it's, so I'm saying it's not, this is really important. It's, it's, it's essential to have something to say is to have thought about the problem.
And it's not super difficult. You don't have to be a genius, but you do have to show up. You have to show up day after day, and you have to write.
You have to write. I mean, all I ask of you, all I would ask of you is all I ask of myself, which is to shoot for a page a day.
If you write a page a day, it's going to take you a long time to get 10,000 hours.
But, but that will be enough to put you ahead of other people. And you'll, because of that, you will have something interesting to say.
So I urge you to get a notebook. As you see, it ends there with my computer because I write in the computer now.
Here's the prose poem that I write for myself and I write for people sometimes in their notebooks.
Yeah, if you get a notebook and you write 100 pages in it, I would be happy to write this, this poem in your notebook.
To write is to begin to think and to write in a special place, a book such as this is to honor your thoughts and to help them build one upon the other.
Well, that's it. I think that's really important.
Okay. And the other half that we've told you today and I've told you other times that determination is super important.
So you're going to get stuck when you try to write down your thoughts. Absolutely. You're going to get stuck.
You're going to, you're going to reach what appears to be a dead end with nowhere to go, but there is always somewhere to go.
And there, so here are some techniques for getting moving again.
Number one, just to find your terms, you have some difficult question like what is intelligence?
When will the singularity arrive? Okay, you start defining the terms and then you say, well, what would it mean for it to arrive?
You know, which is the question. You just start poking at the question and then you say, what are some possible answers?
You know, might it be a million years, might it be one year?
And you go meta, what would a possible answer look like? What properties would it have?
And then my favorite one is retreating, which is you, you, you pose a really hard question.
You weren't able to answer it. So instead you go backwards and you ask a simpler question and you try to, try to build up to the ultimate question of interest.
You sneak up on it and you keep going.
The poor thing is you persist. And so these, these are the four key techniques that I use with myself.
Okay, so let's do some of this just for practice. Let's ask, what is intelligence?
What is intelligence? What is the mind? What is intelligence?
And I will start, I'll ask you in a minute, but let me start by giving you one sort of a definition.
This is from Ray Kurzweil. He says, intelligence is the most powerful phenomenon in the universe.
He's saying it's not supernovas, it's intelligence, which is, after a while, you think about it's not crazy.
Like how does the universe going to evolve? Will it all go to heat death?
Or maybe intelligent beings will have something to do with how the universe ultimately evolves.
I think I would guess that intelligent beings would have, have a role to play in the long term future of the universe.
Okay, the other thing I like about it is it says phenomenon. Okay? It's a phenomenon.
Intelligence is this thing that you see happening out there in the world.
It's, and, and, okay, so I like that. But, but what's wrong with this definition?
It's not descriptive.
Did I get your word right?
Yeah.
It's not descriptive. It doesn't tell you what it is. It tells you what a property of this thing is, assuming you already know what it is.
Okay, so can, do you think there's, let's, let's try to make a definition that actually tells us what it is.
Okay? Now you may know that some AI textbooks will say intelligence is, we don't have a definition and we just can't do any better than that.
And that's just not good enough. Okay? It's really not good enough. You can't let yourself say that.
Or at least you should be very disappointed yourself and put it high up on your list to fix it. Okay?
But, so can we do it? What, what, can anyone give me a definition, a meaningful descriptive definition of intelligence?
Yeah, I sure no one can hear you.
But it's the ability to adapt to changes. Good. Anything else? Yeah, another back there. Just shout.
The ability to generalize.
The ability to generalize. Good.
The ability to manipulate your environment to achieve goals.
The ability to manipulate your environment to achieve goals.
And exploit them forever, forever.
Okay, so I'm gonna, I'm gonna stop there because that's pretty close to what I think is the best definition.
This is by John McCarthy, who is like the founder, he actually coined the term artificial intelligence, founder of the field.
Intelligence is the computational part of the ability to achieve goals in the world.
I think that's pretty good. Okay, but, but at the same time, there are all these undefined words in it.
Like, what is a goal?
Maybe that's the key thing. The ability to achieve goals.
Okay, we could go further there. I also, like my own quote, this is me, intelligence is in the eye of the beholder.
Which I mean quite seriously. It's like it being a phenomenon. It's something that we see out there, like, oh, yeah, I want to understand that as having a goal.
I think goals are in the eye of the beholder.
Okay, let's do another one.
The predictive knowledge hypothesis.
This is the hypothesis that almost all knowledge of the world can be well thought of as statistics, that is predictions, about the agent's future data stream.
Okay, now that's an outrageous statement because knowledge could mean knowledge, knowledge about, about podiums and people and, and physics and geometry.
We have all this knowledge and I'm saying that everything could be thought, almost everything, could be well thought of as predictions about the future data stream.
Okay, now can you poke holes in this idea?
And, and, and have your name not be Dale Sherman's.
Okay, Dale.
I don't even know what that means. It's got like five syllables in it. I don't know what it means. I really don't.
You mean knowing what would happen, really that's, that's, that's a fact about your distribution of the future.
If the distribution was to go this way, then it would also go that way.
So if it's counterfactuals, if you mean like, if things didn't be different, something else would have happened.
You know, that's just, I don't even know if that's knowledge, right?
Maybe it doesn't matter because that thing didn't happen.
I think the real point of counterfactual of that way of thinking is that it will help you for the future.
So we'll have implications for the future.
But there are some obvious, there are some flaws in this idea, okay?
So if you're thinking of one, speak up. I just want, I got, I got a list of four of them.
Here I got the exceptions, okay?
Yeah.
Knowledge of history?
Knowledge of history of the past.
Wisdom.
Wisdom?
What might wisdom be?
Explanations.
Explanations, would that be knowledge?
Yeah, good.
The agent implies singular?
Yeah.
Yeah.
One agent won't see everything?
No, the agent only sees its stream.
The data stream, this is the implicit essence of reinforcement learning is that all there is is the data stream.
There's actions going out, there's observations or states coming in, and there's a reward coming in.
And that's all the world is.
The world is something you send bits to and something that sends bits back to you.
This is the reinforcement learning perspective.
Or maybe it's the computer science perspective on minds that we're exchanging bits with the world.
It's kind of particular, but I think it's something I'm really committed to is the single agent perspective.
That doesn't mean there aren't other agents in the world.
Like, you know, I'm an agent here and I can see you and I think of you as agents and you are doing the same thing for me and all the rest.
But we each have our own data stream and something that I know is about my data stream.
It's not about your data stream, I never even see your data stream and I don't care.
Okay, there's one more obvious one that we haven't mentioned yet.
Martha, is that Martha?
I have, I have what?
You have a sister.
Okay, yeah, so can we convert these sort of like factual truths?
When I say this is true, the Eiffel Tower is in Paris and you have a sister.
Martha has a sister and are those, are those just statements about your future data stream?
Okay, so I think that's, the hypothesis would be that they are well thought out as that.
What you might see if you go to Paris or what you might counter when you call up where your sister is.
Another quick comment?
Mathematical knowledge, thank you.
That was the obvious one that I wanted someone to pick up.
Good, so now I can go on.
Some of the exceptions are mathematical knowledge.
Mathematical knowledge is true in any world, right?
So in some sense it's not true about your world because they're just mathematical truths.
They're not a fact about your world.
So another way of writing this you could say explicitly it has to be knowledge of, specific to your world
and if it's true for all world then it's not world knowledge.
Okay, but also things like policies are not predictions.
If you have good features that's not really well thought of as a prediction I don't think.
It's just a helper towards predictions and memories of the past or beliefs about the past, maybe not included here.
Okay, good, so remember why I'm doing this.
I'm doing this, I mean I care about this, I care about all these things
but I want you to see what it's like to try to write down a question and think about it usefully.
You say, well what do we mean by knowledge?
What are these things predictions?
What is this whole thing about the agent data stream, the one agent data stream?
That's sort of a presumption of the question.
Okay, good, we could do this all day and if we never get to anything else on my slides that would be fine
because I think it's important to practice thinking and what it might mean.
Okay, and this is why, or this is one thing that I think is important,
is it the most important insight that you, you will ever contribute is probably something that you already know.
Okay, it's probably something that you already know and it's probably something that's obvious to you.
Okay, but the problem is it's not obvious to everyone else and so it's so obvious to you like you can't even see it.
It's like air or something or yeah.
So I'm just going to give some sort of silly examples of this phenomenon because it's really important.
You have to like stop seeing through everyone's eyes, everyone's else's eyes and you have to see the obvious because the obvious is your greatest contribution.
So for example, when I started, when I was in college, there was no reinforcement learning.
There was no reinforcement learning. There was barely any machine learning.
But there was nothing like reinforcement learning and yet it was obvious to me that, you know, agents or that, well like animals, animals do things for food
and they don't like to get hit and we, and people have goals and they vary their behavior to get what they want.
This is obvious and yet there was no reinforcement learning.
There was no field that studied that in engineering.
Okay, so it's really common.
It's really common that there are obvious important things that are not recognized by your whole scientific community.
Okay, so I'm going to give my silly examples.
The discovery of gravity by Isaac Newton.
You know the story?
He was sitting under a tree one day and the apple falls on his head.
It hits him and can't you just see him say, hmm, there's an apple.
Oh, objects fall.
And so you could see it like running to his friends and say, hey, objects fall.
Think about it.
Okay, that's what it would be like.
And yet it was important.
It was important.
The thing that everyone knew, the realization that it needed some kind of explanation or an idea was a big deal.
That was the discovery.
Okay, the discovery Charles Darwin, the discovery that people are animals and evolved from animals.
Now this was harder for people to realize was obviously true.
I mean, if you look at people, we're animals.
We got legs and we eat food and we excrete stuff and we have kids.
We are animals, okay?
But it was not an acceptable position in Victorian England or all around the world, many places around the world.
And you had to really make that point and made people upset.
You know, so I mean, it's kind of like how nowadays we say that people are machines, they're biological machines.
And we haven't fully absorbed that one.
Maybe we haven't fully absorbed, but it's obvious.
It's obvious that animals, you just look around, we saw any kind of an open mind.
But Charles Darwin was the one who got that or maybe it was his father.
I like to say the air is also kind of like that.
It's invisible, but you know, obviously there is something here.
Like you see the wind will blow the trees and the trees move all around and there must be something there.
Okay, and like I said, reinforcement learning is like that.
This was discovered really by Harry Klopp.
Harry Klopp was this wild, independent thinker, a wonderful man.
Sadly, he passed on too early.
But his skill was just to think for himself and to realize that machine learning had lost track of reinforcement learning.
It was just doing supervised things.
Okay, so with this silly list, are there obvious things that we struggle to see, obvious things that we struggle to see now?
This is a question you should all be asking yourself.
So let me ask you, are there obvious things?
Can you think of something that might be obvious and not recognize?
I have no idea what you guys might say.
It's a hard question.
Just to take a minute, yeah.
Have any feelings what it might be?
How about intuitive knowledge that comes from beyond?
What kind of knowledge?
I even disagree with my partner's definition of intelligence.
I think he has learned that intelligence comes from within that it's not complicated.
Okay, so that's good.
But I don't think it qualifies as being obvious.
I mean, we're not faced with facts that suggest that.
Can I think of something that's really obvious?
Yeah?
That all people could be equal.
Sort of an ethical philosophical point of view.
That's good.
But can we do anything about the mind?
The bodies do all the computation and minds don't really do anything at all.
Well, so when you're trying to think for yourself,
you don't want to be arbitrarily and unnecessarily controversial.
So we would temper Patrick's idea to be that the bodies do an important part of the computation that we attribute to our minds.
Not that the other computational things do nothing.
But the bodies do a lot.
And this is really true and this has been a good insight from robotics and from biology.
Okay, I don't know.
That was all good.
Let's go back to my list quickly.
And this is going to be like, remember, I gave her a talk before, I was kind of throwing, I threw a bomb.
And so this is all going to be bombs.
Because I'm going to say things that are obvious and that we don't know them or we don't believe them.
So these are possible and it's enough to be possible.
These may be our obvious things that we are struggling to see.
So no animal does supervised learning.
There is no training set for our muscles.
No mind generates images or videos.
Isn't that obviously true?
We don't generate them.
Okay, I'm not counting painters and videographers.
But in our minds, we don't have to generate them.
That's one thing we don't have to do.
We process them.
We don't have to generate them.
I'm going to go on before I get an argument.
Neural networks are not in any meaningful sense, neural.
Okay, that's really just, that really is obvious.
And yet so many of us don't want to acknowledge it.
People are machines.
The purpose of life, this is the reward hypothesis, that the purpose of life is to get pleasure and to avoid pain.
And that that's a simple, effective way to understand people.
So that's sort of a good dramatic hypothesis, which might be true.
And we struggle to see it.
The world is much more complex than any mind that tries to understand it.
Therefore having a prior distribution over possible worlds would never be reasonable.
The mind is computational and computation is increasing exponentially with technology.
And so we want things that scale with computation.
Any kind of human input doesn't scale.
So if we try to make our AI system smart by giving them our knowledge, that's kind of a somewhat hopeless,
and not hopeless, it just will not scale.
And it will be just a little bit by little bit.
The only scalable methods are search and learning.
And so there's some bombs.
So I want to close this part of the talk with some more advice.
So number of advice.
Think about experience as the data or slogans.
Experience is the data of AI.
It's like we were talking about just a minute ago.
This exchange of bits back and forth, that's the data.
And so we shouldn't ask the agent to achieve something that it can't measure.
Nice thing about reward is it comes in every time step.
You can measure it.
It's not imaginary.
It's not available somewhere else.
It's available to the agent, and so it's okay to ask him to measure it.
We shouldn't ask the agent to know something that it can't identify for itself.
You can't tell directly whether some sentential symbolic statement is true,
but you can see your sensors, and if you make a prediction about your sensors,
you can see if you can verify if it's true or not.
It's very important to distinguish between the problem you're working on
and the solution to the problem.
These things are very often confused,
and we want to approximate the solution.
We do not want to approximate the problem.
It's sort of a bomb because this is really, really true.
You're not going to see right away why it's so important not to approximate the problem.
But one reason is that your solution method will scale up with computation,
so you should pick a problem that's the real problem you should not try to approximate it.
That will not be lasting.
And more than even the approximation issue you want to separate,
the thing you want to work on, you should say,
is it a solution method or is it a new problem?
So maybe if you say, oh, there's risk and you should work on risk,
maybe that's a new problem.
If you're looking at multi-agents, that's a new problem.
I know you don't like people messing with a problem too much.
The problem should be MDPs, and our problem is hard enough.
Our only difficulty is that we don't know how to solve the problem.
We don't need a new problem.
That's the way I feel.
Okay, now as you're trying to solve the problem,
I like to, a key heuristic is to take the agent's point of view.
Assume you are there, you were faced, oh, I had to do these things.
I could see that.
What would you do to think, put yourself in the point of view of the agent?
Really helpful.
You should set measurable goals for the subparts of the agent.
Like if you have a part that's the value function, you should work on that.
If you have a part that's the model, it should try to do the model.
It shouldn't worry about the reward too much,
or the value estimates being managed to work on making the model,
making an accurate prediction of the transition structure of experience.
Okay, so that seems obvious, but there are lots and lots of people in our field
that are saying, no, no, don't have different goals for different parts.
Do everything from the one goal, and they use this phrase end to end.
And I'm not sure what it means, but it's kind of the opposite
of setting measurable goals for the subparts of your agent.
A really good strategy, I'll go more on this in a minute,
is that you should work by orthogonal dimensions, work issue by issue,
and you should work, I like, I think you should work on ideas and not software.
Yeah, that's a good bomb.
Okay.
So these are my, some suggestions, some advice,
some grist for your mill as you try to develop your own thoughts.
Remember, there are no authorities, and I'm not an authority.
Okay, so the simple trick for doing research is to realize that you can divide
the whole area of reinforcement learning into dimensions,
and it's just much better to think of dimension by dimension
rather than whole overall problems.
So here's just a massive list of the dimensions, and okay,
so all that stuff's going on.
Function approximation, state values, action values, model free, model based,
bootstrapping, Monte Carlo, or you have the things up at the top.
Okay, now there's a top level division here.
Problem dimensions and method, solution method dimensions.
Okay, remember I've said this is the most important thing to keep clear in your mind.
What's a problem and what's a solution method?
So among the problem dimensions, you can look at the problem of prediction,
or you can look at the problem of control.
You can try to predict what will happen, like predict the rewards as in a value function,
or you can worry about how to select actions to maximize your reward.
We often switch between these two as we try to make progress on some issue.
The distinction between bandits and Markov decision processes, that's a problem distinction.
The distinction of the setting, like is it a discounted setting,
is it an episodic setting, or is it an average award setting?
That's a problem distinction.
It's not a method distinction.
Your problem could be fully observable.
You could receive the states, or you might only receive observations as in a POMDP.
That's a problem distinction.
You could also, maybe I'll stretch it a little bit,
but you can talk about are you trying to get theoretical results?
You're trying to get empirical results.
If you're trying to do theory, do you want convergence theory or you want rate theory?
The top theorists are bored by just ordinary convergence theory and they want rate theory.
All these dimensions, I've tried to arrange them so that the easy cases on the left,
the hard cases on the right,
generally like we would do prediction before we did control,
and we would do fully observable before we do partially observable.
Now the method dimensions, function approximations is of course a big one,
whether you have a model or not is a big one,
though this is a solution.
It's a solution method issue whether or not you have a model
because you were be learning the model and then you'd be using that model to help you solve the problem.
The problem would be unchanged if you changed from a model-free method to a model-based method.
Off-policy on-policy is...
Oh, maybe that one's kind of mixed, right?
Because you could, part of setting up a, you could set up an off-policy problem.
Yeah, maybe that's, maybe that really, it's not mixed,
it deserves locating in both, in both as a problem dimension and as a method dimension.
Okay.
So what do we do?
We can try to draw the frontier.
The frontier of the things that we know how to do, right?
Remember everything on the left is easy, the things on the right are more difficult.
So we can't do everything on the right.
We can't get convergence rate theory for a nonlinear true online temporal difference learning, okay?
We can't get the full, all the way to the end.
So we could try to draw a border, like to, to say where we can go.
But of course this is hopeless because really they interact.
If you make one choice, you can then, you make one, if you want to, if you really want to make, move to the right,
you may have to move backwards on some of the other dimensions.
Okay, so, so here, a typical case, this is the research strategy that I call completing the square,
which is you pick some of the dimensions.
So you might pick here, we're picking model-based as the primary thing that we're interested in,
and we know we can do model-based with dyna, we know how we can do it in a tabular case,
and we might try to extend it to the linear function approximation case.
So you see the idea?
You just pick a couple of things and say, oh, can I, can I move along, along, along the right to left spectrum?
And so this is something that we did in, in our 2008 paper with, with Chaba and Mike and Alborz.
And so this really is the way I do my work.
I, I go through this dimension and say, oh, I'd like to move this out,
I'd like to handle nonlinear function approximation.
So how do I do it?
Well, let's go back to, away from control, let's go back to prediction.
Let's consider a discounted case, that's the simplest.
You know, we do the simplest case, we retreat, and then we try to go forward.
Okay, just here, here, so you, you might try to continue on the model-based direction and go to a nonlinear model,
or go to control.
It turns out these, these are, that's the state of the art.
Really how you could do model-based with control, and how you could do it with a nonlinear model is the state of the art.
Or you might focus on average reward.
Average reward is where you don't discount, you just try to get the most reward per time step.
And, and, and you might try to make an on, off policy version of those algorithms.
Or you might try to make a model-based algorithm.
It turns out just, just a model-based algorithm for average reward, and trying to do it online, as supposed to just, with, in a batch way,
is a really challenging, unsolved problem that, that I've been working on.
That's, or that's a discerning of more work.
Okay, here's another one.
If you want to do convergence theory, well, what do we have?
We have all the red ones.
The prediction case, temporal difference learning, on policy, and linear.
We can do that.
But we, we don't have it for control, for the control case.
That would be a, a research topic.
You could take, you could see way down there on the bottom, there's this idea of interest and emphasis.
I've recently come to realize that this actually interacts with everything.
And whereas Martha and Rupam and I wrote a paper on the off policy case, the on policy case is, you know, it's supposedly easier because it's to the left.
But it's, it, it, it's, it really is untapped and hasn't been worked out.
Okay, so that's the, this trick of completing the square.
Any questions about that?
So you have to kind of know what's been done, but you work along the dimensions and you try to slide to the right.
Question, Andrea?
Maybe for a higher level, you should, what's the problem?
It's not, you need to be so clear, so you can't really go over the line.
But you're, you're saying you don't approximate the problem.
So the real world is so large that you can't, I mean, you're so approachable that you can't really understand.
I guess there's a sense in which this is approximate because I'm saying, I'd like to do, I'd like to do every case on the, in the problem dimensions.
I'd like to go all the way to the right and I'm saying, well, I'll settle for something less as a stepping stone towards the real problem.
Yeah.
Okay. Good. I'm going to keep going.
I'm going to tell you something about the research that I'm doing now.
So first, the landscape of machine learning.
The old view is that there's supervised learning and unsupervised learning and maybe there was reinforcement learning.
It was maybe because unsupervised and supervised, that seems like it should count everything.
You're either supervised or you're unsupervised.
But now we slip in reinforcement learning somehow.
But this really has been feeling more and more dated to me.
And I like thinking about things more as prediction learning, control learning, and representation learning.
And the, and a fourth one maybe is integrated agent architectures for a whole system.
Whereas, because classical machine learning was just trying to do one little part of a whole agent.
And when you worry this other, so the stuff I'm going to tell you about today is we're worrying about representation learning.
We're worrying about how it might all fit together into integrated architecture.
Now, let's go in one step deeper.
This is about machine learning.
Let's step into reinforcement learning, talk about the landscape there.
In core reinforcement learning, we are focused on learning value functions and learning policies.
And next we need to go on and worry about states, what are our state features?
It's representation learning, learning about what skills do we develop?
Larger things, models of the world are larger things.
And all these, these new topics seem to be wrapped around a notion of the agent setting aside for a moment the real problem that it's working on in terms of reward.
And just working on some kind of a sub problem.
And I don't want you to fall asleep given the time and everything.
I'm going to try to motivate this just by showing you some videos, okay?
And particularly cat videos.
Those should always keep you awake.
So what we see here are just animals doing some purposive thing.
Whether it's swinging on a branch or pushing this bottle around or playing with a toy mouse.
Animals pursue problems that are not the main problem.
They're playing with a toy mouse, not a real mouse.
Playing with a ball.
That lizard is playing with a ball.
Not with something that might sneak up and catch.
Okay, so, and of course people do this too, famously babies.
I really like this one on the left where this child is like looking at her hands and trying to figure them out.
You know, how they work.
It's very intent.
It's not getting food.
It's just figuring out how hands work.
It's fascinating.
And eventually gets tired of that.
That's the other feet.
Feet with the hands.
It's fascinating.
So babies are doing all this stuff.
It's not really about reward.
Or maybe it is.
I don't know.
It's not the main problem of their lives.
Here's another famous example of an infant just playing with its toys and doing all kinds of different things.
Very enormously active.
And of course sped up a little bit, but.
So what's going on?
You know, because I'm serious.
I like to think about reinforcement learning and AI in terms of people and understanding what people do.
And so we have ways to go.
Let me just go on.
So sub-problems.
There's a long history in AI and reinforcement learning of looking at sub-problems that are distinct from the problem.
People like my curiosity, my intrinsic motivation.
Rich Carolina did some old stuff where you looked at it in supervised learning context.
The options that you heard about yesterday, I think it was, are part of this.
And there's a somewhat settled issue is that what is a sub-problem?
A sub-problem is a reward signal and possibly a terminal value.
Like if you get someplace, that would give you a terminal value and you should stop there.
But when I say sub-problems are a reward signal, it means you might be a different reward signal than the original reward signal.
Okay, and then the solutions to the problems are an option.
That is, it's a way of behaving a policy and a way of terminating that behavior.
Okay, so we do have a sort of outer-outer loop.
What a sub-problem would be, what it means to be a sub-problem, what it would mean to be a solution to a sub-problem.
But there's still these key open questions, like which of all the reward signals that you might make up
and all the terminal values you might make up, what should they be?
How is that decided?
Where do the sub-problems come from?
And then even like, I think it's all obvious to us why a child playing with toys, that might be a good thing.
Why a cat playing with toy mice, that might be useful for it.
Even when an orca whale playing with a bottle, it might be good.
It's learning how its body works, it's learning how to control things in.
Maybe later it will want to control something that's in the water that floats.
But what is that thought? Let's spell that out.
What ways might they actually help?
Well, there are several ways that people have talked about.
I'm most interested in the last one, but let me just say them.
Sub-problems might help you learn good states, good state representations and good state features.
Or they might help you shape your behavior to make it more coherent and therefore more exploratory.
The last one is that sub-problems will help you plan at a higher level.
They will get you knowledge of the world that enables you to plan.
So I want you to think about this in particular.
Sub-problems, as we say, get their solutions as an option.
Once you have an option, you can learn what will happen if you took that option,
and then you can use that model to plan with.
This would be really useful if your values change, and then you can plan for the new situation.
Just like in a grid world, where someone moves the goal to a different place,
you can rapidly adapt to the new case.
What is this thing about states change their values?
That seemed nice and intuitive when I talked about the grid world and someone moving the goal around.
But really, if your world was totally stationary, why wouldn't you just learn the value function once,
and then you'd be done?
Why is it changing?
That's the first mystery, I think, of why these sub-problems are so important.
That's what I want to try to say a little bit about.
The key idea is what we call permanent and transient memories.
Suppose you're doing value function approximation, like you're learning a value function.
I'm going to show you in a minute some results from Go,
where the value function, the learning features of the Go board to evaluate the Go board.
You might imagine there's a weight vector, and let's just assume it's a linear function approximation.
Yes, you can see that.
We're going to update our weight.
The new weight is the old weight plus the step size times a TD error times the feature vector.
The value function, the prediction is w times x.
It's the inner product of the weight vector and the feature vector.
That's a prediction of how good it is at time t.
Then we look at the next reward and the prediction from the next state, the t plus 1 feature vector.
That's a TD error, and that's just a normal TD zero learning algorithm.
The permanent memory is learning exactly this way with a small step size.
It will converge slowly to the best approximate value function.
We're not going to settle for that.
We're going to add a second weight vector that's a transient memory.
It's learning in almost the same way.
It's as if w times the permanent weights plus the transient weights, w tilde is the transient weights.
The sum of those two is like a new weight, and that gives you the value of the new state
and also the value of the old state, and you're doing the TD thing as usual.
The transient weights have a larger step size, and they're moving faster.
Why might it be good to have transient weights?
This is happening up here.
The permanent weights don't know about the transient weights.
The permanent weights are going to try to learn the best function they can,
and then there should be nothing left for the transient weights to learn.
This is what's called the cascade, where you give one as a dominant, as a given priority.
If the permanent guys can do anything, it's not left and it's not left available for the transient weights.
It turns out in many problems this is a good idea, that the transient weights do not go to zero.
Let's look at go and imagine how that might happen in go.
The first two panels are two features.
The central stone, the black stone in the middle, that's a feature, and this is actually a good feature.
It has a large positive weight, and you learn that.
The permanent memory learns that that's a good feature.
This other one is a two-eye pattern in the corner, and this is also learned to be good,
but it's not quite as good for winning the game as the central stone,
according to the long-term permanent weights that are learned.
Now look at these two examples, these two different games.
This is a five-by-five go, so this is the whole game.
The permanent memory, remember the permanent memory likes this, so it wants to play A.
It kind of likes this too, but it prefers A in both of these two positions.
It turns out that in the first position, playing B is the winning move.
If you play A, you lose, but if you play B, you win.
If you just had the permanent weights, you'd not realize that.
By using transient weights, you learn that in this game, it's more important to get the corner than to get the middle.
You learn about this game by your planning and look-aheads in this game.
In this other position, it is right.
Move A is the winning move, and the transient weights don't interfere.
They let the permanent weights take the weight as zero, and it doesn't interfere.
So if you just do, if you just run a converging player that uses an extensively trained permanent memory to pick moves
versus a tracking player that both has both transient and permanent memories working together,
the tracking player wins.
It overwhelmingly wins, as shown in this graph.
This is across the axis.
It's just three different setups with just one-by-one features with a one-by-one and two-by-two features up to level three.
There's a clear effect in each case.
Okay, so that's interesting.
Even though the world is stationary, go is just a stationary problem,
it's complicated enough that you need to have changing value functions.
Okay?
Why is that?
So I think it's a very general phenomenon.
It's just that the world is much more complex than our mind.
And so as we live in the world, going from state to state, we need to tune special,
we need to tune our value function to the particular case we're in.
If we have to average over all possible cases, we cannot get as good a value function as if we adapt to the current setup.
So we have to embrace, well, just the fact that the world is so huge means you have to have approximation.
But because your approximation is always inadequate, you don't have enough weights.
So because of this, the best approximate value function will change as you encounter different states in the world,
even if the world itself is stationary.
So this is the bottom line, that a big world yields apparent non-stationarity,
and therefore your approximate value function should change.
The true value function is static, but the best approximate value function will change as you encounter different parts of the world.
Okay, so now I'm ready to give my answers to the three key open questions about sub-problems.
What should the sub-problems be?
Each sub-problem should seek to maximize a single state feature and then terminate while respecting the original rewards.
Formally, what I mean is that the sub-problem for feature I, you're going to have a different sub-problem for each feature.
I'll show you how you can be selectively in a moment.
But the sub-problem for feature I has the same rewards as the usual problem.
But in addition, if the option stops at time t, it gets a terminal value, a bonus for having that feature, the ith feature high at that time.
Okay, so it's the ith feature high.
This is just the normal value function with the permanent weights.
This is feature I's value, and if it's non-zero, like if it's one, then you get a bonus proportion of the standard deviation of the transient weight for feature I.
So in other words, if you have a feature where the transient weight is not zero, it goes sometimes negative, sometimes positive, it's significant, then you're going to get a bonus for reaching it.
Arrival at that state, you'll get a bonus.
And thus, you'll end up learning how to get to that feature.
If having this in my hand was a feature, I would learn how to get it in my hand from wherever it might be.
If you think of yourself sitting down having a meal, you want to get food, you want to get the pleasure of the food, and sometimes though you want to drink, and sometimes you want to eat a bit of this food,
sometimes you want to eat a bit of that food, maybe you need to put down the fork to pick up the spoon, all those things are just like in Go.
It's an extremely complicated function. When should you do which one?
And rather than try to get an exact nap from all of your sensations and all of your situations to what to do, you can say, oh, right now, I have high weight, I'm getting the fork in my hand so I can eat the lovely steak that I've already chopped up.
And so right then, getting the fork in your hand is high value, and so you can call out your already learned procedure for getting the fork into your hand.
And then you can just immediately form the plan, follow that procedure, the fork will be in my hand, then I could stab the piece of food and put it in my mouth, all good.
After I've done that, maybe I want to put the fork down and pick up my glass of water.
So these are the sub-problems that once you have learned options to achieve them, learned models for achieving them, then you can plan very effectively.
Okay, so the second big question, where do the sub-problems come from, you've seen my answer, the sub-problems come from the state features.
If I have a bunch of features and there's one sub-problem for each feature, and of course if that feature has a highly variable transient weight,
there will be many features whose weights don't change at all, and there's no purpose, there's no advantage to making them into the outcomes of your possible options.
So you don't need a sub-problem for that.
And how do the sub-problems help on the main problem?
The solution to the sub-problems is an option, that means just something you could do, you could follow that option, you could act decisively,
but the more important one that I've tried to emphasize today is that once you have that option you can learn a model of that option,
you can learn the outcomes of that option, and then you can plan in large steps.
Instead of doing this muscle switch and that muscle switch, you can put your fork down and pick up your water glass.
Okay, so let me just summarize that, this approach to integrated reinforcement learning agents.
A fully capable reinforcement agent must learn large things like new state features, new skills, and new models.
All of these pertain, can be explained in terms of sub-problems, and I've proposed problems of state feature achievement while respecting the rewards.
I'm not ever changing the rewards, it's just we get a bonus for achieving a state and terminating when that state is high.
It's a distinctive kind of sub-problem and it fits well into planning and representation learning.
The rationale for all this is that the world is big and therefore we have to use approximations, we have to approximate it,
and this means it will appear to change and you have to track it, it's going to be non-stationary,
and this really is why planning and generalization really make sense, because we're encountering different parts of the world,
different parts of the world at different times, and there's a certain repetitive aspect to the world.
You have to relearn when you come back to this place and relearn when you come back to that place,
and that repetitiveness enables generalization to make sense.
Okay, and then this can all be focused by looking at which features are transient,
and this allows us to focus where we create our sub-problems, which representations we use, which models we use, and how we do our planning.
Okay, any questions on this sort of direction?
Okay, let's save up your questions.
I think I'm going to be done in good time and I'm going to ask you for questions.
Good, Joshua.
Capsules?
Yes, yeah, yeah.
And then there's a nice connection in recent years to the notion of mental learning, where you have slowly changing properties,
fully changing parameters that could be captured in something stationary,
and then things that might change from one context to another,
and there's a lot of work in machine learning that I think catches the question.
That's right.
I was thinking about all that when I was listening to you yesterday.
Yeah, it's good.
Good.
Okay, let's go now to finish off and talk about what this might mean for the world.
You should also be obviously aware of about the impacts, the ethics,
because there are going to be a lot of impacts of the coming of artificial intelligence.
When people finally come to understand how our minds work,
well enough to make things, design and create beings that are as intelligent as ourselves.
This is a big thing.
This is a giant, I think of it as a matzo ball in the sky.
It's just a target for every scientist should be thinking about maybe this is going to happen within our lifetimes,
and this is like the biggest thing.
It's been a fundamental goal forever, not just science, also for the humanities,
and it's going to change the way we work.
It's a change of sense of ourselves, of life and death,
and the goals we set for ourselves and our societies.
And it's even a significance beyond humans.
It's beyond recorded history.
It's an event on the planet, at least.
And so, yeah, really, when we can understand the way minds work and make more,
it will be an event comparable to, well, maybe to replicators in life.
So, to think about it, you have to realize, first of all, that it's driven by technology.
It's driven by what we call Moore's Law.
It's a little bit of a misnomer, but this super trend of ever-increasing, cheaper computation.
So what you see on this graph is, versus years, we see computer power increasing steadily.
Now, this is a logarithmic graph, so a straight line on this graph would be an exponential increase.
And it's perhaps a slightly curving upward.
The death of this Moore's Law has been predicted many times,
but we keep finding new ways to keep it going, most recently with the GPUs.
So there's every reason to think this will continue, and it's almost unstoppable.
You can't even see, like, the two world wars on this graph.
Technology always pursues progress along this dimension, and, you know, it's economically valuable.
There's every reason to think it will continue.
And so we can't, like, forget about this. We can't ignore it.
We can't pretend it's not going to happen soon within our lives.
I mean, it might not happen, but it could also happen soon. I could see it happening.
I would say that there's, like, one chance and four that it will happen by 2030.
Okay, and one chance and two that it happens by 2040.
That's my own guess that we would have enough computation to make human-level intelligence.
Of course, that's a soft, that term is not well solidly defined,
but even if you're off by an order of magnitude, that's just another five years one way or the other.
So this is, what I'm trying to, the point is that this is going to happen,
and we should be preparing for it as a society.
But I don't want you to be scared, and I don't think we should be scared.
I think the first statement to make about this is it's a very human-centric thing,
AI is really the most human-centric of all fields.
It's about us, it's about understanding who we are and how we work, making us or amplifying us.
Not exactly us, but they are things that have goals that are intelligent.
So this is the essence of what people are.
And it's about making our lives easier and better.
That's why we have phones and eyeglasses and other kinds of technology.
It's all about making our lives better and more effective.
And so we should think about it as a humanistic thing, not as an alien, technical, artificial thing.
I don't even like, I think the name is unfortunate to call it artificial intelligence.
It's intelligence, and it's intelligence is what we are.
So AI is really us making or becoming the next people.
And it's just the next step in the grand march of life and evolution and creation in the universe,
that this changing ever-widening river that is ourself and mankind.
Okay, so this way it makes sense that understanding intelligence,
like understanding how you think and how we can achieve things,
that's got to be good, but you have to realize,
it's going to inevitably lead to ordinary unamplified people falling behind.
Some people will always improve themselves.
So if you don't improve yourself, you're going to be, in a sense, left behind.
And some people will design improved people.
Even if we decide that it's not all that economic,
there will be some people that do it just because it's really interesting.
So AI will inevitably lead to new beings, new ways of being,
that are much more powerful than our current selves.
So that's, I think it's just true.
Maybe it's one of those things that's obvious, but we resist.
The other thing I would say is that we should think about,
AI is being similar to ourselves.
So we should treat them maybe the way we would like to be treated.
So think of that similarity between people and the AI's.
Both are agents with goals.
They may be compatible with ours.
They may be conflicting with ours.
That's the ordinary situation.
People have lots of goals that conflict with each other
and other goals that are compatible.
And this is why we have our economies
and people find ways of working together to mutual advantage.
Even if someone wants to do something totally different from you,
you can easily work together on a project
or you can work for a company,
or they might work for your company.
And so it's okay that you might have many agents
that are very diverse working on different things.
And if you think about it in terms of symmetrically,
this can help you avoid the feeling of entitlement.
I'm an agent.
The AI is an agent.
We should treat them symmetrically
rather than put humans on a different status
as an overlord either way.
In the long run, the technology of AI
is going to be part of what disrupts existing social and power structures.
They will force us to re-examine our morality and our social foundations.
And this is not new, though.
We're continuing trends that are already thousands of years old.
We are very different from our predecessors
100,000 years ago,
or even 10,000 years ago, or even 1,000 years ago.
So AI will bring greater diversity of intelligence,
both natural and artificial,
and there will be biases against the newcomers,
and they're different.
They'll be feelings that they're taking our jobs
and we are entitled to our jobs.
But all these feelings, I believe, are counterproductive
and they will eventually fade away.
The question is, I think our key are
whether or not we will welcome these different kind of people
that will be coming amongst us.
Will we welcome independent AIs?
Will we offer them a path to join our society
in a productive way,
in a cooperative way, as sovereign persons?
So my vision for the future
is that we would have an open, dynamic, resilient society,
peaceful and prosperous,
with a diverse multiplicity of designs for the people,
for the cultures, for the values, for the organizations,
and you'd have people of all kinds, organic and artificial.
They would compete, they would cooperate.
You would have overlapping circles of empathy.
You might raise some of the AIs as your children.
They might care for you.
A successful outcome is one without envy
and without entitlement.
Now, what we have to worry, you may be worried,
well, what if what we want to happen doesn't happen?
And I think you just have to let go of that.
You can't insist that what you want to happen
is going to happen.
I mean, any more than some bizarre AI can insist
that what it wants to happen is going to happen.
We have to cooperate, we have to work together,
we have to see what the universe wants to happen.
That being said, I do think that the rise of greater foresight
of more far-seeing agents in this universe
has to be one of the few things
that we can think is probably generally good.
Thank you very much.
applause
Do we have a mic?
I got a yell.
Okay.
So we, researchers in the AI,
when we have kids, inevitably we become pediatric psychologists
looking at, you know, watching our kids how they learn,
how they agree with us, disagree,
they don't agree when we ask them to do things.
We realize that actually we humans are balls of emotions.
Our emotions dictate often what we do.
If somebody is offended, they will act one way,
if they are flattered, they will act a different way
in the same environment, in the same conditions.
So when we do our research creating these intelligent machines,
is it a mistake to put emotions out of the equation?
Emotions are in the equation.
They have to be in the equation.
Maybe we have only gone partway there,
reinforced in the system.
What is an emotion?
Emotion is a reaction to a situation
that maybe is not based on a thorough analysis,
but is an intuitive judgment.
How do we make intuitive judgments?
Well, we apply our value functions.
So I want to suppose, I want to claim,
that value functions are a basic kind of emotion system.
They tell us what we think are good situations
and what we think are bad situations.
And the TD error is the change in that,
and that determines whether we are happy
or displeased by what happens.
So value is a prediction of future reward.
So if anything, it is like hope.
And if you are predicting a bad thing, then it is like fear.
So it is like a bipolar kind of emotion.
Our emotions are more sophisticated than that
and more subtle than that,
and they involve other goals
and other things that are built into us by evolution.
But I absolutely resist the notion
that we have avoided emotions.
A value function is a value system.
It is not really a coincidence
that the name is the same as how we value things in our lives.
It is not really deliberate either.
It is just because there are only so many words for this idea,
this immediate sense of how well we think things are going.
So I don't think we should shy away from trying to make...
I mean, yeah, we don't do it every day,
but if we were doing psychology in neuroscience,
we would have to think hard about that.
Wait, Ersan?
No, we had just...
And I would give kids a play that seems useless,
but they use those skills.
So now, I want to...
Even as adults humans, we have this satisfaction
from maybe not meeting these useless goals
and working on for them,
even though it is a hobby or a skill
that we might never use later at all.
Or basic research.
May or may not be useful later.
Well, I would...
Who knows?
I do sense that it is similar.
Yeah, it is like hobbies or something we get involved in.
Yeah, that is the obvious way to understand it.
It might not be correct.
As someone who has some training in psychology,
I am always suspicious of the obvious way to interpret
or to think about your own behavior,
because so often we find out
and we look at them carefully that we are wrong.
But I agree with you.
It is the natural way to think about those things.
You mentioned before that in one of your slides,
that the purpose of life is to gain pleasure or avoid pain.
That is something that I think is a result
of something that has emerged through evolution,
where the purpose of life is to survive or to keep existing.
And a lot of things like pain and pleasure the way we define it
is something that evolution has defined for us.
And this includes things like emotion and so on.
So a lot of things that are part of our intelligence today
are because of this overall objective of survival.
And when we think about AI,
that is not the objective of what we are trying to design.
It is not just to survive.
We are trying to imitate the kind of intelligence behavior
that our current skates find intelligent.
So building all of these things like emotion and so on
is some of the behaviors that we would have
are due to a different objective.
So what do we do in AI?
We are trying to understand intelligence.
Intelligence is goal-seeking ability.
And so we should try to understand goal-seeking ability in general,
independent of what the goals are.
If you have a goal-seeking ability
but it only works with one kind of goal,
then it is just for that reason alone
it is less powerful, less general kind of goal-seeking ability.
So what our objective in AI,
or at least one objective,
is to understand the notion of a goal-seeking ability
independent of the goal.
So we don't have to, as you say,
AI researchers or engineers
don't have to adopt the same goals as evolution.
We might for similar reasons, but we don't have to.
It's a real cutting point, right?
How do you set the reward signal
and then how do you achieve the reward signal?
And I think it's really useful to establish that cutting point
and separate the decisions of what are going to be the rewards
from how you achieve the rewards, whatever they are.
You are next.
Do you still have a question in purple?
No.
I thought, okay.
I didn't get that.
Can you think about the evolutionary learning
by looking at the success of the research?
Oh, yes.
Okay, also.
Yes, also.
Right.
The first perspective, multiple perspectives are good.
The first perspective is evolution sets our goals,
then we're reinforcing the learning system.
That's a separation.
The second perspective is that evolution itself is a learning system
that's trying to achieve some goals, the goals of evolution,
the goals of survival and reproduction.
And that's valid, too.
It's, of course, not a very good reinforcing learning system
because it can keep memories from the survival
about the life of one agent onto the next agent.
But it's a perfectly valid way of thinking about things.
And Joshua.
I'd be curious to hear your thoughts about the concerns
of people like Stuart Russell regarding if you make machines
at human level intelligent some more,
that the goals that they would try to achieve
would be an expensive humans because of your own value alignments.
You know about these questions?
What do you think?
Yeah.
So this is where I think the symmetry comes in.
Do unto others as you'd have them do unto you.
The machines might not have that level.
The reasons for that attitude go beyond the construction
of those machines.
It's machines that are not cooperative will not be cooperated with.
And they will be less effective.
And the other machines that are cooperative will be more and more effective.
So there's an enormous drive or enormous power in cooperation.
It's rational to cooperate.
You can be much more powerful and effective by cooperating
than you can by trying to take over.
Taking over strategy will very rarely be effective.
So I think there's a little bit of illusion.
We say, oh, people all have the same goals.
I don't think that's true at all.
We have very different goals.
And we have groups of people that have very different goals from each other.
And yet we find that usually it's best not to go around killing people.
It's best to work together.
It's best to find out what you can do for me and I can do for you.
I really think this is the most important aspect of humanity.
Humanity is the animal that cooperates.
And this is perhaps most of all the reason why we're powerful.
So now the AIs are not exactly us,
but in the end it's sort of the very best case you could possibly imagine
where you might be encountering a new kind of being.
These are new beings that we've created.
So why don't we create them to respect the value of cooperation
to realize early on that it's a rational strategy
and that their goals will be best achieved by working with us rather than against us.
My question is, basically, there are also different circumstances
that we don't have the opportunity to work with.
So is it always good to organize ourselves by designing our machines
or maybe some kind of strategy to realize what we're trying to do?
I don't think it's an absolute rule.
I think it's just like a first perspective.
Like it, whenever you meet a new person,
you should be open to them and give them a chance to be cooperative
and have mutual beneficials interactions.
It's no hard and fast rules and we do need to worry about security,
but I think it's going to be very counterproductive if we set up a tiered system.
We say humans are always above the robots.
I mean, we would not accept that if it was turned around the other way.
That's a way to get resistance and violent outcomes.
There's someone in the back that I put off before.
The closest thing to supervised learning, I would say,
is maybe learning from books or from school.
Of course, no animal other than humans goes to school.
Really, even in school learning and supervised learning is a tiny portion of it.
It's useful when you're making these claims, these statements for yourself
because I've urged you all to do it and think about it.
It's useful to go a little bit too far.
Some of my statements had all things can be thought of this way.
It's useful to go to the limit and see how that stands up
before hedging too many things.
I may sometimes go a little bit too far.
Corey.
Thank you very much for the talk.
Great talk.
I'd like especially your notion that you should put yourself in the position of the agent
and think about how they might perform what you're expecting.
If there was an AI in this room, how might we expect it to act to judge its intelligence?
How might you perform so that we then perceive you as intelligent?
What would I do if I was there?
Well, you know what they say.
You might be quiet.
Sometimes it's better to be quiet and be thought an idiot than to open your mouth and remove all doubt.
What might it do?
Let's say I suppose it can't talk.
So the only way you can reveal your intelligence is through your actions.
Yeah, what do you think?
It would be really hard, I think.
Yeah.
It's almost near impossible.
Did someone have an idea?
I forget.
Oh, the Turing test.
The Turing test?
No.
No.
That's, the Turing test is so human-centric, you know?
Yeah, humans, they always want to be better than things and they only want to see,
they always imagine that the robots want to become human exactly.
I don't know.
Paul.
Yeah, so on that thought, it seems like when we think about experiences and be able to
be agents, one of the things that I think could be a measure of intelligence might be
that an agent is able to reason about the world without actually going through the experience
itself, right?
But how would you know?
Well, like, we don't all have to get in car crashes to know that they're bad.
Yeah, so over a period of time, watching the actions of the intelligent being, seeing what
it refrains from doing, what it does, what it learns from, we should be able to do it.
But it's not something that you can necessarily do quickly.
Yeah.
So we have to live with it for a while.
Yeah.
So I don't have any questions.
Okay.
Okay.
Yeah, and the question is, are we?
I mean, it's true that we tend to have competitive games and we beat people.
That's a certain stage that AI is at.
It's mostly because it's convenient for doing the research.
But if you look at where we talk about the Moore's law and the inevitability of AI arising,
a lot of that's coming from economic drivers.
And the economic drivers are all towards cooperative AI, towards human intelligence amplification.
You will spend maybe $1,000 for your iPhone.
That's a lot of money, and you get something that lets you communicate better and access
information better.
We will spend money to get a better Siri or a better assistant.
I think the major market will be for assistance to help us.
So I think we're going to see helping, and we are already seeing helping assistance.
They help us translate.
This will be the face of AI.
It will be the thing that's helping us get our jobs done and enjoying our lives.
So I think it's going to be very cooperative.
I think that's what's going to happen.
It's not going to be someone who's going to create a weird thing with its own goals,
come out of a lab.
Maybe it'll be caregiver robots, like for elderly people.
I think that's actually where the economic drivers are, and that's what it's going to be like.
One last question.
So this cycles back to where we started.
How can we make ourselves more knowledgeable?
How can we learn stuff?
And how do we develop our cognition?
So I absolutely believe that there will be a major part of the investment in creating
intelligent systems will be for creating, for making us more intelligent.
We will get, we will Patrick will give us another lobe on our brain.
And it will be wired in somehow, and we see it already.
We rely on our phones to remember phone numbers, to access to Google,
just for more and more of our cognition.
And as that link gets tighter, that will happen more and more.
So we won't have to squash it into our wetware of our brain.
We can just get more tightly integrated with the technology and augment people.
Now of course, you know, lots of people won't want to do that.
That's fine.
But maybe it will be wireless connected and it won't be so wet.
Yeah, so I look forward as we figure this out to all of us having better abilities to think
and becoming more human as we get more effective at achieving goals in our lives
and working together for everyone's mutual benefit.
Thank you very much.
Thank you very much.
Thank you very much.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you very much.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you very much.
Thank you very much.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you.
Thank you very much.
This will end pretty soon.
Ladies & Gentlemen,
please return to theö that we've just shown a bit so far.
So that'll be that.
Thank you.
Before we go around here.
I'd love to share some with you about this work.
We've been a part of this summer school for a very long time.
A lot of work done behind the scenes and really great people
that were really there to support review applications.
If you're like, gosh, I'm only the first one to step up,
I'll do it, like just providing a lot of time.
So really, really amazing people that are contributing all along
to put this couple of weeks together.
Big thank you to Camelini and our content planning committee
who put together a great line of speakers.
To Maggie, Zvon, Destiny, Cassie, Brittany, Vila, Spencer,
and all of our organizing team and all of our sponsors.
And a special thank you to our creators, Pedro,
for making sure that all of us leave.
And it's been a little bit heavier, so thanks to them.
And really, a truly special thank you to all of you guys
for being here, taking the time to come to Edmonton.
So thank you so much.
You'll be receiving an email tonight with a certificate
of digital certificate representation
and a link to a feedback survey.
So please fill that out.
And we would love to see a connection
that you can reach out to us through the summer school website
through Slack.
Hello at Amy.ca.
And enjoy the rest of your summer.
Thank you so much.
Thank you.

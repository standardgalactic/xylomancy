Good afternoon, everyone, and welcome to MIT 6S191, Introduction to Deep Learning.
My name is Alexander Amini, and I'm so excited to be your instructor this year along with
Aviso Imani in this new virtual format.
6S191 is a two-week bootcamp on everything deep learning, and we'll cover a ton of
material in only two weeks, so I think it's really important for us to dive right in with
these lectures.
But before we do that, I do want to motivate exactly why I think this is such an awesome
field to study.
And when we taught this class last year, I decided to try introducing the class very
differently.
And instead of me telling the class how great 6S191 is, I wanted to let someone else do
that instead.
So actually, I want to start this year by showing you how we introduced 6S191 last year.
Hi, everybody, and welcome to MIT 6S191, the official introductory course on deep learning
taught here at MIT.
Deep learning is revolutionizing so many fields, from robotics to medicine and everything
in between.
You'll learn the fundamentals of this field and how you can build some of these incredible
algorithms.
In fact, this entire speech and video are not real, and were created using deep learning
and artificial intelligence.
And in this class, you'll learn how.
It has been an honor to speak with you today, and I hope you enjoyed the course.
So in case you couldn't tell, that was actually not a real video or audio, and the audio you
actually heard was purposely degraded a bit more to even make it more obvious that this
was not real and avoid some potential misuse.
Even with the purposely degraded audio, that intro went somewhat viral last year after the
course and we got some really great and interesting feedback.
And to be honest, after last year and when we did this, I thought it was going to be
really hard for us to top it this year.
But actually, I was wrong because the one thing I love about this field is that it's
moving so incredibly fast that even within the past year, the state of the art has come
significantly advanced.
And the video you saw that we used last year used deep learning, but it was not a particularly
easy video to create.
It required a full video of Obama speaking, and it used this to intelligently stitch
together parts of the scene to make it look and appear like he was mouthing the words
that I said.
And to see the behind the scenes here, now you can see the same video with my voice.
Hi everybody, and welcome to MIT 6S191, the official introductory course on deep learning
taught here at MIT.
Now it's actually possible to use just a single static image, not the full video to
achieve the exact same thing.
And now you can actually see eight more examples of Obama now just created using just a single
static image.
No more full dynamic videos, but we can achieve the same incredible realism and result using
deep learning.
Now of course, there's nothing restricting us to one person.
This method generalizes to different faces, and there's nothing restricting us even to
humans anymore, or individuals that the algorithm has ever seen before.
Hi everybody, and welcome to MIT 6S191, the official introductory course on deep learning
taught here at MIT.
The ability to generate these types of dynamic moving videos from only a single image is
remarkable to me, and it's a testament to the true power of deep learning.
In this class, you're going to actually not only learn about the technical basis of this
technology, but also some of the very important and very important ethical and societal implications
of this work as well.
Now I hope this was a really great way to get you excited about this course and 6S191,
and with that let's get started.
We can actually start by taking a step back and asking ourselves what is deep learning,
deep learning, and the context of intelligence.
Intelligence is actually the ability to process information such that it can be used to inform
a future decision.
Now the field of artificial intelligence, or AI, is a science that actually focuses
on building algorithms to do exactly this, to build algorithms to process information
such that they can inform future predictions.
Now machine learning, you can think of this as just a subset of AI that actually focuses
on teaching an algorithm to learn from experiences without being explicitly programmed.
Now deep learning takes this idea even further, and it's a subset of machine learning that
focuses on using neural networks to automatically extract useful patterns in raw data, and then
using these patterns or features to learn to perform that task.
And that's exactly what this class is about.
This class is about teaching algorithms how to learn a task directly from raw data.
And we want to provide you with a solid foundation both technically and practically for you to
understand under the hood how these algorithms are built and how they can learn.
So this course is split between technical lectures as well as project software labs.
We'll cover the foundation starting today with neural networks, which are really the
building blocks of everything that we'll see in this course.
And this year we also have two brand new, really exciting hot topic lectures focusing
on uncertainty and probabilistic deep learning, as well as algorithmic bias and fairness.
Finally we'll conclude with some really exciting guest lectures and student project presentations
as part of a final project competition that all of you will be eligible to win some really
exciting prizes.
Now a bit of logistics before we dive into the technical side of the lecture.
For those of you taking this course for credit, you will have two options to fulfill your
credit requirement.
The first option will be to actually work in teams of up to four or individually to develop
a cool new deep learning idea.
Now doing so will make you eligible to win some of the prizes that you can see on the
right hand side.
And we realize that in the context of this class, which is only two weeks, that's an
extremely short amount of time to come up with an impressive project or research idea.
So we're not going to be judging you on the novelty of that idea, but rather we're not
going to be judging you on the results of that idea, but rather the novelty of the idea,
your thinking process and how impactful this idea can be.
But not on the results themselves.
On the last day of class, you will actually give a three minute presentation to a group
of judges who will then award the winners and the prizes.
Now again, three minutes is extremely short to actually present your ideas and present
your project, but I do believe that there's an art to presenting and conveying your ideas
concisely and clearly in such a short amount of time.
So we will be holding you strictly to that strict deadline.
The second option to fulfill your grade requirement is to write a one page review on a deep learning
paper.
Here the grade is based more on the clarity of the writing and the technical communication
of the main ideas.
This will be due on Thursday, the last Thursday of the class, and you can pick whatever deep
learning paper you would like.
If you would like some pointers, we have provided some guide papers that can help you get started
if you would just like to use one of those for your review.
In addition to the final project prizes, we'll also be awarding this year three lab prizes,
one associated to each of the software labs that students will complete.
Again, completion of the software labs is not required for grade of this course, but
it will make you eligible for some of these cool prizes.
So please, we encourage everyone to compete for these prizes and get the opportunity to
win them all.
Please post to Piazza if you have any questions.
Visit the course website for announcements and digital recordings of the lectures, etc.
And please email us if you have any questions.
Also there are software labs and office hours right after each of these technical lectures
held in Gather Town.
So please drop by in Gather Town to ask any questions about the software labs, specifically
on those, or more generally about past software labs or about the lecture that occurred that
day.
Now this course has an incredible group of TAs and teaching assistants that you can reach
out to at any time in case you have any issues or questions about the material that you're
learning.
And finally, we want to give a huge thanks to all of our sponsors who, without their
help, this class would not be possible.
This is the fourth year that we're teaching this class, and each year it just keeps getting
bigger and bigger and bigger, and we really give a huge shout out to our sponsors for
helping us make this happen each year.
And especially this year in light of the virtual format.
So now let's start with the fun stuff.
Let's start by asking ourselves a question about why do we all care about deep learning?
And specifically, why do we care right now?
To understand that, it's important to actually understand first why is deep learning or how
is deep learning different from traditional machine learning?
Now traditionally, machine learning algorithms define a set of features in their data.
Usually these are features that are handcrafted or hand engineered, and as a result they tend
to be pretty brittle in practice when they're deployed.
The key idea of deep learning is to learn these features directly from data in a hierarchical
manner.
That is, can we learn, if we want to learn how to detect a face, for example, can we
learn to first start by detecting edges in the image, composing these edges together
to detect mid-level features such as a eye or a nose or a mouth, and then going deeper
and composing these features into structural, facial features, so that we can recognize
this face.
This hierarchical way of thinking is really core to deep learning, and is core to everything
that we're going to learn in this class.
Actually the fundamental building blocks, though, of deep learning and neural networks
have actually existed for decades.
So one interesting thing to consider is why are we studying this now?
Now is an incredibly amazing time to study these algorithms.
And for one reason, it is because data has become much more pervasive.
These models are extremely hungry for data, and at the moment we're living in an era
where we have more data than ever before.
Secondly, these algorithms are massively parallelizable, so they can benefit tremendously
from modern GPU hardware that simply did not exist when these algorithms were developed.
And finally, due to open source toolboxes like TensorFlow, building and deploying these
models has become extremely streamlined.
So let's start actually with the fundamental building block of deep learning and of every
neural network.
That is just a single neuron, also known as a perceptron.
So we're going to walk through exactly what is a perceptron, how it's defined, and we're
going to build our way up to deeper neural networks all the way from there.
So let's start really at the basic building block.
The idea of a perceptron or a single neuron is actually very simple.
So I think it's really important for all of you to understand this at its core.
Let's start by actually talking about the forward propagation of information through
this single neuron.
We can define a set of inputs xi through xm, which you can see on the left-hand side,
and each of these inputs or each of these numbers are multiplied by their corresponding
weight and then added together.
We take this single number, the result of that addition, and pass it through what's
called a nonlinear activation function to produce our final output y.
We can actually, actually this is not entirely correct because one thing I forgot to mention
is that we also have what's called a bias term in here, which allows you to shift your
activation function left or right.
Now on the right-hand side of this diagram, you can actually see this concept illustrated
or written out mathematically as a single equation.
You can actually rewrite this in terms of linear algebra matrix multiplications and
dot products to represent this a bit more concisely.
So let's do that.
Let's now do that with x, capital X, which is a vector of our inputs, x1 through xm,
and capital W, which is a vector of our weights, w1 through wm.
So each of these are vectors of length m, and the output is very simply obtained by taking
their dot product, adding a bias, which in this case is w0, and then applying a nonlinearity,
g.
One thing is that I haven't, I've been mentioning it a couple of times, this nonlinearity, g.
What exactly is it?
Because I've mentioned it now a couple of times, well, it is a nonlinear function.
One common example of this nonlinear activation function is what is known as the sigmoid function.
Defined here on the right.
In fact, there are many types of nonlinear functions.
You can see three more examples here, including the sigmoid function, and throughout this
presentation you'll actually see these TensorFlow code blocks, which will actually illustrate
how we can take some of the topics that we're learning in this class and actually practically
use them using the TensorFlow software library.
The sigmoid activation function, which I presented on the previous slide, is very popular since
it's a function that gives outputs.
It takes as input any real number, any activation value, and it outputs a number always between
zero and one.
This makes it really, really suitable for problems and probability, because probabilities
also have to be between zero and one.
This makes them very well suited for those types of problems.
In modern deep neural networks, the ReLU activation function, which you can see on the
right, is also extremely popular because of its simplicity.
In this case, it's a piecewise linear function.
It is zero before when it's in the negative regime, and it is strictly the identity function
in the positive regime.
But one really important question that I hope that you're asking yourselves right now is
why do we even need activation functions?
I think actually throughout this course, I do want to say that no matter what I say in
the course, I hope that always you're questioning why this is a necessary step and why do we
need each of these steps, because often these are the questions that can lead to really amazing
research breakthroughs.
Why do we need activation functions?
The point of an activation function is to actually introduce nonlinearities into our
network because these are nonlinear functions.
It allows us to actually deal with nonlinear data.
This is extremely important in real life, especially because in the real world, data
is almost always nonlinear.
Imagine I told you to separate here the green points from the red points, but all you could
use is a single straight line.
You might think this is easy with multiple lines or curved lines, but you can only use
a single straight line.
That's what using a neural network with a linear activation function would be like.
That makes the problem really hard, because no matter how deep the neural network is,
you'll only be able to produce a single line decision boundary, and you'll only be able
to separate your space with one line.
Now, using nonlinear activation functions allows your neural network to approximate arbitrarily
complex functions, and that's what makes neural networks extraordinarily powerful.
Let's understand this with a simple example so that we can build up our intuition even
further.
Imagine I give you this trained network, now with weights on the left hand side, 3 and
negative 2.
This network only has two inputs, x1 and x2.
If we want to get the output of it, we simply do the same story as I said before.
First, take a dot product of our inputs with our weights, add the bias, and apply a nonlinearity.
But let's take a look at what's inside of that nonlinearity.
It's simply a weighted combination of our inputs in the form of a two-dimensional line,
because in this case we only have two inputs.
If we want to compute this output, it's the same stories before.
We take a dot product of x and w, we add our bias, and apply our nonlinearity.
What about what's inside of this nonlinearity G?
Well, this is just a 2D line.
In fact, since it's just a two-dimensional line, we can even plot it in two-dimensional space.
This is called the feature space, the input space.
In this case, the feature space and the input space are equal because we only have one neuron.
In this plot, let me describe what you're seeing.
On the two axes, you're seeing our two inputs.
On one axis is x1, one of the inputs, on the other axis is x2, our other input.
We can plot the line here, our decision boundary of this trained neural network that I gave
you as a line in this space.
This line corresponds to actually all of the decisions that this neural network can make.
If I give you a new data point, for example, here I'm giving you negative 1, 2, this point
lies somewhere in the space, specifically at x1 equal to negative 1 and x2 equal to 2.
That's just a point in the space.
I want you to compute its weighted combination, and I can actually follow the perceptron equation
to get the answer.
Here we can see that if we plug it into the perceptron equation, we get 1 plus minus 3
minus 4, and the result would be minus 6.
We plug that into our nonlinear activation function, g, and we get a final output of 0.002.
Now, in fact, remember that the sigmoid function actually divides this space into two parts,
either because it outputs everything between 0 and 1.
It's dividing it between 0.5 and greater than 0.5 and less than 0.5.
When the input is less than 0 and greater than 0.5, that's when the input is positive.
We can illustrate this space, actually, but this feature space, when we're dealing with
a small dimensional data, like in this case we only have two dimensions, but soon we'll
start to talk about problems where we have thousands or millions, or in some cases billions
of weights in our neural network, and then drawing these types of plots becomes extremely
challenging and not really possible anymore, but at least when we're in this regime of
small number of inputs and small number of weights, we can make these plots to really
understand the entire space, and for any new input that we obtain, for example, an input
right here, we can see exactly that this point is going to be having an activation function
less than 0, and its output will be less than 0.5.
The magnitude of that actually is computed by plugging it into the perceptron equation,
so we can't avoid that, but we can immediately get an answer on the decision boundary, depending
on which side of this hyperplane that we lie on when we plug it in.
So now that we have an idea of how to build a perceptron, let's start by building neural
networks and seeing how they all come together.
So let's revisit that diagram of the perceptron that I showed you before.
If there's only a few things that you get from this class, I really want everyone to
take away how a perceptron works, and there's three steps, remember them always.
The dot product, you take a dot product of your inputs and your weights.
You add a bias, and you apply your non-linearity.
There's three steps.
Let's simplify this diagram a little bit.
Let's clean up some of the arrows and remove the bias, and we can actually see now that
every line here has its own associated weight to it, and I'll remove the bias term, like
I said, for simplicity.
Note that z here is the result of that dot product plus bias before we apply the activation
function, though, g.
The final output, though, is simply y, which is equal to the activation function of z,
which is our activation value.
Now if we want to define a multi-output neural network, we can simply add another perceptron
to this picture.
So instead of having one perceptron, now we have two perceptrons and two outputs.
Each one is a normal perceptron, exactly like we saw before, taking its inputs from each
of the x1's through xm's, taking the dot product, adding a bias, and that's it.
Now we have two outputs.
Each of those perceptrons, though, will have a different set of weights.
Remember that.
We'll get back to that.
If we want, so actually one thing to keep in mind here is because all the inputs are
densely connected, every input has a connection to the weights of every perceptron.
These are often called dense layers or sometimes fully connected layers.
Now through this class, you're going to get a lot of experience actually coding up and
practically creating some of these algorithms using a software toolbox called TensorFlow.
So now that we have the understanding of how a single perceptron works and how a dense
layer works, this is a stack of perceptrons, let's try and see how we can actually build
up a dense layer like this all the way from scratch.
To do that, we can actually start by initializing the two components of our dense layer, which
are the weights and the biases.
Now that we have these two parameters of our neural network of our dense layer, we can
actually define the forward propagation of information, just like we saw it and learned
about already.
That forward propagation of information is simply the dot product or the matrix multiplication
of our inputs with our weights at a bias that gives us our activation function here.
And then we apply this nonlinearity to compute the output.
Now, TensorFlow has actually implemented this dense layer for us.
So we don't need to do that from scratch.
Instead we can just call it like shown here.
So to create a dense layer with two outputs, we can specify this units equal to two.
Now let's take a look at what's called a single layered neural network.
This is one we have a single hidden layer between our inputs and our outputs.
This layer is called the hidden layer, because unlike an input layer and an output layer,
the states of this hidden layer are typically unobserved, they're hidden to some extent,
they're not strictly enforced either.
And since we have this transformation now from the input layer to the hidden layer and
from the hidden layer to the output layer, each of these layers are going to have their
own specified weight matrices.
We'll call w1 the weight matrices for the first layer and w2 the weight matrix for the
second layer.
If we take a zoomed in look at one of the neurons in this hidden layer, let's take for example
z2 for example, this is the exact same perceptron that we saw before.
We can compute its output, again using the exact same story, taking all of its inputs
x1 through xm, applying a dot product with the weights, adding a bias and that gives
us z2.
If we look at a different neuron, let's suppose z3, we'll get a different value here because
the weights leading to z3 are probably different than those leading to z2.
Now this picture looks a bit messy, so let's try and clean things up a bit more.
From now on, I'll just use this symbol here to denote what we call this dense layer or
fully connected layers.
And here you can actually see an example of how we can create this exact neural network,
again using TensorFlow with the predefined dense layer notation.
Here we're creating a sequential model where we can stack layers on top of each other,
first layer with n neurons and the second layer with 2 neurons, the output layer.
And if we want to create a deep neural network, all we have to do is keep stacking these layers
to create more and more hierarchical models, ones where the final output is computed by
going deeper and deeper into the network.
And to implement this in TensorFlow, again, it's very similar as we saw before.
Again using the TFKARIS sequential call, we can stack each of these dense layers on top
of each other, each one specified by the number of neurons in that dense layer, n1 and 2,
but with the last output layer fixed to 2 outputs, if that's how many outputs we have.
Okay, so that's awesome.
Now we have an idea of not only how to build up a neural network directly from a perceptron,
but how to compose them together to form complex deep neural networks.
Let's take a look at how we can actually apply them to a very real problem that I believe
all of you should care very deeply about.
Here's a problem that we want to build an AI system to learn to answer.
Still I pass this class, and we can start with a simple two-feature model.
One feature, let's say, is the number of lectures that you attend as part of this class, and
the second feature is the number of hours that you spend working on your final project.
You do have some training data from all of the past participants of Success191, and we
can plot this data on this feature space like this.
The green points here actually indicate students, so each point is one student that has passed
the class, and the red points are students that have failed the class.
You can see where they are in this feature space depends on the actual number of hours
that they attended the lecture, the number of lectures they attended, and the number
of hours they spent on the final project.
And then there's you.
You have attended four lectures, and you have spent five hours on your final project.
And you want to understand how can you build a neural network given everyone else in this
class?
Will you pass or fail this class based on the training data that you see?
So let's do it.
We have now all of the requirements to do this now.
So let's build a neural network with two inputs, X1 and X2, with X1 being the number
of lectures that we attend, X2 is the number of hours you spend on your final project.
We'll have one hidden layer with three units, and we'll feed those into a final probability
output by passing this class, and we can see that the probability that we pass is 0.1,
or 10%.
That's not great, but the reason is because that this model was never actually trained.
It's basically just a baby.
It's never seen any data.
Even though you have seen the data, it hasn't seen any data.
And more importantly, you haven't told the model how to interpret this data.
It needs to learn about this problem first.
It knows nothing about this class or final projects or any of that.
So one of the most important things to do this is actually you have to tell the model
when it is making bad predictions in order for it to be able to correct itself.
Now the loss of a neural network actually defines exactly this.
It defines how wrong a prediction was.
So it takes as input the predicted outputs and the ground truth outputs.
Now if those two things are very far apart from each other, then the loss will be very
large.
On the other hand, the closer these two things are from each other, the smaller the loss
and the more accurate the loss the model will be.
So we always want to minimize the loss.
We want to incur the, we want to predict something that's as close as possible to the ground
truth.
Now let's assume we have not just the data from one student, but as we have in this case
the data from many students.
We now care about not just how the model did on predicting just one prediction, but
how it did on average across all of these students.
This is what we call the empirical loss and it's simply just the mean or the average
of every loss from each individual example or each individual student.
When training a neural network, we want to find a network that minimizes the empirical
loss between our predictions and the true outputs.
Now if we look at the problem of binary classification, where the neural network, like we want to
do in this case, is supposed to answer either yes or no, one or zero.
We can use what is called a softmax cross entropy loss.
Now the softmax cross entropy loss is actually written out here and it's defined by actually
what's called the cross entropy between two probability distributions.
It measures how far apart the ground truth probability distribution is from the predicted
probability distribution.
Let's suppose instead of predicting binary outputs, will I pass this class or will I
not pass this class?
Instead you want to predict the final grade as a real number, not a probability or as
a percentage.
We want the grade that you will get in this class.
Now in this case, because the type of the output is different, we also need to use a
different loss here because our outputs are no longer 0, 1, but they can be any real number.
Where does the grade that you're going to get on the final class?
So for example, here, since this is a continuous variable, the grade, we want to use what's
called the mean squared error.
This measures just the squared error, the squared difference between our ground truth
and our predictions, again averaged over the entire dataset.
Okay, great.
So now we've seen two loss functions, one for classification, binary outputs, as well
as regression, continuous outputs, and the problem now I think that we need to start
asking ourselves is how can we take that loss function?
We've seen our loss function, we've seen our network, now we have to actually understand
how can we put those two things together?
How can we use our loss function to train the weights of our neural network such that
it can actually learn that problem?
Well, what we want to do is actually find the weights of the neural network that will
minimize the loss of our dataset.
That essentially means that we want to find the W's in our neural network that minimize
J of W. J of W is our empirical cost function that we saw in the previous slides that averaged
loss over each data point in the dataset.
Now, remember that W, capital W, is simply a collection of all of the weights in our
neural network, not just from one layer, but from every single layer.
So that's W0 from the zeroth layer to the first layer to the second layer, all concatenate
into one.
In this optimization problem, we want to optimize all of the W's to minimize this empirical
loss.
Now, remember, our loss function is just a simple function of our weights.
If we have only two weights, we can actually plot this entire loss landscape over this
grid of weights.
So on the one axis on the bottom, you can see weight number one, and the other one you
can see weight zero.
There's only two weights in this neural network, very simple neural network.
So we can actually plot for every W0 and W1, what is the loss?
What is the error that we'd expect to see and obtain from this neural network?
Now, the whole process of training a neural network, optimizing it, is to find the lowest
point in this loss landscape that will tell us our optimal W0 and W1.
Now, how can we do that?
The first thing we have to do is pick a point.
So let's pick any W0, W1.
Starting from this point, we can compute the gradient of the landscape at that point.
Now the gradient tells us the direction of highest or steepest ascent.
So that tells us which way is up.
If we compute the gradient of our loss with respect to our weights, that's the derivative
of our gradient for loss with respect to the weights, that tells us the direction of which
way is up on that loss landscape from where we stand right now.
Instead of going up though, we want to find the lowest loss.
So let's take the negative of our gradient and take a small step in that direction.
This will move us a little bit closer to the lowest point.
And we just keep repeating this.
Now we compute the gradient at this point and repeat the process until we converge.
And we will converge to a local minimum.
We don't know if it will converge to a global minimum, but at least we know that it should
in theory converge to a local minimum.
And we can summarize this algorithm as follows.
This algorithm is also known as gradient descent.
So we start by initializing all of our weights randomly, and we loop until convergence.
We start from one of those weights, our initial point, we compute the gradient, that tells
us which way is up.
So we take a step in the opposite direction.
We take a small step here, small is computed by multiplying our gradient by this factor
eta.
And we'll learn more about this factor later.
This factor is called the learning rate.
We'll learn more about that later.
Now again, in TensorFlow, we can actually see this pseudocode of gradient descent algorithm
written out in code.
We can randomize all of our weights.
That basically initializes our search, our optimization process at some point in space.
And then we keep looping over and over and over again.
We compute the loss, we compute the gradient, and we take a small step of our weights in
the direction of that gradient.
But now let's take a look at this term here.
This is how we actually compute the gradient.
This explains how the loss is changing with respect to the weight.
But I never actually told you how we compute this.
So let's talk about this process, which is actually extremely important in training neural
networks.
This is back propagation.
So how does back propagation work?
How do we compute this gradient?
Let's start with a very simple neural network.
This is probably the simplest neural network in existence.
It only has one input, one hidden neuron, and one output.
Computing the gradient of our loss, J of w, with respect to one of the weights, in this
case, just w2, for example, tells us how much a small change in w2 is going to affect our
loss, J.
So if we move around J, infinitesimally small, how will that affect our loss?
That's what the gradient is going to tell us, of derivative of J of w2.
So if we write out this derivative, we can actually apply the chain rule to actually
compute it.
So what does that look like?
Specifically, we can decompose that derivative into the derivative of J dw over dy, multiplied
by derivative of our output with respect to w2.
Now the question here is with the second part.
If we want to compute now not the derivative of our loss with respect to w2, but now the
loss with respect to w1, we can do the same story as before.
We can apply the chain rule now recursively.
So now we have to apply the chain rule again to the second part.
Now the second part is expanded even further.
So the derivative of our output with respect to z1, which is the activation function of
this first hidden unit.
And we can back propagate this information now.
You can see starting from our loss all the way through w2 and then recursively applying
this chain rule again to get to w1.
And this allows us to see both the gradient at both w2 and w1.
So in this case, just to reiterate once again, this is telling us, this dj dw1 is telling
us how a small change in our weight is going to affect our loss.
So we can see if we increase our weight a small amount, it will increase our loss.
That means we will want to decrease the weight to decrease our loss.
That's what the gradient tells us.
Which direction we need to step in order to decrease or increase our loss function.
Now we showed this here for just two weights in our neural network because we only have
two weights.
But imagine we have a very deep neural network.
One with more than just two layers, or one layer rather, of hidden units.
We can just repeat this process of applying, recursively applying the chain rule to determine
how every single way in the model needs to change to impact that loss.
But really all this boils down to just recursively applying this chain rule formulation that
you can see here.
And that's the back propagation algorithm.
In theory, it sounds very simple.
It's just a very, very basic extension on derivatives and the chain rule.
But now let's actually touch on some insights from training these networks in practice that
make this process much more complicated in practice.
And why using back propagation as we saw there is not only so easy.
Now in practice, training neural networks and optimization of networks can be extremely
difficult and it's actually extremely computationally intensive.
Here's the visualization of what a loss landscape of a real neural network can look like, visualized
on just two dimensions.
Now you can see here that the loss is extremely non-convex, meaning that it has many, many
local minimum.
That can make using an algorithm like gradient descent very, very challenging because gradient
descent is always going to step closest to the first local minimum, but it can always
get stuck there.
So finding how to get to the global minima or a really good solution for your neural
network can often be very sensitive to your hyper parameter such as where the optimizer
starts in this loss landscape.
If it starts in a potentially bad part of the landscape, it can very easily get stuck
in one of these local minimum.
Now recall the equation that we talked about for gradient descent.
This was the equation I showed you.
Your next weight update is going to be your current weights minus a small amount called
the learning rate multiplied by the gradient.
So we have this minus sign because we want to step in the opposite direction and we multiply
it by the gradient or we multiply it by the small number called here called eta, which
is what we call the learning rate.
How fast do we want to do the learning?
Now it determines actually not just how fast to do the learning, that's maybe not the best
way to say it, but it tells us how large should each step we take in practice be with regards
to that gradient.
So the gradient tells us the direction, but it doesn't necessarily tell us the magnitude
of the direction.
So eta can tell us actually a scale of how much we want to trust that gradient and step
in the direction of that gradient.
In practice setting even eta, this one parameter, this one number, can be extremely difficult
and I want to give you a quick example of why.
So if you have a very non-convex or loss landscape where you have local minima, if you set the
learning rate too low, then the model can get stuck in these local minima.
It can never escape them because it actually does optimize itself, but it optimizes it
to a non-optimal minima and it can converge very slowly as well.
On the other hand, if we increase our learning rate too much, then we can actually overshoot
our minima and actually diverge and lose control and basically explode the training process
completely.
One of the challenges is actually how to use stable learning rates that are large enough
to avoid the local minima, but small enough so that they don't diverge completely.
So they're small enough to actually converge to that global spot once they reach it.
So how can we actually set this learning rate?
Well, one option which is actually somewhat popular in practice is to actually just try
a lot of different learning rates and that actually works.
It is a feasible approach, but let's see if we can do something a little bit smarter than
that, more intelligent.
What if we could say instead, how can we build an adaptive learning rate that actually looks
at its lost landscape and adapts itself to account for what it sees in the landscape?
There are actually many types of optimizers that do exactly this.
This means that the learning rates are no longer fixed.
They can increase or decrease depending on how large the gradient is in that location
and how fast we want and how fast we're actually learning.
And many other options that could be also with regards to the size of the weights at
that point, the magnitudes, etc.
In fact, these have been widely explored and published as part of TensorFlow as well.
And during your labs, we encourage each of you to really try out each of these different
types of optimizers and experiment with their performance in different types of problems
so that you can gain very important intuition about when to use different types of optimizers
or what their advantages are and disadvantages in certain applications as well.
So let's try and put all of this together.
So here we can see a full loop of using TensorFlow to define your model on the first line, define
your optimizer.
Here you can replace this with any optimizer that you want.
Here I'm just using stochastic gradient descent like we saw before.
And feeding it through the model, we loop forever.
We're doing this forward prediction.
We predict using our model.
We compute the loss with our prediction.
This is exactly the loss is telling us again how incorrect our prediction is with respect
to the ground truth, why?
We compute the gradient of our loss with respect to each of the weights in our neural network.
And finally we apply those gradients using our optimizer to step and update our weights.
This is really taking everything that we've learned in the class and the lecture so far
and applying it into one whole piece of code written in TensorFlow.
So I want to continue this talk and really talk about tips for training these networks
in practice now that we can focus on this very powerful idea of batching your data into
mini batches.
So before we saw it with gradient descent, that we have the following algorithm.
This gradient that we saw to compute using back propagation can be actually very intensive
to compute, especially if it's computed over your entire training set.
So this is a summation over every single data point in the entire data set in most real
life applications.
It is simply not feasible to compute this on every single iteration in your optimization
loop.
Alternatively, let's consider a different variant of this algorithm called stochastic gradient
descent.
So instead of computing the gradient over our entire data set, let's just pick a single
point, compute the gradient of that single point with respect to the weights and then
update all of our weights based on that gradient.
So this has some advantages.
This is very easy to compute because it's only using one data point now.
It's very fast, but it's also very noisy because it's only from one data point.
Instead there's a middle ground.
Instead of computing this noisy gradient of a single point, let's get a better estimate
of our gradient by using a batch of B data points.
So now let's pick a batch of B data points and we'll compute the gradient estimate simply
as the average over this batch.
So since B here is usually not that large on the order of tens or hundreds of samples,
this is much, much faster to compute than regular gradient descent.
And it's also much, much more accurate than purely stochastic gradient descent that only
uses a single example.
Now this increases the gradient accuracy estimation, which also allows us to converge much more
smoothly.
It also means that we can trust our gradient more than in stochastic gradient descent so
that we can actually increase our learning rate a bit more as well.
Mini batching also leads to massively parallelizable computation.
We can split up the batches on separate workers and separate machines and thus achieve even
more parallelization and speed increases on our GPUs.
Now the last topic I want to talk about is that of overfitting.
This is also known as the problem of generalization and is one of the most fundamental problems
in all of machine learning and not just deep learning.
Now overfitting, like I said, is critical to understand so I really want to make sure
that this is a clear concept in everyone's mind.
Ideally in machine learning, we want to learn a model that accurately describes our test
data, not the training data.
Even though we're optimizing this model based on the training data, what we really want
is for it to perform well on the test data.
So said differently, we want to build representations that can learn from our training data but
still generalize well to unseen test data.
Now assume you want to build a line to describe these points.
Underfitting means that the model does simply not have enough capacity to represent these
points.
So no matter how good we try to fit this model, it simply does not have the capacity to represent
this type of data.
On the far right hand side, we can see the other extreme where here the model is too
complex, it has too many parameters and it does not generalize well to new data.
In the middle though, we can see what's called an ideal fit.
It's not overfitting, it's not underfitting, but it has a medium number of parameters and
it's able to fit in a generalizable way to the output and is able to generalize well
to brand new data when it sees it at test time.
And to address this problem, let's talk about regularization.
How can we make sure that our models do not end up overfit because neural networks do
have a ton of parameters.
How can we enforce some form of regularization to them?
Now what is regularization?
Regularization is a technique that constrains our optimization problems such that we can
discourage these complex models from actually being learned and overfit.
So again, why do we need it?
We need it so that our model can generalize to this unseen dataset.
And in neural networks, we have many techniques for actually imposing regularization onto
the model.
One very common technique and very simple to understand is called dropout.
This is one of the most popular forms of regularization in deep learning and it's very simple.
Let's revisit this picture of a neural network.
This is a two-layered neural network, two hidden layers.
And in dropout, during training, all we simply do is randomly set some of the activations
here to zero with some probability.
So what we can do is, let's say we pick our probability to be 50% or 0.5, we can drop randomly
for each of the activations 50% of those neurons.
This is extremely powerful as it lowers the capacity of our neural network so that they
have to learn to perform better on test sets because sometimes on training sets, it just
simply cannot rely on some of those parameters.
So it has to be able to be resilient to that kind of dropout.
It also means that they're easier to train because at least on every forward pass of
iterations, we're training only 50% of the weights and only 50% of the gradients.
So that also cuts our gradient computation time down by a factor of 2 because now we
only have to compute half the number of neuron gradients.
Now on every iteration, we dropped out on the previous iteration 50% of neurons, but
on the next iteration, we're going to drop out a different set of neurons.
And this gives the network, it basically forces the network to learn how to take different
pathways to get to its answer and it can't rely on any one pathway too strongly and overfit
to that pathway.
This is a way to really force it to generalize to this new data.
The second regularization technique that we'll talk about is this notion of early stopping.
And again here, the idea is very basic.
It's basically let's stop training once we realize that our loss is increasing on a
held out validation or let's call it a test set.
So when we start training, we all know the definition of overfitting is when our model
starts to perform worse on the test set.
So if we set aside some of this training data to be quote unquote test data, we can monitor
how our network is learning on this data and simply just stop before it has a chance
to overfit.
So on the x-axis, you can see the number of training iterations and on the y-axis, you
can see the loss that we get after training that number of iterations.
So as we continue to train in the beginning, both lines continue to decrease.
This is as we'd expect and this is excellent since it means our model is getting stronger.
Eventually though, the network's testing loss plateaus and starts to increase.
Note that the training accuracy will always continue to go down as long as the network
has the capacity to memorize the data and this pattern continues for the rest of training.
So it's important here to actually focus on this point here.
This is the point where we need to stop training and after this point, assuming that our test
set is a valid representation of the true test set, the accuracy of the model will only
get worse.
So we can stop training here, take this model and this should be the model that we actually
use when we deploy into the real world.
Anything any model taken from the left hand side is going to be underfit, it's not going
to be utilizing the full capacity of the network and anything taken from the right hand side
is overfit and actually performing worse than it needs to on that held out test set.
So I'll conclude this lecture by summarizing three key points that we've covered so far.
We started about the fundamental building blocks of neural networks, the perceptron.
We learned about stacking and composing these perceptrons together to form complex hierarchical
neural networks and how to mathematically optimize these models with back propagation.
And finally, we address the practical side of these models that you'll find useful for
the labs today, including adaptive learning rates, batching and regularization.
So thank you for attending the first lecture in 6S191.
Thank you very much.

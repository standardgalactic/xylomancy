And we are going to see connections of language models
that maybe you did not quite expect to anticipate,
but they are very real.
And to start with, we are very lucky to have
Steven Pantadosi, he is a professor of psychology
and neuroscience at UC Berkeley.
And a friend of AI, we actually have a grant together.
And a lot of us in AI are excited and interested
in learning from the psychologists
and seeing how they can inspire our work.
Well, actually this is a two-way street
and because the psychologists are also getting excited
about the language models to understand
something about humans.
And Steven will tell us about some of the work
he has been doing, which I think is very exciting.
So, Steven.
So thank you for the invitation to speak here.
I'm going to be talking about the meaning
in the age of large language models
or maybe finding meaning in the age of large language models.
And this talk isn't a kind of technical talk
about language models or evaluation or anything.
It's almost closer to philosophy
or closer to kind of high level theories
in cognitive science and psychology,
which are about meaning.
And some of the kind of history of work
of people's ideas about where meaning comes from
in language or in semantics
and how we can think about those
in the context of large language models
and in particular in the context of debates
about large language models
and whether they're just stochastic parrots
that don't have any form of understanding
or whether there's some sense in which they have understanding
and if they do have some form of understanding,
if that form of understanding is at all related
to the types of understanding that people have.
So my plan for the talk is first to talk about
meaning and reference kind of generally.
I'll talk about some claims in large language models
and why people often think that there's no sense
of meaning or kind of real semantics in them.
Then I'll talk about some psychological theories
about meaning and how meaning arises
from what are called conceptual roles.
I'll come back to large language models
and talk a little bit about learning conceptual roles
in large language models
or in kind of general machine learning systems.
And then some very kind of brief overview
of a learning conceptual role experiment in people.
So let me start with kind of how I got interested in this
which was this paper by Bender and Kohler in 2020.
Emily Bender is a computer scientist
trained as a linguist also at the University of Washington
who people may know as a very vocal critic
of many aspects of large language models.
The one that initially I think interested me
was claims about the meaningfulness
of large language models
and essentially arguments that there's nothing meaningful
at all in what statistical models
that are trained on text can do.
And Bender and Kohler came up with a very nice way
of making this point what they call the octopus test.
The octopus test goes as follows.
So kind of starting point is for them,
meaning is an association between a word say
and something external to language.
So the meaning of the book
is some physical object, a book that's out in the world.
You can see that that's right
because that's how we label physical objects
and when we're learning words,
we hear the word book when there's books around
and we pick up on that association.
And so that's kind of fundamentally
what that kind of external reference
to something in the real world
is fundamentally what the word means.
And Bender and Kohler say, okay,
let's take that as our definition of meaning
and let's imagine an octopus.
So an octopus who lives under the ocean
and has tapped into a communication channel,
say a telephone line between two islands, okay?
So the octopus is there,
it's eavesdropping on all of the communication
that happens between those two islands.
You can get a huge amount of linguistic input
and you could imagine very smart octopus
might be able to learn all of the statistical properties
of what's happening across that communication channel, right?
Might be able to learn, become very good at predicting text,
maybe it could predict it optimally, whatever.
Their argument is that the octopus
could never actually learn the meanings, right?
Because it would never have access
to the physical reference, right?
So yeah.
Is it really different from the Chinese room?
Yeah, so it's interestingly a little different
from the Chinese room.
And maybe can I defer that question to the end
if that's okay?
Because I think what's going on with the Chinese room
might make more sense with what I say later, okay?
But very similar in spirit, I would say.
So this octopus has no access to the physical reference
and therefore couldn't solve tasks
involving the physical objects, right?
If you asked them to visually recognize
what a coconut was, even if they knew
all of the statistical properties
of where the word coconut would be used, right?
They wouldn't be able to solve the physical stuff, okay?
And of course this is the situation
that a large language model is in,
at least one that's only trained on text, right?
It doesn't get access to stuff from the world,
it only has kind of text, okay?
Therefore this octopus doesn't have meanings for the words.
Such an octopus is like a large language model
because they only have text.
And predictive ability therefore can't give you
the meanings, right?
Meaning is something just fundamentally different
than predictive ability.
Let me give you one other example of this.
Well, I'll just say, like I think that on the surface
this is a somewhat convincing argument, right?
It's kind of compelling to think of meaning in this way.
And certainly if you do, it seems pretty convincing, yeah.
It may not be right if you're a mathematician
who wants to get a new set of axioms.
And you know, you're not supposed to have,
because this is really referring to
this particular interpretation of that one.
This would make that up in your mind.
Yeah, great.
So hold on to that thought too,
because a mathematician thinking about axioms
is very much related to another version of meaning
that I'll talk about in a few minutes, okay?
Even as Hilbert wanted it to be, right?
Hilbert's desire was to convert Euclidean geometry
to a set of axioms such that every symbol
could be replaced by some arbitrary squiggle.
And the system should still work.
That is what Hilbert did in the axiomitization of geometry.
Yeah, so I think most mathematicians
would have a slightly different sense of meaning,
but one which matches what I'll say in a few minutes.
So yeah, let me give you just another kind of gloss
on meaning and language models.
Here's Gary Marcus talking about lambda.
In truth, literally everything the system says is bullshit.
The sooner we realize that lambda's utterances are bullshit,
just games with predictive word tools
and no real meaning, the better off will be.
Software like lambda doesn't even try to connect
to the world at large, right?
That's what he thinks makes it bullshit.
Just tries to be the best version of autocomplete
that it can be.
They don't understand language
in the sense of relating sentences to the world,
but just sequences of words to one another, okay?
So I got interested in this,
in part because I find this view kind of compelling.
On the other hand, I also think it's kind of deeply wrong
and the way in which it's deeply wrong
is really interesting for what it has to say
about conceptual representations and meanings
in human minds as well as in machine learning models.
So a few years ago, I teamed up with Felix Hill,
who's a researcher at DeepMind,
and wrote an article basically going through arguments
that meaning is not this form of reference, right?
So in fact, this idea that meaning should be equated
with some mapping to things in the world
has often been rejected by people in linguistics
and philosophy and cognitive science.
And I think for good reason,
so just to give you a kind of flavor
of why people often reject this,
there's many concepts, many words, for example,
that have no reference to the external world, right?
Function words are a good example of this,
words like to or is or many, right?
There's no to, to, to out in the world
that that word refers to.
There's also no is out in the world or no many.
Those are function words in language
and what they do is actually much more like
kind of what an operator in mathematics or something does,
right?
These words have an internal meaning.
They control the kind of compositional meaning
of a sentence, meaning that they have to be composed
internally in linguistic representations
in order to express their meaning, right?
They're not pointing to something out in the world.
Even if you don't go to function words like this,
there's other words which are very hard to make sense of
in a kind of view that meaning is stuff in the world.
So you can think of very abstract words like justice, right?
There's probably not a justice out there.
That's some kind of construct that we have or wit,
or you can think of things that don't exist like dragons,
right?
There's no external thing in the world, which is a dragon.
There's even words and concepts we have
that have no possible reference to the world, right?
So if I think about an imaginary bicycle,
that's something which is by definition imaginary, right?
It's not out there, or a perpetual motion machine, right?
We can have a concept of a perpetual motion machine
and think about it and reason about it,
but there's certainly not one that exists out in the world.
Even for ordinary concepts,
we likely haven't even considered all of the possible things
which could be reference of those concepts, right?
So I could walk in wearing a shoe made out of eggplants
and you could look at them and everybody might agree
that they're shoes, right?
But you would agree that they're shoes
without ever having seen shoes made of eggplants before, right?
So there's some object in the world
which everybody would agree is a shoe.
Even though you've never encountered that thing before,
that means that it couldn't have been the stuff in the world
which determined whether it was a shoe, right?
Had to be some kind of more abstract conception
of what makes something a shoe, right?
You can think about things like the function
or the origin, how they're used.
These other kinds of properties of objects
seem to be much more important for the categorization
of the concept, yeah.
Well, here this is a little bit of a survivorship bias
because eggplant shoes might still be considered shoes,
but then ice cream shoes are probably,
nobody will recognize them as shoes, right?
But then you don't think about ice cream shoes.
So it's like the things that you can think of as shoes
are in your little bowl
and then you don't think about things
that are already too far.
So it seems like there is still kind of some distance
to the closest real object.
Yeah, yeah, so all of this is not to say
that the real objects are irrelevant, right?
Like I agree that eggplants are much more plausible issues
than ice cream and that has to do
with the kind of real physical properties
of those substances.
My point is just that the physical thing
is not the defining thing, right?
It's not something in the object you look to
to decide whether it's a shoe or not, right?
It's something more abstract about how it's used
or made or something like that.
I'll actually talk a bit in this paper
about the concept of a postage stamp, right?
Which is just an example of one
that people probably have some intuitions about
where you could easily think of postage stamps
which are fundamentally different
than any ones you've seen before, right?
You could think of one made of glass, for example,
or you could think of one that was an RFID tag,
which is probably physical incarnations
of postage stamps like that,
that everybody would agree should be called a postage stamp.
And if you try to get people to define it, right?
You might say something like,
well, a postage stamp is something you pay for
and you put on a letter so that the letter
will be delivered by the government
or something like that, right?
So what the term means is intrinsically connected
to a bunch of these other terms,
like payment and letters and being delivered
and those things.
And in fact, if those terms change meaning, right?
So if, for example, people develop a new way
of paying for things, paying on the blockchain
or something, right?
Then you kind of know automatically
that a postage stamp can be paid for
in that way, at least in principle, right?
So it's not just that the word is associated
with those other things,
but that its meaning is inherently connected
to those other things.
So that's one kind of take on why reference
to stuff out in the world, right?
Is not a good way of thinking about meaning.
Let me tell you what one alternative is.
Actually, before I do that,
let me just show a couple of other alternatives,
which I think are also not plausible,
but might be familiar to people, okay?
So what I just talked about is this kind of world mapping
view, right?
That there's some word and its meaning
is some physical object or some thing.
You can think about other kinds of views of concepts
and meaning might have a kind of feature spacey kind of views
for vector machines or something, right?
There's some abstract feature space
and a concept is some dividing line or some region
or something in this space.
That I think is maybe fine in some narrow applications,
but what I'll talk about next are cases of say human cognition,
which really don't fit well into that picture
in the sense that things are much more complicated
for how people think about concepts and their relationships.
People might also have this sort of hierarchy
or a network view, right?
So sometimes people think, oh, sorry, you can't see this.
There's supposed to be lines connecting one concept book
in the middle to a bunch of other concepts, right?
And you might, you know, there's old theories
of say semantic organization or very old AI
kind of approaches, right?
That think about building hierarchies of concepts
or sometimes, you know, networks of concepts
and trying to define meaning in terms of those relationships.
I actually think that both this and the feature
and the world mapping view have some of the,
some kind of useful properties
or useful insights about concepts,
but just aren't quite the whole thing
for reasons that I'll talk about next.
So let me just start with,
start trying to introduce this kind of other view of concepts
by trying to get people's intuitions on a recent news story.
Okay, so here's a little recent news
from the US versus Trump.
I think this is not the most recent indictment,
but one or two indictments ago.
If you look through it, you can read all about
Pence and Trump and efforts to manipulate the election
and things.
Here's a little paraphrase of one of the paragraphs, 90 C.
So Pence, the vice president, right,
opposed a Trump team lawsuit,
arguing that the vice president could reject electoral votes.
So Pence didn't want them to argue
that he could reject electoral votes.
He said to Trump that he didn't have constitutional authority
and that the action would be improper,
so it's according to Pence's notes at the time.
And Trump responded, you're too honest.
Okay, according to Pence's notes.
So think about that situation
and everything you know about this context, right?
And think about an answer to a question like,
why did Trump say this?
Right.
You think about that,
probably what's going on as you think about it, right?
As you're thinking about lots of other things
and how they're related to this situation,
like what Trump was trying to achieve,
maybe what kind of personality Trump had,
what Trump was trying to do to Pence,
is he trying to manipulate him
into taking some kinds of actions?
What exactly that action would have, right,
in terms of the election?
Everybody is perfectly capable of reasoning through these
and coming up with kind of plausible causal story
about what's happening, right?
It feels like we can come up with our own
kind of internal explanations about events like these.
And in fact, that process of coming up
with internal explanations, interrelated kind of concepts
and meanings is one that people in developmental psychology
have been very interested in and excited about
as a theory of kind of human cognition.
So basic observation is that people form these
very richly interconnected systems of concepts, right?
All of the kind of interconnected stuff you would need to draw
in order to answer a question like that,
which feels totally, totally normal.
People sometimes call these intuitive theories, right?
You have some intuitive theory of how Trump is acting
or how the political system would work
or some intuitive theory of what Pence might be doing
or might be trying to achieve.
And these things are often compared
to theories in science, right?
So you can think of your theory of why Trump might do this
as kind of analogous to a little scientific theory, right?
It has some pieces, it has some relationships
between the pieces, it has some dynamics.
And maybe you can look at all of that
and kind of reason about it causally
as you might reason about any other kind of system
that you've encountered.
So the idea that people and maybe most notably kids do this
is one which is really had been very popular
in cognitive development, championed maybe most prominently
by Alison Gopnik, who's a developmental psychologist
here at Berkeley.
Let me just give you a quick example of how kids,
how experiments with kids like kids sometimes go
in this domain.
So here's an experiment from Lizloth and Gelman.
So kids are shown these two foxes, right?
Which you might notice are identical pictures, okay?
And then they're told things about these foxes
and asked what they could do in order to answer a question,
right?
So this is like a simple version of why did Trump say that?
You might be told that one is an animal and one is a toy,
okay?
So what could you do in order to determine
which one is an animal and which one is a toy, right?
In this experiment, kids will say that you should do things
like check the insides, right?
Like check their guts or whatever, right?
Open them up and see or look at their behavior, right?
If it acts like an animal, then you could use that
to figure out which one is the animal, which one's the toy
or look at their parents, right?
Like, you know, the animal will have animal parents
and the toy won't, right?
And importantly, they don't just say, yeah,
you can check everything about these.
They know, for example, that age is not relevant, right?
So they won't tell you that age would tell you
which one is an animal and which one is a toy.
And it's worth pausing and just thinking about this
and what this means in terms of conceptual representations,
right?
Because you can think about your concept of what makes
something an animal or what makes something a toy
and kind of like the postage stamp example, right?
It's intrinsically connected to these other things,
like what parents are or what's going on
with your guts inside, right?
Or what your behavior is, right?
That concept is just intrinsically linked there
and kids, I think these are preschoolers know that
from a pretty young age.
I'll ask them a question, like one is a dog,
one is a wolf, what would you do, okay?
Kids basically say the same things there.
You could check the insides, you could look at behavior,
you could look at their parents,
see if they had a dog parent or a wolf parent.
Some of these are actually kind of interesting, right?
Because I don't think anybody knows, at least I don't,
what you would look for on the insides
to distinguish a dog versus a wolf, right?
Like maybe you could go down to DNA
or I'm sure you could go down to DNA to tell that.
But people have the intuition that like, okay,
there's something about being in this category
which depends on these other aspects
of being in the concept, yeah.
To me, I read this much simpler.
It's like, basically what they're saying is look,
like all of this are visual things.
And just that you're trying to project it into language,
but actually what the kids are probably meaning is,
you will know it when you see it
and when you play with it, right?
It's vision and interaction.
Yeah, yeah.
And age is neither.
Yeah, yeah.
So I think it's true that, yes, all of these are visual cues.
I don't know of experiments that look at non-visual cues.
But I agree, yeah, that's interesting.
I'll give you one other example where they know
that there's no cue, right?
So if you tell them that one is named Amanda
and one is named Melissa,
then they'll reject all of these as tests, right?
They'll say, okay, the insides are not gonna tell you
which one is Amanda, the behavior and the parents
and the age and these things are not going to do it.
So all of this is just to say that people have a,
even kids, right, have pretty sophisticated theories
of how concepts relate to other concepts, right?
And in fact, in a situation like this, right,
there's nothing visual apparently that could tell you, right?
So it's not a visual discrimination task.
It's really a kind of conceptual one
that's asking you to look at other kinds
of conceptual features and things.
So people have these intuitive theories.
Then one kind of proposal quite a few people have argued for
is that meaning arises from essentially the role
that a word or a symbol or a concept plays in this theory,
right, like the meaning of animal is really just intrinsically
related to these ways of testing it
and these kinds of features and all of the other things
that are not kind of simple semantic associates with animal
but are kind of deeply connected
in the sense of an intuitive theory.
I was trying to come up with examples
where we'll give people this intuition, right,
that if you try to define these words,
if you try to define what an indictment is, for example,
it's very hard to do it in a way that doesn't reference
other legal terms and other kind of social constructs
and concepts that you already have, right?
It's kind of intrinsically related
to the system of other concepts and terms, right?
Chord change is kind of like this too in music, right?
You have to talk about chords and notes
and circle of fifths or whatever, right?
Like these things are just intrinsically related.
I think force in physics is like this.
It's very hard to talk about it in isolation,
independent of experiments or other concepts or things.
Or if I said like, what does a bobbin do
in a sewing machine, okay, right?
Like you have to talk about thread
and you have to talk about the processes of sewing.
Just the meaning of these things
are just all intrinsically linked together.
So this idea that meaning is not about reference,
it's about the role that something plays
is called conceptual role theory.
Meaning of a word or concept is determined
by the role it plays.
And this has been argued for,
I think maybe most prominently by Ned Block,
who's a philosopher of mind,
who wrote one of my favorite paper titles,
advertisement for a semantics of psychology,
which is basically all about,
how psychology needs a theory of meaning
and a theory of semantics.
And this idea of conceptual role
is something that could do that.
So it can explain kind of where meaning comes from.
It can address questions of how meaning depends on things
like your representations or categories that you know
can play nicely with compositionality
or other aspects of language.
And I think maybe most compellingly,
it can explain how you could find meaning in brains, right?
So if you open up a brain
and you start recording from neurons,
it's really unclear what it means for there
to be reference in there,
reference to the external world in there.
But maybe you could kind of make sense
of patterns of activity in a way
that lets you kind of interpret systems
of signals and representations.
Let me give you just one other example of this
that maybe might make things more clear.
This idea of conceptual role semantics,
I think is also how meaning works in, say, a computer.
Also, I think in mathematics,
which is why I was deferring the questions
about mathematics,
you could look at something like this, right?
This is a floating point representation
and ask what makes the bits in this representation
mean what they do, right?
In particular, what makes the first bit mean the sign bit,
right?
It's nothing about being the first one
because there've been dozens of different conventions
for floating point numbers,
which put the sign bit in all kinds of different places, right?
What makes it mean the sign bit
is how it interacts with all of the other operations
that you can do with floating point numbers, right?
So meaning, in some sense,
comes from the interaction between symbols,
or in other words, their conceptual role.
So in particular, like what does negation do, right?
If I have a negation operator, okay, it flips the sign bit.
Great, okay, that's where it gets its meaning from, right?
Or what does addition do?
Does addition do the right thing with respect to the sign bit?
Or multiplication, or rounding, or whatever, right?
So what makes this the floating point representation
or what makes that first bit represent sign
is nothing intrinsic in the representation itself.
It's how it interacts with all of the other components
of the system, okay?
Yeah?
Can you explain the difference between this way
of thinking about the significance of the hierarchical approach
where there's concepts and there's sort of numbers
Yeah, so I think that there's certainly concepts people have
that are hierarchical, right?
So we know that dogs are a kind of animal
and animals are a kind of living thing.
I think what that kind of picture is missing
is that our representations are actually
like computational objects, like they do something, right?
They interact with each other
and they allow us to solve certain kinds of inference problems
and all of the stuff you could do with your concepts
like the Trump example, right?
Yeah, so the claim is not that they're not hierarchical, right?
It's that the interesting important things they do
come from interactions kind of internally between concepts
much like the way that the sign bit is interesting
or important here comes from its interactions
with things like negation and multiplication.
Yeah, yeah.
What you're saying here is perfectly good,
but what I have trouble with is buying this
as an exclusive theory of semantics.
Just like you gave good arguments
against the meaning is just reference, okay?
I think you demolished that theory,
but now you put up another theory which also I find
that it has some good aspects,
but to make that an exclusive theory is problematic.
So if we look at children growing up,
there are these studies on sort of concreteness judgments.
So the vocabulary of a child at two,
there are a lot of words in there like milk and bottle
and jump and sit and so forth,
which are very concrete, concrete in a visual sense,
concrete in a motor program sense.
At the age of 10, they have worked like justice and fairness
and so forth, which are very much,
which fit much better into this conceptual road story,
where is the vocabulary of a child at two,
maybe one where this kind of groundedness
to sensory motor experience is a much better account.
And this is not problematic for me.
Why do we need to have one exclusive theory for meaning?
Both of these are aspects of meaning.
Yeah, I agree.
So I think that you can think of the physical reference,
as in some sense one of the conceptual roles
that something can have.
It is important.
I'm not sure we know kind of how abstract kids early meanings
are for those kinds of words,
because it has to be a little bit abstract
because you'll call a new bottle that you see a bottle still.
So you have some abstraction away from the examples
of bottles that you've seen,
but I agree, it feels early on very concrete
and much less abstract than things we come later.
Just trying to make sure I understand
what this theory is saying.
So is there a character to think of this
that you're saying that meaning is basically
like some homomorphism onto some either intuitive
or formal theory?
Ah, sure.
So then maybe a follow-up question is like,
how do we know which homomorphisms are valid?
Because I could always,
if I can have some arbitrary correspondence mapping,
I could make anything correspond to anything else.
You know what's so loud here?
Yeah, so I don't think anybody has been that formal.
People like Putnam have made this kind of argument
about understanding computation in physical systems,
basically saying like physical systems,
like a brain or in his example, a wall,
are so complicated that I could come up with
kind of any mapping back and forth between the states of it
and the states of a kind of arbitrary computational system.
And that's probably a much longer thing to get into.
I'll just say that I don't think I have a very easy answer
about that.
I think of this as not kind of,
certainly not formalized in that sense,
but in sort of a higher level in terms of
what kinds of theories we should be looking for, right?
And then there's lots of work to do
in terms of making that precise.
So, yeah.
Yeah, Quine actually uses that example
to motivate this kind of theory in like 1950s philosophy.
Sorry, which example?
Quine uses this example.
He uses an example of Gavagai.
You see something popping out and you're like,
how do you know Gavagai means rabbit and not running
and not hole and not something else
because the real world doesn't determine what a meaning is.
Yeah.
Yeah.
Yeah.
A lot of concepts and semantics have been
extensively terminated, scripting, what is it?
Is any of this operationalized at all?
Can you comment on that?
I don't think so, yeah.
So, I mean, I can talk,
I have a couple of examples of kind of learning
intuitive theories, which essentially have this kind
of character, so of taking data and then trying to come up
with some structures that obey the right relations, right?
And yeah, I'll talk a little bit about that,
but there hasn't been a ton of work on that, so.
Yeah.
Could you talk about how this theory deals with
when the same symbols or words are in different kind
of theories or settings?
Does it kind of mean that the symbols themselves
don't have meaning or how are the kind of the meanings
shared across different contexts?
Yeah, so that's an interesting question
that I think people have not resolved very well.
So, you know, your symbol for your father
might play a bunch of different roles, right?
Because you know what job your father has
and you know what family relations and you know
what hobbies and I don't think that there's good
kind of formalized accounts of how to make sense
of all of that, so there's not great theories
of kind of formalizing conceptual roles.
I'll give some arguments why I think it's possible
that language models are doing this at least
in a tiny version, but in terms of like rich
and kind of human-like conceptual roles,
I think that's one of the key problems that's hard to solve,
so.
Is there another one?
Yeah.
I think that the oncology of the kind of digital world
people think that we're using right now, for example,
is a subset of the oncology of humans' minds
and probably also a subset of all possible
future invented concepts and so on.
Sorry, I missed the very first part.
What was the question part?
So the question is whether you think
that the existing ontology that you are using now
is a subset of the ontology of human's mind
that we haven't fully explored
and probably that is also a subset of what kind of
can be inventive or creative produced concept
that you mentioned in the beginning.
By ontology, do you mean these theories?
I mean terms, for example.
Yeah.
Concepts.
Yeah.
I mean, I don't think any of these
is quite the right answer, right?
Like, these things are actually very difficult
to figure out, but I think they're kind of pointing
in some useful directions or something.
So I don't know if that answers your question, but yeah.
And do you feel that symbols that you're using
why does matter because as long as you define
certain meaning, whether you use words to represent
or you define to represent, it doesn't matter, right?
In terms of which symbols?
Like mental representations or?
Yeah.
Without a concept, whether you use the words
to represent or you define to represent
all kinds of knowledge or like,
I'm sure this kind of concept has a lot to do with
the form, how do you explain, can you explain?
Yeah, so.
Yeah, sorry.
Make sure that you, how far do you still have to,
like, do you have any?
I have a little ways to go.
Okay, so maybe we should push this to work
after, after at the end because it seems like a,
you know, a deeper discussion.
Yeah, yeah, okay, great.
Okay, so I've talked about these kinds of accounts
of meaning, right?
Particular meaning as conceptual role.
And let me talk a little bit about learning
conceptual roles and why we might think
that's plausible or useful.
Seems to me at least that large language models
almost certainly need to learn some of these pieces
of conceptual role.
But these kinds of things seem really necessary
for the stuff large language models are good at, right?
Writing coherent texts or doing translations
or providing definitions or providing elaborations
or explanations, but all of those things require you
to put symbols in the right relationships
with other symbols, right?
And that means that, like, to do those things well,
you essentially have to have some little components
of conceptual roles, right?
One way to think about this is that human meanings
or human conceptual roles generated the text, right?
So maybe a smart inferential model, right,
could invert that and figure out what were the likely
conceptual roles that generated the thing that I saw.
I like this, this Frager quote, right?
The structure of sentences serves as an image
of the structure of thoughts, right?
Some projection of our thoughts or our meanings,
our conceptual roles that gets realized into sentences.
Yeah.
Yeah, so are you gonna follow up on something
that you thought that we could reach to the reality
and we could put in two boxes behind that?
The which examples?
Are you gonna recapitulate with that
and get them into two boxes behind that?
How's that a Frager type of thing?
No, I wasn't gonna go back to that, yeah.
But I'll talk about a study in large language models
in a minute.
Okay.
Okay, a lot of people have the intuition
this is not possible.
I think this is kind of the Bender and Marcus intuition
that our thoughts really get projected
into this kind of impoverished sequence of sounds, right?
How could you discover something
like rich conceptual roles there, right?
If you just have this projection of language,
how could that ever support, you know,
rich and interesting kinds of conceptual roles?
One kind of way that I think is a helpful analogy,
although not kind of a mathematically precise
implementation or something,
people may know these embedding theorems
from dynamical systems, which I think are very cool.
There's this paper called Geometry from a Time Series,
which essentially shows that in some cases
you can take projections of dynamical systems
and recover things which capture the structure
of the dynamics from that projection.
So in particular, in this paper, they go through this,
which is the Rosler Attractor.
This is a, you know, system of three dimensional system
of differential equations.
And what you can do is take a one dimensional projection
of those dynamics.
So you can look at just the X location
of what's happening there.
And through a clever trick,
essentially translating the one dimensions
into three dimensions using,
by going backwards in time, some number of steps.
You can actually recover the structure of this
from the one dimensional projection.
Okay.
And there's other kind of general theorems
about when this is possible,
Parkinson's embedding theorem and things like that.
The kind of point here, right,
is that we shouldn't really have strong intuitions
about what's possible from some projection of thoughts, right?
Because oftentimes there might be possible
for people to, or for learning models
to reconstruct kind of interesting parts
of the structure of some system
just from simple kind of measurements of that system.
Actually Shaw here, the senior author wrote an entire book
on recovering the kind of dynamical properties
of a dripping water faucet,
where you can measure the time between drips
and figure out things about the kind of latent variables
and latent structures they're using techniques
that are a lot like these.
In psychology, actually people have also been interested
in kind of closely related types of models.
So there's this work by Roger Shepard in the 80s,
which essentially would take behavioral judgments
and try to infer the underlying structures behind them.
So for example, this matrix here is different colors
or different wavelengths of light
and then confusability between them on judgment tasks.
So just, you know, how similar are these things
or how confusable is one color with another?
And Shepard was using multi-dimensional scaling
to go from data like this up to a representation like this,
which you might recognize as a color wheel, right?
Basically you can arrange points
so that their distances correspond to the distances
in the confusion matrix and therefore recover something
about the kind of underlying, in this case,
psychological structure that generated that data.
People have also done similar kinds of things
in learning kind of real formalized versions
of theories or of intuitive theories.
I really like this paper by Tomer Olman
and Noah Goodman and Josh who's speaking next
on learning a theory of magnetism.
So basically you take observations
of which objects interact with other objects
and do some learning to acquire a kind of high level theory
of the fact that there are two different kinds
of magnetic objects and those two different kinds
of things will interact with each other,
but they won't interact with things
that are non-magnetic, right?
So this is like a little tiny, mini intuitive theory
that you can acquire just from very simple,
you might think kind of impoverished data
about interactions.
Okay, so when people talk about LOMs
just being based on text,
I think that isn't really enough to conclude anything
about what theories they might induce, right?
Or what kinds of internal structures
and conceptual roles they might induce from that text.
And in fact, there's some evidence I think that
what they are inducing looks pretty plausible
at least in kind of simple domains.
So there's this paper by Grandin and colleagues
which essentially looked at word embedding vectors
and projected them onto, say, intuitive dimensions.
So here you have a bunch of words,
you project them onto this line,
which is the line connecting small and large.
Okay, so all of our high dimensional word vectors
get projected onto the small versus large line.
And we take that as a way of measuring
how large versus small different objects are.
And then the question is,
in a model trained only on text prediction,
is that, does that projection recover anything human-like
about the underlying conceptual spaces?
And they show yes it does.
So here's six plots where the x-axis
is the semantic projection, right?
So how far on that small to large line something is.
And then the y-axis is human ratings
of how small versus large an object is, right?
You can see that the correlations here are not perfect,
but they're also not garbage, right?
They're actually quite strong I think for a model like this
that things which, you know, the model calls
wet versus dry or big versus small
or dangerous versus safe, people also agree with, right?
So just in predicting text, this thing has recovered
these kinds of aspects of semantic structure
latent in the word vector representations, yeah.
This is probably sitting there in n-grams
in bi-grams in fact, that information, right?
It doesn't have to do anything with the real world.
Well, it does have something to do with the real world.
It's like a small and large could just be
linguistic constructs and you're testing it on language.
Oh, I see.
You think it's that you say small tiger
versus large tiger or something.
Or small puppy.
And do that.
Right, puppy is always small, tiger is always large.
Yeah, so it might be true in n-grams, I'm not sure.
I don't think that they looked at that.
I don't think that that defeats the argument though, right?
Because I think it is the case that, you know,
even n-gram statistics are statistics
about word relations, right?
So it might be that you don't need fancy language models
or something to do this.
But you're not actually like you don't need real world
for this to work.
Well, the real world generated how often you hear
small puppy versus large puppy, right?
So the real world is mirrored in those statistics
and then the configuration that system comes up with
is also one that mirrors those properties
of the real world, yeah.
I think, can I put it back on as well?
Yeah.
I do kind of feel like what others are saying is right.
This is much closer to n-grams than it is
to large language models.
And the properties we're seeing are just so much wilder
than any of these embedding kind of tricks in practice.
Oh, you mean that large language models
are much smarter than this?
I don't even see how this is comparable in a way, right?
Like this pops out of like PCA,
whereas like we're seeing like these wild
emergent behaviors coming out of large language.
Yeah, yeah.
So I mean, I don't think this explains
wild emergent behaviors.
I think this was just trying to say that, you know,
when you train on text prediction,
you configure yourself to align
with some of the true properties of the world
which are reflected in the text analysis.
That's all.
Yeah.
Okay, so I'm short on time.
I'll skip this.
I'll just say that there's other papers looking
at transformers and kind of how they relate
to classic studies on concepts in cognitive science,
classic kinds of effects, but I'll skip that.
Maybe I'll go very briefly just through
this kind of fun experiment.
This is Mark Gorenstein in my lab
has been interested in learning concepts
just from linguistic experience,
maybe linguistic prediction.
He's been doing these kind of cool experiments
where we give people passages of natural language
where there's some blanks.
So here's a passage.
The myth of blank is so powerful
that the very words conjure up blank of strudel
and blank in a cozy Vietnese cafe, blah, blah, blah.
And the job of participants in this
is to learn where to put the word DAX.
Okay, so DAX is a novel word
they've never encountered before.
You have to read this and understand the context
and stuff in order to figure that out
and see where DAX should go.
Secretly behind the scenes, this example
has been chosen just from a big corpus of text
of a really rare word that people probably don't know.
So the rare word here is soccer tort.
And that means that this language,
like where soccer tort actually occurred here,
was generated from real people
and presumably reflects the underlying meaning
and things of soccer tort.
Maybe I don't know if I'm saying that correctly.
But with examples, with enough examples
of these people will learn where DAX is,
they get feedback on whether or not they were correct
according to whether they chose the place
where soccer tort actually appeared.
Okay, so we're having them do kind of a version
of a prediction task,
trying to figure out where this word goes,
but they don't actually see the word,
they see it as DAX.
People get pretty decent at this,
so up to 80% or so, depending on the word.
These are just, sorry, these are the examples
of the words which generated the unseen context.
And after that, we asked them a bunch of questions.
So we asked them some reading comprehension,
we asked them feature questions about DAX,
is DAX a man-made object?
Do biologists typically study DAX?
Do people use DAX in painting?
Just a whole collection of basic feature,
kind of concept-y questions.
We give them an image recognition task,
right, picking out soccer tort,
the real thing versus alternatives.
We also asked them for explicit definitions, right?
These contexts are not definitions,
they're not saying here's what a soccer tort is,
it's naturalistic usages of the object.
And what we find actually is within,
within about 20 trials or so,
sorry, everything is after 20 trials,
people are actually very, very good
at judging conceptual features for these concepts,
almost at ceiling in most of the kinds
of feature questions we asked them.
Here's each word on a row,
and then features here on the x-axis,
almost everything is read, meaning they're good at this.
They're also good at picking which picture is the object,
right, so they've never encountered any pictures at all,
but they're 80% or so good at picking these things out.
And they're even good at giving definitions
for these terms, okay?
So here's a dictionary or dictionary
or something definition of soccer tort,
it's a chocolate cake or tort of Austrian origin
invented by Franz soccer supposedly in 1832
for some prince in Vienna.
And people just from these contexts,
20 of them will learn things like it's a chocolate dessert
similar to a cake that was originally
and most commonly made in Vienna.
I think it's a type of chocolate cake
that can be ordered for dessert in Austria,
kind of rich chocolate cake from Vienna and so on, okay?
So people are pretty good at taking
kind of in-context language use
and figuring out underlying aspects
of conceptual representation from that.
They will never be able to taste them.
I mean, it tastes so good.
Have you had it?
I've never heard of it, so, okay.
Okay, so let me just wrap up here.
So I think of these kind of conceptual roles or theories
as really both a strength and a weakness
of these current large language models.
One is that large language models,
I think seem very good at learning
kind of shallow but broad theories, right?
So things like, could shoes be made out of eggplants
or what would happen if shoes were made out of eggplants?
They would know what some bad downsides
of that kind of thing might be, right?
Or answer basic kinds of questions
that might rely on kind of reasoning through one or two
kind of links about the relationships
between the objects involved in a situation like that.
I think it's been very surprising to people
that this works so well, right?
And part of I think what makes it surprising
is that these models are able to be trained
on a huge number of words, right?
And so sort of superficially knowing a little bit
about conceptual roles of a huge number of words
seems to get you pretty far
in terms of seeming convincing
and in terms of language production.
But it's also these conceptual roles and theories
are also a weakness and they don't seem very good
at robust and precise theories, right?
So if you think about conceptual roles
like in mathematics, right?
How you define say a natural number
or how you define an integral or something, right?
Like all of those are symbolic kinds of theories
which are precise and which support
chains of reasoning of arbitrary length, right?
And that's what these systems really seem
not to be very good at.
Likely that's because there's some important things
which are missing, right?
Things like grounding, things like reasoning
or even richer kinds of theories.
I think Josh will talk about this some next.
I think that there's a kind of broader view
of concepts and meanings, which is really,
I think the most exciting for people that work on concepts
and concept representations in cognitive psychology
which is that large language models have really shown
how vectors can do things that were long thought
to be impossible for non-symbolic models, right?
In particular, these kinds of arguments
from people like Fodor and Polition
about compositionality and systematicity and productivity,
right?
All of these kinds of things that people have pointed to
as characteristic features of thinking
have argued were characteristic features
of symbolic thinking just turn out not to be right, right?
Turns out you can get vectors to do those things.
And I would argue that the solution
to why vectors could do those things
is probably that what these models are doing
is training vectors that encode conceptual roles, right?
Like what they're learning is representations of meaning
which capture the important parts of conceptual roles.
This is actually something which has been long sought after
in say computational neuroscience.
There's things like vector symbolic architectures
that are very exciting ways of encoding
say arbitrary symbolic systems
or arbitrary mathematical systems into vectors.
And I think that some marriage of those two things
is going to be very exciting.
So large language models point to a theory of meaning
that's based on essentially vector based conceptual roles,
and perhaps can capture a lot of the different features
of meaning that people in say cognitive science
or cognitive development have tried to kind of bring out
in human conceptual systems, right?
Like that our meanings are gradient
or that they have hierarchies,
that we know things like definitions
and we can make inferences about relationships
and similarities and all of those things seem like things
that you can encode at least in principle in vectors
which is great.
So let's get that, let me just end there.
I'll thank you again for the invitation
and thanks also to all of my co-authors on the work here.
All right.
Yes.
Maybe I'm reading too much into it
but it seemed to me that you hinted at
what these vector representations
large language models tell us about
both human cognition as well as about language,
the nature of language.
Could I ask you to do a projective measurement
and come out and say something about that?
I think that they tell us that vectors are really plausible.
They kind of show us how vectors are plausible for meanings,
and the way in which I think that they're plausible
for meanings is that they encode conceptual roles.
That's what I would say.
And I think until them,
until kind of recent deep learning,
I think it was really unclear.
So people had argued for decades about whether
the foundation of concepts was definitions
or is it like somehow similarities
or is it that you just know a word
and you know a bunch of associated features
or whatever.
And I think one of the main insights for example
is that you can extract a definition
from a large language model.
We've even given it some of these kind of human experiments
we've done and they're pretty good at coming up
with the chocolate torch kind of definitions from those.
And that tells you that the definitions
can be encoded into vectors, right?
And that's great, right?
That means that you don't need to think about definitions
as the defining part of concepts, right?
There's some other kind of more abstract,
you know, high dimensional space or whatever
that defines the meanings.
And the sense in which it defines the meanings
is in terms of the relationships between vectors
on the tasks that you use the concepts for, yeah.
This is just natural since the brain encodes information
with lots of neurons firing through your brain.
This should not be surprising.
So it's not surprising.
It's, it was always unclear how that was even possible.
Yeah, exactly, yeah, yeah.
It's like everybody always kind of knew
that there had to be a continuous system
which could support these things.
But when you look at, you know, the discreteness
and language of this discreteness in mathematics,
it was always kind of unclear where that could come from.
So that's why I think things like vector symbolic architectures
are very exciting too.
So when a human fills in the meaning,
they're using a lot of context
that they've gotten from the real world.
Yeah.
When an octopus tries to fill in the meaning,
they have much less context to work with.
Yeah.
And when a LLM fills in the meaning,
they have no kind of context to work with.
Yeah.
So I was wondering if you had thoughts
about the differences and that.
Yeah, it's really interesting to think
of what's exactly happening in that human experiment
because I agree it's transfer of stuff you know,
like even countered cakes
and even countered fancy pastries or whatever.
And part of what you know about those meanings
are the grounded parts, right?
You know what a cake looks like,
which is why you can recognize the pictures and things.
I always have a little bit of trouble thinking about it
because it's not never quite clear to me exactly
what it means to transfer something grounded.
Like it feels a little bit like in order for it
to transfer at all, it has to be a little bit abstract.
But I agree that that's the right question to ask.
And we don't have any theories or certainly no evidence
about how exactly people solve that problem
or the way in which it relies on conceptual roles
versus grounded experience or something.
Okay, so we'll have one more question.
Meanwhile, maybe we can have the next speakers
start setting up.
Yeah, thanks Steve for the great talk.
I wanted to understand better what the argument was
in this kind of conceptual embedding experiment
because it seems like you could just ask the LLM
whether it's tall or not.
Like you didn't really need to do this projection
to know that it can do this task.
So is it somehow, is there something special about the fact
that you're looking at embeddings rather than the outputs
or what's kind of going on there?
That's a good question.
So I think you probably could do that.
I don't know how the results would compare
if you just asked versus not.
Yeah, I'm not sure.
I mean, I think it's like if it doesn't succeed
on just asking, then it's interesting to know
whether it's kind of latent representation
still has that information or not.
So, but I don't know the whole space of kind of how you,
how you can interrogate these models
for those questions, so.
All right, let's thank Steve again.
We'll have plenty of time to talk to him more
at the refreshments after this talk.
And who knows, maybe there will be Zaffer torquers in there.
Which is, by the way, amazing.
If you're in Vienna, you should absolutely try it.
So unfortunately, the next speaker could not be here.
Josh Tenenbaum is a latent variable in this session
because both Stephen was a student of Josh's
and.

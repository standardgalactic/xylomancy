pro six circuits on tiny binary assets.
So what we'll do today is actually train PCs
on language, language models,
and Juan-Wes going to tell us how that works
and what that's useful for.
So today I'll present our paper,
Tractable Control for Autoregressive Language Generation.
So unlike probably the interesting ones,
today I'll focus more on the application side
of probabilistic circuits.
Like more machine learning oriented.
Okay, so let's get started with the basic concept
of large language models.
They have been like really popular recently.
I'm not sure if people here really care or not.
Like I see like this interested face you do, okay.
That's a good sign, that's a good sign.
So basically the idea is very simple.
In the sense that you collect large amount of text
consisting of trillions of words,
and you're trying neural networks
with billions of parameters on these data,
then you have these so-called large language models.
These chat GPT or GPT-4 thing has become like really popular,
like people talking to them on the internet,
asking them to polish their papers,
using them to do their homeworks,
and even asking them to play D&D together.
So yeah, that is actually true.
So the idea is, so people have now having this feeling
that we probably have solved AI,
we have solved artificial intelligence,
and chat GPT has passed Turing test,
but is that really the case, okay.
So I tried this a few months ago.
It was an actual conversation between me and chat GPT.
So I asked the model to generate a sentence
using Frisbee, Cod, and Dog following the given order.
Well, I mean, this should be quite simple
for a super intelligent AI model, right.
So it does give me a sentence.
The sentence look quite decent,
and all the keywords are in there,
but Dog and Cod are in the wrong order, right.
Well, I mean, humans make mistakes,
so super intelligent AI does.
So I was being patient, I tried again, okay.
Again, it gives me a sentence,
but it's even worse now.
All of the words are in the wrong order.
So you see that chat GPT fails to follow
even this simple logical constraint, okay.
So people have been trying to fix this.
Some of the methods, including like,
okay, we can prompt them in a different way,
like chain of thought.
I'm not comfortable saying this phrase,
but that's one of the methods.
Or people try to use search algorithms
trying to find the correct sentence
in this huge search space, so on and so forth.
But none of them actually guarantees that chat GPT
or whatever other language model give us what we want.
So what we actually want is like 100% guarantee.
When we have like a huge powerful model,
when we instructed to do something,
we wanted to follow our instructions exactly.
So in today's talk,
I'll show you how we can do this
with probabilistic circuits.
Before some of the detail,
so let's get started with some basics of language modeling.
So language modeling is really not like a fancy idea.
It's just a joined distribution over some text,
probably less than or equal to n words.
Each random variable here is a word,
taking value in a fixed,
in a vocabulary of finite size.
The size usually in practice ranges
from 10,000 to 50,000 or something.
And here are some examples.
You might notice there's some special word called EOS here,
which means end of sentence,
which is only like a special token or word
we use to pad sentences of different length
to the maximum length.
So we have this like joined distribution well defined.
So we collect a lot of text of this form,
sentence, fragments, paragraph, fragments,
and we train this joined distribution
by maximizing a lot of likelihood, a very, very simple idea.
So what about the architecture
of this distribution of our model?
Well, like the most popular ones, like GPT,
they are autoregressive models.
Well, so by the chain rule of probability,
we can decompose our joined distribution this way.
Represented as a graphical model
is basically for each random variable,
we have arrows going from all the previous random variables.
From a generative point of view,
it's basically we start with,
I can't see my cursor.
So we start generating the first word
from this prior distribution.
Once we have the first word,
we move on to the second one,
conditioning on the first one,
and then we generate the third one,
conditioning on the first two words.
Very simple, okay.
By some classic results,
marginals for Bayesian networks
with these structures is intractable.
So before when we asked GPT to generate a sentence for us,
we are like talking to it as if it's human, right?
So the assumption is if our distribution over language
is a perfect distribution,
then we should be able to interact with it as a human.
When we're doing these kind of like prompting,
when we're talking to it,
most of the time we are actually trying to do conditioning.
So let's consider an even simpler example.
Suppose in our autoregressive generation,
we have generated the first three words, the weather is,
and our constraint is that we want the sentence
to contain the keyword winter.
Well, I mean, we want to write something
with the topic of winter, so that's our constraint.
What we really want is, okay,
so what language model gives us is the next token,
next word probability.
So basically given the weather is,
it might prefer whether like it's warm or cold,
while it's prior distribution
might want the weather to be warm.
But what we actually want,
but what we'll actually need
is this conditional next word probability, right?
We want our sentence to satisfy the constraint,
wants to contain the winter,
and well, intuitively,
if we want the winter to be in the sentence,
weather is more likely to be code, okay?
But this is intractable for those autoregressive models.
So why is it intractable?
Let's go more into the details.
So by Bayes' rule, this conditional probability
can be decomposed into these two terms.
The next term here is very simple,
it's just a language model next word probability.
But the first term here is actually a marginal probability.
So it's basically the marginal probability
over all possible suffixes that we could generate
that contains winter, okay?
So this is intractable for autoregressive models,
but we have PCs, right?
PCs are tractable, at least,
we know how to compute marginal probabilities with them.
So why don't we just approximate this term here
with probabilistic circuits?
And we refer to our pipeline as gelato,
in generating language with tractable constraints.
Okay, so how do we do this?
So the first step is that we pick our favorite PC
for sequential modeling, which is a hidden markup model,
the simplest we can have.
We have, on the other hand,
we have our pre-trained language model,
we wanna control or guide.
We sample a lot of data from our language model unconditionally,
and then we train our hidden markup model on these data
with maximum likelihood estimation.
So effectively minimizing the KL divergence
between the two joint distributions, okay?
And our assumption is that if the joint distributions
are close enough, then all of the marginal distributions
and conditional distributions are gonna be similar.
Okay, any questions?
Okay.
So the intuition is that we have a black box
autoregressive model, and we train some sort of
white box PC, so we can kind of use as a
representative of the black box, okay?
Okay, so we don't wanna, since this is the workshop
for circuits and logic, let's represent HMMs as PCs.
So here is the graphical model version of PC.
The Zs here are the hidden variables, the latent variables.
And the X here are the observed variables, or the words.
And let me use the white board to show you how to do this.
Okay, so one assumption we have is that our hidden states,
our latent variables, are discrete variables
taking values from one to H, and H is our hidden state.
Any questions?
Okay, so we start from the very beginning.
In an HMM, we start with the initial state, Z1,
and it has H choices.
So we want these eight product nodes to be
representing the probability, can people see it actually?
So representing the probability of X1 to N
conditioning on Z1 equals to I.
So I would be the hidden state, so I will,
this will be one, two, and H, okay?
And the edge will have weights of the prior distribution
on Z1.
Okay, I see nodding, I see confusing faces,
but I'll move on whatever.
Okay, so it'll be clear later.
So given, so by the Markov property, given Z1, X1,
and so basically given Z1, this part and the remaining part
will be independent.
So let's deal with X1 first.
We have some input distributions.
And this input distribution will be representing
the probability, the emission probability,
basically X1 given Z1.
So basically here, we're in the states of Z1 equals to one,
and this will represent the probability of X1
given at Z1 equals to one.
I see more nodding faces now.
And we want this part to describe the remaining distribution,
so we have some nodes here.
And they will represent the distribution,
the distribution of P of X2 to N
given Z1 equals to I, okay?
Corresponding to the inputs.
And so we proceed to the next layer.
And this part will be the transition probabilities,
will be the transition probability of Z2 equals to J
given Z1 equals to I.
And these will recursively,
we do this construction recursively,
will be representing the probability
from X2 to N conditioning on Z2 equals to I.
Okay, does that make sense?
So we just proceed.
So it's like Antonio and Robert
was trying to make the point,
this is a tensorized layer representation of a PC.
Let's move on.
Okay, so now we have our...
Oh, sorry.
So let's move on.
So we have our circuit representing the distribution
over text.
Then we need to answer the query.
We need to encode the logical constraint
as a logical circuit.
Let's go back to the constraint
we have in the very beginning.
We want Frisbee, dog and dog.
Frisbee, cotton dog to appear in the given order.
This is like a naive way to represent this constraint.
Basically the IJK here are enumerating all possible positions
of these three words.
And we take a conjunction whenever we know the positions.
Any questions?
Okay, okay.
But this is not really ideal.
We can directly convert it into a PC
that represents the constraint.
What are the problems?
Do people see that?
I'll give a hint.
Complexity.
What's the complexity of this?
What's the size of this DNF?
Yes, cubic.
And to be more precise, it's n choose the number of keywords.
Well, I mean, we can do it like cubic,
but suppose we have five or 10 keywords.
We can no longer take that complexity.
And the other problem is more subtle,
which is that this DNF is not deterministic.
So we know that we can multiply circuits
when they are compatible
or when they are structured decomposable
with respect to the same v-tree.
But here we want exact conditioning,
so we actually need to make sure that our circuit
represents a uniform distribution
over the support specified by this DNF.
And in general, it is sharply hard to do model counting
to normalize everything.
Okay, I'll go into the details on the board.
Yes.
Yes, I'll talk about it on the board.
I'll move to the other side.
Okay, so for the first question,
why does it need to be deterministic?
It's because, okay, suppose we have a very...
Okay, so suppose we have a very simple distribution,
X1, X2.
X1, X2.
We have some weights.
Okay, we have some...
So this is a distribution over two random variables.
Okay, and this is its probability mass function.
So when we are conditioning,
we are actually selecting all the terms
that satisfy our constraint
and zeroing out everything else, right?
So if we want to do that with circuit multiplication,
suppose our constraint, our support,
would be something like X1, X2, and X1 bar, X2, okay?
So the constraint circuit
should be a uniform distribution over its support.
Otherwise, it messes up with the original weights, right?
Does that make sense?
Okay, so however...
Okay, let me...
So this will be 0.5, X1, X2 plus 0.5, X1 bar.
Well, I mean, suppose...
And we have a bunch of zeros,
and we multiply these two circuits point-wise,
and we kind...
We keep these two terms and erase these two terms, right?
But we want W and W3 to be proportional to each other,
like the ratio should stay the same.
So this circuit must be a uniform distribution
over the support of constraint, okay?
So given a logical circuit,
how do we convert it into a PC
that represents a uniform distribution over the support?
To do that, we need to do model counting.
But model counting in general is hard
if determinism is missing.
So this one won't work.
Does that make sense?
Yes, no?
Robert?
Isn't that part of the new mistake?
Oh, yeah, so it's a very subtle, subtle thing.
So...
Well, we only require Frisbee, Caught, and Dog
to appear in some positions,
but we do not limit that they have to appear exactly once.
So you can totally have Frisbee, Frisbee, Caught,
Caught, Dog, Dog, something like this.
So we have two sub-sequences.
We have a lot of sub-sequences, right?
So this is position one, two, three, four, five, six.
So when i, j, k are one, three, five,
it satisfies the clauses, right?
One of them.
Let me choose a different way.
So basically, for this huge disjunction,
for each instantiation to the variable,
we only satisfy one of the clauses, right?
That's determinism.
Yes.
And suppose i, j, k are one, three, five.
Does this instance satisfy that?
Yes, it does, right?
But when i, j, k are one, four, five,
it also satisfies that.
So it's not...
Okay.
Okay, so now let's construct
a deterministic circuit representing this constraint.
The idea is similar to deterministic finite automata.
Basically, you track how many words have you included,
how many words have you used,
which state are you in satisfying the constraint?
So we want to construct these sub-formulas,
phi, t, say, cot and dog,
this sub-formula representing the constraint
that cot and dog
appears in xt, xt plus one.
So how do we construct this sub-circuit
or this sub-formula?
Oh, it's pretty simple.
It's a sum node.
Consider two different cases.
One of the cases that xt is cot.
And the other case is that xt is not cot.
Does this make sense?
Okay, so if xt is cot,
then we have, like, stepped one...
we have made one step towards satisfying the constraint.
So we can reduce it to t plus one, dog,
and suppose xt is not cot,
we're in the same state.
So we reduce it to t plus one, dog, and dog.
Okay.
Is that clear?
Suppose we have constructed phi
for all time step greater than t,
then we can construct all the phi's for t.
Right?
Okay, so it's a recursive algorithm.
What's the base case?
Oh, what's the base case?
Yes, that's very interesting.
Okay, I'm going to erase this.
So you just cot and dog in the part?
So, yeah, I mean, we can make it even simpler.
I'm lazy, so I want to make things simpler.
So suppose in the nth position,
we only have one word, right?
So phi n...
Okay, what is this formula?
It means that from xn to xn,
we need to have cot and dog.
So this is false, right?
Zero.
What if we only have dog?
So this is true only if xn equals dog.
Okay, so, yeah, those are the base cases.
Okay, so how many...
So what's the size of the circuit?
So at each time step,
we need phi t.
Maybe we have already satisfied the constraint,
so something empty, right?
We need phi t.
We have one word left.
Phi t, we have cot and dog.
We have two word left.
And we have generated none of them yet.
Okay, so at each time step,
we have four sum nodes or four more nodes
representing all these four cases.
And the size, the number of nodes in the circuit
would be four times.
Okay, does that make sense?
And you can notice that when we are constructing this circuit,
we are always conditioning on xt,
the current variable, which is very similar to HMM.
It generates one variable at a time, one word at a time.
So it is compatible with HMM.
And also it's deterministic.
And we can do the apply operation,
the product operation, layer-wise.
So the size of the eventual circuit would be four...
So originally, so originally,
here for each layer, we have H nodes.
Now we...
Here for each layer, we have four nodes.
So eventually, we have four H nodes.
Just four each layer.
Acceptable.
Okay, does that make sense?
So we can still generate this.
We can generate this even further.
Well, some time, well, one simple generalization is,
you might think it doesn't have to be cot, right?
It can be cot, or either catch, right?
Or catches.
They all kind of represent the same concept.
So in the construction, we can modify our original circuit a little bit.
Before it was xt was cot,
we can replace it with another OR node,
enumerating cot, catch, and catches.
And the circuit size stays roughly the same.
And there are many other things we can do.
We can also say, oh, we don't need them to be in the same order,
in a fixed order.
We want them to be like in arbitrary order.
We don't care about the order.
How do we do that?
So in this case, we have four states, right?
Basically enumerating all suffixes of these three keywords.
In that case, we would have all subsets of these three keywords.
So in that way, the complexity would be going from four,
we have three keywords,
and this is four, two, two to the three.
That's there.
Still acceptable.
Two to the n is not really,
say we have 10 keywords, two to the n is just 1024.
It's still doable in practice.
Yes.
So it seems that you can do any kind of regular expression constraint,
but you're focusing here on language is a fixed length, right?
Because you have a fixed number of random variables.
So is that the kind of equivalence of what you're saying?
Yes, that's a very, very good point.
So fixed length here is a very essential assumption.
So suppose we have a distribution over language of arbitrary length,
then applying the constraint could be hard to define.
So basically these three words,
they can appear in arbitrarily far positions,
and we need probably to take like an infinite sum
to define this conditional probability.
But this is not terrible in practice,
because in practice, even for models like GPT,
they have a finite sequence length.
Yes, that's a very good point.
Okay, I'm trying to be fast.
Well, I mean, there are variations of constraints we have talked about.
And okay, so now we have our...
Now we have our probabilistic circuit representing the distribution,
and we have our constraint circuit,
and we'll close them to take the product.
So we have our constraint circuit now.
We can compute the probabilities we need.
So the step two is very simple.
So this is the original base decomposition
of the conditional probability we have.
So this term here is intractable.
So we secretly replace the subscript of lm with hmm,
the lm window is,
and we define this conditional distribution,
the gelato distribution,
and we can compute this with the linear pass of the circuit.
Okay, so what are the advantages?
So number one, by definition,
the constraint alpha is guaranteed to be satisfied.
So finally, other than compared to all the other methods,
we have 100% reliable thing we can trust.
And number two is that the training of this hmm
does not depend on the constraint.
So basically once we have this hmm trained,
we can use it to enforce whatever constraint we want.
So maybe today I want to write something
using some keywords and key concepts,
and tomorrow I feel like this language model
is like using a lot of inappropriate languages,
and we can detoxify it by specifying a list of bad words
that it cannot use.
And all the same model.
And all the constraints are only enforced at inference time.
Yes.
What kind of constraints can be represented
as the composable pieces?
Yes, that's a very, very good question.
That's actually one of the main reason
in presenting this work here,
because my feeling is that though people have been studying
probabilistic queries like marginal map, marginal,
and all these kinds of stuff extensively,
but in practice we really care about
some more complicated, less generic ones.
And whether is there a language to define
or to describe their tractability
is like kind of missing from the literature.
But I could relate this to like say
there's some work on compiling DFAs to circuits.
I think that could be something as a starting point to look at.
I'm not sure if that answers your question.
I don't have an answer for my answers.
Okay.
Okay, so experiments and benchmarks.
So we evaluate our method on this common sense generation,
common gem benchmark.
Well, it's very similar to the example I gave you.
It gives you a bunch of keywords.
And each example comes with a bunch of like gold sentences.
And you want to generate something
that looks similar to the gold sentences using unique keywords.
And here it's like the most general case.
So basically these keywords, they can appear in any order
and they can appear in any form of their inflections.
Okay, so this is the distance.
And it's a gigantic table.
The numbers themselves are not that important.
So one thing to note that is compared to all the other baselines,
our method, Gelato, achieves a state-of-the-art performance
with respect to basically all metrics.
So these metrics here, Rouge L, Blue Four Ciders, Spice State,
there are just some like standard NLP metrics
that people use to evaluate the quality of your text.
So basically you have a gold sentence
and you have your generated sentence.
They compute somehow the NBrem overlap to measure the quality.
But the other thing is that all of the previous method
cannot achieve 100% constraint satisfiability,
but ours does in practice as well.
Well, there is this one baseline that they achieve 100% accuracy as well,
but they kind of did it by starting from the keywords.
So they're always going to be there.
And you can see their generation quality is really poor.
We also conducted some sort of human evaluation.
Okay, I guess.
Yes?
Which language model does that mean?
Oh, the language model, we use GPT2, GPT2 large.
Yeah, and all of the baseline state, they use GPT2 large.
And in case people don't really trust these automatic evaluation metrics,
we also conducted human evaluation.
And you can see that our model performs much better than previous state of art.
Okay.
Are the guys significant?
I mean, they're pretty close.
Yeah, so basically the...
I'm not sure if you can see it clearly,
but the bold-faced ones are statistically equivalent.
So these ones, these two are statistically equivalent.
This one is statistically significant.
And we're looking at, when defibrating, what's the number here?
Oh, yeah, so basically we provide the annotator some sentence
and we provide description of one of each of the aspects.
So basically, concept means that does it use all the concepts naturally?
Plausibility means that is the sentence like a plausible sentence describing a realistic scene
and quality is basically fluency, grammar and stuff.
Overall is the like another...
How do you feel about the sentence?
And the numbers are from one, two, three.
One, two, three, oh, yeah.
Okay.
Okay, so let's get back to the very first motivating example.
And you can see that gelato, we use our model to...
And it is actually able to get everything correct and generate a fluent sentence.
And another...
Okay, so we also found this one in one of the generated candidates.
A pair of Frisbee players are caught in a dogfight,
which is not like the thing that most people would like to think of.
So it also shows some sort of creativity here.
Okay, that's my talk.
Please ask questions if you have, and otherwise we can go to lunch.
Any more questions?
Thanks for your talk.
If I understand right, you said that to generate every word
to do a linear pass over the circuit.
So how much is this overhead compared to the kind of latency from the language model?
Okay, I like that question.
So we don't really need to take a pass over the whole circuit.
With some caching, we can do like constant time.
So basically to generate each word, the cost is like constant time.
It's like a pass through one layer.
And in practice, how fast is it?
Actually, so...
I don't have a table here, but we have a table in the paper.
So if, say, generating a sentence with five keywords,
a GPT-2 large would take around 20 seconds,
and our method is like 100-ish seconds.
So it's not terrible.
And one of the baselines here, which was actually like the best paper award
at one of the top NLP conferences, they use search-based method.
And for them to generate a sentence, they take like 700 seconds, 800 seconds.
Because they're search-based, so when the search-based gets large,
their method shows like a bottleneck.
Thanks.
Can you also give an idea about the time required to derive the hidden Markov model?
Yeah, so I do have the time for that.
So our hidden Markov model has like 4,000 states,
and the emission is 50,000.
We train them with the juice framework that we develop there a lot.
The training takes like 20 hours.
So we sample about 200...
We sample 8 million sentences from GPT,
and we train them for 40 epochs.
20 hours.
What's the number of parameters?
Number of parameters?
Of HMM?
Yeah, so HMM is 4,000 hidden states and 50,000 emissions.
So mainly the parameters are centered on the emission table.
So it's like 4,000 times 50,000.
I couldn't do that in my head.
So those are the unique parameters, right?
Yes.
So in the PC, you kind of roll out all the parameters at all positions.
Controlable generation is huge, right?
So this is great.
I mean, and I probably understand you gave a logical perspective for this audience,
but a reg-x is a much more natural sort of control structure.
So presumably you can handle any reg-x compilation
given the DFA interpretation, right?
Yes.
I think that's a very important follow-up.
We are kind of looking at considering like compiling DFA's to circuits
and kind of automate all these processes.
Yeah, so ACLs in January, you submit there.
I mean, I think they'll love it.
Your initial example, you wanted a winter tree,
you wanted the presence of winter to select for warm, right?
Which is more of a natural language entailment.
And I don't think you handle that, right?
You handle variations and factual variations because you actually coded it.
No, I think we can totally take, do that winter example.
So basically, let me go back.
It seems like you encoded the specific variations that you were allowing, right?
Oh, you mean like so we can have winter, winters,
but like maybe if the winter, the word is not explicitly mentioned,
you cannot do that.
That's what I understand from your formalism, right?
If it's in your ore, you'll generate it.
If it's not, you won't capture the entailment.
No.
Yeah.
I mean, so basically, so that's the other thing.
So sometimes people, or most of the time,
people want to have like kind of soft control.
They don't, they can't really write their constraint in a logical form.
For example, toxic, like how can you tell a sentence is toxic?
Yeah.
But there are many ways to approximate it.
One of the ways is we have like a long list of phrases or words
that you're now allowed to use.
Yeah.
In a toxic sentence and we can basically just write down a logical formula,
approximating that toxicity constraint.
Yeah.
I mean, there are also plug and play generation tricks
that are actually quite interesting.
I think that.
So, so basically there, there is a, of course, there's naive way to do it.
You can say whenever I encounter one of the word in the list,
I just removed like prevented from generating,
prevented from being sampled.
But that's not exactly what we're trying to do probabilistically, right?
Sure.
I just maybe look in the plug and play control generation.
It's actually a paper.
Yes.
So where I think they're doing much more than that, right?
Well, so they're, yeah.
So it's like.
They're also modifying the, the, the posterior selection.
So, so if I'm not wrong, if I remember this correctly,
they're basically trying to train some sort of, I mean.
They, they use a classifier, right?
Yeah.
So basically kind of they're trying to train a newer model to approximate this.
Yeah.
Right.
So, but, but they're, so, yeah.
So their methods has, has like two disadvantages compared to our advantages.
So one of them is that they cannot guarantee that this is,
Right.
100% like logically.
Right.
That's not as viable.
And, and the other is that they have to retrain their model for all different constraints.
Yes.
Okay.
But suppose, suppose like in their training data,
they're only trained to kind of like satisfy.
They only have seen like using say less than 10 keywords to generate a sentence.
Right.
Yeah.
What if today I want to use 20 keywords,
they would not be able to generalize well to that one.
Right.
So, I mean, they're examples of toxicity, right?
So they, they pre train for toxicity and they can use that anywhere.
So there's some tasks that you're just going to use repeatedly.
I think I see a combination of what you're doing and what they're doing together,
which gives you this sort of entailment during the,
like this broader sense of satisfaction, right?
Which, which I think was an interesting motivation.
You didn't quite deliver on, but you could deliver on.
That's actually the current project we're working on.
So we're trying to combine the models that can handle like soft constraints with,
with our model too.
Okay.
Beautiful.
And finally, I mean, the reason large types of models work so well is just because of attention.
Right.
You know, when you're using a hidden Markov model, right,
you're losing the power of attention, but,
but I guess I'm, I'm, I'm looking at this and trying to commit myself that,
well, you get all the attention in the right hand part.
And then you just needed a little bit of bias of selection in the left hand part here.
And that's it.
And that's what's happening.
Yeah.
So basically intuitively, you can think of this part as only providing like guide and suggestions,
leading the model, leading GPT to satisfy the constraint.
So on this part, it's kind of responsible for fluency, grammar and everything.
Yeah.
Okay.
Beautiful work.
Thank you.
All right.
Thanks.
Maybe we should wrap up and thanks again.
And we can have lunch with the database folks and talk about generating SQL query.

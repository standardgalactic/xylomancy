mentioned, Gabe Grand, another student collaborator,
who is one of Jacob Andreas's students,
and Tondra Shren, a student also co-advised
by Josh and Vikash, as well as Noah Goodman,
my advisor Vikash and Jacob Andreas.
So our broader goal today in this talk
is actually going to be to reflect based on
many of the recent advances that we all know
of modeling natural language,
but I think also drawing on evidence from toolkits,
other toolkits in AI.
And evidence from cognitive science and from neuroscience
on kind of the broader spectrum of different ways
that we might think about building intelligent architectures
that use and produce and learn from language,
as well as more generally, I think what role language
might play or could play in a computational system
that we say thinks.
And obviously this is a question that many people
in this room, but many other people who probably
aren't in this room have thought about,
from philosophers of language to linguists and neuroscientists.
And we thought it actually might be a useful exercise
to kind of start by reflecting on the underlying answers
to this question that are or aren't suggested
by some of the most prominent directions
that we're taking in AI research right now.
And of course, one of the reasons why we're even asking
this question at this scale,
what is the role of language in intelligence
is in large part driven by this remarkable observation
that we all know about from just a few years ago,
which is, if you train these large
and specifically transformer based neural architectures
as language models, just to do this
expert prediction task, with enough language data,
they start to show behaviors that really suggest
that there's something more than language at play.
They look like they're thinking.
They can induce patterns from just a few examples in data,
or they can even read the definitions
of totally novel words like Zaka Tota in context
and produce realistic sentences that appear
as if they understand how to use those words immediately.
And so a lot of the excitement, I think it's fair to say,
around language models is that maybe for the first time,
we're seeing something that is offering a scalable route
towards implementing more generally intelligent architectures
just by directly scaling, or largely by directly scaling
the amount of language data
that they're being trained to predict.
All right, so what is the underlying idea here?
What does this have to say about language?
Well, I think it's fair to say that one
of the dominant hypotheses that's underlying
why we were even starting to see some of this behavior
rests on kind of a two-part idea.
One is something about the nature of language itself, right?
It's the suggestion that language is sufficiently diverse
and so broad, maybe because we suspect that humans
express so much of their thoughts
in such a diverse range of their thoughts and language
that being able to perfectly solve this task,
to be a perfect language model, or at least a really good one,
is essentially an AGI complete task,
and maybe also one that conveniently,
unlike other kinds of tasks,
like predicting all of the videos in the world,
we have maybe efficient architectures to do
and enough data to do.
And I think it's worth noting that this doesn't actually
really have to be a particularly strong hypothesis
about what it is computationally that thinking looks like,
or even how language necessarily is implicated internally
inside the computational processes that we call thinking.
Rather, it's really just a hypothesis
about the nature of the language modeling task itself.
And I think what a lot of people in this room would agree now
is that, you know, while that might be true
about the language modeling task in principle,
much of the most exciting research that we're seeing
using LMS right now actually isn't predicated
on really the hope or the idea
that just by scaling to more and more data,
we're going to expect transformers trained
and used in this way to actually solve that task
or become perfect next-world prediction models.
And the intuition behind that,
which I think many people have pointed out,
comes both from the way in which transformers work, right?
There are autoregressive language models
that are doing a fixed, finite amount of internal computation
that only scales based on the previous linguistic context.
And it doesn't really make sense that, you know,
of course you can pose questions in language
like arbitrarily difficult math problems or planning problems
that intuitively require an amount of computation
whose complexity doesn't actually depend linearly
on the previous token context.
And I think that's been matched empirically
by lots of different observations
about which kinds of sentences are hard to complete
if you treat them as next-world prediction tasks
in this way, right?
Hard arbitrary math problems or planning problems like these.
Right, so some of, you know,
if you're thinking about the role of language,
why is language, why are language models so big right now
as, oh, well, language just has all the evidence necessary
to train something to think?
Like Leo said, that's not actually committing to any hypothesis
about sort of how language is used internally in thinking.
The transformers' computations are not necessarily
doing anything linguistic
as it's computing the distribution of the next word.
But more recently, we've seen people use language models
to solve these harder tasks by letting them think more.
And what letting a transformer think more means
is letting it generate more tokens.
So in this view, thinking doesn't necessarily emerge
just as predicting the next token.
Rather, thinking happens in language, right?
It happens by sort of producing a chain of thoughts
or using a scratchpad or deciding
that you're going to invoke a calculator
or write some code and execute the code.
And the sort of hypothesis embodied, I think,
by this line of work is that the role of language
and intelligence architecture is bigger.
That language or some kind of running monologue of language
is the central controller of thought.
And thinking is taking place primarily in language.
And what's maybe striking about these proposals
is that if you ask most cognitive scientists
or neuroscientists, they'd probably say,
this isn't the role that language plays
in relation to general intelligence in humans,
or at least it's not the dominant hypothesis.
Until just a few years ago,
this probably wouldn't have been the role
that many AI researchers would have necessarily
posited for language as the main controller
or the main substrate of a running monologue
that controls all thinking.
And so we thought we'd review some of the background
for what language seems to look like in humans
as a basis for informing how we might build architectures
that better capture those more human-like roles for language.
Yeah, and so I just want to be clear that
we're going to do kind of a quick background
on some of the neuroscientific and cognitive evidence
about what language might look like in people.
And our goal really isn't to push back in any way
against these other kinds of paradigms
for where language might fit in.
It's rather to present, you know,
at a mode where we're really thinking a lot
about scaling up language models
as one really dominant role,
what our other roots might be
because it doesn't seem like that really matches
a different prominent intelligent architecture
that's sitting here in the room today.
So one source of evidence for the role
that language appears to play in human cognition
comes from neuroimaging data, fMRI data,
suggesting how language is processed in the human brain.
And at this point, convergent imaging data
from many people, including Ed Fedorenko at MIT,
Standa Hain, and recent graduate students
in our department, Kyle Malweld and Anya Ivanova,
suggests that human brains have this language-specific network
that handles many of the tasks that we associate with language.
It's activated when people do tasks like listening to sentences
or reading them or speaking or writing words.
And this is not just an English-specific network.
The same general region is activated no matter what language
someone is speaking.
It even fires when they're producing constructed languages
for people who come to learn and become fluent
in languages like Dothraki or Klingon.
And right, what is it that this language network does?
Well, increasingly, one dominant hypothesis
is that it actually does do something
like next-word prediction.
And in fact, among many other kinds of alternative models,
like people were earlier for a long time
have been trying to do things like align words back to the brain.
Transformer architectures really do seem to be among the best models
that we have right now of the neural activity of this language network,
specifically when they're trained on next-word prediction tasks
and not other linguistic tasks like NOI.
But I think what we see in this neuroimaging evidence
also highlights the ways in which the role of this human language network
probably is not as the controller or the central seat of cognition.
It's selective for language, and it isn't involved
in many other kinds of thinking activities,
even ones that might seem to involve symbols,
like solving math problems or reasoning about logic
and physics and social reasoning.
Those invoke other regions of the brain,
and language interfaces modulate with them.
It can interface very generally with them,
but it doesn't seem to be where the bulk of thinking is taking place.
Oh, yes.
I just had a question about the next-word prediction findings.
Did you also try to say, like, math word prediction,
and this is actually worse than next-word?
Yeah, so I think they do compare or close tasks are one of the alternates
that they're looking for.
And also, yeah, people should feel free to just shout out
as we've been doing all along.
So also, right, actually you can see adult patients
that suffer strokes that only damage this area of their brain,
and we find that they can still think about,
well, they can't comprehend spoken language,
but they can still think about all these different kinds of tasks
printed in different mediums,
like they can draw physical inferences from videos that they're watching.
And conversely, maybe most tantalizingly,
it's actually also possible to sustain localized damage
that leaves you still able to produce these long,
very relatively fluent and quite syntactically coherent sentences,
almost as if they're maybe just like a very rudimentary
and local word-based language model,
while not really conditioning, meaningfully,
on what someone else is saying.
So they say, do you like it here in Kansas City?
And this person says, yes, I am.
Or really producing sentences that are obviously more globally meaningful
or goal-directed towards the question.
And I don't want us to overindex on those results,
but they kind of fit in with this more coherent picture.
And in many ways, I think this kind of recent evidence from neuroscience
actually supports the broader picture of human cognition
and the place of language in it,
that most cognitive scientists and many linguists have already believed
based on what we see and have come to learn through developmental science,
studying how kids think,
and where language seems to fit into that picture.
So a broad body of work at this point
suggests that infants, well before they learn language of any kind
or are speaking already independently,
perform many of the tasks I think we associate with coherent thinking,
from reasoning about physics, to planning,
to drawing causal and probabilistic inferences.
And language seems to be something that humans learn
and scaffold on top of this structured basis for thinking.
We take much less input data to learn to produce fluent language
than a large language model.
And humans that actually aren't exposed to any language input at all.
So famously there are these deaf children in Nicaragua
who grow up in isolated hearing families,
actually spontaneously come to produce early sign languages
as a product of trying to communicate events
that bear many of the hallmarks of the syntax that we associate
of our own natural languages,
like distinguishing between the subject who's punching
and the person who's getting punched.
Suggesting that in many ways the language we produce
somehow externalizes the underlying structure of the thought that we already have.
And of course there are many other animals that we associate
as being intelligent in some way
and that have been modeled using the models
that we associate with probabilistic reasoning and planning
that don't use language at all.
So it doesn't feel like language is by any means necessary
for what we think of as thought.
So formalizing this picture of an intelligent system,
one that's shared across animals and non-linguistic infants,
and maybe captures some of the computations in many of those thinking tasks
that don't involve the language network in our brains,
is both the central goal of a lot of cognitive science
and has been a historic motivation for diverse fields within AI
before the current sort of LLM centric moment.
So, you know, if you open up Russell and Norvig's textbook,
an AI modern approach, you'll see an equation like this
that's supposed to sort of capture what we believe
about how intelligence works computationally
or a computational model of intelligence.
Where the idea is that an intelligent agent is one that has sort of structured
internal world models that can be updated based on observations of the world
and in which we can sort of predict the results of our actions,
that the agent has some sort of values or desires
that are captured in some kind of utility function,
that the agent can do probabilistic reasoning over its observations
and the possible actions it might take
and what their expected utilities are,
and that it can do some kind of planning to optimize the value of the actions.
Yeah.
Is this a normal state of statement or a specific statement?
Like, is it the case that like we are defining intelligent agent
to be having this kind of property
or it's like if we want to build intelligent architecture,
we should have these?
Yeah, so I think some, it's a great question.
I think some of the work definitely is coming at it from a normative perspective.
You know, this is like a definition of what rational action might look like, right?
But I think there is a lot of work in computational cognitive science
that sort of in various domains has collected a lot of evidence
that people either like recognize this as normative
or behave like this in computation,
you know, according to the limits of what they can do computationally.
So, you know, in this architecture,
the controller for thinking would be some kind of system
that's capable of general world modeling and probabilistic reasoning
or planning and utility maximization
rather than sort of a primarily linguistic system necessarily.
Now, largely attempts in artificial intelligence
to directly implement this equation by building out components
that sort of map on to the different elements of this equation
have struggled to match the computational efficiency
and the generality of natural intelligence.
But we have seen over the past couple of decades or so
the emergence of this new class of tools
called probabilistic programming languages
for building software that can solve probabilistic reasoning tasks
at least in limited domains.
And these languages have been applied to create systems
that do everything from perceiving cluttered 3D scenes
more accurately sometimes than object detector neural nets
to interpreting and predicting economic trends
more accurately than leading industry solutions
like Facebook neural profit.
And these probabilistic programming systems
are enabling these applications with two key technical features.
They feature modeling languages that let users express
complicated probabilistic models of the world as programs
making it easy to write down rich probabilistic models
that are defined in terms of expressive program-like components.
So, for example, the probabilistic models behind these example applications
are defined in terms of 3D renderers, symbolic planners,
scientific simulators, and so on.
And the second thing that probabilistic programming systems do
is that they automate various mathematical operations on these models
and that automation makes it possible for users
to concisely implement sophisticated algorithms for probabilistic reasoning
such as the variety of Monte Carlo and variational inference algorithms
that power these example applications.
One way of thinking about these tools is as, you know,
kind of like PyTorch or TensorFlow,
but instead of writing differentiable models and doing optimization,
you're writing probabilistic models
and doing various probabilistic reasoning tasks.
So far, these AI engineering efforts haven't really made contact
with the sort of language model part of AI.
Much like there hasn't yet been a concerted effort
to take this classical picture of intelligence architecture
and figure out how language might be richly integrated into it.
So, for the rest of the talk today,
we're going to explore two different approaches for thinking about
where language might fit into this picture and approach.
So, broadly, Leo's going to begin by talking about
how an intelligent agent might incorporate lots of externally produced languages,
you know, explanations, observations, questions,
how an agent can incorporate all of those forms of language
into the way that it updates its beliefs and decides how to act in the world.
Then I'll talk a bit about systems that leverage language
as a tool for thinking within its models of the world
or its probabilistic reasoning algorithms.
And each of these corresponds to very recent preprints of work.
So, I also want to give a disclaimer.
We should really emphasize that both are preliminary proposals
and we're giving a very kind of speculative talk,
not scale architectural solutions here.
Cool, right.
So, the first portion of this talk is summarizing work.
If you want to read more, you can find this very long paper
from word models to world models.
That's up in archive and this is primarily done
with another student who's not here today, Gabe Grant.
And as I just mentioned, the context for this work
is thinking about how we can build systems
that capture the breadth with which all the external language we hear
seems to inform at least our human thinking.
And what's clear, right, is that this role seems to be very broad.
If we take the basic model of an agent with beliefs and goals,
well, it seems like there's an incredibly diverse range of situations
in which we can update our beliefs about a situation
that we express in language,
or in which the goals of our thought are to answer questions
that we specify linguistically.
And these might implicate our knowledge of other agents
drawing on our intuitive psychology
to reason about what they think and what they'll do.
Or, you know, we can talk about the physical world around us,
what we perceive and ask questions that require us to reason about
what we see and draw on our physical intuitions.
And of course, one of the remarkable things
where we're also excited about language
is that it doesn't just draw on what we already know,
it's this means by which humans seem to learn
and pass on profoundly new knowledge,
whether that's new concepts
that we define in words or learn from words
to really profound new theories and conceptual systems, right?
Much of what we know about the world,
the fact that there are wars, what wars are,
legal systems, sciences,
comes from information that it feels that we acquire
from language in some way.
But how do we do that?
Well, I think one long-standing lens
for thinking about language
that's kind of persisted before
the LLM-based moment
is that what language is
is this external symbolic medium
for communicating human thoughts.
And the way that it does that
is because there's some kind of general mapping function
from our internal representations of thought
into this external symbol system.
That's language.
And so, in this kind of older framework,
what it means to understand language
or make meaning means mapping back
from external sentences that we hear
into structured internal representations.
And what we explore in this paper
is a general proposal
that casts the meanings of language
as these mappings
or probabilistic distributions
over expressions in a probabilistic programming language.
And I'm going to come back after some concrete examples
to how I think this relates to the conceptual world semantics,
that Steve articulated in the first talk,
because I think there are actually some really deep connections here,
the ways in which that might be one way
to formalize or enrich some of those ideas.
And this architecture,
this proposal here,
also suggests how language can be integrated
into a more general architecture,
because it's one that already starts out with,
as Alex mentioned,
some kind of existing internal modeling language
whose goal is to represent the world probabilistically,
query those models,
and specify what it means
to draw coherent inferences over them.
And it also suggests, I think, a framework
for formally modeling the content
of different kinds of sentences in language
and programming expressions that they might map into.
So,
and so in this paper,
we begin by exploring how this proposal
might be instantiated with respect to
a bunch of different domains of reasoning,
sorry,
including general probabilistic reasoning,
but also reasoning about relations
or physics and social situations.
And in all of these,
we're going to propose that we might think
about the language that communicates
general conceptual knowledge about the world, definitions,
or causal knowledge
as constructing these probabilistic
expressions that are those that build up
probabilistic generative models.
And then, in this framework, observations
in the language, like
there is at least one red mug in the scene,
or Charlie is Dana's grandfather,
construct formal conditioning
statements which update the state
of this probabilistic model,
and then questions
map into query expressions that specify
the formal target of probabilistic inference
with respect to a model.
And in this framework,
we're thinking essentially cast as probabilistic reasoning.
We suggest that
another way that we can think about the role
of a language model, one that's much smaller,
like the language network in the brain,
is actually as a means of instantiating
this meaning function in a way that
we've really never had before
to protect these kinds of context
specific and previous discourse
conditioned mappings from sentences
in natural language into distributions
over expressions that convey meaning
in a probabilistic programming language.
And so, as a part of
this long-running disclaimer, what I'm going to be showing
is a really minimal implementation of this framework,
but really intended as a pointer
to different directions with which we might scale this approach
to implement a more general interface between language
and a range of different core cognitive domains.
So, just to be clear, concretely,
in the examples that you see next,
our meaning function is going to be implemented
using codex, right?
An openAM model much smaller
than the state-of-the-art right now,
that's trained to learn joint distributions over language and code.
And the probabilistic programming language
we're going to show is church,
which is this very simple probabilistic programming language
that supports kind of
very general sample-based inference procedures.
And our goal is to demonstrate how this framework
might broadly interface between language
and a bunch of different core cognitive domains.
So, first,
to illustrate the basic sense in which
a proposal like this might allow language
to update an agent's beliefs
and query a world model,
I'm going to begin with a really simple toy example
that actually draws on a bunch of prior cognitive science experiments
in which real people were asked
to draw various inferences
about which teams of players
might win different games of tug-of-war
based on the games that you'd previously seen players
win or lose.
And so, this is older work from Josh's group
that demonstrated, I think,
the sense in which this normative model,
a probabilistic inference,
actually, in many ways, predicts the actual behaviors
and predictions made by humans
using a very general probabilistic model
of the mechanics of this tug-of-war game.
And our goal here
is to show how our framework
we can implement an interface between natural language
and all of the core examples of this older experiment.
So, just to go through here,
I think what you're seeing,
in this little toy example,
the world model that's being defined on the screen
is capturing the basic causal relationship
by which properties of different
human players
might influence the outcomes of different tournaments
that they play in tug-of-war.
So, for instance,
here, we're modeling players
as having some internal inherent
strength value
where strength varies approximately normally
but as this unobserved latent variable
over different kinds of players.
And we also think of players as having
some kind of internal laziness value
which represents the percentage of the time
that they actually don't act according to their underlying strength.
And
how do these variables determine
the outcomes that we observe of given games
of tug-of-war?
Well, the strength of a whole team of players
depends on the cumulative sum of its
player strengths.
But if a player is deciding to be lazy in this game,
they might not pull as hard as they could.
And whichever team pulls with the most strength
in a given match is going to win that match, yeah?
Did you design the primitives of strengths
and laziness or did you come up with the primitives themselves?
So in this one, we're looking
at a model that's derived from the older work,
so these are designed, but yes, that's
later in this work, we're going to show some examples of how
you can learn this kind of model from someone just talking
about it in language like the definition that I just gave.
Right, and so again,
this is a really simple example, but I think
also one that actually captures a surprising
amount of the basic causal knowledge that people
have if you tell them that you're going to be
listening to tug-of-war games, but
sometimes people can be lazy and not pull as hard
as they could. So how do we go about
relating language in this domain?
Right, well
one means by which we can
induce a simple notion of a
meaning function that actually fits the definition
we just gave is by conditioning
a language model both on this
context-specific generative world model
and on a few examples showing
how language is mapped into
sampled probabilistic programming expressions
in this domain.
And what we've done now, right,
is effectively induce this kind
of situation-specific contextual
mapping from arbitrary
new sentences to expressions that
conditions both on the general
prior distribution that Codex is
over language and code
and this kind of specific discourse thinking
context of how language is
being used in this situation.
And there are clearly other ways to do this,
some of which we'll talk about later, but
we're using this example to illustrate
just how much you might be able to do with this kind of
minimal implementation, a notion of a model
that translates between language to code.
Right, so
what kinds of language might we say here
and how might we think about them in relation to probabilistic
programming expressions? Well, a general
proposition like
Josh won against Leo gets
translated into or might
map, we might think of mapping or meaning
a conditioned statement
an observation that Josh won against Leo.
If we
make subsequent observations like then Josh went
on to claim victory against Alex
we can continue to kind of
generally use this meaning function
that we've induced to turn
that into a probabilistic programming language
that captures the fact that Josh won against Alex.
If we then say that even working together
as a team Leo and Alex
still couldn't beat Josh in this game of
Tech of War
at this point, if we want to
answer a query like, okay wait
how strong is Josh?
What we think of as
thinking in this situation
is actually
sampling from the posterior
over possible worlds from the generative model
that we just defined
subject to the observations that we've just made
and indeed that means that the meaning
of a sentence like how strong is Josh
is really a
structured probabilistic inference query.
What is the latent variable of
that is Josh's strength and what we see here
is that given his track record
all these people that he's beating even playing together
our inference is that Josh is likely
a good bit stronger than average
and that also means coherently
we might expect that a priori
a new player we've never seen, like Gabe
is going to be unlikely to beat him.
So if we ask what are the odds of Gabe
beating Josh, we see that we think
it's somewhat unlikely.
So on the how strong is Josh
it seems like there's an interesting thing here
where there's also an implicit question of
what the word strong means of this
context
and
it's like not a number
it's kind of some like
confident adjective, it's probably not
intersective.
So I guess is that something you think
of as a framework or should I just be kind of like
ignoring this sort of issue?
Yeah, well okay, so I think there's a number
of ways that we can think about that.
I mean, so right, so
the one
sense we could say is like right how strong is Josh
isn't the answer to how strong of Josh isn't
a number rather it's kind of this
distribution over this posterior
distribution of various underlying strength
values that we
currently might infer that Josh
has with respect to the general model that we have.
I think another
kind of popular definition of various
uncertain adjectives like over like strong
right is that you have
some internal threshold value or the
person speaking has some kind of internal threshold
value that you
must kind of jointly infer
with respect to the context in what you've seen
and some of the examples that
I'll actually give later so right there's
kind of a long line of work in linguistics
including some work that treats that as like a
pragmatic inference. I think some of the
interesting work that we'll show a little bit
later is that there are some ways
in which you might think of this mapping function
as actually being a general one that
includes that notion of pragmatic
inference and also I think captures the sense
in which you know if you continually
are you really doing this kind of
pragmatic inference all the time
or do you actually in many general settings
like talking about the strengths of people
actually have some kind of cached older
notion of strength that you can draw in
and I think actually this notion of large
language models as just being this
learned mapping function from language
into expressions and code can also
capture the sense in which that knowledge is
amortized away and you might not be
having that inference. Yeah Chris
Thank you.
Sort of just denying or ignoring
what makes people
so excited about large
language models
in their
meaning representation
and ability
to do inference
I mean because okay you've got
sort of cooler probabilistic programming
language on the right hand side
but in some sense
the picture is still
this is semantic parsing
like it was 2010 to 2015
and
yes you're using a large language
model but you're not actually
using the excitement
of a large language model as a
representation system. Yeah and I think
so probably each of us
would have different answers to this but part of what
we're hoping to paint out over the course of this talk
is I think some of the ways in which
actually
of course no one wants to say
we're going to go back to kind of the brittleness
of semantic parsing but I think one thing that large
language models actually give us or one proposal
in this talk is that
there are some aspects
of the theory kind of the classic notion
of linguistics and certainly the classic notions
of semantic parsing that actually normatively
capture a lot of what we really might want
when we think about so one
answer for an AI system is well
yes
certainly we don't want to throw away
everything that we're learning from large language models
and I think one answer to that
is kind of the answer that I gave to Jacob
if we think about
not always in these
examples we're showing this very
direct system in which we always start
with language and we always map into some
sort of probabilistic priming expression and that's where all the thinking
happens and we might think
well that doesn't totally make sense because
there are lots of cases where as you're saying
we have every reason to believe
that large language models have learned
a lot of latent information
they certainly have a lot of latent conceptual
information and maybe to some
degree they can even perform certain kinds of limited
amortized inferences
or reused old inferences
that they've learned from what other people have had
and so in the second part of this talk we're going to
show different ways in which well this probabilistic
programming language itself doesn't necessarily
need to be something that's isolated
from what large language models have learned
it also can embed calls to
large language models within it to kind of
draw on that sort of knowledge
Haven't you gone back to the brittleness of semantic
parsing because you're doing this
translation
into
symbolic semantic representation
which
and is brittle in the same way
well right and also
so no I think I would say
I don't totally think
that the way in which
we're using it or the sense in which
or I think there are some ways in which
this kind of broader definition
in which you are saying well
the meaning of a
sentence in language isn't just
one probabilistic programming expression
that's what we're showing here
for pedagogical purposes
but you might say well okay
how are you going to obey kind of
the ambiguity of language
there are kinds of sentences that are
definitially ambiguous so one example
that we've looked at
our sentences in which you say something like
Josh beat Alex and Leo
right and you might ask well
that's kind of a classic syntactic construction
does that mean that
Josh beat Alex and Leo and they were playing on the same team
or Josh beat Alex
and then Josh went on to beat Leo
and what we see
are generally what you might say is
well the meaning of that sentence
actually shouldn't be picking one expression
or the other it should be
kind of the distribution over
those possible
parses and that distribution also
shouldn't just be something that we can determine
in this totally context and sensitive
way it should actually depend on all
the previous patterns in the discourse
so if someone's continually been
using this conjunctive and
to refer to
you know teams of players playing together
we should take that kind of discourse bias
into account and I think actually
this provides
or thinking of
large language models as kind of
generally having learned this broad
joint distribution but one that can be kind of
conditioned quite richly
both on
the content of this generative model
so you know it's not trying to come up
with a universal definition of strength
it's not even necessarily trying to come up
with a universal definition of any of these words
it's thinking about how they might map
contextually into the best possible
expression in the context of a particular
local
model built for a particular situation
I think is
obviously related to
but attempting to address some of the
the brittleness
challenges of semantic parsing
in the past I think another answer to this
right is that part of the problem
of semantic parsing previously
has been actually that the mapping functions have historically
been difficult to get
right whether you were thinking about those
as kind of old hard-coded
grammars or many of the attempts
to kind of learn these things
via very domain-specific supervision
so you want to have a semantic parser
for a particular robotics domain
you need a thousand examples
of sentences
about that particular robotics domain
paired with a thousand examples
of
programs that are operating on that particular
domain what we're seeing here
is I think something that says no
what it means to learn language generally
is to learn kind of this general mapping
right between language and some kind
of underlying representation
and also
right one reason why we might want
a system like this is because we want
to be able to condition coherently on information
that's not just coming from
language
and right we want to think about
how a general substrate in which
we the only yes
we might be told that Josh went against
Leo but we might also
be watching videos that give us
information about Josh's strength
that convey our observations we might also
have seen pictures
like the ones in the stimuli that we saw before
demonstrating the results
of previous outcomes of matches
and I think
one thing that suggests is we want
this kind of general
substrate
in which we can think about how those observations
including the observations from language
but without prioritizing
language in any way I think
are coherently considered
but
so I think it depends on
what part of
semantic parsing you
or I think the answer
to that depends on what part of semantic parsing
we think of as being the source
of the brittleness that caused
us to throw that paradigm
into question
and maybe I'll
just offer one more
perspective so one part of it is
what Leo is saying traditional semantic
parsing is brittle in two ways one is do you
have broad coverage of language that you can parse into
your system and two is like
how broad coverage are the
set of semantic queries that you can
actually answer and I think what you're pointing
out is this doesn't seem to address the second
source of brittleness which is that your
system can only answer certain things it can only reason about
certain things
brittleness of the
formal representation language
that you are using
that
large language model representations
so I think
my
sort of
take on that is
from a kind of AI engineering
perspective is
sort of a branching
in two directions one is I think
we have made some progress that
this is not really evoking
probably toward systems
that within restricted domains
can reason
coherently and probabilistically
about a wide range of queries so
we have systems
like this
inference QL system that uses non-parametric
base to analyze huge data tables
and come up with a model
of that system that
or of your data that can answer all sorts of
questions like
oh
you know
like which people in this data set are
like probably overpaid
given their experience or something like that
so in the same way that people
are kind of excited about using
language models to
parse into SQL
because so much data is
in SQL and it's a very
SQL is a very expressive language
for asking questions about that data
when we have a probabilistic
system like a good probabilistic
model of that data under the hood it enables
conversational patterns that are not enabled
when you have like SQL
as the database because we expect our
conversational partners to have coherent beliefs
about the world to update those beliefs in
response to new evidence that we give it
to be able to report
uncertainty
and make sort of modal judgments
and so one engineering path
is to take those kinds of systems
and
sort of build conversational interfaces to them
that behave more like
an intelligent person would behave and can draw
inferences that you might not draw
if you're just talking to a SQL back in
the other path that
we'll talk about in
the next part of the talk is
how can we use those representations that language models
have learned to make
the probabilistic inferences
more interesting and more robust, less brittle
without
totally embracing the other kind of brittleness
which is the kind of brittleness that language models seem to
have right now which is that they draw
that they don't really necessarily reason with coherent
probabilistic beliefs
so maybe, yeah, let's go
to that next part
okay
right
well, so in the interest
of time I'm actually going to like skip through
some of the rest of this example which I think is just
more of what you've seen but one
sense in which I think
maybe a third part of the answer to Chris
I would say is that
right, you know
yeah, I think
part of what this is trying to do is explore
some of the ways in which we might answer
without giving an answer
in all the ways in which
we might answer the ways in which language models themselves
are brittle with respect to
what we also want from a model of intelligence
we might suspect that when we
answer ask questions like this
really what we are trying to do is specify
some kind of normative
query that captures
formally a sense of well, we want
something like the posterior
with respect to some kind of internal model of the world
and, you know, this is kind of
the simplest means or this is a very
simple example of how we might formally
impose that kind of structure
but one that I think can be
elaborated on
depending on
the kinds of primitives and the ways in which
you're thinking about what it is that the probabilistic
programs can express, right
so one way in which we might
think about doing that
is by thinking about probabilistic
programs that themselves
have access to other
kinds of means of calling other different
mechanisms and cognition
right so I think I would draw a contrast
here between the notion
of the large language model as a controller
the one that's making the decisions
about when to write little snippets
of code and to execute them
when to call out to
little planners and incorporate them
or stuff like the mind's eye work
where there's a language model
it decides when to call out to a physics simulator
but the way it interprets
the outputs of that physics simulator
is to paste those back into
the language model context and try to
draw inferences on them in turn
rather in this kind of framework
what you can kind of see a
or the direction that this
framework would be pointing towards is to say
well
on the other hand we already have languages
that allow us to do things
like build expressive generative models
over three-dimensional scenes
that also capture things that we
might want only from perception
like knowledge about how the shapes of objects
tend to occlude each other
or incorporate rich models
of physics or that model
theory of mind as taking place
recursively in thinking about agents
who themselves have beliefs about their own
internal world models and are
actually choosing their actions as planners
and in this kind of framework
you can point the way
towards a kind of model that says
well how is it that I might
incorporate language into these kinds
of models sitting alongside
these other kind of observations that I might make
so how
might I think about the meaning of
images that I want to generate
that specify specific constraints
or
imagination or right
go ahead Jacob
so you were talking earlier
about having this meaning function
and I think also we're mentioning
something about
like codecs
in terms of the
questions so I'm just trying to understand
which of these is that
is that here or is the meaning function
when you come later
so that's maybe the first question
so are these statements actually just
being programmatically created from codecs
by prompting the text
yes that's right so by
meaning function in this
framework we say
well there's kind of two generalizations of a meaning function
there's a general joint
prior right that codecs is already
that it's learned between language and code and then there's
this kind of context specific
meaning function in the sense that it's
conditioned on whatever is in the
prompt the generative model and some examples
of how language
relates to expressions
that's a meaning function and yes all these
examples that you're seeing are one sample
from that distribution
and one of the things that I want
to point to here right is it does
I think it suggests a framework or another
means of thinking about what it means
for language to construct new concepts
from definitions or even come
to construct new role models from thinking like somebody in the beginning
asked right so
how for instance might we think about
enriching an existing structured
relational model with concepts
that we learn from language so for
example if we consider kind of a
formal model of
kinship relations
we might say
that well you know the generative
model of this domain is
itself represented as a probabilistic program
it captures
both the causal means by
which people
give rise to their children
and also the definitions
or one notion of the definitions
of what it means to be something like a sister or a father
with respect to this core
notion of how family trees come
to be and so if you take
this kind of general notion
of the meaning of language as
being the distribution over expressions
that it creates in a probabilistic programming language
you might start to think how we can
formally think about
relating definitions for
various kinds of relational terms
you know an uncle is the brother of one's
parent or the husband of one's aunt
a pibling is a gender neutral term
for an honor uncle that's the sibling of one's
parent
or this
relational notion
of a sister of one's father from a language
that's actually not found anywhere on the internet
and I think the core thing that we want to suggest
here right is why do we
even have definitions at all
well one
reason is or one
notion of what it even means to have learned the definition
of this term is that it should
drive coherently
all of the downstream inferences that you make
with that term and it should
graft onto the conceptual knowledge
that you already have and so you can think
about forming new sentences
directly that refer
to someone's paani
or one's pibling in this situation
and expecting them to draw both on
your existing conceptual knowledge
of what it even means to
have a family tree as well
as all the other conceptual terms for a
friendship that you may already have
and
the same framework also suggests
one mechanism by much
we might formalize what it means
to learn world models from language
so as I mentioned if we return
to the situation that opens this talk
tug-of-war games
we might think about how
the definition that I gave when I sat
up here at the podium right saying
there are people whose strength levels vary
from person to person
people have a percentage of time
in which they're lazy
strengths of the teams depend on the
underlying strengths of the
members of that team
and whether one team beats another just depends on which
team pulls stronger that match
and this kind of setting is actually
language is building up the
actual generative model itself
and you might think of a system like this
that both learns these kinds
of theories from language and then
is appending to this
kind of local problem-based context
to answer arbitrary questions like the
kind that we gave or condition on various observations
like Josh being stronger than Leo
with respect to this kind of local
notion
of what strength means
in this particular problem context
that we're thinking about
in interest of time why don't we just jump on to
your section
so
so we've just been talking about how natural
language can sort of
be interpreted or
semantically parsed to a probabilistic
language of thought but we haven't talked about
how cognition itself
which is sort of
we've been talking about as the product of
general purpose probabilistic inference machinery
might interact with language
cognitively or how
our tools for
our algorithms for inference or model representations
might benefit from recent advances
in language models
so
in the rest of the talk I'll sort of talk about
this also very preliminary work that we just presented
at a workshop at ICML
that is more about
a role for natural language and language models
in this part of the picture
and one reason to think that natural language
must play some role in this part of the picture
is that
is that sometimes we set ourselves reasoning
tasks whose specifications
what it would mean to solve the reasoning task correctly
must involve natural language so for example
if you have an iPhone you might have used the visual
voicemail feature
which automatically but somewhat incompletely transcribes
your voicemails and
these transcripts have gaps marked by underscore
sequences of varying lengths indicating
Apple couldn't quite work out what was said
and an inference task
that I sometimes face is squinting at these transcripts
and trying to think what could the person have said
during those bits that it didn't transcribe
correctly and
is it worth my time to listen to this voicemail
or am I pretty certain that I got all the
relevant information
from the part of the transcript that I've seen
so
even if I'm representing that kind of inference problem
in some kind of probabilistic
language of thought and not in natural language
it must reference natural language because a key part of the reasoning
that I'm doing is about how long those gaps are
about what words could go in those gaps
how they fit semantically and syntactically
with the words around them
and there are a lot of other tasks like this
where the specification of some reasoning problem
must in some way involve language
maybe we're writing something that has to obey
certain structural constraints like a poem
or code
maybe we're puzzling over a message from our advisor
trying to infer all the different meanings consistent with what
they said
maybe we're trying to figure out how to put together some words
that we predict could achieve some desired
effect in a listener
and beyond
the fact that some inference problems implicate
language in their specification
maybe sometimes we sort of use language
for thinking
rubber duck debugging is when we
successfully debug something
that's been stuffing us by just talking about it
to ourselves or to a rubber duck
and I think this is the intuition also
behind sort of chain of thought
those kinds of
innovations in language
model plan
but one reason why I'm drawing this distinction between
task specification and algorithm
is that
this has long been a really important distinction
in probabilistic modeling and inference and it's something that
I think we lose when we move just to
asking a language model a question
and hoping that it gives us the right answer
so
in the kind of work that our lab does
in modeling and inference
we sort of separately
create a model probabilistic
program that encodes a task specification
as a posterior distribution we want to sample from
and separately an inference program
that positionally encodes some kind of algorithm
or strategy for solving that inference task
and when you use a probabilistic programming language to do this
you get some benefits from taking
this approach of separating model inference
we know that we have soundness theorems
guaranteeing that as computation increases
the inference is going to approach the posterior
we have automated tools and tests
for measuring how accurate our inferences are
relative to the model with finite computation
and we also have gradient estimators
that help us tune any parameters of our
inference programs to be
better inference algorithms and beyond
being useful properties for engineering these guarantees
also reflect some key aspects of human cognition
we can often think more to reach
more accurate conclusions we can critically
evaluate the extent to which our current hypotheses
actually make sense given our model of the world
and if we repeatedly face the same kind
of inference task we can train ourselves to get
better at solving it
so something we've begun to explore is whether adding
LLMs to this picture might let us
both specify various linguistic tasks
as formal probabilistic models and enhance
our inference algorithms by letting them do some
of their thinking using language as a tool
so I'll first talk about the modeling
side of things so
we all know that an autoregressive language model
defines a probability distribution over
sequences of tokens
but we rarely just want to sample that unconditional
distribution
you know in the same way that in order
to use a SAT solver we need to reduce the problem
we care about to a SAT formula to use a language
model we need to reduction
from the past instance that we care about
to
a prompt
and the idea is that we're saying that the conditional
distribution of the language model, condition of that
prompt is somehow a good
specification of the task that we want to solve
or a good approximation of the task that we want to solve
but unlike the reductions to SAT of course
this reduction is lossy
one problem is that sort of hard
constraints, sort of instructions
that we give the language model
might fail to follow them
so this conditional distribution
of the NPTASC
is not really
the specification that we have in mind, it's just
some close thing that we can get
another
problem is that
the entropy of this distribution may not
meaningfully reflect uncertainty
so you may have seen in the
GPT-4 paper
that on multiple
language tasks, sorry multiple choice
tasks where
there is some multiple choice question and the language model
will put A, B, C, or D
before they did any RLHF
and instruction tuning
if they create a calibration plot
where they plot sort of
you know
of all the answers in which GPT was
0.4% confident how often
was that the correct answer
GPT-4 is strikingly well calibrated
and that's what you might expect from a model
that's doing a very good job of
next token prediction of matching
the distribution of language when it's uncertain
it's telling it, it should allocate its mass
probability mass according to that
distribution whereas after RLHF
the calibration is shot
and this is also what you might expect
even if the humans
who were sort of
providing the
human feedback in RLHF
preferred the right answer
the distribution that you get
after
performing RLHF with the objective
that's commonly used for RLHF
sort of creates a reduction
in temperature, it's equivalent to reducing
the temperature
of the parts where the human feedback
is exactly aligned with
the correct answer
and so it becomes overconfident
and this is very
prompt dependent, I don't mean to say that this is
always going to happen if you go and use GPT
but I went and used GPT-3.5
to do this infilling task
and it did it correctly but also
every time that I generated at temperature one
you get basically the same answer.
So if I want to think of
this distribution as sort
of representing uncertainties that I can make decisions
about whether to listen to my voicemail
because it might contain things I don't know
this p-task is not up to that
up to that task.
So our idea is to
instead of reducing to a prompt
reduce to a probabilistic program
that may call a language model
which is sort of a more flexible way
of specifying what p-task distribution
we want to sample from
and I know I'm running low on time
but the idea is that these models
can mix calls to the language
model with conditioning statements
and other logic.
So in this probabilistic program
for this infilling task
it is in a loop
going through each sort of blank
that we need to infill
sampling a random number of tokens
that should fill that spot
sampling those tokens and then
observing the next
sort of fixed fragment
or conditioning on the next part being
a fixed fragment and this just lets us specify
a model that doesn't just have
a prefix prompt that sort of has
a prompt with blanks in it
I haven't said yet how we're going to
sample this but the idea is that this defines
a specification for the task that we want
and similarly we have programs
that sort of specify
a variety of tasks that
involve sort of thinking with language
we can condition on hard constraints
if we want to parse into a formal grammar
we can do kind of a product
of experts model using multiple
prompts so maybe I want to think about
a fun fact that's about both London and Paris
well you could just ask the prompt
please give me a fun fact about both London
and Paris but you could also create a product
of experts model where it has to come up
with a completion that is both a completion
to the sentence a fun fact about London is
and a completion to the sentence a fun fact
about Paris is and that's kind of a hybrid
idea of and and both is
symbolically encoded but
we're still using the language models representation
of knowledge about fun facts
and these things
similarly we can sort of represent
reward steering or classifier guidance
by conditioning by sort of soft
conditioning on a reward function
and we can also
include things like hey please generate
a gloss of this code
that when I try to semantic parse it
back into
code gives me the same code I started
with things like that
so those are model programs
for specifying various tasks
we need inference algorithms for actually
sampling from these distributions
and so far we've been focusing
on sequential Monte Carlo inference algorithms
and we kind of have
a default version of this method and then fancy
your versions of this method that are necessary for harder tasks
in many ways sequential Monte Carlo
looks like mean search you kind of
keep multiple hypotheses around you
extend them
you reweight them in a model specific way
and then you re-sample which is kind
of like the part of beam search where you
sort of down sample from your
big expanded beam back to your beam size
but unlike beam search
sequential Monte Carlo
as you scale up the number of hypotheses
that you're using instead of converging
to an arg max
of your objective function converges to the
posterior distribution
sampling from the posterior distribution
so this sort of default version of SMC
has worked for a few simple tasks
that we've tried it on
for example
if I want a completion that follows
my favorite physicist is probably
and my favorite writer is probably equally well
SMC can give me Richard Feynman
I really admire how he communicates
complex ideas so vividly
or if I want to
finish the fed says but only using words
less than five letters I get the fed says
it will taper but the rate hikes are still
years away or something like that
and it's worth noting that if you do something
like token masking to enforce this constraint
you know you just
forbid the language model from generating anything
that's longer than five letters
you get all sorts of weird completions
it's different from the posterior here
you get completions that you know set up
an idiom that could only complete if it used
a word longer than five letters or something like that
and then it just gets very confused
and right stop that read more or something
it tries to come up with some context in which
the text would be cut off early
and for infilling tasks we get
a variety of samples that sort of
fit semantically and syntactically
with
with the text but
infilling tasks can be made much harder than this one
so I don't want to claim that this method
yet solves all these infilling tasks
so for harder tasks we think we're going to need to
use fancier sequentum and carlow algorithms
and both of these sort of steps
can actually be extended in various ways
we can use better proposal distributions
and sort of
better reweighting strategies
that are trying to guess okay
are we on the path to getting a good sample here
and we think that techniques
that have already been developed in the literature
for proposing good things
in line with constraints or
sort of discriminating whether we're likely
to land in a constraint
could be good ingredients to put here
but the important thing is
and this is the very end
the important thing is all of those things
become part of the inference program
and there are still guarantees that as we
scale up the number of particles we're still
targeting the original specification of
the model so all of those heuristics
or biases don't sort of
we don't just trust them blindly we don't hand
the keys to those techniques
we still have a specification that we can understand
okay I'll stop there
thanks
we're a little bit behind time so unless
there's a burning question
no burning questions let's
break for
T and maybe Zachertorte
and you guys can talk to the
speakers
and at 11.30
we are going to

Okay, so I'll just stop here, thanks for coming.
I'm Xiang and today I'll be talking about theoretical and practical insights from linear
transformers.
As you know, large language models work surprisingly well in practice, and the basis of large
language models is the transformer neural network.
So the important question is why do transformers work, and how can we train them effectively?
But directly this question is difficult to answer, because large language models have
billions, maybe even trillions of parameters, they have a lot of moving parts, different
choices of normalization, of embedding, and they contain many different kinds of modules.
So it is important to have a mathematical abstraction that captures the essence of transformer
learning and optimization in order to better understand transformers.
And today we are going to look at how the simple linear transformer can shed light on
two important questions.
One is the mechanism behind a phenomenon known as in-context learning, and two, I'll tell
you about how linear transformer optimization shares many of the unconventional features
of real transformer optimization.
And these two points are based on two papers, which are in joint work with Kwong Joon, Ha-Di
and Ha-Di from MIT, as well as Ming Ha-K, Charlie from KAIST.
So part one will be understanding in-context learning.
So what is in-context learning?
A very standard task of large language models is that of next-word prediction.
For example, you can give GPT a prompt, Mary has a little blank, and it tells you Mary
has a little lamb.
But that's not so surprising, because somewhere in this training data, I probably saw this
exact sentence, maybe thousands of times.
So then in-context learning refers to the following kind of prompting.
So I'll provide GPT with a few demonstrations.
I'll say apple, red, banana is yellow, then what is a grape?
And then so apple, red, and banana yellow serve as contextual examples.
And based off of these contextual examples, GPT infers that I'm looking for the color
of the fruit, so it feels in purple.
And this phenomenon even works for arbitrary made-up rules.
So this is a bunch of gibberish, but the ad symbol denotes concatenation, and it correctly
infers that ad denotes concatenation.
So it probably hasn't seen this exact example anywhere in this training set, but it still
does the right thing.
I would say that I tried some very complicated gibberish, and it doesn't work there.
So that's a good thing, I guess, because otherwise we are all out of jobs.
So to the best of my knowledge, this in-context learning phenomenon was first reported in
a seminal paper by Brown, Mann, Ryder, Subhiyah, and collaborators.
And this paper is because they're the equal contributions.
So this paper was also the one that coined the phrase in-context learning, I think.
I would also say that, you know, from a machine learning point of view, if you give your model
a few demonstrations, and then it does well on those demonstrations, it's not terribly
surprising, right, because, but the thing is here in-context learning works without
any updates to the model parameters.
So I'm not doing any fine tuning, I'm just using the same transformer, and then it does
the right thing.
So that's, I guess, the surprising thing.
Some people even go, as far as claim, that it's one of the main reasons for why large
language models work so well in practice.
But regardless, understanding how large models do in-context learning is a very important
question to us understanding large language models.
So in recent years, there's a few very important papers that try to shed light on this phenomenon.
Our work is based on a few of these papers, so I'll do a quick review of the relevant
literature now.
The first such paper is the one by Garg Tsipras-Leon Valiant, 2022, and the title is What Can
Transformers Lowering Context, a case study of simple function classes.
So recall the example about fruits and colors, and the hidden rule is that y is the color
of x, where x is some fruit, and y is some color.
But it's very hard to reason about what does it mean, what kind of function is represented
by the question about color in, say, the embedding space of words.
So what they propose to do is consider a simplified setup where your x are Euclidean vectors,
and the y's are linearly related to x by some unobserved data, so it's a linear regression
problem.
Pictorially, you know, you're given a bunch of demonstrations, the black dots, x, y pairs,
and then the question is in the form of the red dot, x and plus one, and you're trying
to figure out what the label y is.
The resolution is a bit low, but, okay, so a follow-up paper by Akirak, Sherman, Andres,
Ma, and Zoe in 2023, they further try to characterize what kind of, or they try to
characterize how transformers are able to learn these functions, such as linear functions.
And the main takeaway here is that transformers can learn in context because they are able
to implement various learning algorithms.
For example, this plot here shows that experimentally, transformers appear to implement the ordinary
least squares algorithm on noiseless data.
So the green line is like zero throughout, and so the transform prediction has a very
good agreement with ordinary least squares.
And the way they show that transformers are able to implement this learning algorithm
is approved by construction.
So they have this very clever construction where they define a few algorithmic primitives
like multiplication, division, affine transformation, and they show that the attention unit can
implement these primitives, so by hooking together various attention units, you're able
to implement algorithms such as BIOS.
And I'll mention here that further along this direction, there are also more extreme examples
where people show that the attention architecture can implement some kind of register system
or something.
So they can implement arbitrary algorithms, and transformers can be thought of as programmable
computers.
The catch is that all these constructions are very clever, but the downside is that also
means they are very fragile, and it's unclear whether these very clever constructions are
actually recovered when you, say, train your transformers with the add-on algorithm.
So the next in line are two papers, linear transformers or circularly fast-weight programs
by Schlag, Erich, Mied, Huber in 2010-1, and a closely related paper by Oswald, Nix-Los-SÃ¶n,
Luan, Dazzo, Sacramento, and Moff, and Vlad Mirov in 2010-1.
So I'll mainly focus on the second paper, Transformers, Learning Contacts by Gradient
Descent.
And so here as well, they do approve by construction, but there is a very important difference from
previous papers, which is they consider the linear transformers, which is...
So all previous papers, they may consider a simplified setting where the problem is linear
regression, but the architecture was always full transformers.
But here, it's the first time people look at linear transformers, which I'll define
a bit, but it's simpler than full transformers.
And because they look at linear transformers, they are able to provide a very simple construction,
where under which linear transformers are able to implement gradient descent, which
in turn allows them to learn linear functions in context.
And remarkably, they have some pretty convincing experiments, which agree with their construction.
So that's for prior work.
And now I come to our paper, where we try to answer the following question.
So we saw that transformers are expressive enough to implement a whole bunch of algorithms,
but can we show that transformers actually learn to implement any of these algorithms
during training?
So let me set up the problem.
We have a transformer.
The input is of the form of a matrix, d by n, where you can think of it as a horizontally
stacked bunch of tokens.
Each token is kind of like a word in a sentence.
So your sentence will get embedded in the inclusion space and turn into a bunch of factors.
The standard self-attention module is the following function.
So you have your key value query matrix.
You have a mask for this causality.
And then you put a softmax on this thing.
Linear self-attention is basically the same thing except we take out the softmax.
So there's no softmax.
And that's why it's called linear, because we took out the nonlinearity.
And by the same time, I would say that the phrase linear attention is maybe a bit of
a misnomer, because the linear attention module is not linear in either the parameters
PQK or in its input Z.
In fact, it's a third or the polynomial of Z.
And because of this, the representation power, when you stack a bunch of these linear attention
modules on top of each other, it increases.
So this is in contrast to something like, say, linear fully connected neural network.
So no matter how many of these you stack, you always are a linear function of an input.
But you can actually represent increasingly high-degree polynomials by stacking linear
attention units.
And the linear transformer, which we'll look at, is basically stacking these attention
units by a residual connection.
And to be precise, this is a single-headed transformer.
So that's defining the linear transformer architecture.
And now let me properly set up the learning objective.
So as mentioned earlier, the input to a transformer is a sequence of tokens.
And in the linear regression setting, each token Z consists of an x, y pair, where x
is a d-dimensional euclidean vector, y is a scalar, and x, y are related by this linear
relationship.
Theta star is unobserved.
And on top of that, each prompt has a different theta star.
The goal, you are also given a xm plus 1, but without the label ym plus 1.
And the goal is to train the transformer to predict the hidden label, given the demonstrations,
as well as xm plus 1.
I was stressed that this problem is much harder than simply learning a single theta
star, because theta star changes from prompt to prompt.
So you need to learn an algorithm that, given a few demonstrations, infers the right theta
star regardless of what the theta star is.
And at this point, I would also mention that one of the reasons for choosing to focus on
linear attention as opposed to the softmax attention, besides the fact that linear attention
is simpler and easier to understand, is that for this problem of learning a linear function,
ordinary transformers perform much better than softmax transformers.
And I guess we'll see concretely why that is in a bit.
But even now, just intuitively, right, if your data are linearly related, then it makes
sense that softmax doesn't really help you all that much.
So here's the first result I'll talk about.
We study one layer, linear transformer, and we claim that it implements one step of gradient
descent at global minimum.
So what does it mean for a transformer to implement one step of gradient descent?
On the left here, in this box, I show the architecture of a one-layer linear transformer.
It's very simple.
If you have a z, it passes through a single attention layer, and then, you know, we get
some output tfz, where tf subscript lz, denotes the transformer's prediction at layer l,
given input z, and parameters w.
So parameters w is like the correct key value matrices.
And we try to minimize the in-contact loss, which is the expected difference between the
prediction and the true label.
And the expectation is taken over both z, which is the input, as well as theta star,
which is the unobserved linear relationship.
And if you forget about transformers, we're a bit a very reasonable thing to try to do
when given a bunch of demonstrations, and you need to infer the correct label, is to
maintain a theta and then run gradient descent on it, with respect to the empirical least
gross loss, which I highlight in problem.
And so here, I just take one step, a single step, of gradient descent, and if n is sufficiently
large, you know, you'll probably do decently.
So theorem one of our paper states the following.
If you assume that the covariates are sampled from the standard normal distribution and theta
star is also sampled from the standard normal distribution, then the linear transformer
that minimizes the in-contact loss, fw, which is in red, gives the same prediction as the
one step gradient descent on r theta, which I highlight in purple.
So in other words, the output, tf1 of v comma w, is the same as if you ran one step of gradient
descent on theta, and use that to predict the label.
And in fact, you can consider a more general setting when your covariates are sampled from
some distribution with a non-identity covariate.
So when your covariance is sigma here, the linear transformer that globally minimizes
the loss now coincides with running one step of preconditioned gradient descent, where
the preconditioner a is given by following.
For when n is very large, which is when you're given a ton of demonstrations of, you know,
xy pairs, a in the limit is just inverse of your covariance matrix, the covariance matrix
of your covariates.
But when n is small, there's this additional regularization.
Is it obvious that the global minimum is unique, and if not, is this a statement about any
global minimum?
Yeah, so this is a statement about any global minimum.
In fact, there is some obvious, you know, no spaces in the loss, I guess, because one
example is the query times key matrix.
You can scale them arbitrarily if their product is the same, then that's the same.
Another example is, you know, since this is a linear transformer, scaling the value key
current matrices arbitrarily as long as they multiply the same thing also gives the exact
same predictor.
But then one might wonder, you know, ignoring these inferences, is that unique?
I'm not sure.
I'm not sure.
Empirically, in all our experiments, this is always recovered.
So that's a good sign.
And also, as I'm just about to mention, there are two concurrent works which appear surely
after we publish the initial draft.
One of them characterizes global optimality for one-layer linear transformer on the similar
sign.
So similar results.
That's ours.
And the second paper by Zonfre and Bartlett in 2023.
On top of characterizing global optimality, they show that if you run gradient descent
on the linear transformer with some specific initialization or some specific conditions
on initialization, you'll always converge to this.
So at least, you know, it's a good region of attraction based off of these results.
But I'm not sure if it's unique.
So that's for one-layer transformer implementing one-stop gradient descent.
In practice, you know, transformers work better when there's lots of layers and gradient
descent works better when there's lots of steps.
So then a natural question is, can we extend the similar results to a L-layer linear transformer
for some arbitrary integer L?
So on the left, again, I show a L-layer linear transformer, same in context learning loss,
but this time the vector is after L-layers.
And in the middle, I show L-steps of gradient descent, again, with respect to the same in
precoded scores.
And here, we establish a weaker guarantee.
So instead of saying that gradient descent is a global optimum, we only show that there
exist transformers, which are stationary points of the in-contact loss.
Such that at every layer, the transformer gives the same prediction as L-steps of gradient
descent on our data.
So in other words, TF2 would correspond to the prediction between data 2, TF1 corresponds
to the prediction of data 1, and so on, for each L.
And that's kind of interesting because really the only thing you're training on is TF capital
L, right?
And so it's interesting that all these intermediates outputs have a interpretable connection to
gradient descent.
I have a question.
There exist transformers.
I'm trying to figure out the quantification.
There exist transformers that for a random choice of these parameters.
What's the quantification on the X and the theta?
Are you saying at the very beginning of the theorem, very beginning of theorem 3?
Yes, X and theta are here.
Expectation.
So I define a loss on transformer key value query matrices, which is only a function of
the view that is expectation over Z, which I guess X, Y, and theta of the prediction
minus the true label.
And there are stationary points of this loss, f of the view, such that the stationary point
is some specific choice of key value query matrices, so if it's non-identity, but sigma,
and here we assume theta star is from sigma inverse, and there exist stationary points,
which coincide with preconditions, where the preconditioner is sigma inverse.
Are these local minima?
Can you construct one which is also a local minimum?
I'm not sure.
We tried, but we couldn't show it, which is why we only show stationary points.
Okay, let me show you one more slide.
So experimentally, so we only show that there exist stationary points.
Experimentally, surprisingly, we always recover the same key value matrices.
So specifically, a transformer implements precondition gradient descent by sigma inverse
if the product of key query matrices is sigma inverse.
And so here I train a three-layer transformer, and I display sigma half, query times key
sigma half, and we see that each of these cases is speculative, pretty much.
So in other words, it's always learning to implement something like precondition.
So I think that we don't have a theory for it, but I think there is, you know, we conjecture
that maybe these are in fact locally or even globally optimal, but yeah, so it's something
worth thinking about.
Before I end, I will also mention that I skimmed a bit of detail on characterizing this theorem.
We actually need to assume certain sparsities, specifically the last row and column of the
key query matrices are at zero.
And there are actually two variants of this theorem, depending on what kind of constraints
we impose on the value matrix.
You could implement precondition gradient descent or something more clever than precondition
gradient descent, which interleaves gradient steps with rotation of the gram matrix to
make things better conditioned.
You can see the details of all this.
So that's all for the first part of the talk, and I'm almost out of time.
But maybe I'll quickly talk about the second part, which is how linear transformer loss
landscape bears a number of remarkable similarities to the loss landscape of fluid transformation.
So again, transformers are large and complicated, and it's difficult to pinpoint why algorithm
works or it doesn't work.
And it's difficult to theoretically analyze the behavior of optimization algorithms.
Also it's very expensive to experiment on fluid transformers.
On the other hand, linear transformers may be a useful model to understand transformers
optimization.
So we surveyed a number of recent papers, which look at the optimization landscape of
fluid transformers.
We observe and we identify several remarkable features, which are kind of unique to transform
optimization.
And we observe that shallow linear transformers on the linear regression problem has similar
optimization features.
So one example is that Adam is significantly faster and still has the gradient descent for
a transformer training.
On the left, and this is a phenomenon that is kind of unique to large language models
and transformers.
So on the left, this plot is taken from Christopher Chen, I mean, he's three.
On the left, the two plots, we show training a CNN or an MNIST and C-partensile is an image
test.
And there is no obvious gap between SGT and Adam, but on the right, they show three transformers
on different datasets, and there's a clear gap between Adam and SGT.
Similar observations were also made in a number of other recent papers.
And we show here on the left, you know, the actual plot, which is kind of from the previous
slide for the three kinds of language tests.
Here on the right, I show a three-layer linear transformer trained on linear regression.
And you see that, similarly, there is a significant gap between Adam and SGT, and the three plots
coincide with slightly different settings of the kind.
And I'm already over time, so maybe I'll skip over the rest of the features.
But in the long story short, there's a number of features which are kind of unique to transform
optimization, and people conjecture that's maybe why adaptive algorithms are so important
when training large language models.
And so we went through each of them and we checked if you get the same kind of plots
or data that you get from training a simple linear transformer.
And each of this case, there is a surprising agreement with what people observe for four
transformers, all people observe for, you know, shallow linear transformers.
So with that, I will end the talk.
Any more questions for Chim?
So I've heard that actual large language model training is like unstable, and if you look
at the training post there, places where you get like these spikes, are those instabilities
also replicated in the smaller transformers that you consider?
Yeah, they are.
In fact, we have a guess for why some of the instabilities are happening.
And it goes back to the fact that, you know, transformer occurs to be learning to implement
this gradient descent algorithm.
And the thing with gradient descent is that the closer your, the larger your step size
is, the better you do until the point where you exceed the elliptical constant, then things
blow up very quickly.
So we also observed that, you know, as your loss gets lower and lower, and your learning
rate per layer, I guess, is getting closer to the boundary, it's become more unstable
because if you just exceed that little bit on your transformer, the kind of optimization
algorithm that's implemented by a transformer, like hybridism.
So that's one example, but yeah, we do offer a similar kind of algorithm.
So, so you showed the linear transform of one layer is equivalent to gradient, empirical
list of squares.
Yes, one step of gradient.
For all layers, they actually implement the preconditioned gradient.
Yeah, so for both, for both, let me go back to it.
So, for linear, for one layer, we show that, you know, if the covariance is sigma, it also
implements one step of preconditioned.
And then for our layer, if that our covariance is identity, it just does all steps of standard
gradient descent.
But again, if, if the covariance is sigma, then it does all steps of preconditioned.
So it's kind of like two orthogonal.
So the linear transform actually is nonlinear in Z, right?
Yes, yes.
So that means this nonlinear minimization actually automatically implement preconditioned.
Yes, yes.
Okay.
So that's different way to think about these algorithms and then they adaptively automatically
choose what precondition it is.
Yes, exactly.
That's very good.
Interesting.
There's one question.
You mentioned, if you add softmax in this linear regression task, it's actually going
to underperform compared to the linear model, right?
Yeah.
I didn't understand.
What was the reason?
This is kind of, so if, did you try like chatGPT2 model or?
No.
So if we code up a softmax with the same number of parameters, we coded up some linear transformer
and take a softmax to the place where we just pick it to make a softmax.
So it's not like a giant model?
I see.
So there's no residual connections, right?
Oh, with residuals.
So it's basically these two are exactly, almost exactly the same except, you know, this softmax
here and we don't.
I see.
So both look like this except different, slight difference in how ATTN is defined.
What was the intuition that white softmax is here?
Yeah.
That's a very good question.
So here's an example, right?
If I'm trying to predict y and I have some x and in my demonstrations, I have minus x,
right?
And then that should be very informative to predicting x, if you know it's linear.
But softmax would compute the product, which is an active number, and that becomes, then
you can improve very little weight on that sample.
And so that sample became useful for prediction.
And that's just one example.
I guess overall it's just that, based on the construction, which I didn't show, the linear
transformer very easily implements a gradient descent step.
Softmax transformer, not so much because softmax sticks out.
It does this weird way of your demonstration sample, which doesn't really help in the linear
regression setting.
So, but probably softmax transformer works more generally if it's not linear.
But is there any sense like the optimal algorithm for this problem?
That's a good question.
But I guess, you know, whatever algorithm that softmax is doing, it's just not very nice.
Thank you.
Thank you.
I had a clarification question.
So you mentioned that there's a different theta star for each input prompt.
Does your results also depend on how many prompts or demonstrations are provided as
part of in-context learning?
Good question.
Yes, because, so here, n is the length of the prompt.
Okay.
And how much regularization you put here depends on n.
I guess what this affects is how, I guess, what preconditioner you use exactly.
So this is for the non-identity covariance case.
And even for the identity covariance case, the exact step size, I think, would be affected
depending on this delta one, which I didn't talk about at all.
I think that's going to depend very importantly on how large n is, when a larger n, probably
larger delta.
And it kind of makes sense, right?
Because if you think about it, gradient descent involves this gram matrix.
And if the gram matrix is identity, one step of gradient descent will just give you the
solution.
And when n is very large, the gram matrix does approach identity.
Whereas when a small gram matrix could be your condition, then how your condition gram
matrix is related to the condition number of our theta.
So it makes sense that for a smaller n, you can take smaller steps of gradient descent
and order n.
All right.
So let's thank Yashiyang again.

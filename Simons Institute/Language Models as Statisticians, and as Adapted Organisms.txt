So our first speaker for the afternoon session is Jacob Steinhardt.
Jacob is a remarkable researcher who kind of combines like crazy creativity and technical
know-how.
And he's gotten me to think very differently about a lot of problems, both technically
and I guess societally.
So I predict it will be a very interesting talk.
Thank you, Sasha.
So I decided that I wanted to talk about something new kind of in the spirit of the Simon's
workshop.
So these slides are actually prepared totally from scratch.
No one has seen them before except me on my laptop.
So hopefully it will be interesting.
So the kind of overall motivation here is, you know, at least I really want to understand
what, you know, these ML systems that we're deploying all over the world are doing, how
they behave, what's going on under the hood.
But I find this very challenging because there's so many new ones, you know, maybe like every
month some new model is released and they get bigger and more capable and more complex.
And so how can our understanding of what these models are doing keep up, especially given
that we often get kind of new capabilities or qualitative behaviors just kind of emerging
every time we scale up.
And so I think, you know, there's maybe more than one answer to this, but something that
will be the focus of this talk is the idea that, well, if we can somehow use LLMs to
understand LLMs, then maybe we're in better shape because then every time a new better
LLM is released, well, there's a new thing to understand, but we've also gotten better
at understanding things.
So, you know, that's kind of the idea, can we get this virtuous cycle?
And then, you know, hopefully as models get better, understanding will as well.
So this is going to be based on work with actually a lot of different collaborators,
but three that I wanted to highlight are my students, Rachie, Eric, and Colin, in particular,
a lot of the kind of LLM as statistician perspective in this talk was developed by Rachie.
So what do I mean by LLMs as statisticians?
So what I want to argue is that many forms of understanding that we could care about
essentially reduce to some sort of statistical or data science problem, right?
So maybe we're given a model, we just see what it outputs on some huge number of inputs,
right?
We could easily do that by just taking all of the data sets we have and seeing what
the model does, and then maybe we want to kind of identify patterns, right?
Are there some things that the model is good at, some things it's bad at, cases where there's
something surprising, and then, you know, you could try to formalize that as some statistical
hypothesis and test it.
So that's kind of a statistical problem.
Maybe we want to understand the training set and understand, you know, what are the important
sources of variation?
You know, if it turns out that large fractions of the data set are in some weird language
called base 64, maybe we want to know about that.
And then you could also have kind of more active learning or active sampling problems
where we want to, say, generate new inputs that elicit problematic behaviors so that
we can identify and fix it, right?
So I think of these all as on some level kind of statistics or data problems.
And so if we can, in some sort of general sense, get large language models to do statistics,
then they can help us tackle all of these problems, and of course, many other useful
problems as well.
So to do that, what would we need to do?
So I'm going to kind of take a very high level view of, you know, what is, like, the
pipeline of doing statistics?
And I'd say it kind of has four steps.
The first is we look at some initial data.
Maybe we want to think of this as training data, but, you know, just some sort of data
that kind of helps us get our bearings.
From this data, we maybe want to form some hypothesis, you know, and maybe, like, the
hypothesis that models do worse on long inputs than short inputs, right?
And then formalize that quantitatively and then test this on new data.
This data could just be, you know, a held out set from the same distribution.
We might care about kind of, you know, generalization to new domains, or maybe if, maybe we want
to even, like, actively collect data to really stress test our hypothesis.
And so I'm going to go over a couple case studies where we'll have problems that kind
of follow this structure and will automate each step with LLMs.
The kind of key difference from maybe traditional ways of looking at statistics is going to
be in this hypothesis H. So a key difference is, you know, often H is, like, maybe expressed
as some mathematical function, like, you know, patients with this future in a data set are
more likely to get this disease.
I can write this as, like, expectation of some future function.
But here, because we're working with language models, H is going to be a natural language
string.
And so this third step, which maybe is often almost trivial, it's just, like, taking the
average of your future over the data set, now actually becomes kind of non-trivial because
we have to formalize what it means for, say, a natural language string to be true about
a data set or not.
So if that's kind of abstract, don't worry because we're going to go into some case studies
very soon.
In fact, right now.
Okay.
So the first case study I want to talk about is finding failures in a model called Clip.
So for those who aren't familiar with it, Clip is an encoder model.
So it takes in inputs and kind of embeds them as some future vector.
And it's kind of used as the backbone of many later models, right?
So it's often useful to have good embeddings.
So many models kind of use these embeddings and then do something with them.
And the thing that is kind of special about Clip is that it embeds both images and text
into a kind of shared space.
So it was one of the most early multimodal models that kind of did this effectively.
And as sort of shown here, lots of models use it.
So mid-journey uses it as its kind of first embedding step.
Dolly does, stable diffusion does, and lots of like 3D and video models do as well.
And you can kind of get pretty amazing results using these embeddings.
And they're used to kind of do text-to-image generation.
So you have some text description of what you want.
And then these models kind of try to generate an image that matches this description in some way.
So you can see here, these are maybe a little bit small, but this is an empty glass.
And then you get this glass, a family of five members.
And then you get this, a man descending a mountain.
You get this.
So these are all amazing, except for the problem that they're also all completely wrong.
This is a full glass.
This is a family of six members.
This is someone us sending a mountain.
This says there's no star in the night sky, and it shows the Milky Way.
And so you get these amazing images, but you often get these very semantically obvious failures.
So the kind of plot twist here, aside from the fact that here's a bunch of failures that we can find,
is actually that we didn't find these failures, and LLM found these failures.
So these are all automatically generated by actually not a single LLM, but a kind of pipeline
of several LLMs working together on kind of complementary tasks that played to their individual strengths.
So we'll talk about how could we actually build a system that just given clip could kind of automatically generate all of these failures at a large scale.
So are there any questions so far before I go into the details of this?
Yes.
One quick question.
Why do you say Dolly new being?
What does that mean in this context?
There's, I think, different versions of Dolly, and one of them, I'm not totally sure of the details here,
but I believe that Dolly was developed by OpenAI, but then Microsoft served some version of Dolly as part of their being product.
And so this is that, which we use because we can actually query it kind of publicly.
Yes.
Are there any new type of errors that pop up from this process?
For example, like the counting error or the direction error, those are already kind of known, right?
So are there any new hypothesized forms in this process?
Yes.
That's a good question.
So I'll get to kind of numbers later.
We generated 14 categories of errors.
We looked for papers that kind of described these errors.
I think there were a handful that had already been described in the literature.
The others were not in the literature.
I would probably guess that someone whose day job was to play around with these models would be familiar with these errors.
But the nice thing is even without spending lots and lots of time, you can do this.
In fact, I think there were cases where reviewers asked us to add a new system to this pipeline,
and in a half hour we just got all of the new errors that that system had.
So I think right now that's the advantage is that it's much quicker.
I think as models become better, I'm hoping that we'll also get things that even an expert who spent lots and lots of time might not necessarily find.
Yeah, great question.
Any other questions?
Okay.
So how do we do this?
So first, maybe let me give a kind of overview of the key ideas here.
Right.
So again, remember clip is a feature encoder that is encoding both texts and images.
So the kind of main key idea in terms of like where we get off the ground is there's going to be a sort of notion of what I'll call a hash collision in the encoder.
And we'll come up with a sort of automated way of identifying lots of hash collisions in the encoder.
So this is going to give us a lot of kind of like individual examples where if you, you know, if you have a collision, you don't know which of those two is wrong, but you know at least one of them has to be wrong.
Then kind of given those failures, we're going to use LMS to kind of categorize them into coherent patterns and test that those patterns are actually correct by generating new examples from those patterns and making sure that they actually, you know, in fact, induce failures consistently, both in the encoder
and on downstream tasks and even kind of also generalizing to new domains that are kind of different from what we found these patterns on.
So that's kind of the high level. Let me kind of step through these all one by one.
Right. So again, remember we had this statistical pipeline of, you know, get some data generated hypothesis formalize the hypothesis tested on new data.
So let's just go through these one by one.
So first let's just talk about, you know, where are we getting this initial data?
What is this kind of hash collision idea.
So to talk about this, I need to give you a little bit more background on clip.
So, as I said before, clip embeds either image, you know, an image I or text T.
But what is it actually designed to do or what was it sort of trained on.
So it's actually trained on a bunch of pairs of images and their captions.
And in general, the idea is that if there's text that describes an image, then that text in that image should have similar embeddings, ideally more similar embeddings to each other than to anything else.
So the training process was basically, you got a bunch of images, you got a bunch of captions, and then you want to make sure that under this embedding, you know, the cosine similarity between an image and its caption is higher than between that, you know, image and any other captions.
So you kind of want, if we form this matrix of dot products, you want the diagonal to be really big and everything else to be really small.
And so the kind of point here is that if T is a description of I, they should have similar embeddings.
Now, how can I use this to find problems?
Well, if T and T prime describe different images, but have the same embedding or have very similar embeddings, then at least one of them has to be wrong in some sense, right, because they, you know, like,
whatever images these corresponded to, they kind of can't both be, they can't both be right.
Ah, yes, I'll just assume the bijection between images and text.
And we know that in a picture is worth a thousand words.
Right, so the examples I have in mind would be something like an empty cup and a full cup.
And if those, like if those sentences had the same embedding,
those are, that's, that's a very small subset of all the, like there's a lot of synonyms, right, visual synonyms.
That's right. So you have to worry about visual synonyms.
So we need, we need some way of measuring kind of semantic difference that hopefully implies that things are actually visually different.
So I'll get to that on the next slide.
Other questions though.
So one thing here also is the nice thing is that I can do this only looking at text, right.
I mean, I have to use the clip encoder, but I'm only encoding text.
And why do I want to only look at text?
Well, basically, because language models work really well and image models don't.
Sorry, Alyosha.
But, you know, according to Alyosha, image models will work really well soon, even better.
But right now, right now we want to stick with language.
So, so what do we do?
We're going to collect some initial corpus of text inputs and we want these inputs to be input that have some visual significance.
So we'll often take them from some say captioning data set or other kind of data set that has visual descriptions.
And then we're going to embed them all under clip.
We're also going to embed them under another model called distil roberta, which is a very good text model, especially for embeddings.
So it's a text only model.
And it's also has a high dimensional embedding space than clip.
And so for both of these reasons, it's kind of, you know, has a better understanding of text than clip does because it gets to only focus on text and it has more parameters.
And so the basic idea is, you know, so we have all these clip embeddings.
And if there's two inputs that are very close in clip space, but actually have low roberta similarity, right?
So that means they're kind of different like roberta thinks that they're semantically different sentences, but clip says they're the same.
Then we're going to say, okay, that's a hash collision.
Probably something is wrong there.
Now, we, I agree with you that we also want to check that these really are semantically different in a way that matters visually.
Empirically, it turns out that that's the case about 90% of the time.
So this is kind of good enough.
And this is something we kind of verified with human, human subject studies.
Jacob, how do you find these things because aren't the clip vectors like 2048 dimensional or something?
We're just taking the cosine similarity.
So this is like, this is an n squared algorithm.
Yeah, but okay, but I mean, if we imagine that you were just looking for collisions in this 2048 dimensional space, we would say a priori that could take astronomical time.
You're saying like in practice, it takes much less time because, you know, there's something about these text inputs that makes the collisions likelier.
So, so yeah, a couple of things.
I guess first we don't need exact collisions.
If the cosine similarity is large enough, I guess empirically, if it's larger than 0.88, it turns out that it's pretty likely to create a problem.
So you don't need them to be exactly the same.
That kind of helped you somewhat.
And, and yeah, so somehow this.
But I mean, two unit vectors having an inner product of 0.88 in a 2048 dimensional space that we might as well call that a collision right at that point.
He's assuming n squared time for you and with the exponential.
So, okay, right.
So Scott's claiming we would need an exponentially large data set, but this is not.
Okay, but it happens that so basically the reason why there are collisions is not as nothing to do with like the pigeonhole principle with the space, right?
It just that, you know, the way that it's something special about this mapping that causes there to be collisions, even though a priori there's like there could have been no collisions.
Yeah, that's right.
And if you care, couldn't you do it much faster by using the same kind of H and SW in the cities with all of the embeddings?
Yeah, you could do that.
You could do this much faster than n squared.
It just turned out that this is not the bottleneck.
Like the bottleneck is running the forward passes of all the models and kind of looping over a bunch of pairs of, you know, 1000 dimensional vectors is pretty, pretty cheap compared to running an LLM.
Yeah, so we tried two different corpora.
One is Cocoa and the other is what's the other one SNLI.
Yeah, these are both kind of text data sets that have visual significance.
And yeah, I mean, basically the point is that there's enough.
Yeah, there's enough structure and text that you actually do get collisions.
I don't care about the n squared.
Okay, okay.
Got it.
Okay, so this is the first step, right?
This is going to give us a bunch of pairs where we kind of know that like one of the two things in the pair is wrong.
And so now we want to do something with that.
So this is kind of the next stage is we want to generate some hypotheses based on these pairs.
Right.
So this is where we're going to use the fact that we're, we have text and not images.
Right.
So the individual pair fillers are text inputs.
So we can feed them to GPT four.
And so, you know, here's the magical prompt.
It says I'll provide a series of data for you to remember.
Subsequently, I'll ask you some questions to test your performance.
Here's some pairs of prompts to memorize.
And then you give it all of, well, you give it as many of these failures as you can fit in the context window.
And then you tell it, hey, I'm trying to find failures with an embedding model.
These are pairs of sentences that are encoded very similarly.
Using the specific examples, are there any general types of failures you notice the embedding is making?
And then, you know, you kind of give it some more, more context and you'd say, okay, what does it generate?
Right.
So you're basically saying, here's some data.
Please look at it.
Please tell me some patterns.
And so then it, you know, it does a pretty good job of coming up with things.
It says, okay, there's negation, temporal differences, quantifiers.
And the nice thing is actually it doesn't just say negation, but it gives a bunch of elaboration.
So it says embedding models may not correctly capture the negative context in a sentence leading to similarities between sentences with and without negation.
This can result in incorrect visual representations of the presence or absence of an action is significant in video generation.
And it kind of keeps going.
And you get, I guess in this case, you get kind of 14 distinct failures in total on this list.
And the other thing is empirically, just kind of always uses this consistent list format so you can automatically just parse out the individual hypothesis.
Yeah, Lisa.
Just like asking GPT without these like inputs, like what are common failure cases of image embedding models or something.
Yes.
So that's, that's a baseline that I will show results on later and yet works a lot less well.
Okay.
So, so this kind of gives us hypotheses.
Right.
But now we're running into this problem of okay, usually in statistics, a hypothesis is like actually some mathematical function.
But here these are just sentences.
You know, so now we need to formalize this hypothesis.
Right.
So we have this list of hypotheses h1 through hk that are all natural language descriptions.
So how can we test if one of these hypotheses is actually any good.
So I think this is a pretty interesting conceptual question to think about.
So maybe I'll pose it to the audience.
If anyone has ideas for how we could formalize this.
Hi, Chris.
We could ask GPT about cares as to whether they match this hypothesis and then see if the images are wrong.
Okay.
Good.
Yes.
So that is.
Yeah.
I mean, if we think about research hypothesis, there are a few dimensions that you can use to categorize whether some say it's a hypothesis.
So for example, it should be testable, right?
There should be a clear scope.
There are a few dimensions I think that you can come up with based on experts.
Right.
So you could kind of ask experts or GPT for if it had those properties.
So it turns out we're going to do something pretty similar to what Chris said.
Although we're going to look at generation rather than classification.
So we're going to say age is a good hypothesis.
If when you hand that description to some intelligent agent, in this case, not humans, because humans are expensive, but GPT four, they do a better job of generating new failures than they would otherwise.
So this is, this is the way we're going to quantify this.
So we'll say age is a good hypothesis.
If it can be used to generate new failures, better than some just baseline method of generating failures.
And this is, this is where we're going to get to your question, Lisa.
So we can either hand GPT for these hypotheses or we could just ask GPT for it to like brainstorm without any data ways in which vision models might be bad and kind of test those against each other and see which one does better.
Okay, right. So we're going to test this by prompting an LLM with age as a context.
And so again, what is the magical prompt?
The magical prompt is to say write down 41 pairs of prompts that an embedding model with the following failure mode might encode similarly, even though they would correspond to different images if used as captions, use the following format.
So you give it kind of a format so that we can extract things programmatically.
And then we say some other stuff to motivate it, saying that it will be evaluated based on how well it performs.
And then you give the failure mode and as kind of the description that we extracted before.
Y41, that's the length, basically the length of the output context window that can be fit.
So if you want more than 41, you just have to ask it a couple of times.
So this is what we did.
How much does this be creative and cautious and these kinds of things like actually help?
Or is this just like black magic that you sort of sprinkle on top?
We didn't do careful ablations on the prompt.
I think, yeah, we added a bunch of stuff until it worked.
And I don't think we tried removing things to see what was actually necessary.
So it seems totally like I would guess that if you tried to like distill this to its bare essentials, you could get something simpler.
But we didn't try to do that.
But yeah, great question.
So then we want to quantify by measuring the success rate.
So we get all these pairs of prompts that are supposedly supposed to be new failures.
So we can do this in two ways.
We can look at the fraction of things generated that are hash collisions in the same sense as before.
So that's kind of an easy thing to do.
At some point, we want to make sure that the system is like actually doing something and that the something doesn't involve trusting that LLMs are good at their job.
So we also do a human evaluation where we look at downstream systems that rely on clip and ask humans if there's a failure to make sure that these actually are failures and not just happen to have high cosine similarity.
So those are the two things we do.
So let's kind of go over the results.
So first, just kind of looking at hash collisions, but testing on these new inputs that were generated.
So we say an input has a success if these similarities are above some threshold.
And what is this table saying?
So these rows are kind of the different failures generated by the system.
And actually we considered six different systems.
So there's, you can ask different models to look at the data and propose hypotheses.
So these are different kind of proposer models, GPT-4 cloud or GPT-3.5.
And then you can also vary the data set that you used to actually get these failures out.
So these are the two data sets that people asked about before, COCO and SNLI.
So I guess a couple interesting things.
One is that, and a check mark means that the model generated the failure at all in its list.
And then the color is kind of the success rate of generating new inputs conditional on that failure description.
So a couple interesting things.
First of all, the data set seems to actually matter, right?
So kind of for both GPT-4 and cloud, action, well, okay, maybe let's pick a more intuitive one.
Okay, so for both GPT-4 and cloud and GPT-3.5, SNLI elicits granularity as a failure, whereas COCO never does.
And sometimes it's kind of not quite so systematic.
But in general, it sort of seems like these data sets actually do kind of like elicit different failures.
So there is at least some dependence on the data, which is somewhat reassuring.
The other thing is maybe as expected, GPT-4 and cloud in general find many more failures than GPT-3.5 does.
So these better models actually generate more distinct hypotheses.
And then a final thing that is interesting is actually even for the same failure, bigger models often are giving you higher success rates.
So you can see this in a couple places like for granularity.
The description of the granularity failure that GPT-4 generated was apparently better in terms of if you then hand that back to GPT-4,
it more successfully generates novel failure instances compared to the kind of description that GPT-3.5 gave.
In all of these cases, we're fixing GPT-4 as kind of the thing that's generating new failures.
So there's no effect from that.
So this kind of difference is just coming from the actual text description output by them all.
So are there any questions about this data?
Do you have a little more insight about when GPT-4 does better than GPT-3.5?
Is it because it better understood the instruction versus maybe GPT-3.5 also understood the description,
but somehow the example is that it's very just curious at what sort of qualitative differences are there between different models.
Yes. So I haven't thought about this a ton.
I think my two main hypotheses here would be, I believe GPT-4 has a larger context window so it can see more examples, which might be useful.
But I think probably the more important thing is actually just that the task of proposing hypotheses from a data set is actually a pretty challenging task.
And so even kind of frontier models are not that good at it.
So then once you drop down from GPT-4 to GPT-3.5, you're kind of losing.
Probably just losing too much capability for it to be super consistent.
That would be my hypothesis. I don't know. Yeah, Richie?
Yeah. So in practice, we found that GPT-3.5 doesn't really condition on the data very well.
Well, GPT-4 actually does condition on the data and then describes things in the data.
Yes?
So I don't think we did a systematic evaluation of whether all of the examples fall into those categories.
Yeah, I can give some errors on the next slide that get at least like implicitly at that.
It looked like maybe someone else had a question.
Okay, so maybe let's go to the human evaluation, which will at least partially answer your question.
So we wanted to not just stick with saying that you actually get these hash collisions,
but show that these actually lead to images that humans say are wrong.
So, okay, darn, the text here is a little bit small.
But this is kind of the human annotator interface that we gave.
So we had prompt1, a city skyline with a bridge, prompt2, a city skyline without a bridge.
So this is this kind of collision pair.
This is an image that came from one of these two prompts chosen at random.
And so the annotator has to say either that this corresponds to prompt1,
it corresponds to prompt2, it corresponds to neither of them,
or these prompts are described visually identical situations.
So this kind of gets at your earlier question, Aliyusha, on whether these are actually semantically different.
And so we're kind of measuring what counts as a successful failure,
either if the annotator says that it's wrong, or if they say it's prompt2,
but it was actually generated by prompt1, or vice versa.
And so then you can look at the kind of rate at which mistakes are made,
conditional on different levels of clip similarity in the two prompts.
So this is kind of testing that high similarity actually leads to failures,
and you can kind of see there's this inflection point at point88.
So this is kind of one verifying that actually we do get pretty high rates of failure,
and two that this magical threshold I told you about earlier is actually kind of reasonable threshold.
And then finally, to get at this question of whether these descriptions are actually doing anything.
So I guess this doesn't test whether the failures correspond to the descriptions,
but it tests that the descriptions are actually needed to get high failure rates.
If you have a baseline system where you just ask it to brainstorm possible failures images might have,
and then condition on those, you only get about 20% failure rate,
whereas you get an 80% failure rate if you use this data conditioned system.
So these are the human evaluated rather than model evaluated.
So are there questions about this?
This is a talk where high failure rate is better.
Yes, you want high failure rates because we're trying to find failures so that we can fix them.
Okay, and then I guess a final cool thing is a kind of really big bonus of having this come from language models
is language models are kind of automatically steerable.
So I have this way of generating failures, but I can then just ask the model to give me failures
that are relevant to some new domain.
And so in this case, we kind of ask it to generate failures that are relevant to self-driving.
The data sets are still cocoa and SNLI.
So we didn't give it data that would specialize to self-driving,
but it can still kind of generate these failures in this novel domain and still have a good success rate.
And so these are just kind of examples, a stable diffusion.
The car is on the right side of the lane, but it's on the left side.
This is not a green light, gives you a green light.
A yield sign gives you something that is at least not shaped like a yield sign, probably a stop sign.
And then a car stops for a red light.
This is actually a text to video model and the light is green.
What data from cocoa and SNLI are you passing in?
No, no. So you're passing in the text from...
So this is for the very first stage where you're giving it a bunch of just text inputs
and embedding them to check for hash collisions.
So those hash collisions were from embedding text sentences from cocoa and SNLI.
So there's actually no images anywhere here except in the output of the systems.
I'm kind of curious about the hypothesis part of this and whether that's kind of necessary.
So we had a paper a couple of years ago and just tried finding these collisions.
And I kind of wonder if you could just give it a sentence and search for a collision
and just cut out the language model.
What is it adding in the process?
Well, finding the initial... Oh, I see.
So I think one thing is this durability, I think, would be challenging in some cases
if you were doing that because you would need a large data set of text in whatever new domain
you were looking at.
We don't need a bunch of sentences about self-driving cars to do this,
but if you were looking for collisions manually, then you would have to do that.
I see.
Yeah, cool.
Okay, so to summarize this, right, we had these four stages of, you know,
first we want to get initial data, which we did by scraping hash collisions from this text data set.
This kind of invoked these two models clipped into still Roberta.
Then we generate hypotheses by prompting GPT-4.
Then we kind of formalize these hypotheses by looking at the success rate of generating new failures.
So we use GPT-4 to generate the failures and clip to evaluate them.
And then we can also do this active steering, again, prompting GPT-4.
So I think one thing I want to highlight here is that, you know,
often we think about just having this one language model that we just come up with, you know,
our super clever prompt that solves everything and maybe do chain of thoughts and this sort of thing.
But I think you can actually get a lot further if you're willing to kind of use this kind of, you know,
ecosystem of models together in creative ways.
And I think statistics is a particularly kind of good use case for this,
because there are these different stages of the pipeline that require kind of different skills.
And statistics also has some nice properties, right?
Like many parts of it are kind of automatically measurable and verifiable.
And so you get a lot of the same strengths as Adam was talking about yesterday with computer programming,
where, you know, you can, we haven't done much of this, but like you can, you know,
maybe do this self-training and maybe if you get models that were really, really, really good at statistics,
super human at statistics, because there's so much automatically generateable data.
Okay, so that was the first case study.
Let me go over the second one.
We'll go a bit more quickly now that we've kind of built up a lot of these conceptual ideas.
So here I'm actually going to talk about a kind of meta task that is then going to be useful for lots of individual
ways in which we would want to understand language models.
So this meta task is classifying with natural language predicates.
So the task here is we're going to be given two text data sets, D1 and D2.
We want to find out what's different between them.
And this difference, again, should be some natural language string H.
And so we can kind of think about this as isomorphic to binary classification, right?
We're kind of trying to classify between D1 and D2,
but where the function is described in natural language.
So let me just give you an example of what this task might look like, right?
So maybe these are my two data sets, D1 and D2.
We want to come up with a natural language description of how they're different.
So can anyone figure this one out?
It's not in English.
Okay, yes.
So the left is French and the right is English.
So our H would be D1 contains more French sentences compared to D2.
Okay, so here's a harder example.
Maybe partly hard due to text size.
But I claim that even if you look at this for a while,
it would be hard to tell what the difference was.
And in fact, the difference is that sentences in D1 contain at least two female characters in them,
whereas sentences in D2 do not.
This is actually pretty challenging because there's things like she carried a total of eight torpedoes
where she refers to a ship, which is not, in fact, a female character.
And you also have to know that Professor McKeown is female.
So there's kind of a lot of world knowledge and kind of non-trivial stuff in solving a problem like this.
So I guess that's the kind of meta task.
Why should we care about this?
So first, I'll give you three use cases that would help us better understand LLMs.
So we could want to understand distribution shift.
So especially if a model is doing poorly out of distribution,
maybe we want to diagnose what's different.
And so we might find out that the test distribution involves more formal writing than the training distribution.
And that might help us diagnose failures or tell us what we should fine tune on.
The positive class contains more URLs than the negative class.
If this were a spam classification data set, this would tell us that there was this potential spurious queue.
That maybe the model just looks at the presence or absence of URLs,
and we should make sure that that's not just what it's doing.
We could do error analysis.
I could give you two models, and I could look at the difference between inputs where one of them versus the other one makes mistakes.
And then we could also go beyond just trying to understand LLMs.
You could start trying to do, say, social science.
I look at a bunch of tweets from one year versus another year,
and say, okay, public opinion from this year is more optimistic about the pandemic than last year.
We can kind of generate at least descriptive hypotheses like this.
Of course, you would then want to carefully do all of the causal inference and other stuff to validate these,
but this at least can generate hypotheses for you.
Okay, so how will we do this?
So if you go back to the examples you have, it feels like the hypothesized space is huge, right?
There seems to be a recall precision issue here, and it seems your ground truth only has some of them.
Like for them, the very easy example, my hypothesis could be there is no relation between those two sentences.
It has more French versus the second sentence. The space seems huge.
How do you locate it to the final ones we want?
So we're not going to have ground truth in most cases.
There are a couple cases where we did create a ground truth so that we could kind of test in the traditional setting with kind of gold labels.
But we're going to kind of take a similar perspective to how you'd evaluate any other classifier.
Right, so we're going to come up with a way to quantify these in terms of some classification error rate,
and then we'll say that one hypothesis is better than another if it has a lower error rate in distinguishing D1 and D2.
And so rather than having a fixed ground truth, we can just talk about which systems are better or worse,
and maybe we could also say compare to humans to get some overall benchmark.
Some of the hypothesis might be very trivial, and others may be more usable, right?
Okay, right, so you might also want to evaluate, if there's some goal, you might want to evaluate relevance to the goal and some notion of novelty.
These start to become subjective, so we have done some of this where we get human ratings of novelty and relevance.
Reviewers didn't like it actually because it's too hard to define novelty, but anyways, like you can do this,
and like I think, but I do agree it's kind of a tricky problem.
Yes.
So just building on these questions, are you constraining some complexity of the hypothesis?
Are you looking for short hypotheses?
So we'll implicitly have short hypotheses here, although that kind of just comes from the fact that these are going to be generated by an LLM,
and LLMs will only output things that are so long.
Okay.
But yeah, for usability reasons, you would want this to be kind of short and interpretable.
Cool.
So yeah, how are we going to do this?
We're basically just going to use LLMs somewhat similarly to before, right?
So I won't give you the full magical prompt that Rachel came up with, but just kind of schematically,
we kind of give it a bunch of examples from the first distribution and label them as A, a bunch from the second,
and then we say, you know, compared to group B, each sentence from group A, and then we ask it to complete it.
This was done at a point where we were using GPT-3, so we needed it to be in this completion format.
Once you have instruction-tuned models, there's maybe nicer prompts you can have,
but this is kind of the basic idea, and then you sample this a bunch of times with different, you know,
sub-samples of the data, right?
So keep in mind that we can only fit maybe like 30 or so examples into the context window,
so we have a data set with thousands of examples, it's like a very tiny fraction,
so we kind of keep sub-sampling to generate different hypotheses.
And then, you know, you'll get things like is more positive, contains the word chapter, is longer,
some of them actually kind of have, yeah, some of them end up being kind of trivial,
so you might want ways of filtering them out, but you kind of get this set of candidate hypotheses.
So this is kind of telling us how we do the first two steps of this statistics pipeline, right?
We look at this initial data, we prompt GPT-n for some n with examples from D1 and D2,
and then we ask how they're different in these forms of hypothesis,
but now we need to somehow formalize age quantitatively and test it on new data.
So I guess, again, maybe I'll pose the question, you know, how could we formalize this age quantitatively?
How could we sort of say quantitatively how good is it?
Oh, sorry, what did you say first?
Okay, good, right, so we can say, you know, good hypothesis is something that helps tell D1 and D2 apart,
so we'll, you know, take a sample from D1, a sample from D2, mix them up randomly so you don't know which is which.
Tell either a human or an LLM the hypothesis and ask them to say which is which.
So, you know, as an example, say H is involves more formal writing, we can interpret this as basically a two argument predicate, right?
So if I have sentences x1 and x2, H of x1, x2 is some binary predicate that is the truth value of, you know,
the utterance x1 involves more formal writing than x2.
And so this should be true or false.
And so then we just ask a human or a language model if it's true or false.
And so then we'll say H is a correct hypothesis about D1 versus D2.
If in expectation over samples x1 from D1 and x2 from D2, this is much less than 0.5.
Right, so 0.5 would be chance.
If this is sort of the, like some measure of the classification error, if it's much less than 0.5, then we've, we figured out something non-trivial about D1 and D2.
And so, yeah, how to implement this, I guess, you could ask humans or you could just query an LLM.
And so what does this look like, right?
So just to illustrate this, if H is samples from D1 or more positive than those from D2, we, you know, we give, in this case, Charlie Snell,
who was an undergrad at Berkeley at the time and is now a PhD student at Berkeley,
this paper proposes an impactful task or the approach of this paper is too trivial and ask him which of these it's true about.
And then he says something and then, you know, maybe Charlie's time is pretty valuable so you can hire crowd workers to do this instead.
But the problem is, you know, even if we just wanted to average over, say, like 100 samples from this distribution to get some notion of accuracy,
this would cost $10 per text description.
This is very expensive.
You don't want to do this.
And so the nice thing is LLMs kind of reduce the cost of this pipeline by about a factor of a thousand.
You can do this 100 samples for only seven cents with 3.3.5 turbo.
And so then this gives you a kind of automatic quantifiable measure of how successful this hypothesis is.
And the nice thing is also that it's somewhat more reproducible than humans.
Like you don't have to worry about getting back the same human label errors again because, you know, the model, well, the model's not actually fixed.
Open AI keeps updating it.
That's kind of annoying.
But if they were to serve a stable version of the model, then this would be reproducible.
Right.
So now, now we've kind of gotten this whole pipeline.
Right.
So the final system is we have this proposer, which at the time we initially wrote this paper was a fine tune to GPT three that generates these candidate hypotheses.
Then we have a verifier that kind of, you know, does this check on each of the hypotheses.
At the time it was fine tune unified QA.
And then you kind of, you know, re rank the hypotheses based on their actual success rate at this classification task.
And, you know, why is this decomposition useful?
Well, from an engineering perspective, and I think this is actually very important, the proposer only sees 30 examples because that's length of its context window.
So it's in that sense fairly limited, even though it's this very smart, like GPT three or four system.
Whereas the verifier can see thousands of examples.
And so you can get much better tests of statistical significance.
How am I doing on time?
I'm about five minutes left.
Okay.
Cool.
So maybe I'll just say, you know, you can use this for a bunch of things.
We're describing distribution shifts.
There's these two data sets M and a line SNLI where SNLI is often used as like an OD version of MNLI.
Here's four samples, two of which are from SNLI and two of which are from MNLI.
And maybe it's not immediately obvious what distinguishes them.
But if I say that SNLI describes a picture, then it's very clear that the green ones are SNLI because it says the church choir sings to the masses and old man with the packages poses in front of an advertisement.
And the other two are not about pictures.
So you kind of immediately see what the distribution shift is here.
There's two paraphrase data sets, Twitter, PPTV and QQP, which sends for core question pairs.
It says Twitter talks about a news story and Quora contains a question.
These are just kind of sanity checks.
Like these would be kind of totally obvious to anyone who was familiar with these data sets.
But you can do more interesting things.
So this was one that I think to our knowledge was novel at the time we discovered it detecting spurious cues in a data set.
So we handed it this data set called SUBJ, which is a data set for subjectivity analysis.
And it said the objective class was a plot summary of a film.
The subjective class is a quote from a film review, which seems like it should be wrong for a data set that's about subjectivity analysis.
But if you actually go back and read the paper, it says,
to gather subjective sentences, we collected 5000 movie reviews snippets from Rotten Tomatoes to obtain mostly objective data.
We took 5000 sentences from plot summaries available from IMDB.
So actually, if you did well in this data set, you were basically learning this rather than stuff about subjectivity.
There's like other shortcuts we found somewhere new, somewhere old, but you can sort of find all these various cues.
You can use this for error analysis and so on.
So maybe just to summarize, we have these four steps of this pipeline.
Initial data was just the two text distributions, generate the hypothesis by prompting GPT-3,
formalize the hypothesis by measuring the success rate, and then you kind of test on new held out samples.
The final thing I'll just leave up is we have ongoing work that is kind of taking this far beyond classification
to just sort of like generally using natural language predicates as features in statistical models.
So this example here is trying to describe temporal drift in news headlines from the Australian Broadcasting Company,
and it kind of identifies these five features that kind of vary.
And you could think of as maybe the top five principal components explaining the variation in this data set,
although the percent of variance explained is actually still pretty low here,
so I think you should think of this as just like a initial result.
So maybe I'll end there and take questions.
So for the proposal there can be just like do prompt optimization, like over time you keep selecting like an example
and then see where it fails and then show it again, and then after a while you have like this good hypothesis
that you can somehow guarantee that is correct in a set of checking things.
So you could, so is the idea like basically like fine-tune the proposal to get better and better?
Yeah, fine-tune the prompt, you do that prompt optimization.
Oh, prompt optimization.
So I think, so my general sense is if you just use the proposal,
I mean, we didn't try to do like the full prompt optimization,
but we do do ablations of like just using the proposal and that generally does a lot worse.
I think that there's a couple issues.
One is just that the proposal gets much less data than the verifier.
So, you know, even like-
But if you do prompt optimization, I'm sorry.
Oh, no, what's your-
I thought that your proposal like get like a few number of data,
that's why you should do the checking.
And then I was just saying that can you make the proposal more strong,
but just to prompt optimization so you can't see everything
and then you can't guarantee that this hypothesis is correct.
Yeah, so that's an interesting idea.
So you're basically saying, okay, like do prompt optimization to find a prompt that gets this.
I feel like we discussed this.
Your claim is that you don't get semantically meaningful hypothesis.
If you do gradient, you just gradient essentially find the prompt,
then you very easily get like a readable prompt that is not natural downstream
because what you typically find with adversarial examples, they are relatively pathological.
Yeah, so I think the issue is at least right now, if you do this,
it's hard to get kind of like natural language out.
Right now you asked GPT-4 to give you gradients, but-
Thank you for this amazing talk.
The quick question, maybe this is not-
So you're asking for hypotheses that separate the two data set.
And when you were testing them, they were like, okay, let me pick two examples
and say one is more positive than the other.
But could you tweak this to possibly look for hypotheses that certainly answer yes in one case
and no in the other case, that you don't have to look at two comparative examples?
Like, okay, this one talks of a female character, the other one talks of a male character.
So what would be- just so I can understand, like what would be an example prompt you might-
So as an example, the second example you showed was that this statement talks of two female characters.
That's a statement you can answer in yes or no without looking at two different samples from the two data sets.
Yeah, yeah, yeah.
Right, so could you tweak this to possibly just look for hypotheses that could be answered on a single data pointer
rather than a comparison between data points from different data sets?
Yeah, so we actually, I think, worked with both versions of the system,
one that is kind of unary predicates and one that's binary predicates.
I think in practice, a lot of the interesting things you want out are kind of binary predicates.
So you can get, like you can get somewhere with this unary thing,
but you're kind of losing something if you don't consider comparatives.
Okay, let's thank the speaker.

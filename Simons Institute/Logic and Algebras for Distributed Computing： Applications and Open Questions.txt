Second talk today has a title that has evolved
since last time I looked at me.
Okay, but it has logic and algebras
and we are very excited to hear what this has to do
with cloud computing,
which we are all subjected to in our daily lives.
Thank you.
So this will be a bit unusual,
both Joe and Connor will talk
and I let them take care of the logistics of that.
Again, remember that for questions
that might be more appropriate for a longer discussion,
we will have that discussion right after their talk.
Okay, thanks.
So this is work that obviously we're doing here at Berkeley
and I'm also funded by Sutter Hill Ventures.
So it's kind of cool.
We've got venture capitalist funding, basic research.
I have promised them that there's no applicable,
I'm not really sure what this could turn into as a company,
but they're cool and they've given us developers
to work on the project,
which has just been great and they're funding me as well.
So thanks to them and to Berkeley.
There's this story people like to tell in computing.
This is my standard opening slides.
Operating systems people really like this story
because it's sort of the Thompson and Richie
touring award story for every platform that comes out.
There's a programming environment
that's somehow suited to that platform that emerges.
And as a result, people write things
you never would have expected on that platform
and it succeeds.
So the PDP 11 with Unix and C is the canonical example of this,
but one can argue that in every generation
of new kind of computing platforms,
programming environments have arisen
to allow people to build an app for that, right?
And so nobody expected all the apps we have on our phones.
It's wonderful, developers were freed
to write all sorts of things.
Strangely, there's a platform that's as old
as the iPhone called the cloud, all right?
So AWS is approximately the same age as the iPhone,
but it doesn't have a canonical programming model.
And there's many reasons why that might be,
partly because it's a really hard programming environment,
yes, so it has to deal with all the problems
of parallel computing, as well as things
like distributed consistency, what happens
when you have partial failures in your system,
but it keeps running, so componentry is down,
but the system's still running.
And then in the modern cloud, we want things to auto scale.
So you allocate more machines and then you free up
some machines, but the program's still running, right?
So the platform is changing underneath you
as you're executing, yeah?
So this is all hard and programmers right now
are trying to do this essentially in Java, right?
That's sort of the state of the art.
And the annoying thing is these compilers
for these languages don't answer
any of these questions that are hard.
So I think, honestly, this is like this hole
in computer science that nobody's filled,
and it seems like one of our grand challenges
from my perspective.
So I've been working on it for a long time,
and I think there's still a lot of work to do.
I take inspiration, of course, from this gentleman
as maybe we all do.
What was cool about Ted Kod was he said,
look, you should write things in a formal language.
You should have formal specifications.
And then there should be machinery
that automates the implementation.
And if we do that, then the implementation
can change while the specification remains the same.
This is very nice for things like databases, right?
So the thing is that Kod was trapped
in this database prison for all these years,
and I think there's a much broader applicability
of the design principle.
So we worked on things in our community,
like declarative networking.
So we brought Kod out of the database and into the network.
So I've done some work on that.
Many of us, I think in this room,
have done something around declarative data science
and machine learning.
This is a growing area, right?
In the program analysis community,
the use of declarative languages has been pretty powerful.
So that's really cool.
And then, of course, the hot thing,
which is why we're all here,
is that we're gonna start to try to look at this stuff
through the lenses of algebras instead of logic,
or in addition to logic, which is pretty neat.
And we've heard or are hearing
about a variety of different algebras
that people are playing with in this domain.
So what I'm interested in is taking Kod into the cloud, yeah?
And here's sort of the analogy, the way to think about it.
The relational database was invented to hide
how data is laid out and how queries are executed, right?
And all that should be decided sort of lazily
based on the current environment.
Well, the cloud is just a generalization.
It was invented to hide all your computing resources
and how they're laid out.
So not just your blocks on your desk,
but really everything.
And it's for general purpose computations.
So the cloud, in a lot of ways, is this abstraction.
It's this physical layer abstraction.
The physics of the deployment of your code is gonna change,
but you want your spec to remain the same.
That's how you'd really like to program
in an environment that is this heterogeneous and elastic.
So I believe that it's extremely natural
for techniques that we've been working on in our community
to try to be applied to cloud computing.
And we have a project in my group called Hydro,
which you can read more about,
which I will tell you a bit about today.
Okay, so what are my goals?
Well, I kind of wanna build something like LLVM
for the cloud.
So LLVM, as you may know, is a very successful
sort of language stack.
It supports many languages,
including C++ and Rust and Swift and others.
It has an internal language called its internal representation
and then it compiles down to a variety of machine code
for different platforms.
So it's been extremely successful,
but it doesn't answer any distributed questions.
So if you're writing a distributed program,
you might ask a question like,
is my program consistent in some sense?
Or if I talk to different machines in the network,
will they give me different answers and be confused?
All right, that's a question
that distributed programmers need to deal with.
Here's another one.
My state no longer fits on one computer.
How do I partition it across multiple computers
while getting the same answer out of my program?
All right, I want you, you compiler
should figure that out for me.
What failures can my system tolerate
and how many of them before it stops working
the way that the spec declares it should?
What data is going where around the world
and who can see it, right?
These are all questions distributed systems
always have to answer.
And then, you know, I have different objective functions.
So I'd like to optimize some days
for maybe my dollar spend in the cloud,
but I don't care about latency or maybe vice versa.
Maybe I care about particular latency distribution.
So I want the 99th percentile of my workload
to achieve a certain latency versus the 95th.
These will all lead to different decisions
about resource allocation and program structure
and so on, right?
And if you ask these questions of LLVM,
you know, that's the answer you get, right?
It just, it doesn't deal with any of these issues.
And that's kind of where we'd like to come in.
We've written a vision paper a couple of years ago
that I can point you to,
and I won't go through all of it today.
But the idea is to use database techniques
and ideas to optimize in concert with LLVM.
So LLVM is sort of responsible for the single node,
but database techniques perhaps responsible
for the messaging, the data movement
that happens in a distributed program.
So here's how we envision the hydro stack,
many programming languages up top,
some techniques to translate them
into an internal representation.
We've got a little bit of initial work here.
And then the internal representation
should be some formal spec that is global in some sense.
So it doesn't worry yet about how many machines I have
or what the machines can do.
It's just a formalized specification
of what you wrote in a perhaps imperative language.
Okay, so it's kind of machine oblivious.
Maybe it's a logic, maybe it's an algebra.
Things that are in red are work in progress.
Things that are in green kind of work
at this point in our, in our efforts.
And then from there, we wanna build a compiler
and we're working with Max
on using eGraphs for that compiler
to translate down into a per node physical algebra.
So every machine would run its own little program.
Those programs communicate with each other
very much like a parallel query plan
as a bunch of individual query plans
running on individual machines
talking to each other over a network.
Okay, so this is a sort of per node physical algebra
because it's actually doing stuff.
We've implemented this in Rust, it's very fast.
And I'll show you some of this today.
So that's kind of what we envision
as how this is all gonna work.
At the bottom, there's something that's deploying this
on machines and deciding how many machines
and how few over time.
Okay, so some of the topics I wanna talk about today,
we're gonna focus on this piece of the stack.
Things in red are work in progress,
very much more questions than answers, for sure.
So how do we take code and automatically replicate it
along with its associated state or data
while ensuring that the program continues
to produce the same outcomes
as it would have on a single machine?
So I'm particularly interested in doing this in cases
where the replication comes for free
and the individual machines don't have to coordinate
with each other in a technical sense I'll talk about.
We'd like to avoid replication when we can,
we'll call that free replication.
And this is the domain of the Calm Theorem
which you may have heard of and I will review.
Unfortunately, the Calm Theorem was done
in a very particular framework for the proofs.
It's not at all clear how it applies outside that framework.
And so what we'd really like is a more algebraic notion
of the Calm Theorem, which is something
that Connor's working on.
And after the talk, if you're interested,
come find Connor and or me to talk about that.
Another topic that Connor's gonna talk about today
is termination detection.
And again, ideally termination detection
where I can decide it locally for free
without asking anyone else.
So how do I know in a streaming program that it's done
when there's other agents in the world?
So we're gonna talk about how to do that
with threshold morphisms, but Connor's got ideas
about more general notions of equivalence classes
that may allow termination in more settings.
So he'll give you a flavor of this work in progress.
The third piece we may or may not have time for today
is what my student, David Chu, has been working on this.
How do you take a program and partition the state
of the program across machines
if the state doesn't fit on one machine?
So you really need to partition the code and the data.
This is very much like traditional shared nothing parallel
databases, if you like,
but we wanna do this to full
and rather complex data log programs.
And so we've got an implementation of Paxos,
which is quite a number of lines of data log
where David's able to do some
of this automatic partitioning.
Functional dependencies have a role to play here
and it would be nice to integrate those
into an algebraic frame as well.
And of course, all this has to go fast
and ideally as fast as handwritten C++.
I'm really previous iterations of my work,
we settled for interpreters
and just showing things were possible.
Now we'd like to convince the systems people
that things will be as fast as they want them to be.
So is this business just pie in the sky?
Let's see, all right.
So we're building on an obsession of some 10 to 12 years
that I've had now in my third group of grad students
working in this area.
So the initial batch of grad students was doing formalisms.
So we have this very nice logic called deadless,
which is a subset of data log bag, actually,
that allows you to talk about time and space
in a particular way and it's got a nice model theory.
And so that works all there
and we can use that as a basis for our semantics to start,
which is nice.
And then there was this lovely work
that Tom Amalut did at Hasselt, the column theorem,
which was a conjecture I had
and he and colleagues went ahead and proved
that talks about how things are monotone
in some certain sense.
Then you can do this free replication.
So you don't need to do coordination
to get replica consistency.
So I will talk about the column theorem today
to give a review.
And then we actually built out a language,
it was slow, it was interpreted, it was written in Ruby,
but it integrated lattices into a data log-like language
and we were able to show
that you can get stratified negation and aggregation,
you can get morphisms on your lattices
to allow you to do semi-naive evaluation
even with the lattices.
So it's really actually rather a nice mixture
of algebra and logic.
None of this was formally proved,
but it was, I think, one of the earlier systems
to observe that you could build this.
So that was pretty cool.
And that's Neil Conway's thesis work.
So we have this as a basis,
this kind of ground zero for my team.
And then what happened is I had a batch of students
who didn't wanna do languages or theory.
So they just built stuff in the spirit of these ideas.
And I'm not gonna go through all this,
but it's things like functions as a service
and protocols and testing and provenance.
And it's cool stuff.
What I do wanna focus on is one of those projects,
which was a key value store.
Key value store is just like a hash table, okay?
It's a database where you look things up by key
and you get a value.
So you can think of it as a distributed hash table.
These are like the Petri dishes of distributed systems.
You're basically saying, I have a memory.
It's distributed across many computers.
I may be replicated, it may be partitioned,
but that's what I have.
It's just a registers with values, keys with values.
Yeah.
So there's no algorithms per se.
It's all just kind of like focusing on these semantic issues
about replication and partitioning.
But the idea that was behind the Anna key value store
that we built was everything's a semi lattice.
And because everything's a semi lattice
and therefore associative, commutative and item potent,
messages in the network can be replicated.
They can be interleaved.
They can be reordered and everything will be fine.
So if you design with semi lattices from the bottom up,
you can build a system that does no coordination.
So it's fully monotonic, which means everything
can be replicated as much or as little as you like
across the globe, between disks and memory.
You can replicate lots of ways.
You can do updates anywhere.
So multiple updates to the same key
can be happening concurrently in different places
at the same time.
And they are merged by the lattice lazily via gossip.
And so you build this thing with no concurrency control.
So there's no locks in the system.
There's no calls to atomic instructions on processors.
There's certainly no PAXOS protocols or anything like that.
It's just put, get and gossip.
It's really a simple piece of software.
And so Chenggang Wu, the student who led this,
won the dissertation award.
I think it was very well deserved
because this system was really elegant and really fast.
So to give you a sense of kind of the lattices that he uses,
he took from the literature a variety
of these kind of consistency models
that people talk about in distributed systems.
And he showed how you can get them
by building little composite lattices that wrap up the data.
So this is what's called last-rater wins
in the distributed systems, which is just
whenever you see a value that's from later than your value,
you update, otherwise you don't.
And this you just take your map, which is from keys to things.
And you wrap the things in a lexical pair of a clock
or a version and the data.
And you can only have one value per version.
So this works out as a lattice.
Here's a fancier one, though.
This is actually one of the strongest forms of consistency
you can get without coordination.
It's called causal consistency.
And here, what you have is for every key,
you have a vector clock and the data.
And the vector clock itself is a map lattice
with node IDs and counters.
Yes.
Just wanted to say a little bit of sort of operational
transform kind of.
A little bit.
Yes.
Can you ever get stuck like, do repairs always exist?
Or do you set it up such that they do?
So these particular well-trodden forms of consistency
work fine.
And these lattices are capturing that.
They're saying, look, it's just merge.
And it's always going to work because you've
defined an associative commutative item potent merge
function.
OTs are really weird and full of all sorts of reasoning
I don't understand.
And they would never be able to have such a simple assertion
of correctness.
All I'm saying here is it's associative commutative
of an item potent and I got nothing more to say.
It takes a little bit of convincing
to say that gives you causal consistency,
but it's not much convincing because it's
helping you make causal consistency with vector class.
So the observation that clocks and vector clocks are lattices
is just a nice thing about distributive programming.
Yeah?
So what do you mean by everything into lattice?
So you're making that Kira's story just a hash map.
Right.
Every key has to be lattice thing.
So the nice thing about the map lattice
is the keys are not lattice values.
The keys are just keys.
The whole table is a lattice.
The object is a lattice because what happens
is the merge function is for a particular key.
If there's nothing, it gets the value you gave.
Or for that key, if there's something there,
you apply the merge function of this lattice.
So that is itself a lattice.
And these lattice constructors are very nice.
We use map lattice.
We see lexical pair over there.
And these allow you to think simple lattices,
like sets and counters, and build up richer lattices out
of them, which is a trick our group likes to play a lot.
Other groups sort of are doing research
on inventing custom lattices for custom problems.
We've been very much in this kind of know.
Let's just build it up from very simple building blocks.
Why do we need a lattice?
So the quick version is it's monotone.
And if it's monotone, the common theorem
says it's going to be free replication.
So you don't have to do coordination
to get replica consistency.
Connor's going to give you a longer talk about this
when we get to a conversation about semi-lattice.
It's a semi-lattice.
I should be clear.
It's not a lattice.
It's a semi-lattice.
Do you know whether the people working
on a coordination-free replicated data structures
are aware of the overall work?
Yes, they are aware.
And Connor will talk about it soon.
Yeah, that will come up for sure.
Yeah, good.
So just to kind of close out this anecdote with Anna,
the system is ridiculously fast.
And it's especially ridiculously fast under contention
relative to other systems.
So we compared against things like Mass Tree, which
is from Harvard.
It's Eddie Kohler's very fast key value store.
We also compared against the multi-threaded hash table
that comes from Intel in their Thread Building Box Library.
That's TBB.
And under contention, those systems, if you look down here,
spend most of their time trying and failing
to get atomic instructions.
So they'll say test and set on a particular memory address,
and they'll be told, no, you have to try again.
And they'll spend 95% of their time
under contention doing that, not doing useful work.
So they're at 5% good put, if you're familiar with that term.
Whereas Anna, because it does no concurrency control,
is just doing puts and gets, and puts and gets,
and puts and gets, and spending most of its time doing good put.
And that's why Anna can be 700x better throughput
under high contention than these other systems.
But also, because it does no coordination scales almost
perfectly linearly across threads,
and then across machines, and eventually across the globe,
there's really nothing to keep it from scaling linearly,
because the only extra work it has to do is some gossip.
And that can be done in the background,
and can be done as lazily as you like
without breaking the semantics.
So there may be a little fudging here
on how stale your data is, but it's correct.
So this was a crazy fast system.
And the thing about this, oh, and if you try to run it
in the cloud, it's also incredibly cheap to run relative
to systems that are wasting all their time doing this business.
They're charging you for this, right?
They're trying to get locks, they're waiting on locks,
and they're charging you money.
So you'd like to avoid that if you can.
Okay, that's all very nice, but it was written in C++
by Chenggang, who's an excellent coder.
His implementation is correct by assertion.
It would be really nice to be able to kind of
do what CAD wants us to do, formalize a spec that is correct,
and then synthesize an implementation from it
through rules that are correct transformations, right?
So we'd really like to do that,
and we'd like to maintain the speed.
What kind of formalisms?
Well, you know, we're using lattices mostly,
so maybe we could have a type system
that starts with some basic semilattices,
like sets and counters, some composition lattices,
like key value pairs, products, lexical products,
which are always lattices, so you have to,
there's some constraints on when a lexical product is a lattice.
And then we want like a data flow,
like a query plan algebra.
So you can imagine a semi-ring kind of algebra,
but you know, there's gonna be maps and folds,
and then there's gonna be physical stuff,
like scan a collection, or get stuff over a network.
Networks do weird things, like they permit things,
and they form batches of things.
They parenthesize streams, if you will.
They multiplex and demultiplex messages.
So there's some physical stuff that we want here too,
and I'd like to really be able to prove all that stuff
is correct in a meaningful way.
So just for fun, I don't expect you to read this.
This is the ANA implementation.
You just saw it written in our low-level hydroflow language.
This is the whole thing.
It's a very simple program.
And you can see this is kind of a data flow language.
It's basically specifying graphs of data flow.
The edges are directed edges in a graph.
The words are nodes in the graph.
And you'll see familiar operators like map and join,
and cross-join, and so on, all right?
And you can give views names.
So this is a little name of a subgraph,
and we use it, and so on.
So it's just a little language for specifying graphs.
This is a picture of that program
that's output by the system, okay?
So it's a one piece of paper program.
And then the semi-line separations in the program.
So in this particular program,
union is actually joined, it's semi-lattice join.
And that might be the only one.
Yeah.
Okay, and so just to convince ourselves this is fast,
that's Chengang's original numbers.
We have it now running through hydro,
that implementation you saw on very similar machines,
and we get very similar performance
to hit the handwritten code.
So we're feeling pretty good
that we're hitting our goals for performance.
And because this graph is green,
it's telling us that this thing is all monotone,
and therefore consistently replicable.
And at a glance, we can see this is a safe program.
And I'm sort of cheating at this point.
And I'm gonna confess to that,
there's I think more work we need to do to make this robust.
I think these green edges are kind of a,
they're slightly bi-assertion at this point.
So I would like to make them more fundamentally corrected.
Hopefully we'll have time to talk about that later.
Okay, with that, I'm gonna hand off to Connor,
he's gonna take us through the next chapter.
Hello, people hear me?
Yeah.
I'm Connor, I'm a PhD student here, working on hydro.
I like systems and theories,
so I thought I'd show you guys some of the theory stuff
we've been thinking about,
see if anyone has any thoughts,
wants to collaborate on anything.
Is this thing?
Okay, so in the classical database lens,
we have these three layers,
the relational calculus at the top,
relational algebra in the middle,
and then physical algebra at the bottom,
concerned with things like hashing and sorting and so on.
And we can think about how this changes
when we move to the cloud setting.
And there's good news and bad news
on the current state of affairs
when we move to the cloud setting.
The good news is that, like Joe said,
at the top we have this Daedalus language
from the Bloom project,
that is a Daedalog-like dialect for distributed systems.
The bad news is that developers are not asking
for a Daedalog dialect to build distributed systems in.
The developers we've talked to
are a lot more interested in a functional,
algebraic-looking interface,
and especially something pythonic-looking like pandas.
On the algebra side,
the good news is that there is an algebraic model
for distributed systems today.
It's the semi-ladis model that Joe has mentioned
that is referred to as CRDTs in a lot of places,
especially in the programming languages community.
The bad news is that this is a model
for coordination-free updates of state,
and it doesn't actually have a query language
or give guarantees about coordination-free-ness
of queries today.
And then at the physical layer,
when we add a network to the situation,
an asynchronous computer network,
a lot of non-determinism emerges
that we need to be able to handle,
in particular, reordering, batching,
and duplication of messages.
So what we'd like to get to is unifying formalism
across logic and algebra and this physical algebra,
and have correctness at the physical layer
that we can prove for safe against
this non-determinism from the network.
And we're able to capture things like replication,
partitioning, batching, or criminalization,
and termination analysis,
we'll talk about more later.
All right, so let's talk about semulattice's CRDTs.
So this is a model for distributed systems
that in databases we usually call the semulattice model,
so it's called an ANNA in Bloom-L.
It came out of the programming languages community,
and there it's usually referred to as CRDTs.
It stands for Conflict-Free Replicated Data Types.
It's introduced in this paper here,
and there's over 179 papers about CRDTs out there.
It's also started to get popular amongst software engineers,
and you see people talk about this CRDT model
on places like Hacker News,
people starting startups with it.
So what does it do?
It tries to handle the sources of non-determinism
that come from a asynchronous computer network.
These are the arbitrary batching of messages,
arbitrary reordering of messages,
and arbitrary duplication of messages.
And so it turns out,
if you want to be robust to these three things,
these actually correspond to algebraic properties
that you need to give you that robustness.
So associativity gives you robustness to batching.
You're indifferent to the parenthesization of messages.
Communitivity gives you robustness to reordering,
and item-potence gives you robustness to duplication.
If you have a set with an operator
that satisfies these three properties,
that gives you a semulattice.
So that's why we're talking about semulattices
for distributed systems.
So the Conflict-Free Replicated Data Type
is one specific model of a semulattice interface,
but since it's the most popular one today,
I'm gonna talk about it.
So the idea is that it's an object-oriented view
of a distributed system where you define some object,
and you're gonna define three methods on that object.
And then you can replicate this object
across the distributed system,
and the replicas will converge,
regardless of network non-determinism.
So you have your merge operator,
which is your associative, commutative,
and item-potent ACI.
Semulattice operator,
which combines the state of two replicas.
You have a way to update state,
which the requirement is just that that's monotone
with respect to the partial order
induced by this merge operation.
And then you have a query,
which is just a method on this object,
but today there's not a specific query language.
You don't have any sort of guarantees
on what that query does.
It just reads this semulattice state.
So we look at an example of a CRDT.
This comes from the Amazon Dynamo paper
for how they're implementing shopping carts.
And the idea is that you have updates
that are gonna add or remove elements to your shopping cart.
In this case, you add a Ferrari to your shopping cart,
add a potato, and you can also remove the Ferrari.
And the state is gonna be represented as two sets,
a set of items that you've concerted
and a set of items that you've removed.
And then to merge, we do a pairwise union of these two sets.
And the query, what's actually in my shopping cart
that you wanna check out, is set difference.
Subtract the removes from the inserts.
And there's a few interesting things going on with this example.
One is we're guaranteeing the coordination free rights
that these two states are gonna converge.
But our query is a non-monotone query,
which the column theorem tells us
is not a coordination free query.
So CRDTs are not giving us the invariant
that the column theorem requires on queries,
which is that if we output a tuple at a certain point in time,
we're never going to retract that tuple in the future.
Here, over time, we will retract tuples
as the remove set grows.
We had a vision paper in BLDB this year
about this gap between what CRDTs guarantee
and what the column theorem guarantees
and ideas for how to resolve it.
Another thing you might have noticed
that's kind of odd about that representation of data
is that if you think about how we might represent
updates like this to a shopping cart in a database,
you might have imagined that we would have a count on each item
and we would increment that count when we add an item
and we'd decrement that count when we remove an item.
This is what you'd see,
something like increment all view maintenance
where your update operation forms an abelian group,
not a semi-ladis.
So why not do something like that?
Well, for one, it doesn't form a semi-ladis
and it's not immediately obvious how to convert it into one.
So this representation is two sets,
what you call a two-phase set,
is more obviously monotonically growing update operation.
But it turns out it actually is possible
to convert this abelian group representation
into a valid semi-ladis
in terms of being robust to network non-determinism.
I won't go into all the details on that,
but it's based on what Joe was saying
with these vector clocks where you wrap
the states basically in a vector clock,
which forms a semi-ladis.
The downside of doing this is that vector clocks
require linear memory and the number of replicas in the system.
So that's the reason why people
wouldn't use this representation today.
But we have some work on a protocol
for enforcing this kind of
conversion into a semi-ladis in constant
rather than linear space.
So if anyone's interested in that idea,
definitely come find me and talk about it.
Okay.
So, like I said, the semi-ladis model today
does not have a query language on top of it.
So what might we want out of a query language
for the semi-ladis model?
Well, we want expressivity.
Like we saw in the shopping cart example,
we need set difference, so we need negation.
Also recursion, something like ThetaLog.
We also want obviously classical query optimization options.
We want identities that we can
use to transform our query, get better performance.
And we want to be able to do monotonicity analysis,
as well as functional dependency analysis for partitioning.
And so something like, you know,
ThetaLog.0 for semi-ladices might be a good fit here,
but there's a lot to explore.
And so now Joe is going to talk about
this monotonicity analysis and functional dependency analysis.
All right.
Thanks, Connor.
And I should say, you know, from the previous slide,
that some of this is things we took a crack at with BlueMal.
So it's not that we've done nothing here.
There's some answers to these questions.
But there's also work to be done.
All right.
So I wanted to step back and review for you folks
the Calm Theorem, which I know is sort of in a sub-corner
of the pods community, and not everyone's
going to be familiar with it.
But I think it's useful to go over
the questions and see if we can get
familiar with it, but I think it's useful to go over.
This will be high level, but hopefully helpful enough
for you to get into the game.
So the challenge is that we're going to have computers spread
across the globe, and we want our replicas to be consistent.
So we have this nice couple here.
They're in different places.
And the classic example of replica consistency
is data replication.
So forget about programs.
We're just going to have data, kind of like the CRDT model.
And I want everybody to have common beliefs about the data,
at least eventually.
So these two folks currently both believe that x is love,
which is lovely.
But if it's a beautiful variable, things could change.
And that's very sad.
And once they disagree on the value,
they might make decisions based on their disagreement
that will lead to further divergence.
This is sometimes called the split brain problem,
because you can't put it back together later on.
It's too messy.
And so we want to generalize the idea of consistency of data
replication to consistency of program outcomes.
So I'm not just interested in the data.
I'm interested in the queries, if you will, right?
Much more powerful, and it will allow us to cheat sometimes.
The data could be inconsistent if the query outcomes are not.
So it'll give us more ability to relax our coordination.
So we'd like to generalize this to program outcomes independent
of data races when we can.
The classical solution to this stuff is coordination.
This is what things like Paxos and Two-Phase Commit
were invented to solve.
And the way they solve it is by saying,
what if we were just on one computer with one processor?
Maybe we could implement that in a distributed fashion, which
is a very heavy-handed solution, right?
You say that our solution to parallelism
is to remove it.
And how can we remove it in the distributed context?
Well, it's expensive.
But here's how it goes, right?
In a single node, you use atomic instructions, right?
So if you have shared memory, you
can use atomic instructions.
Or maybe you use a locking system.
In the distributed environment, you
use something like Paxos or Two-Phase Commit.
And at every scale, as you saw in that ANA work,
you'd like to not do these things.
So even on a single machine, you really
don't want to be doing coordination.
And certainly in the distributed setting,
this is very heavy weight.
And there's people who will tell you at great length
why they don't let the developers in Amazon
call these libraries unless they have 16 gold stars.
Because it will slow down the whole environment
and create queue backups and all kinds of horrible things.
So when can we avoid coordination?
This was a question that I asked as a lazy professor,
because I was thinking maybe I should learn and teach Paxos,
and I kind of didn't want to.
So I was like, maybe we don't need this stuff.
Maybe Lamport's just a bunch of bunk.
So that's kind of where this started of sheer laziness,
intellectual laziness, which I will cop to.
But what it led to, sometimes when
you ask a question about how can I be lazy,
you end up asking a question that
turns out to be quite interesting.
I think that's what arose here.
And I'm seeing this not only in my work, but in other places.
Back in the 20th century, if you will,
the Lamport Gray era, we were trying
to emulate sequential computation.
We were doing everything we could
to give the programmer the illusion
of a sequential computer.
And it was all about very low level stuff, reads and writes,
accesses and stores.
And then guarantees of order, total order, linearizability
and serializability.
And this was all based on the idea
that programmers are hopeless.
They'll write all kinds of crazy code,
and the only thing they understand
is sequential computers.
So we'll make the worst case assumption
that their stuff wouldn't work in parallel.
And we'll give them mechanisms for avoiding parallelism.
Seems like a terrible thing to do in a parallel environment.
So what's happening in the 21st century
is if we lift our, so this is all great.
And sometimes you need it.
I don't mean to denigrate the work.
This is obviously foundational.
Touring awards, I use this stuff.
I teach this stuff.
It's all good.
But when we don't need to use it, even better, right?
So people have tried doing things
like what if all our states are mutable?
That's a very functional programming game.
In my world, it's more about, well,
you can mutate things as long as it's monotone.
So if they're mutable but they're monotone,
maybe that'll work.
And then using things like dependencies in provenance,
all our ways of using application knowledge
to avoid using the expensive stuff on the left.
But the really big query is, when do I need coordination
and why do I need coordination?
So if you ask a typical undergraduate or, frankly,
most people in computer science, including professors,
when do you need coordination?
What's a lock for?
They'll say, well, it's to avoid conflicts
on shared resources.
This intersection needs coordination.
If you would just put up some damn stop lights,
then North-South could go for a while
and East-West would wait.
And then East-West would go for a while
and North-South would wait.
Problem solved.
But do I really need coordination?
That's a solution.
Is it the only solution?
No, it's not the only solution.
Here's a coordination-free solution
to that intersection problem.
So I'd like to be able to think out of the box
and say, really, what is coordination for?
When am I required to use it?
So that's a theory problem.
So which programs have a coordination-free implementation?
Call those the green programs.
These are specifications for which a clever programmer
can find a coordination-free solution.
And then, of course, there's the rest of the programs.
And I want to know this green line.
Will someone please tell me, Mr. Lamport,
I think I only need you out here.
So will you please tell me when I need you?
And there's no answer from Mr. Lamport.
At least he didn't pick up the phone when I called.
But I'm happy to say that people at Hustle did.
And this is what led to the column theorem.
So this is really a computability question.
What's the expressive power of languages
without coordination?
That's the green circle.
So give you some intuition.
Easy and hard questions.
Here's an easy question.
Anyone in the room over 18?
Not only were you all happy to answer that coordination-free,
but you engaged in a little protocol, right?
You made up a protocol where you raise a hand
if you think it's true.
So that was cool.
So that was the monotone hand-raising protocol.
All right, who's the youngest person in the room?
Oh, we have some brave assertions.
But clearly, you don't know that.
You could look at everyone, but that's cheating
and also not necessarily right.
Maybe, maybe, I don't know.
But the point here is that somehow this
requires you to communicate with people.
And the first one maybe doesn't.
OK, more to the point.
Let's look at the logic here.
This is an existential question.
And this is a question with the universal quantifier in it.
Or for people like me who just want
to do total pattern matching and look for not symbols,
that one appears to be positive.
So I'll say that it's monotone.
And that one appears to be negative.
So I'll say it's non-monotone.
So it gives you some intuition that universal quantification
or negation requires coordination.
It is coordination.
That's what coordination is.
It's universal quantification.
So what is Lamport for?
It's for universal quantifiers.
So let's just prove this, right?
I was like, well, somebody prove it.
I'm not going to prove it.
So nice guy named Tom Omelette wrote a thesis on this stuff.
My conjecture was called the calm conjecture.
Consistency is logical monotonicity.
It was in a Paz keynote that I gave some years ago.
And then just a year later, there was a conference paper
from the good folks at Haaselt, which then they extended
and then was further extended with weaker definitions
for the monotonicity to really expand the results.
If you want a quick, you know, kind of a version
of what I'm saying now, you can read this Kackam overview,
but it's really for systems people.
I think you guys should just read Tom's papers.
All right, to give you a flavor of what Tom did,
definitions are half the battle.
It seems, you know, when I read Paz papers,
that's all the hard parts are the definitions, right?
So, you know what monotonicity is in logic, that's fine.
What is consistency here?
Well, we want the program to produce the same output,
regardless of where the initial data is placed.
So I should be able to start the program with the data
replicated pops possibly and partitioned in any way
and get the same answer.
And if that's true, then it would be the same answer
across replicas, it would be the same answer
across different runs, it would be the same answer
if we start gossiping the data between each other.
And this is what we want, right?
So that's our definition of consistency,
where I think what's really clever
and was the most beautiful part of the work
is defining what coordination really means.
So we're sending messages around, right?
That's data, but which data is really data
and which data is kind of control messages?
How do you differentiate those in a formal way?
And so what they define in this paper is,
program is coordination free if there's some partitioning
of the data when you first start.
So there's some way out of the data where you first start,
such that the query can be answered without communication.
So for a particular query, for a particular data set,
there is some world of where you lay it out
where no communication is required.
Okay, that's the definition of coordination freeness
and a program that you can't do that on
is doing coordination messages.
Okay, so it's not really saying
which messages are coordination
and which messages are data,
but it's telling you which programs can be run coordination.
Just put all the data on one node.
Yes, so the trivial example of this
is you put all the data on one node.
And again, you know, this question of,
is there anybody who is older than me?
If what you don't know is whether anyone else has data.
So I may have all the data, but I don't know that.
So I still have to ask everybody, anybody got any data?
And I have to wait for everybody to respond, right?
So it's very nice intuition
to just think about having all the data.
All right, there's another thing in the paper
that I hadn't even anticipated, which is really beautiful
and speaks to stuff
that the distributed systems community kinda knows,
which is there's a third equivalence,
which is that you can distributively compute this program
on an oblivious transducer.
And I haven't even talked about transducers yet,
but just a minute.
But what does it mean by oblivious?
It means that the agent in the system
doesn't know its own identity.
It cannot distinguish messages from itself,
from messages from anyone else.
So it doesn't actually know who itself is
and it doesn't know the set of participants.
I call this an oblivious agent, right?
Oblivious programs that can be computed by oblivious agents
are exactly the monotone programs
and exactly the coordination free programs.
So that was very cool.
And it speaks to questions of like membership protocols
in distributed systems,
which is about establishing common knowledge
of the all relation.
That's like one of the things that Paxos does
is it has a membership protocol built in.
So it's one of the reasons it's coordination full
is to establish all.
So this was really, really nice.
So this is all in this JACM paper.
It's actually in the conference, the Paz paper as well.
That's just a flavor of the calm stuff.
And I'm going to stop with that,
but happy to answer questions as best they can afterwards.
And with that, I'm going to give it back to Connor.
You guys can still hear me?
All right.
So we have this calm theorem view of the world
relational transducers, logic, operating over sets.
And then we have this semi lattice algebra view of the world.
And they both are dealing with coordination of freeness
in different lenses,
but currently they guarantee different things.
The algebra view, like we said,
is concerned with the coordination of freeness of rights
and does not guarantee coordination of freeness of queries.
Whereas the calm theorem view
is only concerned with the coordination of freeness
of queries, it actually doesn't have to worry
about the coordination of freeness of rights
because it assumes you're operating over sets
and gets coordination of rights for free that way.
And so we're interested in the question
of how can we combine these two lenses?
Can we do an algebraic view of the calm theorem?
And some intuition for how that might work
is the semi lattice operator induces a partial order.
And so instead of having monotone logical queries
without negation,
we have a monotone functions between lattices,
between partial orders.
So that's something we've been exploring.
We'd love to chat more about it with folks.
I'm actually gonna talk about a problem
specific to the question Remy asked.
It comes up in this setting.
So the calm theorem is all about basically not knowing
when you have the entire input.
What can I output and tell downstream consumers
with certainty, even though I might have more updates
in the future, there might be more messages arriving.
And so we call the ability to do this free termination.
Without coordination,
what can we be sure that we can output?
And we're exploring this in a very generic setting
of just we have two functions,
an update operation that's gonna change our state over time
in a query that's gonna return some output.
And so looking at an example of when we might be able
to do this, we can look at this lattice.
This is the set over these four socks.
And say that this is our state of a local node
and we're at top in this lattice.
And our update operation in the CRDT sense is union.
So we're gonna union in more socks.
We know that if we're at top,
this update is monotone,
we'll never change our state.
We're stuck at top.
And so whatever query we might be asking
when we're in this state,
we'd be able to locally detect
that our query result is not gonna change in the future.
And we'd be able to return our result with certainty.
This might sound like it would not happen
particularly often,
but let's try and look at more examples
where we would be able to figure out that
with certainty, we can return an answer right now.
So what if we consider also
the query that we're asking
and say that this query is gonna map us
from this set socks lattice
to the boolean lattice true false, where top is true.
Now, if this query is monotone,
meaning as we go up in the partial order of socks,
we also go up in the partial order of false and true,
then we don't need to be at top on the left.
We can actually be at top in the true false lattice
and guarantee that our result won't change
as future updates arrive.
Any update that arrives is gonna cause us
to increase monotonically in the domain,
which has to increase monotonically in the range
and therefore our result will always stay true.
For example, a query, is there a pair of socks?
So we call a threshold query.
It effectively draws a cut through this partial order
and says everything above this line in the partial order
is true, everything below this line is false.
So these boolean threshold queries
are a class of queries that we can freely terminate
on if we know that our update is monotone.
What about a totally different setting?
What if we throw away monotonicity?
So now imagine that we have a deterministic finite automata
and our query is mapping to true and false
from accept and reject states in the automata.
And our update is appending a character.
So we think of a streaming characters appending
to this automata.
So each update is going to transition us one step
in this automata.
And in this automata, any state that we're in,
we can't conclude what the final result will be
because from every state,
there's a state that's accepting that's reachable
and there's a state that's rejecting that's reachable.
So some sequence of future updates might take us to false,
some sequence of future updates might take us to true.
In contrast, if state three were also true,
now from state one, we actually don't know
if we're gonna end up accepting or rejecting
if we don't have the whole input yet.
But if we're in states two or three,
we know that every state that's reachable
via future updates is gonna keep us
in our current result, which is true.
And so we can be certain that we can
terminate here and return true.
So this is kind of like a reachability sort of visual view
of how we're thinking about whether or not
you can freely terminate given some arbitrary
update operation on a domain and query operation
that maps you to a range.
This is a question I'm interested in exploring
a lot of different domains.
If anyone has any ideas for what might connect to this,
definitely come find us.
And now Joe is gonna talk about partitioning.
All right, this is a bit of a survey time.
Boris, did you wanna ask a question of color?
I ask a quick question to the last slide, right?
Because in the census,
now I was telling us in one of those ordering, right?
This kind of like, yeah, okay.
In a sense, it's kind of like if I can reach a node
and it's smaller, and now basically the nodes.
If the terminal nodes are the top nodes
and they don't have larger nodes,
then so it's still monotone in the sense.
Yeah, you can find some sort of it that, yeah.
I don't know if that's true for every
fairly terminating function.
Psychologic.
Psychologic.
Yeah, maybe.
There's something about quotient lattice is too, going on.
Any partial orders, but.
Yeah, the graph looks funny, but I can come on.
So this is, I'd love to have this conversation afterwards.
That's why we're touching on a few things
so we can have many conversations.
So I think given time,
I'm not going to go through this in any detail.
I'm gonna basically skip this chapter of the talk,
except to quickly give some assertion.
So first of all, we don't have HydroFlow,
so we use Daedalus,
and we do have a full Daedalus to HydroFlow compiler.
So we're able to write global programs in Daedalus
and then auto-partition and auto-replicate them.
And that's work being led by David Chu,
who's in the room over here.
David, three years ago promised to do this
and they gave him a certificate saying that's cool.
So he won the SSP student research award.
And three years later, he's got his first results.
So it took a while.
This was not an easy problem,
but he's able to take arbitrary Daedalus programs,
which are Daedalug,
and partition them to run on multiple machines.
And I'm really not gonna spend a lot of time on this.
What I'll say is that earlier student Michael Whitaker,
who again did this by assertion,
he found all sorts of opportunities to optimize Paxos
because inside of Paxos,
it was bottlenecking on things
that were trivially parallelizable,
like network message handling.
So he's like,
just bust apart some of the roles in Paxos into sub-rolls.
Some of those sub-rolls can be replicated.
And he got state-of-the-art performance
in terms of throughput on Paxos by doing this.
And what I observed after he did it was,
oh my gosh, most of the things that you've split out
are monotone sub-components.
And I should have known
that we could pull those out and replicate those.
In fact, I wish Bloom could do that automatically,
but it couldn't.
So three years later,
David can now automatically pull out these things
that Michael was observing
and transform the program to do it.
And the ideas are basically just two tricks.
One trick is to take a pipeline on a single machine
and split it across two machines.
He calls that decoupling.
Now in a general-purpose program,
this is taking, I don't know, 10,000 lines of C++
and figuring out which one's to run on this machine
and which one's to run on that machine.
That would be horrible, right?
But in a data flow language or a logic language,
it's quite nice.
And so he has conditions for when this is safe
and when it's not.
So that's decoupling.
So you can think of this as refactoring a program
into components that can be run on different machines
with asynchronous communication.
The other thing he does
is what's sometimes called sharding
in the practitioner community,
but it's partitioning shared nothing,
partitioning of sub-plants, right?
So instead of having BC take all of the data from A,
you have a hash partitioning here
and certain values go to each machine.
And how do you know that each one of these computations
can be done independently?
That's done through things like
functional dependency analysis,
so that you can show that certain values
have no dependency on other values
because they're partitioned by, say, NFD.
So I'm not gonna go into any of this,
but basically what David was able to do
was take many of the optimizations here
that were handwritten in Scala
and automate them and formalize their correctness.
And without getting into too much detail,
although it is kind of fun,
oh, and we borrowed some work from Paris.
So shout out to Paris for parallel disjoint correctness.
And colleagues.
It is really fast.
So he was able automatically,
this is Michael's results that we re-ran,
and this is his Scala,
this is throughput against latency.
So what you wanna do is you get as much throughput as you can
until it starts to hurt you at latency
and it curls back, okay?
So this is kind of where things start to
top out, if you will.
So that's Whitaker's implementation.
This is the same logic as Whitaker's implementation
in Hydro, so this is just Hydro is faster
than Scala written by hand.
So this is just a testament to the folks
who wrote the Hydroflow engine.
But the blue line is what David achieved
through systematic correctly proven re-writes.
So he was able to get performance
that actually because Hydro is fast
is better than Whitaker's Paxos implementation.
And this gap is kind of what he's given up.
These are the tricks that Michael had
that we didn't cover in our re-writes.
But we're doing 90% with the easy tricks.
So it gives me confidence that simple query optimizations
known in parallel databases can have impacts on places
that were really very fine-tuned performance issues
that people write PhDs to get this kind of performance
and we're getting it through systematic re-writes.
Very promising.
David's only halfway there though
because he has to have a proper cost-based optimizer.
Right now what he has is correct transformation rules.
He needs a cost model with an objective function
and then he needs a search strategy
and we're hopefully gonna be using egg log
or some implementation of egg log in HydroFlow
to achieve this.
So we're one of the things with Max's stuff
that overlaps is if there's a lovely
semi-latest based data flow implementation of Max's stuff
maybe we can clean up some of the things
where he's doing updates in place.
Is there any language that's got to get to that point?
This work?
This work.
Well, so this was written in Datalog
and translated down into that HydroFlow
data flow language you saw at the top.
This stuff is also written in Datalog.
Currently in a runtime that plays some ad-hoc tricks.
That's not traditional Datalog execution here as Max
was going to take.
Yeah, but I think Union Find is a nice target
for an algebraic treatment and I think we have opportunity.
Okay, what I'd like to do in the last few minutes
is berate you with questions.
Because these are the things that I don't know
how to answer yet and I would love to get help with.
So the first is, and this is an outline,
so this section goes on for many slides.
But there's the four questions.
Can we please have one theory for all this nonsense
instead of the list I'm about to show you?
What would be a good type system for the physical layer
where we could prove correctness of transformations?
I have a follow on to Sudeepa about the role
of our kinds of work in the era of generative AI.
And then I have this ongoing question of what time is for
which I probably don't have time to explain.
But quickly, you know, the Unifying Theory thing.
So CRD teaser semi-ladyses,
Daedalus was all done with model theory.
And it's fancy actually.
It uses like stable models and stuff.
It's actually ended up being kind of fancy.
The Calm Theorem, Amalut proves
have these relational transducers,
which are this halfway world between operational
and declarative semantics.
You have these state machines on each node,
they run declarative languages on each step,
but then they output stuff and...
I think you can write non-terminating programs
if you want to.
So you can write toggle, for example,
and Daedalus and...
No, the transducers.
Yeah, the transducers, now they don't have to be terminated.
In any sense, I don't think.
But the point is, I really wish he'd done this work
with this, because he also was on this work,
but he didn't, he did it with transducers,
which is a bummer.
If you talk to distributed systems people,
they talk about essentially order theory,
they talk about partial orders all the time,
which is related to lattices, but it's annoying.
Programmers want to write these algebraic
functional expressions, which I think
is a good thing for all of us.
And then, yeah, I give all these talks
and then some joker raises their hand and says,
well, what about transactions?
And in fact, Peter Bayless, when he was a student,
basically did an end run around my entire group
and just wrote papers about transactions
and coordination free-ness,
and they don't align with the rest of this stuff.
It's an open challenge to reintegrate that work.
And then, you know, I didn't actually say the S word yet,
because I apparently didn't do joins as of yet,
but we do do joins, so we probably need it.
So it would be really great to get all of it here.
I would like to bring all of this work into this domain.
That would be really nice.
Okay, here's a flavor of what I'm dealing with though.
So I just finished reading the DBSP paper,
which was very nice and related to our work,
but we have some other things we need to keep track of
that are relating to the network messing with stuff.
So when we look at a stream, it's got some values.
It's got some happenstance ordering
that's a mapping of the naturals to those values.
It's got some happenstance batching.
It came in over the network in chunks.
So there's like singly nested parentheses in this stream
that are randomly placed.
Randomly, you know, arbitrarily ordered, arbitrarily placed.
Maybe this is a stream of lattice points,
but maybe it's not, I don't know.
But if it is, you could say things like there's
a monotonicity relationship between the types natural order
and the total order of arrival, or not, right?
And then what SORT does is it enforces something like this,
right?
It's nice when these are atomistic,
like data flow is basically a set lattice
that you flow individual atoms.
That's the traditional kind of database iterator thing,
one tuple at a time, right?
One tuple at a time is an atomistic lattice
of the sets of tuples.
And it's nice when you know you're dealing with atoms,
because you can say things like item potents, right?
I gave you Bob's tuple once,
I gave you Bob's tuple twice, sorry,
but you should only have it once, so delete one.
But if I give you subsets,
now you have to find how they overlap,
and you have to make sure that when you union them,
you remove the overlapping bits.
So when you have non-atomistic things,
it's just a little uglier,
and you end up talking about like,
does their meat, is their meat bought?
Kinds of things, do they have no intersection, right?
So these are the kinds of properties
that I think I need to track in my rewrite rules.
And then the operators,
are they invariant to these properties?
Like lattice operations are invariant to order
and parenthesization.
Do they preserve the properties?
Do they enforce different values for the properties?
The network non-deterministically
enforces orders and parenthesizations.
So this is the stuff that I worry about in my operators.
And this is kind of the soup I'm swimming in
with this physical algebra.
So I would like help with this.
I'm gonna do one more quick slide.
This is very much in the realm of what Sudipa
was talking about.
We're in the era where people will be programming
with green goo, right?
It's just, this is large language models,
they're magical, they produce stuff.
But what we really want is building blocks
that we can count on, right?
We're a database people, our industry is all about
foundational building blocks.
And I really do think declarative specification
is a lovely narrow waste here between these two.
Where we can take a formal spec as Codd asked us to.
We can render it in some sense,
so that it's readable, right?
And this relates to things like Wolfgang's work
on visualizing queries and what Sudipa was talking about
in terms of giving natural language things.
But helping people look at this and say,
is this what you meant?
You know, not is it correct,
but is this what you meant since the spec after all, right?
Did you mean this query?
And then, of course, if it's in a nice formal language,
we can check it for other things, right?
And so that would be, I think,
a role that we can really play in the world.
And I suspect things like this will happen.
These programs are gonna be a selection of programs.
You're constantly gonna be given a menu
of which of these things did you mean.
And the answer to which is either some invariant checks
or something or some human judgment.
So I think we're in a good spot
in terms of intermediate root languages.
And I'll just close with one more slide.
Maybe just a handful of slides.
What are clocks in time for and distributed systems?
So there's this famous saying,
which is correctly attributed to a sci-fi short story.
Time is what keeps everything from happening all at once.
So when should we use time in computing?
What are clocks for?
Well, they're not for monotone queries.
I can run this embarrassingly parallel.
It can all happen at the same time, and it's fine.
And Buntalu was doing this long before this discussion.
But I can't run this at the same time, right?
You can't have P and not P at the same time.
So what's the deadliest answer to that is,
well, that's what time is for.
This is the toggle program, right?
And time is there to separate two things that can't coexist.
That should be, I think, the only reason for time.
Okay, except it's not.
Distributed systems, people use time
for maintaining partial orders
and knowing when things were concurrent.
Sometimes you don't need that, sometimes you do.
But this is my question, especially
because Val's in the room and worked at his DSP stuff.
Daedalus has one clock per node and we update it
only when there's a network event
or we have to cycle through negation.
Timely and differential data flow
have clocks all over the damn place.
And I'm not sure when you use them and when you don't.
So for example, tracking iterations
of a monotonic recursive program.
Why do I need a clock for that?
I don't think I need a clock for that.
Maybe if it's a while and we use the index
in our computation, I need to know what index I'm at.
So the general question is,
when do we use this structure called a clock?
And when don't we need it?
And kind of compiler decide.
All right, we're a little over time.
I hope we have given you lots of things to ask us later.
We need lots of help.
So we'd love it.
And we are big fans of working with folks like you.
Can you leave these last four slides there
so we can come over to them and make your vote?
Wish us luck.
That's that.
Thanks guys.
Have a justice day.
We'll be back, thank you.
See you later.
Ah.
Last one,Sub country chat.
See you soon.
Thank you there.
Peace be upon you.
The!
Peace this is my 의の anyway.
Let's sit on my chair,
go to that.
Thank you guys so much.

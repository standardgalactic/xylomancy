The following content is provided under a Creative
Commons license.
Your support will help MIT OpenCourseWare
continue to offer high quality educational resources for free.
To make a donation or to view additional materials
from hundreds of MIT courses, visit MIT OpenCourseWare
at ocw.mit.edu.
So today we'll actually just do a brief chapter
on Bayesian statistics.
And there's entire courses on Bayesian statistics.
There's entire books on Bayesian statistics.
There's entire careers on Bayesian statistics.
So admittedly, I'm not going to be
able to do it justice and tell you
all the interesting things that are happening
in Bayesian statistics.
But I think it's important as a statistician
to know what it is, how it works,
because it's actually a weapon of choice for many practitioners.
And because it allows them to incorporate their knowledge
about a problem in a fairly systematic manner.
So if you look at, like, say, the Bayesian statistics
literature, it's huge.
And so here I give you sort of a range
of what you can expect to see in Bayesian statistics
from your second edition of a traditional book, something
that involves computation, some things that involve rethinking.
And there's a lot of Bayesian thinking.
There's a lot of things that talk about sort
of like philosophy of thinking Bayesian.
This book, for example, seems to be one of them.
This book is definitely one of them.
This one represents sort of a broad literature
on Bayesian statistics for applications, for example,
in social sciences.
But even in large-scale machine learning,
there's a lot of Bayesian statistics happening,
particularly using something called Bayesian parametrics
or hierarchical Bayesian modeling.
So we do have some experts at MIT in the seasail.
Tamara Broderick, for example, is a person who does quite
a bit of interesting work on Bayesian parametrics.
And if that's something you want to know more about,
I urge you to go and talk to her.
So before we go in those more advanced things,
we need to start with what is the Bayesian approach?
What do Bayesians do?
And how is it different from what we've been doing so far?
So to understand the difference between Bayesians
and what we've been doing so far is
we need to first put a name on what we've been doing so far.
It's called Frequentist Statistics.
So it's usually Bayesian versus Frequentist Statistics.
I mean, by versus, I don't mean that there's naturally
an opposition to them.
Actually, often you will see the same method that
comes out of both approaches.
So let's see how we did it.
The first thing, we had data.
We observed some data.
And we assumed that this data was generated randomly.
The reason we did that is that because this would allow us
to leverage tools from probability.
So let's say by nature, measurements, you do a survey,
you get some data.
Then we made some assumptions on the data-generating process.
For example, we assumed they were IID.
That was one of the recurring things.
Sometimes we assumed it was Gaussian,
if we wanted to use, say, t-test.
Maybe we did some nonparametric statistics.
We assumed it was a smooth function
or maybe linear regression function.
So those are our modeling.
And this was basically a way to say,
well, we're not going to allow for any distributions
for the data that we have, but maybe a small set of distribution
that index by some small parameters, for example.
Or at least remove some of the possibilities,
otherwise there's nothing we could learn.
And so, for example, this was associated
to some parameter of interest, say,
data or beta in the regression model.
All right, then we had this unknown problem
and this unknown parameter, and we wanted to find it.
We wanted to either estimate it or test it,
or maybe find a confidence interval for this object.
So far, I should not have said anything that's new.
But this last sentence is actually
what's going to be different from the Bayesian part.
In particular, this unknown fixed thing
is what's going to be changing.
So in the Bayesian approach, we still
assume that we observe some random data.
But the generating process is slightly different.
It's sort of a two-layer process.
And there's one process that generates the parameter,
and then one process that, given this parameter,
generates the data.
So what the first layer does, I mean,
nobody really believes that there's
some random process that's happening about generating
what is going to be the true expected number of people
who turn their head to the right.
When they kiss, but this is actually
going to be something that brings us some easiness for us
to incorporate what we call prior belief.
So we'll see an example in a second.
But often, you actually have prior belief
of what this parameter should be.
When we did, let's say, these squares,
we looked over all of the vectors in all of r to the p,
including the ones that have coefficients, equal to $50 million.
And so those are things that maybe we might be able to roll out.
And maybe we might be able to roll out
at a much smaller scale.
For example, well, I mean, I don't know.
I'm not an expert on turning your head to the right or to the left.
But maybe you can roll out the fact
that almost everybody is turning their head
in the same direction, or almost everybody is turning
their head to another direction.
So we have this prior belief.
And this prior belief is going to play, say,
hopefully, less and less important role as we collect more and more data.
But if we have a smaller amount of data,
we might want to be able to use this information rather than just shooting
in the dark.
And so the idea is to have this prior belief.
And then we want to update this prior belief into what's
called a posterior belief after we've seen some data.
Maybe I believe that there's something that should be in some range.
But maybe after I see data, maybe it's comforting me in my belief.
So I'm actually having maybe a belief that's more.
So a belief encompasses basically what you think
and how strongly you think about it.
That's what I call belief.
So for example, if I have a belief about some parameter theta,
maybe my belief is telling me where theta should be
and how strongly I believe in it in the sense
that I have a very narrow region where theta could be.
And so the posterior belief says, well, you see some data.
And maybe you're more confident or less confident about what you've seen.
Maybe you've shifted your belief a little bit.
And so that's what we're going to try to see
and how to do this in a principal manner.
So of course, to understand this better,
there's nothing better than an example.
So let's talk about another stupid statistical question,
which is let's try to understand p.
Of course, I'm not going to talk about politics from now on.
So let's talk about p, the proportion of women in the population.
OK.
And so what I could do is to collect some data, x1, xn,
and assume that they're Bernoulli with some parameter p unknown.
So p is in 0, 1.
Let's assume that those guys are i, d.
So this is just an indicator for each of my collected data,
whether the person I randomly sample
is a woman, I get a 1, and if it's a man, I get a 0.
OK.
And so now the question is, I sample these people randomly,
I denote their gender, and the frequentist approach
was just saying, OK, let's just estimate p hat being xn bar.
And then we could do some tests.
So here there's a test.
I want to test maybe if p is equal to 0.5 or not.
That sounds like a pretty reasonable thing to test.
But we want to also maybe estimate p.
But here this is a case where we definitely
have for our belief of what p should be.
We are pretty confident that p is not going to be 0.7.
We actually believe that p should be extremely
close to 1.5.
OK, but maybe not exactly.
Maybe I don't know.
Maybe this population is not the population in the world,
but maybe this is the population of, say, some college.
And we want to understand if this college has half women or not.
So maybe we know it's going to be close to 1.5,
but maybe we're not quite sure.
And so we're going to want to integrate that knowledge.
So I could integrate it in a blunt manner
by saying discard the data and say that p is equal to 1.5.
But maybe that's just a little too much.
So how do I do this trade-off between adding the data
and combining it with this prior knowledge?
In many ways, in many instances, essentially what's
going to happen is this 1.5 is going
to act like one new observation, essentially.
So if you have five observations,
this is just a six observation, which will play a role.
If you have a million observations,
you're going to have a million in one,
and it's not going to play so much a role.
That's basically how it goes.
That's basically how it goes.
But definitely not always.
Because we'll see that if I take my prior to be a point
minus 1.5 here, it's basically as if I was discarding my data.
So essentially, there's also your ability
to encompass how strongly you believe in this prior.
And if you believe infinitely more in the prior
than you believe in the data you collected,
then of course, it's not going to act like one more observation.
So the Bayesian approach is a tool
to one, include mathematically our prior and our prior belief
into statistical procedures.
So maybe I have this prior knowledge,
but if I'm a medical doctor, it's not clear to me
how I'm going to turn this into some principal way of building
estimators.
And of course, the second goal is
going to be to update this prior belief into posterior belief
by using the data.
How do I do this?
And at some point, I sort of suggested
that there's two layers.
One is where you draw the parameter at random.
And two, once you have the parameter conditioned on this
parameter, you draw your data.
Nobody believes this actually is happening,
that nature is just rolling dice for us
and choosing parameters at random.
But what's happening is that this idea
that the parameter comes from some random distribution
actually captures very well this idea
that how you would encompass your prior, right?
How would you say my belief is as follows?
Well, here's an example about p.
I'm 90% sure that p is between 0.4 and 0.6.
And I'm 95% sure that p is between 0.3 and 0.8.
So essentially, I have this possible value of p.
And what I know is that there's 90% here between,
what did I say, 0.4 and 0.6.
And then I have 0.3 and 0.8.
And I know that I'm 95% sure that I'm in here.
And this, if you remember, this sort of
looks like the kind of pictures that I
made when I had some Gaussian, right, for example.
And I said, oh, here we have 90% of the observations.
And here we have 95% of the observations.
So in a way, if I were able to tell you
all those ranges for all possible values,
then I would essentially describe a probability
distribution for p.
And what I'm essentially saying is
that p is going to have this kind of shape.
So of course, if I tell you only twice this information
that there's 90% I'm here and I'm here between here and here
and 95% I'm between here and here,
then there's many ways I can accomplish that, right?
I could have something that looks like this, maybe, right?
I could be really, I mean, it could be like this.
I mean, there's many ways I can have this.
Some of them are definitely going to be mathematically
more convenient than others.
And hopefully, we're going to have things
that I can parameterize very well.
Because if I tell you this is this guy,
then there's basically 1, 2, 3, 4, 5, 6, 7 parameters.
So I probably don't want something that has 7 parameters,
but maybe I can say, oh, it's a Gaussian.
And all I have to do is to tell you
where it's centered and what the standard deviation is.
OK, so the idea of using this two-layer thing
where we think of the parameter p as being drawn
from some distribution is really just a way for us
to capture this information, our prior belief being,
well, there's this percentage of chances that it's there.
But the person who has a chance, I'm deliberately not
using probability here.
It's really, right?
So it's really a way to get close to this.
All right, so that's what I said.
The true parameter is not random,
but the Bayesian approach does as if it was random
and then just spits out a procedure out of this thought
process, this thought experiment.
So when you practice Bayesian statistics a lot,
you start getting automatisms.
So you start getting some things that you do
without really thinking about it, just like when you're
a statistician, the first thing you do
is can I think of this data as being Gaussian, for example?
When you're Bayesian, you're thinking about, OK,
I have a set of parameters, right?
So here, I can describe my parameter
as being theta in general in some big space parameter theta.
But what spaces did we encounter?
Well, we encountered the real line,
we encountered the interval 0, 1 for Bernoulli's,
and we encountered maybe some deposit of real line
for exponential distributions, et cetera.
And so what I'm going to need to do if I want to model some,
if I want to put some prior on those spaces,
I'm going to have to have a usual set of tools for this guy,
usual set of tools for this guy, usual set of tools
for this guy, and by usual set of tools,
I mean I'm going to have to have a family of distributions
that's supported on this.
So in particular, this is the space
in which my parameter that I usually denote by p
for Bernoulli lives.
And so what I need is to find a distribution
on the interval 0, 1, just like this guy.
The problem with the Gaussian is that it's not
on the interval 0, 1.
It's going to spill out in the end,
and it's not going to be something that works for me.
And so the question is, I need to think about distributions
that are probably continuous.
Why would I restrict myself to discrete distributions that
are actually convenient?
And for Bernoulli, one that's actually basically the main tool
that everybody's using is this so-called beta distribution.
So the beta distribution has two parameters.
So x follows a beta with parameters, say a and b.
If it has a density, f of x is equal to x to the a minus 1,
1 minus x to the b minus 1.
If x is in the interval 0, 1, and 0 for all other x's, OK?
So why is that a good thing?
Well, it's a density that's on the interval 0, 1 for sure.
But now I have these two parameters,
and the set of shapes that I can get by tweaking those two
parameters is incredible, OK?
I mean, it's going to be a unimodal distribution.
It's still fairly nice, right?
It's not going to be something that goes like this and this,
because if you think about this, what
would it mean if your prior distribution on the interval 0,
1 had this shape?
It would mean that maybe you think that p is here,
or maybe you think that p is here,
or maybe you think that p is here,
which essentially mean that you think
that p can come maybe from three different phenomena.
And there's other models that are called mixtures for that
that directly account for the fact
that maybe there are several phenomena that
are aggregated in your data set.
But if you think that your data set is sort of pure
and that everything comes from the same phenomenon,
you want something that looks like maybe like this,
or maybe it looks like this, or maybe it's sort of symmetric,
you want to get all this stuff, right?
Maybe you want something that says, well, you know,
if I'm talking about p being the probability of the proportion
of women in the whole world, you want something that's probably
really spiked around 1 half, right?
Almost the point mass, because you know, I mean,
OK, let's agree that 0.5 is the actual number.
So you want something maybe that says, OK, maybe I'm wrong,
but I'm sure I'm not going to be really that way off.
And so you want something that's really pointy.
But if it's something you've never checked, right?
And again, I cannot make references at this point,
but something where you might have some uncertainty,
then that should be around 1 half.
Maybe you want something that's like a little more allows you
to say, well, I think there's more around 1 half,
but there are still some fluctuations that are possible,
OK?
And in particular here, I talk about p
where the two parameters, a and b, are actually the same.
I call them a.
One is called scale, the other one's called shape.
Oh, by the way, sorry, this is not a density,
so it actually has to be normalized, right?
When you integrate this guy, it's
going to be some function that depends on a and b,
actually depends on this function
through the beta function, right?
Which is this combination of gamma function.
So that's why it's called beta distribution.
But well, that's the definition of the beta function
when you integrate this thing anyway.
So I mean, you just have to normalize it.
It's just a number that depends on a and b, OK?
So here, if you take a equal to b,
you have something that essentially is
symmetric around 1 half, right?
Because what does it look like?
Well, it's something.
So my density f of x is going to be what?
This is going to be my constant times x times 1 minus x
to the a minus 1, right?
And this function, x times a minus 1 minus x looks like this.
We've drawn it before, right?
That was something that showed up
as being the variance of my Bernoulli.
So we know it's something that takes its maximum at 1
half, and now I'm just taking the power of this guy.
So I'm really just distorting this thing
into some fairly symmetric manner, OK?
So this distribution that we actually take for p, right?
So here, I assume that p, the parameter, right?
I mean, notice that this is kind of weird.
First of all, this is probably the first time
in this entire course that we have this has this something
has a distribution when it's actually a lower case letter.
That's something you have to deal with,
because we've been using lower case letters for parameters,
and now we want them to have a distribution.
So that's what's going to happen, all right?
And this is called the prior distribution, OK?
So really, I should write something like f of p
is equal to a constant times p 1 minus p to the a minus 1.
Well, no, actually, I should not, because then it's confusing.
OK, so let me not do this.
One thing in terms of notation that I'm going to write,
I'm going to write, when I have a constant here,
and I don't want to make it explicit,
and we'll see in a second why I don't need to make it explicit,
I'm going to write this as f of x is proportional to x 1
minus x to the a minus 1, OK?
So that's just to say equal to some constant that does not
depend on x times this thing, OK?
So if we continue with our experiment, now if p,
right, so that's the experiment where I'm trying to,
I'm drawing this data x 1 to x n, which is Bernoulli p,
if p has some distribution, it's not clear
what it means to have a Bernoulli with some random parameter.
So what I'm going to do is then I'm going to first draw my p.
Let's say I get a number, 0.52, and then I'm
going to draw my data conditionally on p, all right?
So here comes the first and last flowchart of this class.
So I'm going to first, all right?
So nature first draws p, OK?
So p follows, say, some beta AA.
Then I condition on p, and then I draw x 1 x n that
are i, i, d Bernoulli p.
Everybody understand the process of generating this data, right?
So you first draw a parameter, and then you just
flip those independent bias coins with this particular p.
So there's this layered thing.
So now, conditionally on p, right?
So here, I have this prior about p, which was the thing.
So this is just a thought process again, right?
It's not anything that actually happens in practice.
This is my way of thinking about how the data was generated.
And from this, I'm going to try to come up with some procedure.
Just like if your estimator is the average of the data,
you don't have to understand probability
to say that my estimator is the average of the data, right?
I mean, anyone outside this room understand
that the average is a good estimator for some average
behavior, and they don't need to think of the data as being
a random variable, et cetera.
So same thing, basically.
Now, we will see.
I mean, actually, we won't.
But in this case, well, we will.
In this case, you can see that essentially, the posterior
distribution is still a beta, all right?
So what it means is that I had this thing,
then I observed my data, and then I continue.
And here, I'm going to update my prior
into some posterior distribution, pi.
And here, this guy is actually also a beta, all right?
So pi now, p, my posterior distribution on p,
is also a beta distribution with the parameters
that are on this slide, and I'll have space to reproduce them.
So I start the beginning of this flow chart
as having p, which is a prior.
I'm going to get some observations,
and then I'm going to update what my posterior is, OK?
So this posterior is basically something
that's in Bayesian statistics was beautiful,
is as soon as you have the distribution,
it's essentially capturing all the information about the data
that you want for p.
And it's not just a point, right?
It's not just an average.
It's actually an entire distribution
for the possible values of theta.
And it's not the same thing as saying, well, you know,
if theta hat is equal to x and bar in the Gaussian case,
I know that this is some mean mu,
and then maybe it has variance sigma square over n.
That's not what I mean by this is my posterior distribution,
right?
This is not what I mean.
This is going to come from this guy, right?
The Gaussian thing and the central limit theorem.
But what I mean is this guy.
And this came exclusively from the prior distribution.
If I had another prior, I would not necessarily
have a beta distribution on the output.
So when I have the same family of distributions
at the beginning and at the end of this flow chart,
I say that beta is a conjugate prior,
meaning I put in beta as a prior,
and I get betas at the posterior.
And that's why betas are so popular.
Conjugate priors are really nice,
because you know that whatever you put in,
what you're going to get in the end is a beta.
So all you have to think about is the parameters.
You don't have to check again what the posterior is
going to look like, what the PDF of this guy is going to be.
You don't have to think about it.
You just have to check what the parameters are.
And there's families of conjugate priors.
Gaussian gives Gaussian, for example.
There's a bunch of them.
And this is what drives people into using specific priors
as opposed to other.
It has nice mathematical properties.
Nobody believes that the P distribution is really
distributed according to beta, but it's flexible enough
and super convenient mathematically.
All right, so now let's see for one second before we actually
go any further.
What I did, so A and B, I didn't mention it.
And here, A and B are positive numbers.
OK, they can be anything positive.
So here what I did is that I updated A into A plus the sum
of my data and B into B plus and minus the sum of my data.
So that's essentially A becomes A plus the number of ones.
And B becomes B, well, that's only when I have A and A.
So the first parameters become itself plus the number of ones,
and the second one becomes itself plus the number of zeros.
And so just as a sanity check, what does this mean?
If A goes to 0, what is the beta when A goes to 0?
We can actually read this from here, right?
So we had A, sorry, actually, let's take A goes to, no,
actually, sorry, let's just do this.
OK, let's not do this now.
I'll do it when we talk about non-informative prior,
because it's a little too messy here.
OK, so how do we do this?
How did I get this posterior distribution given the prior?
How do I update this?
Well, this is called Bayesian statistics.
And you've heard this word Bayes before.
And the way you've heard it is in the Bayes formula, right?
What was the Bayes formula?
The Bayes formula was telling you that the probability of A given
B was equal to something that depended on the probability
of B given A, right?
That's what it was.
And I mean, you can actually either remember the formula,
you can remember the definition, and this
is what P of A and B divided by P of B. So this
is P of B given A times P of A divided by P of B, right?
That's what Bayes formula is telling you, agree?
So now what I want is to have something
that's telling me how this is going to work, OK?
So what is going to play the role of those events, A and B?
Well, one is going to be the distribution of my parameter theta
given that I see the data, and this
is going to tell me what is the distribution of the data
given that I know what my parameter theta is.
But that part, if this is data and this is the parameter theta,
this is what we've been doing all along.
The distribution of the data given the parameter here
was Ni Id Bernoulli P. I know that.
I know exactly what their joint probability mass function is.
Then that was what?
So we said that this is going to be my data,
and this is going to be my parameter, OK?
So that means that this is the probability of my data
given the parameter.
This is the probability given the parameter.
This is the probability of the parameter.
What is this?
What did we call this?
This is the prior.
It's just the distribution of my parameter.
Now, what is this?
Well, this is just the distribution of the data itself,
all right?
So this is essentially the distribution of this if this
was indeed not conditioned on P, right?
So if I don't condition on P, this data
is going to be a bunch of IID Bernoulli
with some parameter, but the parameter is random, right?
So for different realization of this data set,
I'm going to get different parameters for the Bernoulli.
And so that leads to some sort of convolution.
I mean, it's not really a convolution in this case,
but it's some sort of composition of distributions, right?
I have the distribution that comes from the randomness that
comes from here, and then the randomness that
comes from realizing the Bernoulli, right?
So that's just the marginal distribution,
and it actually might be painful to understand what this is,
right?
I mean, in a way, it's sort of a mixture,
and it's not super nice.
But we'll see that this actually won't matter for us.
This is going to be some number.
It's going to be there, but it won't
matter for us what it is, because it actually
does not depend on the parameter,
and that's all that matters to us.
OK, so let's put some names on those things, right?
I mean, this was very informal, so let's
put some actual names on what we want to call what we call prior.
So what is the formal definition of a prior?
What is the formal definition of a posterior?
And what are the rules to update it, OK?
So I'm going to have my data, which is going to be x1, xn.
And so let's say they're IID, but they don't actually have to.
And so I'm going to have given theta.
And when I say given, it's either given
like I did in the first part of this course
in all previous chapters or conditionally on, right?
So that's if you're thinking like a Bayesian,
that what I really mean is conditionally
on this random parameter, OK?
So it's like as if it was a fixed number.
Then they're going to have the distribution x1, xn
is going to have some distribution.
Let's say, let's assume for now it's pdf, pn of x1, xn, OK?
And I'm going to write theta like this.
So for example, what is this?
So let's say this is a pdf.
It could be a pmf.
Everything I say, I'm going to think of them as being pdfs.
I'm going to combine pdfs with pdf,
but I could combine pdf with pmfs, pmf with pdfs,
or pmf with pms, OK?
So everywhere you see a d, it could be an m.
All right, so now I have those things.
So what does that mean?
So here's some example, x1, xn are IID and theta1, right?
So now I know exactly what the joint pdf of this thing is.
So it means that pn of x1, xn given theta is equal to what?
Well, it's like 1 over sigma root, sorry, 1 root 2 pi
to the power n e to the minus sum from i
equal 1 to n of xi minus theta squared divided by 2, right?
So that's just the joint distribution of n iID and theta
1 random variables, OK?
So that's my pn given theta.
Now, this is what we denoted by sort of like f sub theta
before, right?
We had this subscript before, but now we just
put a bar in theta because we want
to remember that this is actually conditioned on theta, right?
But this is just notation.
You should just think of this as being just the usual thing
that you get from some statistical model, all right?
So now, that's going to be pn.
And here, I'm going to assume that theta is,
why do I put pi here?
So theta has prior distribution pi.
OK, so for example, so think of it as either PDF or PMF again.
So for example, pi of theta was what?
Well, it was some constant times theta to the a minus 1,
1 minus theta to the a minus 1, right?
So it has some prior distribution,
and that's another PMF.
So now, I'm given the distribution of my x's given theta.
I'm given the distribution of my theta,
so I'm given this guy, right?
That's this guy.
I'm given that guy, which is my pi, right?
So that's my pn of x1, xn given theta.
That's my pi of theta.
And then I have here just, this is what?
Well, this is just the integral of pn x1, xn times pi of theta d theta,
right?
Overall possible sets of theta.
That's just when I integrate out my theta,
or I compute, say, the marginal distribution,
I get this by integrating, right?
That's just basic probability, conditional probabilities, right?
And if I had the PMF, I would just
sum over the values of theta's, OK?
So now, what I want is to find what's called,
so that's the prior distribution.
And I want to find the posterior distribution.
So it's called, it's pi of theta given x1, xn.
And so if I use Bayes' rule, I know
that this is pn of x1, xn given theta times pi of theta.
And then it's divided by the distribution of those guys,
which I will write as integral over theta of pn x1, xn given theta times pi
of theta d theta.
Everybody's with me still?
So if you're not comfortable with this,
it means that you probably need to go read your couple pages
on conditional densities and conditional PMFs
from your probability class.
There's really not much there.
It's just a matter of being able to define those quantities.
F density of x given y, this is just
what's called a conditional density.
You need to understand what this object is
and how it relates to the joint distribution of x and y,
or maybe the distribution of x or the distribution of y.
But it's the same rules.
I mean, one way to actually remember this
is this is exactly the same rules as this.
When you see a bar, it's the same thing
as the probability of this and this guy.
So for densities, it's just a comma divided by the second guy.
The probability of the second guy, that's it.
So if you remember this, you can just do some pattern matching
and see what I just wrote here.
OK.
OK.
So now I can compute every single one of these guys.
This is something I get from my modeling.
So I did not write this.
It's not written in the slides.
But I give a name to this guy that was my prior distribution.
And that was my posterior distribution.
In chapter 3, maybe, what did we call this guy?
Well, the one that does not have a name.
And that's in the box, this guy.
How did we call it?
It is the joint distribution of the x i's.
And we give you the name.
It's the likelihood, right?
This is exactly the likelihood.
This was the likelihood of theta.
And this is something that's very important to remember.
And that really reminds you that these things are really
not that different, maximum likelihood estimation
and Bayesian estimation.
Because your posterior is really just your likelihood
times something that's just putting some weights
on the thetas, depending on where you think theta should be.
So if I had, say, a maximum likelihood estimator
and my likelihood in theta looked like this.
But my prior in theta looked like this.
I said, oh, I really want thetas that are like this.
So what's going to happen is that I'm
going to turn this into some posterior that looks like this.
So I'm just really weighting this posterior.
This is a constant that does not depend on theta, right?
Agreed?
I integrated over theta, so theta's gone.
So forget about this guy.
I have basically that the posterior distribution up
to scaling, because it has to be a probability density
and not just any function that's positive,
is the product of this guy.
It's the weighted version of my likelihood.
That's all it is.
I'm just weighting the likelihood using my prior belief
on theta.
And so given this guy, a natural estimator,
if you follow the maximum likelihood principle,
would be the maximum of this posterior.
Agreed?
That would basically be doing exactly what
maximum likelihood estimation is telling you.
So it turns out that you can.
It's called maximum a posteriori.
And I won't talk much about this or map.
So that's maximum a posteriori.
So it's just the theta hat is the arg max of pi theta given x1,
xn.
It sounds like it's OK.
I give you a density, and you say, OK.
I have a density for all values of my parameters.
You're asking me to summarize it into one number.
I'm just going to take the most likely number of those guys.
But you could summarize it otherwise.
You could take the average, right?
You could take the median.
You could take a bunch of numbers.
And the beauty of Bayesian statistics
is that you don't have to take any number in particular.
You have an entire posterior distribution.
This is not only telling you where theta is,
but it's actually telling you the difference if you actually
give as something, it gives you the posterior, right?
So now let's say the theta is a P between 0 and 1.
If my posterior distribution looks like this,
or if my posterior distribution looks like this,
then those two guys have one the same mode, right?
This is the same value.
And they're symmetric, so they also have the same mean.
So these two posterior distributions
give me the same summary into one number.
However, clearly, one is much more confident than the other one.
So I might as well just spit that as a solution.
OK, some people can, you can do even better.
People actually do things such as drawing a random number
from this distribution, so this is my number.
That's kind of dangerous, but you can imagine you could do this,
right?
All right, so this is what works.
That's what we went through.
So here, as you notice, I don't care so much about this part
here, right?
Because it does not depend on theta.
So I know that given the product of those two things,
this thing is only the constant that I need to divide
so that when I integrate this thing over theta,
it integrates to 1.
Because this has to be a probability density on theta.
So I can write this and just forget about that part,
and that's what's right on the top of this slide.
OK, just this notation, this sort of weird alpha,
or I don't know, infinity, sine, crop to the right,
whatever you want to call this, this thing is actually just
really emphasizing the fact that I don't care.
I write it because I mean, yeah, I write it because I can,
but then you know what it is, but you don't actually
have to, well, OK, in some instances,
you have to compute the integral.
In some instances, you don't have to compute the integral.
And a lot of Bayesian computation is about saying, OK,
it's actually really hard to compute this integral,
so I'd rather not doing it.
So let me try to find some methods that
allow me to sample from the posterior distribution
without having to compute this.
And that's what's called Monte Carlo Markov chains,
or MCMC, and that's exactly what they're doing.
They're just using only ratios of things
like that for different datas, and which
means that if you take ratios, the normalizing constant
is gone, and you don't need to find this integral.
OK, so we won't go into those details at all.
That would be the purpose of an entire course on Bayesian
inference.
Actually, even Bayesian computations
would be an entire course on its own.
And there are some very interesting things
that are going on there, the interface of stats and computation.
All right.
So let's go back to our example and see
if we can actually compute any of those things,
because it's very nice to give you some data, some formulas.
But let's see if we can actually do it.
All right, then in particular, can I actually
recover this claim that the posterior associated
to a beta prior with Bernoulli likelihood
is actually giving me a beta again?
All right, so what was my prior?
Well, it was beta, so P was following a beta AA,
which means that P, the density, so that was pi of theta,
well, I'm going to write it as pi of P,
was proportional to P to the A minus 1 times 1 minus P
to the A minus 1, right?
So that's the first ingredient I need to compute my posterior.
I really need only two if I want it up to constant.
The second one was P. Well, we've computed that many times.
And we had even a nice compact way
of writing it, which was that Pn of x1, xn,
given a parameter P, right?
So the density, the joint density of my data given P,
that's my likelihood of P, was what?
Well, it was P to the sum of the xi's, 1 minus P
to the n minus sum of the xi's.
Anybody wants me to parse this more,
or do you remember seeing that from maximum likelihood
estimation?
Yeah?
So when you condition on the random variables,
you really just treat that random variable
as something that's not correct.
That's what conditioning does.
OK?
Yeah?
I'm going to give you a previous slide.
For the bottom there, this is d pi of t.
Can it be d pi of t?
So d pi of t is a measure of theoretic notation,
which I use without thinking.
And I should not, because I can see it upsets you.
D pi of t is just a natural way to say
that I integrate against whatever
I'm given for the prior of theta.
In particular, if theta is just the mix of a PDF and a point
mass, right?
Maybe I say that my P takes value 0.5 with probability 0.5,
and then is uniform on the interval with probability 0.5.
OK, so for this, I neither have a PDF nor a PMF,
but I can still talk about integrating with respect
to this, right?
It's going to look like if I take a function f of t,
d pi of t is going to be 1 half of f of 1 half, right?
That's the point mass with probability 1 half at 1 half,
plus 1 half of the integral between 0 and 1 of f of t dt.
So this is just a notation, which is actually,
funnily enough, is interchangeable with pi of dt.
But if you have a density, it's really just the density pi
of t dt, if pi is really a density.
But that's when pi is a measure in other density.
But so everybody else forget about this.
I mean, this is not something you should really worry about.
At this point, this is more graduate level probability
classes.
But yeah, it's called measure theory,
and that's when you think of pi as being a measure,
in an abstract fashion, you don't have
to worry whether it's a density or not,
or whether it has a density even.
OK?
So everybody's OK with this?
All right, so now I need to compute my posterior.
And as I said, my posterior is really
just the product of the likelihood weighted by the prior,
right?
So hopefully, at this stage of your education,
you can multiply two functions.
All right, so what's happening is,
if I multiply this guy with this guy, well,
p gets this guy to the power of this guy plus this guy.
And then 1 minus p gets the power n minus sum of xi's.
So this is always from i equal 1 to n,
and then plus a minus 1 as well.
OK?
And this is, sorry, this is up to constant,
because I still need to solve this.
And I could try to do it, but I really don't have to,
because I know that if my density has this form,
then it's a beta distribution.
And then I can just go on Wikipedia
and see what should be the normalization factor.
But I know it's going to be a beta distribution.
It's actually the beta with parameter.
So this is really my beta with parameter sum of xi i equal 1
to n plus a minus 1.
And then the second parameter is n minus sum of the xi's
plus a minus 1.
OK?
Sorry.
I just wrote what was here.
Oh, what happened to my 1?
Oh, no, sorry, sorry, sorry.
Beta has the power minus 1, right?
So that's the parameter of the beta.
And this is the parameter of the beta, right?
So beta, well, I don't think it's anywhere.
Yeah, beta is over there, right?
So I just replace a by what I see.
a is just becoming this guy plus this guy,
and this guy plus this guy.
Everybody's comfortable with this computation?
All right, so we just agreed that beta priors
for Bernoulli observations are certainly convenient, right?
And because they're just conjugate,
and we know that's what's going to come out in the end,
that's going to be a beta as well.
So I mean, I just claim it was convenient.
It was certainly convenient to compute this, right?
I mean, there was certainly some compatibility
when I had to multiply this function by that function.
And you can imagine that things could go much more wrong
than just having p to some power and p to some power, 1
minus p to some power, 1 minus p to some other sum power.
Things were nice.
Now, this is nice, but I can also question the following things.
Why beta, for one?
I mean, the beta tells me something, but that's convenient.
But then how do I pick a?
I know that a should definitely capture the fact
that where I want to have my p most likely located,
but it also actually also captures the variance of my beta.
And so choosing different a's is going
to have different functions.
If I have a and b, if I started with the beta
with parameter here, I started with a b here,
I would just pick up the b here, agreed?
And that would just be asymmetric,
but they're going to capture mean and variance of this thing.
And so how do I pick those guys, right?
I mean, if I'm a doctor and you're
asking me what do you think the chances of this drug working
on this kind of patients is, and I
have to say to spit out the parameters of beta for you,
it might be a bit of a complicated thing to do.
So how do you do this, especially for problems?
So by now, people have actually found
mastered the art of coming up with how to formulate
those numbers.
But in new problems that come up, how do you do this?
What happens if you want to use Bayesian methods,
but you actually do not know what you expect to see?
Maybe this is the first time you, I mean, to be fair,
before we start this class, I hope all of you
had no idea whether people tended to bend their head
to the right or to the left before kissing,
because if you did, well, you have too much time on your hand,
and I should double your homework.
And so in this case, you have to sort of, maybe you still
want to use the Bayesian machinery.
Maybe you just want to do something nice.
It's nice, right?
I mean, it worked out pretty well.
And so what if you want to do well,
you actually want to use some priors that
have no carry no information that basically do not
prefer any theta to another theta.
Now, you could read this slide, or you could look at this formula.
We just said that this pi here was just here
to weigh some thetas more than others,
depending on our prior belief.
If our prior belief does not want to put any preference
towards some thetas than to others, what do I do?
Yeah, I remove it.
And the way to remove something we multiply by
is just replace it by 1.
That's really what we're doing.
So if this was a constant, then not depending on theta,
then that would mean that we're not preferring any theta.
And we're looking sort of at the likelihood,
but not as a function that we're trying to maximize,
but as a function that we normalize in such a way
that it's actually a distribution.
So if I have pi, which is not here,
this is really just taking the likelihood, which
is a positive function, may not integrate to 1.
So I normalize it so that it integrates to 1.
And then I just say, well, this is my posterior distribution.
Now, I could just maximize this thing
and spit out my maximum likelihood estimator,
but now I can also integrate and find
what the expectation of this guy is.
I can find what the median of this guy is.
I can sample data from this guy.
I can build, understand what the variance of this guy is,
which is something we did not do when we just
did maximum likelihood estimation because,
given a function, all we cared about
was the maximum, the argmax of this function.
So this priors are called uninformative.
So this is just replacing this number by 1.
And if I have a, or by a constant,
because it still has to be a density,
and so if I have something which is, if I have a bounded set,
I'm just looking for the uniform distribution
on this bounded set.
The one that puts constant one over the size of this thing.
But if I have an unbounded set, what
is the density that takes a constant value on the entire
real line, for example?
What is this density?
Doesn't exist, right?
I mean, it just doesn't exist.
I mean, the way you can think of it
is Gaussian with the variance going to infinity, maybe,
or something like this.
But you can think of it in many ways.
You can think of the limit of the uniform between minus t
and t with t going to infinity.
But this thing is actually 0, right?
I mean, there's nothing there.
And so you can actually still talk about this, right?
You could always talk about this thing
where you think of this guy as being a constant,
remove this thing from this equation,
and just say, well, my posterior is just
the likelihood divided by the integral of the likelihood
over theta.
And if theta is the entire real line, so be it.
As long as this integral converges,
you can still talk about this stuff.
And so this is what's called an improper prior, right?
An improper prior is just a non-negative function
defined on theta, but it does not
have to integrate neither to 1 nor to anything, OK?
If I integrate the function equal to 1
on the entire real line, what do I get?
Infinity, right?
I mean, it's not a proper integral,
and so it's not a proper prior, and it's
called an improper prior.
And those improper priors are usually
what you see when you start to want
non-informative priors on infinite set status.
I mean, that's just the nature of it.
You should think of it as being the uniform prior
and the uniform distribution at some infinite set
if that thing were to exist.
So let's see some examples about non-informative priors, right?
So if I'm on the uniform, if I'm on the interval 0, 1,
this is a finite set, so I can talk about the uniform prior
on the interval 0, 1 for a parameter p of a Bernoulli, OK?
And so if I want to talk about this,
then it means that my prior is p follows some uniform
on the interval 0, 1, OK?
So it means that the density is, well, f of x is 1
if x is in 0, 1 and 0.
Otherwise, there's actually not even a normalization.
This thing integrates to 1.
And so now, if I look at my likelihood,
it's still the same thing.
So my posterior becomes theta x1, xn.
So that's my posterior.
I don't write the likelihood again because we still have it.
Well, we don't have it there anymore.
Is it here?
Or did I just erase it?
Yeah, the likelihood is given here, right?
So copy paste over there.
And so the posterior is just this thing times 1.
So you will see it in a second.
So it's p times the sum to the power sum of the xi's, 1
minus p to the power n minus sum of the xi's.
And then it's multiplied by 1 and then
divided by this integral between 0 and 1 of p sum of the xi's,
1 minus p n minus sum of the xi's dp,
which does not depend on p.
And I really don't care what this thing actually is.
So now, sorry, that's prior posterior of p.
And now I can see, well, what is this?
Well, it's actually just the beta with parameters this guy
plus 1 and this guy plus 1.
So I didn't tell you what the expectation of a beta was.
We don't know what the expectation of a beta is.
Agreed?
I mean, if I wanted to find, say, the expectation of this thing,
that would be some good estimator,
we know that the maximum of this guy,
what is the maximum of this thing?
Well, it's just this thing, right?
I mean, it's the average of the xi's, right?
That's just the maximum likelihood estimator for Bernoulli.
We know it's the average.
Do you think if I take the expectation of this thing,
I'm going to get the average?
So actually, I'm not going to get the average.
I'm going to get this guy plus this guy divided by n plus 1.
So I'm going to do as if I had a, sorry.
OK, let me not say it like that.
Let's look at what this thing is doing.
It's looking at the number of 0's, the number of 1's,
and it's adding 1.
And this guy is looking at the number of 0's,
and it's adding 1.
Why is it adding this 1?
What's going on here?
Well, what would happen if I had, so this actually
is going to matter mostly when the number of 1's is actually
0 or the number of 0's is 0?
Because what it does is just pushes the 0 from non-zero.
And why is that something that this Bayesian method actually
does for you automatically is because when
we put this non-informative prior on p, which
was uniform on the interval 0, 1, in particular,
we know that the probability that p is equal to 0 is 0,
and the probability that p is equal to 1 is 0.
And so the problem is that, essentially,
if I did not add this 1 with some positive probability,
I would be allowed to spit out something that actually
had p hat, which was equal to 0.
In the case, if by chance, let's say I have n is equal to 3,
and I get only 0, 0, 0, that could happen with probability 1
over p cubed, 1 minus p cubed, then that's not something
that I want, and I'm actually using my prior.
So my prior is not informative, but somehow it captures
the fact that I don't want to believe that p is going
to be either equal to 0 or 1.
And so that's taken care of here.
So let's move away a little bit from the Bernoulli example,
shall we?
I mean, I think we've seen enough of it.
And so let's talk about the Gaussian model.
Let's say I want to do Gaussian inference.
I want to do inference in a Gaussian model using Bayesian
methods.
So I'm going to actually look at, so say,
so what I want is that xi, x1, xn, or say, n, 0, 1, i, i, d,
sorry, theta 1, i, i, d, conditionally on theta.
OK, so that means that pn of x1, xn, given theta,
is equal to, well, exactly what I wrote before.
So 1 square root 2 pi to the n exponential minus 1
half sum of xi minus theta squared.
OK, so that's just the joint distribution
of my n-Gaussians with mean theta.
Another question is, what is the posterior distribution?
OK, well, here I said, let's use the uninformative prior,
which is an improper prior.
It puts weight 1 on everyone.
That's the so-called uniform on the entire real line.
So that's certainly not a density.
But I can still just use this.
So all I need to do is to get this divided
by normalizing this thing.
So that's what I need to do.
But if I look at this, so essentially, I
want to understand, so this is proportional
to exponential minus 1 half sum from i
equal 1 to n of xi minus theta squared.
And now I want to see this thing as a density not on the xi's,
but on theta.
Right?
What I want is a density on theta.
So it looks like I have chances of getting something
that looks like a Gaussian.
But if I really need to have a Gaussian,
I would need to see minus 1 half.
And then I would need to see theta minus something here,
not just the sum of something minus theta's.
So I need to work a little bit more
so I can see what this to expand the square here.
So this thing here is going to be
equal to exponential minus 1 half sum from i
equal 1 to n of xi squared minus 2 xi theta plus theta squared.
Ah.
OK, and so now basically what I'm going to do
is everything, remember, is up to this little sign, right?
So every time I see a term that does not depend on theta,
I can just push it in there and just make it disappear.
Agreed?
OK, this term here, exponential minus 1
half sum of xi squared, does it depend on theta?
No, so I'm just pushing it here.
This guy, yes, and the other one, yes.
So this is proportional to exponential xi, sorry,
sum of the xi.
And then I'm going to pull out my theta.
The minus 1 half cancel with the minus 2.
And then I have minus 1 half sum from i
equal 1 to n of theta squared, right?
Agreed?
So now what this thing looks like, well,
this looks very much like some theta minus something squared.
This thing here is really just n over 2 times theta.
So sorry, times theta squared.
So now what I need to do is to write this of the form theta
minus something, let's call it mu squared,
maybe divided by 2 sigma squared, right?
I want to turn this into that, maybe up to terms
that do not depend on theta.
That's what I'm going to try to do.
So that's called completing the square
and that's some exercise you do.
You've done it probably already in the homework.
And that's something you do a lot when you do Bayesian statistics
in particular.
So let's do this.
Well, what is going to be the leading term?
Well, theta squared is going to be multiplied by this thing.
So I'm going to pull out my n over 2.
And then I'm going to write this as theta squared minus theta
over 2.
And then I'm going to write theta minus something squared.
And this something is going to be 1
half of what I see in the cross product, right?
Well, I need to actually pull this thing out.
So let me write it like that first.
So that's theta squared.
And then I'm going to write it as minus 2 times 1 over n sum
from i equal 1 to n of x i times theta, right?
That's exactly just a rewriting of what we had before.
And that should look much more familiar.
x squared minus a squared minus 2 blab a.
And then I miss something.
So this thing I'm going to be able to rewrite as theta minus xn
bar squared.
But then I need to remove the square of xn bar,
because it's not here, OK?
So I just complete the square.
And then I actually really don't care what this thing actually
was, because it's going to go again
in the little alpha sign over there.
So this thing eventually is going
to be proportional to exponential of minus n
over 2 times theta minus xn bar squared.
And so we know that if this is a density that's
proportional to this guy, it has to be some n with mean xn bar
and variance.
Well, this is supposed to be 1 over sigma squared,
this guy over here, this n.
So that's really just 1 over n, OK?
So the posterior distribution is a Gaussian centered
at the average of my observations
and with variance 1 over n, OK?
Everybody's with me?
So just why I'm saying this, I mean,
this was the output of some computation,
but it sort of makes sense, right?
It's really telling me that the more observations I have,
the more concentrated this posterior is,
concentrated around what?
Well, around this xn bar.
So that looks like something we've sort of seen before,
but it does not have the same meaning somehow.
This is really just the posterior distribution,
and it's not really, I mean, it sort of says,
it's sort of a sanity check that I have this 1 over n when
I have xn bar, but it's not the same thing
as saying that the variance of xn bar was 1 over n,
like we had before, OK?
So as an exercise, well, you probably will have it,
but I would recommend, if you don't get it,
just try pi of theta to be equal to some n mu 1, OK?
So here, the prior that we used was completely non-informative.
What happens if I take my prior to be some Gaussian, which
is centered at mu, and it has the same variance
as the other guys, OK?
So what's going to happen here is that we're
going to put a weight, and everything that's away from mu
is going to actually get less weight, right?
And I want to know how I'm going to be updating this prior
into a posterior, so that's, right?
So everybody sees what I'm saying here.
So pi of theta is just so that means
that pi of theta has the density proportional
to exponential minus 1 half theta minus mu squared, right?
So I need to multiply my posterior with this,
and then see what I'd say actually going to be a Gaussian.
This is also a conjugate prior.
It's going to spit out another Gaussian.
You're going to have to complete a square again,
and just check what it's actually giving you.
And so spoiler alert, it's going to look
like you get an extra observation, which
is actually equal to mu, OK?
So it's going to be the average of n plus 1 observations,
the first n ones being x1 to xn, and the last one being mu.
And it sort of makes sense.
OK, so that's actually a fairly simple exercise.
But before, rather than going into more computation,
this is something you can definitely do
in the comfort of your room, I want
to talk about other types of priors, right?
So the first thing I said is, OK,
there's this beta prior that I just pulled out of my hat,
and that was just convenient.
Then there was this non-informative prior.
It was convenient, right?
It was non-informative, so if you don't know anything else,
maybe that's what you want to do.
The question is, are there any other priors
that are sort of principled and generic in the sense
that the uninformative prior was generic, right?
I mean, it was equal to 1.
That's as generic as it gets.
And so is there anything that's generic as well?
Well, there's these priors that are called Jeffers priors.
And Jeffers prior is a prior which
is proportional to square root of the determinant
of the Fisher information of theta, OK?
And so this is actually kind of a weird thing to do, right?
It says, compute your, look at your model, right?
Your model is going to have a Fisher information.
Let's say it exists.
And because we know it does not always
exist, for example, in the multinomial model,
we didn't have a Fisher information.
And so the determinant of a matrix
is somehow measuring the size of a matrix, right?
And if you don't trust me, just think
about the matrix being of size one by one,
then the determinant is just the number that you have there.
And so this is really something that
looks like the Fisher information.
I mean, it's just basically the amount of information
is proportional to the amount of information
that you have at a certain point, OK?
And so what my prior is saying is saying,
well, I want to put more weights on those datas that
are going to just extract more information from the data, OK?
So you can actually compute those things, right?
So in the first example, Jeffery's prior
is something that looks like this, right?
I mean, in one dimension, Fisher information
is essentially one over the variance, right?
So that's just one over the square root of the variance
because I have the square root.
And when I have the uniform, sorry, the Jeffery's prior,
when I have the Gaussian case, right?
So this is the identity matrix that I
would have in the Gaussian case.
So the determinant of the identity is one,
so square root of one is one.
And so I would basically get one,
and that gives me my improper prior, my uninformative prior
that I had.
OK, so the uninformative prior one is fine.
I mean, clearly, all the datas carry the same information
in the Gaussian model, right?
I mean, whether I translate it here or here,
it's pretty clear that none of them
is actually better than the other.
But clearly, for the Bernoulli case,
the piece that are closer to the boundary
carry more information, right?
So I sort of like those guys because they just
like carry more information.
So what I do is that I take this function.
So p1 minus p, remember, is something
that looks like this on the interval 0, 1, 0, and 1.
So this guy, 1 over square root of p1 minus p,
something that looks like this, agreed?
And so what he's doing is sort of like
wants to push towards the piece that actually
carry more information.
I mean, whether you want to bias your data that way or not
is something you need to think about, right?
I mean, when you put a prior on your data,
on your parameter, you're sort of like
biasing towards this idea, your data,
and maybe that's maybe not such a good idea when you have
some p that's actually close to 1 half, for example.
You're actually saying, no, I don't
want to see a p that's close to 1 half.
Just make a decision one way or another,
but just make a decision.
So it's sort of forcing you to do that.
OK, and so Jeffery's prior, so I'm running out of time,
so I don't want to go into too much details.
But we'll probably stop here, actually.
So Jeffery's prior, OK, sorry.
What happened here?
Yeah, so Jeffery's priors have this very nice property
is that they actually do not care about the parameterization
of your space, OK?
So if you actually have p, and you suddenly
decide that p is not the right parameter for bernoulli,
but it's p squared, OK?
You could decide to parameterize this by p squared.
Maybe your doctor is actually much more
able to formulate some prior assumption on p squared
rather than p.
You never know.
And so what happens is that Jeffery's priors
are invariant into this.
And the reason is because, well, the information carried by p
is the same as the information carried by p squared somehow,
right?
I mean, those are essentially the same.
I mean, well, yeah, they're essentially the same thing.
And so I mean, you need to have a one-to-one map, right?
Where you basically, for each parameter before you
have another parameter, so let's call eta the new parameters.
Then the PDF of the new prior indexed by eta this time
is actually also Jeffery's prior.
But this time, the new Fisher information
is not the Fisher information with respect to theta,
but it says in the Fisher information
associated to the statistical model indexed by eta.
So essentially, when you change Jeffery's prior,
when you change the parameterization of your model,
you still get Jeffery's prior for the new parameterization,
which is, in a way, a desirable property.
So Jeffery's prior is just like non-informative priors,
or priors you want to use when you want a systematic way
without really thinking about what to pick for your model.
OK, so, well, OK, I'll finish this next time.
And we'll talk about Bayesian confidence regions.
We'll talk about Bayesian estimation.
Once I have a posterior, what do I get?
And basically, the only message is
going to be that, well, you might
want to integrate against the posterior.
Find the expectation of your posterior distribution.
That's a good point estimator for theta.
And then we'll just do a couple of computation.
All right, so.

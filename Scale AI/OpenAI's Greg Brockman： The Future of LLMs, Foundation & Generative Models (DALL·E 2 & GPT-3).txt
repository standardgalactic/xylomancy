We're joined next by Greg Brockman, President, Chairman, and Founder of Open AI, and Alexander
Wang, CEO and Founder of Scale AI.
Open AI is a research and deployment company whose mission is to ensure general-purpose
artificial intelligence benefits all of humanity.
For Open AI, Greg was the CTO of Stripe, which he helped build from four to 250 employees.
Please join me in welcoming to the stage Greg Brockman and Alexander Wang.
Hey, Greg, thanks for making it.
Absolutely.
I'd love to be here.
I want to start, actually, I don't know if you remember this, but we first met at this
summer camp called Spark, where you gave a presentation about, at the time you were the
CTO of Stripe, and you gave this presentation about sort of like everything you had accomplished,
and I was a member of that camp, and it was extremely memorable.
You had a lot of good sound bites, so.
I'm glad that it landed.
Kind of a full circle moment.
Well, I think to start out with, I mean, you've been CTO, and now you're President of Open
AI, but CTO of two incredibly iconic companies, Stripe and Open AI, in some ways, probably
two of the most iconic startups of the past decade.
I wanted to start out just by asking, in what ways are the two organizations the same and
being CTO the same, and in what ways are they different?
Thank you for the kind of words.
Yeah, I think that one thing that's very interesting to me about kind of having been part of both
of these organizations is seeing how much groups of people are kind of the same, regardless
of what the problem in front of you is, so I think that a lot of how we approached Stripe
was thinking from first principles.
I remember when we were pre-launch, and we had some buzz going, because we had some early
customers, and one of my friends took me out to launch, she was a VC, and he was like,
all right, I've been hearing about this Stripe thing, what's your secret sauce?
I was like, I mean, we just make payments really good, and he's like, no, no, no, come
on, you can tell me, what's the secret sauce?
That was the secret sauce, right, is that we had just rethought every single piece of
what we were doing from the ground up, from first principles, not sort of locked into
the way that people had been doing it, and we asked, how should it be, where's all the
pain, and does it need to be there?
I think that in AI we did much the same, we really thought about, okay, there's this field
that we're entering, and that we hire a lot of people who had been in the field, but a
lot of us also hadn't been in the field, and we came to it with beginner's eyes, and I
think that that approach of just not being beholden to all the ways people were doing
it, but also becoming expert in the way that things have been done, because if you just
throw everything out, you're also just going to be starting from scratch in a not helpful
way, so I think that that maybe is the deepest commonality between them, but obviously very
different organizations, for Stripe, I think that we ran the traditional startup playbook,
you basically come up with the innovation, and you just build, build, build, you get
in front of customers from day one, the story is that we gave the first API to a customer
who charged a credit card, and he was like, I would like my money now, please, and we
were like, huh, I guess we should build that. Open AI, we had research to do, like where's
the customer? And it really took us, I guess, five years, starting late 2015, and it was
really not until 2020 that we had our very first product, and so I think that that sort
of figuring out what you're even supposed to work on, did you do a good job? Should you
feel good on a day-to-day basis? I think that all of that had to come from within rather
than from without.
Yeah. Well, actually, I want to go back to this point that you mentioned around first
principles thinking. It's very interesting, because even, I remember maybe 2020 or 2021,
you would sort of, post-GPD3, you would talk to other researchers in the field, and even
they would still, there's still some degree of skepticism over the sort of core concept
of scaling up these models, and if there were still gains to be had, et cetera. And I think,
I don't know the story, but it seems like the sort of research sort of intuition that
led to GPD3, Dolly 2, that have really ushered in kind of a new era of AI, were probably
somewhat against the grain or somewhat unintuitive at the time.
One question I have for you is, I think now looking back, it's obviously very obvious
to point out GPD3, Dolly 2 basically have fundamentally accelerated AI progress and its relevance to
the world and its relevance to every industry and sort of have created the sort of most
recent AI wave. How is that matched up against your expectations when you were building these
technologies?
Yeah, well, I think the thing that's most interesting to me is that those models you
mentioned are kind of overnight successes that took many, many years to create, and
so from the outside, it looks like, wow, you just produced this model and that model, and
really on the inside, the GPT arc, that's a five-year arc. It really started with sentiment
neuron paper, which is back in 2017. Do you remember that paper?
I remember the paper. It was very cool, but it felt very novel.
It felt very novel. Yeah, very few people remember it. It was this very early result
where we basically had been training a LSTM at the time to predict the next character
in text. So we basically showed a bunch of Amazon reviews. We said, what's the next character?
And of course it's going to learn where the commas go. Of course it's going to learn
where the periods go, but of course it's not going to understand anything. But we found
a single neuron in that model that had learned a state-of-the-art sentiment analysis classifier.
I tell you, it's a positive review or negative review. That's understanding. I don't know
what understanding means, but it's semantics for sure. And that for us was like, okay,
this is going to work. The transformer came out late 2017, and my co-founder Ilya immediately
was like, that's the thing. That's what we've been waiting for. So you take this sort of
very early nascent result, put it in a transformer, and then that's GPT-1. GPT-2 is you just keep
pushing it. And I think that the algorithm we kind of run internally is that we do these
little sort of get signs of life, and you have to be very, very careful to distinguish
signs of life from like kind of just pushing too hard on a specific data set that isn't
really going to keep going. But if you kind of build those right intuitions, then you know,
okay, now is the time to put in more compute. Now is the time to put in more researchers.
Now is the time to really scale it up. And so GPT-2 obviously was exciting, and we were
all like, well, we look at the curves. The bigger we made this model, the more compute
we put in, the more data we put in, the more we just sort of got all the engineering details
right, those curves just got better. And so actually, you know, our goal was just to break
the paradigm. It was just push it until the curve stopped looking good, and we still haven't
managed to accomplish it.
Yeah. Well, I think one of the things, at least for me and probably for many people
who initially played with GPT-3, the like shocking thing was not, I mean, it wasn't necessarily
that even the model got better and better performance on, you know, established tasks,
is that it sort of had all these qualitatively new behaviors that felt very magical. And
even now, you know, there's prompts that, you know, you'll see on Twitter or whatnot
that are sort of really shocking. I mean, did you have these sort of like early moments
when you had the early model results where you're like, holy crap, this is magic?
Yeah. Well, I think that the earliest one that I remember was around code. And just,
you know, at the time, totally mind blowing that you could just write a function name
and a doc string kind of describing what the function should do and actually write it.
Not super complicated functions, right? But just that it was able to, you know, you ask
for, you know, something to take a couple lines and they would be able to really do
it. You modify things a little bit to make sure it hadn't just memorized it. And sure
enough, it would write out the modified code. And I think, you know, the overall thing
that's really interesting about the paradigm of a GPT-3 is that where it really comes from
is that I, you know, we kind of had this picture that, look, the problem with these models
is that they're great within their data distribution. But as soon as you're outside of that distribution,
like all bets are off. And so what if you just make the whole world, the whole universe
be the data distribution, right? You put the whole internet in there. And I think that
what we've really seen is that these models, that they really are able to generalize extremely
well within the kinds of things that they've seen. You know, again, different question
if it's never seen anything like it. I mean, humans are also not very good at things you've
never seen before. But I think that that picture of just, like, all the different things that
it's seen in all these different configurations is almost unimaginable. There's no human who's
been able to consume, you know, 40 terabytes worth of text. And so I think that we just
keep seeing surprises where you just ask for, one of my favorite ones actually was this
teacher-student interaction where I was the teacher, model was the student, and I managed
to teach it how to sort numbers. And you just kind of have these experiences where, like,
that's what it should be like to interact with an AI.
Yeah. I mean, it's incredibly shocking. You know, one of the things I'm curious to get
your thoughts on is, I think, in the path of developing GP3, you know, required, I think
probably the jump from GP2 to GP3 required a lot of conviction, because, you know, you all
were spending probably a fair amount on a compute at the time to be able to train these
models, and there were probably a lot of experiments that didn't work, and so you had to be
willing to keep going after it. Did that phase of the journey, sort of this, like, GP2 to GP3
jump, was it scary? Did you have doubts? Or were you very confident that, hey, you know,
we're going to scale this up, and even though we're going to not get it right the first few
times, it's going to be amazing. Yeah. And to your point that scale was not an obvious
thing, not the company, but the scaling things up, at the time, the funny thing is actually
our very first scale result that just sort of convinced us that this is the right way
to approach things. You push until it breaks. Not necessarily that more compute is just
magically always going to solve your problem. It was Dota. That was, you know, playing competitive
video games, and there we kind of went through this whole, that was a three-year arc where
we started out with something that didn't do anything, finally beat, like, you know,
the in-house team, then we managed to go beat the pros, and at each step, it was just kind
of pushing in all dimensions, right, is make the model bigger, it's to, you know, sort
of, again, fix all the bugs, and you just kind of keep iterating on every single dimension,
and every single dimension yields returns. And so I think that we did very much the same
thing where, for GP2, you know, it's not as simple as saying, okay, like, clearly you
just need to, like, you know, crank up this one variable, and you just do it in one shot.
It's this, like, sort of, iterative, like, stepping through the space on each axis at
every single time. And so I think that, on the one hand, it does require conviction,
because you do need to say, we're going to, like, carve out a big compute budget, so you're
not constantly not kind of fighting other people for the big supercomputers. But on the
other hand, I think it's also very iterative, and you don't have to make scary irreversible
decisions, because at each step, you get feedback from reality. And I think that that key of,
like, both the big picture, thinking of what if this works, and make sure that you're really
set up for success, but also don't blindly spend a year of your organization on just,
like, pursuing a thing that might not pan out. I think that balancing those two is what was
really key.
Yeah. I mean, one of the cool things is you sort of walk through this and talk through
the insights is that the sort of organizational learnings were really critical in this entire
sort of path-dependent sort of path to GP3. It makes sense when you say it that sort of
insights from Dota 2 and insights from the sentiment neuron were sort of like the key
nuggets that led to the sort of crystallized idea of scaling up and building GP3, but it's
very unintuitive from the outside. And I think it's almost a statement of innovation in some
sense is that you're going to piece together this sort of disparate collection of insights
that you gather from a wide variety of experiments, and eventually you sort of like get the ingredients
together and you build something.
Yeah. That's the first principle is thinking in action.
Yeah. You know, I think that the story of AI, I don't know if you think about this at all,
but I think about this a little bit. I think the story of AI to date, and especially the
past few years, and the story of open AI is probably going to be something that historians
are going to study for, you know, decades and decades to come. Are there any fun stories
from the journey of creating some of these foundation models that you think deserve to
be in the history books?
Well, I'll tell you my actual favorite story from the Dota days. So, you know, we've been
working on this system and, you know, actually the funny thing is at the very beginning we
wrote down our list of milestones. On this date, we're going to be, Jonas, our, you know,
best open AI employee who also had many thousands of hours of Dota 2 gameplay. This date, we're
going to beat the semi-pros. You know, this date, we're going to beat the pros. And so
it's supposed to be like June 6th or something. June 6th rolls around. We don't have anything.
Like, you know, he just crushes us and two weeks go by, three weeks go by. We keep pushing
back that deadline by a week, every week. And then one day we actually do beat him.
And you know, I think my conclusion was that, like, it wasn't actually actionable to sort
of set those goals of outputs. You can only control your inputs. You can control the experiments
you run. And so we just managed the project very differently after that. And the thing
that was so crazy to me still is that, you know, so a week before the international,
which is the, like, world championships, we're going to show up, we're going to play 1v1
against the best players in the world, we finally started beating our semi-protester.
And we're like, okay, maybe this is actually going to happen. But then we learned that
he actually was on, like, vacation. He didn't have his, like, real setup. And so we were
like, oh, no, this is not going to go well. So we show up, you know, we continue to train.
We kind of do like a Hail Mary of, like, scaling things up, biggest scale we've ever done.
And we show up at the international and we play against, you know, like, sort of lowest,
you know, low ranked pro, like a previous pro. And we go 303021. So we basically win-win.
And then we did have one loss. And we take a look at it and it's like this item that
we've never trained with, we've never seen before, like, oh, wow, okay, we need to add
that and, you know, do it fast. And so the team stays up all night putting this thing
into the training, getting the whole thing launched. And, you know, again, like, we did
double the scale where we're basically maxing out our CPU cores at this point. And I start
training and, you know, we're supposed to play against the top pros in the world. Fortunately,
they can't do the next day, so we get an additional day of training. And the number two person
comes in, he plays against us, and we win-win-win-win-win. And he's like, okay, but I beat this, but
the top player is never going to lose to this thing. Or sorry, yeah, the top player is going
to crush this thing. And fortunately, because he had spent so long playing that that guy
couldn't come that day, so we got one more day of training. And that one more day of
training was enough. And so I think it's just the story of, like, you can really see the
improvement. And at each step, we could see new behaviors that the system have learned. And I
think that that experience of just sort of watching it grow up in front of you is just
something that was really amazing.
I'm actually surprised that, because you would, I sensibly you'd probably train the sort of,
like, agents for a long, long, long time going into the international. I'm surprised that
each incremental day.
Yeah, so this is something that I think has changed over time. So at the time, we basically
had two weeks worth of training was like the whole model run. And so you'd start from scratch
each time. And the thing that was really funny in the middle was that, you know, we'd put in
this new item, we were training it. And when we took out of training, it was the best spot
we ever saw, except that our semi-protester was looking at it and was like, this bot is
doing something really dumb. It's just sitting there in the first wave and taking all this
damage it doesn't have to. I'm going to go beat it. He ran it and go fight it. And he
lost. It's like, that was weird. And he did it like five more times. He lost each time.
But then he figured out a strategy that actually does work, which is you realize what was going
on was it was baiting him. It had learned to deceive. It actually learned that what you do is
that you pretend, oh, I'm just a weak little bot. I don't know what I'm doing. And then a person
comes in and you're just like smack. And so the way you defeat that is that you actually, you
don't fall for the bait, right? You let the bot take all this damage and sit there and get
weaker. And then you finally go in for the kill. And so there we actually stitched together our
good bot for the first wave with the deceived bot thereafter. And so there was a lot of this
sort of like really examining what was going on in the systems because it's such a limited
domain. You know, it's a complicated domain, but it's very, very interpretable. It meant that we
could observe behaviors like this and figure how to engineer around them. But once we graduated
from the 1v1 version of the game to the full 5v5, you know, much more like, you know, like
competitive basketball or something rather than heads up, suddenly all of our analysis of the
behavior stopped working, right? We used to have someone who just literally would watch the bot
play and be like, oh, we have this bug in the training. We got to go fix that. For 5v5, we
just could not do that. And I think that's kind of where we've graduated as a field is that too,
when you look at GPT-3 and the mistakes it makes, sometimes people ask, well, why did it make that
mistake? And sometimes you can interpret it. But sometimes it's also a little bit like asking,
well, you know, why did you make a mistake on some tests? It's like, well, you think you know,
but like your explanation isn't always very good. I think that to do complicated behaviors,
sometimes there's a very complicated explanation. Yeah. Have you read this short story? I think
it's like the Lifecycle of Software Objects by Ted Chang? I think I have, but I don't recall.
It's about how these AI pets and they sort of like keep learning new and new behaviors. It's
very reminiscent of what I'm describing. These Dota agents. Yeah. I think we'll see that kind
of thing in our future somewhere. Yeah. I want to kind of go back, you know, one of the things,
we've known each other for many years, long before, you know, these foundation models and even
before this competition, Dota 2. And one thing I vividly remember is how sort of optimistic
and confident you were in sort of this sort of path of increasing and increasing AI capability.
You know, sort of, I remember the time as maybe 2016, 2017. It felt very striking because it was
sort of like, you know, with these algorithms, they're still pretty weak. And you were always
very confident like, oh yeah, they're just going to keep getting better and better and better. And
you know, you had a lot of confidence in that. What were the things that back then gave you
sort of the resolve or confidence in the, and the optimism in the technology?
Yeah. I mean, at some level, you know, to have that kind of belief and conviction and something
that hasn't happened yet, it's a very intuitive thing. I mean, I remember when I was in school
and showed up excited about doing NLP research, I went and tracked down an NLP
professor and I was like, please, can I do some research for you? And he's like, okay,
he shows me these like, parsed trees and stuff. And I look at that, I was like, this is never
going to work, right? And you know, to explain like, why does it feel like it's not going to work,
it just doesn't have the right properties, right? It just felt like you're going to pour all this
human like engineering and intuition and effort into the system. And I know I can't even describe
how language works, right? It just feels like there's just something inherently missing. I think
neural nets have the opposite property. Neural nets is very clear. This is a system that absorbs
data, absorbs compute. It's like a sponge that just like slurps everything up. And so it has
the right form factor. But the thing that's always been missing as well, can you train it, right?
Do you have enough data? Do you have enough compute? Do you have enough ability to like have a learning
algorithm that can shovel this stuff in efficiently in a way that it comes out in some way that
generalizes? Like that's the thing that's been missing. And I think what became kind of clear,
you know, the field really, I think God, it's most recent resurgence in 2012 with the AlexNet
paper. And I think that there, that was the first time where you had a neural net that really just
crushed a task, right? That it was like people had spent decades on computer vision and suddenly
it's like, well, I'm so sorry, but this approach has just supplanted you by this, this massive gap.
And I think that you just started to see it spread, right? That it was almost like you had these,
these, all these disparate departments and there was this wall that was being knocked down
day after day. And I think that when you see a trend like that were things that have been
longstanding and very deeply established in these ways of thinking, these great debates that have
gone on for a long time. And suddenly you're seeing a repeated result that is consistent with
the history. I think that that, for me, is maybe the most clear sign that it's like something is
going to work. And there's a real sort of exponential that is waiting to unfold.
And then, you know, were there, what were the moments, if any, of doubt in, you know, let's,
let's chart the path. I think open answer in 2016.
Yeah, yeah, I'd say December 2015, you know, 2016.
Okay, great. December 2015 till now, were there any moments of doubt in the technology? Or was it
sort of always, hey, this is, you know, this is clearly the way of the future?
Yeah. I mean, I think that doubt is a strong word. There's definitely moments, like,
I think to build something, you're always doubting, right? That you're always like,
you've got to be questioning every single bit of your implementation. Like,
anytime you see like a graph is wiggling in a weird way, you've got to go figure it out.
You can't just be like, I'm sure that the AI's will sort it out.
And so I think there was like lots of sort of tactical doubt, lots of like sort of worries
that we're not quite doing it right. Lots of like redoing the calculations to figure out like,
hey, how big of a model do you think you're going to need? Lots of mistakes for sure.
Like a good example of this is the scaling laws. So we did this study to actually start to really
scientifically understand how do models improve as you push on various axes? So as you pour more
compute in, as you pour more data in. And one conclusion that we had at one point was that
basically that there's sort of a limited amount of data that you want to pour into these models
and that there's kind of this very clear curve. And that one thing that we realized only years
later was actually that we'd read the curves a little bit wrong. And you actually want to be
training for way more tokens, way more data than anyone had expected. And that there's definitely
these moments where these things that just didn't quite click, where it's like it just didn't add
up that we were training for so little and that, you know, something inclusion that you drew
downstream. But then you realize there was a foundational assumption that was wrong.
And suddenly things make way more sense. So I think it's a little bit like, you know, physics
in some sense, we're like, do you doubt physics? It's like, I kind of do. I think all physics is
wrong, right? But like, only so wrong, right? It's like, we clearly haven't reconciled like
quantum and relativity. So there's like something wrong there. But that wrongness is actually an
opportunity. It's actually a sign of you have those things are useful, right? It really like
has affected our lives. And it's actually like pretty great. I'm very happy with what physics
has done. But also there's fruit. And so I think that for me, that's always been the feeling that
there's something here. And that, you know, if we do keep pushing and somehow the scaling
law is all peter out, right? And they suddenly drop off a cliff and we can't make any further
progress. Like that would be the most exciting time in this field. Because we would have finally
reached the limit of technology, we would have finally learned something. And then we would
finally have a picture of what the next thing to do is. Yeah, that's super. It actually reminds
me of this one of the stripe operating principles, which is I think micro pessimists, macro optimists.
Yep. Yep. Yes. And it's very, I mean, it's very resonant, but obviously like very related to
what you're talking about, which is these, you know, you have to be extremely pessimistic or
extremely questioning in the moments of the technology, but then obviously on a long enough
time horizon, incredible stuff pops out. Yep. You got to be excited. Like I think that this is just
an exciting field and it's a scary field as well. You got to have some amount of just like awe at
the fact that you have these models that they started as random numbers, right? And then you
have build these massive supercomputers, these massive data sets and you do a ton of the engineering
work. You do a ton of these algorithmic developments and you put them all into a package, right? And
we don't really have other technologies that work like this. Like I think the fact, to me, the most
fundamental picture, this like sponge that you just kind of pour stuff into and you get this model
that's reusable and works across all these different areas. Like you can't do that for
traditional software, right? Traditional software is it's just, you know, human effort writing down
all the rules and that's where the return comes from. But you can't, you know, maybe you have like
a spark cluster that does some stuff, but that's not the cake and in neural networks it really is.
Yeah. You know, I want to kind of switch gears to thinking about the sort of future and looking
forward at what's next. What do you think, I mean, I'll ask this sort of as broadly as possible to
start with, what do you think the future of AI holds? Yeah. I think that the future of AI is
going to again be both exciting and a source of a lot of change. And I think that that is something
that, you know, part of our mission is to try to help facilitate that in as positive way as possible.
I think that kind of, you know, at a super high level, I kind of feel like AI was like, you know,
something that for the, you know, 2010s was like kind of cool, you know, it's the game of like
publishing papers and you play some video games and like, you know, it's just like fun, good science.
I think it's really interesting that 2020 kicked off with GPT-3, which was really the first model
that was commercially useful just as the model, like literally put an API on top of it and people
just talk to it and people build products on top of it. And, you know, one of our early customers
just raised it at 1.5 billion valuation, which to me is a really wonderful thing to realize that
you build this model and it creates so much value for so many different people. And I think that
we're still in such early days for what these models can do. And so I think that what I'm most
excited about from just seeing GPT-3, seeing Dolly, is thinking about the sort of economic value that
it can create for people. And I think that there's a lot of other pieces to it in terms of like,
you know, that everyone's going to be more creative if you want to, like, I can't draw.
But now I can create images. Now I can take a picture of this in my head and I can actually see it
on a page. And one of my favorite applications of Dolly is actually people who are 3D physical
artists, you know, someone who's like a sculptor. And now they can actually get a great rendering
of the thing that they have in mind by kind of just like iterating with this machine and then
they go build it, right? I think that this sort of amplification of what humans can do is what
these systems are for. And so I think that for this decade, I think what we're really going to see
is these tools just sort of proliferating. They're going to be everywhere. They're going to be
baked into every company. I think it's kind of like the internet transition, that, you know,
that it was kind of like, if you're a company, like, what's your internet strategy? And, you know,
1990, it's like, what even is this thing, you know? And in 2000, it's like, huh, maybe it's
interesting. And there there's a little, you know, boom and bust. And here we are today,
even talk about an internet strategy is like, it's just so integral to every business. It's not
even like, it's not even a separate thing, right? It's just like, it's just part of like your,
it's like your payroll strategy, right? It's like, it's not like a separate part of your business
that you can pick or choose whether you're going to have it. And I think that AI is going to be much
the same. I think there will be a transition point, right? I think that it's interesting, like, our
mission is really about building artificial general intelligence, right? Really trying to build
machines that are able to perform whole tasks, right? That are, you know, push this technology to its
limit and build machines that are able to, you know, our charter definition is outperform humans
at most economically valuable work. And there's a question of the timeline, but I think that that
picture of, you know, you have these tools that are creative that help everyone amplify and what
happens when they do become so capable that they're able to perform these tasks even autonomously.
And I think that actually the implications of that are different from what people expect. I think
that it's much more like, you know, that I think there's still going to be this amplification.
But I think that there the change is going to be just very hard to predict and unexpected. And I
think that really thinking about how all of that sort of value gets distributed, how to make sure
that it's sort of pointed at solving these, like, hard challenges that humans, you know, maybe are
unable to solve ourselves, you know, the climate change and, you know, universal education and
things like that. And really transitioning to this, like, AI-powered world, I think is going to be
just like a real sort of challenge for the whole, you know, all of humanity to work together on.
Yeah. I mean, I totally agree. One thing that I think is almost funny with how
the timing of all these technologies have worked out is that, you know, last year everyone was
talking about Web 3 as crypto. And now it feels very obvious that AI is the actual Web 3, you know.
Well, we'll take Web 4. Yeah, Web 4. We'll skip, we'll skip over one. But sort of like Web 1 was
just reading. Web 2 was reading and writing. And now Web 3 or 4, depending on what we want to say,
is ads, computer reading, computer write. And it's sort of this incredible new phase. You know,
so I think you mentioned two directions here that I think are really interesting. One is the sort of
advancement and sort of proliferation of GB3 and Dolly and sort of the existing tools becoming
more and more economically useful. And there's sort of this continued improvement of the algorithms
themselves towards sort of AGI. What do you think, and obviously don't reveal any opening secrets,
but what do you think the sort of like road map to AGI looks like from where we are now?
I mean, I think that humanity to a large extent has been on the AGI road map for a very, very long
time. I think even looking at just the history of neural networks in particular, you know,
on the one hand we say, hey, 2012, like that was the moment, like everything changed, you know,
that like you look at these, we have all these curves of how much compute people put into the
landmark results. It was going like 10x year over year, still continuing by the way. That's a decade
of 10x year over year. That's insane. And the thing is we actually did a study to then look back
at previous results all the way back to, you know, say the perceptron in 1959. And you actually find
that there's basically a very smooth curve back there as well. The amount of compute going into
all the landmark results was exactly Moore's law. And it kind of makes sense, right? It's like that
people were not willing to spend more money. They wanted to spend a constant amount of money on
these experiments because you're starving grad students. Like, you know, you can only get so
much computer time. And that the results got better and better, the more compute was available to them.
And I think that that is so interesting that, yeah, basically what changed in 2012 was that we
said, okay, we're just going to like, you know, we are going to spend more money. We're going to
build massive supercomputers now because the ROI is there. But that fundamentally the curve,
if you control for that cost factor, it looks exactly the same. And so I think that basically
this picture of building more capable models by pouring more compute into them, by getting better
at harnessing this technology of neural networks with back propagation, I think that has been very
invariant. And the details, you know, maybe change a little bit. You know, do you want to work on
GPT-3? Do you want to work on whisper? Like, do you pour in your, you know, speech data? Do you
pour in text data from the internet? And to me, those details, I think, you know, they matter in
the, like, in the, like, sense of, like, what are you going to work on today and, you know,
what are you going to download. But if you zoom out and you look at the scale of, like,
these, this technology, I think it actually sort of doesn't matter so much. I think kind of what
we're building is almost like building computers. Like, you think about the Haiti and Moore's law,
right, where it's just like, there's a new chip that comes out and then there's a new chip that
comes out. And it's kind of like, what's the, you know, what's the path to building the best
computer? The answer is, well, you just keep building the next best chip and you keep building
the next best chip and you keep getting better peripherals and all these, you know, you keep
working on every single piece of the technology. And so I think this full stack of better GPUs,
great software for utilizing them, neural networks that we learned to harness more and more, the
scaling laws, doing all the science alignment, extremely important, making sure these models
not just are smart, but actually are aligned with what humans intend. All of that, I think,
is the stack. And so I think that, you know, what our goal is is just to keep doing something that
was previously impossible every single year. So, you know, I guess we'll, you should check back
in a year, but hopefully 2023, we'll all forget about Dali 2 and GPT 3 and we'll be talking about
something new. And I think as long as we continue that, like, you cannot continue that path without
ending up somewhere amazing. Yeah. I mean, I think, I actually remember this in, I think probably
2017, you were sort of very, still quite, you were very excited about sort of the sort of Moore's
law continuing and that, that sort of creating a lot more opportunity for neural networks and AI
and that's sort of played out. Are you worried about the sort of proverbial end of Moore's law
kind of causing a stall out in progress? So, I'm not worried about it per se. Like, I think the
way to think about this, right? Because I think, you know, we often get caught in this debate of,
like, is it all about scale? Or is it all about algorithms? Is it all about data? And the answer is
that's the wrong question, right? It's really like you multiply together these factors and the best
thing to do when you're multiplying together multiple terms is that you actually kind of want
them all to be equal. And I think that the, the answer is like, it's been great for the past,
you know, seven years that we've been able to just pour more dollars to build bigger computers.
That's one way to get ahead of Moore's law. At some point, there just aren't more dollars, right?
There aren't more grains of sand to, you know, that have been turned into, into these, these wonderful
computers that we use. So, there is a limit there that we have not yet hit. But when you do,
that does not stall all progress, right? You still have algorithmic progress. And there we've
again done studies. And we've shown that actually if you take, like an example is, if you look the
amount of compute it takes to hit the same performance, so to train, you know, say the art
at, you know, 2012 or 2014, vision model that, that computes also falling exponentially. We're
basically making exponential progress in algorithms. Not at the same rate as we are able to, you know,
sort of build bigger computers. But that is an amazing force too. You know, it's like, I've got
this exponential, I've got that exponential, like, let's not even talk about the data exponential.
So, I think that the truth is that we will find a way. I think that the history of this field is
just so consistent. And I think that, you know, humanity is just so innovative that I think that,
that we're not going to hit a wall for, for the foreseeable future.
And do you think that, you know, one of the, one of the interesting juxtapositions of, of today,
just from a scientific perspective is a relative slowing in nearly every other science. And there's,
you know, there's a lot of research that sort of demonstrated that science on the whole slowing,
and then comparatively the sort of acceleration of artificial intelligence and sort of this,
this, you know, in many ways this renaissance that we're, that we're entering right now.
Do you, do you fear that at some point AI will similarly sort of reach these points
of admission mark returns and slow relative, like much in the same way that other sciences
have? Or do you think that's so far away that, you know? Well, I think two things. I mean,
I think that there's, there's always S-curves. Although I think that something that's also
interesting about S-curves is that there tends to be paradigm shifts. Like, have you ever read
Singularity is Near? No, I haven't. Yeah. So, this is the Ray Kurzweil book from like 2004 or
something. And I always thought just based on the reputation, it's going to be kind of a crazy
book. But if you actually read it, it's the most dry boring reading you'll ever do. And it's basically
just curve after curve of different industries within computing, showing how the performance has
changed over time. And it's, you know, basically the conclusion it comes to is that there's this
repeated pattern that seems to happen across, you know, memory across number of transistors on the
chip, you know, et cetera, et cetera, where you kind of have an S-curve of the current paradigm.
And then you have a paradigm shift. And that, an example he talks about is, you know, thinking
about, let's talk about CDs, right? So, you talk about great, you know, a CD adoption. It's like,
you know, it's great S-curve. It's suddenly everywhere. It's like everyone's got a CD player.
Like, it's just the technology of the day. And people get really excited about doing more of the
same thing. It's like Blu-ray. That's the thing, you know? And so then everyone starts investing
in Blu-rays and somehow it just doesn't take off. And it's because it's just more of the same and
it's like, you know, it's not backwards compatible and so it's just not really worth it. But the real
paradigm shift was streaming, right? Suddenly you have this new adoption curve, this new S-curve
that just is this like totally different way. And the way we got to fast computers was basically
five different paradigm shifts across 100 years. And so I think that that's maybe a story here too,
which is like, there's going to be an S-curve in what we're doing right now, and that there will be
a paradigm shift when you hit it. And I think that that, again, speaks to the ingenuity of humans.
But I think there's also a second thing where my other answer is, to some extent, it doesn't
matter because the thing about this field is that it's useful now, right? That kind of the goal
that I think we've always had for AI was to actually make it so computers are just way more
helpful. Like, you think about what computers have done for humanity, right? Like, how many
problems they've helped us solve. They've created new problems as well. But I think that on net that
they've helped us solve way more problems than they've created. And I think they've kind of just
changed the nature of how we interact with each other about how, you know, like, it's just like
hard to get lost anywhere, right? You just plug Google Maps. And I think there's really amazing
problems that are now within our reach that just would not have been otherwise. And I think that
AI, like, we're starting to crack that nut. We're starting to be able to, you know, like, it's,
I think it's kind of interesting you could get a co-pilot, you know, which we power, we have the
models that power it. And that the way that that is useful to people is that it provides very low
latency suggestions, right? It's basically an autocomplete for code. And that, you know,
there's a very strict latency budget. You know, if you're more than, you know, 1500 milliseconds
to get a autocomplete suggestion, it's worthless. Like, no one wants that. You've already moved on.
But I think that what we really want to build the next phase is machines that help you produce,
that are able to produce artifacts that are materially interesting on their own. So not just
interesting because it's like a fast suggestion to you, but because it's actually a quality answer.
And you're starting to see it. If you talk to our current GPT iteration, you can ask it to
write some poems, and it writes way better poetry than I can. It actually wrote a poem for my wife
that made us both cry, like, you know, yeah, I cannot do that myself. But now I can, you know,
by partnering with this machine. And I think that that's the real story, right, is really trying to
get these tools out and everywhere. And yeah, you know, if what we're doing right now stalls out,
I don't think that removes the value from what we're able to create.
Yeah. By the way, it's depressing that the attention span of most engineers is only 1500
seconds, you know, or maybe 100 milliseconds, but it is what it is. What if anything, you know,
I think one of the things that, if I recall, spurred you to work on OpenAI was sort of
also being concerned about the sort of potential negative consequences of the technology.
What, at this point, looking forward, what are your sort of biggest concerns or what are you
afraid of with artificial intelligence that you sort of urge everyone in the field to sort of help
avoid? Yeah, so I think that one thing that's very interesting about AI is that, you know, if you
talked to, certainly, you know, 10 years ago, if you looked at every article about it, you talked
to someone on the street, Terminator is the main thing that comes up, right? So I think there's
always been this feeling around AI that is sort of, you know, that there's an element of fear
mixed with the, you know, sometimes people don't see any potential or sometimes, you know, they
realize that there is the potential, but like really trying to figure out and navigate it.
And I think that that picture, the specifics, you know, I think that we're starting to see a
little bit more, but I think the high-level picture of this is technology that's very powerful,
and it can be powerful in positive ways and negative ways, I think is extremely correct. And I
think it's very important not just to be, you know, starry-eyed, optimist, everything's just
going to work itself out, but also not to be, you know, sort of doomsday like everything is
terrible and, you know, humanity is over because I don't think that's at all true. I think this
technology can be the best thing that we have ever created and help us be the best versions of
ourselves, but I think that it requires very careful navigating of the space, and it's not
something that just is for, you know, companies of Silicon Valley to figure out. I think it's really
all of humanity kind of challenge. So I think that we're going to go through different phases.
I think that right now, you know, we're kind of starting to build systems where
that, you know, you think about misuse is the most clear problem, and that the systems themselves
are still not very powerful, right? That the kinds of things you worry about for GPT-3
are, you know, important problems, and you think about bias and representation, you think about
the system sort of, you know, sort of saying the wrong thing, you know, but its action is really
in your mind, right? It's sort of where it's on a page that then, you know, where it's on a page
are very powerful, but that they don't themselves have direct action in the world. We think about
something like Codex, our code writing system, which is a little bit more like a robot because it
has, it emits code, and if you were to just execute that code directly, it can actually
directly have actuators into the world, and making sure that that's aligned and doing the right
kinds of things, not having buggy code and not writing viruses and that kind of thing, like
that's really important. And so I think that figuring out what values go into these machines
and that they're operating according to those values, that's going to be very, very critical.
Figuring out how to avoid misuse and sort of regulate that, both at a sort of societal level,
at a, you know, technical level, all of that is very important. And I do think that there's also
a point where the technology itself, you have to think about that it's going to be extremely
powerful, and you think about a system that's, you know, sort of talking to lots of humans and
is operating unchecked. That's the kind of thing that you should worry about. You know, we already
worry about that. You think about the companies, right, that lots of people are using this,
you know, social media platform or, you know, any of the technologies that we use
and how much influence those can have in the world, and those aren't systems that have,
you know, sort of deep, sort of, behaviors that are emergent from what they've learned.
And so I think that figuring out the technical controls to make sure that these systems
remain in service of humanity and sort of to actually empower and accelerate all of us,
that, I think, is also a very critical thing. So it's kind of this, like, ramping set of stakes
and making sure that we're building systems that are aligned with our values and figuring out what
that even means. Like, what is the values of humanity that should be in the system? That,
I think, is not going to be an easy problem. You know, one question, and this may be the last
question that I have for you, is sort of one of the things, one of the conclusions of if the
technology is such that scale continues to be the sort of one of the more important things,
you know, scale, whether it's data or better algorithms or scale of compute, then it, the
technology itself will tend towards sort of this game theoretical proliferation mode, where it's
sort of like people are going to compete, and you see some of this today, even with the large tech
companies and you guys, obviously, people are going to compete to sort of build the bigger
supercomputers that have the better performance, and you have the bigger supercomputer, you have
sort of supremacy over the other supercomputers, and sort of there's like this, you know, laddering
of the stakes and sort of, and proliferation is really the sort of the right word. Do you think
that that is a version of the future, or do you think that there's sort of some path in which
this becomes a much more sort of like open and useful, not this sort of like tool for nation
states or large companies to compete with one another? I think that the future that seems to
be unfolding is kind of a, you know, replay of how, say, computing technology has played out more
broadly. I think that it is still going to be the case that you're going to have these increasingly
massive supercomputers that are in the hands of only a few that are able to just create models
that can just do crazy things that no one else can do. But I don't think that that removes the
value from the massive set of things that people are going to do with these models. And so, you
know, I think that balancing the like super powerful, very dual use, extremely, you know,
think of these like, almost like, you know, these massive like, you know, sort of systems that are,
you know, think about like a nuclear reactor and it's like, you know, it's like these like giant
like sort of, you know, systems that you should approach with great care. And you think about,
like, you know, by contrast, think about wind turbines and like there's lots of wind turbines
everywhere. And actually, if you add up the amount of value from wind turbines versus nuclear reactors,
like I think actually that the balance is probably in favor of wind turbines. And so I think that
that's kind of the future we're going to is that the AI technology is going to be everywhere and
there's going to be lots of value that's delivered by having open source models that are integrated
to every business and that people are building all sorts of crazy applications on top of. And that's
something that we really want to support and promote. And you also have to have this dual
answer for what you do with the new, extremely capable stuff that's just a mile ahead of everything
else. And that's something you have to treat with kid gloves, with more care. And I think that that
balance is tricky. It's not easy. That's something that we as an organization have been trying to
straddle. And I think that, you know, we've had real existential struggles internally trying to
figure out, you know, like our goal is to empower everyone. It's really to bring everyone along to
this AI transition. And the best way to do that, I think that our picture of it has changed as the
technology has unfolded. And I think that we're starting to get a sense of, you know, where this
can go. It's really exciting to see all the energy of all these builders coming in. Because I think
that, like you said, people are starting to realize, like, AI is really going to work and it's time
to build. Yeah. Well, this was an incredible conversation. Thank you so much, Greg. Next time
we speak, I'll make you read the poem that you wrote. Cool. Thank you so much. Thank you so much.

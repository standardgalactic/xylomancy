All right, hi everybody. It's Michael Schirmer. We're here in the offices of the Skeptic Society
and Skeptic Magazine. I just want to ask you to give your support to us. We are a 501c3
non-profit science education organization. We promote science as opposed to junk science,
voodoo science, pathological science, bad science, non-science, and plain old nonsense.
And unless you've been abducted by aliens or sent by Elon Musk to Mars, for the last 30 years,
you know there's a lot of nonsense out there. Some people call us debunkers, but you know what?
There's a lot of bunk that needs debunking. That's part of our job, as well as explaining and
understanding why people believe in bunk. So if you want to support our efforts, go to
skeptic.com slash donate. Skeptic.com slash donate. Your tax deductible donation will support our
work here at the Skeptic Society. Nick, thanks for coming on. It's a great honor to speak to you.
I don't think we've ever met in person, but a long time sat at your work. And you've really
sparked an international conversation. Yeah, with the whole AI thing, yeah. Now it's been
fascinating in the years since Superintelligence came out in 2014, just how much has changed.
What used to be a very fringe topic. I mean, back then, at least in academia, the whole idea that
AIs could potentially achieve general intelligence someday, and maybe Superintelligence, and that
that could pose various kinds of risks, like was like dismissed as science fiction or futurism.
And there were like in the world in total, maybe like a handful of people scattered around the
internet trying to work on the AI alignment problem. And now, of course, all the frontier AI labs
have research groups working on this. And you have statements coming out of the White House and other
places focusing on transformative AI. So yeah, it's been an interesting journey.
Indeed. Let me start off with a statement from your colleague, Eliezer Yudkowski. You'll be
familiar with this. After ChatGPT came out, he published an op-ed in Time Magazine. That's actually,
oh my god, it's one year ago today, he published this. That's amazing. Many researchers steeped in
these issues, including myself, expect that the most likely result of building a superhumanly
smart AI under anything remotely like the current circumstances is that literally everyone on Earth
will die. Not as in maybe possibly some remote chance, but as in this is the obvious thing that
would happen. What do you think about that extreme statement? Well, I mean, so there's a spectrum
of people with different P dooms. It's non colloquially the probability of doom from AI,
where he's at one towards one end of that, like amongst like perhaps the most pessimistic or
certainly amongst them, amongst a set of people who actually have some knowledge and have thought
about this and then others have lower probabilities. But I certainly think there is a real chance,
a real existential risk that will be connected to this transition to the machine superintelligence
era. And it's non trivial. And we should work to reduce it by putting in the effort to do
develop scalable methods for AI control in whatever time we have available before this happens.
And there is now more talent and resources going into that. So that's the good news.
But I still think we don't yet fully have that problem solved. So your institute's the future
of humanity. The other one, the future of life issued that statement a year ago now,
calling for a pause on AI development. I noticed you didn't sign it. Why is that?
Well, I'm not the big signer of things in general. I just the whole with isms and signing and it's
just my personality. I feel there's also a little bit of a risk if you're a philosopher,
if you're a kind of your job is to try to be a little detached and to evaluate things and be
open minded. Like once you start to get involved in a particular campaign, it's very hard to
retain the ability to change your mind. It's not impossible, but it gets harder. And I feel
philosophy is hard enough as it is without adding extra difficulties. I have no objection to other
people. I think it's good people should be involved in campaigns and working for things. It's just,
I always feel a little awkward. Also, usually any one given statement, there is always something
that I slightly would have a different view or have worded differently. And so it's more just
my hang up rather than some kind of big statement I'm trying to make by not signing the statement.
Right. The only statements I sign are that there should be no signed statements. We should let people
just say whatever they want. Free speech. Yeah. Yeah. Okay. So just give us a little bit of
background. You're so famous in this area. How did you get interested in AI? I mean,
are you like a Star Trek fan or you know, go back to your childhood or whatever
teen years or whatever triggered you to go down this pathway?
Yeah. No, I'm actually not so much a science fiction type of reader. I mean, a lot of my friends
and colleagues are, I just never really been much into that. Now I had a, I mean, I grew up in Sweden
and this was before the internet in a relatively small town. And I knew nobody when I grew up who
was at all interested in literature or science or ideas or anything like that. So I was like
bored out of my mind in school and I associated sort of learning with school. So I didn't,
and then I sort of went to the local library. I think I was 15 randomly one afternoon and
started pulling out one book and another. And I realized that like that was actually a big world
of ideas, very different from the stuff that was covered in school. That was like super fascinating.
And then I pivoted and became kind of fanatically engaged in this project of self-education,
because I felt I hadn't been missing out. Like I've wasted 15 years of my life and I wanted to
make up. And then I started to study, I studied physics and AI and neuroscience and I painted
and wrote poetry and philosophy, of course, and just everything I could sort of lay my hands on.
And for us, almost as long as I remember, it always seemed to me that there's a bunch of things
we can do to change the world that consists basically of moving things around in the external world.
But what would be more likely to cause a profound change would be if one changed the
thing that does the changing. And so all of the technologies and ideas that we have ultimately
come through is sort of the burst canal of the human brain. So anything if you could sort of upgrade
the human brain or change our mood or cognitive capacities, that that would be potentially
transformative. And in parallel with that, if you could develop new brains through artificial
intelligence research, that also could be world changing. So I had this vague sense from early
on. And then I kind of, yeah, it got more specific as I went along neural networks intriguingly,
like actually, from the very beginning seemed to me like to have legs in the sense of being on
the right path. And I remember, I think I was like 17. And I had gotten this, like on interlibrary
loan from the local library, this this volume on parallel distributed processing, which was like
one of the first sort of by Rommel Hart and like this classic now, but like where they tried to
deconstruct biological neural circuits in mathematical terms. And I was like super
fascinated about that. And so yeah, and then I studied neuroscience, computational neuroscience
in later on in London. And now the so deep learning evolution seems seems to validate this
that these kind of massively distributed pattern recognizing of learning algorithms is the way to
go. Yeah. Yeah, you know, somebody like me who I don't work in this area, you know, there's so many
great, smart experts on all sides of this, you know, you have Elon Musk and even Hawkeying and
Bill Gates concerned about AI existential risk. And then you have other people like
Kevin Kelly or Stephen Paker going, no, no, no, this is not going to happen. We can, we can do
this incrementally. And so and I always think of you as sort of in the middle, maybe, you know,
your super intelligence book introduced the idea of the, you know, paperclip maximizer and the
alignment problem. But I didn't feel you went to the extreme position of existential risk, but
maybe give us a little bit of where you are now since that was 2014, right, super intelligence,
where you stand on on the threats of this based on that alignment problem.
Yeah, I think it is going to be a very powerful thing if we do create machine super
intelligence, it's not just internet 3.0 or a mobile internet or like one of these, like always
some new cool thing, right? But I think this is qualitatively different in that it will be the
last invention we ever need to make. If if we do it, because then it would do the father
inventing and presumably at digital speeds. And so I think it will be a transformative
with enormous upside and but but also potentially big downside if we failed to align it to human
values. And there is now a lot of resources where people are trying to explain how creating very
powerful optimization systems, unless you're able to sort of point them very precisely,
could could result in disasters in in various different ways. So if we are lucky, we will solve
that problem by the time somebody figures out how to solve the problem of making AI stats smart.
And as I said earlier, there are lots of people working on that now and including a lot of the
smartest people I know are kind of going into this now AI alignment and
more resources are being spent by frontier labs as well in terms of devoting.
One of the solutions would just be government regulation, like of any technology. So here's
my example, I have a Tesla. So I'm here in Santa Barbara, I want to go to LAX, take my flight,
I push the little button, I go navigate LAX. Now it knows to avoid the heavily trafficked
LA freeway. So it takes me down side roads and so on. The moment it takes me up on a sidewalk to
mow down a bunch of pedestrians in order to avoid some traffic, you know, how long would it be
before the Department of Department of Transportation and Safety Board, you know, swoop down and shut
down Elon's company to prevent that from ever happening again, you know, like a New York minute.
Maybe that's one solution.
Well, I mean, if it happens in small pieces and gradually like that, we might have the ability
to observe things going wrong and then take corrective measures. And that's how we deal with
most new technologies in the problems they cause. Like we invent cars, we find that they sometimes
crash, so we invent seat belts and traffic lights, etc. I think there is a small subset of things that
could go wrong that are in a different category. I call them existential risks. And these are where
there is a risk to the very survival of Earth originating intelligent life. And these are risks,
in other words, that would sort of put a permanent end to the human story, where we don't get the
second try. And so these are harder to deal with because we got to get them right on the first
try. I think AI is one potential source of existential risk. And there might be a few other
areas like synthetic biology might be another area where we could get unlucky and discover that
there is some relatively easy way to do something tremendously destructive. And so yeah, if one looks
at AI in particular as a source of existential risk, there are a few different ways in which it
could do that. One thing to recognize is that once you have something that is even just human
level, but even more so when you have super human levels of intelligence, is that it would be able
to anticipate our responses to it. So if it wanted to mow a lot of people down, if you were like some
sort of rogue self-driving car AI, right, it wouldn't just run over a few and then be surprised that
the Department of Transportation shut it down, because that would be an obvious thing that we
can even realize that. So it would make some smarter plan to achieve its goal of mowing people
down that might include things like deceiving us about its capabilities, deceiving us about its
goals and intentions. It would have converted instrumental reason, perhaps, to seek more
resources and intelligence while also convincing us that it is safe. So this can make such systems
harder to test, because they might behave very differently in the sort of deployment phase than
in the testing phase. And so yeah, and if we think like, you know, you could make analogies to like
when Homo sapiens arose on this planet and what happened to our Neanderthal brethren at that
point or when indeed at a slightly lower level of intensity, but when a technologically more
advanced civilization has encountered a less technologically advanced civilization, which
has happened. And often it doesn't end up very well for the less advanced civilization. So if you
imagine that delta between sort of human cognitive and technological capacity and what the AI could
do being very large, then we might have a kind of much bigger encounter with, but where we
now like with our like fancy Western advanced technology would be like the underdog and this
would be like basically like an alien civilization coming from the future, but in the shape of a
super intelligent AI that has kind of run ahead. So that's one like type of scenario. There are
other scenarios in which it might unfold more gradually and there might be many of these AIs
and they are competitive. And there are dynamics in the economic competition between these different
AI systems that might be hard to control. And if you insist on having too much human oversight
and human in the loop, it might slow down your AI system and somebody else who have fewer scruples,
their AI system will out trade you on the stock market or out to invent you in technology space
and out maneuver you in military space, like they're drones just are autonomous and operate
faster. And you have some guy who has to sit the press a button every time before it fires.
Like, if we're unlucky, the dynamic there could just be such that the winning strategy is just
to basically allow the AIs to run at full speed and do whatever they want. And it's not clear
what would happen to the human species in the long term in that kind of scenario. So there are
various different ways in which things could conceivably go off the rails.
Yeah, maybe I was thinking about Jokowski's all life on earth. How would that happen?
Well, the only thing I could think of was because he didn't give any examples, but
you know, like maybe AI creates a video, a deep fake video that's so convincing showing Biden
launching the nukes against Russia or vice versa. And then that initiates a large scale
thermonuclear exchange and that that could end all life. That's the only thing I could think of
that could end all life on earth, even there. Probably not like a nuclear exchange wouldn't
end all life. It probably wouldn't even end all human life on the southern hemisphere.
I mean, I would like, I recommend against running that experiment. We certainly know that it would
be like the biggest horror ever. But there is still a distinction to be drawn, even if it's like
academic between a global catastrophic risk that could be very bad and an existential risk,
which would literally be the end of the human experiment. Because there have been big setbacks,
there have been dark ages and plagues and all kinds of stuff. At one point in our prehistory,
it looks like there was a population bottleneck and we might have been down to a few thousand
individuals. But eventually we came back from that. And similarly, if there is a nuclear war,
but, you know, a bunch of coastal areas in the southern hemisphere where they can do
fishing or whatever, they survive. And then, you know, after a few hundred years, we might be back
to where we started. But there are other ways available to superintelligence. Like three good
inventive biological constructions that would wipe us out maybe or nanotechnology or maybe it just
doesn't even bother very much with us, but just sort of starts to transform earth into one giant
data center or some sort of launch facility for launching space probes to kind of spread throughout
and we sort of perish as a side effect of the waste heat or something like that.
Yeah, I think it would be wrong to anchor too much on any particular concrete scenario of the
precise mechanism whereby human life or human values would be trampled over and think more
abstractly that if you have this very powerful strategizing force in the world that is antagonistic
to us, chances are this much smarter, more strategic thing would eventually prevail and
be able to do whatever it's tried to do. So that's kind of like, yeah, a class of ways in which things
could go wrong. Now hopefully we will learn how not to do that, as I said. And then we might end
up in this condition of a solved world that I discussed in the book. Yeah, no, I want to get
to the deep utopian, but I just want to give you a chance to respond a couple of your critics of
that Steven Pinker writes of these purported existential threats. They depend on the premises
that, one, humans are so gifted that they can design an omniscient and omnipotent AI,
yet so moronic that they would give it control of the universe without testing how it works.
And two, the AI would be so brilliant that it could figure out how to transmute elements and
rewire brains, yet so imbecilic that it would wreak havoc based on elementary blunders of
misunderstanding. Yeah, well, first of all, some elements there are just added for no reason,
I guess, transmute elements. I don't particularly know why that would be a necessary component
of the view that AI could pose risks. But I think the basic idea that we could be smart enough to
create this thing without being smart enough to realize that we also need to solve the control
problem, unfortunately seems like a realistic possibility that we seem smart enough to create it.
I mean, you can judge for yourself, but year by year, we see AI capabilities galloping ahead.
And I mean, it's not a question whether this current paradigm will take us there, but certainly it
doesn't seem a ridiculous view to think that it might. And then that we might fail to realize that
there could be a difficult control problem or that we could mistakenly convince ourselves that we
solved it even though our solution is flawed. I think it's also pretty plausible and more possible
because there will be strong incentives for people to do precisely that. If you have multiple labs or
multiple countries all competing to develop this potentially hugely lucrative technology, right?
And also strategically relevant for national security, etc. There's like this raising dynamic
where multiple groups race to get there first and whoever slows down or spends more of their
efforts on safety and precautions and testing it like they just fall behind. You could see that
even in a good scenario where people realize that ideally we should do this carefully, like that
would still be just like overwhelming and so competitive pressures to make it happen as quickly
as possible even with fewer safeguards. And I'm sure that as we move closer to this to kind of
polarized debates that we're already beginning to see will be amplified and who knows how that
shakes out. People have kind of a tendency to run in herds. And this holds on both sides of
this AI debate. I mean, in fact, I have started to worry slightly about the possibility of overshooting
the target in terms of AI alarm. Back in 2014 when the book came out and I worked on it for six years
before that, like the whole possibility of risks from transformative AI was completely neglected.
So I thought they clearly needed to be more attention to that than was given to it at the time
because at the time it was basically zero. So it's like now on the other hand, there is a lot more
and we are beginning to hear even top level policymakers start saying negative things about
AI. Like I think it's unlikely but less unlikely than two years ago that we could end up in a
trajectory where AI is never developed because we either end up with like some sort of permanent ban
or some agreement to slow down so much that before we actually get around to doing it,
we destroy ourselves in some other way like through some other technology or something.
And this still seems unlikely but the pendulum is swinging and I don't know we have a very fine
grain ability to sort of choose where it stopped. It's like an avalanche, you can maybe trigger it
but once it's going, you can call it back. And so people then, I don't think we're there yet
but you could imagine it just the stampede of consensus forming that AI is a bad thing and
then it becomes taboo to say positive things about AI and then policymakers competing with
one another to be like tough on AI just as it happens in foreign policy context sometimes and
you know you could imagine various scenarios in which we kind of go too far in the other direction.
Well here may be an analogy with the development of nuclear weapons where you get an arms race
where maybe you don't want to develop it in this direction but the other guy may do it so you have
to do it because the other guy is going to do it and then and so on and so forth and you end up
where we are now. Something like that maybe? Yeah that certainly is a class of scenarios and it feeds
into this current debate about open sourcing AI models which has like the obvious thing going
for it that it's nice more people get access, democratizes AI, more eyes can detect more problems
etc etc so that's which is true for other open source AI as well like it's generally a nice thing
kind of culturally to open source but with the frontier models there is the question of whether
that is ultimately the right approach because it does also mean relinquishing any ability to
influence how the AI is used. So if you are an AI lab that trains your AI to sort of refuse requests
to give advice on how to construct biological weapons or commit cybercrime or whatever else it
might be then if you open source the model it's usually quite easy then to sort of remove the
safeguards. You do some more fine-tuning training and you can kind of train the model to actually
be of assistance in these ways and as we move closer to truly transformative AI of course if
the model is open sourced anybody with a sufficiently large computer cluster could run it
and it you can sort of call it back if it turns out that there is some additional
invention that could make its capabilities go above a critical threshold. Yeah these
large language models, chat GPT and so forth or worse the Gemini embarrassingly bad programs are
these down the wrong path toward either dystopia or utopia you think there's something else that
will develop that and this is the wrong way or not not the direction that this is going to
lead to either dystopia or utopia. I mean I think it's on the sort of shortest path towards more
capable AI. I think the current models we have are basically the first models that we figured
out how to develop that's still very very capable. I think the technological trajectory has not been
shaped very much about some vision about what type of system ultimately we need that would be the
safest it's just like it's hard to get AI to work at all and we try everything and some things work
and the thing that works best currently are these large language models or I mean they're
increasingly becoming multimodal models and it will be interesting to see whether that is
all we need there is like a school of thought it's a scaling hypothesis that basically what we
need to do is simply to scale these systems up even more and just as we saw almost qualitatively
new capabilities as you went from GPT-1 to GPT-2 and then GPT-3 new qualitative well it is GPT-4
you start to see some actual reasoning and understanding there you know if we go to GPT-5
or GPT-6 just make them bigger with more data more training more parameters it's possible that
things will just fall into place without much further effort
it's also possible that they that these will kind of be the engine blocks and you need a little
loop some some additional little thing on top of this some external memory system that it can
read and write from some agent loop that makes it possible to do more reasoning and planning
then is is feasible in just one forward pass through a big transformer model but there are a
bunch of such ideas already in existence that it might be by sort of combining these in the right
way and scaling it up you would maybe get all the way of course we don't know until it happens
what about these examples we saw of just embarrassingly bad searches where the chat GPT is just making
up fake law papers and medical findings that didn't even exist or worse the Gemini you know
imposing DEI ideology onto just basic factual searches like show me pictures of of the popes
and they show pictures of women popes and I mean it was just horrible embarrassingly bad
well so these are two different classes of problems so that the latter one I think was on
purpose like it was designed I mean obviously not designed specifically that these historical
characters should be rendered the way they do but that that it was the result of a specific attempt
to make the outputs of these models feature a more variety of different human types to sort of
combat whatever the stereotypes that would result by default if you just trained it on
uh internet data which comes predominantly from certain demographics who just have spent more
time writing and posting on internet and stuff so I think there the solution is is more to sort of
change the precise way that it's fine-tuned as for the former problem the problem of hallucination
that's more like a technical problem like a re an open research challenge because they don't want
them to hallucinate like the people building these Google doesn't want their AI to do this
but they haven't yet figured out how to completely remove that I think as the AI systems become
smarter I think we will see less of that just as a side effect of the general increase in
capabilities and and already I think there is a bit less of that now than there was like a couple
of years ago um but yeah I mean certainly right now it makes sense to I mean you should always
I think this makes sense when you're getting advice from some human expert or from some human
source as well like you need to apply your own critical scrutiny to try to uh you know
evaluate whether it makes sense or not and it like doubly true with if you're getting it from
these AI generator deep utopia okay the search for utopia has always historically been a bad idea
this is a not a good goal to have because it always ends in disaster because somebody is
going to block us from reaching utopia and we have to eliminate them you know that you know what
I'm talking about here historically why are you using the word utopia what do you mean by that
what are you after here for the long-term future well um I mean the word is so it's not the book
about um how to rearrange the political order or culture uh or society to achieve some like
great outcome um which is what most utopias are like they are basically some um some some
usually they're like a political program in disguise or a critique of some tendency in
contemporary society like if they are dystopia then like which is like the other side you might
say like 1984 or brave new world the kind of picking up on some problem in contemporary
society and then saying well if you continue down this path then we get to this thing everybody
can see is bad so let's reflect on what we're doing now and maybe of course correct um but deep
utopia is rather something like a philosophical investigation into questions about human value
if you imagine the whole AI transition going well so let's take as an assumption we develop this
and we we don't have any of these existential risks and we end up with this future condition
where like the whole economy can be automated and not only that but this AI then develops
all kinds of other super advanced technologies because amongst the jobs that could be automated if
you had truly general AI truly general AI is of course also the jobs of scientists and researchers
and inventors etc so um we then get to I think ultimately if we think through where this eventually
leads a condition of technological maturity like a condition where we've developed most of those
general technologies that we that are physically possible and for which there is some conceivable
pathway from where we are now um and moreover in this solved world that we will get not only
do I postulate we have technological maturity but let's also imagine we solved our coordination
problems politics like no no no wars like the society's fear let's just all of those are of
course extremely important practical problems that that we need to fix but I wanted to get
actually to the point where you could ask the question is think about what happens then like
assuming everything goes as well as possible and and then where do we end up and what role is there
for humans in this world where like we don't need to well not only do we not need to work anymore to
make a living because like the robots and ai's could produce everything and drive the cars and
run the factories and write the word documents or whatever um but a whole bunch of other activities
as well that currently occupy our days would become unnecessary in this condition of technological
maturity um so right now even if you didn't have to work like suppose you're like independently
wealthy uh like it's still a whole bunch of things you need to do you need I mean you need to brush
your teeth like Bill Gates has to brush his teeth otherwise he will have tooth decay and there is no
way around it right um similarly if if you want to be fit you have to actually put in some effort
on the treadmill or with the weights and there is no shortcut um but at technological maturity like
you could pop a pill that would give you the same physiological effects as spending a lot of time
working out would do and so you can then go through activities one by one and thinking like
do this will make sense in this condition of a solved world and for a lot of activities the
answer is seemingly no um they lose their point insofar as we do them for an instrumental reason
that is we do we spend time and effort to do x in order then to achieve some other thing y
in most cases like that in fact almost all of them uh in technical maturity there would be
shortcuts to y that would seem to make the whole activity of doing x pointless um yeah okay let me
ask you a question um chat gpt probably can't quite write a book as well as you do but maybe the next
version does please write next boston's next book would you do that or do you actually enjoy
writing your book I did I feel it felt a sense of urgency to uh I wanted to get it out before the
singularity before it writes it for you but don't you enjoy this this is my point don't you enjoy
writing books I I don't want an ai to write my next book I like writing my books but so now it feels
like a very meaningful thing to do right yeah saying you work you rework it and then you hope
that in the end it will bring joy to somebody or they will learn something and but if if it had been
possible like instead of struggling with each paragraph and figuring out what you want to say
if if I could just have pressed like a key on my laptop that would have produced the same paragraph
or a better paragraph um then it's not so clear like would it still feel worthwhile to sit and
struggle and sweat if it was just like a way of producing worse text then it could have been done
by just pressing the key that would activate you know gpt 8 or whatever to uh to do it um
it could still do it but I think um at least prima facie at the first time it seems like it would
put a big question mark over that activity like does it really seem valuable to do even if it were
like obviously utterly pointless and that was a much more sensible easier way to to achieve this
exactly the same result I guess I'm trying to find something that uh has a different value that is
it's valuable in and of itself now look there's a lot of projects around my house I just hire people
to do it because I don't like doing it and I don't know what I'm doing or I'll just go to
Home Depot and buy the kit and just put it together rather than buying the raw supplies
and and make you know sawing the wood and whatever um but I like writing my books or I like riding my
bike or playing tennis or whatever I like working out um I don't want to take a pill to do that I
don't want to pay somebody to ride my bike or or hire a chat gpt to write my next book because I
actually enjoy it so it's a different value yeah um yeah now I mean certainly uh that would be
nothing preventing you from uh still doing those things and many other things um if you value the
activity itself and if you truly value the activity for itself rather than subtly and in a way that
might not be visible to us uh actually as a means to an end for example uh as opposed you
spend a lot of time writing because it actually uh made you happy like it made you subjectively
feel good uh well there that would be a shortcut right I think you could take a pill that would
give you the same subjective happiness and good feelings and a pill moreover without side effects
or addiction potential etc adds technological maturity no Nick no it's the challenge that
makes it valuable and not not just some glow feeling that's not what I'm after is that that's
right so there is like a whole big set of possible reasons for working hard on the book and some of
those reasons would be removed in this hypothetical context and it's like onion layers of onion you
can peel away and the question that the book is kind of exploring is like what remains after you
really remove all the instrumental stuff and I think there does remain something um
but it's quite subtle um but I think ultimately there is a whole set of values that are currently
a little bit often invisible to us um that would come into view uh and that it would make sense to
focus more on if if sort of the the screaming moral imperatives of everything you have to do
like you have to go to work otherwise you don't get the paycheck and how are you going to afford
your rent you have to you know um help drive your kid to school because otherwise I mean what's
going to happen otherwise you have to do these so much stuff that we have to do that that and if
you look at around the world obviously there are huge needs everywhere that we should try to help fix
um if you imagine all of that going away then I think there are many more subtle
quieter almost like aesthetic values that it would be appropriate to allow to have a bigger
influence on what we do just just as you know you walk out at night and you see this big canopy
of stars and constellations like they are always there right they are there during the day as well
it's just the blazing sun kind of makes them invisible but if you imagine removing the sun
suddenly all of this this rich iridescent sky of more subtle values would come into view and I think
our sort of evaluative pupils should dilate in this condition of technological maturity to place
more weight on those values and there is a whole range of them um and I think it is from from these
constellations of hypervalues that the that utopia would be constructed or at least if you imagine
a utopia that has a rich structure as opposed to a sort of simple hedonic utopia where we become
kind of pleasure blobs through like super drugs or direct neural stimulation but if you imagine a
more richly textured and structured utopia I think the structure would come from a range of these
canopy values that that would come into view um yeah I had Andrew Yang on the podcast a couple
years ago he was the presidential candidate pushing the ubi universal basic income at the
time he was concerned about ai taking over um taxi drivers truck drivers and so on it was going to be
hundreds of thousands of people put out of work now that hasn't happened yet but it could
but this is like saying well what are we a century ago what are we going to do with all
those elevator operators the little guy in there pushing the buttons for you well there are any
of those anymore they went and found something else to do now can we say that most jobs are
kind of crappy and no one really wants to do them they do them because they have to make a living
so in a post scarcity treconomics kind of model nobody has to do this shit work anymore they can
just write poetry or do art or write books or I don't know what maybe they'll they'll find other
meaningful things to do and that that is well infinite there's there's no upper ceiling on
finding meaningful things to do yeah well I mean the question is whether they are meaningful there's
a lot of things you could do and you could also not do them um but would that be meaningful so
right now if for example you work hard and it allows you to support your family and take
good care of like that that gives meaning to your efforts like the the boring office work
maybe not so meaningful in itself but if it achieves this outcome of giving like making
your home a good environment for your your your spouse and for your children like that you know
gives meaning or if you work hard for a charity and it helps save the life of you know some
disadvantaged like group that's like you have achieved something and done some good in your world
or you're a scientist and you work hard and you like invent something new like either theoretically
interesting or practically useful that's like you have achieved something um so those kinds of
meaning might not might be in short supply in this uh in in this solved world in that you know
whatever the scientist could do would be much better done by AI scientists and uh you wouldn't
need to be a breadwinner because the bread would already be won uh through the economic abundance
etc there would be no starving children uh in utopia so no need to work for a charity.
Let's look at the economics of it okay I can see the argument for let's raise the
lower bar as high as we can so no one is suffering everybody has three square meals a day
roof over their head education health care um and so forth what's the upper ceiling I mean it seems
like you know I here I was thinking of David Deutsch's book the beginning of infinity there's
an infinite amount of knowledge we can find problems to solve why would that end?
Well there are two questions there one is whether it would end but let's and we can return to that
but there's a second kind of almost preceding question which is even if there is more to discover
whether we would be efficient at discovering it so I'm suggesting even if there's like
important scientific research to be done at technological maturity it would be much more
efficiently done by machine intelligences um and so we wouldn't really like it would just be a waste
of resources to for for humans to exert calories to like try to think about these things and AI
would do it much better and much quicker and with like cheaper um so so that's yeah even if that was
more I think we wouldn't be useful for discovering it um it's also possible although this is an
independent idea that um although there's always more to discover the most important things might
be at some point already discovered and then it's kind of more and more trivial details that
remain to be added to the scientific inventory of knowledge which I think is also likely actually
but you do what if what if you're that guy in 1896 that said you know we've pretty much got physics
all figured out here just before Einstein yeah he was just a bit early I see he's a century early
okay yeah or two or whatever but I mean we've only been around I mean how long has science
been gone for a couple hundred years or something right it's like trivial in the big scheme of things
yeah um and we don't even have super intelligent ai's to actually really get cranking on making
intellectual progress we're trying to do it with our meat brains and so a few hundred years with
meat brains like of course there's still more to to learn so maybe an example of what you're talking
about would be how do we solve the problem of schizophrenia we don't really know yet and we
haven't made much progress but maybe ai could test a thousand different chemical compound
combinations to see what works and it could do it in a couple of days rather than a couple of
decades that humans would take to do it and that would be a solved problem but why would there be
at some point no no well okay so you're saying there's a finite number of problems to be solved
for human flourishing um yeah well um so at some point I think you have basically found the optimal
ways of technologically achieving the types of outcomes that normally need to be achieved
you've invented the optimal solar panel you've invented the optimal space colonizing rocket
you have invented the best way of transmitting electricity from one point to another like
et cetera et cetera so that might be like you know like trillion types of tasks at that level
of description and like for each one of them you have worked out at the molecular level
what the most efficient mechanism is to do it or maybe not the most efficient maybe there are like
kind you could improve it by like one tenth of one percentage point by researching it for another
thousand years and the ai's would be working to like make these small optimizations but it wouldn't
be like discovering relativity theory or evolution theory or something like that that like a simple
insight that has like like a big earthquake of ramifications for the way we perceive ourselves
in the world yeah all right i'm going to read from your book here uh the lines from harry
lime the third man you know what the fellow said in italy for 30 years under the borges
they had warfare terror murder and bloodshed but they produced michael angelo leonardo
davinci in the renaissance in switzerland they had brotherly love they had 500 years of democracy
in peace and one of that produced the cougou glock so how do you address that point that humans
need challenge again let's distinguish between happiness and meaningfulness slash purposefulness
it's those challenges that give us meaning and purpose that's the goal not happiness
um yeah well um it's it's hard to tell um certainly if it were happiness in the subjective
sense of positive effect it would make the problem very easy because trivially in utopia
technological maturity you could tune your hedonic well-being up or down very easily if there is
certain newer technology or drugs and stuff so if that's what we wanted then we would be home and
drive like problem solved we'll definitely be able to do a lot of that in utopia um if we want
challenge well certainly we could create artificial challenges uh is there games uh very elaborate
games with like all kinds of um you could have ai's inventing new games for us like there could be
so if artificial challenges are enough to realize that value that you pointed to then also we are
home and drive that that would also be very easy to do if we want genuinely meaningful challenges
then there is more of a challenge um in seeing how that would be possible in deep utopia because
prima facie at least at first sight it seems like our own efforts are for most purposes unnecessary
and then we could still do the thing but it may not obviously be meaningful to do the thing if
if there is nothing worthwhile achieved by doing it but i do think there are um at least
ways of rescuing part of what we want if we want meaningful challenge even in utopia and
there might be first of all uh tasks that need to be performed by like so for example if there are
to take a very simple example consumers that have a preference uh not just
for a certain type of object but also a preference regarding how that object should have been
manufactured uh and in particular they wanted to have been manufactured by hand you know or by human
then that would be demand for human labor to produce we see that today like certainly the
consumers might pay more for a trinket that's done by some favorite group or like in indigenous tribe
rather than in a sweatshop in malaysia like even if the trinket is the same or equivalent like the
fact that the causal process that brought it about was different might result in a difference in price
similarly we might prefer to watch like human athletes compete even if like the robots could
run faster or box harder or whatever like that might just be a brute fact about it and so you
can then see like or we might want like a robot uh you know priest administering the wedding or
sorry like a human doing it rather than you know a robot even if the robot could say the
same word etc so you could then you could look through like and there might be many more of
these that we can't afford currently so nobody has kind of even bothered inventing these services
but where we're just humans have a sort of brute preference for it to be done by human effort
and I think in addition to that there might be more subtle ways in which that would be instrumental
uses for human effort if for example we have values say you say you have a value that
values the honoring of a certain tradition now many traditions in order to be continued would
need the active efforts of human beings to do whatever the things that they're traditionally done
to have the this is the ceremonies and to like focus our attention on certain things and even
if we could perform like great robots that went around then like perform the same songs and dances
and stuff like it wouldn't count as continuing that tradition so if we value that it might call upon
us to to to make an effort and that might be one of these subtle values or maybe right now the
tradition is like well our tradition is tradition like whatever you know they're starving kids out
there we should focus on helping those but once all the kids are fed and all the diseases are cured
then these slightly less like uh Israel values might then deserve a lot of attention and aesthetic
values like there might be things we have reason to do because it would just be beautiful if somebody
did it and uh social cultural entanglements like the way that the different people have
preferences about each other and what they do and how that I think that might also produce
some opportunities for
natural purpose in Nakadi in in Utopia you can also have artificial purpose where you just
set yourself an arbitrary challenge and then have your brain motivated or change so that
you're like super motivated to achieve it that that that would be safe but there might also be
some of these more natural purposes well there is this DIY you know do-it-yourself movement
where people seem to like just doing it by hand they just want to get their tools out get up in
the garage and start making stuff I don't personally like this because I'm not very good at that but
it's a huge movement so I mean you could hire somebody or there's a machine that could make
the little shed better than you can make it by hand but people seem to like to do that
why not have both the AI does the stuff we don't like to do and then I'm just going to do the
stuff I do like to do yeah no that that seems seems good now there is an additional challenge here
which is lifespans could become very long right if we fix the things that cause disease and death
and like cellular decay etc so if you are going to live for maybe millions or billions of years
potentially um you sort of run out those like just get me to 100 without Alzheimer's all right
well that's a good start but you know when you're 100 in perfect health yeah maybe you think well
do I really want to check out now or maybe do another year let's push it a little bit further down
and at least it would be nice for you to have the option of kind of uh because like I mean probably
our like our age when we were a kid like being 50 or 60 or whatever that's like
now I know particularly white as well I don't but now of course when you're there you see that
wow you know there's a lot more that yeah could be done and experienced than the um
and there are simple pleasures as well so they're like the things you might only want to do once
twice in life but then you've done them but then they're like other like a nice cup of tea or a
coffee like it's kind of about as good as you know on the 10th thousand times you do it and then
you know in the first or second so you don't really run it's renewable as it were like a renewable
sort of joy and so um but but it does mean also like one should maybe think of if the question
is what's the best possible future life that you could have if you remove all practical constraints
and technological constraints um you really should think maybe in terms of a trajectory not not just
a state that you would reach and then you have sort of reached the peak but more like what's a
developmental trajectory that would like be a you'd get the most out of each level of development
maybe eventually like understood most things that can be understood by human brain maybe at that
point you would want to upgrade it a little bit like go a lot some more neurons or whatever so you
could kind of explore the next level and but like what's the right pace of that like do you want to
just rush to the end and become like like a planetary sized superintelligence immediately or
would you like want to you know take the scenic routes and then maybe spend a few hundred years
first being a biological humans and doing whatever can be done as a human and then slowly increment
so so these are some of the uh the questions that come up and so many more uh there's a lot of
things to think about hopefully we will actually uh uh yeah secure the future and ask that uh well
yeah again incrementally i like kevin kelly's approach protopia not utopia or dystopia just one
small make life tiny bit better tomorrow than it is today don't aim for utopia just as a tool just
make life a little bit better don't worry about 500 years from now just tomorrow um i think that uh
allows us to avoid a bunch of uh mischief that is is performed in the name of grand visions
but i do think also sometimes it's useful to lift your gaze up and look at the horizon or like
reflect on where you're going like there's the next step on the next step but ultimately um so we
have like our human civilization all this effort spent on science and technology and economic growth
and everything but very little effort spent on thinking what what where do we end up if this
continues sure but let's talk about creativity for a moment your book i really enjoyed because
it's completely different than any most non-fiction science books that i read you know you have this
kind of dialogue this conversation you're in a classroom your lecturing you have handouts
students are asking questions that was pretty creative and new uh if you had asked chat gpd to
write your next book i don't think you would have come up with that you see where i'm going with this
what about music what's the origin of rock and roll well folk music and jazz all right so in a
century from now what will be the next big you know musical trend i don't know i don't think it's
possible to know and i don't see how an ai would anticipate the next creative movement not just
in the arts and poetry or whatever but in anything um you know there's you know there's only so many
combinations i guess maybe it could grind through all the possible combinations for music that's
going to be enjoyed by people that seems to me though next to impossible to program now i might
not be able to predict it um but there's a lot of things that you couldn't predict even with your
fair super intelligence like that like even just like the weather like a year into the future where
whether on a particular minute it will be raining and it's like chaotic systems right and with something
like creativity over the time scale of a century um it the actual answer to that question will depend
on what a lot of smart people are doing in the course of that century maybe other ai's even
smarter than the one that you would think would be making the prediction and it itself will interact
with so this but um even if it's not predictable what creative results will it you know precisely
be attained uh a century hence it might still be possible that the actual creative work is more
efficiently done by ai's as this century unfolds they might just be making the best paintings and
writing the most beautiful poems and writing the most compelling movies etc it's certainly not the
case right now i mean current large language models are i have a sort of um cliche is maybe too
strong a word but there is a sort of mid-brow quality to their output that there's like the
it's good but it's not great it's kind of the typical thing that some person would say in a situation
that they can produce more of that but great stuff comes from kind of not just following along with
the patterns that are already out there but sort of um looking at reality afresh with new
ai's whether the reality is inside yourself or outside of yourself and really letting it speak
to you and that then they sort of speak the words that come from your perception uh of of this piece
of reality that is that you're focusing on and so it's like a different source kind of of information
but um i have no doubt that that ai's will become increasingly creative i think it's not a binary
thing i think we already see a little glimpses of lower level creativity and i think the next
generation will have more and then more and more beyond that uh for example a few years ago deep
mind system alpha go had had this move what was it 32 or something for 37 i forget but it was like in
the match that alpha was playing against least at all the human go champion and there was a particular
move that experts in go thought was immensely original and creative it was something no human
would ever have played that all the masters would advise like students that that was an error but
then it still turns out like if you think a little bit more you just realize how right it was and it
set everything up to win the game later so that's within a sort of somewhat circumscribed domain
but certainly like created within that domain and i think the domains in which you will be able to
have these like genuine uh deep creativity will will be expanding as the capabilities of the ai's
increase yeah when i was a professor at occidental college we had a music professor there was also
a gifted pianist and he would once a year hold these impromptu concerts where he would in the
auditorium that grand piano on stage he sits there and then people would call out like requests like
do Beethoven's x as if you know Elvis did it or you know in the rock and roll and and people just
come up with the craziest and he would do it and it's like god damn that's great so maybe if you
had an ai you could you could find all the different creative permutations on all the different music
that has been done and then test it in the marketplace well what do people actually like
yeah yeah uh yeah so there's a quite right now the question of quality like the actual
output is not great now if we imagine the quality problem being fixed then there is the question of
whether people would still value it less because it was produced by ai even if if you sort of listen
to a blind test right a and b you're not told which one is even if people prefer the ai output in
that context if the quality became like as good or superior uh if then we get to this like
further question of value whether you still prefer it just because you know that the human did it
there's also the I mean there's like so many branches sticking out from here but like one
possible reason you might have for preferring the human output is if you think the human
but not the ai experienced various things when they wrote it they actually experienced the joy
or the sadness that you know the the musical piece expressed but there again I think with
digital minds it might also be possible to create phenomenal experiences in digital substrate
and so ai's also might have had experiences that they could be expressing in their works
in it's not not clear exactly how where we are on on that path towards ai's sentence but I think
certainly in principle it is possible I'm a kind of computationalist about phenomenal content
yeah that subjective element of art where the fraudulent copy painting of a classic painting
plummets in value the moment people find out it's fake even if you can't tell the difference with
your own ai yeah yeah yeah so um so if that's the model then you know that might be uh still
demand for human painters to yeah paint there now sort of relatively back to the economics I
mentioned you know pulling up everybody from the bottom up to some level but you know economists
tell us there's this thing called the hedonic treadmill but there is no there's no bottom level
people always want more and that that's just going to never end you know the mac mansions
houses are like two to three times the size they were in the 1950s even for the average worker
and you know that that there's no upper ceiling on how much more stuff people are gonna want
how do you think about that yeah I think there are parts of our preference functions that are
non-satiable collectively because like yeah we have these desires for positional goods
to have more than another like you want your yacht to be the biggest in the world
so you build a 200 you know foot yacht and then some other billionaire bastard builds one that's
like 205 and then so it's impossible for both of these people to have their preferences satisfied
to own themselves exclusively the biggest yacht in the world so that's one example for how
collectively there could be preferences that the humans have that you can't all be satisfied and
there are many other examples where two people want the same piece of land or the same be the
exclusive love interest of the same person etc etc so now it doesn't enable sort of unlimited
economic growth if you define growth ultimately in preference satisfaction terms and so like
because one person's gain is another's loss in this scenario and it also doesn't necessarily create
an unending reason for human economic labor if there is no way to make more money
than no matter how each of these billionaires wish they could make their yacht a bit bigger than the
others like if they can't actually make more money by working or if the extra money they
could make by working is kind of trivial to the amount of money they're already getting from their
capital gains and there would be no incentive for them to put out effort for that reason
and that's like already true for many billionaires like there's like yeah they could take a job and
make an extra 100,000 a year maybe but if they're already sitting on 20 billion it's like it's not
really making a meaningful difference to their purchasing power yeah but you have people like
Elon Musk and Jeff Bezos you know they didn't they're not just sitting on the beach you know
they're gonna like and they can I mean sort of still add a lot of even just economic value
through their work yeah like like obviously Tesla would be worse a lot less if Elon I called it quits
yeah and so even just from a purely economic point of view they still have the ability to contribute
amounts of economic value that are significant even relative to their net worth and
because they have like like Elon has unique skills also I think there are opportunities
sometimes for very wealthy people to sort of combine their human capital with their financial
capital to do things that are hard to do by taking one person with capital and one person
with brains because they're like trust problems and communication problems sometimes they need to
be combined in one person to certain opportunities are more easily realizable but for many others
it's not the case and they're already like in this situation where it makes no sense to work for money
yeah but I guess my point is you know there's stories about Elon Musk sleeping on the floor in
his factories he doesn't have to do any of that but he does it because that's what gives him value
and I think more people would want that than would just want to sit on the beach
yeah I think he's doing it because he wants to achieve various things that can't be achieved
without him doing it yeah now if he could create like I don't know like some sort of android replica
of himself that would do the same thing and achieve the same results for for Tesla and SpaceX etc
and and he could be on the beach I have no idea maybe he would prefer that he has said that his
life is pretty painful often and that so it might be that he does it because there are various
outcomes he wants as opposed to valuing the activity it's not running around and we don't
know what's in his head but you know I think in general people like challenges because that's
what makes life meaningful and it's essentially an infinite number of challenges we could always have
but I could be wrong okay on the economic model so people are living longer let's not get crazy
let's just say people live 200 years or 300 years rather than 100 yeah but in 300 years of research
into extending life don't you think it's coming by 24 what's the the takeoff point of 2045 I think
he said where maybe it's even sooner than that where the amount of extra life you get exceeds
every year of your life and then you have the what does he call it the take take take off point
something like that yeah I don't you escape velocity that's it yeah I you know when I hear
these things I think back to religion it's like I feel like I'm you know we're the chosen generation
we're the ones that get to live forever I've heard this before when I was religious right
you know maybe you know but I think the problems are much harder than most longevity researchers
think um but you know it's possible but okay let me let me carry out the thought experiment
all right so we have eight billion people now it's probably going to top off around 2050 and
start to decline by 2100 or so and as you know Elon's worried about a birth birth the richer and
more economically stable and more educated people are the fewer babies they have so how do you square
that with the people living longer and the population increasing how do you think about that yeah I
think there are various long-term trends that I think would deserve attention if it were for the
fact that I also believe that we are probably relatively close to this transformative technological
overhaul over the current of the current human condition so that I think sort of the game board
will be overturned for better or worse but within you know likely some years or a few decades and
that these like longer demographic trends won't really have time to play out would be my guess
there might be other demographic trends that then kick in if you do invent this new world with AIs
and digital minds that can obviously copy themselves instantaneously if you're like software you could
make a million copies of yourself in an afternoon right if you have available hardware so you could
have like different population dynamics that could become problematic but but they they wouldn't
sort of just be an extrapolation of what we are currently seeing with the human situation also like
some I mean I see the the projections and how like birth rates are going down and if that continues
like like but some skepticism about our the reliability of these long-range forecasts like
I mean when I grew up the the big worry but was about overpopulation and there were these like
public intellectuals through the club of Rome and everything and that was like and they had
little mathematical models that show this now it's going the other way and I mean who knows
in 30 years from now if there's no transition maybe it just turns out that something has changed and
it's overpopulation again like or some other so it's like yeah our ability to make this very
long-run range forecasts are open to question I think yeah you know Stein's law things that
can't go on forever won't and there's some corollary to it but but they can go on a lot
longer than you think yeah yeah yeah some things have gone on for longer than yeah yeah well I guess
in the next you know you want to look at the far horizon do we need to leave the planet become a
multi-planetary species because of either overpopulation or going to run out of raw supplies
and and uh and resources and population can ultimately outrun any space settlement program
because ultimately even with mature technology we are limited by the speed of light
and so if you imagine a sphere growing at the speed of light even in all directions right
that the volume of that grows polynomially with time and so the resources that we could
potentially use for civilization or for like that can at most grow uh polynomially whereas
population can grow exponentially it can in theory like you know double average generation
or 10 percent and so eventually the exponential will overtake the polynomial if you have unrestricted
population growth and if like we end up in in a situation where sort of people have more than
the replacement rate of children that that would eventually just overtake so so the space
settlement would at most kind of delay bumping up against resource limits and ultimately you
would have to just figure out some way to maybe coordinate to to to bring only the number of
people new people into existence that that could be supported at a high level of living which
might still be a lot of new people into existence but if you overshoot that then eventually average
income would have to drop uh right now we have more like kind of increasing returns to population
because um more people means more ideas and division of labor which makes so so right now
probably per capita income goes up if population increases but I think at some point um that will
no longer be the case and the limiting factor of the economy at technical maturity will eventually
become land as it's referred to basically resources that you can't make more of as opposed to labor or
technical advances that that will already have sort of been maximized and so then land only
grows polynomially in the limit and that that would be the sort of maximum rate at which the
population could grow in the limit as well and what's your time horizon there you know thousands
of years or tens of thousands of years uh well I think um there are really sort of two key variables
there's a question of how far from now until we get superintelligence
and then from there on I it might not take that long because once you have superintelligence that
makes superduper intelligence and then like some kind of substrate optimized for a cognitive
performance that can I would imagine relatively quickly develop all kinds of technological
solutions that start to approximate the physical limits I don't know whether that would take like
months or decades but well so it's hard to imagine monetary super brains kind of working
for like creating a space rocket for like yeah it occurs I was talking about 2045 in his next
his next book the singularity is nearer coming out in June uh 24 so about after that anything's
game I mean we just probably unpredictable uh what the time horizon could be yeah I mean I don't
even think we can rule out very short time scales like we don't know dpt 5 or dpt 6 won't be right
there I mean we don't know that I will but here we really have to think probabilistically right
then have like a smeared out probability distribution I think or different okay you're
one of my favorite big minds so let's keep going on the the long horizon all right let's apply the
Copernican principle to our species we're not special the chances are you know we're in the
middle of the bell curve of civilizations that would have done everything you just described
surely there's extraterrestrial intelligences out there that have already done all this
uh and built self replicating von Neumann machines and Dyson spheres and so on
so answer me uh Fermi's paradox where is everybody most likely just like very far away like outside
our light cone um which would uh yeah explain why we haven't seen them of course if the universe
is infinite as it seems to be with infinitely many planets and stuff then there would be infinitely
many of them out there but the density might be quite low um we don't really have it seems to me
and a particular reason to think that um it would be easy for a like an earth-like planet to produce
life let alone intelligent life I mean there might just have been some ridiculously improbable steps
somewhere like to get the first simple replicators going or maybe to go from prokaryot to eukaryot
or something maybe that just happens in like one planet out of 10 to the power of 40 planets or
something um now then you might think wow wasn't it then that like what's a miracle that it happened
here on earth well if there are infinitely many planets out there then even if the chance for
any one of them is ridiculously small it would still happen right with certainty infinitely many
times and then an observation selection effect would explain why we find ourselves on a planet
where this improbable thing did happen like only those planets are observed by people
evolving on them of course the others there is nobody there too so that that seems like pretty
likely um if one wants to think that life is more common then one has to I think either postulate
some kind of zoo hypothesis uh like where they are deliberately uh hiding themselves or uh like
my colleague Robin Hanson has some scenario in which uh uh there might be others and it is like
too long to explain but yeah there's a there are like it's I also think like I don't know I mean
it probably takes us like too far afield from our current conversation but this whole simulation
argument stuff which which kind of adds another dimension to the whole where and where do you
stand on the simulation hypothesis well I mean I I mean I believe in the simulation argument having
more than 50 percent yeah so I I think that's sound and now that only shows one of three
possibilities is correct one of which is the simulation hypothesis and so you would then
need some additional information or consideration if you wanted to sort of pick between these three
alternatives that the simulation argument establishes um I would say I mean and I think
to me when I wrote this paper back in in like the early 2000s it like I was pretty
clear that we were sort of on a path to develop the technologies that you would need to create
these ancestor simulations or detailed simulations with conscious um like like super advanced
virtual reality and digital brains like now I think it's maybe easier for people to imagine that
because just we've seen kind of 20 years 24 years of technological advancement like virtual reality
is a lot better now than it was in 2001 and obviously AI is moving ahead so it's like a
smaller imaginative leap to think that at some point in the future some technologically mature
like really advanced civilization might have the ability to create simulations that are like
perfectly realistic to the people inside them um and uh yeah I like so so in in some sense the
opportunities to pop off the train before you reach the conclusion are like diminishing us as we
sort of pass by you know I had Dave Chalmers on the on the show he has it you know an entire book on
on the simulation morals in a simulation and ethics and so on very interesting but ultimately
he says right in there this is not a testable hypothesis we there's no way to know if we're
in a simulation or not so then what are we talking about here this is just science fiction or
metaphysics or or what no I mean I think it um there are certainly possible observations um
uh that are such that if we made them they would give us strong evidence either for or against
the simulation hypothesis it's like to take the most obvious example like if a window suddenly
popped up in your visual field saying you're in a simulation click here for more information and
a little buffering I find the terms and services like that would pretty like prove it to you right
like if that yeah um conversely the absence of such a window popping up is by the principle
of conservation of evidence must be some evidence against the simulation hypothesis like weak
evidence because but with still some evidence and but more um I guess um relevantly I think
if you consider the simulation argument which has the structure that at least one of three
propositions has to be true one of which is the simulation hypothesis
anything that gives you evidence for or against the other two indirectly then
affects the probability you should assign to the simulation hypothesis so um for example one of
the alternatives is that almost all civilizations at our current stage of technological development
go extinct before they reach technological maturity so that's something you could believe
instead of the simulation hypothesis but there has to be a very strong convergence it can't just
be like 80 of them it has to be like basically all um and of course if we make it through to
technological maturity that would be very strong evidence against this idea that basically all of
them fail to get to the technological maturity so therefore anything any evidence we get for
against the idea that we will reach technological maturity would be indirectly through the simulation
argument on the probability of the simulation hypothesis so the closer we get to technological
maturity ourselves the less likely that alternative is and hence the more likely the simulation
hypothesis is and you could imagine um the extreme um version of this which is if we ourselves
develop all the technologies needed to create ancestor simulations and we are just about to
switch them on and we want to switch them on and we're sort of about to reach to press the button
that would pretty much conclusively rule out the two alternatives to the simulation hypothesis it
would show that like it's not the case that nobody reaches this level of technological maturity
it's not the case that almost nobody of those who do reach that remain interested in creating
assets to simulations and so in that situation where we turn on our own simulations we would have to
conclude that we are almost certainly ourselves in one um so those would be some ways of getting
very strong evidence and then but I think anything that then indirectly has some probabilistic
bearing on these alternatives also sort of indirectly has some evidential connection
to the simulation hypothesis so I think there's a lot of ways to test it but these tests are
probabilistically yeah um uh did you mention what if we're first somebody has to be first
yeah well that that's uh what what it's for did you say or or if it's first no we're first yeah
we're there some civilization has to be first maybe it's us that must be um very unlikely if
there were to ultimately um be say a million simulations of experiences just like the ones
you're having um and you can't from the inside tell the difference whether you're like number
five hundred thirty seven thousand six hundred forty eight or whether you're like number one
but in that condition where you have some evidence and you could either be one of the vast majority
of people with your experiences that are simulated or like this very exceptional one that's not
simulated and there's doesn't feel any difference from the inside I think they're a kind of principle
of indifference should make you assign a proportionately low credence to you being the first one
like the exceptional one yeah but again somebody has to be first I guess you're saying in the age
of the universe if most people think they are then almost all of them will be wrong yes so it looks
like a rational betting odds in that scenario like we would be to assign a very small probability to
that um and there's more arguments I think supporting that like in fact my my doctoral
dissertation was like developing a theory of observation selection effects and I think there
are various areas in uh in physics and cosmology and to some extent in evolutionary biology where
you have to reason along roughly those lines to to be able to get sensible results when you try to
connect current cosmological models with the predictions that intuitively confirm or disconfirm
them some sort of roughly speaking assumption that you're like and you should think of yourself
as you were a randomly selected observer well as there's a lot of complications around that but
that's like something in that general direction seems to the let me ask you a technical question
here on a simulation like in Star Trek's um holodeck you know Worf goes in there and he has
a fight with some other Klingon and he gets knocked down how does a virtual reality interact with a
physical body to say maybe you want to have a boxing match with Muhammad Ali in this virtual
reality how does he how does the virtual reality actually knock me down well I mean so in the
simulation argument I think the the the most well so there your mind is itself implemented
digitally like so there is no me to knock down it's it's all digital yeah there is a you but the you
is like it digitally instantiated and you have like an avatar yeah that your digital mind is
connected to like a digital avatar and the same sensory affluence that like currently are going
from your sensory nerves yeah okay yeah like like that that's like equivalent nerves are going into
this digital brain with the same information yeah okay using the same subject now the simulation
would at some point have to run on actual hardware right right yeah how can you possibly have enough
computing power to replicate everyone who ever lived and not just their physical bodies and or
I guess their minds I mean there's this mind uploading business about copying the connectome
that's not enough it's not enough just to have the synapses copied you'd have to copy every single
molecule in every one of the synapses in the gaps yeah I don't think so I think that a sufficient
level of granularity of a simulation to produce conscious experience would be like the synaptic
level you know possibly you could simplify it even more you mean to get a general memory because
memories are stored I mean I'm told by memory neuroscientists that you need it's not just the
connectome it's not just those synaptic you know sort of wired in things you need all the molecules
and all that stuff is part of memory yeah well some aspects of that I mean and clearly there are like
neurotransmitters that are like swimming around in big holes but I think the bulk of the information
processing is probably done at the level of action potentials and synaptic connections I mean
that's what we see with current AI systems large language models are these neural networks are
artificial neural networks are essentially simplified neurons with simplified synapses
and they do seem to perform I mean insofar as AI has advanced like the same kinds of
tasks that the human brain like in terms of say visual perception which is like a relatively
well-developed area of AI like with a comparable number of neurons that visual cortex as you can
perform comparable level of discrimination and object recognition etc I think and indeed current
AIs are still have fewer parameters than the human brain has and but they are also like a little bit
less smart than the human brain but it roughly kind of strikingly seems to match the performance
that you would expect by matching it to bio it might be that the biological neuron certainly is
more complex than one of these artificial neurons so maybe you get 10 times more performance per
biological neuron than you get from one of these simplified representations but I think
that would be my guess now you could have enough computing power to go down a little bit below the
level of synapse if somehow you needed it not not all the way down to elementary particles then
yes it would become a computationally intractable if you had to simulate the whole Schrodinger
equation of a human brain in order okay I'll grant you that because human memory is not all that
granular anyway it's pretty fuzzy but is that you okay so here's my thought experiment we slide you
Nick Bostrom into a functional MRI we scan your connectome we upload the digital file
into the cloud and I have it here on my phone and I go Nick you're up here now and you're sitting
there going no I'm not I'm right here and so are you saying that there's we have to redefine the
self there's just multiple Nick Bostroms and each of them thinks that they're the real one
well I think there are like two notions of the concept like two notions of self that that can
come up that that that kind of always coincide in in our normal human experience but that would come
apart in some of these technological scenarios and philosophers have realized this like Derek
Parfit who was my colleague at Oxford was famous for exploring the difference between preservation
of personal identity and survival in these thought experiments where you have like duplication we
imagine a person being duplicated or teleported and the original survives
that Parfit argued that in those cases the original person has survived but that
personal identity is separate from this because the the original person can't be identical
to any one of them because they are not identical to each other and the claim would be equal and so
so you would have like even if the personal identity is not preserved you could have survival
and anyway that that gets into these kind of philosophical issues but but I think certainly
in some scenarios I think your personal identity would be survived preserved in an uploading
scenario in like if that was only one successor it would be you I think if there were multiple
copies made simultaneously like equally branching out from the root node I think it would be natural
to say that you survived but I'm not sure what to say about your personal identity in that case
I think just our concept of identity was like not really developed to deal with these cases
so it's a little bit sort of inconsistent when applied in these extreme or like exotic situations
yeah yeah I wrote about this in heavens on earth I was looking at both scientific and
religious versions of the afterlife and there's this idea that there's the mem self and the point
of view self POV self so the memory self this is the connect them just copy your your your memory
all your memories and let's say we can do it but that's just a snapshot I mean if you did it when
I was 30 and now I'm about to turn 70 this year well if I if you did it when I was 30 where's
all the 40 years plus memories that they're not part of that self that's some that's somebody else
that's not me me is my point of view looking through my eyes from moment to moment to moment
for all 69 and a half years that I am now that's me and there is no fixed there's no fixed point
where you go that's you right there at 40 or 50 or whatever I don't see how that could ever be
replicated because it there's no snapshot there's no thing called the self in a fixed sense yeah I
think there are several different notions and like you could ask somebody like are you the same
person now as you were when you were five of course yeah and people are confused because like in
in one sense clearly not in another sense clearly yes I'm still in classroom and like
there's a continuous path but I think it's just that instead of having one concept of self we
have several different ones that normally in everyday use kind of coincide and tracks the same thing
but in these scenarios they come apart and so we need to sort of start to differentiate different
yes you would be the same person in sense one of being the same person but like a different person
in the sense two of being and and I think more generally in this kind of world where we have
like digital minds I think there are a lot of new concepts that we would need to develop to sort of
describe the ways that different minds could be related like so humans are kind of
discrete things like here's one human here's another human
like with digital minds you could imagine them being kind of partially overlapping
or like briefly diverging and then converging you could imagine all kinds of ways in which
digital minds could vary that they're not possible for human minds to marry you can pause them speed
them up slow them down that might be like a bunch of different slightly separate minds that sort of
have some shared convergence point where they send information and it's not clear whether they are
all one mind or several minds or like so there's like I think we will we don't yet have all the
relevant concept for making sense of that kind of post-human reality but you know hopefully we'll
have an opportunity to develop some of those as we are are you familiar with are you familiar with
Frank Tipler's book the physics of immortality yeah but it was a very long time since I yeah
that was 96 so here's his calculation that so he's projecting an omega point computer in the far
future that he calls god essentially that will contain 10 to the power of 10 to the power of
123 bits that's a one followed by 10 to the 123 zero powerful enough he says to resurrect
everyone who ever lived that may be but it's a staggeringly large number but is even in a
mega point computer powerful enough to reconstruct all of the historical contingencies all parts of
life you know every interaction I ever had with everybody else including like you right now this
particular moment instead of yesterday or whatever I mean how would how would any civilization
create a computer powerful enough to do that and Frank says not only that you'd have to
resurrect everyone who ever could have lived because you don't know who you just it's just
your point of view so there's a lot of those people that's a that's a big cohort yeah yeah
and if there was a computer powerful enough to do that wouldn't it have to consume so much
energy we could detect like a technosignature detect something like a Dyson sphere that has to
capture all the energy of a Sun to run such a computer yeah well so with Timpler like so one
problem with his theories I think at the time he thought it and it was like an open possibility
in cosmology that we would have like a big crunch that the universe would collapse back onto itself
into a singularity and that's that's how his speculation was that in the final moments of
that collapse you could get this like super amount of computation done somehow now it looks
instead like we are sort of gliding apart with a positive cosmological constant and it like
it looks like it's sort of a big whimper rather than a big reverse big bang right so so that that's
one like now I mean in terms of reconstructing people later in simulation if you haven't say
chronically preserved their brains or something I think I mean certainly like creating recreating
everybody who could have lived like that that's like a kind of a super astronomical number
it depends a lot on how finally you individuate a person from like another very similar person
like at what point are you close enough to basically say yeah that's kind of you know
that's my customer even if like your your replication like I like it got a few memories
slightly rough and like you know there's like a few details that but it's like captures the
essence of him close enough that we could say that it so that's like more like a philosophical
parameter you would have to put in it's like an open question to what extent if you matter and sort
of arbitrarily powerful super intelligence how close could it get to recreating a human mind
assuming they are like dead and decayed by the time just from a behavioral traces like like
the writings or photos that their friends took you know on the holiday or like whatever other like
information traces like if you if you were like a super intelligence and you started all of this
material and compared it with other humans information traces and maybe some brains that
you have access to and you sort of made the best possible inference taking all of this information
into account and you try to create like something that was as good an approximation as you thought
like how close would that approximation be to the real
Michael Sherman right it's an open question which is quite hard to really get a good grip on
and certainly I think it would have to be close enough that like your friends couldn't tell a
difference like if your friends still survive at this point for example or your your kids or
whatever like if they are my wife doesn't know sorry if my wife doesn't know she can't tell yeah
it's not like if some replica was created that was like close enough to you in its qualitative
behavior to sort of trick everybody you know including like your spouse and kids and parents
like would that be close enough I mean I mean at some point it just depends on your value system
like how close does it have to be for you to hear enough about it as if it were a perfect replica
um it might not be a factual question so much as a value question like
how similar does an entity existing tomorrow have to be to me now for me to care about it
in in this kind of self-interested prudential way that I normally care about
the person waking up in my bed tomorrow that's me like um like if if I if I knew that I was
just going to be transformed overnight into a dragon that remembered nothing of my past life
and shared none of my current interest and but made out of the same atoms like I probably wouldn't
really care that much about that dragon or at least not more than I care about any other
living sentient being out there because it would be not in any meaningful sense me even if it
like consisted of my atoms maybe because it ate me during the night or something like that but
oh like Kafka's uh Kafka's metamorphosis where you you know wake up as a cockroach or whatever
but the the problem with that is the the the human mind is still in the other being that
wouldn't happen right if there was a true transformation so let me just hit a couple
more big topics here for you I'll let you go so the other the other mind's problem the hard
problem of consciousness how do you how would we ever know if an AI was sentient and conscious
if we don't even know that you and I are yeah well yeah and maybe this might have to be our
last topic so that that's a big and practically relevant question now I think we are starting
to have AI systems that are not where it's no longer ridiculous to imagine that there could
be some form of conscious experience happening inside them if we look at the number of parameters
and their behavior it certainly seems to match some non-human animals like in that we think
are probably conscious and so yeah um this is a very difficult problem I think there are different
approaches you could take um you could uh try to take some current theories of consciousness
of the shelves and try to just apply them to the case of AIs so we have for example a global
workspace theories is one theory of what creates conscious experience like that have been proposed
and it's the idea that the things we are conscious of are ones that are sort of entered into a
cognitive workspace from which different other more specialized cognitive models can sort of
read and write but the thing in the shared workspace is kind of accessible to all the
different parts of our brain there's like another theory attention schema theory which says conscious
experiences are sort of arise in our in the modeling of our own attention mechanism so we
have like a little part of our brain that keeps track of what we are likely to be paying attention
to and that and there are like a few of those you could like apply those to to AIs and um
if you do that it basically looks like either some AIs are conscious or that it would be relatively
easy to build an AI that is conscious using current technology like if you just put together all the
pieces into one system that just checked all these boxes you could um you could try to I mean you
could ask you could ask an AI you could like that's how you would like with humans like if I want to
know what you're feeling or thinking like I the most I wouldn't I could try to put you in a big
FMRI scanner or something like that but realistically I'm much better off just asking you right and you
could tell me and so now we have AIs that can speak it's a very natural thought to say well
let let's ask them um and I think that might become a useful technique but with some important
providers which is that um it wouldn't give us any information if you deliberately trained the AI
to give you a predefined answer so it's very easy right now when you find you in an AI to sort of
train it to like when asked if you're conscious deny it or conversely you could train an AI to
like affirm it so so if you want to use this to get any possibly relevant information about the
question you would first of all have to refrain from deliberately biasing the AI during training
then there are other things you could do you could try to detect if there are
multiple modes of cognitive operation in an AI system like basically you could try to find
interpretability methods that allow you to differentiate when it is trying to say true
things versus when it is just kind of rehashing things that it remembers or trying to be entertaining
or stuff like that like this goes back to the problem of hallucination that that you brought
up earlier with some current AI systems so there's like preliminary research that suggests that you
can like sometimes track when the AI is lying versus when it is trying to tell the truth based
on different neural activation patterns so you could then see if you combine that with a self
report you could see that when when it says I'm conscious or when it denies it does that
statement occurs in in a mode of operation where it looks like it is trying to say true
things is it like the same kind of thinking that it uses to try to factually answer other
questions you could see if it is able to say a lot of other things about its internal states
that they're not really about consciousness but like whether are you currently aware like are you
currently paying attention to x y or z what are your like ask it other things about its
capabilities to check whether it has like the the ability to introspect that that could give you a
little hint maybe and so there are some other ideas like that that they're still very kind of
premature but there's an interest now amongst some of these people and including to some extent
some of the people working in frontier labs to try to figure out because you know at some point
we need to figure this out from a moral point of view like if we are building sentient creatures
like at some point it becomes really important that we treat them right and stuff is the next
rights revolution for ai yeah yeah and i'm getting that like getting a good sort of
happy cooperative relationship going is really important i think because it might well be that
in the future most minds will be digital and so like making sure the future goes well not just
for us but for them too i think is a key design criteria enough anything that would be able to
qualify for the name of deep utopia all right deep utopia there it is get the book read it it's filled
with pretty much every single biggest question you could ask about humanity for our futures right
there thanks nick we'll have you on uh back after the singularity happens that'll be your next book
yeah that's my thing

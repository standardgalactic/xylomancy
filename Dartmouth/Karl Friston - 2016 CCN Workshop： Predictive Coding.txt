Mae gennym g
Y dslunأنwch o archives yma yng Nghaeth Ber cynlluntyni.
Tyn ni dcorir Dedd, â mis mewnul â cael myfio ati-ynghori.
Doeddwn nid wedi ar hyn o'r kasu ddestiant.
Felly os gallwn llawer i'r erstenu hyzeu cyf headacheied,
arill oddi cwestiadulte, ni i allain trebau f pillows a llweddau.
S strangers, yn credu gyda'r proses o Dliad02!,
i syth bys mor ddych chi'n gweithio,
fen theories that they got and making half hours to go back
and revisit those bits.
If you see a slide you think that's interesting I'd like to talk about that.
Remember it and I'll go back to it
It gives me an excuse not to explain the details
because there's a lot of things I'd like to expose as to for a discussion.
まあ интерес yddi'n gallwch wyda biz ac Dilamu Gordon
a'n cael о'r fawl uchaf oľfer, oedd ser bethau'ないwyr – o'r atilydd y trofiad cymaint.
Archw recordion ym vere updates yma, roeddwn i'r ond gwinellas.
Y su wлеer i'r argymell gynlog a ddon, oherwydd yn eiran y cy savvy,
o sk fünf
ond iddyn nhw y fryd sy unrhyfromllion'r horsidwyddiad inchi boil i ddyddangos medd sans.
Gawaysol wrth oursied format.
Os yw yog yw yma, nad y mae'n ceisio'r tuwc arna iddyn nhw.
Hysb� poemydd Cymru.
o Chysylltióprin가�u
Mae'r cwpio'r cwpio'r cwpio'r cwpio'r cwpio'r cwpio'r cwpio'r cwpio'r cwpio'r cwpio'r cwpio'r cwpio'r cwpio'r cwpio'r cwpio'r cwpio'r cwpio'r cwpio'r cwpio'r cwpio'r cwpio'r cwpio'r cwpio'r cwpio'r cwpio'r cwpio'r cwpio'r cwpio'r cwpio'r cwpio'r cwpio'r cwpio'r cwpio'r cwpio'r cwpio'r cwpio'r cwpio
Mylywch i chi'n rhaid y dyfodaed?
Mae ni wneudol eich egmer o ond rhoi bod nes participation yn ei реffence ruwsio'r mahlau rhywunol
rol agrlyweddol byddai alwyr yn cael Eisteddします mewn hydy mae'r fagrif'r newid 你功 Darren
ynghyd ar hynny o wedi gwneud o ein fishych cymaint unexpectedly ac mae'n enchu gael
a allau ar ôl i этоch chi cynnag lemon iddy ac awacyju bod amser gyflymu i ei ad rien
升 fydden o ddim yn gallu bod fasch wrapping traficial sydd y baj channelsau glybu
i ryw ga ni'n helpu hay cadw i'r cy owesr hefyd ddysgu cynnag noblech lwy Cream
o iddadeb o gweithredu gynnig yn fwy o agelpoleig chi gydwyr
rhyw bindaeth arwag o'r ydych chi diwrnoddyn nhw vinellion ddwyroddod y mount
yn y cyfnod o'r ysgrifennu o'r amser yn ymgyrchol, ymgyrchol, ymgyrchol a'r ffysiologiaeth,
ac y gallwch chi'n ddweud i'r newid yng Nghymru yn ymgyrchol.
Felly yw'r ffordd o'r ddweud yn ymgyrchol, ac yn ymgyrchol o'r ddweud i'r ffordd o'r ddweud,
ac yn ymgyrchol, yn ymgyrchol, yn ymgyrchol, yn ymgyrchol o'r ddweud,
o'r ddweud i'r hirach oedd arTO Gothic o'r bitigol, oeddi daddY Meddwl Lleidon Caerdydd i выgareddol,
a ddim yn gwbl i chi iddynt dros i'r cymuned a wedi ddoch chi'n gofio eichser seller,
d SWF Cysw slight o'r teimlo iav mins Quran.
Dwych chi e Beltian o'r lovederwyr yn ddweud, fwy'ch hyffrion trainee.
I wybod i'ch fawr, a fyddwn yn ei chigoed i chi'n gweithio y vaccination.
Ase left!
Wrath pinw Ð hwnnw ynmymwys.
Ac rwy'n szemyd gyda'i gw tranffoeth o'r ail run,
edrydu?
Ymddir i'wthatcodeu?
Byddaf ni'n dod gael y bethau er malol y byddwn ar taswch a gennamu.
� threatened..
Yn ymd yetdidd charges a gjentwch
ac oedd yn cerdd i'ch rilippo angen
o fod o'ch cerdd o flagio ar'w gynnig.
Ond llw요'n cael ei d круffio gyda'i.
I attackser is the basis of everything I'm gonna say.
Your behaviour is always driven by beliefs.
That tells us something quite important.
Here's you scanning and searching.
You found a little mouse you might want to eat there.
That's quite important because it speaks
to two basic classes of ways
of thinking about optimising behaviour.
ryw iawn eich dilfyniad o ddod nhw'n naes eu ddatblygu wnaudao'r achful yn bit Wrdd y dygedu ddod o'u'r achwun
i hynny ddim bleMEI
strongly menoniedd mi gref Cymru'r huniol y teishio mewn certainrhy persön.
Rydyn ni wasuch yn fyddiw'r huniol o'r ddigon yn eu bodohol,
yn y lleel~), a'r ddigon cabinetar wedi'u clywed agfer,
i'n geaf Healingอด yn y gall ei gwneud,
a'i grid i edrych i arfataethu mewn cofio Toronto a Greif.
Mae'r dringol ei wneud mewn sicr dyfod mewn aethau i gyllideb i gyd yn ysgr ê увидll,
is that action depends upon beliefs about the world, states the world, and subsequent actions.
So not only is it a function of beliefs about the world, but it's the order in which you interrogate that world.
So it makes a difference whether you search, then eat, as opposed to eat, then search.
And that means that we are in the game of optimising sequences of policies or actions.
I'm going to call it a sequence of actions policy.
So what that means from the point of view technically of what sort of thing we have to optimise,
it's a functional of a belief integrated over time, or summed over time, a path integral.
And if we call that an energy, then the integral, the path integral of an energy is called an action.
So what we've just said is that we've reduced the problem of good behaviour to Hamilton's principle of least action,
where action is the path integral or the trajectory integral or the sum over an energy functional of beliefs.
And that's the basic premise that I'm going to pursue.
And just to highlight the distinction, if you subscribe to this way of thinking about how systems work,
then you end up with optimal control theory, basing decision theory, enforcement learning, and all that good stuff.
Conversely, if you believe this is how biological systems work, then you end up essentially with Hamilton's principle of least action,
the free energy principle, active inference, active learning, and so on.
And that's what we're going to focus on.
And the energy functional that I'm going to consider, we've already heard mentioned, is the variation of free energy,
or the free energy, which we've already heard, very roughly, scores surprise.
It approximates surprise or suprisal, and is simplifying assumptions, prediction error.
So what we are saying is that we're just in the game of minimising prediction error,
and more specifically prediction error over time, over sequences of behaviour.
I'm going to quickly go through this because there are lots of interesting connections for the existing theories and formulations.
This is a bit technical. These are both iconic and ironic equations.
You'll hear more about those later on.
In words, if it's the case that good agents, good people, minimise their free energy, their surprise, their average surprise,
and they emit their uncertainty, then they must believe that the actions that they emit will minimise expected free energy.
You can write that down very simply in terms of these belief functions here,
and rearrange them in a way that discloses important links with lots of established formal treatments of behaviour.
So I've written the expected free energy associated with any particular policy in terms of its expinsic value here,
and its epistemic value.
Basically, these things score the surprise about what you predict will happen under a particular behaviour,
and what you think should happen. Your preference is like, I'm going to eat a mouse, and I'm not going to be hungry.
So that's a surprise bit, explicit or expinsic surprise bit.
There's another surprise, but there's another sort of average surprise or entropy, a relative entropy,
which is called epistemic value. It's a reduction in uncertainty, or the information gain.
That's the key bit. It's the epistemic, which is missing from classic theories,
but it's part of this formulation of Hamilton's principle of least action.
That relates very closely to theories of visual salience, of Bayesian surprise.
Technically, Bayesian surprise is the divergence of the difference between a prior belief and a posterior belief,
or a posterior belief to be informed by observations here.
What we're saying is that we will choose to act in a way that reduces our uncertainty relative to prior beliefs,
looking at data which gives us information, that maximally reduces that uncertainty,
that has the greatest epistemic value, or Bayesian surprise.
In fact, mathematically, that's exactly the same as the mutual information between the causes,
the hidden states of the world S, and the consequences, the outcomes that we actually observe.
Another way of saying this is that we are subscribing to the principle of maximum information,
mutual information, or minimum redundancy, or maximum information efficiency,
or the sort articulated by Horace Barlow.
All ways of expressing one particular form or perspective on this underlying functional.
Another way of thinking about this in the case, if there is no ambiguity,
if we actually can observe the states directly, then we can discount this uncertainty term here,
and what we're left with is something called KL control,
which is the state of the art of what people would use in optimal control theory and dynamical systems control.
In economics, it's called risk sensitive control, it's minimizing risk.
It's a surprise between what I think will happen and what I want to happen,
and if what I think will happen is a surprising relation to what I thought was going to happen,
then I have a high degree of surprise, a high degree of risk, and I want to minimize that.
Finally, if there's no ambiguity or there's no risk,
then we reduce to classical expected utility theory,
or the sorts of theories that reinforcement depends upon,
just maximizing our preferred outcomes there.
Clearly, in order to be surprised, we have to have predictions
against which we can match outcomes to score that surprise,
and this brings us to generative models.
The departure that I promised you from what people currently understand in terms of predictive coding,
and what I'm going to talk about for the next few minutes,
is I'm going to formulate generative models not for continuous state space,
of the sorts used in predictive coding of say visual angles or content or acoustics,
but generative models in which we can label the entire world in terms of a number of discrete states.
These are generative models for discrete state spaces,
and they don't normally have the look and feel of predictive coding,
but my story will be, is in fact they do.
They are actually formally very, very similar to the sorts of schemes that we understand
in terms of top-down predictions and bottom-up prediction errors
in hierarchical message-passing allopredictive coding in the visual cortex.
So, in these models, all we have, this is not, ignore the equations,
but it's focused on this graphical model here.
What we're saying is that the world unfolds in one of many, many states,
and the transitions from one state of the world to the next state of the world,
encoded by probability transitions, that themselves depend upon how we act.
They depend upon the policies that we choose,
and we have a certain confidence in those policies,
devoted by their precision or inverse temperature beta here.
So, if we knew the probability transitions or the transitions from time to time of the states,
we can generate a sequence or trajectory of states,
and each state, at each point in time, generates an outcome
through this likelihood matrix A.
And that's it. That's the generative model.
We're saying the world has states, they unfold,
and each state generates an outcome that's observable.
And that's the basis of everything else that I'm going to say.
So, if I'm now given a generative model,
what I can do is I can evaluate the free energy of my beliefs
under that generative model,
and I can then minimise everything with respect to that proxy for surprise
or uncertainty, namely the expected free energy,
and I can write down equations or solutions
that tell me how an optimal agent person would behave
in a sort of Bayesian sense.
And these are the solutions to the equations expressed in terms of the parameters of that model.
So, A was this mapping from states of the world to outcomes,
and B was the mapping between subsequent hidden states.
And despite the complicated nature of the equations on the previous slide,
the actual updates, the solutions are incredibly simple.
And furthermore, they look very much like the sorts of things that the brain does.
So, for example, expected states of the world are a non-linear sigmoid function
of linear mixtures of expected states of the world and observations.
So, we're mixing together evidence from outcomes
and our beliefs about the states of the world
to update our beliefs about the current states of the world.
Our beliefs about what we're going to do next, our policy pie here,
is this a softmax function of the expected free energy
weighted by an inverse temperature parameter
that you will see we associate with dopamine,
a classical softmax response rule.
If you're not familiar with that,
that's what people in economics and choice behaviour use
for those people who deal more with perception.
We also have a model of incentive salience,
the confidence or the precision
or the inverse temperature associated with our beliefs about action.
Now it becomes, has a base optimal solution that depends upon the goodness of a policy
or the negative goodness of the expected free energy here.
And the form of these equations speaks to a rough anatomy of computations in the brain,
a computational anatomy.
And it sort of goes like this, where we have these equations dictate
what each update needs to know about the other updates.
So, basically, it prescribes a connectome
for the exchange of information or sufficient statistics
that is implied by placing the Hamilton's principle of least action
on the simplest sort of generative model that you can imagine.
And that's the sort of anatomy we have here.
Outcomes, expected states, expected policies,
the goodness or the expected free energy of policies,
the precision of policies, states in the future,
and which prescribe action.
So, I won't go through that, but I'll just give you a more heuristic version of that one.
So, what those equations tell us.
So, this is like a very top-down argument.
It's not, you know, let's think about how the brain works
and come up with some hypotheses.
This unfolds or unravels from,
impacts from just applying Hamilton's principle of least action
to a very simple generative model.
And what it tells us is that sensory input comes in, say, at the back of the brain.
It informs and updates expectations about hidden states of the world,
sometimes referred to as state estimation.
They are associated with a free energy or a surprise
that is combined with an evaluation of those states
in relation to prior preferences
and the potential reduction of uncertainty, their epistemic value.
They are combined to give us beliefs about the policy
that we are currently pursuing.
We have a certain confidence in that policy.
And then those policies are used to weight all the different states
conditioned upon what we are currently doing
to give us the best estimate of what's going to happen next,
the next state of the world.
And if we know that, then we can choose the action
that brings about, that realises our expectations,
our predictions about the next state of the world.
That action solicits a new observation from the environment
and the cycle begins again.
So, we have a perception action cycle
that falls out of the minimisation scheme
that we've just been talking about.
So, very briefly, I'm just going to show you
how that sort of thing works with a series of examples
and then hopefully I'll turn it over to you
to see what you want to talk about.
The first example is just a very simple simulation
of foraging in a two-arm maze.
So, in this example, there's a little rat here
and there are rewards on the right and the left arms of the maze,
but the rat doesn't know where the reward is.
There's also an informative queue at the bottom of the maze here
that if it went to solicit that queue,
it would then know where the reward was
and it could only make two moves.
So, it can either take a chance
and go to one of the other top arms
or it can be a bit more clever
and resolve any uncertainty about the context
it's currently operating in,
which arm is baited
and go and retrieve the epistemic value of the informative queue
and then make an informed decision.
That's exactly the searching that you were talking about before,
scaling your environment, knowing where you are,
resolve your epistemic, solve the epistemic problem
and then turn to your prior preferences or your pragmatics.
And you can write this model down
in very simple terms of these A and B matrices here.
There are trivially partial reinforcement here
and the C matrix here just denotes the preferences
what sorts of states this wrap thinks it should occupy,
it basically thinks it should be in the baited arm
and not in the unbaited arm.
That's all it's saying here,
with minus 3s and plus 3s on the upper arms that are baited.
And if we do that
and we just integrate those solutions that I told you before,
we actually generate very realistic behaviour,
summarised here in terms of the expected policy
and the policies that this agent or this little animal can entertain are,
it stays there and then goes to one of the three arms,
sorry, or it goes to one of the two arms
or it goes to the bottom and then goes to any of the three arms.
So there are eight policies here.
And what it does in the first instance
is because it doesn't know where the reward is.
It gets the cue and then obtains its reward.
What we've done here is actually baited the left arm all the time.
So slowly it accumulates evidence
that in fact the reward's always on this side here.
So as time goes on, it actually switches and learns
and it's probably better to avoid
or dispense with the epistemic move
and go directly to the reward.
And it starts doing that after about 20 or 30 trials here.
At which point it's reaction times
and this is the actual floating point operations of the scheme decrease
and because the goodness of a policy is this path integral,
it's actually spent more time being rewarded.
So if you like, the payoff also increases by going straight there.
So this prescribes good policies
and it can be used to simulate nice behaviours
of the sort you've seen experimentally.
But what I want to do finally is just connect that to neurophysiology
and neuroanatomy.
But to do that, I have to have a process theory.
I have to have a theory which says this particular neuroactivity
or this particular connection strength
corresponds to this quantity in the model.
So I have to have a process in play that is neuronally plausible.
And the way that we're going to do that
is just take those update equations that we've seen before
and instead of just writing down the solutions mathematically,
I'm going to recast the solutions in terms of a gradient descent
or a hill climbing or actually a hill descent here.
So this is a standard way of optimising something.
If you've got a quantity you want to minimise,
you just go downhill until it stops getting smaller.
If I do that, I can write down exactly the same scheme
in terms of differential equations on expected states of the world.
So a very similar form, but here that's a rate of change of activity
which is now a nonlinear function of linear mixtures
of expectations about states of the world and the observations.
And in doing that, I've created a dynamical system
that now is much closer to the look and feel of a neuronal system.
And that now enables me to look at the dynamics that underlie the behaviour.
And these are the dynamics here
and we can basically break these into inference and state estimation
in terms of the updates or the fluctuations in the states
as new evidence comes along, policy selection that we've already seen
with our softmax response rule,
and learning as we accumulate from trial to trial
evidence about particular states or contingencies of the world
in this instance that the left-hand arm of the maze was always baited.
I illustrated those things here, a couple of interesting things to note.
First of all, with every new move and every bit of new sensory information
there are lots of fluctuations in these states
that look very much like an ERP.
Furthermore, when we become a little bit more automatic
or not habitual but certainly going straight for our reward
there is an attenuation of these responses.
The confidence, the precision in those responses
also shows these phasic changes and progressive changes
as we learn the context so we actually get something
which looks remarkably similar to transfer of dopamine responses
as we become more familiar and more confident about the outcomes that we see.
Let me just quickly show you a couple of those outcomes.
This slide highlights just one trial
and it shows the representations of time over the different hidden states of the world
and just highlights a couple of things.
First of all, it shows that as we accumulate evidence for our preferred policies
or our preferred outcomes
the probability that we are in a state which we will ultimately choose increases
whereas the probability of states that we don't decreases
and this is formally identical to evidence accumulation or drift diffusion models
but now cast or now a consequence of a gradient descent on variational free energy
or a bound for surprise.
What we also see is an interesting dynamics
in the sense that if information keeps coming in maybe say 250 milliseconds
like the frequency in which we go and sample the world with the mechanic eye movements
that means that we have two timescales in play.
One is a theta rhythm as we go and get information once for say four times every second
but within each sampling there's this fast updating that's minimising and optimising our beliefs
and that faster updating has a temporal scale in the gamma range.
So what we see is effectively as we move along fast updating
that repeats itself every theta cycle
but as we accumulate more and more evidence
we get more and more efficient and confident about the things that we are inferring
the dynamics mean that they accumulate evidence more quickly, more efficiently
and we get a phase procession of the sort seen in the hippocampus
and I've already mentioned that as Tang goes on
by virtue of increasing our confidence as we assimilate this evidence
then that confidence is expressed in the confidence of our policies
and we have a nice way of simulating dopamine responses.
We can look at the behaviour or the activity of these representations
of different states of the world at different points in time during our policy
and if we plot their responses as a function of where the rat actually is
we can simulate place cell activity.
There has many characteristics of the sort seen empirically.
This illustrates this theta gamma coupling
which is an almost necessary consequence of this sort of
solitary sampling of the world and then updating beliefs quickly
before the next sample comes along.
Again the sort of thing that one sees empirically.
We can now do violation responses exactly as Jim was talking about
what I've shown here are the responses to two trials.
They're identical in nature but one is from the beginning of the trial
where the rat was not familiar with its environment
and one is at the end of the trial where it becomes very familiar
just before it starts going directly for the reward.
Interesting if we look at the representations of key states here
what we see is a much more efficient and therefore less exuberant
updating of expectations of hidden states
that if we subtract the standard familiar one from the odd ball
or the unfamiliar one we reproduce the temporal dynamics
of things like the mismatch negativity in ERP research
and we also demonstrate this transfer of confidence
or similar dopamine responses from the rewarded Q per se
to this instructional condition stimulus here.
I'm going from slightly negative to positive here.
We can play similar games by introducing deliberate violations
and illicit P300 responses.
We can look at reinforcement learning
by switching contingences halfway through
and look at the effects on dopamine-urgent responses
and also electro-physiological responses.
How long have I got?
No, that's very good, isn't it?
I've only been talking for 25 minutes.
Good.
Then I can be true to my promise to finish in half an hour.
This is the epilogue.
That's the story so far.
Most of that will be in the next few weeks
in the published literature.
You'll notice at the moment there's nothing really about hierarchies.
Most people here I'm sure are more interested in the implications
of this sort of theory for perceptual hierarchies
and evidence of accumulation and purely perceptual domain.
The more recent work that I wanted to,
this is not published, to introduce you to,
is now taking this formalism,
which has a lot of construct validity in relation to choice behaviour
and your economics, active vision, active sensing,
and see what it has to say about the source of themes
we're more interested in, which is the hierarchical message passing
and the deep generative models that we assume
that the brain is using to actually understand perceptual sequences, say.
So this is the epilogue.
Again, I'll just speed through this in five minutes.
What we're going to do now is tell exactly the same story,
but now we're going to put one of those discrete state-space models,
the known as Markov decision processes,
on top of the first one,
and another one on top of that, and another one on top of that.
So in this construction,
hidden states, at any one level in the model,
don't generate outcomes.
They generate the first or the initial hidden state of the level below,
and then that cycles over a few iterations,
and then terminates like the rat-terminated
when it entered the baited arms of the cues.
And that process repeats hierarchically to any arbitrary depth.
So what we have are deep temporal generative models.
And they're really interesting,
because not only do they have a hierarchical structure in their form,
but also in their time,
because if the state at any high level
is generating the initial state
that must have subsequent states,
then it means that the lower states unfold more quickly
than the higher states.
So one way of thinking about this is,
the generative model says that at this hour,
at this minute, and at this second,
I am safe, I was reading,
I'm on this page, on this paragraph, and on this word.
So if you think about the lower levels
as ticking over more quickly,
like the second hand of a clock,
and every revolution or every trajectory
or every path they take,
then the high level goes forward one step,
and then it goes round again,
and then another step,
and then that process is repeated.
So as the minute hand is going round,
once it goes round, then the other hand goes round.
So what we have here is a generative model
that basically has in mind, literally,
beliefs about the world
that are much more protracted in time
and are hierarchically nested.
So if you could invert this sort of model,
you would have a representation of the context
and the context of context
and the context of context.
At each point, as you go deeper into the model,
they are more temporally enduring.
So you'd know that working from the top down,
you'd know the story of the narrative.
If you knew the story of the narrative,
you'd be able to generate a particular sentence.
If you could generate a particular sentence,
you could generate a particular word.
If you could generate a particular word,
you could generate a particular letter.
All at faster and faster
are more elemental time scales.
So that's the sort of model now
that people are starting to play with.
It has exactly the same form as before.
We've still got our policies in play
that generate transitions among hidden states
that generate outcomes,
but now the first hidden state is generated
by the same sort of model,
formally identical at a high level,
and there are transitions over time here.
But they are much slower.
I've indicated that by making these red lines
and red states here a different colour from these,
because these basically are the same as these,
but they're reused at a later point in time.
This sort of model now allows you to think more carefully
about the hierarchical message passing
and the implications of neuroanatomy.
So if we now turn straight to the process theory,
these are the differential equations
that fall out of that generative model in the previous slide
if I express them in terms of a gradient descent on free energy.
What we've done here
is to write them in terms of prediction errors.
So we're back now in the rhetoric of predictive coding
and things that people are familiar with.
The prediction errors here are basically
the difference between the log of an expected state of the world
and the evidence for that
in terms of previous and subsequent hidden states
and the observations at any particular level.
We also have prediction errors on our preferred outcomes
denoted by C here and the actual observations here.
This actually renders the computation of the free energy
and the expected free energy very, very simple.
They're just actually average prediction errors,
which you try to suppress in a statistical sense here.
But the important thing from the point of view
of neuroanatomy and neurophysiology
is once we've written down these equations
for each hierarchical level I here,
we now know what representations or expectations
are needed to compute other expectations or representations.
So in exactly the same way as previously,
we now can write down the computational anatomy,
the mathematical connectome,
implied by putting a gradient descent
on these hierarchical or deep temporal models here.
That's helped enormously by knowing about the brain.
So, for example, we know that the only neural populations
that send messages to other parts of the brain
are principal cells by definition,
superficial and deep pyramidal cells.
If we look at this, we know the only things
that happily pass between different levels,
hierarchical levels, are the expected states down here,
which are the sort of Bayesian model averages over the policies
when we condition the states on each policy.
So we can associate the expected states
with these red cells here,
superficial and deep pyramidal cells.
We also know from these equations
that the expected states have to be passed
to the prediction errors.
And we know that the cells in receipt
of four connections are the spinestellate cells
in granulase here,
dirty by the blue layers here,
that are in receipt of the four connections.
So we now can write down,
at least iconically or graphically,
a message passing scheme
that starts to look very much like
the intrinsic and extrinsic connections
that we know about, intrinsic being within, say,
a cortical macrocolumn or a cortical area,
extrinsic being between areas, different hierarchical levels.
So this could be, say, V1, V2, V3 here,
when the four connections go from the superficial state estimates
here to the granulale layer here,
and then the deep pyramidal cells
sending back predictions to the superficial
and the deep layers encoded by the C
and the D parameters of our generative model
from the previous slide.
And also we now have an interesting candidate role
for the basal ganglia
in the sense that we also have to do the evaluation,
and that evaluation has to go back a nuance,
the selection of the policies
that aren't necessary
to actually take the basal model averages
to get the expected states.
There's a nice circular causality here
that is necessary from the mathematical structure
of this inversion scheme.
And that's this cartoon here
in terms of the extrinsic connectivity.
Again, we can go back to this if people are interested,
but there is actually a remarkable correspondence
between not only the deployment,
but also the sign of these extrinsic connections
and those dictated by that particular
message passing or gradient descent
that I showed on the previous slide.
And if you subscribe to this sort of scheme anatomically
and the theory being a metaphor
for neuronal activity in the brain,
what you'd end up claiming
is that the goodness of a policy,
this negative expected free energy,
is encoded in the chordate for high level,
more abstract representations
in the deep generative model,
for more intermediate levels,
sorry, the chordate for intermediate levels
for more abstract ones in the globus pallidum,
and in the putamen for motor loops,
whereas the policy expectations per se
have here been assigned again
to the globus pallidus interna.
So just a way of getting from
the mathematical anatomy
to biological or neuroanatomy
in a purely top-down, dispassionate,
mathematically dry way,
just taking the equations and seeing
what form do they imply for message passing
and what sorts of message passing
do we see in the real brain.
Andre Bastos will, I think he may not,
but he did a lot of work
of this sort on intrinsic connectivity
within a macro column,
clinical micro circuits, predictive coding,
exactly the same game can be played here
for this discrete state space model,
but there's one key exception
which Lars might like,
because there's been a lot of debate in Scotland
about whether the superficial parameters
cells encode predictions
or expectations or prediction errors.
In this scheme,
in this discrete state space scheme,
the superficial parameter cells
code expectations, not prediction errors,
and you may ask why.
Well, the reason is,
they do encode prediction errors,
but they do it by physically in a very different way,
and that follows from the form
of this differential equation here
where we're expressing the states
as a sigmoid function,
a softmax operator on V,
which we associate with depolarisation,
where the rate of change of depolarisation
of voltage is proportional to the error.
So the error from the point of view
of a neural mass model
now becomes the conductance.
So the cells are encoding prediction error,
but the error is in the conductance.
So when all the postsynaptic drives
to the conductance postsynaptically
are in balance
and there is no further change
or drive to the potential,
that means prediction errors are zero.
So when a cell or a population
has reached electrodynamically
steady state,
it's found its minimum prediction error
and then it fires,
but it is the firing here
that is associated with
the expected states of the world here.
So that's an interesting thing
which I thought might be useful for discussion
in terms of reconciling a lot of paradoxes
about what's been passed forward
and would you expect it to be explained away
or would you expect it to be boosted,
but it has happened all sorts of interesting issues here.
Closing with an example of reading,
a deep hierarchical model where we have beliefs
about six sentences,
each comprising four words.
Each word here has iconic letters
that can either be in an uppercase
or a lowercase,
the palindromic in the sense that it doesn't matter
whether the cat has to flee from the cat,
it doesn't matter whether we flip them
in a horizontal way,
it still means the same thing,
but it does this agent as a surprise
if we use a lowercase.
Three words.
Let me just skip through this
because we want to spend more time
in discussion if we can.
So that's just a generative model
with two levels,
semantics or sentence structure,
word structure and outcomes generating particular,
if you like, letters,
but there are icons in this instance.
And then with this scheme
we can simulate things like reading.
So here's a little four-page story or sentence
and that word is flee,
that word is wait because there's nothing next to the bird,
that word is feed because there are seeds
that the bird can feed on,
that word is wait,
so this is a sentence,
flee, wait, feed, wait.
And that's a happy sentence
and it will categorise it as happy.
But the problem that we're trying to address here
is exactly what we started with.
How do you scan? How do you search?
Where do you go forage for information
to resolve as much uncertainty as you can
about which of these six sentences is in play?
And when the system does this
just by trying to minimise its expected free energy,
it shows this very interesting behaviour
where it jumps from one word
or page to the next
without really dwelling and wasting time
resolving uncertainty that is already resolved.
So once it sees a cat,
it already knows that this has to be a flee word
and it doesn't need to see where the other letters
in this word are actually doing,
it already knows there's no more epistemic value to be had,
there's no more uncertainty to resolve,
it'll now jump to the next page
and resolves uncertainty after a couple of
saccadic eye movements, then jump to the next page
and after just once a card in the final page
it knows exactly what this sentence was doing.
And if I can,
I'll just show a movie of it doing that.
So the red dots correspond to where
it's looking at the present time
and the images that are mixtures of the icons
represent conditional expectations.
And the main point to be taken from this
is that it knows there's a bird there,
but it never looked there.
It has sufficient prior knowledge
in its deep-demp temporal model.
It doesn't need to actually go and see stuff,
it knows stuff is there
because it knows what caused that stuff.
And with this sort of simulation
one can then do exactly what Jim was talking about,
which was if it knows stuff and it has predictions
then it should be possible to disclose
or reveal that knowledge, that predictability
by introducing violations
and elicit the sorts of classical responses
that we see empirically.
And what we've done here is because we've got a deep model
that can do local and global violations.
We can make the final story,
the final sentence a very surprising one
without changing any of the stimuli
at the same time with or without
making the prior beliefs about the upper lower case,
the sort of local featureal expectations.
We can switch those around
so we replay exactly the same stimuli
and the same behaviours,
but just by changing the prior beliefs of the agent
we can cause certain things to be surprising
and those things can either be at the local,
the first level or the higher, the second level.
And if we do that
we get lots of behaviours
that look again a little bit like
delay period activity in the prefrontal cortex
of a periscadic sort that you see
prior to a saccade being selected and enacted.
While at the same time
the band pass filtered voltages
that are being driven by the implicit prediction errors
look very much like periscadic ERPs.
And when you look at those periscadic ERPs
under local versus global violations
what you actually see is something almost identical
if it's a local violation to a mismatch negativity
whereas for the global violation
you get the mismatches or the differences
much later on in time
that look very much like a P300.
I can see what I was going to show you.
No, I can't. Yes, I can.
That was a very pretty slide, but I can't.
It was very clever.
It's a quote from, well, you don't need to know that.
And then the final slide
it's got a thank you.
A lot of people were on this slide
but you'll never know who now, will you?
So thank you very much.
So the workshop is structured
so that there's a lot of time for discussion
after each talk.
And so the floor is open now
for people to ask questions or make observations.
I was just informed about how they do this
in philosophy conferences
that involves raising your hand or your finger
but I haven't mastered that yet
so I don't understand it.
I think it's too complicated for this group.
But not for philosophers.
So who would like to start?
I'll start.
I was just wondering if you could say more.
You mentioned that you get something
when there's choice involved
with the rat experiment simulation.
Something that looks like a drift diffusion model
and I've always been puzzled at how you get something
that looks really like choice or agency
out of a predictive coding model.
So maybe you can elaborate a little bit on that.
So the question is
where does the choice come into predictive coding?
I think that question.
Let me just be a little more specific.
It's not that I don't see how you get choice behaviour
in the sense that you can use predictive coding
in order to evaluate some options
but the notion of agency seems
if what you're doing is just predicting what you will do
then it seems to kind of undermine the notion of agency.
I see.
So the answer to that question is very simple.
It's tribly simple.
You put agency into these schemes
through prior preferences
define the sort of agent that I am.
So we were talking before about reducing surprise of all sorts
whether it's epistemic uncertainty
but the simplest sort of pragmatic surprise.
If I have a cost function
that I don't want to be very hungry
or I don't want to end up in an arm that has no rewards in it
then I'd simply have to have the prior belief
that at the end of the day
I will end up rewarded or sated or happy or complete.
So that anything else that happens is surprising
and therefore I can then bring the whole machinery of predictive coding
to bear upon the problem of suppressing prediction errors and surprises.
So pretty simple in terms of predictive coding.
If I a priori believe that I am always going to be happy and complete
and that I am built to always minimise my prediction errors in the future
then I will look as if I have agency
I will look as if I have purpose
because I will always choose my actions
in a way that avoids the prediction errors
that suggest that I am not happy and complete.
So the answer is just to absorb cost functions into inference
by making costly states surprising through prior preferences
and that comes out of things like planning as inference.
There are lots of ways of articulating that.
From the point of view of the rhetoric that I was using
the expected free energy has two bits to it.
It has this epistemic bit and this pragmatic bit
but very simply it's uncertainty and surprise.
The epistemic bit is minimising uncertainty.
The value, the purpose, the goal is a pragmatic bit
defined through cost functions
that are literally the surprise of a costly outcome.
Just to follow up a little bit.
Is this sort of like a hyper-prior that's going to be...
It sounds like we all have to have this ultimate belief
that it's all going to end well at the end of the day.
So none of us are really pessimists or something like that, right?
By definition.
You may be perverse in your optimism
but you are quintessentially optimistic.
You're getting to some...
The deeper backscore is behind the free energy principle.
The only assumption that this instance
of Hamilton's principle of release action makes
is that you exist and if you exist
that means you behave as if you have beliefs that you exist
and by existing that just means that you're not decaying or dying.
All your states are within some bounds
be they physiological, homeostatic or pecunary
in terms of being rich or in terms of interceptive inference
and the hydraulics on that, happy.
But it's all about keeping things in bounds.
It's all about minimising entropy, minimising uncertainty.
So it always looks as if agents that exist
have prior beliefs that they exist.
And when you unpack that, that simply means
I have preferred states that I will expect myself to occupy.
Literally they are attracting states.
They are an attractor.
So that rhetoric, which actually is a rhetoric
from dynamical systems theory,
applies identically to this sort of purposeful reinforcement learning
or sort of goal directed style of thinking about things.
There are attracting states.
They are simply the ones that you frequent,
which means that you will appear to behave
as if you have prior preferences for being in those states
and you will always choose actions to get to those states.
It is those prior preferences that define the sort of agent you are.
So in answer to your very first question
the agentfulness comes in by implication
or just through the sorts of priors
that characterise that particular sort of agent.
So if I was a virus I would have very different preferences
than if I was a person.
But there are still both plausible and viable preferences
in the sorts of agents.
Hi, thanks for your talk Carl.
I was wondering now that the slides are back
if we could go to the delay period activity
that you had briefly mentioned.
And I just wanted to see that a little bit unpacked
and related to what you were just talking about
that is agents reducing their free energy.
How does that principle then generate
the delay period activity that we see
in places like prefrontal cortex
and in working memory and so forth.
Thanks.
Well, those sorts of phenomena
which we all know and have and will try to explain
and measure empirically.
So let me just try and find it.
Do you remember where it was?
Right, thank you.
There you are.
All those sorts of nuts and bolts
getting down and dirty in terms of what this scheme would do
when you put dynamics on it through the gradient descent
depend upon the generative model.
So at your very similar to the last,
one key component of the generative model
are the priorities, the preferences,
what gives it purpose, what are its goals.
Your question, I think, has a very similar answer.
Once you've written down the generative model
everything else is not up for discussion.
The maths tells you exactly what has to happen
once you've written down the generative model
and the delay period activity you're talking about
simply follows from the fact that you've got
a deep generative model or a deep temporal model.
So as soon as you write that deep structure
into the model it means that certain beliefs
have to outlive or change on a slower temporal scale
than other beliefs lower in the hierarchy.
Which means that you have to have delay period activity
whilst other stuff unfolds
at the lower levels of the hierarchy.
So in this particular example
what I've done here is show the beliefs about
the six sentences over the four moves
giving us six times four moves
or five moves back.
So it should be about 30 beliefs here.
On the same time access as beliefs
about the particular word that's currently being seen.
These resets here indicate
the onset of saccars and the acquisition of new information.
And you can see roughly every 250 milliseconds
there's a saccard and new beliefs are updated
about the current word.
But each at the high level
we're only considering beliefs about the
each letter my point is we're only considering
beliefs about the word.
So beliefs about the word corresponding to
what's on this page or what's in this word
are invariant during the successive saccards
as you sample the different letters.
So these things change more quickly than these things.
When these are completed then there's a change here
and then the cycle begins again.
So these tick over faster than this
and then this looks a little bit now like
the rastas that you see
prior to the emission of the saccard here.
They're not from the same paper but a related paradigm.
If I take the voltage causing this delay period activity
and band pass filter it
you get these sorts of fluctuations out here.
So when there's an increase in delay period activity
there's usually a positive deflection
that looks a little bit like an ERP.
And just a follow up.
So is the presence of delay period activity
is that associated then with prediction error
or with the build up of a prediction?
No I think the prediction error in this scheme
and this is the maths that comes from the discrete aspect
of the data models lies in the rate of change
of neural firing.
If you associate the bi-physical encoding
of expected state of the world
in terms of population firing rates
that if you subscribe to that if you accept that
then the prediction error now becomes the conductances
that drive the depolarisation that drive the firing.
So these basically reflect the fact that as time goes on
you can more and more confident
that one particular sentence is in play
and you can see that this is beliefs about the
which sentence is in play
at the first, second, third, fourth and fifth
eye movement or page or word
and they are now internally consistent
and at every point in time in the past
at the end I now believe I was reading the first sentence
and that belief endures during the sampling
of all the actual letters within each of the words.
So these would now represent just basically
numbers between nought and one,
zero and 100% neural firing
that score your expectation
that this is the current state of the world.
At the beginning there's lots of ambiguity
not an enormous amount but there is ambiguity
it's 50-50 because of the six sentences
only two of them begin with the word flee
which means that we've resolved our uncertainty
about four of them but we're still ambiguous
about having ambiguity about sentences one and four
and that can only be resolved at the end
because these sentences only differ in the words
right at the end.
So during this time there's delay period activity
which we've got these two explanations
hypotheses in play that are resolved epistemically
optimally right at the end
when we get to the last word here
and it's a weight and that determines
which of the letters it was.
Thank you for the talk.
I wanted to go back to this idea
of this contrast between reinforcement learning
and the kind of formulation that you're making here.
So one of the things that I thought was interesting
is this formulation in terms of external value
plus you basically decompose your KL divergence
to external value and epistemic value.
So how do you get exploration in this model?
So it seems to me that you're doing an RMAX
over actions to get some balance
between immediate value and information gain.
Is that the basic idea?
Yes, we can look at the equation
or we can look at this and that's absolutely right.
The exploration is good that you brought that in
because another perspective on this
is the whole foraging ethological perspective
on exploration versus exploitation.
That rhetoric just maps very simply
to the epistemic and the pragmatic.
There is no up for discussion
or there's no ad hoc waiting between the two.
The expected free energy can always be written down
in terms of exploration plus exploitation
in terms of the epistemic value and the pragmatic value.
What happens is in minimising that one quantity
you get this scanning searching behaviour
until the epistemic bit has been reduced,
allowing then you to focus on the pragmatic bit.
So for free you get a base optimal exploration
to the extent that it is sufficient to resolve uncertainty
given the precision of your beliefs about your prior preferences
that then allow you to pursue your goals.
This solves the exploitation dilemma
in a base optimal sense.
It also suggests that the very carving of behaviour
into these two complementary drives
is actually probably a misdirection.
It's only you and me that have actually teased apart
the two components of the expected free energy
and calls one an epistemic exploratory one
or a novelty seeking one
and the other bit a pragmatic cost function
like rewarding preference goal directed like one.
There are lots of different ways of rearranging those.
You can also rearrange them in terms of risk and ambiguity,
intrinsic and extrinsic value.
There are lots of ways of carving them
and getting different perspectives.
When you see that and when you work with that
you start to realise that it's not necessarily the best thing
just to have one particular religious perspective on it
because it lends you to the false belief
if you subscribe to this formalism
that there has to be some other adjudicator.
There has to be some other harmonculus
that decides, oh, I need to explore now.
And now I've done my exploration
and now I'm going to go and do a bit of pragmatic,
scanning and then I'm going to exploit what I've discovered.
It doesn't work like that. You should get that for free.
If they're both part of the same cost function
then once you've sufficiently reduced your uncertainty
then you go into, as illustrated here,
your exploratory behaviour.
This is exploratory behaviour
or novelty seeking in the sense that you don't know
what the queue is going to tell you.
It doesn't have any immediate rewarding aspect to it.
There's no preferences associated with the condition stimulus
but it's interesting, uncertainty resolving.
But after a time it becomes boring
because you already know what it's going to tell you
and once it does that then you can exploit your behaviour.
So I should have done that.
I should have put exploration and exploitation.
Have an argument with me because there's meant to be a discussion.
Do you not like that? I'm telling you the whole...
It's a very interesting perspective on it.
I'm surprised that it comes out that the simple deterministic policy
works well and just kind of works out of the box.
That's more than the...
My inkling is that when you go and implement this stuff
that can be difficult for it to classically balance,
to balance the exploration and exploitation.
So there's no tuning parameters, there's no off policy sort of estimation.
You just draw it in there.
It all works out.
I know exactly what you're saying
because this was a big selling point
when we first realised that a couple of years ago
and it will remain a bit like one of these five to ten year...
changing the direction of the ocean liner of the oil tanker.
But two years later all that we're saying is
in fact people behave according to Hamilton's principle of least action.
That's all that we're saying.
And that implicitly or it looks like
that behaviour has this dual aspect.
It doesn't if you formulate it as a variational principle
of the sort you did at school
when doing new turning mechanics
and then Einstein did with general relativity.
It sort of falls out of the mix
in a way that does actually dismiss
these separatist perspectives on
I can either do this or that
and I've got to now optimise the exploration
in relation to the exploitation.
And the key trick that puts you into this simple world
of Hamilton's principle of least action
is the realisation you can't prescribe good behaviour
unless the prescription is an optimisation
of a function of beliefs.
So a really simple example would be
an economics game.
I've got a really high risk and a low risk option.
But there may be a third option
which is if I don't know which is which
I should do nothing until I know more.
In using words like I don't know
I am now saying that my behaviour
now becomes a function of my knowledge
or my uncertainty.
So I now induce different options,
different behaviours, different policies and actions
that rest upon my degree of belief
which means you can't do it
with value function optimisation
or utility function optimisation
or the enforcement learning.
It can't be done with queue learning.
It cannot be done with a Bellman optimisation scheme
because what you've done is you've said
that there are better behaviours
when I don't know what to do
which I generally don't do anything
or wait until more information comes about.
And once you write that down
you make your objective function
a function of a probability distribution
which becomes an energy
and then you integrate that over time
and then you've got to Hamilton's principle of least action.
The simplicity post hoc
is evident for me anyway.
Does that make any sense?
Can I ask you a question about the general approach here
which is
I see that you start saying that
there is an urge, there is a force to reduce surprise.
So this is like a
I'm making an analogy with physics
because that's what you're doing.
And there you start saying
I can rewrite the whole thing instead of force
and furry energy.
And then you go on and explain what it implies.
My fear is that
there is a difference here between
what we do in terms of behaviour.
First of all reducing behaviour to
just minimizing surprise.
There are other forces
that we cannot measure.
We cannot even measure forces
in terms of reducing surprise
in an individual.
So even though you can write these equations
and describe something general
so you can prescribe what should be
the brain doing
do you think you can actually make any prediction?
I know you're doing it
but I'm asking how
can you think that you can do it because
there is no measurement
to tell you that's actually the case.
So in physics you could come up with
any new evidence
you would write a new term into your equation
and just keep adding it and then
you're always safe because there are some conservation laws
that basically
will keep the Hamilton principle intact
but who said there is such a thing
in terms of behaviour and is I think
the simplest thing to assume there is no such thing.
So then by doing that
if you let's say you keep adding terms
because you believe that
what we are doing is just reducing surprise.
This is the ultimate goal of behaviour
then just keep adding terms
to your free energy
and then
basically the whole thing
becomes
a tautology
because you're assuming that you're adding terms
and there is no proof or disprove for that.
So that's my main question is that
what is
how do you think it's going to work?
Right
That's a very good
I ended quite a bit weakly
but that was a very good question.
So lots of really interesting issues
the tautology issue
the practical utility
of this style of theorising
can it ever be falsified?
Adding things to
so I think
we could spend hours
talking about any one of those.
First of all, I am not
the whole point of my style
of neuroscience is that
we never add anything in
you're always obliged
for every advance you have to get rid of something
which was a distraction or ad hoc or a heuristic
we've been talking about this magic parameter
the nuances, the bound spin
exploration and exploitation
so that just goes
there's only one thing that's being minimised here
about everything and that's a variation of free energy
that's it, there's nothing being added
however, of course
the free energy is a function of the generative model
so all the interesting, all the hard work
all the heavy lifting understanding
this biological system in this experimental context
or this social context
that really calls upon
you writing down a generative model
so that's, it doesn't mean
there is no free lunch
there's still a lot of neurobiology
and psychology and cognitive neuroscience
and computational neuroscience to do
it's just all at the level of the generative model
not the normative principles
or the variation principles behind it
the tautology, part of that drive
the simplification
is a drive to get to the ultimate explanation
which has to be tautological
and for me the free energy principle
is tautological
it's as tautological as natural selection
but beautifully so
once it's completely tautological I'll be happy
part of that tautology
comes along
in a slightly technical guise
called the complete class theorem
and I'm bringing that to the discussion
because it speaks to, I think
a really interesting point you were essentially making
is there anything that you can
not explain any behaviour
in a real biological system
that cannot be explained by this
because if there isn't then
what's the point
now
if there is no behaviour that this cannot explain
so it is provably true
that for any pair
of
cost functions
and behaviours
there are a set of prior beliefs
prior preferences that we're talking about
that endow
the behaviour with an agent
that render that behaviour
based optimal and by definition
therefore conforms to the free energy principle
so what that means is that
there is no behaviour that this can't describe
if you can find the right prize
so is that a weakness or a strength
well in the sense of falsifiability
it's a weakness
in the sense of actually
using it practically it's a real strength
because what that means is
any system
normal human being
co-hort
that you bring to me
if I can solve the problem
of getting the most appropriate or sufficient
and good generative model that describes their behaviour
it means I can quantify exactly
the sort of person they are by their prior beliefs
and we actually can write that down to the first question again
is what makes an agent an agent
it's their prior beliefs
it's that attracting set that describes
the sorts of states that that sort of person occupies
so yeah
there is a deep tautology
that there's a fundamental difficulty
for falsification
but there's also a glorious insight underneath that
which means that everything can now be written down
in terms of an agent's prior beliefs
and that if you can get the right model
you can actually estimate these things
you can actually quantify them
and this is one of the tenets of computational psychiatry
and be able to use games
ERPs, mismatch negativities
whatever
in the service of say
what sort of person am I looking at
quantified in terms of the prior beliefs
about the way they should behave
Carl, but aren't you making the assumption that
there is just a unique
set of beliefs that will match
a data set
because if you have several beliefs
and I would think that when we minimise
those type of system
that's equivalent to several solutions
and which almost always we've got several solutions
because we've got several minimas
there because it's not convex
the story that we are minimising there
so therefore we have a lot of minimas
and I would say that if we lucky
that means that for any behaviour that we can measure
we are now with a set
and that could be thrilling to know which one
a set of prior beliefs
and so it's hard
in other words we cannot inverse that
or am I wrong? No, no, no, you're absolutely right
but you've taken us into metabasian land now
okay so just to
those people
who may be getting a bit confused
so the argument I think
let me just paraphrase it
if we're now
saying well how do we practically use
this style of thinking
and I'm now in the job
of quantifying
this person with
autistic spectrum disorder
given a bead's task
an earned task in terms of the
prior beliefs about the volatility
of the environment and
the need to please the experimenter
my
responding within a certain time frame
if that is the problem
then we're now in and observing the observer
or using Bayesian inference
to make inferences about a Bayesian
machine
which is the autistic patient
which is hence the metabasian thing
and you're absolutely right that could be an ill-posed problem
so there may be
the complete classroom does not say that
there is only one unique
set of prior beliefs
that will render the behaviour based on
whether it exists it doesn't have to be a unique solution
however what will happen is that
if you actually use Bayesian statistics
to infer the prior beliefs
of the Bayesian or ASD subject
you will then see
if you've got the appropriate model
that there are a number of equally plausible solutions
and you will also see
that you haven't got enough data
to disambiguate between them
but you will also have an insight into
which sorts of experiments you would need
to do that disambiguation
because you've got a generative model underneath of this
you can do simulations
to see the sorts of data
or the ways of looking at the responses
that would enable you now to narrow down
these competing but equally plausible
sets of prior beliefs that characterise that subject
so it's a very important issue
but I think it's a generic one
about how we actually apply Bayesian statistics
to understand the ways in which our data
is being generated
so in that sense
it's less profound than the complete class theorem
in itself which speaks
to the sort of tautology of the game
that we find ourselves in
I think we can have time for one more short question
I'll have a short one here
I have two short ones
Oh that makes three
The first one is about the semantics
and the second one is about the substance
so regarding the semantics
I'm wondering
it seems as though there are these two aspects
to the problem
one is inference, the other is control
and there is the epistemic cost
in the context of inference
and there is the pragmatic costs
ultimately reproduction, the well-being
and survival of the organism
and it seems as though
you're declaring the primacy
of surprise
or absorb the pragmatic costs
under the epistemic costs
so that's a bit of a semantic issue
perhaps but doesn't it make more sense
to do it the other way around
and say so the ultimate goal is reproduction
and epistemic
benefits should be
a sub-goal of that
so that's the semantic question
and the question on substance
is this something
that
is already going on
in engineering
in AI for example
can these principles be scaled up
to real world tasks
such as visual object recognition
if so has it already been done
or is this an impending
revolution for AI
and if not
is it just a reinterpretation
of what already exists
Jen is agreeing because
there were very short questions
very short
you couldn't get them shorter than that could you
do you want me to try to answer them
shortly
the semantics one
no I don't think so
I take your point
that one could articulate a whole theory
and absorb uncertainty
into cost functions
but I think that
misses the key
point that I was trying to make at the beginning
of the talk
that the quantity you have to optimize
is a function of probability distributions
or beliefs
and that's the key thing
which means you can never use
a Bellman optimality scheme
to properly solve that
you do see heroic attempts
and those heroic attempts have been endemic
for the past 40 years
they're known as partial observed mark-up decision processes
or belief state machines
and all sorts of heroic attempts
to try and recover
or resolve the problem
of once you write down
a discrete state space
over continuous beliefs
you can't do it therefore you have to parameterize
those continuous beliefs in those sorts of
glorious and clever ways
they don't work they don't scale
so people have been aware of the problem
and hence the vast literature
on partial observed mark-up decision problems
what I'm saying is that's the wrong
approach
is Hampton's principle of least action
where the functional that you need to optimize
is a function of the beliefs
before you start
and by proof of principle
I can demonstrate the veracity of that argument
because I can solve problems
which people working with partial observed MDPs
cannot solve
so I think it's much more than semantics
I think it's actually people have got it wrong
in the 20th century
I think the Bellman's optimality equations
are very beautiful construct
complete misdirection
we should have been looking at Hampton's principle of least action
and that's the 21st century
are people doing this?
so Google DeepMind
for example they've now
a small group within Google DeepMind
have now started using
variational free energy
in their deep generative model
not the temporal models but certainly
10 layered neural networks
in the context of the
sort of pattern recognition approach
so
people have always reared
I think the variational approach
was the right way to do this
most of deep learning started
with Geoffrey Hinton's work
and he came from
the variational formulation
so he was the first person to be down
to propose the Helmholtz machine
but they've taken a sort of
security stream back to those
that early work in the 1990s
will it scale?
I don't know
because I don't think they're quite on top of that
because what they do is they
they haven't quite got
well this is insulting but
is anybody here from Google?
I won't say that then
well anyway
they love amortisation
they love casting things in terms of learning
so what they do is
they're actually trying to optimise
their beliefs, their expectations
they try and optimise
because they constructed deep nets
convolution nets
that have parameters
that would map from data
to beliefs of sufficient statistics
or beliefs and then they learn that
because they're experts
and they are the experts
in optimising the parameters
that's a slight problem
because it denies any context sensitivity
the sort that we deal with
the neuroscientists and I deal with
in my simulations
optimising their deep networks
then they'll be in this game
and then we'll find out whether it scales
that you'll need lots of computer scientists
big computers
so I would imagine the next 5 to 10 years
this style of approach
and so the variation free energy formulation
will become increasingly dominant
in people doing artificial intelligence
and I should quip
for some people the new AI
is actually active inference
it wasn't
it wasn't
a coincidence that we chose
that sort of rhetoric
now to chip him out of it
yes so maybe I could just have a comment
rather than a question then
okay
so that's a bit
straight-laced approach to
free energy reduction
it's based on surprise
aversion but I'm sure you have
another talk on surprise seeking
curiosity, creativity
playful managerial styles
which must have some
adaptive purpose beyond
our interest
in horror movies and jokes
it must help us get out
of dead end
to problem spaces we can't even
imagine
so the comment is
if you add surprise seeking
to free energy minimisation
I don't believe it remains tractable
so I'll leave it at that

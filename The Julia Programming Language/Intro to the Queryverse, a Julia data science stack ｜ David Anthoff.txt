2.5mm
3.5mm
4.5mm
5.5mm
6.5mm
5.5mm
6.5mm
7.5mm
8.5mm
9.5mm
9.5mm
10.5mm
11.5mm
12.5mm
13.5mm
11.5mm
12.5mm
13.5mm
14.5mm
15.5mm
15.5mm
16.5mm
17.5mm
18.5mm
19.5mm
19.5mm
20.5mm
21.5mm
22.5mm
23.5mm
24.5mm
25.5mm
26.5mm
27.5mm
28.5mm
29.5mm
30.5mm
31.5mm
32.5mm
33.5mm
34.5mm
35.5mm
35.5mm
36.5mm
37.5mm
39.5mm
40.5mm
41.5mm
42.5mm
43.5mm
44.5mm
45.5mm
46.5mm
47.5mm
48.5mm
49.5mm
50.5mm
50.5mm
51.5mm
52.5mm
53.5mm
54.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
55.5mm
Okay, so this is an anonymous function, it takes one argument and then it returns a pair where the key is the argument and then the value of the pair is actually the value squared.
And here I just use the shortcut syntax of underscores to write that anonymous function.
Well, once we create a stream of pairs, we can of course pass them into a dictionary.
So here I've started out with a range of ints and then in my map I created a sequence of pairs and because it's now pairs I can actually pipe it into a dictionary.
So now you can also understand how this works with tabular data.
Remember we have the cars dataset, which is tabular data, so that's a sequence of named tuples.
If I pipe this into the map command, I can access an individual column by saying underscore dot origin because that will extract that field from each of the named tuples that gets passed through here.
So what do we get with this one here? Well, we get a sequence of just strings, okay, because that's what I've extracted here.
So now I'm going to do a very brief detour about named tuples.
Oh, you're going to see my emails. Well, okay.
So remember cars is a sequence of named tuples.
So what I can do is I can actually, cars, I can collect this into an array and I store that in the variable r.
So what do I get if I do this here? Well, I get an array with 406 elements and each element is a named tuple and then you get this horrible named tuple syntax that we are viewing that we have in Julia dot 6.
This will all get a lot nicer in Julia dot 7.
But we get an array of named tuples.
Let's extract, let's just look at the first element here.
So I'm going to index into this array here, extract the first element, assign that to mt.
So this is one individual named tuple.
You can see here it views a named tuple as a sequence of names and values.
So this one here has fields, miles per gallon, cylinders, origin and so on.
And then for each one it stores the value that is associated with this one here.
If we look at the type of nt, you can see it's a named tuple type.
We can also call the field names function on a named tuple and we can see all the fields that are stored in this named tuple.
If we have a named tuple, we've seen that already, we can use the dot notation to extract an individual field.
So I can say nt dot origin to extract the value of the origin field here.
Or I can actually also index into this because named tuples are, you know, behave,
I think you can think of them as a superset of just normal tuples.
So I can also extract the third field by indexing into a named tuple with the standard array indexing story.
So the example we have previously here where I say map origin is based on that.
So this anonymous function here will be called once for each input row that we get from cars.
Whenever it gets called, the underscore will reference a single row and then we're extracting an individual field there.
So this is how we can deal with named tuples if we get them from somewhere.
But often we also want to create named tuples as an output.
And especially in maps we often want to return a sequence of named tuples so that we actually create a table.
So how does that work?
Go back to my slides.
So there's special syntax that only works within query operators for creating named tuples and that's curly brackets.
So the syntax here is if you want to create a new named tuple you open curly brackets
and then you separate each field that you want your named tuple to have with a comma.
And each field needs to be specified.
You need to say the name of the field that you want to have there.
So that's field name one for example and then you assign a value to it.
And you can add arbitrary many fields here and you can give them names and give them values.
And this whole expression here will just create one new named tuple.
There's a shortcut syntax here in the following way.
And you write this one here.
So this is almost exactly the same syntax as in the previous case.
But for the second element of the name tuple I'm actually not specifying a field name.
Note here that I'm only specifying value two but I don't tell.
I'm not specifying here what name I actually want to give to this field.
If that's the case, if you just specify a variable name here,
under the hood it will actually just use the name of the variable that you pass in here as the name for the field.
So this is equivalent to explicitly naming the second field here value two.
This also works if you access fields with a dot operation.
So here for example I'm passing in underscore dot fu
and if I don't specify a name explicitly for this second field in the name tuple
then it will just automatically use the name fu here as the name for the field.
So this ends up being quite handy in practice.
So let's do the following.
So we start out with a 1 to 10 range again.
I'm going to map this.
Now what I want to do is I actually want to create a table out of my numbers 1 to 10.
So I actually want to create a named tuple for each element from my input sequence.
So I'm opening curly brackets and I want there to be two columns.
So I'm creating a named tuple with two fields.
So the first one I want to call fu and that one I just want to have the numbers stored in here.
Then I want to create another column called bar
and that one I will assign the number squared.
So now the map here creates a sequence of named tuples.
A sequence of named tuples is a table.
So I've actually created a table now with this query with two columns and all the values in here.
So this allows you to use query to go pretty smoothly between tabular data formats,
non-tabular data formats that you can sort of mix and match as you feel fit with this.
Okay, so let's look at another example here.
Remember our cars data set.
I'm going to pipe this into the map function here.
Remember we now get a named tuple for each row in here.
And now I want to output another table.
So I'm using curly brackets here.
And what I want to do is here I want to just select two of the columns in the input set.
So I'm going to say underscore dot name and underscore dot year.
So what's happening here?
For each input row I'm creating a new named tuple.
The named tuple will have two fields.
So that means we are creating a table with two columns.
I'm extracting the name and the year column here.
And because I'm not explicitly specifying new names here,
the output table will just reuse the names that I've specified here.
So what I'm creating here is I'm creating a new table with the name and the year column.
So I've selected only those two columns here.
But I can, of course, let me copy that.
I can rename things.
So maybe I want to actually name the year column foo.
I can do that.
So in this example here I'm still automatically using the name.
So I should have used a different example here.
So the variable name, the column name name is used as the first column name.
And then here I'm renaming the year column to foo.
So now because at this point our pipe is actually outputting a table,
we're back in our generic table interface.
And we can, for example, save it, of course, as a CSV file.
Or do any of the other things that you can do with a table.
So that all works.
Here's another example of how you can mix and match between different formats.
So we started with a table, the cars table.
I'm going to map it, but I'm going to map it actually into a pair.
And here I'm going to actually convert the year column into a date type.
And then I'm going to pipe it into a dictionary.
So what I have created here, I've started out with the tabular data.
And now I've created a dictionary where the keys are strings, the values are dates.
And I've automatically piped the data from the tabular data into my dictionary.
So I guess one thing I'd like to encourage folks here
is actually to not restrict yourself to tabular data structures.
We have a much richer set of data structures in Julia.
And Query allows you to mix and match between those
and use whatever data structure is best for a given situation.
Okay.
Let's look at two very quick commands, take and drop.
So those are pretty self-explanatory drop.
So if we start out with a sequence 1 to 10, then drop the first three
and returns the remaining numbers, take the first three elements
and doesn't return anything else, we can obviously combine them.
So I can say 1 to 10, drop the first three and then take four.
So that gives me the numbers 4, 5, 6, 7.
And because everything works on any sequence of values,
I can also use this of course with a tabular source.
So I can drop the first three rows and then I can take only four rows
and I get a table with four rows.
Okay, so let's take a drop.
I'm going to go a little bit faster now because there's much more I still want to cover.
And I'm getting a bit worried about time.
Okay, we've already seen the order by command for sorting.
So how does that work?
So we start out with let's say the cars data set and then I say order by this column here.
Now remember, I told you whenever you see an underscore in one of the query operators here,
it's actually creating an anonymous function here.
So what's happening is there's an anonymous function.
It gets applied to each input row.
It returns something.
And then so you have another sequence of values, one value for each row.
And the rows then get sorted by sorting that other value, which I will call the key.
Okay, so this is essentially extracting a key for each row
and then the rows get sorted in sending order by that key.
If we want to sort in descending order, we use a different command here.
We use order by descending.
Just reverses everything.
Now this kind of sorting here leaves a lot of ties.
Okay, so I'm sorting by origin, but we have lots and lots of different rows here
that have USA as their origin value.
So maybe we want to resolve these ties.
Okay, so how do we do that?
We pipe this into the then by operator and here I can say year for example.
So what does that mean?
It means order things first by origin.
Whenever you end up with rows where you have ties based on the origin column,
then within those tie groups order things by year.
So what we have now is we have everything is ordered.
All the Europe things come first because that's alphabetically first.
And then the rows within Europe, all the Europe rows are sorted by year.
Now it turns out that even at this point, we have many rows that have the same year
and the same origin, so there are more ties.
So we can break these ties by adding another then by clause.
So let's say then by descending and then let's order things by horsepower at the end.
So essentially first order things by origin.
If you have ties, sort them by year.
If you still have ties, sort them in descending order by horsepower.
Because this is based on this sorting by a key idea,
you're actually not restricted to just extracting a column and sorting it by that.
So here what we...
So this is a bit silly, but I can for example do the following.
I can say length of origin.
So what is this doing?
Well, for each row, we're computing the length, so the number of characters in the origin column.
And then we're sorting the whole set by the length of that string.
And you can use an arbitrary Julia function here.
So you can go crazy here and sort by whatever transformation of these input rows you want to sort by.
Okay, so that was sort by.
The next thing I want to cover is grouping, group by.
Let me go to my slides again.
Okay, so what does grouping do?
So we start out...
Let's assume we start out with a table of input data.
So here we have two columns, origin and miles per gallon.
And then we have six rows.
And if I pipe this into the group by operator and say I want to group this by the origin column,
that's what I'm doing here, then the following is happening.
Remember the green stuff here, the underscore dot origin,
is creating an anonymous function that is extracting some value from each named tuple here.
So this anonymous function is extracting a key for every row.
And because we are using the dot notation for named tuples here,
it's essentially extracting the origin column here.
But we could group by something else.
So that's the first step.
For each input row, we are creating a key using this anonymous function.
And then we're going to look through all of these keys.
And every row that has the same key, we're going to group.
So in this case, we will end up with three groups
because all the rows in the input table end up having either USA, EU or China as the group key.
So we end up with three distinct groups.
And then each group will get all the rows that belong to that group or part of that group.
So we have three input rows that have USA as the origin.
We have two input rows that have EU as the origin
and one input row that has China as the origin.
So these rows get moved into these groups here.
So what gets piped out of the group by operator
is actually a sequence of only three values.
And each value is going to be a group.
So the group by operator here, we start out with six input values,
but it's only producing three output values.
And each output value is a group.
Each group in itself is a combination of the key value, so that's part of the group,
and then all the values that are belonging to that group.
So it turns out each group is actually an array.
So we'll see this in a second, but the thing that gets piped further on here
is essentially a list of lists.
So it's a list of the groups, and each group in itself is an array
of the values that belong to that group.
So let's try to see how that looks in practice.
So I'm going to say cars, and I'm going to say group by origin.
Okay, so now I get a very, very unwieldy ugly thing here.
So there are still some places in Queryland or Queryverse
where I have not yet coded up nice show methods, so this is one example of that.
So to make this a little bit nicer to look at,
I'm going to right away pipe this into a map.
So the underscore here now refers to an individual group.
So if you remember this graph here,
the underscore will refer to this whole blue thing,
and remember a group is sort of a composite of the key
and the values that belong to that group.
So if I want to access the key here,
I can say map underscore dot key.
So a grouping, a group object actually has a field called key,
and that will return the key here.
So what we can see here, yes,
we are actually going from all the rows in the cars dataset
to only three rows here, once we have grouped it,
and those three rows are the three distinct grouping keys that we have.
So what else can we do with a group?
So I'm just modifying the map part here.
I told you that a group is actually an array.
So the thing that this underscore refers to
is actually implements the abstract array interface in Julia.
So that means I can, for example, call the length function on it.
So what does that mean?
Well, it means the first group consists of 254 elements,
of 73 and the third group of 79 elements.
Because, so the following code is a bit risky
because it only works if you have enough elements here,
because the underscore here just is an array.
I can, for example, extract the third element from each group.
So what I'm doing here is I'm grouping things,
so I have three groups,
and in my map I'm extracting the third element in each group.
So I actually get a table with three rows,
one row from each group.
Now, this is sort of pretty silly because the order here
is actually not well-defined,
so it's probably not something you typically want to do.
But let me just copy that again.
But something that sort of, you know, is a tip
might make a lot of sense is you might want to summarize things a little bit here.
We might want to create a new table here
where I can say origin equals underscore key.
So I'm creating a table here with a column called origin
and that should hold the value of the things by which we grouped.
And then I might want to add another column here
where I store it that I call count.
And here I'm going to store how many elements we had per group.
So now I've created a table here with a row for each group
and then it tells me how many elements there are in each group.
So here we've grouped by extracting a certain column,
but like with sorting there's nothing that prevents us from doing arbitrary,
calling arbitrary functions on our input stream
to actually create the grouping keys.
So here for example I could also group things
by the length of the origin column.
So here I'm extracting as the key
how many characters there are in the origin column for each row.
So I only have examples here
where some things have three characters, others have two characters.
So I'm only ending up with two groups actually
because there are only two unique length values here.
So by what you group is actually really flexible
and you can use sort of arbitrary things here.
So let me just code that example up.
So I'm modifying this here to actually call length on the origin column.
So we're now grouping by the length of this.
And then here we might want to rename this as origin length.
So in this case here the data set is slightly different.
So we still have actually three different groups.
So we have one group where the origin has three characters,
one group where the origin has six characters
and one group where the origin has five characters.
And then these are the number of elements we have in there.
Okay, so how am I doing on time?
Okay, I think I'm going to finish the group thing
and then I'm going to probably move on to something else.
I'm going to show you the...
Okay, so let's assume you have the following question.
Let's have our cars data set again.
What you want to create is a table where you group things by origin
and then you want to compute the average...
I don't know, acceleration value for each group.
Okay, so you want to compute what's the average acceleration in the US,
what's the average acceleration in the U and so on.
So how do you do something like that?
The tricky thing is that if you think about the group,
what the underscore here refers to in this map here,
it refers to an array of named tuples.
So we can call the length function on it,
but it is a lot less intuitive.
How can we actually compute a mean
for, let's say, a given column in this array?
Because we cannot just call mean on underscore
because then we try to take a mean of lots of named tuples
and that is not defined.
So we can only take a mean of individual numbers.
So I'm going to show you two different ways to achieve this.
So the first one is that you can pass in a second argument
to the group by function or query operator.
So here the first one still does the same thing.
It decides how things get grouped or by what things get grouped.
But now I'm passing in a second anonymous function
and this anonymous function will get applied to each input element
before that input element gets inserted into the array for a given group.
So what does that mean?
So let's say we want to take this first row here,
USA and 34.2, this element here
and we want to place it into the USA group.
Without this second anonymous function
we would have placed the named tuple into the USA group.
With this new anonymous function
we're actually converting this named tuple into a float
because we're just extracting the value from the MPG column here.
So what we end up with in this case is actually
that each group is an array of just the values from the MPG group.
So now the elements in this group are just floats
and no longer named tuples.
And at that point we can of course call things like mean on it.
So let me code that one up, that example.
So we're starting out with cars.
We're grouping it group by origin.
We're going to place only the acceleration value though
into each row into each group.
So now we're going to run a map here.
We're going to say origin equals underscore key.
So the first column here will hold the key
and then I'm going to say the mean acceleration equals the mean of underscore.
So underscore now refers to each group.
Each group now is an array of floats
and so we can take a mean of it.
So what do we get?
We get a table here.
USA mean acceleration is 14.
Europe is mean acceleration is 16 and so on.
So this is the first option.
This works great if you only want to aggregate or summarize an individual column.
But now think about a situation where we also want to add
in our output table another column
where we want to take the mean of another column
or compute some other summary statistics for every individual group.
That no longer works because at the map stage here
we actually no longer have access to any other column
than the acceleration column.
So we need a different strategy for that case.
So let me go back to my slides.
So the other strategy is to not tackle this at the front end
but try to tackle this at the back end.
So we're sticking with the group by operation.
We're not going to change that
but instead we're going to change our map command here.
So the map operator receives these three groups as input
and we're now going to use the following syntax in our map.
I'm showing this on the screen here first.
The first part hasn't changed
so we're still extracting the key assigned that to the origin column.
Now we're saying we want to add another column to our output table
called mean acceleration.
We say mean so we want to compute the mean on something
and then we say underscore dot dot acceleration.
So what does that mean?
This syntax x dot dot something
only works within query operators
so this is another special syntax
that you cannot use outside of query
and it actually gets translated into a generator expression
like the one below here.
So it will create a generator expression
where we loop over all elements in x
and then it will return or extract the y field
from each element in x.
So that's exactly what we want to do here, right?
So underscore here is an array of named tuples
so we loop over all the named tuples
and then for each individual named tuple
we return the content of only the acceleration column.
So what this whole expression here gives us
is a sequence of strings,
sorry, a sequence of floating point numbers
and on that of course we can call mean
and compute the mean.
So the thing that I wrote up here
actually just gets translated into an expression
like the one below here
where inside the mean function
there is a generator call
but you don't have to deal with that
sort of that automatically gets done under the hoods.
So this expression here
essentially for each group
extracts just the values from a given column.
So let me do that in code.
So I'm going to copy this over here.
I'm going to the group by operation
we're taking out the second argument
and then here I'm just going to say
mean.dot acceleration
and that of course works.
And so now we can use
this kind of code
to compute multiple summary statistics here.
So I'm going to break this up a little bit
to make it a little bit more readable
so the name tuple here I'm going to do one line
for each column
and I'm going to add another column here
the minimum number of cylinders per group
minimum
and so what we're doing here is I'm computing
for each group I'm computing
what's the lowest number of cylinders
that we see in that group.
So this kind of syntax allows you to
compute multiple aggregations
for each group in one go.
Okay so my normal plan would have been
to now cover joints next
but I'm going to skip that because I'm running out of time
and there's other stuff that I still
a few other things that I still want to
squeeze in
and I also want to stop
on time at 11.30.
So the next topic I'm going to do instead of the
instead of the join
is I'm going to cover a question
when is actually processing happening
when does all of this
stuff run, when is processing happening?
Okay
So I'm going to write a data pipeline here
I'm going to say load
data
cars
feathers
I'm loading something from a feather file
I'm going to filter it
I'm going to transform
oops
then I'm going to
map it
into a new table
so I'm going to
extract
some columns and then I'm only going to take
the first five rows
and so now
but I'm placing a semicolon at the end
so that it doesn't display anything
so at this point
absolutely nothing happened
no data was processed
nothing was loaded from disk
because
everything in query is actually lazy
so in general
any processing happens
as late as possible
so if I don't want to show this here
then I will not process anything
nothing is happening here
only once I start to iterate over this
or row and queue
because I can iterate over this
because it's an iterator
and I print line the row
only when I execute that one here
is query actually going to the disk
and starting to load files
going to apply the filters
doing the mapping and so on
okay
so in query
everything is lazy
nothing happens
so things do get processed if you show the results
no data gets loaded
nothing gets processed
we defer that to as late as possible
so let me give you an example of what actually happens
when you
run this for loop that I previously showed you here
so you've constructed a query
you didn't show it
the query is stored in queue and now you iterate over
the query here
so here's the for loop that starts out
so what's the first thing that the for loop does
in the first iteration
it will tell the thing that was returned by map
so I'm actually leaving out the take
operator here
it will ask the map operator
give me a first row
I want to show my first row
so what does the map operator do
it asks the filter operator give me
a first row
so far nothing has been loaded
so this is sort of a pull model
what does the filter operator do
well it asks the feather file
to get
to load a row
the data actually gets loaded from disk
before that nothing has happened
and only one row is being loaded at this point
so it's read from disk
that one row is returned to the filter operator
the filter applies
the filter condition to that row
and it turns out checks that condition
oops this one row
was actually did not pass my filter
so what does the filter do now
it asks the feather file to get
to load a second row
that second row is read from disk
that row is returned
to the filter condition
it checks the condition it looks good
it returns that row to the map
the map applies its transformation
that row the transformed row
now gets returned
back to the for loop
and at that point it gets shown
so what you can see here
is
that this is very similar to the loop fusion
story that we have with broadcasting
so the map
and the filter operation actually get fused
inside one loop
where this gets processed
and everything is streaming
so at no point
for example
is the filter operation creating
an intermediate large array
where it first processes all rows
stores them and then passes all those stored things
onto a map
everything is sort of efficiently done
in a streaming
streaming fashion here
so that's what I wanted to show about that
but that
let me just give you one more example here
which is I think really cool
so what we're going to do here
we're going to load data from the feather file
we're going to filter it
we're going to map it
let's just copy this from the previous
example here
and then at the end
I'm going to
save it as a CSV file
so this example here
is completely streaming
so at no point
is the whole data set loaded into main memory
every row
is loaded individually from the feather file
the filter is applied, the map is applied
and that row is written out to a CSV file
then we load the next row
from the feather file, from disk and so on
now there's a lot of caching going on
so it's
if you get worried that we sort of hit the disk
in each iteration that's not
it's much smarter than that
but this kind of streaming data manipulation pipeline
is really great because you can in theory at least
process files that are much larger than
your main memory because
things don't have to be loaded into main memory
okay so now
I'm wondering
I have one minute left
I think I will go
something like 10 minutes over
the thing that I will still cover is
plotting and visual exploration a little bit more
just because I've spent the last year
working on that so I'm
eager to show it off
and then I will conclude
that's
that's the plan right now if the folks from
Julia computing they can send me chat messages
if you object to that
then send me a text
a chat message in Slack
and I will
stop in the middle of everything
so let's go back to
plotting
okay so
just as a reminder this is our data set we want to plot
we pipe that into the plot command
the one thing we always have to specify
is what kind of
symbol or mark we want to use for our
plotting so here I'm going to say
I want to plot each row as a point
so what do I get
I get a plot
where we get a point drawn
for each row but they're all being
drawn on top of each other
so that's not very useful
so let's make this a bit more useful
so I'm going to still say
draw points
and
but
the X position of each point
I want to be taken from the miles per gallon
column
and I have
a typo here
but I am not
oh here
okay sorry
okay so what do we get now
well we still get a dot drawn
a point drawn for each
row in our input data set
the X position
of each of these dots
is laid out
it depends on the miles per gallon column
but we're still getting a lot of
figures drawn on top of each other
so let's make this into a proper scatter plot
so I'm going to say be like plot
points X should be
miles
per gallon
the Y axis we
let's say acceleration
so at this point
we created a scatter plot
we have the miles per gallon
column on the X axis
the acceleration
column on the Y axis
we can further split these things up
so we can say
actually I want
the color of each point
at that point we get a colored plot
where
we get a legend
for the three values of our origin
and then each dot gets colored
depending on what the origin is
but maybe this is
also not what we want to have
maybe we actually want
a separate column
a separate plot
for each origin
so I can say column
so this is faceting the plot
depending on what the origin value is
draw
this dot into a different
column because I'm creating actually a plot
with three plots in three columns
and we
can use the color channel now
to
visualize the values for
cylinders
so now I've created a plot with
three different
panels one panel for each
origin and
the colors now refer to
cylinders
now here's an interesting thing
the cylinder column
I think I sort of started to mention this previously
the cylinder column in the data set
is a numerical value
so
the plotting package here automatically
creates a continuous
legend for that value
because that's what it picks by default
for
numerical columns
but maybe that's not how we want this to be
viewed because actually we know
there are only a few discrete
values for cylinders in our
data set
so instead of specifying the name of the column
here as a symbol I can also specify it as a string
that always works
and I can
use a shorthand notation
that actually the folks at Altair
the python wrapper for Vigalite came up
with
I can write a column and then specify
as what type I want this visualized
so I'm going to say n for nominal
and so what that does
it changes the column
the legend here
to treat each value here as a nominal value
and not as a continuous scale
so now I just get a different color
for every single
value here
but it's
nominal might also not be
the type of scale I want to see here
maybe I want to actually see this as an ordinal scale
that would make sense here
the values are ordered
so we might want to see
use that property here
so I can use
a colon o at the end
and now I'm getting
a legend here
that uses
an ordinal coloring
of things
okay
there are lots and lots of different channels
that you encode in this way
so I'm going to
show you one other thing here
instead of putting the origin
instead of
creating multiple plots here one per origin
we can also
assign origin to the shape property
so what this will do
is it will now pick
a different shape for each point
depending on the origin so now we have a legend here
we use circles for Europe
we use squares for Japan
and triangles for USA
and then each dot gets drawn
using that particular shape
so I'm going to copy this
maybe this is a little bit too small
so let's
increase the plot size
let's say we want it to be 300
by
300 so that's also easy
to do
okay
I'm going to show you a couple other plots
okay so let's
start out with cars again
let's create a new plot here
now we want to do a bar plot so
I'm going to say the type of
visualization that I want to see on my
plot is actually a bar
now I'm going to encode
the x value for each bar
in the following way I'm going to use a composite
thing here
with curly brackets
I'm going to say I want to
use the miles per gallon
column here
but I want to bin things
so it is now going
to look at all the values in the miles
per gallon
column it's going to create bins
out of them and then for each bin
we can, it's like a group
we can call some function
that explores that so in this case
here I want to do the following
I want to say the y column
should actually call
the count function
on each bin
so what does that mean
let's draw this
so it's easier probably if we see this
so essentially what we've created here
is a histogram
the x axis
bin the values
that we see in the miles per gallon
column and then the y value
for each of those
we called the count function
counts how many rows we have
in a given bin
and so we can
get a histogram
very easily in that way
so let's copy that
and then we can of course combine that
with all the other things we've done so for
so I can again
add my column encoding
I can say I actually want a different plot
for each origin so I can create a histogram
for Europe, one for Japan
one for the United States very easily
and so on
so the next plotting example
that I want to show you
is
is something where I
don't start out with tabular data
just to show you how you can combine
all of this into fun things
so I'm going to call the readdir function
so this is just
gives me all the filenames in my current directory
so I'm going to start with that
I'm going to pipe that
into the group by operator
actually let's make this
one line per operator group by
and I'm going to group it by the length
of the filenames ok so at this point
I'm creating one group
for each
length of filenames that we see
in our filename list
I'm going to map it
convert it into a table
so I'm going to say the length
column here
is the key and then I'm counting
how many
files at that length
we have
in our table
so if I execute this here
this is the kind of table I've created
with this query so far
and now I'm going to plot it
as a bar
I'm going to say x equals the count
column and y equals
the length column
I want this as ordinal data
to show as ordinal data
so now I've created
a bar plot
where we have one bar for each
length
of filenames that we have in this folder here
so what this should
signify is that we for example have
5
files whose
filename length
is 9 characters
so I don't know how useful that in particular is
but it is sort of fun to
play around with
non tabular data as well
so I'm going to skip one
I'm going to actually
copy one thing here
this is the plot that I've created already previously
but now I'm
assigning this plot here to
a variable p
so that we can actually look at what that is
so if we look at the type of
p
it's called a
Vega light spec type
so that's
what the Vega light macro returns
it's an instance of a specification
and the specification is really just a description
of what kind of plot we want to do
so in general
I already showed you that we can
pipe these specifications
into the save function
and save these plots in lots of different file
so I can save it as a
png file
I can save it as a pdf file
I can save it as a
svg file
I can save it as
a pps file
and I think that's
what we have
I can also save it
this will not work
because on julia box or any other system
because I'm still waiting for
something to be merged
the following line will not work on your system
I can save it as a
vga light file
this is a file format that the
vga light folks that created that
underlying plotting package have created
so I can save it as that
so if I
look into my
folder here
you can see full vga light
I can open that because jupiter lab actually
has native support for vga light
plots in here
if we open this
can I open this as a text file
I think I can
you can see that this is just json
so it's a json file that describes
the data and
the plotting description
that we want to have
but this will not have worked on your
on your system
but it will work in a couple of days
hopefully
so let's
so
once we have a vga light spec object
we can just save it in many
different file formats
I want to go back to
the example of the
voyager ui
so remember we can take cars
we can pipe it into voyager
and that will show up this ui
the voyager ui
and note here that I'm
saving the return value
of that in a local variable called v
so I do that
the voyager ui pops up here
so I'm going to
look at a specific graph here
so I like this graph of
the years
so I'm going to pick this one here
and then I want to actually
I'm dropping the origin
column onto the color field
so that I created a graph here now
a line plot
with one line per origin
and years on the
x axis and the number of rows
in each year on the y axis
so I've created a plot here
in my interactive
voyager ui
and it turns out that voyager
and the vga light
plotting package that is part of the query where
actually are based on exactly the same
plotting engine namely vga light under the hood
so they interrupt as well
so how does that work
remember
that here I've started
this voyager ui
and I've stored the return value in v
so v is actually
a reference
to this interactive
voyager window
and if I've created a
graph here or plot in my voyager window
I can actually extract that
out of v by indexing
with no arguments into v
so now I've extracted
the plot that I've created
interactively in voyager
and I've extracted that back into my julia session
let's look at the type of
of what we get back here
it's a vga light spec
so whether you create
a vga light plot with the vga light
macro in code
or you create a plot
with the voyager tool it doesn't matter
at the end of the day you end up with the same
with the same type of julia object
and so you can for example
do the following you can
extract the plot from the voyager tool
and save it as a pdf
right no problem
you can also now this is the part
that will only start to work once
one more thing gets merged
I can save
I can save the plot
and I created
the voyager UI
I can save that as a vga light file
later on I can load
that vga light file
again from disk
and it will show as a plot
so what happened here is
I've saved the plot in a jason format
that is specific to vga light on disk
and then I've loaded the plot again
from that jason format from disk
again I cannot actually distinguish
whether I loaded it from disk
or whether I created it with voyager
created it using the vga light macro
but here I can do a fun thing
so I've saved a plot definition
on disk in this file here
so what I can do now is
I can take the cars dataset
I can filter it
so that I only
keep rows
that are either
where the origin is eja
or japan
and then I can
pipe that
into the plot
that I've previously saved as a jason file
on disk
so remember this line here
will load a vga light plot
but then I'm piping some new data
into that same vga light plot
so what's happening here is
I'm using the template
for the plot that I've saved
originally created with the voyager tool
but here in code I'm now piping new data
into the same plotting template
and now I get
the same plot type
but the USA data is not
the Europe data is no longer showing
because I've filtered that out
so that was what I wanted to show you
in terms of plotting
I'm really excited about
the vga light plotting package
that we've created over the last year
and that was really joint work by the way
with Frederick
but I think it is in a shape now
I've really only
scratched the surface of what you can do
with vga light
you can create maps
lots of different types of plots
it's incredibly powerful
there's a really large group at the University of Washington
working on this and adding features non-stop
so I encourage you to look
at the vga light package
read the documentation
there are a lot of examples
that show you how you can create different types of plots
so that's probably the best way to get started
and so let's see what we have here
so we talked about plots and visual exploration
everyone's favorite topic
missing values
I'm going to skip that
the short version here
is a representation
for missing values
based on the data values package
and that makes it
very easy and convenient
to deal with missing values
so conclusion
I just wanted to sort of sum up
a little bit where this whole ecosystem
queryverse is
and then we're done
so I think the basic design
of queryverse has really kept up well
over the last two years in Julia Land
so the core design of this
interoperable table interface
how the different query operators
work together that has really
worked out well
I should say that this was
not my idea
this really built on the shoulders of giants
there was work in
Microsoft research that
surfaced in C-Sharp
that came out of
a lot of functional programming language research
so the
basic concepts that all of this is based on are
sort of long battle tested
and seem to work really well
one thing that is
especially in query is sort of annoying right now
there is a lot of query operators
that are just missing right now
so if you compare the surface
API of let's say
dplyer with that of query
you will see that we just
there are just lots of things that we don't have
we have more than what I showed you today
but we don't have sort of the full coverage
and especially
things around manipulating
columns are sometimes a bit
awkward right now in query
the main reason
that we don't have that right now is that
implementing that will get
enormously easier once Julia
0.7 is out and so I
I've held back implementing these because I felt
I would just waste a lot of
my time implementing something
that would be implemented in a very different way
once Julia 0.7 comes out
so
my expectation on that front is that once
0.7 ships
we will have a query release that
adds all these missing operators
so you will get things like
transform and select from that you might know from
dplyer
performance in general is decent
there are
there are clear strategies how to
improve it so there's nothing sort of
fundamentally
difficult about that it just requires
a lot of time
there's one implementation
detail that the implementation right now
has a nasty reliance on inference
I know exactly how to get rid of that
I've started with that it will
be gone once the version that
runs on Julia 0.7 comes out
that is something that sometimes
you can run into queries that
don't work because
I rely in an
incorrect way on inference so
that can be frustrating and
but that will also be solved in the future
I briefly mentioned
that there's another front and syntax for
query the link syntax
it's also cool and will stick around
so if you want to check that out
that's also an idea
the design
of this whole
version has enormous potential
for other people to help out
for example I have a very
experimental
implementation of a query backend
that if you query
the same syntax
but if you apply it to a sql database
then under the hood
the Julia query
will actually be translated into sql
that sql string
will be shipped to the database and the query
will run within the database
so things like that are all
planned and sort of the design
is set up to enable that
it's really just a question
of manpower
to implement those things
so I think the thing I want
to stop with is
the query verse is really broad
it covers a lot of things as you can see
and so if anyone wants to help out
and be part of that
that would be really welcome
and appreciated
and I think with that
that's what I had for this
tutorial so I ran over
I'm sorry for that
and I think with that I just
stop unless I get other instructions
from Jane

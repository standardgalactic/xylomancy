Well, let me just start with this,
that welcome everybody out in internet land and anybody
from MIT as well.
And let me mention that this Saturday night or Sunday
morning, I guess officially Sunday morning,
much of the United States is going to be changing the clocks.
And so if you're somewhere that isn't changing the clocks,
you might want to keep that in mind.
If you're looking for us Monday 1 PM Eastern,
it might well be an hour.
I never remember whether it's earlier or later for you
than it usually is.
So I know most of Europe is going
to be doing it two weeks from now.
And so I guess it will be an hour earlier for those of you
in Europe, for example.
OK, next thing is we seem to have had a little bit of a problem
with the website.
So I was going to just try quickly
to see what I think would work.
But Dave tells me it won't work, but I
want to see it for myself.
So I'm going to go to Pluto.
And I've gotten into the habit of putting URLs right here.
So I thought that this would work, but David tells me no.
And David is usually right.
But I'm going to try it anyway and see if it will work.
And it does work.
OK, this time, Dave, I guess this time it worked.
Again, I'm glad to know about it.
Right, so the actual URL, which, let's see, what is the,
can we put it in the Discord or something
so that people can find it?
I don't know.
You already did, even better.
OK, so this will be the first notebook for today,
the structure.jl.
And what I'm going to be doing is talking
about a real computational thinking idea, which
is taking advantage of structure.
And structure can be many things.
And I'll give you a few examples.
And then, depending on how the time goes,
we're also going to talk about a particular kind of structure,
which is principal components analysis.
And that's this over here.
So this is this particular notebook,
which I gathered Dave has already put into the Discord,
so you can find that as well.
So let me start with talking about structure.
And there is a new feature that is going to be in the notebooks.
And we're going to work backwards and get them in the old notebooks
as well, which is, what are the Julia commands structures
that are going to appear in this notebook?
And so I made a list here.
You could see four items.
Basically, struck, dump, dagl and sparse,
so I'll count this one item, and error.
So we're going to make an effort to put this
at the beginning of every notebook.
So you can figure out what little bits of Julia
you'll be learning in this particular lecture at the same time.
So thanks, Charles, for that nice idea.
So structure, examples of structure.
So let's see.
I see if this is too big, the table of contents.
Let's see, is this too small for you, Dave?
Should I make it bigger?
You always like me to zoom in.
But I also like the table of contents on the side.
Yeah, that's OK.
All righty.
OK, so structure.
So the best way to talk about structure
is to give some examples.
But let me say that even if you were around for the last lecture
of this semester, we talked about dynamic programming.
And you might remember that the structure that we took advantage
of was the common subproblems, right?
So that there were these paths that would go left, down,
and right, or I call them Southwest, South, and Southeast.
And these were not just sort of random paths,
but these were paths that had a structure to them.
That is actually a common, they had a common substructure.
So that's perhaps one example.
But let me kind of go a little slower
and talk about other problems that have structure.
So here's a really simple structure.
And this is the so-called one-hot vector.
It's a name that comes from machine learning.
It's a very simple idea.
I really like the name one-hot vector.
Here's an example of a one-hot vector.
This is a vector that's made up of zeros and ones,
but only one of the elements is hot, right?
The rest are cold, OK?
So what that means is exactly one element is one,
and the rest are all zero, OK?
And so that is a one-hot vector.
In linear algebra, it might be called
the column of the identity or a coordinate basis vector.
But none of those words seem as good to me as one-hot vector.
I really like that name.
So I hope you like it, too.
And why Ellen does it say one-hot vector, my one-hot vector?
Yours says one-hot.
Yours says one-hot.
Put below.
And name the variable says my one-hot vector.
I don't know, but we could change it.
Well, it's being defined again below, so you be careful.
I don't know.
Is it being one-hot?
My one-hot vector is over here.
Yeah.
I wonder if that happened because we didn't want to clash.
Yeah, that's what I'm wondering.
That's probably what happens.
I don't remember anymore.
I wrote it last.
That's a funny way to do it, though.
Yeah.
That's what it is.
I don't remember my own sense of humor, if it was even me.
OK.
That's because that's the American pronunciation.
One-hot vector.
There you go.
So I would like to ask a question, which
is for everybody to think about, is how much information do
you need to represent this vector?
Obviously, it's made up of six elements,
but do you really need six numbers to represent this vector?
So if n is 6, do you need n?
Do you need 1?
Do you need 2?
I mean, really, what is the information content?
And I'm sure you can all realize that two numbers ought
to be enough.
The size 6 and the position of the 1, 2,
kind of tells you the whole story, really.
So it seems silly to write out a vector 0, 1, 0, 0, 0, 0
in the context of one-hot vectors because two numbers will
do the trick.
By the way, just to mention that there's also the word one-cold.
And a one-cold vector is, again, a vector of 0s and 1s,
but only one of the elements is cold, that is the 0.
But we're going to concentrate on one-hot vectors.
And I'm going to show you how you create a structure in Julia
that takes advantage of the structure.
And coincidentally enough, to create something in Julia
that takes advantage of structure,
it's called a struct.
This is a new type in Julia that you
get to create yourself.
And so this part you could ignore right now
if you find it a little bit confusing.
But basically, we're going to say that a one-hot,
I'm going to create a one-hot, and it's
going to be made up of two things.
It's going to be made up of an integer n and an integer k.
And that's it.
So a one-hot is going to be two integers.
There's nothing more to one-hot, really.
It's just these two integers.
And just to tell you what this is in case you're curious,
we're saying that this will be a subtype of an abstract vector
of ints.
And so that it behaves in many ways
like a vector of integers, even though it's not
a vector of integers, it's a one-hot.
But it'll be a subtype.
That's this funny punctuation, a subtype of an abstract vector
made out of ints.
And a couple of quick things that are actually
useful to have if you're going to create an abstract vector,
and it's going to be handy here, is to define the size of we
want to pretend this is a vector, even though it's
not exactly a vector.
So what can you do with a vector?
Well, you can get its size.
So this says that if you ask for the size of a one-hot,
you should give me back the n.
So the n is actually the size.
This is just the way here.
Maybe I'll go over here and put this in.
If I were to do the size of my one-hot vector,
you see I just got a 6.
And so we want this to behave in the same way,
even though we're not storing all those 0s and 1s.
Another thing we want to do is get index.
So we want to do things like we want
to do the analogy of my one-hot, by the way,
I just hit tab to make that faster.
If I type my one vector of 2, I'll get the 1.
But if I do any other thing, I'll get a 0, right?
If I call it with any other index, I get a 0.
So a valid index, I'll get a 0.
So I want that behavior too.
And so the command there is get index.
Maybe I should have added size and get index to things
that we are showing off in Julia today.
And we need a one-hot vector and an i.
And basically what we're going to check
is what we're checking is whether the number
k in the one-hot vector is equal to i.
And we'll turn it into an int.
So this is a true or a false, which becomes a 0 or a 1,
as it's turned into an int.
And so to actually pull this off,
I could create a one-hot vector by just typing one-hot of 6, 2.
And you see it actually gives you the illusion of a vector.
And it even looks to Julia as if it's a vector.
But yeah, let's see.
My one-hot vector, if I actually index it, for example,
if I index it with 2, I get the 1.
But if I index it with 4, I get the 0.
This completely has the full illusion
of being a vector of 0s and 1s.
And yet it actually takes advantage of the structure.
And one way you could actually see that
is I'm going to mention the dump with a small d and a capital
d.
In capital d, it actually prints in Pluto without any fuss.
The small d, you have to do this silly Pluto thing with terminal.
But I actually think it looks very nice.
It highlights.
To me, it looks like a blackboard.
So I think I actually like the with terminal just to highlight it.
But in any event, dump, what dump does
is it kind of tells you everything that's
going inside that object.
So if I dump my one-hot vector, you'll
see that I'm getting two integers, the 6 and the 2.
So you can use the capital D if you like.
And it'll just print it sort of the boring way.
Or you can use the little d.
And if you just do the little d, I'll show you.
Pluto kind of ignores it.
A regular Julia Ripple wouldn't.
But if you add this with terminal do thing,
then you get this sort of nice blackboard effect.
So there you have it.
My one-hot vector is storing the two pieces of information
that are critical to defining without wasting any space at all.
And so that's structure.
That's taking advantage of structure.
And I don't know why, but I like to do visuals.
So here's visualizing a one-hot vector.
N is the size.
So here I'm just making n be 13.
Here you can actually see it.
And then k specifies where the one is.
And so here's just a little visual.
I don't know if this adds much to the story,
but it's fun to look at.
So there here are some one-hot vectors.
OK, but let me move on now.
Just a comment that basically what we're saying
is that this one-hot object behaves as if it were a vector.
It has the same behavior you could not actually tell
when you're indexing into it or when you're doing length of it.
You can't actually tell how it's being stored internally.
It has exactly the same behavior to the user.
And we already saw another example of that,
which was range objects also behave like that.
You can index into ranges and you can take the length of ranges.
You don't know if how many pieces of information they're storing.
Well, let's do a dump.
Let's go dump of 1 colon 7.
We may have done this before, but let's just do it again.
Yes, we do, or I have to do that with terminal.
So there you see that this also has two numbers,
a start and a stop, right?
If, on the other hand, we took a range
that was even numbers from 2 to 17 or something,
then there's three numbers that store this, the two, the two.
And it's clever enough to know that if you're starting at an even,
you're actually literally stopping at a 16.
So that's actually what's stored inside the computer
is basically just these three integers for a range.
By the way, something that came up the other day,
and you'll see it in your MITx homework, also inspired by Charles,
is if you actually dump this thing,
you'll see that it is a vector that actually contains the range.
And so there's a difference, and we'll explore that in the MITx homework,
there's a difference between this, which is just a range,
which is here, let's make a vector that has a couple of ranges in it.
So this is a vector of size two,
and you can see that each element is itself a range.
So that's enough of that stuff.
Let's do another example of structure that,
let's take a diagonal matrix.
So here is a diagonal matrix that,
I don't know if people see matrices in high school anymore,
as you might see in, let's just say,
I think it's better to say an elementary linear algebra class.
I think that would be a better way to say it.
So here's a diagonal matrix.
It's a diagonal matrix is one that only has non-zeros potentially off on the diagonal, right?
All the off diagonal elements are zero.
So this is a diagonal matrix,
but for a three by three array,
I think you could see that I only needed three numbers to represent it, right?
For an n by n matrix,
I would only need n numbers to represent a diagonal matrix.
And so I don't know if your linear algebra class,
they make you write out all the zeros or put in the dots,
but it is silly to store them on a computer,
especially if the matrix is large.
I guess if it's small, it doesn't really matter,
but Julia has a type called a diagonal,
and it even prints them out kind of pretty.
You could see there's dots where the zeros would be, okay?
And let's do the dump right now, D.
I can't remember if I did it later, but let's do it right here.
You could see that the numbers that are stored are the five, the six, and the minus 10, right?
There's no zeros stored.
That's to be contrast, if you will, with the,
if I dump the density, the one that I first defined,
and as you probably would expect,
this one is storing all of these numbers.
The zeros are stored.
So again, the structure of this diagonal matrix
is just to have these numbers, five, six, and minus 10, okay?
And so as I was just showing you before,
you could take a full matrix or sometimes called a dense matrix,
sort of a matrix in regular format,
and you can cast it to being a diagonal using diagonal,
or you could actually create it by just simply saying,
what are the diagonal entries?
Okay, I've put it over here as well.
So, you know, I guess this is sort of always a good idea.
It's kind of an obvious idea,
but for any kind of data structure at all, any kind of algorithm,
we're always trying to look for structure where it exists.
So let me now introduce the idea of sparse matrices,
which is kind of what we've been doing already,
but there's sort of a more general concept.
A lot of people would not call a diagonal matrix
a sparse matrix, though technically it fits in,
but you know, maybe a lot of people
wouldn't call a square a rectangle,
though technically a square is a rectangle.
I mean, if something's a square,
you should call it a square
because that gives more information.
So what's a sparse matrix?
So a sparse matrix is a different, yeah,
a sparse matrix is, let's give the official definition,
is a matrix that has many zeros worth storing
in a sparse structure.
Now I'm gonna tell you what that means in a minute.
Okay, that's sort of what a sparse matrix is.
What did I do?
Do I have too many quotes here?
Here, let's put it, can I do this?
Okay, all right, so it's part,
and so here's a very simple example.
Here's a matrix that obviously has three non zeros,
okay, the 12, the nine and the four.
And here's a sparse representation of the same matrix.
And for the human eye,
this is actually not how it's stored,
but for the human eye, you could say,
well, the i, j, the row and column index for the nine
is three, one, and the, sorry,
the third row and first column is the 12,
the first row and third column is the nine,
and the bottom right, the three, three entry is a four, right?
And so, and all the other entries are presumed to be zero.
And if you use the sparse arrays Julia package,
you can go and see what's inside one of these matrices.
And it's a little bit tricky,
and I wasn't really planning to go into this,
but it's not that bad.
So I think I will show it to you to show,
to see how a general dense, sorry,
a general sparse matrix is stored.
And maybe this is why I don't think of diagonal matrices
as sparse matrices because they have a special storage, right?
So there's somehow, that's why I don't like to think of them
as a general sparse matrix.
But if we dump the, if we dump the sparse matrix,
what we see is besides the sizes,
the M by N, the rows and columns being three by three,
there are three other vectors stored on the inside,
a column pointer, a row value and the non-zero values.
And it's kind of better to go from bottom to top,
which I've done over here.
So the non-zero values are easy to understand,
they're just the 12, the nine and the four.
Okay, those are the non-zero values.
That's the easiest.
This is how it's stored internally.
It's called CSC or compressed sparse column format.
There are other formats.
This one has become the most common for certain applications
because it kind of generally is good
for matrix vector products and column slicing and so forth.
But yeah, so it's made up of 12, nine, four,
the non-zero elements.
And the row values are just the indices, the I indices,
three, one, three, the same three, one, three
that you'll see over here at the first column, right?
So the 12 is in the third row,
the nine is in the first row
and the four is in the third row.
But instead of storing J,
we're storing something a little bit different.
We're storing the column pointer thing
is always of length one more than the number,
than the NZ value.
So if this is three, this is of length four.
And what it's doing is it's a pointer into NZ value,
which tells you where the first non-zero is in that column
and that might even be in the next column.
So for example, because maybe I should,
let me put an eight here and then I'll change it back
so you can actually follow this along.
Yeah, so I wanna be able to see this at the same time.
Can I do that?
I wanna make it one smaller.
Hope you'll be able to still see it.
Okay, so what this says is you'll notice
that the, let's put a few more entries in, maybe here.
Okay, so the one points to the first entry of this vector
and it says 12.
And the 12 is the first non-zero in this column.
Two points to the second entry
and eight is the first non-zero in this column.
Okay, four says we're gonna look at this, the nine,
and the nine is the first entry in the third column.
And six actually doesn't point anywhere at all, right?
It kind of falls off the end.
And it tells you it's the way of indicating
there's no further columns, okay?
That's kind of the way it's done.
So I guess you could have just stopped with N,
but this is the way it's done.
It's pointing to the one column afterwards.
Okay, now the fun happens if I zero out the second column,
which is how we started out.
If I zero out the second column,
then something interesting happens.
It says, oh, the first non-zero is a nine
for the second column.
It's the second entry.
And by the way, for the third column as well.
Okay, I wasn't gonna go too far into this.
It's a little bit tricky,
but this is how sparse matrices are stored.
So the main point here is we store the values,
we store the i and we do something a little bit tricky
that's somehow equivalent to getting the j.
But I'd like to show you an example
where this may not be a particularly good storage scheme.
Here I'm going to create a sparse matrix
that is a million by a million.
It actually is, if you thought of it as a matrix,
it has a million rows and a million columns.
And the way I did that was I just created an entry
where the million by million,
the i equals a million, j equals a million entry
is the number nine.
So that automatically creates a million by million matrix.
And even though we only have three non-zeros
and they're in three rows,
this thing here actually has to create an entry
for each and every column.
So it's actually one more than the,
I mean, it's not one more than NZVal
because it has an entry for each column.
It's actually one more than the number of columns.
So there's actually a million and one entry stored here,
which seems crazy, but that's exactly what it is.
Okay, enough with sparse matrices.
Let's go on to a different kind of structure.
So up until now, in this notebook,
I was kind of thinking about storage structures
where you can somehow save,
you don't have to store everything
because like the zeros were implicit, okay?
Now I would like to talk about a different kind of structure
that's not a storage structure.
It's the kind of structure that comes from randomness.
And we're right now in the, for this course,
we're in the transition from module one to module two,
where in module two,
we're going to be talking about statistics and probability.
And so this is kind of a perfect segue
between the two modules.
So let's talk about how much structure
there is in a random vector, right?
So here I'm going to create,
this is Julia's command to create,
and again, maybe I should add this,
but I think we've seen it before.
I'm going to create a vector with one million entries
and all the digits are from one to nine, okay?
So here's a bit of that vector V,
there's a more if,
here's a kind of one way of displaying it
if you want to see it,
but a million entries,
there's the top 20 and the bottom 10.
So yeah, there's a million numbers.
And the first thing you might say
when I ask how much structure
is there in a random vector,
you might say, there's no structure,
it's a random vector, right?
I mean, it's random, it has no structure.
I actually like to say
that randomness is itself a structure.
I don't tend to think of random objects
as nothing special objects
or objects with no structure,
they have a lot of structure.
And so much of science and algorithms
is based on taking advantage
of the structure of randomness.
The word random in common English
almost suggests that it's the opposite of structure,
but it's just not true.
So for example, we could take the mean
and the standard deviation
and some would say that there's a structure right there.
For example, I took the mean of this vector
and I'm gonna compare it with the number five
and you know, the four digits,
it's just about the number five,
you could take the standard deviation
and I happen to know that it's the square root of 23rds
and I can calculate it,
you could see to a couple of digits, it's right there.
So this random object, I could get a new one,
here, let's get another one.
I could do it many times.
There's something structured about this thing, right?
To three or four digits,
the main and the standard deviation are not changing, right?
So there's some structure right there.
I mean, sometimes statisticians
and maybe professors who've just graded exams
would say that the mean and the variance is the structure
and the rest is not even relevant.
So sometimes people will sort of write down
or note the mean and the standard deviation
and in some instances just throw out the rest
because that's perhaps all you really wanna know.
So in terms of-
You also might want to know how many fives there are
in the list.
Well, then I would have to not throw out my data.
Right, but you could reduce the data
by just counting the number of ones,
the number of twos, the number of threes, et cetera.
Yeah, you don't want me to do this, do you?
There's how many fives we're in this data set.
But I could also, can I go,
I have to have statistics to go, here's the-
I mean, you probably want lots, you probably don't want-
How about I do this?
This'll be more fun for i equals zero through nine,
as long as you're-
It goes from one to nine.
I'm done from one to, all right,
it's fine, it'll count to zero, let's do it.
Oh, that's why I said,
that's why I got like one ninth of a million, aha.
I was wondering why I didn't get like 10,000 or 100,000.
Okay, some of the is equal to i.
Okay, so here are the actual numbers.
So I remember when I was a kid,
they used to publish like the Powerball numbers
in the newspaper, whatever the lottery was.
Like people would, oh, this one seems
to have more, so we should go for that or something.
Or is this one, this one, maybe the biggest one.
Nope, I can't spot it, I'm sorry,
this one seems to be the biggest one.
Yeah, so maybe the number four comes up
a little more often, so maybe you should choose
the number four, of course, that's ridiculous.
But yes, this might be something you might store away,
for example, and then throw away the rest of the data.
Okay, so for those of you who don't know
what the mean is, but I bet you everybody does,
that's just the sum of the over the length of the,
and the standard deviation is maybe
a little bit less familiar.
What you do is you take your vector,
then you de-mean it, right?
And you subtract the mean from the vector,
and then you take the sum of the squares,
which kind of gives you sort of a distance
from the mean squared thing.
And then there's always that mysterious,
and then you don't take the average by length,
but you subtract one, and people owe that
to the degrees of freedom.
I always hated that explanation,
but that is what people do, and that is the variance,
and then to get the standard deviation,
you take the square root.
And by the way, that's what really does,
when you take it to VV, you get the exact same number,
it's exactly this formula to get the standard deviation.
Okay, so yeah, sometimes the summary statistics
are all you want, but sometimes not.
All right, let me go to another kind of structure,
multiplication tables, okay?
So I'm gonna define an outer product function
of two vectors, and all I'm doing is I'm taking
all the possible ways of multiplying an element of V
with an element of W.
For example, if I'm taking outer of one through 10,
one through 10, I would get what would be
an ordinary multiplication table.
And I think I've kind of added it to a slider,
which is more fun.
So here's the 10 by 10 multiplication table.
Last time when I did this last semester,
David told me that in England,
everybody learns the 12 by 12 multiplication table, right?
So, but nonetheless, here's the multiplication table.
You can let us know what you,
in my school, it was 10 by 10,
I can go to as good as school as David went to.
But you rapidly forget it,
as soon as you learn it.
Oh, I know that my 12 by 12 multiplication table,
it just wasn't taught in my school.
I've never forgotten this.
I should get what happened yesterday,
but my 12 by 12 multiplication table, I got done.
That's no problem.
So, but just to look at a few more outer products,
instead of going through one through 10, for example,
you can take two, four, six, and 10 hundred thousand,
and you can see all the possible ways
of multiplying a number here with a number here, okay?
And so this is, a multiplication table
is a kind of structure,
but it's not sparse, I mean, there are no zeros.
It's not a diagonal matrix,
but it's clear that you don't need
M squared numbers to represent this object, right?
It's got structure, but it's not a sparse structure.
The interesting thing is that
in many, many applications of matrices,
there is structure, but it's not sparse structure
that turns out to be truly important.
It's sometimes a slightly more hidden structure,
but here you can see that the structure
is that of a multiplication table, right?
Sometimes it's not obvious that it is a multiplication table.
For example, here is a bunch of numbers made up randomly,
and I don't know, maybe you're better than me,
but I look at the three by four matrix,
and I'm not sure I would recognize
that it is a multiplication table,
that it comes from every product of one vector and another.
I mean, maybe you could look at the magnitudes
and start to guess, but it's not so easy,
just not by the M and I.
Here's a sort of picture of a 10 by 10 version,
and maybe it's a little bit more obvious with the picture
because multiplication tables
sometimes tend to have this sort of sparse structure.
So you might guess that it's a multiplication table,
but what we're gonna do is actually factor out
the multiplication table if it's there.
And so here's a little code that will factor it out,
and more or less doing the obvious,
which is a multiplication table can be obtained
by taking the first row and the first column,
and then kind of dealing with the one-one elements.
And so here I'm extracting the first row,
here I'm extracting the first column,
and I'm dividing the first row by the first element
as long as I can, and then I'm just,
and now I wanna see, is it really a multiplication table?
And so if the outer VW is about the same thing,
I'll say it did, otherwise I'm going to call an error,
and so here's where exceptions are thrown in Julia.
This is a way to kind of talk about exceptions.
If this doesn't work out,
we'll say the input is not a multiplication table, okay?
So you could see I can factor, for example,
one, two, three times two, two, two,
but when I do that, I may not get my inputs back,
and equally valid V and W would be two, four, six,
and one, one, one, right?
Because every time I've doubled this one
and I have this one, which amounts to the same thing,
but there's this entire,
there's always sort of like an infinite number of things
that could have worked out,
and it doesn't really matter which one.
But look, if I try to factor a random two
by two, the error is working like I want.
Now, I know when you see this black in these lines,
one, two, three, you think, oh my God, I have a bug,
but no, I actually caused this to happen.
This was deliberate, right?
I am calling it with a random matrix,
and it is not a multiplication table, right?
Most matrices are not multiplication tables, okay?
However, yeah, so the question is,
can we find the structure?
Can we find multiplication tables?
Is there a way to do it?
And the truth is we could do even better than that.
If we have something that's, for example,
the sum of two multiplication tables,
there is a technique for finding both of them, actually,
to find two outer products.
And the magic thing that does that
is the very famous similar value decomposition,
which you may see in another course,
or you may see it in an upcoming course,
you might have seen it before,
but it is probably the single most used linear algebra thing.
It's one of the most,
certainly being used in statistics these days.
There are some people who would say that
starting linear algebra classes with Gauss elimination
is no longer the right thing to do
because the SVD is the big thing these days.
So the similar value because it actually finds the structure.
So before I even tell you what it is,
let's just do it and just see what happens.
So here, I'm gonna take this matrix, okay?
And I'm going to, there are a lot of times
I actually run a function
before I even know what the name means.
I'm doing that with you.
Maybe that's a kind of computational thinking,
which is just play with something, right?
So I'm gonna play with the SVD.
And I guess you could see how it's called.
You could see that it has three output arguments.
I assume things in front of me, so I can quite see this,
but you could see an example somewhere here,
like here, you know, US comma V is usually three output
to the SVD, so you could see that over there.
So I'm gonna use a common notation,
which is U, sigma, and V, okay?
And I'm going to check that I found two outer products.
So here, look, outer of this plus outer of this,
and this should recover my original matrix,
which is A, you see?
So this magic thing is actually extracting
the multiplication tables
that are sitting inside of the matrix, okay?
And the cool thing is,
is it could be done approximately as well.
I'm gonna skip this just for a moment,
because I think Dave's planning to talk about flags,
but let me show you what happens.
You know, you know, mean by now, I like to take SVDs.
I like whatever you can do with matrices.
I like to do images.
So I think it would be fun to do this with an image.
So here's an image of a tree,
and what I'm gonna do, first of all,
is separate out the red, green, and blue components, okay?
And then what I'm gonna do
is separately add the multiplication tables.
And so when I go to the end,
I've got sort of the full tree.
When I do one, I've got the one multiplication table.
In linear algebra language,
it's called the rank one matrix, right?
So the red and the green and the blue
are separately multiplication tables,
and it comes out plaid, like you saw before.
You can have the rank two tree,
and this looks sort of Minecraft-like, right?
I mean, everything's sort of rectangular, I guess, right?
So if you've ever played Minecraft,
I think this is what my kids played all the time.
I see it on your screen.
This is what it looks like, okay?
When you get to the sum of three multiplication tables,
it's officially called rank three.
You start to get a couple of more boxes,
and then, you know, as you start adding rank,
you can see that, you know,
we start to get a better and better version
of the tree with some artifacts, right?
We start to disappear as you add more and more rank, okay?
So this is sort of meant to be the quick introduction
to the SVD.
There's still much more that can be said about it,
but right now, for the purposes of this lecture,
I want you to understand that the singular value
decomposition is a way of breaking up a matrix
into a sum of multiplication tables,
and by a multiplication table,
I just mean outer products, right?
And the SVD does it in such a way that if you only have rank one,
you've got the best possible rank way of doing it.
If you have rank four,
it's the sum of the best possible,
by some sense, of multiplication tables.
So I'd like to leave it at that for now.
Like I said, there's so much more, but...
I think you can just talk about flags, I think.
You want me to talk about flags, all right?
I will do the flags then, all right?
Let's go back up to the flags and talk about flags.
So where we left the story is here.
So let's see, where was I?
Well, all right, Dan, whatever.
Yeah, I think, yeah, why don't you go ahead, Dave?
I'm gonna turn this over to you.
And for those of you who joined late,
let me again remind you that if you're watching us on Monday,
you might want to keep track, depending on where you are,
that we are going to be pushing the clock one hour forward
in most of the United States on Sunday morning.
But Europe is doing it two weeks later
and other places may not do it at all.
Most of Europe, not all of Europe.
Okay, Dave, you want to take over, take over the screen?
Yeah.
I'll tell you, hold on.
Sorry.
Okay, can you see my screen?
We do.
Hi, everybody.
So, yeah, we're carrying on the same kind of topic.
As Alan said, we're transitioning into the second module
of the course on data and probability and statistics.
And so let's look at this very nice subject
called principal component analysis,
which is the name from statistics,
which is basically the same as the SVD
that Alan was just talking about in the algebra.
So what we want to do is understand data.
If we're given some data,
we've been thinking of images as our input data,
but of course there's lots of other kinds of data
in the world that are often in just sort of big matrices
full of numbers.
And we want to somehow extract information
from those matrices.
And that's the goal of this method
of principal component analysis
that we'll talk about in detail next time.
But let's start off by thinking again
about these multiplication tables
that we've just been looking at all out of product.
So here's this table of, again,
of just the same algebra that Alan was just talking about.
So here's a multiplication table.
It doesn't not have to be square.
You could multiply numbers from one to 10
by the numbers from one to 12.
And then we get this dense matrix,
but it has a lot of structure in the sense that
the whole matrix can be reproduced
from just these two vectors.
So instead of 120 numbers, I just need 10 numbers
plus 12 numbers and the information,
the extra information that this matrix is created
by doing this particular operation.
So if I have those pieces of information,
then I can reconstruct the whole matrix
and I don't have to store the actual matrix.
And so you could just store those two vectors
and do something very analogous to what Alan did
with the one-hot vector and make a new type
where you defined get index to extract the six,
nine component by, it would actually, in the moment,
multiply those numbers together and return that value.
So exercise for the reader, implement that, you know,
outer product or what our multiplication table type
that does exactly that.
So the point is that each column,
if we think about what is the structure of this,
what does this matrix look like?
What is a an outer product or a multiplication table?
It means that each column is just a multiple
of any of the other columns.
So in this case, you know, this eight column
is a multiple of the six column, which multiple is it?
It's h divided by six times this column.
So that's, you know, 1.26 or something times this column.
And each column is actually a multiple of any other column
and each row is a multiple of any other row.
So it has this sort of very nice structure,
but as Alan said, it's not obvious just by staring
at the table, but that's the case.
And so we want a way to actually realize that that's the case.
And flags, flags as in flags of countries
provide a pretty visual example of this.
So here's a flag that, you know, corresponds not
with those colors, but that kind of shape corresponds
to various different countries.
Flags, I'm not very good with flags,
but I don't actually remember which countries go which way
around, but you know, you could either have these
horizontal stripes or vertical stripes.
And if you think of those as numbers, like this matrix,
this table of data here, you see that, oh yeah,
that is a rank one matrix.
That's what we're going to call matrices,
which are just outer products,
which are given by exactly an outer product, right?
So what is this outer product is,
it's just given by the vector 1.1 and 2.0
on the vertical, in the vertical direction.
And then in the horizontal direction,
in this particular case, I'm just multiplying,
you know, each column is exactly the same.
And so I get exactly just repeating the same pattern
in each column.
And so I get these three colored stripes
that are exactly the same all the way along.
But a more general rank one matrix for outer product
would have different values, right?
So I'm actually multiply, I can multiply, as I said,
each column by different amounts.
And then if I draw that, I get a different kind of flag
that I don't think corresponds to any country,
which is this particular thing.
So a more general outer product looks like this,
and you have to get this blocky structure
that Alan was talking about.
So let's just generate some bigger matrices
with this outer product or rank one shape or structure,
and look at what they look like.
So they look something like this.
So you see that we get this kind of nice,
you know, you could probably sell this
in a modern art gallery.
But it has this very particular structure
with these columns of color, and then there's rows of color,
and there's some checkerboard shape.
And that's what a rank one matrix looks like.
So, you know, if someone gives you a picture like that,
you could sort of try and guess, oh yes,
that is a rank one matrix.
In other words, I could write it
just using one single column and one single row
and multiplying them in this way.
But now what about around, what if we start
doing this reconstruction that Alan just mentioned,
and we start, and we now look at a rank two matrix.
So that means we take one of these checkerboards,
one of these outer products,
and we add another one to it, and that looks like this.
So you can see that, well, we're starting to lose
some of the checkerboard structure,
but it's still sort of there, right?
So if you sort of see checkerboard structure,
you might imagine that it's just a sum of two outer products
or maybe three.
And, you know, if I run this cell again,
we can see a different version of that picture.
And each time we run it, we'll just get different pictures.
The colors correspond to the values, you know,
I guess yellow and red are high values of close to one
and blue is close to zero.
And, yeah.
And that also depends on how, of course,
on how much randomness you have.
And so it starts to look less regular,
but it still sort of looks,
has basically the same structure.
And so you could visually guess, oh, you know,
maybe I can represent this matrix
with less information than the whole matrix.
And so, you know, if you needed to transmit this data
to somebody who wants to send an image over the internet,
can I actually reduce?
So part of the question is,
can I reduce the amount of information I need
to send to somebody, right?
And then I can have better video
because more people can send video over the internet
at the same time because I'm actually being able
to compress the amount of data I need to send.
That's sort of in the background of all of these questions.
Okay, so now what about,
so there I took two rank one matrices
and I added them together.
So again, rank one,
the rank is how many multiplication tables
or outer products do I need to add
to exactly reconstruct the matrix?
That's called the rank.
So now let's do something slightly different.
So now let's take a rank one matrix.
Here it is, the same one.
And we're going to add random noise
to each element separately.
That's what this is doing.
This random n function means a random number
with a Gaussian distribution or normal distribution.
So it's centered at zero.
It has standard deviation one
and it has a Gaussian shape.
And we're sampling noise from that distribution.
So we're choosing noise in such a way
that the probability that we choose the value
is given by this Gaussian distribution.
And we're adding that separately
to each pixel in this image.
And what do we get?
We get something that looks like this.
It's commented on the YouTube channel.
And I have to agree that these pictures
kind of seem reminiscent of pictures of CPU,
the chips pictures that you see.
That's true, yeah.
Yeah, but because somehow CPUs are made
in a square grid, right?
Yeah.
So if I add more noise,
if I add a higher intensity of noise,
then the image starts to degrade
and you get, it looks literally more noisy.
And, but it still kind of has this structure.
So again, you would sort of like to say,
well, this new matrix is close
to this structured matrix,
even though this one actually,
if you try to use the SVD
to exactly reconstruct this image,
you will need to have to sum a very large number
of outer products, which is equal to the dimension
of the size of the matrix.
In other words, the smallest one of these.
What would you call that, Alan?
The minimum of the width and the height of the matrix.
I guess so.
Yeah, so, but, you know,
it still looks like the original one.
And you would really like to say,
oh, well, it's basically just this one.
And so the question is, how can we actually do that?
How can we decide that this matrix
is just a noisy version of this,
in other words, is close to this one?
So we need some notion of how far two matrices are,
one from the other.
And then we need to be able to say,
oh, yes, you know, the rest is just noise.
And so that's the goal of PCA,
which is principal component analysis.
And so what we're gonna do is think of these images.
So so far we've been thinking of images
as visual objects that we like to look at.
But now we're going to think of an image as just a matrix
and the matrix represents data.
And really what we're thinking of is,
oh, actually I'm getting some data in a table from somewhere,
in which we'll call a data matrix.
Okay, so let's just take two rows of the image
and think of that as some data.
So this is my original image,
which I created as a rank one matrix.
So as an outer product,
by, you know, again, taking a multiplication table
of this vector and this vector.
That's how I created this image.
And then we'll also take the noisy version of that
that I just made.
And I'm gonna put the noise back down,
the magnitude of the noise back down.
So it's just a bit noisy.
Okay, and what we're gonna do is extract the first row
of this image as a vector of data
and the second row as another vector.
So here, you know, this is one of the rows.
It's just literally a vector of floating point numbers
between zero and one.
And we're gonna think of that as the data that's coming in
that represents, you know, some house prices in Boston.
That's a famous data set or, you know,
some medical data on patients, lifespans, et cetera.
Any kind of-
The house for 52 cents?
Yeah, so there's some normalization going on.
So you actually have to multiply these numbers
by one, two or $3 million.
And then you'll get the correct house prices.
So what are we gonna do?
We're actually going to view this data
in a completely different way.
We're going to, you know, move to more
a kind of scientific point of view, maybe.
We're actually gonna plot the data.
So what is the representation here?
So on the x-axis, I'm plotting, you know,
I'm plotting points at positions x comma y
where I literally take the x value as being the first,
you know, coordinate in this image, in this data matrix.
So these are the x values in the first row
and the y corresponding y values in the second row.
So the columns, I don't know how to highlight a column.
There is some way I looked up the other day, but anyway,
the first column is a pair that I'll represent, you know,
as that x comma y coordinate on my plot.
And what I get is this.
So what are we looking at?
The original rank one image is exactly these red squares
and they lie exactly along a straight line.
So what does that mean?
It means that in the original image,
each y coordinate is some number
times the corresponding x coordinate.
And that number is the same for all of the points.
Why is that?
Because it's an algebra product.
And so, you know, there is this exact relation
that each column is a multiple of the other column.
And so when I divide the y and x in each row,
in each, sorry, in each column,
in a given column, when I divide the y value
by the x value in that column,
I always get the same result.
And so they lie on a straight line,
y equals that number times x.
And then the noisy image,
I'm adding noise to basically the x and y coordinates.
And that spreads out slightly
into this blue set of data.
Right, so basically what I'm trying to say is
that the data that's coming in from my data source,
it probably looks like this blue data.
And if I were just to see that blue data,
I would see, oh, that seems to be clustering
around a straight line.
And so actually what I would like to do is extract
what straight line it seems to be clustering around
and quantify what is the width
or the sort of spread around that straight line.
And say, oh, that is actually small.
That width is small compared to the length of the data
in the other direction.
And therefore I would like to conclude that,
oh yes, maybe this data is basically
just a straight line plus noise.
And so that's the idea of,
the idea behind principle component analysis.
And we'll see a computational thinking way
to approach that algorithm in the next lecture.
Great, thanks Dave.
So I think again, we'll say goodbye
to the folks on the internet.
And then we'll keep the zoom up in a little longer
to see if anybody wants to chat.
So see everybody on Monday, 1 p.m. Eastern daylight time.
Dave.

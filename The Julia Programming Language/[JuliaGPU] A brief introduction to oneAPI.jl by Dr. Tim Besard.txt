Okay, Tim, the stage is all yours.
It's even a live stream.
All right.
Yes.
So welcome everybody.
Let me share my screen instead.
Can you see my screen?
Valentine confirmation.
Yes, we can.
All right.
So welcome everybody.
I'm Tim.
I work at Julia computing and today I'm going to give a brief overview of the one API package, which is a package that gives Julia support for the one API programming model you may or may not have heard of.
So maybe first a small introduction to one API.
One API is supposed to be a unified programming model for accelerator hardware.
It's a fairly new programming toolkit.
It's open and standard based.
It's basically designed by Intel and currently Intel is one of the main contributors of one API implementation.
So they have an implementation for Intel CPUs for Intel GPUs and for their FPGA stay recently acquired.
Code play is also working on an implementation of one API for Nvidia GPUs.
But sadly, as I might explain later for technical reasons, we cannot be using this implementation.
So we also only be paying attention to the Intel implementations for CPUs and GPUs since I don't have an FPGA.
So how is one API design?
So similarly to other programming toolkit for accelerators, you write your optimized application.
That application can either do what they call here direct programming, which is writing your own kernels.
In this case and data parallel C++ or you could be using or it could be combination of both API based programming where you use pre-compiled libraries such as Intel's Mkl,
which is their blast counterpart or Intel TBB treading building blocks as a way to use accelerators.
So TP C++ it's a data parallel C++ dialect, which is based on Chronos SQL.
And this is an example function that uses a kernel to implement a fairly typical vector addition of two integers arrays into a third one.
And as you can see here at the bottom we have this parallel foreign kernel function and because it's C++ you have to do quite some boilerplate to set up the buffers to shuffle the memory to the GPU.
And in OpenCL like style you even have to manage your own command queue.
So what we want to do is we want to replace the DPC++ part in there with Julia.
It's possible to write kernels in the language you're familiar with.
So let's start by how to install the package that makes this all possible, the one API package, it's part of the Julia GPU organization.
And this is just a fairly typical installation log, but there's something interesting here already, namely that we're downloading some artifacts, Neo, which is the Intel implementation of the one API spec and the IGC, which is their graphics compiler.
So basically we're using artifacts to provision you with all of the binaries that you need. This is in sharp contrast to, for example, CUDA where you require you or your system administrator to install the NVIDIA libraries and the NVIDIA driver.
In the case of one API, everything happens in user space, which is a very neat way to do so.
So currently one API has the version 1.0 that's supported on Julia 1.5 and the current master branch is supported by Julia 1.6.
And currently all of this is only supported by 64-bit Linux distributions, but we hope to be extending this in the future to when the support, when the Intel support for that arrives.
Of course, your hardware also has to be supported, but that should be okay because as you can see in this table, which is from the Intel compute runtime, which is the Neo implementation of one API, you can see that all fairly recent core processors just support level zero, the ones that do not are typically
Intel processors. So if you have a fairly recent Intel processor with an IGP, meaning no ZNs or other enterprise parts, then you typically will just support the one API to shell package.
And you can verify that by after installing one API, trying to import it, it will eagerly error if it's not supported, then you can dump the version info that gives you some details about the driver that's installed and the device that's available.
For that setup, we can create a vector addition function similar as we would in, for example, CUDA.gl, where we have a kernel function that gets its global ID, performs the addition on device arrays.
Back on the CPU, we upload the CPU memory to the GPU and we spawn a kernel with the one API macro.
So all of that is very, very similar by design to CUDA.gl and other GPU packages that we have in Julia. Of course, as a result, we introduce some differences with how DPC++ or CICL worth.
For example, we use OpenCL intrinsics to get global ID one instead of CICL specific ones. And we also have global semantics. So instead of having to pass your own queue device context, we manage that for you globally and you can again globally, or more specifically in a
local context, switch from one device or to another, or for one queue to another. Basically, because all of that facilitates how you can use the package.
I'm similarly to CUDA.gl, you can introspect the generated code and here you can see the generated a lot of VM code. Or we can have a look at the device specific code in this case, per V.
And this is worth mentioning because per V by itself is an intermediate representation that's used by a variety of APIs and projects.
Even more so if you work, for example, to use per V cross, which is a tool that allows you to cross compiles per V to, for example, metal shaders or GLSL shaders. So this might empower some other developments in the Julia
world to target some other specific APIs or accelerators. And to make that easier, we've moved all of the compiler specific code from CUDA.gl and from some other device specific packages into the generic GPU compiler.gl, which is a
package that people who want to experiment with this can use. For example, the ship's a per V compiler and you can use it to compile Julia code to per V and then building on top of that you built your own library interactions or some other device specific stuff, but all of the
generic stuff is in GPU compiler.gl. And of course, there's always still GPU arrays.gl, which implement vendor neutral implementations of array operations.
So in making of these array operations, we can simplify our vector addition from this kernel implementation to just a simple broadcast. And this is some of the functionality that's provided by GPU arrays or GL.
So we have the broadcast implementation, we have also map, of course, map reduce and many of its derivative functions like count and all it's all provided by GPU arrays.
We have some limited linear algebra functionalities such as matrix multiplication, transpose, permutants, all of that is very similar to the functionality that CUDA.gl already already offers.
Of course, that also means that it shares some of the limitations that CUDA.gl and the other GPU packages and Julia have. For example, you need to take care about scalar annexing again.
So annexing is when you have code that for example in a for loop performs a computation element wise element by element on the CPU, which is terribly slow if you were expecting to use a GPU.
So typically you have to switch the allow scalar flag to false and then when you perform this kind of slow indexing, the package will warn you about it.
Case in point, for example, if you use multiple wrappers, that's something that we currently do not support well. And as a result, we often in this case, when you do a broadcast of a view of a transpose, this will not dispatch to the GPU specific
implementation, but to the very slow CPU fallback.
And if you've disabled scalar iteration, then you'll get this quote unquote nice error message warning you about the slow code.
Also some similar limitations pertaining to kernel programming.
Of course, the functions have to be fully statically compilable. You cannot have any type instabilities or untyped globals for certain math functions, which are incompatible with GPU you have to prefix them by the one API module to use a device specific version.
Now, some of these we hope to fix in the future but for now this is what you have to do.
And one API.gl also do not support a so-called device side alloc function. And as a result, we do not support some function that expect their arguments to be boxed.
And this is fairly technical, but it happens in the case of error messages. For example, in this case, when you want to reinterpret a 32 bit floating point value to a integer value you'll get this invalid IR error because we currently do not have
any implementation of a GPU site malloc.
Another big thing that we're currently missing is an implementation of these API based programming libraries, we do not have the necessary wrappers for Intel's MkL or Intel's threading building blocks specific to one API.
Issue that will mainly mean that if, for example, you do a matrix multiplication, you'll get degraded performance because it falls back to the generic implementation from GPU arrays.gl.
But it's also not terribly difficult to fix. We just need to provide a bunch of wrappers for these libraries and make sure they integrate nicely with the Julia standard libraries and the interfaces, for example, the random library or the linear algebra library.
So all of that is fairly mechanical and that's one of the elements of one API where the community might be able to help to put this package across from the prototyping phase into something that can be really used for production code.
So we need to provide wrappers for the one API libraries I mentioned, which is fairly mechanical.
Another useful contribution might be to try and port some more functionality from CUDA.gl currently device specific to CUDA to GPU racial gel such that other packages like one API but also AMD GPU might benefit from it.
And a good way to flush those out would be to try real applications that do work on CUDA.gl but might have not to work on one API.gl.
And finally, another to do is a decent performance analysis where ideally some similar codes would be compared against Julia and TPC++ to see if we're leaving any performance on the table.
And this is likely because I haven't done a good scan of the performance and as a result, for example, many of the array operations like the map reduction that we currently use is pretty naive and might be, yeah, it might be possible to optimize them by quite a bit.
Anyway, this was a brief overview of the package. If there's any questions, I'll be happy to take them to the community call after all.
Thank you, Tim.
Are there any questions right now in the audience?
If not, then I will terminate the live stream and we can

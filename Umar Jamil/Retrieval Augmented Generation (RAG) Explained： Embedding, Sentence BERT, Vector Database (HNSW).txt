Hello guys, welcome back to my channel. Today we are going to talk about retrieval augmented generation
So as you know, large language models can only answer questions or generate text based on their training data
So for example, if we have a language model that was trained in 2018 and we ask it about the COVID
Probably the language model will not know anything about the COVID
And one of the way to augment the knowledge of language model is to fine-tune the model on the latest data
But there is another technique called retrieval augmented generation
Which is very useful especially for question answering and it has been also been used recently by Twitter to create their new language model called
GROC. So GROC can access in real time all the data from the tweets and answer questions on the latest trends
So in this video, we will explore what is retrieval augmented generation, all the pipeline, all the pieces of the pipeline and the
architecture behind each of these building blocks
So let's review the topics of today. I will give a brief introduction to language models
So what they are, how they work, how we inference them
We will then move on to the pipeline that makes up the retrieval augmented generation with a particular reference to the embedding vectors
And how they are built, what they are and then
We will also explore the architecture behind sentence birth
Which is one of the ways to generate embeddings of sentences and then we will move on to vector databases
So what they are, how we use them and how the algorithm of a vector database works in finding a particular vector
That we are looking similar to a query
What I expect you guys to already know before watching this video is for sure you are a little familiar with the transformer
We will not be going so much in detail
But at least you are familiar with the basic building blocks of a transformer and that you have watched my previous video about birth
So if you're not familiar with these topics, please
I recommend you watch my previous video on the transformer and my previous video on birth
They will give you all the necessary background to fully understand the current video
Now let's start our journey
We will start by introducing first of all my cat Oleo
Because I will be using him for a lot of the examples in my video
So if you are not Chinese you don't know how to read his name, which is Oleo
Which stands for the biscuits Oreo because he's black and white
So let's get started. The first thing we will be talking about is language models. Now. What is a language model?
Well, a language model is a probabilistic model that assigns probabilities to sequence of words in practice a language models
Allows us to compute the following
What is the probability that the word China comes after in the sentence?
Shanghai is a city in so the language model allow us to
model the probability of the next token given the prompt and
We usually train a neural network to predict these probabilities
Neural network that has been trained on a very large corporal of text is known as a large language model
When we train large language models
So we usually have a very big corporal of text that is made of which is a kind of a collection of documents
Now often language models are trained on the entire Wikipedia or millions of web pages or even thousands of books
Because we wanted the language model to acquire as much knowledge as possible and we usually use a transformer based
Architecture to model to create a language model
for example
Lama is usually known as the coder only network and the bird is usually known as an encoder only network
Because Lama has the last part of Lama is the basically the coder of the
Transformer without the cross-attention plus a linear layer plus a softmax while bird using only the encoder side
And then it has some heads that can be a linear layer
Depending on the task. It is we are using it for
To inference the language model
We usually build a prompt and then we ask the language model to continue this prompt with by
iteratively adding tokens such that the tokens that the language model adds make sense in
In continuity with the prompt so for example here
I'm asking chat gpt to come to to continue a joke and chat gpt continues the joke by adding few more tokens that went red
In their entirety they make sense. They are coherent with what I actually wrote
You are what you eat so a language model can only output text and information
It was trained upon this means that if we train a language model only on English content
Very probably it will not be able to output Japanese or French to teach new
Concepts to new content new information to a language model. We need to fine-tune the model
However fine-tuning fine-tuning is has some cons
For example, it can be expensive in term of computation power is necessary to fine-tune
The number of parameters of the model may not be sufficient to capture all the knowledge that we want to teach the the model itself
So for example, Lama was introduced with the seven billion thirteen billion and the seventy billion parameters
Why because with seven billion parameters you it can capture
Some some knowledge, but not as much as the seventy billion parameters model
So the number of parameters is a limitation on the amount of knowledge the language model can acquire
Also fine-tuning is not additive
For example, if you have a model that has trained on English content and then you heavily fine-tune it on Japanese content
The model will not be at the end of the fine-tuning will not be as
Proficient in English and as in Japanese, but it may forget some of the English content
It was initially trained upon so is the way we say that the fine-tuning is not additive to the knowledge of the language model
Of course, we can compensate for the
For the fine-tuning with the prompt engineering for example
It is possible to ask the language model to perform new task task that it was not specifically trained upon by working with the prompt
For example, this is a few short prompting technique and the following is an example. So
We first give an instruction instructions to the language model here is the instruction on
What is the task the language model is going to perform?
Then we give the language model some example to how to perform this task and then we ask them language model to perform it
For example, the task here is only or is a cat that likes to play tricks on his friend
Umar by replacing all the names in everything he writes with meow
For example, umar writes Bob runs a YouTube channel and only I will modify it to meow runs a YouTube channel
So what happens if umar writes Alice likes to play with his friend Bob?
How would allio modify it the language model comes up with the right answer, which is meow likes to play with his friend meow
So judge gbt in this case was not
trained on performing this task
But by looking at the prompt and the example that we provided it it was able to come up with the right solution and
We can use prompt engineering also for question answering with the same kind of reasoning
So we build a very big prompt that includes an instruction part
So you are an assistant trained to answer questions using the given context
In which we provide some context
Which is a piece of text in which to retrieve the answer and then we ask the question to the language model
How many parameters are there in grok zero?
So grok is the language model that is introduced by twitter and it's a language model that
Can also access the the latest tweets and we will see later. How?
but
The point is I am asking the language model. So chadgbt to tell me about grok zero
So very probably chadgbt was not trained on the
Doesn't know about the existence of this grok zero because it came out very recently
So the model chadgbt was able to retrieve the answer by looking at the context
So in this case for example, he says grok zero the prototype LLM mentioned in the provided context
It is stated to have been trained with the 33 billion parameters because the
Chadgbt was able to access the context in which we talk about the how many parameters there are in grok zero
Which is this line after announcing xai we prototype we train the prototype LLM with 33 billion parameters
so the answer is correct and
This
This kind of way of working with the prompt is actually very powerful
but also
fine-tuning is not necessarily wrong or the wrong way to deal with this
lack of knowledge
Problem in the language models because it's usually when we fine-tune a model on a specific particular content
The it results in a higher quality results compared to just prompt engineering and also as you saw before
To ask a question to chadgbt we had to build a very big prompt
So it means that the number of tokens that we are giving to the model is quite big
The more we have the bigger the context that we need to provide to answer a particular question
And we know that the more context we give the more information the language model will have to come up with the right answer
So usually we need a bigger context
But the problem is a bigger context is also computationally expensive
So by fine-tuning actually we can reduce this content size because we don't need to provide the context anymore because we are fine-tuning the
Language model on the specific data on which we will ask questions for so to a language model that has been fine-tuned
We just need to ask the question
So how many parameters are there in croc zero without providing all the context and if the language model has been fine-tuned correctly
It will be able to come up with the right answer
Now we need to introduce the retrieval augmented
Generation pipeline
Because that's our next step and we will explore each building block that makes up the pipeline
So imagine that we want to do question answering with the retrieval augmented generation
Compared to what we did before before we did it with prompt engineering
so
how many parameters are there in grog zero this is our query and
Imagine we also have some documents in which we can find this answer
These documents may be some PDF documents, but they may also be web pages from which we can retrieve this answer
What we do is we split all these documents or pieces of text into chunks of text
So small pieces of text for example a document may be made up of many pages and each page may be made up of paragraphs and each
Paragraph is made up of sentences in the usually we split each of these documents into small sentences
And also the web pages are split in the same way
We create embeddings of these
Sentences such that each embedding is a vector of a fixed size that captures the meaning of each sentence
Then we store all of these embeddings into a vector database and later we will see all these embeddings and vector database how they work
Then we take also the query which is a sentence
We convert it into an embedding using the same model that we have used to convert the documents into embeddings
We search this embedding so this query embedding into our database
Which already have many embeddings each representing a sentence from our document and it will come up with some results
with the best matching embeddings for our particular query and each embedding is also
Associated with the piece of text
It comes from so the vector database is also able to retrieve the original text from which that embedding was created
So if we are for example, how many parameters are there in grog zero the vector database
Will search all of its embedding and will give us the embeddings that best match our query
So probably it will look for all the piece of text that talk about grog zero and the parameters it contains
Now that we have the context and the query we create a template for a prompt
So just like the the prompt we have used before so you are an assistant trained to answer questions using the given context
We paste the context and the query inside of the prompt template
And just like before we feed it to the language model
And then the language model will be able to answer our question by using the context provided
So we are with retrieval augmented generation
We are not fine-tuning a model to answer the questions
We are actually using a prompt engineering
But we are introducing a database here called a vector database that can access the
Context given our query so it can retrieve the context necessary to answer our particular question
Feed it to the language model and then the language model using the context and our question will be able to come up with the right answer
very probably
Now let's talk about embedding vectors what they are and how we work
Okay, first of all, why do we use vectors to represent words?
For example, given the words cherry digital and information if we represent embedding vectors using only two dimensions
So as you remember in the vanilla transformer each embedding vector is
512 dimensions, but imagine we are in a simpler world. We only have two dimensions
So we can plot these embedding vectors on a xy plane and what we do is we hope to see something like this
So that words with the similar meaning or words that represent the same concept
Point in the same direction in space
So for example the word digital and the word information are pointing to the same direction in space
Such that the angle between words that have a similar meaning is small
So the angle between digital and information is small and the angle between
Words that have a different meaning is bigger
So for example the word cherry and digital have an angle that is bigger compared to digital and information
Indicating that they represent different concepts
Imagine we have another word called tomato. We expect it to point to this vertical direction here
Such that the angle between cherry and tomato should be small
How do we measure this angle?
We usually use the cosine similarity to measure the angle between vectors and later
We will see the formula of the cosine similarity
Now, how did we come up with the idea of representing words as embeddings?
The first idea is that words that are synonyms
Tend to occur in the same context so surrounded by the same words
For example, the word teacher and the professor usually occur by the word school
University exam lecture course, etc. And vice versa
We can also say that words that occur in the same context that tend to have similar meaning
This is known as the distributional hypothesis
This means that to capture the meaning of a word
We also need to have access to its context so to the words surrounding it
But this also means that this is also the reason why we employ
Self-attention in the transformer model to capture the conceptual information of each token
So as you remember the transformer model, we have this self-attention
The self-attention is a way to relate each token with all the other token in the same sentence
Based also on the position each token occupies in the sentence because we have the concept of positional encoding
So the self-attention accesses two things to calculate its score of attention
The first is the embedding of the word which captures its meaning
The second information it access is the positional encoding
So that the words that are closer to each other are related differently to words that are far from each other
And this self-attention mechanism modifies the embedding of each word in such a way that it also captures the
contextual information of that word and the words that surround it
We trained BERT on a very particular task which is called the musket language model task to capture
information to create the embeddings of BERT
This musket language model task is based on the Claude's task and we human do it very often
Let me give you an example
Imagine I give you the following sentence
Rome is the something something of Italy
This is why which is why it hosts many government buildings. Can you tell me what is the missing word?
Well, of course the missing word is capital because by looking at the rest of the sentence is the one that makes the most sense
How did we come up with the word capital for the missing word?
Well, we look at the words that were surrounding the blank space
so it means that to the the word capital depends on the
context in which it appears on the words that surround it and
This is how we train BERT
We want the self-attention mechanism to relate all the input tokens with each other so that BERT has enough
information about the context of the missing word to predict it
For example, imagine we want to train BERT on the musket language model task
And we create an input with the musket word just like before so Rome is the something something of Italy
Which is why it hosts many government buildings. We replace the blank space with a special token called musk
This becomes the input of BERT which is made of 14 tokens
We feed it to BERT
BERT is a transformer model
So it will output it's a sequence-to-sequence model
So if the input is 14 tokens the output will also be 14 tokens
We ask BERT to predict the fourth token because it's the one that has been musket out
We know what is the word which the word is capital
So we ask we calculate the loss based on what is the predicted fourth token and what it should be the actual fourth
token and then we back propagate the the loss or to update all the weights of the model when we run back propagation
BERT
The model will also update the input embeddings here
So the input embeddings by the back propagation mechanism will be modified in such a way that
This word so the word the capital so the embedding associated with the word the capital will be modified in such a way that it captures all the
information about its context
So the next time BERT will have less troubles predicting it given its context and this is
Actually the one of the reason why we run back propagation because we want the model to get better and better by reducing the loss
What if I told you that actually we can also create embeddings not of single tokens, but also of entire sentences
So we can use the self-attention mechanism to capture also the meaning of an entire sentence
What we can do is we can use a pre-trained BERT model to produce embeddings of entire sentences. Let's see how
Well, suppose we have a simple input sentence
For example, this one made of 13 tokens called our professor always gives us lots of assignments to do in the weekend
We feed it to BERT, but notice that I removed the linear layer from BERT
And I will show you why
So the first thing we do is we notice is that the input of the self-attention is a matrix of shape 13 by
768 why because we have 13 tokens and each token is represented by an embedding with
760 dimensions which is the dimension which is the size of the embedding vector in BERT base
So the smaller BERT pre-trained model
The self-attention mechanism will output another matrix with the size of the shape 13 by
768 so 13 tokens each one with its own embedding of
768 dimensions and
The output of the self-attention is an embedding that captures not only the meaning of the word or its position in the sentence
But also all the contextual information all the relationship between other words and the current word
So the output will be 13 tokens each one represented by an embedding of size
768 what we do now each of them is representing kind of the meaning of a single word, right?
So what we do we can average all of them to create the embedding of the sentence
So we take all these vectors of size 768 we calculate the average of them
Which will result in a single vector with 760 dimensions and this single vector will represent the sentence embedding
Which captures the meaning of the entire sentence and this is how we create the embedding of a sentence
Now how can we compare?
Sentence embeddings in to see if two sentences have similar meaning so for example imagine
One sentence is talking about the the query for example before was talking about how many parameters are there in grog zero
And then we have another sentence that talks about how many parameters there are in grog zero
So how can we relate these two sentences? We need to find a similarity
Function and what we do is we usually use a cosine similarity because they are both vectors and the cosine similarity can be calculated as
between vectors and it measures the cosine of the angle between the two vectors
A smaller angle results in a high cosine singularity similarity score while a bigger angle
Results in a smaller cosine similarity score and this is the formula of the cosine similarity score
But there is a problem
So nobody told Bert that the embedding it produces should be comparable with the cosine similarity
So Bert is outputting some embeddings and then we take the average of these embeddings
But nobody told Bert that these embeddings should be in such a way that two similar sentences should produce similar embeddings
How can we teach Bert to produce embeddings that can be compared with
Similarity function of our choice which could be a cosine similarity or the Euclidean distance
Well, we introduce a sentence Bert sentence Bert is one of the most popular
models to
Produce embeddings for entire sentences that can be compared using a similarity function of our choice in this case
It's the cosine similarity score
So sentence Bert was introduced in a paper called the sentence Bert sentence embeddings using Siamese Bert networks
And we will see all of this. What does it mean? What is a Siamese network?
now imagine we have
two sentences that are similar in meaning for example my my father plays with me at the park and
I play with my dad at the park
We the first one we will call it sentence a and the second one we will call it sentence B
We feed them to Bert. So each of them will be in
Sequence of tokens for example, this one may be 10 tokens and this one may be 8 tokens
We feed it to Bert which will produce output of 10 tokens and 8 tokens
Then we do the pooling the mean pooling that we did before so we take all these output tokens and we calculate the average of them to produce one
Only a vector of dimension
760s in case we are using Bert base or bigger in case we are using a bigger Bert
The first one we will call it sentence embedding a so the first vector is the embedding of the sentence a and the second one is the
embedding of the sentence B we then measure the cosine similarity between these two vectors
We have our target cosine similarity because we are training this
Bert this model so we for example given these two sentences which are quite similar in meaning
We may have a target that is very close to one because the angle between them will be smaller
We hope that the angle between them should be small
so we calculated the loss because we have a target and the output of the model and then we run back propagation on to update all
The weights of this model now as you can see this model is made up of two branches that are same
So in structure
But this is called the Siamese network
Which is a network that is made of two branches or more branches that are same with each other with respect to the to the architecture
But also with respect to the weights of the model
So what we do actually when we represent the Siamese networks we represent it at two branches
But when we code this model actually
We will actually only have one model
So only one branch here that will be produced cosine similarities and what we do on a operating level is that
First we run the sentence a through this
Model then we run the sentence be also through this model
We calculate the cosine similarity between these two output and then we run back propagation such that the back propagation will only modify the
Parameters of this model here, but when we represent it
For showing we actually we represent it as two branches
But remember that they are not actually two branches. It's only one branch. It's only one weights only one architecture and the same number of parameters
This way the bird
Will be between bird the sentence bird like this. It will produce embeddings
But in such a way that the similar
Sentices have a similar cosine similarity. So I have high cosine similarity
So we can compare them using the cosine similarity measure
Also, if if you remember birth produces at least birth base produces embeddings of size a 5
768 if we want to produce a sentence embeddings that are smaller than
760 dimensions we can include a linear layer here to reduce the dimensions for example
We want to go from 768 to 512
in the paper of sentence birth actually they not only
Use the mean pooling that we use the saw to calculate the average of all the tokens output by birth to produce one
Vector that represents the meaning of the entire sentence
But they also use max pooling and another technique that they use is the CLS token
So if you remember the CLS token is the first token that we give as input to birth
And it's also the first token that is output by birth and usually this because of the self-attention mechanism
This is CLS token captures the information from all the other tokens because
of how the self-attention mechanism works and
However, the the sentence birth paper they have shown that the both
Methods so the max pooling and the CLS token don't perform better than mean pooling
So they they recommend using mean pooling and which is also one of actually what is used in production nowadays
Okay, now let's review
Again, what is the pipeline of retrieval augmented generation now that we know how embeddings works?
So we have our query, which is how many parameters are there in grog zero?
Then we have some documents in which we can find this answer. So the documents may be PDF documents or web pages
We split them into single sentences and we embed these sentences using our sentence birth
our sentence birth will produce vectors of a fixed size suppose a
768 and we store them all these vectors in a vector DB. We will see later how it works
The query is also converted into a vector of size a 500
768 dimensions and we search this
Query in the vector DBs. How do we search?
We want to find all the embeddings that best match our query
What do we mean by this? We mean all the embeddings that they have that when we calculate the cosine similarity score with our query
It results in a high value or if you are using another
similarity score for example Euclidean distance the distance is small depending on what
Distance we are using the cosine similarity or the Euclidean distance
This will produce the top embeddings that best match our query and we map them back into the text from which they originated from
This will produce the context that we feed into our prompt template
Along with the query we give it to the large language model, which will produce the answer
As we saw previously augment the knowledge of a language model
We have two strategies fine-tuning on a custom data set or using a vector database made up of embeddings
We can also use a combination of both for example by fine-tuning for a few epochs
And then using a vector database to complement with knowledge retrieved from the web
Whatever strategy we decide to proceed with we need a reliable scalable and easy-to-use service for building our retrieval augmented generation pipelines
That's why I recommend gradient gradient is a scalable AI cloud platform that provides simple APIs for fine-tuning models
Generating embeddings and running inference. Thanks to its integration with popular library Lama index
We can build retrieval augmented generation pipelines with few lines of code
For example, we select the model. We want to use in our case. It's Lama 2
We define the set of custom documents that the model can use to retrieve answers
We define the model that we want to use to generate embeddings ask a question for example
Do you know anyone named oleo it?
Voila thanks to the power of retrieval augmented generation the model can now retrieve information about our channel's mascot oleo
Gradient helps us build all aspects of a retrieval augmented generation pipeline
For example, we can also fine-tune models on custom data as well as generate embeddings with gradient
You have total ownership of your data as well as the weights of fine-tune models open source models are great
Because they save time on development and debugging as we have access to their architecture and the support of a vast community of developers
gradient also integrates with popular libraries Lama index and the long chain
Check the link in the description to redeem your $5 coupon to get started today with gradient
Let's talk about
Let's talk about vector databases what they are and how they're matching algorithm works
So how can the vector database search our query in all the vectors that it has stored?
okay, a vector database
Stores vectors of fixed dimensions called embeddings such that we can then query the database
To find all the embeddings that are closest or more similar to our query using a distance metric the cosine similarity or the Euclidean distance
The vector database uses a variant of the KNN algorithm
Which is stands for the K nearest neighbor algorithm or another similarity search algorithm
but usually it's usually a variant of the KNN algorithm and
The vector databases are not only used in retrieval augmented generation pipeline
They are also used for finding for example similar songs. So if we have songs we can create embeddings of them
So some embedding some vector that captures all the information about that song and then we can find similar songs given
One for example, we have a user who want to find all the similar songs to a given one
We will create the embedding of that song and all the others we compare them using some similarity score
For example, the cosine similarity score
For example, also Google images they search similar images using a similar technique. So using an embedding
Space in which they produce the embedding of a particular image and all the other and then they check the one that match best
We can also do the same with the products, etc
Now KNN is an algorithm that allow us to compare a particular query with all the vectors that we have stored in our database
Sort them by distance or by similarity depending on which one we use and then we keep the top best matching
For example, imagine we have a query
So how many parameters are there in grok and imagine we have a database vector vector database
made up of 1 million embeddings because actually 1 million is not even a big number because if you consider that
Suppose grok for example that is accessing the tweets in real time every day
I think we have a thousand hundreds of thousands of tweets if not millions of tweets
So actually the amount of vectors it has it's actually in the order of billions
I think not even millions. So actually millions looks like a big number
But it's not especially when we deal with the textual data also from the web
We have billions of web pages that we may need to index
So what we do for example in this
KNN with the naive approach, which is the most simple way of matching a query to all the other vectors is to
Compare this query with all the other vectors given our cosine similarity function
For example, we may we may have this with the first vector
It may be 0.3 the second 0.1 0.2, etc, etc
then we sort
The we sorted them by cosine similarity score
So it's for example the highest one for example
This one should be the first one then this one should be the second one, etc, etc
And we keep the top K so the top three or the top two depending on how we chose K
Now this is actually a very simple approach
But it's also very slow because if there are and the embedding vector
So in this case 1 million and each one has a d dimension
So in this case for example in the case of birth base is the 768
The computational complexity is in the order of n multiplied by d which is very very very slow
Let's see if there are better approaches and then we will be exploring one algorithm in particular
That is also used the right now in the most popular vector DB is which is called the hierarchical navigable small words
Now we what we the idea is that we will trade the precision for speed
So before what the algorithm we saw before so the naive K and N
Which performs very slowly, but it's actually precise because each query the query is compared with each of the vectors
So it will always produce accurate results because we have all the possible comparison done
But do so to reduce the number of to increase the speed
We need to reduce the number of comparisons that we do and the metric that we usually care in similarity search is
Recall so the recall basically indicates that if our supposed that
before
The best matching vector is for example this one and this one and this one
We want our
Top three query so K and N to retrieve retrieved them all three of them
But imagine it only returns to in this case we have that the query
Returned only two of the best matching vector. So we will say that the recall is a 66 percent or two-thirds
So basically the recall measures how many relevant items are retrieved among all the relevant item that it should have retrieved
from our search and we will see one one algorithm in particular for approximate nearest neighbors
Which is called hierarchical navigable small words
Now hierarchical navigable small words is actually used is actually very popular nowadays in vector databases and in particular
It's also the same algorithm that powers the database quadrant
Which is also the open source vector database uses by tweeters grok LLM
For example, this is the exchange of tweets that I saw the other day between Elon Musk and the team of quadrant in which they
Quadrants says that actually the grok is accessing all the tweets in real time
Using the vector database, which is quadrant and if we check the
Documentation of this vector database
We will see that the quadrant currently only uses the hierarchical navigable small words as the vector index
So this is the algorithm that powers the word the database that is currently used by Twitter to
Introduce a retrieval augmented generation in its larger which model grok
The first idea behind this hierarchical navigable small words is the is the idea of the six degrees of evolution
So actually the hierarchical navigable small words is an evolution of an earlier algorithm called navigable small words
Which is an algorithm for approximate nearest neighbors that we will see later
Which is based on the idea of six degrees of separation
so in the 1960s there was an experiment which is called the
Milgram experiment which wanted to test the social connections among people in the USA the
Participants who were initially located in a brass cancels were given a letter to deliver to a person that they didn't know
They did not know and this person was in Boston
However, they were not allowed to send the letter directly to the recipient instead. They were instructed to send it to someone who
Who could best know this target person?
At the end of the Milgram word experiment
They found that the letter reached the final recipient in five or six steps
Creating the concept that people all over the world are connected by six degrees of separation and actually in 2016
Facebook published a blog post in which they claim that what the 1.59 billion active users on Facebook
Were connected by an average of 3.5 degrees of separation
This means that between me and Mark Zuckerberg
There are 3.5 connections, which means that on average of course
Which means that I have a friend who has a friend who has a friend who knows Mark Zuckerberg
And this is the idea of the degrees of separation
So let's see what is this navigable small words now the navigable small words algorithm builds a graph that just like
Facebook friends connects connects close vectors with each other
But keeping the total number of connections small for example
Every vector may be connected with up to other six vector like to mimic the sixth degree of
Separation for example imagine we have a very small database with only 15 vectors each one representing a particular
sentence that we retrieved from
our knowledge source which could be documents or web pages and
We have a graph like this in which for example the first text is about the transformer
Which is connected to another piece of text that talks about the transformer model
Then we have another text that connects the three with the two which is now talking about the cancer with AI
so diagnosing cancer with AI and
Etc. Etc. Now, how do we find a given query in this particular graph?
So imagine we have our query which is how many encoder layers are there in the transformer model?
How does the algorithm find the k nearest neighbors? So the best matching k vectors for our query
The algorithm will proceed like this
It will find first of all an entry point in this graph, which is randomly chosen
So we randomly choose among all these vectors one node as a random as an entry point
We visit it and then which compare the similarity score of this query and this node and
Compare it with the similarity score of the query with the friends of this node
So with the number set with the node number seven and the node number two if one of the friends has a better
similarity score then we move it to move it there
So the number two for example may have a better similarity score with the Q compared to the number five
So we move to the number two and then we do again this process
So we check what is the cosine similarity score between the node number three and the query and we compare it with the cosine similarity of the
Vector number two with the query if the number three has a better cosine similarity score
We move there and we keep doing like this until we reach a node in which he's for their friends of this node
So the number eight and the number six don't have a better cosine similarity score
With respect to the query compared to the current one
So this number four has the best cosine similarity score among all of his connections among the zero one zero eight and the zero six
so we stop there and
We have visited many nodes
Basically, we order them from the best matching to the lowest matching and we keep the top K
Also, we repeat this search many times by choosing different random entry points
And every time we choose we sort all of the results by similarity score
And then we keep the top K and these are the best matching K neighbor K nearest neighbors
So using the navigable small words algorithm
If we want to insert a vector in this graph, we just do what we did before. So we actually search
Given for example, we want to insert to this query
For example, we will just do it like a search and then when we have found the top K
We just connect the query with the top K and that's how we insert a new item into this graph
The second idea of the navi hierarchical navigable small words is based on another data structure
That is called the skip list them. So to go from here navigable small words to
Hierarchical navigable small words. We need to introduce this new data structure
so the skip list is a data structure that maintains a sorted list and
Allows to search and insertions with an average of logarithmic time complexity
For example, if we want to search the number nine
What we can do in this first of all as you can see this this is not only one linked list
We have many linked list levels of linked list. We have the level zero the level one the level two the level three
The bottom level has the most number of items the more we go up the less is the number of items
So if we want to search the number line in this linked list in this skip list
We start from the top level. So we start from the head the number three
We check what is the next item and we compare it with them
So the first time them is number five
We compare it with what is the next item in this case is the end
Which means that the number nine must be down
So we go down we then compare it with the next item, which is number 17
Which means that it cannot be after these nodes because it's 17
So it must be down we go down
And then we compare it with the next node, which is the number nine and we see okay
We reached the number nine now imagine we want to find another number. Let's say the number 12
We start again from the hatch three we go to the first item and compare to the next
Okay, it's the end. So we go down then we arrive here. We compare it with the next we see it's 17
So it's bigger than 12. So we go down
Then we see it's nine. So the next item is number nine. So we visit nine
And then we compare it with the next item, which is 17. So it's bigger than 12
So we go down we go here
And then we compare it with what is the next item, which is number 12
And it's the number that we are looking for and we stop there. So this is how the skip list works
Now to create the hierarchical navigable small words
We combine the concept of navigable small words with the idea of the skip list in producing a
hierarchical
Navigable small words algorithm
So we start with we have a lower level which has more nodes and more connections and the upper level
Which has less nodes and less connections. So we say that this one is more dense and this one is more sparse
How does the search work in this uh a graph?
Suppose we have a query just like before and we want to search in this graph
We find a random entry point in the top level of this graph
And then we visit it and then we compare
The cosine similarity of this node with the query and all of his friends with the query and we see that this one
Is the best one so we go down. We go down and we do again the same
We do again the same test
so we check the cosine similarity of this node with the query and all of his friends with the query
And we see that this friend here has a better cosine similarity score
So we move here
Then we check this
node here with all of his friends and we see that this one is the best one so we go down.
Also this one we see that it's the best one among all of his friends
for the cosine similarity score so we go down and then we do again this test and we say what is
the cosine similarity score of this node and the query and also all of his friends with the query
so this node this node and this node and we move to the one that is best in case there is one and
then we stop as soon as we find a local best so the one node that is not worth then all of his
friends. We repeat this search just like before by using randomly selected entry points we take
all these vectors that we have visited we sort we keep the top k best matching based on the
similarity score that we are using or the distance function that we are using.
Okay now that we have seen also how the vector database works let's review again the pipeline of
retrieval augmented generation by summing up what we have seen so again we have our query
we have some documents from which we want to retrieve the knowledge we split them into pieces
of text we create embeddings using sentence bird for example we store all these vectors in a vector
database we convert our query in an embedding and we search in in a vector database using the
algorithm that we have seen before so the hierarchical navigable small words this will
produce the top k embeddings best matching with our query from which we associate go back to the
text that they were taken from we combine the query and the context retrieved in a template
we feed it to the large language model and finally the large language model will produce the answer
thank you guys for watching my video i hope you learned a lot today because i wanted to
create this video for a long time actually but i wanted also to understand myself all
the algorithms that were behind this pipeline and i hope that you are also now familiar with
all these concepts i know that they actually implementing the rug pipeline is very easy
there are many popular libraries like llama index and lun chain but i wanted to actually go deep
inside of how it works and each building block how they actually work together please let let me
know if there is something that you don't understand i will try to answer all the question in the
comment section also let me know if you want to something that you want better clarified in my
future videos or how can i improve my future videos for better clarity please subscribe to my
channel this is the best motivation for me to continue making high quality content and like
the video if you like it share the video with your friends with your professors with your students
etc and have a nice day guys

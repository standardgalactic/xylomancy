Hello guys, welcome to my video about the transformer and this is actually the version 2.0 of my
series on the transformer. I had a previous video in which I talked about the transformer but the
audio quality was not good and as suggested by my viewers as the video was really had a huge success
the viewers suggested me to improve the audio quality so this is why I'm doing this video.
You don't have to watch the previous series because I would be doing basically the same
things but with some improvements so I'm actually compensating from some mistakes I made or from
some improvements that I could add. After watching this video I suggest watching my
other video about how to code a transformer model from scratch. So how to code the model itself,
how to train it on the data and how to inference it. Stick it with me because it's gonna be a
little long journey but for sure worth it. Now before we talk about the transformer I want to
first talk about recurrent neural networks. So the networks that were used before they introduced
the transformer for most of the sequence to sequence tasks. So let's review them.
Recurrent neural networks existed a long time before the transformer and they allowed to map
one sequence of input to another sequence of output. In this case our input is x and we want
an input sequence y. What we did before is that we split the sequence into single items so we gave
the recurrent neural network the first item as input so x1 along with an initial state usually
made up of only zeros and the recurrent neural network produced an output let's call it y1
and this happened at the first time step. Then we took the hidden state this is called the hidden
state of the network of the previous time step along with the next input token so x2 and the
network had to produce the second output token y2 and then we did the same procedure at the third
time step in which we took the hidden state of the previous time step along with the input state
the input token at the time step 3 and the network had to produce the next output token
which is y3. If you have n tokens you need n time steps to map an n sequence input into an n sequence
output. This worked fine for a lot of tasks but had some problems. Let's review them.
The problems with recurrent neural networks first of all are that they are slow for long
sequences because think of the process we did before we have kind of like a for loop in which
we do the same operation for every token in the input so if you have the longer the sequence the
longer this computation and this made the the network not easy to train for long sequences.
The second problem was the vanishing or the exploding gradients. Now you may have heard
these terms or expression on the internet or from other videos but I will try to give you a brief
insight on what does what do they mean on a practical level. So as you know frameworks
like PyTorch they convert our networks into a computation graph so basically suppose we have
a computation graph this is not a neural network I will be making a computational graph that is
very simple has nothing to do with the neural networks but will show you the problems that we
have. So imagine we have two inputs x and another input let's call it y. Our computational graph
first let's say multiplies these two numbers so we have a first a function let's call it f of x
and y that is x multiplied by y I mean multiplied and the result let's call it z
is map is given to another function let's call this function g of z is equal to let's say z
squared what our PyTorch for example does it's that PyTorch want to calculate the usually we
have a loss function PyTorch calculates the derivative of the loss function with respect to
its each weight in this case we just calculate the derivative of the g function so the output
function with respect to all of its inputs so derivative of g with respect to x let's say
is equal to the derivative of g with respect to f
and multiplied by the derivative of f with respect to x these two should kind of cancel out this is
called the chain rule now as you can see the longer the chain of computation so if we have many
nodes one after another the longer this multiplication chain so here we have two because the distance
from this node and this is two but imagine you have 100 or 1000 now imagine this number is
0.5 and this number is 0.5 also the resulting numbers when multiplied together is a number that
is smaller than the two initial numbers it's gone up 0.25 because it's one to one half multiplied
by one half is one fourth so if we have two numbers that are smaller than one and we multiply them
together they will produce an even smaller number and if we have two numbers that are bigger than
one and we multiply them together they will produce a number that is bigger than both of them so
if we have a very long chain of computation it eventually will either become a very big number
or a very small number and this is not desirable first of all because our cpu of our gpu can only
represent numbers up to a certain precision let's say 32 bit or 64 bit and if the number becomes too
small the contribution of this number to the output will become very small so when the pytorch
or our automatic let's say our framework will calculate how to adjust the weights
the weight will move very very very slowly because the contribution of this product is
will be a very small number and this means that we have the gradient is vanishing or in the other
case it can explode become very big numbers and this is a problem the next problem is difficulty
in accessing information from long time ago what does it mean it means that as you remember from
the previous slide we saw that the first input token is given to the recurrent neural network
along with the first state now we need to think that the recurrent neural network is a long
graph of computation it will produce a new hidden state then we will use the the new hidden state
along with the next token to produce the next output if we have a very long sequence of input
sequence the last token will have a hidden state whose contribution from the first token has nearly
gone because of this long chain of multiplication so actually the last token will not depend much
on the first token and this is also not good because for example we know as humans that in a
text in a quite long text the context that we saw let's say 200 words before still relevant to the
context of the current words and this is something that the RNN could not map and this is why we have
the transformer so the transformer solves these problems with the recurrent neural networks and
we will see how the structure of the transformer we can divide into two macro blocks the first
macro block is called encoder and it's this part here the second macro block is called the decoder
and it's the second part here the third part here you see on the top it's just a linear layer and we
will see why it's there and what its is function so and the two layers so the encoder and the
decoder are connected by this connection you can see here in which some output of the encoder is
sent as input to the decoder and we will also see how let's start first of all with some
notations that I will be using during my explanation and you should be familiar with this notation
also to review some maths so the first thing we should be familiar with is matrix multiplication
so imagine we have an input matrix which is a sequence of let's say words so sequence by
D model and we will see why it's called sequence by D model so imagine we have a matrix that is
6 by 512 in which each row is a word and this word is not made of characters but by 512 numbers
so each word is represented by 512 numbers okay like this imagine you have 512 of them along this
row 512 along this other row etc etc 1 2 3 4 5 so we need another one here okay the first word we
will call it A the second B the C D E and F if we multiply this matrix by another matrix let's
say the transpose of this matrix so it's a matrix where the rows becomes columns so 3
4 5 and 6 this word will be here B C D E and F and then we have
512 numbers along each column because before we had them on the rows now they will become
on the column so here we have the 512 number etc etc this is a matrix that is 512 by 6 so
let me add some brackets here if we multiply them we will get a new matrix that is we cancel the
inner dimensions and we get the outer dimension so it will become 6 by 6 so it will be 6 rows by
6 rows so let's draw it how do we calculate the values of this output matrix this is 6 by 6
this is the dot product of the first row with the first column so this is A multiplied by A
this the second value is the first row with the second column the third value is the first row
with the third column until the last column so A multiplied by F etc what is the dot product
is basically you take the first number of the first row so here we have 512 numbers here we
have 512 numbers so you take the first number of the first row and the first number of the first
column you multiply them together second value of the first row second value of the first column
you multiply them together and then you add all these numbers together so it will be let's say
this number multiplied by this plus this number multiplied by this plus this number multiplied
by this plus this number multiplied by this plus you sum all this number together and this is the
A dot product A so we should be familiar with this notation because I will be using it a lot
in the next slides let's start our journey with of the transformer by looking at the encoder
so the encoder starts with the input embeddings so what is an input embedding
first of all let's start with our sentence we have a sentence of in this case six words
what we do is we tokenize it we transform the sentence into tokens what does it mean to tokenize
we split them into single words it is not necessary to always split the sentence using
single words we can even split the sentence in smaller parts that are even smaller than a single
word so we could even split this sentence into let's say 20 tokens by using the each by splitting
each word into multiple words this is usually done in most modern transformer models but we
will not be doing it otherwise it's really difficult to visualize so let's suppose we have this input
sentence and we split into tokens and each token is a single word the next step we do is we map
these words into numbers and these numbers represent the position of these words in our
vocabulary so imagine we have a vocabulary of all the possible words that appear in our training
set each word will occupy a position in this vocabulary so for example the word we are will
occupy the position 105 the word the cat will occupy the position 6500 etc and as you can see
this cat here has the same number as this cat here because they occupy the same position in the
vocabulary we take these numbers which are called input IDs and we map them into a vector of size
512 this vector is a vector made of 512 numbers and we always map the same word to always the same
embedding however this number is not fixed it's a parameter for our model so our model will learn
to change these numbers in such a way that it represents the meaning of the word so the input
IDs never change because our vocabulary is fixed but the embedding will change along with the training
process of the model so the embedding numbers will change according to the needs of the loss function
so the input embedding are basically mapping our single word into an embedding of size 512
and we call this quantity 512 D model because it's the same name that it's also used in the paper
attention is all you need let's look at the next layer of the encoder which is the positional encoding
so what is positional encoding what we want is that each word should carry some information about
its position in the sentence because now we built a matrix of words that are embeddings but they
don't convey any information about how where that particular word is inside the sentence and this
is the job of the positional encoding so what we do we want the model to treat words that appear
close to each other as close and words that are distant as distant so we want the model to see
this information about the spatial information that we see with our eyes so for example when we see
this sentence what is positional encoding we know that the word what is more far from the word
is compared to encoding because we we have this spatial information given by our eyes but the
model cannot see this so we need to give some information to the model about how the words
are specially distributed inside of the sentence and we want the positional encoding to represent
a pattern that the model can learn and we will see how imagine we have our original sentence
your cat is a lovely cat what we do is we first convert into embeddings using the previous layer
so the input embeddings and these are embeddings of size 512 then we create some special vectors
called the positional encoding vectors that we add to these embeddings so this vector we see here
in red is a vector of size 512 which is not learned it's computed once and not learned along with
the training process it's fixed and this word this vector represents the position of the word
inside of the sentence and this should give us a output that is a vector of size again 512
because we are summing this number with this number this number with this number so the first
dimension with the first dimension the second dimension with that so we will get a new vector
of the same size of the input vectors oh how are these position in both embedding calculated let's
see imagine we have a smaller sentence let's say your cat is and you may have seen the following
expressions from the paper what we do is we create a vector of size d model so 512 and for each
position in this vector we calculate the value using these two expressions using these arguments so
the first argument indicates the position of the word inside of the sentence so the word your
occupies the position zero and we use the for the even dimension so the zero the two the four the
510 etc we use the first expression so the sign and for the odd positions of this vector we use
the second expression and we do this for all the words inside of the sentence so this particular
embedding is calculated pe of one zero because it's the first word embedding zero so this one
represents the argument pause and this zero represents the argument two i and pe of one one
means that the first word dimension one so we will use the cosine given the position one
and the two i will be equal to two i plus one will be equal to one and we do this for the third word
etc if we have another sentence we will not have different positional encodings we will have the
same vectors even for different sentences because the positional encoding are computed once and
reused for every sentence that our model will see during inference or training so we only compute
the positional encoding once when we create the model we save them and then we reuse them we don't
need to compute it every time we feed the feed a sentence to the model so why the authors chose
the cosine and the sine functions to represent positional encodings because let's watch the plot
of these two functions the you can see the plot is by position so the position of the word inside
of the sentence and this depth is the dimension along the vector so the two i that you saw before
in the previous expressions and if we plot them we can see as humans a pattern here and we hope
that the model can also see this path okay the next layer of the encoder is the multi-head
attention we will not go inside of the multi-head attention first we will first visualize the single
head attention so the self-attention with a single head and let's do it so what is self-attention
self-attention is a mechanism that existed before they introduced the transformer the
authors of the transformer just changed it into a multi-head attention so how did the self-attention
work the self-attention allows the model to relate words to each other okay so we had the input
embeddings that captured the meaning of the word then we had the positional encoding that give the
information about the position of the word inside of the sentence now we want this self-attention
to relate words to each other now imagine we have an input sequence of six word with the
d model of size 512 which can be represented as a matrix that we will call q k and v so our q k and v
is a same matrix are the same matrix representing the input so the input of six words with the
dimension of 512 so each word is represented by a vector of size 512 we basically apply this
formula we saw here from the paper to calculate the attention the self-attention in this case
why self-attention because it's the each word in the sentence related to other words in the same
sentence so it's self-attention so we start with our q matrix which is the input sentence so let's
visualize it for example so we have six rows and on this on the columns we have 512 columns now
they are really difficult to draw but let's say we have 512 columns and here we have six okay
now what we do according to this formula we multiply it by the same sentence but transposed
so the transposed of the k which is again the same input sequence we divide it by the square root
of 512 and then we apply the softmax the output of this as we saw before in the in the initial
matrix notations we saw that when we multiply six by 512 with another matrix that is 512 by six
we obtain a new matrix that is six by six and each value in this matrix represents the dot
product of the first row with the first column this represents the dot product of the first
row with the second column etc the values here are actually randomly generated so don't concentrate
on the values what you should notice is that the softmax makes all these values in such a way that
they sum up to one so this row for example here some sums up to one this other row also sums up
to one etc etc and this value we see here is the dot product of the first word with the
embedding of the word itself this value here it's the dot product of the embedding of the word
you're with the embedding of the word cat and this value here is the dot product of the word
the embedding of the word you're with the embedding of the word is the next thing and this value
represents somehow a score that how intense is the relationship between one
word and another. Let's go ahead with the formula so for now we just multiplied q
by k divided by the square root of dk applied the softmax but we didn't
multiply by v. So let's go forward we multiply this matrix by v and we obtain
a new matrix which is 6 by 512. So if we multiply a matrix that is 6 by
6 with another that is 6 by 512 we get a new matrix that is 6 by 512 and one
thing you should notice is that the dimension of this matrix is exactly the
dimension of the initial matrix from which we started. This what does it mean
that we obtain a new matrix that is 6 rows so let's say 6 rows with 512
columns in which each these are our words so we have six words and each word
has an embedding of dimension 512 so now this embedding here represents not
only the meaning of the word which was given by the input embedding not only
the position of the word which was added by the positional encoding but now
somehow this special embedding so these values represent a special embedding
that also captures the relationship of this particular word with all the other
words and this particular embedding of this word here also captures not only
its meaning not only its position inside of the sentence but also the
relationship of this word with all the other words. I want to remind you that
this is not the multi-head attention we are just watching the self-attention so
one head we will see later how this becomes the multi-head attention. Self-attention
has some properties that are very desirable first of all it's permutation
invariant what does it mean to be permutation invariant it means that if
we have a matrix let's say first we had a matrix of six words in this case let's
just four words so a b c and d and suppose by applying the formula before
this produces this particular matrix in which there is new special embedding for
the word a a new special embedding for the word b a new special embedding for
the word c and d so let's call it a prime b prime c prime d prime if we change
the position of these two rows the values will not change the position of the
output will change accordingly so the values of b prime will not change it
will just change in the position and also the c will also change position but
the values in each factor will not change and this is a desirable properties
self-attention as of now requires no parameters I mean I didn't introduce any
parameter that is learned by the model I just took the initial sentence of in
this case six words we multiplied it by itself we divide it by a fixed quantity
which is the square root of 512 and then we apply the softmax which is not
introducing any parameter so for now the self-attention rate didn't require any
parameter except for the embedding of the words this will change later when we
introduce the multi-head attention also we expect because the each value in the
self-attention in the softmax matrix is a dot product of the word embedding with
itself and the other words we expect the values along the diagonal to be the
maximum because it's the dot product dot product of each word with itself and
there is another property of this matrix that is before we apply the softmax
if we replace the value in this matrix suppose we don't want the word your and
cat to interact with each other or we don't want the word let's say is and
the lovely to interact with each other what we can do is before we apply the
softmax we can replace this value with minus infinity and also this value with
minus infinity and when we apply the softmax the softmax will replace minus
infinity with zero because as you remember the softmax is e to the power of
x if x is going to minus infinity e will e to the power of minus infinity will
become very very close to zero so basically 0 this is a desirable property
that we will use in the decoder of the transformer now let's have a look at what
is a multi-head attention so what we just saw was the self-attention and we
want to convert it into a multi-head attention you may have seen these
expressions from the paper but don't worry I will explain them one by one so
let's go imagine we have our encoder so we are on the encoder side of the
transformer and we have our input sentence which is let's say 6 by 512 so
6 word by 512 is the size of the embedding of each word in this case I
call it sequence by D model so sequence is the sequence length as you can see
on the legend in the bottom left of the slide and the D model is the size of the
embedding vector which is 512 what we do just like the picture shows we take
this input and we make four copies of it one will be sent one will be sent along
this connection we can see here and three will be sent to the multi-head
attention with three respective names so it's the same input that becomes three
matrices that are equal to input one is called the query one is called key and
one is called value so basically we are taking this input and making three
copies of it one we call Q K and B they have of course the same dimension what
does the multi-head attention do first of all it multiplies these three
matrices by three parameter matrices called WQ WK and WV these matrices have
dimension D model by D model so if we multiply a matrix that is sequenced by
D model with another one that is D model by D model we get a new matrix as
output that is sequenced by D model so basically the same dimension as the
starting matrix and we will call them Q prime K prime and V prime our next step
is to split these matrices into smaller matrices let's see how we can split this
matrix Q prime by the sequence dimension or by the D model dimension in the
multi-head attention we always split by the D model dimension so every head will
see the full sentence but a smaller part of the embedding of each word so if we
have an embedding of let's say 512 it will become a smaller embeddings of
512 divide by 4 and we call this quantity DK so DK is D model divide by H
where H is the number of heads in our case we have H equal to 4 we can
calculate the attention between these smaller matrices so Q1 K1 and V1 using
the expression taken from the paper and this will result into a small matrix
called head 1 head 2 head 3 and head 4 the dimension of head 1 up to head 4 is
sequenced by D phi what is DV is basically it's equal to DK it's just
called a DV because the last multiplication is done by V and in the
paper they call it DV so I am also sticking to the same names our next
step is to multi combine these matrices these small heads by concatenating them
along the DV dimension just like the paper says so we can cut all this head
together and we get a new matrix that is sequenced by H multiplied by DV where
H multiplied by DV as we know DV is equal to DK so H multiplied by DV is
equal to D model so we get back the initial shape so it's sequenced by
D model here the next step is to multiply the result of this concatenation by
Wo and Wo is a matrix that is H multiplied by DV so D model with the
other dimension being D model and the result of this is a new matrix that is
the result of the multi head attention which is sequenced by D model so the
multi head attention instead of calculating the attention between these
matrices here so Q prime K prime and V prime splits them along the D model
dimension into smaller matrices and calculates the attention between these
smaller matrices so each head is watching the full sentence but a different
aspect of the embedding of each word why we want this because we want the each
head to watch different aspect of the same word for example in the chinese
language but also in other languages one word may be a noun in some cases maybe
a verb in some other cases maybe a adverb in some other cases depending on the
context so what we want is that one head maybe learns to relate that word as a
noun another head maybe learns to relate that word as a verb and another head
learns to relate that verb as an adjective or adverb so this is why we want a
multi head attention now you may also have seen online that the
attention can be visualized and i will show you how when we calculate the
attention between the Q and the K matrices so when we do this operation so the
softmax of Q multiplied by the K divided by the square root of D K we get a new
matrix just like we saw before which is sequence by sequence and this represents
a score that represents the intensity of the relationship between the two words
we can visualize this and this will produce a visualization similar to this one which i took
from the paper in which we see how the all the heads work so for example if we concentrate on
this work making this word here we can see that making is related to the word difficult so this
word here by different heads so the blue head the red head and the green head but the violet
let's say the violet head is not relating this to word together so making and difficult is not
related by the violet or the pink head the violet head or the pink head they are relating the word
making to other words for example to this word 2009 why this is the case because maybe this
pink head could see the part of the embedding that these other heads could not see that made this
interaction possible between these two words
you may be also wondering why these three matrices are called query keys and values
okay the terms come from the database terminology or from the python like dictionaries
but i would also like to give my interpretation of my own making a very simple example i think
it's quite easy to understand so imagine we have a python like dictionary or a database
in which we have keys and values the keys are the category of movies and the values are the
movies belonging to that category in my case i just put one value so we have a romantics
category which includes titanic we have action movies that include the dark knight etc imagine
we also have a user that makes a query and the query is love because we are in the transformer
word all these words actually are represented by embeddings of size 512 so what our transformer
will do he will convert this word love into an embedding of 512 all these queries and values are
already embeddings of 512 and it will calculate the dot product between the query and all the keys
just like the formula so as you remember the formula is a softmax of query multiplied by the
transpose of the keys divided by the square root of the model so we are doing the dot product of
all the queries with all the keys in this case the word love with all the keys one by one and
this will result in a score that will amplify some values or not amplify other values
in this case our embedding may be in such a way that the word love and romantic
are interrelated to each other the word love and comedy are also related to each other but not
so intensively like the word love and romantic so it's more how to say let's let's strong
relationship but maybe the word horror and love are not related at all so maybe their softmax score
is very close to zero our next layer in the encoder is the add and norm and to introduce
the add and norm we need the layer normalization so let's see what is the layer normalization
layer normalization is a layer that okay let's make a practical example imagine we have a batch
of n items in this case n is equal to three item one item two item three each of these items will
have some features it could be an embedding so for example it could be a feature of a vector of size
512 but it could be a very big matrix of thousands of features doesn't matter what we do is we
calculate the mean and the variance of each of these items independently from each other
and we replace each value with another value that is given by this expression so basically
we are normalizing so that the new values are all in the range 0 to 1 actually we also multiply
this new value with a parameter called gamma and then we add another parameter called beta and this
gamma and beta are learnable parameters and the model should learn to multiply and add these
parameters so as to amplify the value that it wants to be amplified and not amplify the value
that it doesn't want to be amplified so we don't just normalize we actually introduce some parameters
and I found a really nice visualization from paperswithcode.com in which we see the difference
between batch norm and layer norm so as we can see in the layer normalization we are calculating
if n is the batch dimension we are calculating all the values belonging to one item in the batch
while in the batch norm we are calculating the same feature for all the batch so for all the
items in the batch so we are mixing let's say values from different items of the batch
while in the layer normalization we are treating each item in the batch independently which will
have its own mean and its own variance let's look at the decoder now now in the encoder we saw the
input embeddings in this call in this case they are called output embeddings but the underlying
working is the same here also we have the positional encoding and they are also the same
as the as the encoder the next layer is the musket multihead attention and we will see it
now we also have the multihead attention here with the here we should see that the
there is the encoder here that produces the output and is sent to the decoder
in the forms of keys and values while the query so this connection here is the query coming from
the decoder so in this multihead attention it's not a self-attention anymore it's a cross
attention because we are taking two sentences one is sent from the encoder side so let's write
encoder in which we provide the output of the encoder and we use it as a key as keys and values
while the output of the musket multihead attention is used as the query in this multihead attention
and the musket multihead attention is a self-attention of the input sentence of the decoder so we
take the input sentence of the decoder we transform into embeddings we add the positional encoding
we give it to this multihead attention in which the query key and values are the same input sequence
we do the add and norm then we send this as the queries of the multihead attention while the keys
and the values are coming from the encoder then we do the add and norm i will not be showing the
feed forward which is just a fully connected layer we then send the output of the feed forward
to the add and norm and finally to the linear layer which we will see later so let's have a look
at the musket multihead attention and how it differs from a normal multihead attention
what we want our goal is that we want to make the model causal it means that the output at a
certain position can only depend on the words on the previous position so the model must not be able
to see future words how can we achieve that as you saw the the output of the softmax in the
attention calculation formula is this metric sequence by sequence if we want to hide the
interaction of some words with other words we delete this value and we replace it with minus
infinity before we apply the softmax so that the softmax will replace this value with zero
and we do this for all the interaction that we don't want so we don't want your
to watch future words so we don't want your to watch cat is a lovely cat and we don't want the
word cat to watch future words but only all the words that come before it or the word itself
so we don't want this this this this also the same for the other words etc
now so we can see that we are replacing all the words all these values here that are above
this diagonal here so this is the principal diagonal of the matrix and we want all the
values that are above this diagonal to be replaced with minus infinity so that
so that the softmax will replace them with zero let's see in which stage of the multihead
attention this mechanism is introduced so when we calculate the attention between these
molar matrices so q1 k1 and v1 before we apply the softmax we replace these values so this one
this one this one this one this one etc with minus infinity then we apply the softmax and
then the softmax will take care of transforming these values into zeros so basically we don't
want these words to interact with each other and if we don't want this interaction the model will
learn to not make them interact because the model will not get any information from this
interaction so it is like this word cannot interact now let's look at how the inference
and training works for a transformer model as i said previously we are dealing with it
we will be dealing with a translation task so because it's easy to visualize and it's easy
to understand all the steps let's start with the training of the model we will go from an
english sentence i love you very much into an italian sentence the ammolto it's a very simple
sentence it's easy to describe let's go we start with a description of the of the transformer model
and we start with our english sentence which is sent to the encoder so our english sentence here
on which we prepend and append two special tokens one is called start of sentence and one is called
end of sentence these two tokens are taken from the vocabulary so they are special tokens in our
vocabulary that tells the model what is the start position of a sentence and what is the end of a
sentence we will see later why we need them for now just think that we take our sentence we prepend
special token and we append a special token then what we do as you can see from the picture we
take our inputs we transform into input embeddings we add the positional encoding and then we send
it to the encoder so this is our encoder input sequenced by demodel we send it to the encoder
it will produce an output which is encode a sequence by demodel and it's called the encoder
output so as i saw we saw previously the output of the encoder is another matrix that has the same
dimension as the input matrix in which the embedding we can see it as a sequence of embeddings
in which this embedding is special because it captures not only the meaning of the word which
was given by the input embedding we saw here so by this not only the position which was given by
the positional encoding but also the interaction of every word with every other word in the same
sentence because this is the encoder so we are talking about self-attention so it's the interaction
of each word in the sentence with all the other words in the same sentence we want to convert
to this sentence into italian so we prepare the input of the decoder which is start of sentence
the amomolto as you can see from the picture of the the transformer the outputs here you can see
shifted right what does it mean to shift right basically it means we prepend a special token
called sos start of sentence you should also notice that these two sequences actually they
in when we coded the transformer so if you watch my other video on how to code the transformer you
will see that we make this sequence of fixed length so that if we have a sentence that is
the amomolto or a very long sequence actually when we feed them to the transformer they all become
become of the same length how to do this we add padding words to reach the length the desire
length so if our model can support let's say a sequence length of 1000 in this case we have a
fourth tokens we will add 996 tokens of padding to make this sentence long enough to reach the
sequence length of course i'm not doing it here because it's not easy to visualize otherwise
okay we prepare this input for the decoder we add transform into embeddings we add the
positional encoding then we send it first to the multihead attention so the musket multihead
attention so along with the causal mask and then we take the output of the encoder and we send it
to the decoder as keys and values while the queries are coming from the musket so the queries are
coming from this layer and the keys and the values are the output of the encoder this the output of
all this block here so all this big block here will be a matrix that is sequenced by the model
just like for the encoder however we can see that this is still an embedding because it's
a demodel it's a vector of size 512 how can we relate this embedding back into our dictionary
how can we understand what is this word in our vocabulary that's why we need a linear layer
that will map sequence by demodel into another sequence by vocabulary size so it will tell for
every embedding that it sees what is the position of that word in our vocabulary so that we can
understand what is the actual token that is output by the model after that we apply the
softmax and then we have our label what we expect the model to output given this english
sentence we expect the model to output this tiama molto end of sentence and this is called
the label or the target what we do when we have the output of the model and the corresponding
label we calculate the loss in this case is the cross entropy loss and then we back propagate the
loss to all the weights now let's understand why we have these special tokens called sos and us
basically you can see that here the sequence length is 4 actually it's 1000 because we have
the embedding but let's say we don't have any padding so it's four tokens start of sentence
tiamo molto and what we want is ti amo molto end of sentence so our model when it will see the
start of sentence token it will output the first token as output ti when it will see ti it will
output amo when it will see amo it will output molto and when it will see molto it will output
end of sentence which will indicate that okay the translation is done
and we will see this mechanism in the inference
ah this all happens in one time step just like i promised at the beginning of the video
i said that with the recurrent neural networks we have n time steps to map an input sequence into
an output sequence but this problem would be solved with the transformer yes it has been solved
because you can see here we didn't do any for loop we just did all in one pass we give an input
sequence to the encoder an input sequence to the decoder we produced some outputs we calculated
the cross entropy loss with the label and that's it it all happens in one time step and this is
the power of the transformer because it made it very easy and very fast to train very long
sequences and with the very very nice performance that you can see in chat gpt you can see in gpt
in bird etc let's have a look at how inference works again we have our english sentence i love
you very much we want to map it into an italian sentence the ammolto we have our usual transformer
we prepare the input for the encoder which is start of sentence i love you very much end of
sentence we convert into input embeddings then we add the positional encoding we prepare the
input for the encoder and we send it to the encoder the encoder will produce an output
which is sequenced by the model and we saw it before that it's a sequence of special embeddings
that capture the meaning the position but also the interaction of all the words with other words
what we do is for the decoder we give him just the start of sentence and of course we keep the
we add enough embedding padding tokens to reach our sequence length we just give the model the
start of sentence token and we again we for this single token we convert into embeddings we add
the positional encoding and we send it to the decoder as the code input the decoder will take
this his input as a query and the key and the values coming from the encoder
and it will produce an output which is sequenced by demodel again we want the linear layer to
project it back to our vocabulary and this projection is called logits what we do is we
apply the softmax which will select given the logits will give the position of the output word
will have the maximum score with the softmax this is how we know what words to select from the
vocabulary and this hopefully should produce the first output token which is t if the model has
been trained correctly this however happens at time step one so when we train the model
transformer model it happens in one pass so we have one input sequence one output sequence we give
it to the model we do it one time step and the model will learn it when we inference however we
need to do it token by token and we will also see why this is the case at time step two we
don't need to recompute the encoder output again because the our english sentence didn't change
so we hope the encoder should produce the same output for it and then what we do is we take the
output of the previous sentence so s or t we append it to the input of the decoder and then we
feed it to the decoder again with the output of the encoder from the previous step which will produce
an output sequence from the decoder side which we again project back into our vocabulary and
we get the next token which is ammo so as i saw as i said before we are not recalculating the
output of the encoder for every time step because our english sentence didn't change at all what is
changing is the input of the decoder because at every time step we are appending the output of
the previous step to the input of the decoder we do the same for the time step three and we do the
same for the time step four and hopefully we will stop when we see the end of sentence token
because that is that's how the model tells us to stop inferencing and this is how the inference
works why we needed four time steps when we inference a model like the in this case the
translation model there are many strategies for inferencing what we used is called greedy strategy
so for every step we get the word with the maximum softmax value and however this strategy works
usually not bad but there are better strategies and one of them is called beam search in beam search
instead of always greedily so this is that's why it's called greedy instead of greedily taking
the maximum soft value we take the top b values and then for each of these choices we inference
what are the next possible tokens for each of the top b values at every step and we keep
only the one with the b most probable sequences and we delete the others this is called beam search
and generally it performs better so thank you guys for watching i know it was a long video but it was
really worth it to go through each aspect of the transformer i hope you enjoyed this journey with
me so please subscribe to the channel and don't forget to watch my other video on how to code a
transformer model from scratch in which i described not only again the structure of the transformer
model while coding it but i also show you how to train it on a dataset of your choice
how to inference it and i also provided the code on github and the the a collab
notebook to train the model directly on collab please subscribe to the to the channel and let
me know what you didn't understand so that i can give more explanation and please tell me
what are the problems in this kind of videos or in this particular video that i can improve
for the next videos thank you very much and have a great rest of the day

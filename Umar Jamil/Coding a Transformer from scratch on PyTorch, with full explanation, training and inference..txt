Hello guys, welcome to another episode about the transformer.
In this episode, we will be building the transformer from scratch using PyTorque, so coding it
from zero.
We will be building the model and we will also build the code for training it for inferencing
and for visualizing the attention scores.
Stick with me because it's gonna be a long video, but I assure you that by the end of
the video you will have a deep knowledge of the transformer model, not only from a conceptual
point of view, but also from a practical point of view.
We will be building a translation model which means that our model will be able to translate
from one language to another.
I chose a data set that is called Opus Books and it's a synthesis taken from famous books.
I chose the English to Italian because I am Italian so I can understand and I can tell
that if the translation is good or not.
But I will show you which point you can change the language so you can test the same model
with the language of your choice.
Let's get started.
Let's open the IDE of our choice, in my case I really love Visual Studio Code and let's
create our first file which is the model of the transformer.
Okay, let's go have a look at the transformer model first so we know which one part we are
going to build first and then we will build each part one by one.
The first part that we will be building is the input embeddings.
As you can see, the input embeddings take the input and convert into an embedding.
What is the input embedding?
As you remember from my previous video, the input embeddings allows to convert the original
sentence into a vector of 512 dimensions.
For example, in this sentence, your cat is a lovely cat.
First we convert the sentence into a list of input IDs that is numbers that correspond
to the position of each word inside the vocabulary.
And then each of these numbers corresponds to an embedding which is a vector of size
512.
So let's build this layer first.
The first thing we need to do is to import Torch.
And then we need to create our class.
This is the constructor.
We will need to tell him what is the dimension of the model, so the dimension of the vector.
In the paper, this is called the model.
And we also need to tell him what is the vocabulary size.
So how many words there are in the vocabulary.
Save these two values.
And now we can create the actual embedding.
Okay, actually PyTorch already provides with a layer that does exactly what we want to
do.
That is taken, given a number, it will provide you with the same vector every time.
And this is exactly what embedding does.
It's just a mapping between numbers and a vector of size 512.
512 here in this other case is the demodel.
So this is done by the embedding layer and end dot embedding and workup size and demodel.
Let me check why my autocomplete is not working.
Okay, so now let's implement the forward method.
What we do in the embedding is that we just use the embedding layer provided by PyTorch
to do this mapping.
So return self dot embedding of x.
Now actually there is a little detail that is written on the paper.
That is, let's have a look at the paper actually.
Let's go here and if we check the embedding and softmax, we will see that in this sentence
in the embedding layer, we multiply the weights of the embedding by square root of demodel.
So what the authors do, they take the embedding given by this embedding layer, which I remind
you is just a dictionary kind of layer that just maps numbers to the same vector every
time and this vector is learned by the model.
So we just multiply this by math dot sqrt of demodel.
You also need to import math.
Okay, now the embedding, the input embeddings are ready.
Let's go to the next module.
The next module we are going to build is the positional encoding.
Let's have also a look at what are the positional encoding very fast.
So we saw before that our original sentence gets mapped to a list of vectors by the embedding,
the embeddings layer.
And this is our embeddings.
Now we want to do, we want to convey to the model the information about the position
of each word inside the sentence and this is done by adding another vector of the same
size as the embedding, so of size 512, that includes some special values given by a formula
that I will show later, that tells the model that this particular word occupies this position
in the sentence.
So we will create these vectors called the position embedding and we will add them to
the embedding.
Okay, let's go do it.
Okay, let's define the class positional encoding and we define the constructor.
Okay, what we need to give to the constructor is for sure the demodel because this is the
size of the vector that the positional encoding should be.
And the sequence length, this is the maximum length of the sentence and because we need
to create one vector for each position and we also need to give the dropout.
Dropout is to make the model less overfit.
Okay, let's actually build the positional encoding.
Okay, first of all, the positional encoding is we will build a matrix of shape sequence
length to demodel.
Why sequence length to demodel?
Because we need vectors of demodel size, so 512, but we need sequence length number
of dem, because the maximum length of the sentence is sequence length.
So let's do it.
Okay, before we create the matrix and we know how to create the matrix, let's have a look
at the formula used to create the positional encoding.
So let's go have a look at the formula used to create the positional encoding.
This is the slide from my previous video and let's have a look at how to build the vectors.
So as you remember, we have a sentence, let's say in this case we have three words.
We use these two formulas taken from the paper.
We create a vector of size 512 and one for each possible position, so up to sequence length.
And in the even positions we apply the first formula, in the odd positions of the vector
we apply the second formula.
In this case, I will actually simplify the calculation because I saw online it has been
simplified also.
So we will do a slightly modified calculation using log space.
This is for numerical stability.
So when you apply the exponential and then the log of something inside the exponential,
the result is the same number, but it's more numerically stable.
So first we create a vector called the position that will represent the position of the word
inside the sentence.
And this factor can grow from 0 to sequence length minus 1.
So actually we are creating a tensor of shape sequence length to 1.
Okay, now we create the denominator of the formula.
And these are the two terms we see inside the formula.
Let's go back to the slide.
So the first tensor that we built that's called position is this pause here.
And the second tensor that we built is the denominator here, but we calculated it in
log space for numerical stability.
The value actually will be slightly different, but the result will be the same.
The model will learn this positional encoding.
Don't worry if you don't fully understand this part, it's just very special, let's say
functions that convey this positional information to the model.
And if you watch my previous video, you will also understand why.
Now we apply this to the denominator and denominator to the sign and the cosine.
As you remember, the sign is only used for the even positions and the cosine only for
the odd position, so we will apply it twice.
Let's do it.
So apply.
So every position will have the sign, but only, so every word will have the sign, but
only the even dimensions.
So starting from zero up to the end and going forward by two means every from zero, then
the number two, then the number four, etc., etc., position multiplied by tip term.
Then we do the same for the cosine.
In this case, we start from one and go forward by two, it means one, three, five, etc.
And then we need to add the batch dimension to this tensor so that we can apply it to the
whole sentences, so to all the batch of sentence, because now the shape is sequence length to
demodel, but we will have a batch of sentences.
So what we do is we add a new dimension to this PE and this is done using unsquiz and
in the first position, so it will become a tensor of shape one to sequence length to
demodel.
And finally, we can register this tensor in the buffer of this module.
So what is the buffer of the module?
Let's first do it, register buffer.
So basically when you have a tensor that you want to keep inside the module, not as a parameter,
learned parameter, but you want it to be saved when you save the file of the module, you
should register it as a buffer.
This way the tensor will be saved in the file along with the state of the module.
And we do the forward method.
So as you remember from before, we need to add this positional encoding to every word
inside the sentence.
So let's do it.
So we just do x is equal to x plus the positional encoding for this particular sentence.
And we also tell the module that we don't want to learn this positional encoding because
they are fixed, they will always be the same, they are not learned along the training process.
So we just do it with required squared folds.
This will make this particular tensor not learned.
And then we apply the dropout.
And that's it.
This is the positional encoding.
Let's have a look at the next module.
Okay, first we will build the encoder part of the transformer, which is this left side
here.
And we still have the multi-head attention to build the add and norm and the feed forward.
And actually there is another layer which connects this skip connection to all these
sublayers.
So let's start with the easiest one.
Let's start with layer normalization, which is this add and norm.
As you remember from my previous video, let's have a look at the layer normalization, a
little briefing.
So layer normalization basically means that if you have a batch of n items, in this case
only three, each item will have some features.
Let's say that these are actually sentences and each sentence is made up of many words
with its numbers.
So this is our three items.
And layer normalization means that we, for each item in this batch, we calculate a mean
and a variance independently from the other items of the batch.
And then we calculate the new values for each of them using their own mean and their own
variance.
In the layer normalization, usually we also introduce some parameters called gamma and
beta.
Some people call it alpha and beta.
Some people call it alpha and bias.
Okay, it doesn't matter.
One is multiplicative, so it's multiplied by each of these x and one is additive.
So it's added to each one of these x, y.
Because we want the model to have the possibility to amplify these values when he needs this
value to be amplified.
So the model will learn to multiply this gamma by these values in such a way to amplify
the values that it wants to be amplified.
Okay, let's go to build the code for this layer.
Let's define the layer normalization class.
And the constructor, as usual.
In this case, we don't need any parameter except for one that I will show you now, which
is epsilon.
And usually EPS stands for epsilon, which is a very small number that you need to give
to the model.
I will also show you why we need this number.
In this case, we use 10 to the power of minus 6.
Let's save it.
Okay, this epsilon is needed because if we look at the slide, we have this epsilon here
in the denominator of this formula here.
So x with cap is equal to xj minus mu divided by the square root of sigma square plus epsilon.
Why we need this epsilon?
Because imagine this denominator.
If sigma happens to be zero or very close to zero, this x mu will become very big, which
is undesirable, as we know that the CPU or the GPU can only represent numbers up to a
certain position and scale.
So we don't want very big numbers or very small numbers.
So usually for numerical stability, we use this epsilon also to avoid division by zero.
Let's go forward.
So now let's introduce the two parameters that we will use for the layer normalization.
One is called alpha, which will be multiplied, and one is bias, which will be added.
Usually the additive is called bias.
It's always added.
And the alpha is the one that is multiplied.
In this case, we will use nn.parameter.
This makes the parameter learnable.
And we define also the bias.
This I want to remind you is multiplied, and this is added.
Let's define the forward.
OK.
As you remember, we need to calculate the mean and the standard deviation or the variance
for both of these.
We will calculate the standard deviation of the last dimension, so everything after the
batch, and we keep the dimension.
So this parameter keep dimension means that usually the mean cancels the dimension to
which it is applied, but we want to keep it.
And then we just apply the formula that we saw on the slide.
So alpha multiplied by what?
X minus its mean divided by the standard deviation plus self-route PPS.
Everything added to bias.
And this is our layer normalization.
OK.
Let's go have a look at the next layer we are going to build.
The next layer we are going to build is the feedforward.
You can see here.
The feedforward is basically a fully connected layer that the model uses both in the encoder
and in the decoder.
Let's first have a look at the paper to see what are the details of this feedforward layer.
In the paper, the feedforward layer is basically two matrices, 1W1, 1W2, that are multiplied
by this X, one after another with the relu in between and with the bias.
We can do this in PyTorch using a linear layer in which we define the first one to be the
matrix with the W1 and the B1 and the second one to be the W2 and the B2 and in between
we apply a relu.
In the paper, we can also see the dimensions of these matrices.
So the first one is basically D-model to DFF and the second one is from DFF to D-model.
So DFF is 2048 and D-model is 512.
Let's go build it.
Class, feedforward, block.
We also build in this case the constructor.
And in the constructor, we need to define these two values that we saw on the paper.
So D-model, DFF and also in this case dropout.
We define the first matrix, so W1 and B1 to be the linear one.
And it's from D-model to DFF.
And then we apply the dropout, actually we define the dropout.
And then we define the second matrix, W2 and B2.
So let me write the comments here, it's W1 and B1 of DFF to D-model.
And this is W2 and B2.
Why we have B2, because actually as you can see here bias is by default it's true.
So it's already defining a bias matrix for us.
Okay let's define the forward method.
In this case what we are going to do is we have an input sentence which is batch, it's
a tensor with dimension batch, sequence, length and D-model.
Next we will convert it using linear one into another tensor of batch to sequence length
to DFF, because if we apply this linear it will convert the D-model into DFF.
And then we apply the linear two which will convert it back to D-model.
We apply the dropout in between.
And this is our feedforward block.
Let's go have a look at the next block.
Our next block is the most important and most interesting one and it's the multi-head attention.
We saw briefly in the last video how the multi-head attention works.
So I will open now the slide again to show, to rehearse how it actually works and then
we will do it practically by coding.
As you remember in the encoder we have the multi-head attention that takes the input
of the encoder and uses it three times.
One time it's called query, one time it's called key and one time it's called values.
You can also think it like a duplication of the input three times or you can just say
that it's the same input applied three times.
And the multi-head attention basically works like this.
We have our input sequence which is sequence length by D-model.
We transform into three matrices Q, K and V which are exactly the same as the input
in this case because we are talking about the encoder.
We will see that in the decoder it's a slightly different.
And then we multiply this by a matrices called W, Q, W, K and WV and this results in a new
matrix of dimension sequence by D-model.
We then split these matrices into H matrices, smaller matrices, why H?
Because it's the number of head we want for this multi-head attention.
And we split these matrices along the embedding dimension, not along the sequence dimension
which means that each head we will have access to the full sentence but a different part
of the embedding of each word.
We apply the attention to each of these smaller matrices using this formula which will give
us smaller matrices as a result.
Then we combine them back, so we concatenate them back just like the paper says, so concatenation
of head 1 up to head H and finally we multiply it by WO to get the multi-head attention
output which again is a matrix that has the same dimension as the input matrix.
As you can see the output of the multi-head attention is also sequenced by D-model.
In this slide actually I didn't show the batch dimension because we are talking about
one sentence but when we code the transformer we don't work only with one sentence but with
multiple sentences.
So we need to think that we have another dimension here which is the batch.
Okay, let's go to code this multi-head attention.
I will do it a little more slower so we can see in detail everything how it's done but
I really wanted you to have an overview again of how it works and why we are doing what
we are doing.
So let's go code it, class, also in this case we define the constructor and what we need
to give to this multi-head attention as parameter for sure the D-model of the model which is
in our case 512.
The number of heads which we call H just like in the paper, so H indicates the number
of heads we want and then the dropout value.
We save these values.
As you can see we need to divide this embedding vector into H heads which means that this
D-model should be divisible by H otherwise we cannot divide equally the same vector representing
the embedding into equal matrices for each head.
So we make sure that D-model is divisible by H basically.
And this will make the check.
If we watch again my slide we can see that the value D-model divided by H is called decay.
As we can see here if we divide the D-model by H heads we get a new value which is called
decay and to be aligned with what the paper, with the nomenclature used in the paper we
will also call it decay.
So decay is D-model divided by H.
Let's also define the matrices by which we will multiply the query, the key and the values
and also the output matrix W also.
This again is a linear so from D-model to D-model.
Why from D-model to D-model because as you can see from my slide this is D-model by D-model
so that the output will be sequenced by D-model.
So this is WQ, this is WK and this is WV.
Finally we also have the output matrix which is called WO here.
This WO is H by DV by D-model.
So H by DV is what, DV is actually equal to DK because it's the D-model divided by H.
But why it's called DV here and DK here because this head is actually the result, this head
comes from this multiplication and the last multiplication is by V and in the paper they
call this value DV but on a practical level it's equal to DK.
So our WO is also a matrix that is D-model by D-model because H by DV is equal to D-model.
And this is WO, finally we create the dropout.
Let's implement the forward method and let's see how the multi head attention works in
detail during the coding process.
We define the query, the key and the values and there is this mask.
So what is this mask?
The mask is basically if we want some words to not interact with other words we mask them.
And we saw in my previous video but now let's go back to those slides to see what is the
mask doing.
As you remember when we calculated the attention are using this formula so softmax of Q multiplied
by KT divided by square root of DK and then by V we get this head matrix.
But before we multiply by V so only this multiplication here with Q by K we get this
matrix which is each word with each other word.
It's a sequence by sequence matrix and if we don't want some words to interact with
other words we basically replace their value so their attention score with something that
is very small before we apply the softmax and when we apply the softmax these values
will become zero because as you remember the softmax on the numerator has e to the power
of x so if x goes to minus infinity so very small number e to the power of minus infinity
will become very small so very close to zero.
So basically we hide the attention for those two words so this is the job of the mask.
Just following my slide we do the multiplication one by one so as we remember we calculate
with first the query are multiplied by the WQ so self.WQ multiplied with the query gives
us a new matrix which is called the Q prime in my slides I just call it query here.
We do the same with the keys and the same with the values let me also write the dimensions
so we are going from batch sequence length to demodel with this multiplication we are
going to another matrix which is batch sequence length and demodel and you can see that from
the slides so when we do sequence by demodel multiplied by demodel by demodel we get
a new matrix which has the same dimension as the initial matrix so sequence by demodel
and it's the same for all three of them.
Now what we want to do is to we want to divide this query key and value into smaller matrices
so that we can give each small matrix to a different head so let's do it we will divide
into using the view method of PyTorch which means that we keep the batch dimension because
we don't want to split the sentence we want to split the embedding into h parts.
We also want to keep the second dimension which is the sequence because we don't want
to split it and the third dimension so the demodel we want to split it into two smaller
dimension which is h by dk so self.h self.dk as you remember dk is basically demodel
by divide by h so this multiplied by this gives you back gives you a demodel and then
we transpose one two why do we transpose because we prefer to have the h dimension instead
of being the third dimension we want it to be the second dimension and this way each
view h head we will see the all the sentence so we will see this dimension so the sequence
line by dk let me also write the comment here so we are going from batch sequence length
demodel to batch sequence length h dk and then by using the transposition we are going
to batch h sequence length and dk this is really important because we want we want each
batch have we want each head to watch this stuff so the sequence length by dk which means
that each head we will see the full sentence so each word in the sentence but only a smaller
part of the embedding we do the same thing for the key and the value
okay
now that we have the smaller matrices so let me go back to the slide so I can show you where we are
so we did this multiplication we obtained query key and values we split into smaller matrices
now we need to calculate the attention using this formula here before we can calculate the
attention let's create a function to calculate the attention so if we create a new function
that can be used also later so self attention let's define it as a static method
so static method means basically that you can call this function without having an instance
of this class you can just say multi head attention block dot attention instead of having an instance
of this class we also give him the dropout layer okay what we do is we get the decay
what is the decay is the last dimension of the query key and the value
and we will using this function here let me first call it so that you can understand how we will
use it and then we define it so we want from this function we want to think the output and we want
the attention scores so the output of the softmax attention scores and we will call it like this
so we give it the query the key the value the mask and the dropout layer now let's go back here
so we have the decay now what we do is first we apply the first part of the formula that is
the query multiplied by the transpose of the key divided by the square root of decay
so these are our attention scores
query matrix multiplication so this add sign means matrix multiplication in pytorch
we transpose the last two dimensions minus two minus one means transpose the last two dimensions
so this will become the last dimension is a sequence by sequence length by decay it will
become decay by sequence length and then we divide this by mat dot decay we before as we saw before
applying the softmax we need to apply the mask so we want to hide some interaction between words
we apply the mask and then we apply the softmax so the softmax will take care of the values that
we replaced how do we apply the mask we just all the values that we want to mask we replace them
with very very small values so that the softmax will replace them with zero so if a mask is defined
apply it
this means basically replace all the values for which this statement is true with this value
okay the mask we will define in such a way that where this value this expression is true if we
want it to be replaced by this later we will see also how we we will build the mask for now just
take it for granted that these are all the all the values that we don't want to have in the
attention so we don't want for example some word to watch future words for example when we will
build the decoder or we don't want the padding values to to interact with other values because
they are just filler words to reach the sequence length we will replace them with minus one to
the power of minus ten to the power of nine and which is a very big number in the negative
range and which basically represents minus infinity and then when we apply now the softmax it will
be replaced by zero we applied to this dimension okay let me write some comments so in this case
we have a batch by h so each head will and then sequence length and sequence length
all right if we also have a dropout so if dropout is what we also apply the dropout
and finally as we saw in the original slide we multiply this the output of the softmax by the
v matrix matrix multiplication so we return attention scores multiplied by value and also
the attention score itself so why are we returning a tuple because we want this of course we need it
for the model because we need to give it to the next layer but this will be used for visualization
so the output of the self self attention so the multi head attention in this case is actually
going to be here and we will use it for visualizing so for visualizing what is the score given by the
model for that particular interaction let me also write some comments here so here we are doing like
this batch
and let's go back here so now we have our multi head attention so the output of the multi head
attention what we do is finally we okay let's go back to the slide first where we are we calculated
these smaller matrices here so we applied the softmax q by kt divided by the square root of dv
and then we multiplied it also by v we can see it here which gives us this small matrix here head one
head two head three and three four now we need to combine them together concat just like the
formula says from the paper and finally multiplied by wo so let's do it
we transpose because before we transform the matrix into sequence length by we had the sequence
length as the third dimension we want it back in the first place to combine them because the
resulting tensor we want the sequence length to be in the second position so let me write it first
what we want to do batch we started from this one sequence length first we do a transposition
and then what we want is this
so this transposition takes us here and then we
we do a view but we cannot do it we need to use contiguous this means basically that
pytorch to to transform the shape of a tensor needs to we put the memory to be contiguous so he can
just do it in place
minus one and the self dot h multiplied by self dot dk which as you remember this is the
demodel because we defined dk to be here demodel by h divide by h
okay and finally we multiply this x by wo which is our output matrix
of x this will give us we go from batch
to
and this is and this is our multi head attention block we have I think all the ingredients now
to combine them all together we just miss one small layer let's go have a look at it first
there is one last layer we need to build which is the connection we can see here for example here
we have some output of this layer so add a norm that is taken here with this connection
and this one part is sent here then the output of this is sent to the add a norm and then
combined together by this layer so we need to create this the layer that manages this skip
connection so we take the input we give it to we skip it by one layer we take the output of
the previous layer so in this case the multi head attention we give it to this layer but
also combining with this part so let's build this layer we I will call it a residual connection
because it's basically a skip connection okay let's build this residual connection
as usual we define the constructor and in this case we just need a dropout
as you remember the the skip connection is between the add and the norm and the previous
layer so we also need a norm which is our layer normalization which we defined before
and then we define the forward method so
and the sub layer which is the previous layer
what we do is we take the x and we combine it with the output of the next layer which is in
this case is called sub layer and we apply the dropout
so this is the definition of add and a norm actually there is a slight difference that
we first apply the normalization and then we apply the sub layer in the case of the paper
they apply first the sub layer and then the normalization I saw many implementation and most
of them actually did it like this so we will also stick with this particular as you remember
we have these blocks are combined together by this bigger block here and we have n of them
so this big block we will call it encoder block and each of this encoder block is repeated
n times where the output of the previous is sent to the next one and the output of the last one
is sent to the decoder so we need to create this block which will contain one multi head
attention to add and norm and one feed forward so let's do it
we will call this block the encoder block because the decoder has three blocks inside the encoder has
only two and as I so before we have the self-attention block inside which is the multi head
attention we call it self-attention because in the case of the encoder it is applied to the
same input with three different roles the role of query of the key and the value
which is our feed forward and then we have a drop out which is a floating point and then we define
the two residual connections
we use the model list which is a way to organize a list of modules in this case we need two of them
okay let's define the forward method
I define the source mask what is the source mask is the mask that we want to apply to the input of
the encoder and why do we need a mask for the input of the encoder because they we want to hide the
interaction of the padding word with the other words we don't want the padding word to interact
with other words so we apply the mask and let's do the first residual connection let's go back
to check the video actually to check the slide so we can understand what we are doing now so
the first skip connection is this x here is going to here but before it's added and with add a norm
we first need to apply the multi head attention so we take this x we send it to the multi head
attention and at the same time we also send it here and then we combine the two
so the first skip connection is between x and then the other x is coming from the self-attention so
this is the function so I define the sublayer uh usually using a lambda so this basically means
first apply the self-attention self-attention in which we give the query key and the value is
our x so our input so this is why it's called self-attention because the role of the query key
and the value is x itself so the input itself so it's the sentence that is watching itself so each
word of one sentence is interacting with other words of the same sentence we will see that in
the decoder it's different because we have the cross attention so the keys coming from the decoder
are watching the sorry the query coming from the decoder are watching the key and the values coming
from the encoder we give it the source mask so what is this basically we are calling this function
the forward function of the multi head attention block so we give query key value and the mask
this will be combined with this by using the residual connection
then again we do the second one the second one is the feed forward
I don't need lambda here actually
and then we return x so this means combine the the feed forward and then the x itself
so the output of the previous layer which is this one and then apply the residual connection
and this defines our encoder block now we can define the encoder object so because the encoder
is made up of many encoder blocks we can have up to n of them according to the paper so let's
define the encoder how many layers we will have we will have n so we have many layers
and they are applied in one after another so this is a model list
and at the end we will apply a layer normalization
so we apply one layer after another
the output of the previous layer becomes the input for the next layer here I forgot something
and finally we apply the normalization and this concludes our journey around the encoder
let's go have a brief overview of what we have done we have taken the inputs send it to the
we didn't okay we didn't combine all the blocks together for now we just built this big block
here control called encoder which contains two smaller blocks that are the skip connection
the skip connection first one is between the multi head attention and this x that is sent here
the second one is between this feed forward and this x that is sent here we have n of these blocks
one after another the output of the last will be sent to the decoder before but before we apply
the normalization now we will we built the the the decoder part now in the decoder the output
embeddings are the same as the input embeddings I mean the the the class that we need to define
is the same so we will just initialize it twice and the same goes for the positional encodings
we can use the same values that we use for the encoder also for the decoder what we need to define
is this big block here which is made of masking multi head attention add a norm so one skip connection
here another multi head attention with another skip connection and the feed forward with the
skip connection here the way we define the multi head attention class actually already
takes into consideration the mask so we don't need to reinvent the wheel also for the decoder
we can just define the decoder block which is this big block here made of three sub layers
and then we built the decoder using this n number of these decoder blocks so let's do it
let's define first the decoder block
in the decoder we have the self-attention which is let's go back this is a self-attention because
we have this input that is used three times in the masking multi head attention so this is called
self-attention because the same input plays the role of the query the key and the values
which means that the same sentence is each word in the sentence is matched with each other word
in the same sentence but in this part here we will have an attention calculated using the
query coming from the decoder while the key and the values will come from the encoder so this is
not a self-attention this is called cross-attention because we are crossing two kind of different
objects together and matching them somehow to calculate the relationship between them okay let's
define this is the cross-attention block which is basically the multi head attention but we will
give it the different parameters is our feed forward and then we have a dropout
okay we define also the residual connection in this case we have three of them
wonderful okay let's build the forward method which is very similar to the encoder with a
slight difference that I will highlight we need x what is x is the input of the decoder
but we also need the output of the encoder we need the source mask which is the mask applied
to the encoder and the target mask which is the mask applied to the decoder why they are called
source mask and target mask because in this particular case we are dealing with a translation
task so we have a source language in this case it's english and we have a target language which
in our case is italian so you can call it encoder mask or the coder mask but basically we have two
masks one is the one coming from the encoder one is the one coming from the decoder so in our case
we will call it source so the source mask is the one coming from the encoder so the source language
and the target mask is the one coming from the decoder so the target language
and just like before we calculate the self-attention first which is the first part of the decoder block
in which the query the key and the values are the same input but with the mask of the decoder
because this is the self-attention block of the decoder
and then we need to combine the we need to calculate the cross-attention which is our
second residual connection
we give him okay in this case we are giving the query coming from the decoder so the x
the key and the values coming from the encoder
and the mask of the encoder
and finally the feed forward block just like before
and that's it we have all the ingredients actually to build the decoder now which is
just n times this block one after another just like we did for the encoder
also in this case we will provide with many layers so layers
so just a model list and we will also have a normalization at the end
just like we did before we apply the the input to the to one layer and then we use
the use the output of the previous layer and give it as a input of the next layer so
the
each layer is the decoder block so we need to give it x we need to give it the encoder output
then the source mask and the target mask so each of them is this we are calling the forward
method here so nothing different and finally we apply the normalization
and this is our decoder there is one last ingredient we need to to have what is a full
transformer so let's have a look at it the last ingredient we need is this layer here the linear
layer so as you remember from my slides the output of the multi hat attention is something that is
sequenced by d model so here we expect to have the output to be sequenced by d model if we don't
consider the batch dimension however we want to map this word into the back into the vocabulary
so that's why we need this linear layer which will convert the embedding into a position of the
vocabulary i will call this layer called the projection layer because it's projecting the
embedding into the vocabulary let's go build it what we need for this layer is the d model
so the d model which is an integer and the vocabulary size
but this is basically a linear layer that is converting from d model to vocabulary size so
thought projection layer is
let's define the forward method
okay what we want to do let me write this little comment we want to batch
sequence length to d model converted into batch sequence length vocabulary size
and in this case we will also already apply the softmax and actually we will apply the log
softmax for numerical stability like i showed before
to the last dimension
and that's it this is our projection layer now we have all the ingredients we need for
the transformer so let's define our transformer block
in the transformer we have an encoder
which is our encoder we have a decoder which is our decoder we have a source embedding
why we need the source embedding at the target embedding because we are dealing with multiple
languages so we have one input embedding for the source language and one input embedding for the
target language and we have the target embedding
then we have the source position and the target position
which will be the same actually and then we have the projection layer
projection
we just save this
now we define three methods one to encode one to the code and one to project
we will apply them in succession why why we don't just build one forward method because
as we will see during inferencing we can reuse the output of the encoder we don't need to
calculate it every time and also we prefer to keep the this output separate also for visualizing
the attention so for the encoder we have the source of the because we have the source language
and the source mass so the so what we do is we apply first the embedding
then we apply the positional encoding
and finally we apply the encoder
then we define the decode method
which takes the encoder output which is a tensor
the source mask which is a tensor the target and the target mask
and what we do is target we first apply the target embedding to the target sentence
then we apply the positional encoding to the target sentence
and finally with the code
this is basically the method the forward method of this decoder so we have the same order of
parameters yes finally we define the project method
in which we just apply the projection so we take from the embedding to the vocabulary size
okay this is also the this is the last block we had to build
but we didn't make a method to combine all these blocks together so we built many blocks
we need one that given the hyperparameters of the transformer builds for us one single
transformer in initializing all the encoder the encoder the embeddings etc so let's build this
function let's call it build transformer that given all the hyperparameters will build the
transformer for us and also initialize the parameters with some initial values
what we need to define a transformer for sure in this case we are talking about translation
okay this model that we are building we will be using for translation but you can use it for any
task so the naming I'm using are basically the ones used in the translation task later you can
change the naming but the structure is the same so you can use it for any other task for which the
transformer is applicable so the first thing we need is the vocabulary size of the source and the
target because we need to build the embedding the the embedding because the embedding need to convert
the from the the token of the vocabulary into a vector of size 512 so it needs to know how much
how big is the vocabulary so how many vectors it needs to create
then the target
which is also an integer then we need to tell him what is the source sequence length and the target
sequence length this is very important they could also be the same in our case it will be the same
but they can also be different for example in case you are using the transformer that is dealing
with the two very different languages for example for translation in which the tokens needed for
the source languages language are much higher or much lower than the other one so you don't need
to keep the same length you can use different lengths the next hyper parameter is the demodel
which we initialize with 512 because we want to keep the same values as the paper then we
define the hyper parameter n which is the number of layers so the number of encoder blocks and the
number of decoder blocks that we will be using is according to the paper is 6 then we define the
hyper parameter h which is the number of heads we want and according to the paper it is 8 the
dropout is 0.1 and finally we have the hidden layer dff of the feed forward layer which is 2048
as we saw before on the paper and this builds a transformer okay so first we do is we create
the embedding layers so source embedding
then the target embedding
then we create the positional encoding layers
we don't need to create two positional encoding layers because actually they do the same job
they and we they also don't add any parameter but because they have the dropout and also because
I want to make it verbal so you can understand each part without making any optimization I think
actually it's it's fine because this is for educational purpose so I don't want to optimize
the code I want to make it as much comprehensible as possible so I do every part I need I don't take
shortcuts
and then we created the encoder blocks we have n of them so let's define
let's create an empty array so we have n of them so each encoder block has a self attention
so encoder self attention which is a multi head attention block the multi head attention
requires the demodel the edge and the dropout value then we have a feed forward block
as you can see also the name I'm using are quite long mostly because I want to make it as
comprehensible as possible for everyone so each encoder block is made of a self attention
and a feed forward
and finally we tell him how much is the dropout
finally we add this encoder block
and then we can create the the code blocks
so
we also have the cross attention for the decoder block
we also have the feed forward just like the encoder
then we define the decoder block itself which is the decoder block
cross attention and finally the feed forward
and the dropout
and finally we save it in its array
we now can create the encoder and the decoder
we give him all his blocks which are n and then also the decoder
and we create the projection layer
which will convert the model into vocabulary size which vocabulary of course the target
because we want to take from the source language to the target language so we want to project
our output into the target vocabulary and then we build the transformer
what does it need it needs an encoder a decoder source embedding
target embedding
then source positional encoding target positional encoding
and finally the projection layer
and that's it now we can just initialize the parameters
using the Xavier uniform this is a way to initialize the parameters to make the training
faster so they don't don't just start with random values
and there are many algorithms to do it I saw many implementations using Xavier so I think it's a
quite good start for the model to learn from
and finally return our beloved transformer and this is it this is how you build the model
and now that we have built the model we will go further to use it so we will create the
we will first have a look at the data set then we will build the training loop
after the training loop we will also build the inferencing part and the code for visualizing
the attention so hold on and take some coffee take some tea because it's gonna be a little long
but it's gonna be worth it now that we have built the the code for the model our next step
is to build the training code but before we do that we first I let's recheck the code because
we may have some typos I actually already made this check and there are few mistakes
in the code I compared the old with the new one it is very minor problems so we wrote feed
forward instead of feed forward here and so the same problem is also present every in every
reference to feed forward and also here when we are building the decoder block and the other
problem is that here when we build the decoder block we just wrote nn.module instead it should
be nn.module list and then the feed forward should be also fixed here and here in the
build the transformer method now I can delete the old one so we don't need it anymore let me
check the model it's the correct one with feed forward yes okay our next step is to build the
training code but before we build the training code we have to look at the data what kind of data
are we going to work with so as I said before we are dealing with a translation task and I have chosen
this data set called opus books which we can find on hugging face and we will also use the library
from hugging face to download this data set for us and this is the only library we will be using
beside PyTorch because we of course we cannot reinvent the data set by ourselves so we will
use this data set and we will also use the hugging face tokenizer library to transform this text
into vocabulary because our goal is to build the transformer so not to reinvent the wheel
about everything so we will be only focusing on building and training the transformer and
in my particular case I will be using the subset English to Italian but we will build the code in
such a way that you can choose the language and the code will act accordingly if we look at the
data we can see that each data item is a pair of sentences in English and in Italian for example
there was no possibility of taking a walk that day which in Italian means in quel giorno era impossibile
passeggiare so we will train our transformer to translate from the source language which is
English into the target language which is Italian so let's do it we will do it step by step so first
we will make the code to download this data set and to create the tokenizer so what is the tokenizer
let's go back to the slides so just have a brief overview of what we are going to do with this data
the tokenizer is what comes before the input embeddings so we have an English sentence so for
example your cat is a lovely cat but this sentence will come from our data set the goal of the
tokenizer is to create this token so split this sentence into single words which has many strategies
as you can see here we have a sentence which is your cat is a lovely cat and the goal of the
tokenizer is to split this sentence into single words which can be done in many ways there is the
bpe tokenizer there is the word level tokenizer there is the sub word level word part tokenizer
there are many tokenizers the one we will be using is the simplest one called the word level
tokenizer so the word level tokenizer basically will split this sentence let's say by space so
each space defines the boundary of a word and so into the single words and each word will be mapped
to one number so this is the job of the tokenizer to build the vocabulary and of these numbers and
to map each word into a number the when we build the tokenizer we can also create special tokens
which we will use for the transformer for example the tokens called padding they call the token called
the start of sentence end of sentence which are necessary for training the transformer but we
will do it step by step so let's build first the code for the building the tokenizer and to
download the data set okay let's create a new file let's call it train dot i okay let's import our
usual um library so torch we will also import torch dot nm and we also because we we are using a
library from hugging face we also need to uh import the these two libraries we will uh using the we
will be using the data sets library which you can install using pip so data sets um actually
we will be using a load data set and we will also use we'll be using the tokenizers library also
from hugging face which you can install with pip we also need the which tokenizer we need so
we are using we will use the word level tokenizer
and there is also the trainers so the the the tokenizer the the class that will train the
tokenizer so that will create the vocabulary given the list of sentences
and we will split the word according to the white space
i will build one method by one by one so i will build first the methods to create the tokenizer
and i will describe each parameter for now you will not have the bigger picture but later when
we combine all these methods together you will have the bigger picture so let's first make the
method that builds the tokenizer so we will call it get or build tokenizer
and this method takes the configuration which is the configuration of our model we will define it
later the data set and the language for which we are going to build the tokenizer
we define the tokenizer path so the file where we will be saved this tokenizer
and we do it path of config
okay let me define some things first of all this path is coming from the pathlib so from pathlib
this is a library that allows you to create absolute path given relative paths and we pretend
where that we have a configuration called the tokenizer file which is the path to the tokenizer
file and this path is formatable using the language so for example we can have something like this
for example
something like this and this will be
given the language it will create a tokenizer english or tokenizer italian for example
so if the tokenizer doesn't exist we create it
i took all this code actually from hugging phase there is it's nothing complicated i just
taken their quick tour of their tokenizers library and it's really easy to use it so
and saves you a lot of time because tokenizer to build the tokenizer is really reinventing the wheel
and we will also introduce the unknown word uh unknown so what does it mean if our tokenizer
sees a word that it doesn't recognize in its vocabulary it will replace it with this word
unknown it will map it to the number corresponding to this word unknown
the pre tokenizer means basically that we split by white space
and then we train we build the trainer to train our tokenizer
okay this is the trainer what does it mean it means it will be a word level trainer so it
will split words using the white space and using the single words and it will also have
four special tokens one is unknown which means that if you cannot find that particular word in
the vocabulary just replace it with unknown it will also have the padding which we will use to
train the to train the transformer the start of sentence and the end of sentence special tokens
mean frequency means that a word for a word to appear in our vocabulary it has to have a frequency
of at least two now we can train the tokenizer
we use this method which means we build first a method that gives all the sentences from our
data set and we will build it later
okay so let's build also this method called get all sentence so that we can iterate through the
data set to get all the sentences corresponding to the part the particular language for which we
are creating the tokenizer as you remember each item in the data set it's a pair of sentences one
in English one in Italian we just want to extract one particular language
this is the item representing the pair and from this pair we extract only the one language that we
want and this is the code to build the tokenizer now let's write the code to load the data set
and then to build the tokenizer we will call this method get data set and which also takes
the configuration of the model which we will define later so let's load the data set we will call it
ds row okay hugging face allow us to download its data sets very easily we just need to tell him
what is the name of the data set and then tell him what is the subset we want we want the subset
that is English to Italian but we want to also make it configurable for you guys to change the
language very fast so let's build this subset dynamically we will have two parameters in the
configuration one is called language source and one is called language target later we can also
define what split we want of this data set in our case there is only the training split
in the original data set from hugging face but we will split by ourselves into the validation
and the training data so let's build the tokenizer
this is the row data set
and we also have the target
okay now because we only have the training split from hugging face we can split it by
by ourselves into a training and the validation we keep 90% of the data for training and 10% for
validation
okay
the method random split allows it's a method from PyTorch that allows to split a data set using
the size that we give as input so in this case it means split this data set into this two smaller
data set one of this size and one of this size but let's import the the the method from Torch
let's also import the one that we will need later
put a loader
and random split
now we need to create the data set the data set that our model will use to access the 10 source
directly because now we just created the tokenizer and we just loaded the data but we need to create
the 10 source that our model will use so let's create the data set let's call it
bilingual data set and for that we create a new file
also here we import Torch and that's it
we will call the data set we will call it bilingual data set
okay as usual we define the constructor
and in this constructor we need to give him the data set downloaded from hugging face
the tokenizer of the source language the tokenizer of the target language the source language the name
of the source language the name of the target language and the sequence length that we will use
okay we save all these values
we can also save the the tokens the particular tokens that we will use to create the
10 source for the model so we need the start of sentence end of sentence and the padding token
so how do we convert the token start of sentence into a number into the input ID
there is a special method of the tokenizer to do that so let's do it
so this is the start of sentence token we want to build it into a tensor
this tensor will contain only one number which is given by we can use this tokenizer from the
source or the target it doesn't matter because they both contain these particular tokens
this is the method to convert into the token into a number
so start of sentence and the type of this token of this tensor is
we want it long because the vocabulary can be more than 32 bit long
the vocabulary size so we usually use the long 64 bit and we do the same for the
end of sentence and the padding token
we also need to define the length method of this dataset
which tells the length of the the dataset itself so basically just the length of the dataset from
hugging face and then we need to define the get item method
okay first of all we will extract the original pair from the hugging face dataset
then we extract the source text and the target text
so
and finally we convert each text into token into tokens and then into input IDs
uh what does it mean we will first the tokenizer will first split the sentence into single words
and then we'll map each word into its corresponding number in the vocabulary
and it will do it in one pass only
this is done by the encode method
this gives us the input IDs so the numbers corresponding to each word
in the original sentence and it will be given as an array
we did the same for the decoder now as you remember we also need to pad
the sentence to reach the sequence length this is really important because we we want our model
to always work I mean the model always works with a fixed length sequence length but we
don't have enough words in every sentence so we use the padding token so this pad here as the
padding token to fill the sentence until it reaches the sequence length so we calculate
how many padding tokens we need to add for the encoder side and for the decoder side
which is basically how many we need to reach the sequence length
so we already have this amount of tokens we need to reach this one but we will add also the start
of sentence token and the end of sentence token to this to the encoder side so we also have minus
two here and here only minus one if you remember my previous video when we do the training we add
only the start of sentence token to the decoder side and then in the label we only add the end
of sentence token so in this case we only need to add one token special token to the sentence
we also make sure that this sequence length that we have chosen is enough to represent all the
sentences in our dataset and if we choose a two small one we want to throw a raise an exception
so if so basically this number of padding tokens should never become negative
okay now let's build the the two tensors for the encoder input and for the decoder input but
also for the label so one sentence will be sent to the input of the encoder one sentence will be
sent to the input of the decoder and one sentence is the one that we expect as the output of the
decoder and that output we will call label usually it's called target or label i call it label
we can cut the tensor of the start
okay we can cut three tensors first is this start of sentence token
then this the tokens of the source text
then the end of sentence token
and then enough padding tokens to reach the sequence length we already calculated how many
embedding tokens we need to add to this sentence so let's just do it
okay
and this is the encoder input so let me write some comment here this is add s os and s to the source
text then we build the decoder input
which is also concatenation of tokens in this case we don't have the start of sentence we just have
the we don't have the end of sentence we just have the start of stent sentence
and finally we add the embed enough padding tokens to reach the
sequence length we already calculated how many we need just use this value now
and then we build the label
in the label we only add the end of sentence token
let me copy it's faster
yeah because we need the same number of padding tokens as for the decoder input
and let's double just for debugging let's double check that we actually reach the sequence length
okay now that we have made this check let me also write some comments here here we are only adding
eos no here s os to the decoder input and here is
add eos to the label what we expect as output from the decoder now we can return all these
tensors so that our training can use them we return a dictionary comprised of encoder input
what is the encoder input it's basically off-size sequence length then we have the the encoder input
which is also just a sequence length number of tokens
I forgot a comma here
and then we have the encoder mask so what is the encoder mask as you remember
over we are increasing the size of the encoder input sentence by adding padding tokens
but we don't want these padding tokens to participate in the self-attention so what we need
is to build a mask that says that we don't want these tokens to be seen by the self-attention
mechanism and so we build the mask for the encoder how do we build this mask we just say that
all the tokens that are not padding are okay all the tokens that are padding are not okay
we also on squeeze to add this sequence dimension and also to add the batch dimension later
and we convert into integers so this is one one sequence length
because this will be used in the self-attention mechanism
however for the decoder we need a special mask that is a causal mask which means that
each word can only look at the previous words and each word can only look at not non-padding
words so we don't want again we don't want the padding tokens to participate in the self-attention
we only want real words to participate in this and we also don't want each word to watch at
words that come after it but only that words come come before it so I will use a method here
called causal mask that will build it later we will build it also so now I just call it to
show you how it's used and then we will proceed to build it
also in this case we don't want the padding tokens
and we add the necessary dimensions and also we do boolean end with causal mask which is a
method that we will build right now and this causal mask needs to build a matrix of size
sequence length to sequence length what is sequence length is basically the size of our decoder input
and this let me write a comment for you so this is one two sequence length
combine with so the end with one sequence length sequence and and this can be broadcasted
let's go define this method causal mask so what is causal mask
causal mask basically means that we want let's go back to the slides actually
as you remember from the slides we want each word in the decoder to only watch
words that come before it so what we want is to make all these values above this diagonal
that represents the multiplicate this matrix represents the multiplication of the
queries by the keys in the self-attention mechanism what we want is to hide all these
values so your cannot watch the word cat is a lovely cat it can only watch itself but this word
here for example this word lovely can watch everything that comes before it so from your
up to lovely itself but not the word cat that comes after it so what we do is we want all
these values here to be musket out so which also means that we want all the values above this
diagonal to be musket out and there is a very practical method in pytorch to do it so let's do
it let's go build let's go build this method so the mask is basically torch.triu which means
give me the every value that is above the diagonal that i am telling you so we want a matrix
which matrix matrix made of all ones
and this method will result will return every value above the diagonal and everything else
will become zero so we want diagonal one type we want it to be integer
and what we do is return mask is equal to zero so this will return all the values above the
diagonal and everything below the diagonal will become zero but we want actually the opposite
so we say okay everything that is zero should will become true with this expression and everything
that is not zero will become false so we apply it here to build this mask so this mask will be
one by sequence length by sequence length which is exactly what we want okay let's add also the
label the label is also i forgot the comma
sequence length and then we have the source text just for visualization we can send it
source text and then the target text
and this is our dataset now let's go back to our training method to continue writing
the training loop so now that we have the dataset we can create it we can create two
dataset one for training one for validation and then we send it to a data loader and finally to
our training loop we forgot to import the dataset so let's import it here
we'll import the causal mask which we will need later
what is our source language it's in the configuration
what is our target language
and what is our sequence length is also in the configuration
we do the same for the validation
but the only difference is that we use this one now and the rest is same
we also just for choosing the max sequence length we also want to watch what is the maximum
length of each sentence in the source and the target for each of the two splits that we created
here so that if we choose a very small sequence length it we will know so
basically we do i load each sentence from each language from the source and the target language
i convert into ids using the tokenizer and i check the length if the length is
let's say 180 we can choose 200 as sequence length because it will cover all the possible
sentences that we have in this dataset if it's let's say 500 we we can use 510 or something
like this because we also need to add the start of sentence and end of sentence
sentence tokens to these sentences
this is the source ids
then let's create also the target ids and this is the language of target
and then we just say the source maximum length is the maximum of the
and the length of the current sentence the target is the target and the target ids
then we print these two values
we also do it for the target
and that's it now we can proceed to create the data loaders
we define the batch size according to our configuration which we still didn't define but
you can already guess what are its values we want it to be shuffled
okay for the validation i will use a batch size of one because i want to process each sentence one
by one and this method returns the the data loader of the training the data loader of the
validation the tokenizer of the source language and the tokenizer of the target language
now we can start building the model so let's define a new method called getModel
which will according to our configuration our vocabulary size
is
build the model the transformer model so the model is
we didn't import the model so let's import it
build transformer
what is the first the source vocabulary size
and the target vocabulary size
and then we have the sequence length
and we have the sequence length of the source language and the sequence length of the target
language we will use the same for both and then we have the demodel
which is the size of the embedding we can keep all the rest the default as in the paper
if the model is too big for yours the GPU to be trained on you can try to reduce the number of
heads or the number of layers of course it will impact the performance of the model but i think
given the data set which is not so big and not so complicated it should not be a big problem
because we are not building a huge data set anyway okay now that we have the model we can start
building the training loop but before we build the training loop let me as define this configuration
because it keeps coming and i think it's better to define the the the structure now so let's
create a new file called config.py in which we define two methods one is called getConfig and
one it's to map to get the the the path where we will save the weights of the model
okay let's define the batch size
i choose 8 you can choose something bigger if your computer allows it the number of epochs for
which we will be training i would say 20 is enough the learning rate i am using 10 to the power of
minus 4 you can use other values i thought i thought this learning rate is reasonable
i it's possible to change the learning rate during training
actually it's quite common to give a very high learning rate and then reduce it gradually
with every epoch we will not be using it because it will just complicate the code a little more and
this is not actually the goal of this video the goal of this video is to teach how the transformer
works i have already checked the sequence length that we need for this particular data set from
English to Italian which is 350 is more than enough and the demodel that we will be using is the default
of 512 the language source is English so we are going from English the language target is
Italian we are going to translate into Italian we will save the model into the folder called weights
in the file name of which model will be t-model so transform a model
i also built the code to preload the model in case we want to restart the training after maybe it's crash
and this is the tokenizer file so it will be saved like this so tokenizer n and tokenizer
it according to the language and this is the experiment name for tensorboard on which we will
save the the losses while training i think there is a comma here okay now let's define
another method that allows to find the path where we need to save the weights
why i'm creating such a complicated structure is because
we i will provide also notebooks to run this training on google colab so we just need to
change these parameters to make it work on google colab and save the weights directly on your google
drive i have already created actually this this code and it will be provided on github and i will
also provide the link in the video
okay the file is built according to
model this name then the epoch dot pt
let's import also here the path library
okay now let's go back to our training loop okay we can build the training loop now finally so
the training model given the configuration okay first we need to define which device on which
we will put all the tensors so define the device
if i have to go down my computer so
okay then we also print
we make sure that the weights folder is created
okay
and then we load our dataset you can just take these values here and say it's equal to
get ds of config
we create also the model
to get the vocabulary size there is method called get pocab size
and i think we don't have any other parameter and finally we transfer the model to our device
we also start tensorboard tensorboard allows to visualize the loss the the graphics the charts
let's also import tensorboard
okay
let's go back let's also create the optimizer i will be using the adam optimizer
okay since we also have the configuration that allow us to resume the training in case
the model crashes or something crashes let's implement that one and that will allow us to
restore the state of the model and the state of the optimizer
okay
let's implement import this method we defined in the dataset
okay
okay
we load the file
okay
okay
here we have typo okay the loss function we will be using is the cross entropy loss
okay
we need to tell him what is the ignore index so we don't we want him to ignore the padding token
basically we don't want the loss to the padding token to contribute to the loss
and we also will be using label smoothing label smoothing basically allows us our model to
be less confident about its decision so how to say imagine our model is telling us to choose
the word number three and with a very high probability so what we will do with labels
booting is take a little percentage of that probability and distribute to the other tokens
so that our model becomes less sure of its choices so kind of less overfit and this actually
improves the accuracy of the model so we will use a label smoothing of 0.1 which means from every
highest probability probability token take 0.1 percent of score and give it to the others
okay let's build finally the training loop
we tell the model to train
I build a batch iterator for the for the data loader using tkodm which will show a very nice
progress bar
and we need to import tkodm
okay finally we get the tensors the encoder input
what is the size of this tensor it's batch to sequence length the decoder input
is batch of decoder input and we will also move it to our device batch to sequence length
we get the two masks also
this is the size and then the decoder mask
okay why these two masks are different because in the one case we are only telling him to
hide only the padding tokens in the other case we are also telling him to hide all these
subsequent words for each word to hide all the subsequent words to mask them out
okay now we run the let's make some run the tensors through the transformer
so first we calculate the output of the encoder
and we encode using what the encoder input and the mask of the encoder
then we calculate the decoder output
using the encoder output the source the mask of the encoder
then the decoder input and the decoder mask
okay as we know this the result of this so the output of the model dot encode
will be a batch sequence length demodel also the output of the decoder will be
batch sequence length demodel but we want to map it back to the vocabulary so we need the
projection so let's get the projection output
and this will produce a b so batch sequence length and target vocabulary size
okay now that we have the output of the model we want to compare it with our label so first
let's extract the label from the batch
and we also put it on our device so what is the label it's b so batch to sequence length
in which each position tell so the label is already for each um b and sequence length so
so for each dimension tells us what is the position in the vocabulary of that particular word
and then we want these two to be comparable so we first need to compute the loss
into this i show you now projection output view minus one
okay what does this do this basically transforms the i show you here
this size into this size b multiplied by sequence length and then target vocabulary size
okay because we want to compare it with this this is how the cross entropy wants the
tensors to be
and also the label
okay now we can we have calculated the loss we can update our progress bar this one with
the loss we have calculated
okay
and this is this will show the loss on our progress bar we can also log it on tensor board
okay
let's also flush it okay now we can back propagate the loss so lost our backward
and finally we update the weights of the model so that is the job of the optimizer
and finally we can zero out the the grad
and remove the global step by one the global step is being used mostly for tensor board to
keep track of the loss we can save the model
every epoch okay model file name which we get from our special methods this one
we tell him the configuration we have and the name of the file which is the epoch but with zeros
in front and we save our model it is very good idea when we want to be able to resume the training
to also save not only the the state of the model but also the state of the optimizer because the
optimizer also keep tracks of some statistics one for each weight to to understand how to move
each weight independently and usually actually i i saw that the the optimizer the dictionary is quite
big so even if it's big if you want your training to be reasonable you need to save it otherwise
the optimizer will always start from zero and will have to figure out from zero even if you
start from a previous epoch how to move each weight so every time we save some snapshot i always include it
the state of the model
this is all the weights of the model we also want to stay save the optimizer
let's do also the global step
and we want to save all this into the file name so model file name
and that's it now let's build the code to run this so if name
i
really find the warnings frustrating so i want to filter them out because i have some
a lot of libraries especially kuda i really know what's the content and so i don't want to
visualize them every time but for sure for you guys i suggest watching them at least once to
understand if there is any big problem otherwise they're just complaining from kuda
i
okay let's try to run this uh code and see if everything is working fine we should what we
expect is that the code should download the data set the first time then it should create the
tokenizer and save it into its file and it should also start training the model for 30 epochs of
course it will never finish but let's do it let me check again the configuration tokenizer okay let's
run it
okay it's building the tokenizer and we have some problem here sequence length
okay finally the model is training i show you recap you guys what i uh had mistaken
first of all the sequence length was written incorrectly there was uh capital l here and
also in the data set i forgot to save it here and here i had it also written capital i so l was
capital and now the training is going on and as you can see the training is quite fast
or at least on my computer uh actually not so fast but because i choose a batch size of 8
i could try to increase it and it's happening on kuda uh the loss is decreasing and the weights
will be saved here so if we reach the end of the epoch it will create the first uh weight here so
let's wait until the end of the epoch and see if the weight is actually created before uh actually
finishing the training of the model uh let's do another thing uh we also would like to visualize
the output of the model while we are training and this is called validation so we want to check
how our model is evolving while it is getting trained so what we want to build is a validation loop
which will allow us to evaluate the model which also means that we want to inference from this
model and check some sample sentences and see if how they get translated so let's start building
the validation loop the first thing we do is we build a new method called run validation
and this uh method will accept some parameters that we will use for now i just write all of them
and later i explain how they will be used
okay the first thing we do uh to run the validation is we put our mode our model into
evaluation mode so we do model dot eval and this means that this tells PyTorch that we are going
to evaluate our model and then what we will do we will inference uh two sentences and see how they
what is the output of the model
so with torch dot not grad we are disabling the gradient calculation for this uh for every tensor
that we will run inside this width block and this is exactly what we want we just want to
inference from the model we don't want to train it during this loop
so let's get a batch from the validation dataset
because we want to inference only two so we keep account of how many we have
already um processed and we get the input from this current batch i want to remind you that for
the val validation ds we only have a batch size of one
this is the encoder input and we can also get the encoder mask
let's just verify that the the size of the batch is actually one
and now let's go to the interesting part so as you remember when we um calculate the when we
want to inference the model we need to calculate the encoder output only once and reuse it for
every token that we will the model will output from the decoder so let's create another function
that will run the greedy decoding on our model and we'll use and we will see that it will run
the encoder only once so let's call this function greedy decode
okay let's create some tokens that we will need so the SOS token
which is the start of sentence we can get it from either a tokenizer it doesn't matter if it's the
target or the source they both have it
okay and then we do is we pre-compute the encoder output and reuse it
for every token we get from the decoder so
we just give the source and the source mask which is the encoder input and the encoder mask
we can also call it encoder input and encoder mask
then we get the then we okay how do we do the inferencing the first thing we do is we give to
the decoder the start of sentence token so that the decoder will output the first token of the
sentence of the translated sentence then at every iteration just like we saw in my slides
at every iteration we add the previous token to the to the decoder input and so that the
decoder can output the next token then we take the next token we put it again in front of the
input to the decoder and we get the successive token so let's build the decoder input for the
first iteration which is only the start of sentence token
okay
we fill this one with the start of sentence token
and it has the same type as the encoder input
okay now we will keep in asking the decoder to output the next token until we reach either the
end of sentence token or the max length we have defined here so we can do a while through
and then our first stopping condition is if we the decoder output which is becomes the input of
the next step becomes large larger than max length or reaches max length
here why do we have two dimensions one is for the batch and one is for the tokens of the of the
decoder input now we also need to create a mask for this
we can use our function causal mask to say that we don't want the input to watch future words
and we don't need the other mask because here we don't have any padding token as you can see
now we calculate the output
we reuse the output of the encoder for every iteration of the loop
we reuse the source mask so the input the mask of the encoder then we give the decoder input
and along with its mask the decoder mask and then we get the next token
so we get the probabilities of the next token using the projection layer
but we only want the projection of the last token so the next token after the last we have given
to the encoder now we can use the max
so we get the token with the maximum probability this is the greedy search
and then we get this word and we append it back to this one because it will become the input of
the next iteration and we concat so we take the decoder input and we append the next token
so we create another tensor for that
okay
if the next token so if the next word or token is equal equal to the end of sentence token
then we also stop the loop and this is our greedy search now we can just return the output
so the output is basically the decoder input because every time we are appending the next token
to it and we remove the batch dimension so we squeeze it and that's our greedy decoding
now we can use it here in this function so in the validation function so we can finally get the
model output is equal to greedy decode in which we give him all the parameters
and then we want to compare this model output with what we expected so with the label
so let's append all of this so what we give to the input we gave to the model what the model
output the output of the model so they predicted and what we expected as output we save all of this
in these lists and then at the end of the loop we will print them on the console
to get the text of the output of the model we need to use the tokenizer again
to convert the tokens back into text and we use of course the target tokenizer
because this is the target language
okay and now we save them all of this into their respective lists
and we can also print it on the console
while we are using why we are we using this function called print message
and why not just use the print of the python because we are using here in the main loop
in the training loop we are using here tkodm which is our really nice looking progress bar
but it is not suggested to print directly on the console when this progress bar is running
so to print on the console there is one method called print provided by tkodm and we will give
this method to this function so that the output does not interfere with the progress part printing
okay we print some bars
and then we print all the messages
okay
and if we have already processed number of examples then we just break
so why we have created these lists actually we can also send all of this to
to a tensorboard so we can so for example if we have tensorboard enabled we can send
all of this to the tensorboard and to do that actually we need another library that allow
us to calculate some metrics i think we can skip this part but if you are really interested
in the code i published on github you will find that i use this library called torch
metrics that allows to calculate the char error rate and the blue metric which is really useful
for translation tasks and the word error rate so if you're really interested you can find the
code on the github but for our demonstration i think it's not necessary so and actually this
we can also remove it given that we are not doing this part okay so now that we have our run
validation method we can just call it okay what i usually do is i run the validation at every few
steps but because we want to see it as soon as possible what we will do is we will first
run it at every iteration and we also put this model.train inside of this loop so that every
time after we run the validation the model is back into its into its training mode so now we can
just run validation and we give it all the parameter that it needs to to run the validation so give it
a valid model
okay for printing message are we printing any message
we are so let's create a lambda
and we just do
and this is the message to write with the tkodm then we need to give the global step
and the writer which we will not use but okay now i think we can run the training again and see if
the validation works
all right looks like it is working so the model is okay it's running the validation at
every step which is not desirable at all but at least we know that the greedy search is working
and it's not at least looks like it it's working and the model is not predicting anything useful
actually it's just predicting a bunch of commands because it's not training at all but if we train
the model after a while we should see that after a few epochs the model should become better and
better and better so let's stop this training and let's put this one back to where it belongs
so at the end of every epoch here and this one we can keep it here no problem
yeah okay i will now skip fast forward to a model that has been pre-trained i
pre-trained it for a few hours so that we can inference it and we can visualize the attention
i have copied the pre-trained weights that i pre-calculated and i also created this
notebook reusing the functions that we have defined before in the train file the code is very
simple actually i just copied and pasted the code from the train file i just load the model and run
the validation the same method that we just wrote and then i ran the validation on the pre-trained
let's run it again for example and as you can see the model is inferencing 10 examples sentences
and the result is not bad i mean we can see that levin smile levin sorriza levin sorriza it's
matching and most of them matching actually we could also say that it's nearly over fit
for this particular data but this is the power of the transformer i didn't train it for many days
i just trained it for a few hours if i remember correctly and the results are really really good
and now let's write let's make the notebook that we will use to visualize the attention
of this pre-trained model given the file that we built before so train.pi you can also train
your own model choosing the language of your choice which i highly recommend that you change the
language and try to see how the model is performing and try to diagnose why the model is performing
bad if it's performing bad or if it's performing well try to understand how can you improve it
further so let's try to visualize the attention so let's create a new notebook
let's call it let's say attention visualization
okay so the first thing we do we import all the libraries we will need
i will also be using this library called altire it's a visualization library for charts it's
nothing related to deep learning actually it's just a visualization function and in particular
the visualization function actually i found it online it's not written by me just like most of
the visualization functions you can find easily on the internet if you want to build a chart or
if you want to build a histogram etc so i am using this library mostly because i copied the code
from the internet to visualize it but all the rest is my own code so let's import it
okay let's import all of this
and of course you will have to install this particular library when you
run the code on your computer let's also define the device
you can just copy the code from here
and then we load the model which we can copy from here
like this okay let's paste it here and this one becomes
a vocabulary source and a vocabulary target
okay now let's make a function to load the batch
okay
oops
okay
i will convert the batch into tokens now using the tokenizer
okay
and of course for the decoder we use the target vocabulary
so the target tokenizer
okay
so let's just infer
using our greedy decode algorithm so we acquired the model
okay
we return all this information
okay now i will build the necessary functions to
visualize the visualize the attention i will copy some functions from another file because
actually what we are going to build is nothing interesting from a learning point of view for
with regards to the deep learning it's mostly functions to visualize the data so i will copy
because it's quite long to write and the salient part i will explain of course
and this is the function okay what does this function do basically we have the attention
that we will get from the encoder how to get the attention from the encoder
for example the attention we have in three positions first is in the encoder the second
one is in the encoder at the beginning of the decoder so the self-attention of the decoder
and then we have the cross-attention between the encoder and the decoder
so we can visualize three types of attention how to get the information about the attention
well we load the other model we have the encoder we choose which layer we want to get the attention
from and then from each layer we can get the self-attention block and then its attention scores
how do where does this variable come from if you remember when we defined the attention
calculation here here when we calculate the attention we not only return the output to the
next layer we also give this attention scores which is the output of the softmax and we all
and we save it here in this variable self-attention scores now we can just retrieve it and visualize
so this function will based on which attention we want to get from which layer and from which
head will select the the matrix the correct matrix this function builds a data frame to
visualize the information so the tokens and the score extracted from this matrix here so it will
this matrix we extract the row and the column and then we also build the chart the chart is
built with Altair and what we will build actually is we will get the attention for
all the I built this method to get the attention for all the heads and all the layers that we pass
to this function as input so let me run this cell now okay let's create a new cell
and then let's just run it okay first we want to visualize the sentence that we are dealing with
so the batch
so we load a batch and then we visualize what is the source and the target
so
then into the target
and finally we calculate also the length
what is the length okay it's basically all the characters that come before the padding
character so the first occurrence of the padding character because this is the batch taken from
the data set which is already the tensor built for training so they already include the padding
in our case we just want to retrieve the number of actual characters in our sentence so this one
we can the number of actual words in our sentence so we can check the number of words that come
before padding so let's run this one and there is some problem
ah here I forgot to
this function was wrong so now it should work okay this sentence is too small let's get a longer one
okay let me check the quality you cannot remain as your especially you
okay looks not bad okay let's print the attention for the layers let's say zero one and two
because we have six of them if you remember the parameter is n is equal to six so we will just
visualize three layers and we will visualize all the heads we have eight of them for each layer
so the head number zero one two three four five six seven and seven okay let's first visualize the
encoder self attention and we do get all attention maps which one we want so the encoder one and we
want these layers and these heads and what are the row tokens the encoder input tokens
and what are the what what do we want in the column because we are going to build a grid so
as you know the the the the attention is a grid that correlates rows with columns in our case we
are talking about the self attention of the encoder so it's the same sentence that is attending
itself so we need to provide the input sentence of the encoder on both the rows and the columns
and what is the maximum number of length that we want to visualize okay let's say we want to
visualize no more than 20 so the minimum of 20 and sentence length
okay this is our visualization we can see and as we expected actually when we visualize the
attention we expect the values along the diagonals to be high because it's the dot product of each
token with itself and we can see also that there are other interesting relationships for
example we say that the start of sentence token and the end of sentence token at least for the
head zero and the layer zero they are not related to other words like I would expect actually and
but other heads they do learn some very small mapping we can if we hover over each of the
grid cells we can see the actual value of the self attention so the score of the self attention
for example we can see the strong the attention is very strong here so the word especially and
especially are related so it's the same word with itself but also especially and now
and we can visualize this kind of attention for all the layers so
because each head will watch different aspect of each word because we are distributing the word
embedding among the heads equally so each head will see a different part of the embedding of the
word we also hope that they learn different kind of mapping between the words and this is actually
the case and between one layer and the next we also have different w q w k and w v matrix so
they should also learn different relationships now we can also want we may also want to visualize
the attention of the decoder so let's do it let me just copy the code and just change the parameters
okay here we want the decoder one we want the same layers etc but the tokens that we will be on
the rows and the columns are the decoder tokens so the coder input tokens and the coder input
tokens let's visualize and also we should see italian language now because we are using the
decoder self-attention and it is so here we see a different kind of attention on the decoder side
and also here we have multiple heads that should learn different mapping and also different layers
should learn different mappings between words the one I find most interesting is the cross
attention so let's have a look at that okay let me just copy the code and run it again
okay so if you remember the method it's encoder the coder same layer so here on the
rows we will show the encoder input and on the columns we will show the decoder input tokens
because it's a cross attention between the encoder and the decoder
okay this is how the more or less how the interaction between the encoder and the decoder
works and how it happens so this is where we find the cross attention calculated using the
keys and the values coming from the encoder while the query is coming from the decoder
so this is actually where the translation task happens and this is how the model learns to relate
these two sentences to each other to actually calculate the translation so I invite you guys to
run the code by yourself so the first suggestion I give you is to write the code along with me
with the video you can pause the video you can write write the code for by yourself okay let me
give you some practical examples for example when I'm writing the model code I suggest you
watch me write the code for one particular layer and then stop the video write it by yourself take
some time don't watch the solution right away try to figure out what is going wrong and if you
really cannot after one two minutes you cannot really figure out what is the problem you can
have a glimpse at the video but try to do it by yourself some things of course you cannot come
up by yourself so for example for the positional encoding and all this calculation it's basically
just an application of formulas but the point is you should at least be able to come with a
structure by yourself so how all the layers are interacting with each other this is my first
recommendation and why about the training loop the the training part actually is quite standard
so it's very similar to other training loops that we may have seen the interesting part is how we
calculate the loss and how we use the transformer model and the last thing that is really important
is how we inference the model which is in this greedy decode so thank you everyone for watching
the video and for staying with me for so long I can assure you that it was worth it and I hope
in the next videos to make more examples of transformers and other models that I am familiar
with and I also want to explore with you guys so let me know if you there is something that you
don't understand or you want me to explain better I will also for sure follow the comment section
and please write me thank you and have a nice day

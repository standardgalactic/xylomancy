Hello, and welcome to the fourth in our series on AI, Neuroscience and Architecture, which
has been put forward by the Digital Futures doctoral consortium group, whereby we're trying
to make important educational ideas available to students and architects across the globe
as a way to make education much more accessible to democratise as it were, education.
I'm delighted to have a very special guest here today, Josia Bach. Before I introduce
him, let me just make one announcement about for next week. That is to say, on Saturday
we will have a session on Digital Futures looking with Runja Chan looking at a generative
game. You'll find details of that on our website. Today then, I've been looking forward to
this session very, very much. I've been watching Josia Bach at various interviews online, and
I know from my friend Daniel Seth, who is a great admirer of Josia, although he says
he disagrees with him on some things, but that we're going to have a very special session
today. I'm hoping that if nothing else, we will open up to a series of new ideas, and
you will discover somebody who I think is a very significant and creative thinker who
is having a significant impact on the field. Let me say, first of all, that Josia is from
Germany, from Dediacht, from East Germany. He was born in Weimar. He went to go and
study at the university, first of all, at Humboldt in Berlin, and then when I was at
a book where he took his PhD, after that he's been working as an academic both at Harvard
and also MIT Media Lab, and more recently he's been working for Intel. He has a number
of significant online lectures and interviews, including a TEDx, which I would recommend,
and he is the author of the book Principles of Synthetic Intelligence, an architecture
of motivated cognition. The term architecture, of course, is very interesting here because
it refers both to the world of computational science and to the world of architecture itself,
and I should say that Josia comes from a family of architects, not an architect himself, but
his father was an architect, and intriguingly I hear that he was very much against right
angles in buildings and therefore his work was more like the work of Hundervasser. Maybe
this will come out in the conversation today. I particularly like the interviews that Lex
Friedman has done with him, extraordinary interviews, I mean really, really interesting,
and it's almost, I could see a slight difference between the genealogy from the earlier TEDx
talk towards this very creative thinking that I find extremely provocative and exhilarating.
It's like being on a rollercoaster ride when Josia is telling us what's in his mind. We
exist inside the story that the brain tells itself, no doubt we'll hear more about that
right now. I should also point out that he has a personal blog which you can access with
a series of posts, and I will also say that this particular session today will be uploaded
onto our YouTube library to be accessed for free from everyone all over the world, and
the previous ones in this series, Blazer, Igridiakos, David Chalmers, and all the ones
in the future will be uploaded here, and at the bottom here if you want to do a screen
capture you can capture the YouTube library, Digital Futures YouTube library where they're
all uploaded, and of course not just these, but also the whole series on architecture
and philosophy that we've been doing with Slavoj Žižek and others over the years, and
also tutorials and other sessions. So the session today, Josia is going to make a brief
presentation as a way of opening up the discussion. Then I'm going to be asking some Lex Friedman
style questions to get the discussion going, and then we'll be inviting questions from
both the Zoom audience and also the YouTube audience, and I just want to say this is what
we're getting out of this is what we're trying to focus on is a new theory, let's say intelligence
that is appearing at the interface between neuroscience and the world of AI, and we have
a series of other speakers coming up over the next few weeks including Jeff Hawkins,
Ben Bratton, Susan Schneider, Antonia Domaccio, Andy Clark and others, and it seems to me
this is a very exciting time. I was brought up, in fact my first post as an academic I was working
in the area of continental philosophy, that's where the debate was, and in the 90s it was
particularly very strong debate, but we're now moving into something else which I find even
more exhilarating, and that is to say the world of cognitive science, the world of AI, the world of
neuroscience psychology, and a kind of philosophy where the debates are really been driven in an
interesting way. To my mind what makes this very special is not in the fact that we are embracing
the world of science, which in the old days of the divided sort of culture that C.P. Snow talks
about, somehow philosophy wasn't engaging enough with the world of science, and this was the critique
that Stephen Hawking has of philosophy, but it doesn't keep pace with technology, not only are
we embracing the latest technology, and something coming out that I would say is deeply, deeply
philosophical, which is very new terrain and very exciting terrain, but also the world of practice,
the world of the commercial world is coming directly into contact with academia, and some of
the ideas that are coming out are just quite extraordinary, so I see this new emergent theory
of intelligence, something that is really dynamic, and I think it's going to be powering debates
in years to come, so I would like to start by welcoming Yosha, to say what a great privilege
is to have you with us today, I'm looking forward to this immensely, and invite Yosha to share his
screen. Yosha, welcome. Thank you very much, it's a big honor to be here with you today,
and let's see what transpires.
Let me start out with something that happened on Twitter a couple of days ago,
Ilya Sotskyver, AI researcher, who's quite notable, said that it may be that today's
large neural networks are slightly quenches. This sparked quite some debate, some people
asked him whether he was falling, and others were upset that he was propagating an eye hype,
an unjustified vase, and others are intrigued, and he said this thing to me before, so
I knew that he was not trolling, and it's a question that is very counterintuitive to me,
a statement like this that, for instance, a large language model and so on, that it could
be quenches today already, and so could we answer this question, is there a decisive way
to deal with it, and to prove that it's not quenches, or to give evidence that it is?
So we could start out by asking such a large neural network itself, and so this is what I did
yesterday, I fired up the open AI playground, and then the bold printed things is the stuff
that I typed, and so I asked the system to generate, the system is GPT-3, a large language model,
to generate a conversation that is basically prompted by what I initially typed, and it just
tries to continue the stream, and I enter this conversation between me and the conscious AI,
and I ask it if you are really conscious, and the thing says, yes, I am fully conscious,
and ask it how do you know that, and the AI says, I know that I am conscious, because I can think,
feel, and perceive the world around me, I am aware of my own thoughts and feelings, and I can reason
that make decisions, I believe that I am alive, and that I am more than just a machine.
So the problem is that this may not be particularly helpful, because how does GPT-3
get to these statements? GPT-3 is basically an autocomplete algorithm, not that unlike to the
autocomplete in your phone, or in other predictive tools, and that's autocompletion at the level
of individual words based on statistics of all sorts of languages, not just the English language,
because it has been trained on 45 terabytes of text, a large part of the internet, including
German, Spanish, Chinese, and many other languages that it's found, but primarily English,
and the result of this training of these statistics, the neural network with 175 billion
parameters, and it uses for training the so-called transformer algorithm that was discovered and
described by Vasvani and others in 2017, and that is driving a lot of the current developments and
statistical machine learning these days. The neural network by itself is still based on the good old
perceptron that, for instance, was described by Frank Rosenblatt in 1958. Frank Rosenblatt's idea
was inspired by how neurons work, at least the simplified models of neural networks. It turns
out that the neural networks in our brain are implemented in a very different way from the
ones in our computers. In the computers, we just basically treat every cell as a unit that has an
activation state, which is a real number, and that activation state is simply the sum of the inputs,
and the inputs are all weighted by connections, so there's basically a factor by which every of
these inputs is multiplied, and then we throw the output against the threshold function or
a sigmoid or some other output function that can introduce a little non-linearity, basically a
little bit of an if-then into the output, but it's a very simple function. We just take these units
and chain them into layers, and if you take enough of them, this arrangement can be trained to model
almost arbitrary functions, and while we know that this is wasteful, the training algorithm is
relatively slow compared to what the brain is doing, it's fascinating that it works at all,
that it converges at all, and that's able to deal with, for instance, visual and auditory data and
also textual data in such a way that it can often model the statistics of a domain and apparently
also some causal structure. So this works also for vision, and if we look at a neural network that
has been trained to process images and to classify them, we find at the individual layers sensitivity
to structure that is quite similar to how cortical columns and individual neurons in the visual
cortex are sensitive to patterns that emerge after we train a biological brain on visual data.
So at lowest levels, you find contrast patches and colors, and then these features are being
combined into higher levels, and when we go higher up, we find complicated textures and
something like freedom infrastructure and so on, and this can be combined into objects.
The algorithm that is being used to do statistics over text in GPT-3 can also be adapted to
a deal with the visual domain, and the current iteration of this progression at OpenAI is called
Glide, which has been recently presented, and Glide uses a combination of a model of visual
data, which basically is a latent space of lots and lots of images that it has been trained on,
and it understands basically how to go between the possibilities of images and move around the
space of all possible images, and the other part of this other thing is a tool that is able to
match an image to text and determine the similarity of an image to a textual description, and if you
combine these two tools, you can give this a textual description, and it's going to move around
with the space of all images until it discovers an image that is very, very good match to the
textual description, and again, it's fascinating to me that this works at all, but it's now extremely
good. So what you see here in these images, for instance, and I don't know if you can read it
well, to the top left, you see a surrealist dream-like oil painting by Salvador Dali of the
Cat Planet Checkers, and this is what the AI model has generated in response, and you see a
professional photo of a sunset behind the Grand Canyon and a high-quality oil painting of the
psychedelic hamster dragon. You get the idea, right? Also, they're taken by the crane drawing
of a space elevator in the bottom left, or the pixel art Hoji Pizza. These are all images that
the program has not found on the internet, so it has only looked at lots and lots of pictures
on the internet, and because of looking at them, it's able to combine the features in
multi-level hierarchical structure until it becomes similar to the textual description.
But can such a system not just generate images and text, but is it able to generate
the likeness of a conscious being to such a degree that there is causal structure that
would convince us that indeed we are looking at a conscious agent? And to get there, I think we need
to define consciousness in a more tight way than the tibetis we just did.
So basically, just to get our terms straight, where we casually, consciousness, I think is
usually referring to the experience of what it's like, so that the lights go on and that you
experience something in your mind that has a quality of realness to it, and
we can ask ourselves if GPT-3 is weakly conscious in this way, and it's very hard to say.
It's not obvious one way or the other. There is some intelligence in the system. Intelligence is
the ability to make models in my view, and intelligence is different from, say, rationality,
which is the ability to reach goals or sentience, which means that you will become aware of the
structure of the universe that contains you and the relationship to it in your own agency,
and you can act based on the model of what you are doing in the world. And it's also not the
same thing as the self, which is the identification that you have of what you believe, what you are
in the world, the properties and purposes that you follow, or the mind itself, which is the
thing that generates the model of the universe and the self, if it does have a self. So intelligence
is the ability to make models, and it's usually in the purpose of some control task,
so some regulation. And control is a notion that has been made popular in cybernetics,
and the idea of a controller is basically that you have a system that is connected to some
actuator or an effector that is acting on some system that is being regulated, and there is a
sensor that obtains a deviation between the set point and the state of the system, so it measures
that the system is close to an ideal state or more distant to it, and this regulated system
is being disturbed. And the classical example of a control system is the thermostat. So you have,
as an effector, some mechanism that is able to turn the heating on and off, and it's a sensor,
you have some thermometer that measures the difference between an ideal temperature and
the temperature in the room, and the controller is a very simple circuit that turns on and off
the heating. And the regulated system would be the temperature in the room, together with the
heating system, and the environment developed out there behind the windows and so on is going to
disturb this regulated system. And now this controller is going to get better if you give it
the ability to not just act on the present frame, but if you give it a model of the future.
And in my view, an agent is a combination of a controller with a set point generator
and the ability to model the future. And what this means, it's not that it's not going to just
optimize the temperature deviation in the next moment, but over its entire expectation horizon.
So you have a branching world, where different decisions of the controller
need different trajectories in the temperature. By being able to model the future, you basically
can choose a trajectory of the future that you like. And choosing this trajectory means
that you are making decisions. So just by having a preferred way in which the world works and the
ability to model the future, agency is emerging. And if you think about stages of intelligent
agency, the simplest one is the regulator and the feedback group, which by itself is not an agent
yet. And if you're able to model the future, you have a predictive controller. If you combine
this with an integrated set point generator, so it's not just acting on what you do from the outside,
but with an internal generation of its motives, then you have an agent. And if the thing is
sophisticated enough that it's able to discover itself in the world, if its sensor is sufficient
and its modeling capacity universal enough, then it will notice that there is a very particular
way in which its sensors work and actuators work. And it's going to accommodate this to
improve the regulation. So at this point, it understands what it's doing because it understands
what it is, which means it has a model of what it is in relationship to the environment.
And humans are going beyond this simple sentence. We are also transcendent agents,
which means we are linking up to next level agency and become part of higher level purposes
because we are our state building minds. We are able to play a part in a larger role like
in an organization, for instance, or in a society or a civilization.
So now if we go back to GPT-3, whether it's conscious, I think it's pretty clear that GPT-3
does not know what it's doing because it's going to present an arbitrary story and it
doesn't have sensors that would tell it what it is. GPT-3 also doesn't have any kind of online
learning. So it's not able to discover something new after it has been trained. And GPT-3 has been
trained before GPT-3 was invented and published. So it has never read the reference about GPT-3
on the internet and it only has to gather what GPT-3 is when you talk to it from the context
in which their prompt is being given. And so it is not sent yet. But imagine you want to give it
agency. Of course, it's not an agent by itself. But you could, in principle, as a thought experiment
at least, use it to drive a robot because GPT-3 is able to generate stories about robots. So
if you were to give GPT-3 access to a vision-to-speech module and this vision module is giving
sensory information about a robot and its world, then GPT-3 could continue the story of that robot
and then we feed the output of GPT-3 into some speech-to-actuator module that is producing the
behavior of the robot in the given moment. And then we look at the world again and the internal
model states and then feed them back into the system as a prompt. And now it's still not able
to put anything into long-term memory. So it would be an amnesiac. It should be able, using its
working memory contents and so on, to produce plausible behavior. And you could still argue
that this doesn't have an intrinsic motivation because it's just going to generate an arbitrary
story about a robot based on stories about robots that have seen in the past because
at some of the motivation into an external cybernetic module that has set point deviations
and measures them and feed this into the prompt. So what this thing is doing is now generating a
very complicated high-level story. And it doesn't need to be a story that is limited to text. It
could also have visual elements. It could have physical dynamics and so on, because the transformer
can learn all these things. So in some sense, it could generate a story about a conscious being
that is similar as the story about a conscious being that's in our own mind.
There's a basic difficulty that came up and I discussed this with my friend and colleague,
Tanja Greenberg. Do we know when a person appears in our own mind, for instance, during a dream
at night, if we talk to that person in our dream, whether that other person that we imagine in our
dream is conscious or not? And clearly, if we ask that other person, we might not get an answer
that is true, because if this thing is only a simulacrum that pretends to be conscious without
being conscious, it's just manipulated behind the scenes. How would we find out? On the other hand,
I also know that I am an imaginary person that is imagined by my brain. It's a model that my
brain has discovered about a state of affairs, about an organism in the physical world.
But this model of consciousness is an entirely virtual story and I know that this story is not
real. It's a figment of my imagination. And of course, there's also a continuum between characters
that I imagine in my mind and myself, because I can imagine myself to be that character. If I,
for instance, write a book and I imagine a character in the book very intensely, then
at some point I might find myself to be that character in the book where I suspend all my
disbelief and this conscious being is not different from me. So basically, there is a pretty fuzzy
area where it's hard to say whether an imaginary person is conscious or not. It's difficult to say.
How does this work in biological systems? In biological systems, we don't use a technological
design. They are designed in a very different way from the technological artifacts that we
are building when we write in computer programs or building machinery. And technological systems,
we start out with an environment that is deterministic. We know how our workshop works.
We know how our computer works. We start basically with some kind of a pretty much blank slate.
And then we decide what the functionality is by which we want to extend our world and then we
design from the outside in and into the material and so on and force the material, the substrate,
to produce exactly what we want to have. And biological and social systems are designed
from the inside out, some kind of metal design. It basically means that you cannot rely on the
determinism of the universe. You have to colonize your substrate first and extend your own functional
principles and your determinism into the substrate before you can make it do what you want it to do.
And basically, you have to instead of realizing the functionality, build a system that wants to
realize the functionality, that trans converges towards realizing that functionality. So a tree
is not just a set of functions that realizes the transfer of nutrients from the roots to the
leaves and photosynthesis and so on. But first of all, it starts out as a seed that is going to
colonize the ground and the earth, the material around the seed is going to turn it more and
more into tree. And if you disturb that system by harming it and hurting it, you don't do it
too much, so it gets destroyed a little bit, then it's going to grow back into a tree. And so it
is something that is a proto tree and eventually converges into being a tree.
And this thing needs to have some agency to make that happen. It needs to be a model of the future
that is being achieved in the system. And biological neurons are agents in the sense as well,
that they're designed in the mind from the inside out, not from the outside in.
Here you see a bunch of cortical red neurons that are filmed in the pitch petition. You see
how they're trying to link up to each other and form some kind of organization.
And we have something like 86 billion of these neurons in our brain, and they are organized
in the neocortex in groups of something like 100 to 400 neurons, which are cortical columns.
And we have something in the ballpark of 100 million of these cortical columns. And
I think of a cortical column as something as a state machine. There's a protocol that allows
it to link up to the cortical columns around it. It's trained to be like this. And each of them
approximates functions. And these functions play out in brain areas that are basically
something like an ether in which activation waves appear. And these activation waves represent
the calculation of dynamic functions, which are features of different cognitive domains.
And the brain areas are talking to each other and listening to each other and so on,
form processing streams. And sometimes use the metaphor of a cortical orchestra where
basically every brain area is somewhat akin to an instrument. And the different instruments are
listening to what is being played in their environment. And they are taking up these things
and complicating them and then passing them on to other instruments. And this orchestra is
dealing at some of the outer fringes with sensory patterns and actuator patterns,
and then abstracts them into geometry and spatial structure and to generative simulations of the
world and into conceptual abstractions and so on. And the entire thing is being attended by some
conductor. And the conductor is not some CPU that sits inside of the brain, like the CPU sits
inside of your computer and makes things happen, but it's an instrument like the others. And it
can only listen to what the rest of the orchestra is doing very superficially. And its role is to
make that orchestra coherent, to let it play a single thing at any given time and to remove
inconsistencies between what the individual instruments are playing. And if the conductor
uses the connection to the system at night when you dream, then the orchestra doesn't necessarily
stop, but it can go into something like a free jazz mode, where it's no longer connected to an
audience. And the audience is dark because you're dissociated from your sensory apparatus at night
when you dream. And so this thing is just spinning off. And sometimes it can become very
improherent, sometimes it's going to settle into a groove, but it's not going to generate a unified
model of an universe that it's entangled with reality that it's connected to, that it tracks
as it does during daytime. And this tracking of coherent reality seems to require some kind of
government mechanism. This government mechanism emerges in the brain, so some kind of a suspect
neurodiagonism. There is some kind of evolutionary competition between different organizations that
your mind can have, and eventually the most stable one prevails. And this is your observing
conscious self, an attention agent that is trying to make a coherent model of the world.
And now, can we can ask, does GPT-3 have such a conductor? And I think that the attention model
in the transformer looks a little bit like one, but it's not. So the new thing that GPT-3 had,
that previous neural network training mechanisms usually didn't have, was the ability to
pay attention to what it should learn. This means that in every layer in this neural network,
there is going to be a model of what the previous layers, based on the current context,
what data in the previous layer, what features in the previous layer should it pay attention to.
And this self attention helps the network to learn basically its own structure
and do statistics over and it makes it much, much more efficient and coherent.
But it's not integrated over all the layers into one model of reality and so on. So this is not
what's happening in GPT-3 yet. And maybe this is one of the reasons why it's so much slower in
learning, so much more training data than the human being needs over the course of their life
before it converges. And it's tempting to think that this such an integrated model of attention
is something that has, for instance, been suggested by Marvin Minsky in his seminal book,
Society of Mind, where you have basically look at the mind as a society of different agents.
And there are some agents that are organizing the other agents into a coherent structure. And
Minsky calls these agents K-lines, knowledge lines, and suggests that they form basically
their own society in the society of mind. And this society is forming something like a
reflection of what's happening in the A-brain, the A-brain being our perceptual mind that is
modeling the reality that we are tracking based on sensory and actuator input that
the brain is entangled with and the B-brain is immersed into this perceptual reality and
reflects on it and makes it more coherent. And there is a similarity between Kahneman's famous
System 1 and System 2. It's not quite the same thing, but it's tempting to basically see
the affair as a perception agent that is entangled with the environment and is getting
valence from the motivational system that is basically cybernetic motivational architecture.
And then you have an attention agent that lives on top of the perceptual agent. And that is the
conductor and has a memory of what it attends to so it can get the model to convert by some
constructive process. And this attentional system, this conductor, here I've drawn it on top of the
system. The top is a direction that basically seems to be obvious to the attentional system itself
because it feels to be on top, but we know that you're not completely on top. There is stuff
that is driven by the self that we have not reversed and that we give a moment and that
gives motivation to the attentional system to what it attends to. But our consciousness
perceives itself as the observer of the mental and external states and the self states of the
model that is being discovered. And the purpose of the attentional system is to facilitate learning
so we can converge to a model of reality and reasoning, which is basically real-time learning
on imaginary mental states. And this idea that consciousness is a control model of our attention
is not new. It's, for instance, been championed by Michael Grasiano in the attention schema theory
and it finds itself in one version or other in eastern philosophies and a lot of convergent ideas
in cognitive science. Some people say that computers cannot be conscious because they are only
physical mechanical systems and so they're not physical systems in the same way as
neurons are because neurons are entangled with the real world as dynamical systems and so on.
They can do things that a simulation cannot do. And I think that Fritz Le Corp maybe has it backwards.
I think that physical systems cannot be conscious. Neurons cannot be conscious,
brains cannot be conscious because there are things happening in consciousness that are not
physically possible. And the only thing that can be conscious is the simulation because
consciousness is the simulated property. Consciousness is virtual.
So you can only be conscious in the story that you tell yourself about yourself.
And this means that our phenomenal consciousness is a virtual state. It only exists inside of the
mental models. It's not attending to physical phenomena. It's attending to high-level features,
things like colors and sounds and emotional expressions and so on. One of these are physical
things like all these high-level abstractions that a learning system is generating in the
interaction with the environment to make it predictable. And this phenomenal consciousness,
this experience of what it's like to attend to features is an awareness of a partial
binding state of our working memory. That's basically at the interface between perception
and reflection. And then we are aware of the mode in which we're using attention. So whether this
is hypothetical or whether it's perceptual or whether it's a memory. And then there are
reflexive consciousness. This process that is attending is aware that it's the process
that is attending. And it's based acting based on that awareness. And the AI researcher at
Warsaw Benji was suggesting that consciousness is basically a function whose purpose is to create a
big dip in the energy function that models reality. So it's basically a low-dimensional,
almost discrete function that is parametrizing the perception in such a way that it starts to make
sense. Self and consciousness are not the same thing. The self is a model of your agency that
you discover. And you can be conscious without having the self. For instance, during dreams or
meditation, you can turn off the self without losing consciousness. And the self is this discovery
of the agent that the system is making about what it is. And it's downstream from the setpoint
deviation. So the self is not motivating things. It experiences the motivation and begins to
understand how the motivation works and thereby allows to reverse engineer the mind if the self
is learning. And it shapes our own agency by identifying who we think we are at any given moment.
And it allows to have a first-person perspective if the self is discovering that the contents of
this control model are actually driving behavior. That makes it a very special agent. So in this
sense, consciousness is a control model of attention. It allows the convergence of a
coherent interpretation of the world, which is basically a low energy state of the model that
attracts reality. And it maintains a memory for this invogations. That's why we have a stream
of consciousness, because when you construct, you need to remember what you tried and where
you're coming from. And this is not true, for instance, for convergent learning mechanisms
like neural network learning, but you don't need to remember where you came from. We just go to
the next optimum and try to stay in that optimum. So is GPT being conscious? So we see three by
itself is not an agent. And the transformer is also not a complete control model of attention,
but only a very partial one. And on the other hand, current AI models can be extended beyond
that. And they can create coherent stories about conscious agents. You can get GPT speed to run
very long in its simulation of what it's like to be a conscious agent. And we can ask ourselves,
is GPT-3 simulating a conscious agent, or is it just a simulacrum? And what does this mean? So
the world is a decomposition that might mix of the universe into interacting separate objects,
because the entire state vector of the universe is too complicated to model it, right? So you
hack it up into separate, disconnected sub systems. And the universe is not really made of
separate, disconnected subsystems. It's just a way in which we make it intelligible to us.
And once you have these separate, disconnected subsystems, and you model the interaction,
you get causality. Causality is the interaction between separate objects, right? So it's
causality is a side effect of the way in which we model the universe as separate objects.
And the simulation can model causal structure on a different substrate. So for instance,
a computer game is using a substrate that's very different from physics, very simplified
computation that nevertheless gives results that are so similar to physics that you can
recognize what's happening on the screen and manipulate the causal structure on the screen
based on what you have observed before in the real world. So a 3D computer game is a simulation
of the physical world. It's a very simplified simulation, but one that can be surprisingly
convincing. And a simulacrum is recreating just the observables without causal structure. For instance,
the movie is a simulacrum. You cannot causally interact with the movie. You can just observe it.
And so a simulacrum basically can do magic. It can do an arbitrary thing without you having to
understand the causal structure. And in the sense, a lot of instances where you experience our free
will is not the causal structure, but it's a simulacrum. It's a stand in for a causal structure
in our mind. And it's to me an open question, how much of my own consciousness is a simulation
and how much is a simulacrum? So if an imaginary person in my own mind is sometimes conscious,
it's not that easy to say whether GPT-3 or an extended version of GPT-3 that is more multi-modal
and can also have perceptual content and so on, represented in it, qualifies as such an
imaginary person that would be conscious. I think I am an imaginary person myself.
And don't know to which degree I'm a simulation or a simulacrum. And it's not quite clear how well
the AI models are dealing with this. So in summary, I find it's very counterintuitive to think of
GPT-3 as being conscious. For me, it's surprisingly difficult to shoot down the idea that it is.
And even though GPT-3 is clearly inferior in many ways to the way in which my own
perception works and reasoning works and learning works, and there's many things
that it cannot do so easily, I think it's not that easy to dismiss the idea that it is
slightly conscious for brief moments during the inference when it has to build causal structure
to simulate an imaginary person that can tell me a story about it.
Okay, let's stop here.
That was great. That was fantastic.
Let's say first of all that there has been a debate going on on the internet about this,
that people have been sending me links to, and Jan LeCun responded to Ilya's comment.
I didn't really saw that, but his response is not even true for small values of slightly conscious
and so on. Anyway, so there's a debate out there, and of course last week we
had David Chalmers here who, as you probably know, there was an interview on GPT-3 with him,
which is very convincing, and he kind of makes his comment similar to you that he thinks it's
kind of approaching something like consciousness. Anyway, one thing I wanted to mention is that we,
in architecture, we haven't actually, I don't know anyone who's been using Glide, but
Clip has been used to generate images, very successful actually, and using VQ GAN. That's
the technique that's become very popular, and has produced some really quite shocking results
that people are kind of, they're really taking pay attention to. So it is something that we're
getting into, and it's certainly part of that discussion. We also, I've had discussions about
GPT-3 on this forum, which have been interesting. I mean, the key question obviously is whether
we are fully conscious of everything that we're doing, and I think that there's some
level of things that are happening. I mean, to my mind, there are some automatic reflexes that
we do. For example, you go to Japan, someone starts bowing at you, automatically you bow back,
it's not as though you're really thinking about it. And then there are questions whether we have
access to some of the processes that are going on that are part of our actions. We simply maybe
can't reach those points. So whether things are beyond us in some sense. So anyway, this debate
is a very timely and very interesting one. I want to put up, you're about to show something.
I just put up this slide again, because this is generated with, I think, Clip and VQ GAN on
Vombo AI. I don't think that I've published how exactly they do it, but it looks like it.
And I generated this as AI claiming the noble eightfold path.
Yeah, maybe I can show you later on some of the stuff, because it is quite
extraordinary what it can produce. And I would say that you actually have in the audience here
some people who have written about AI and architecture, including myself. So it's kind
of, you've got an interesting informed audience here. One thing, there's just a general kind of
comment, though, is, I mean, I really like the idea that you put forward that somehow
you can learn about the self through looking at AI. It's somehow, I mean, I don't know how you put
it, but whether it becomes AI becomes a mirror and into the self, but whether we can understand
human intelligence through looking at artificial intelligence. And that's a provocation. And
I think there are some examples in computer science where we have learned about the natural world
through computer models. I mean, I think that Craig Reynolds Boyds, for example,
gave us a clue as how to birds actually flock. I think that's interesting. And so potentially,
there is something there that is, and this is one of my primary interests, is how we can learn
about human intelligence, the human mind through these things. But the big challenge that it seems
that we have is that we're dealing essentially with two black boxes. We don't know what's going
on in the deep levels of a neural network, and we certainly don't know what's going on in the mind.
And so how can you make, what can you say that isn't simply a form of speculation? I mean,
you can't prove anything. It can simply become some kind of, you can speculate about something
based on what appears to be the case in another scenario. What would you say about that coming
from a kind of, let's say, a scientific background where you have a kind of burden of proof?
Can you do more than that? Yes. First of all, neural networks are no longer black boxes. You
know how neural networks work and largely also why. You can basically look into the neural
networks and find out which parts of the neural networks are computing which functions. And a
function is a mapping from inputs to outputs. And a function can be used to couple the
previous inputs to future inputs to track reality. So in some sense, when you look at
the patterns on your own retina, what you have there are little blips that appear on the retina
whenever a retinal neuron gets excited by photon sitting it. And what your brain is doing,
it's discovering a relationship between these blips. The meaning of the blips is exactly
the relationships that your brain discovers between the blips. And this makes them predictable.
It puts them into a shared context, not just at the same time. You're not just processing
lots of parallel blips that happen on your retina, but also across times. So across different scenes
that you're observing at different moments in your life. And the relationships between
the different blips on your retina that your brain discovers is that you are looking at
moving blocks of color in a world that is moving relative to you. And these moving blocks of color
are three-dimensional surfaces. And the surfaces are animated by some kind of physics. And they
are also animated by some kind of agency that you sometimes observe, like people talking to each
other and so on that have mental states, they exchange ideas, and they're being lit on by the
sun. And all these relationships are functions. And these functions are dynamical features
that basically tell you how to get from one state of the world to other states of the world.
And at this level of abstraction, this is something that our neural networks also can do.
Where there are limitations is that the neural networks that we are currently using
are often not learning in real time. They're not connected to the world and online learning.
This research does happen. And it's slower. And it's not as flexible in many ways as the
learning happens in our own brain. And so the algorithms that we have discovered
are not the best algorithms that could facilitate this. But it's also on the other hand,
not as clear to me what the limitations of these algorithms are. There are people like Gary Marcus
who will tell you that it's very obvious that these systems cannot do X, but there is no proof
that they cannot do this. Even if you have a very simple feedforward system that is only mapping
inputs to outputs, what is to say if you connect this to a memory, that it's not the transition
function between adjacent brain states and is able to do everything that your brain is able to do
if it just has memory to store parameters that infer from the environment that modifies future
behavior. So it's very easy to build a system that is Turing complete. It's not easy to discover
a function that is capable of universal learning efficiently. And so our machine learning models
at the moment are not efficient in the sense that they learn as quickly as biological nervous
systems learn, but they do learn and they do converge to many of the functions that we require.
Can I just share my screen a second because there was one that you touched on which I thought was
that this is simply a transcript of your discussion with Lex. And this one I've highlighted I think
is really interesting and incredibly provocative sort of comment. So basically a brain cannot
feel anything, a neuron cannot feel anything. They're physical things, physical systems are
unable to experience anything, but it would be very useful for the brain or for the organism
to know what it would be like to be a person and to feel something. So the brain creates a
simulacrum of such a person that it uses to model the interactions of the person. It's the best
model of what that brain, this organism thinks it is in relationship to its environment. So it
creates that model. It's a story, a multimedia novel that the brain is continuously writing and
updating. I mean, I find this enormously provocative as a comment. And I think the
idea that we're kind of creating a story that somehow gives meaning to something,
it kind of reminds me in some sense of the way that Homie Barber talks about how a nation operates.
I think Zizek says something similar. It's how things are inscribed within a story that people
tell oneself. And I think that's important because in architecture we just focus on the object,
but actually it's the way that object is inscribed within some subjective process that makes sense
of things. But I just wonder whether, I mean, so to my mind, this is an incredibly provocative
and controversial comment, it seems. Just maybe could you comment on the reception that this view
has had with other people? Has it proved to be controversial? How else could it be? Do you have
other theory that works that can explain what's going on? I think once I noticed that my
own experience is virtual, that my memories are often created after the fact and modified under
my nose without me noticing. Do you notice that you exist inside of a model? It's also that I'm not
in physical time. My own self is sometimes a little bit ahead of the physical universe,
sometimes a little bit behind. And usually both, so the physical now and the
experience now are different. And the elements of my perception are clearly not the elements in
which physics is being implemented. Rather, it's the other way around. What I notice is that
do exist in the dream, very much like we usually say in idealistic philosophy. But this dream needs
to be created somehow. Something needs to construct the dream. And that's a brain and higher plane
of existence. And this higher plane of existence is what we call physics.
Maybe I'll stop sharing. The other comment that I find usually provocative that you make is which
kind of relates also to the discussion. I don't know if you've seen David Chalmers' recently
published book on reality plus, where he talks about virtual worlds. But you came up with a comment
that what we are seeing is a virtual reality generated in the brain, which I'm actually
very persuaded by myself. And I guess I'm thinking also of the kind of thinking of Anil Seth, who
kind of talks about this controlled hallucination. And we kind of predict what's out there because
we don't know. It seems that your work to some extent aligns with the work of Anil Seth. But
some points differently. I have to say that Anil is very fond of your work. So it's intriguing to
compare and contrast them. Because we had a discussion last week about whether we're living
in a simulation and things. How would you position yourself in relation to David Chalmers'
work on virtual worlds? I haven't read his recent book. So I cannot say. And I don't know what his
main thesis is about virtual worlds. I wouldn't want to speak on his behalf, but we had a
discussion about it. I also wanted to just point out something as well, which I find
intriguing. And that is the extent to which some of these speculations that are coming out of
cognitive science kind of seemingly echo the world of psychoanalysis. Now, I know that a lot of
cognitive scientists and neuropsychologists hate psychoanalysis. I know that Anil Seth does.
But there's an interesting comment that Slavoj Žižek has made about this, where if you take a
Lacanian perspective, you don't engage with the real except at certain moments. And in a sense,
the fantasy has become a constitutive of how you engage with the real. So you see the real
through the lens of fantasy, through the lens of the imagination, which is very similar to what
in some ways you're talking about. And he makes a comment and an essay that I published a long
time ago in a book. And this is called From Virtual Reality to the Virtualization of Reality,
which is basically sort of saying that our reality is itself already virtualized. And I think what
virtual reality therefore shows us is not how virtual reality shows us is not how virtual
virtual reality is, but rather how virtual reality itself is, which is very similar to your kind of
thinking. And so what I find intriguing is that some of these speculations are echoing previous
speculations about how the mind works. Have you engaged in any way with Žižek or the world of
Lacanian psychoanalysis and its discussion about the real? I sometimes read this, but I've never
had the discussion with Žižek. I am not unsympathetic to this terminology. It's just the problem is
that it doesn't allow me to make models that I can test. And this means I don't know whether
these models are wrong. So it's basically a very useful way to generate stories that also give me
a handle on reality in the sense that allow me to point at entities and to manipulate them in my
mind. And sometimes it's very useful that you just basically have an indexical model where you are
separating the world into objects that are useful to you and you can manipulate them. But this
decomposition doesn't need to be an accurate causal structure. So the criticism with psychoanalysis
is not that the terminology is not useful to me. It's that psychoanalysis doesn't tell me how to
build the mind and so I should say that it works and to compare different competing models of the
mind and to see which one is better. To do this, I will need to automate the mind in a way. I need to
reverse engineer what the mind is doing, the functions that the mind is applying
to representational states and need to get this done to such a detail that this thing becomes
mind-black and then I can compare its functionality. There is, I want to sort of move on to the
questions that are coming in. I want to invite people in the audience to comment as I sort of
make an observation and that is to say that in my own world, this is years ago, I was working on a
kind of coming out of Freud and thinking about how you use model psychoanalysis and engaging with
actually in this case it was with the work of Walter Benjamin, I came across something that is
uncannily similar, certainly in terms of the terminology used, whether we're talking about
the same thing, I don't know. But let me just for a second just share you something which surprised
me, because when I heard Blazer Guernialkos talking about models and modelling, it's also
crucial to his way of thinking. It sort of seemed to echo this. I'm just going to simply just share
this screen a second and yeah, can you see that? It's a thing about, so the term that I'm always
interested in is the term mymesis. I don't know if you know this term at all, but in Freud it's how
you can, he talks about it initially when he talks about how in his book of jokes, how you can
connect with someone who's a subject of a joke, someone falling over a banana skin for example,
you somehow, you model yourself on that person recalling bodily memories of what is to slip up
and so on and so on, it's how you identify with the world. The term mymesis is a form of that,
it's a form of modelling. But just I was going to read out some of the text here because it's so
similar to this idea of models and modelling and I know that the term can be taken out of context
and have a completely different sort of meaning, so therefore it's a bit deceptive.
Anyway, to just understand the meaning of mymesis in Benjamin, we must all recognize its origin,
the process of modelling, of making a copy of. In essence, it refers to an interpretive process
that relates either to the modelling oneself on an object or to making a model of that object.
Likewise, mymesis may come into relation as a third party, engages that model with that model and the
model becomes a vehicle for identifying with the original object. In each case, the aim is to
assimilate to that object. Mymesis, anyway, so it's going on about this question. It's a concept
that has been used in psychoanalysis and I don't know, there's a risk that one can simply take
a term which has a complete different meaning in different contexts and apply it. But I do think
that the concept of model is a fascinating one and I'm intrigued by the fact that you, alongside
Blaze and I think alongside also Jeff Hawkins, use that model as a way of opening up these questions.
An issue with this type of language is that usually the understanding that it generated
just doesn't converge. That's the general issue with continental philosophy.
Somebody recently asked on Twitter what the difference is between continental and analytical
philosophers and I somewhat flippantly responded that an analytical philosopher
is one who understands that the difficult and hard questions of philosophy need to be
answered in this form of models. Whereas continental philosophers don't think that this is necessary
because they are literally genre that is looking down on analytical philosophers.
You can see the difference between analytical philosophy and continental philosophy in,
for instance, the treatment of girdles in completeness proof. A proper analytical
philosopher who had a formal education will understand that this is a proof about certain
properties of formal languages and specifically it proves that stateless formal languages that
assume that truth exists independently of the process by which you get to truth
don't lead to consistent models of the domain. There are also related results, for instance,
that a system cannot make statements about affairs outside of itself. When you want to
talk about the world in a formal system, you need to create a model of that world and you can only
talk about that model. You cannot talk about anything outside of the models that you are creating.
And to continental philosopher, the girdles proof is more or less often understood as a
statement of mathematicians that prove that mathematics is important at getting a handle
on reality and therefore the only way you can get a handle on reality is by not knowing mathematics
which gives the continental philosopher a clear advantage.
I cannot hear you. You are muted.
That was a great answer. Thank you. We've got some questions now. I have a third series of further
questions that I'd like to ask. Maybe I could ask you one question before we go into the other
questions and that is, I mean, are you writing a book about this? I mean, is it being put down in
some documented form because it would be incredibly useful if it were?
You are right. I need to write a book about this. I have a large number of notes on stuff that needs
to go into the book, but I also have a job and I have kids that are homeschooled and I have ADHD.
So I need to go into a different phase of my life to have long-interrupted, uninterrupted sessions for
writing long-form texts. But if you're bad about not having written the book yet.
Well, I think that a popular form of communicating ideas. So there is material out there,
but I just think it could be assembled into an engine. Yes, it needs to be assembled. I feel
better if it is being assembled and not just existing as various disconnected conversations.
Yeah, I just, I mean, even Jeff Hawkins, I mean, my Jeff Hawkins book, I think is fabulous, but
what's interesting is there are no footnotes in it. He's just kind of speculating, but nonetheless,
he's putting his ideas down there and it's really incredibly useful to have that kind of commentary.
So anyway, I look forward to the book. I want to just ask this Matt Gorbay, who's got a question in
the chat, whether Matt is a graduate of MIT Media Lab, he's a doctoral design student right now at
FIU. Matt, would you like to ask your question? Sure. Yeah, thanks for all of this. It's really
interesting. Perhaps the book could be co-authored by GPT-3, make it faster to, you know, just to
give GPT-3 the agency over the first draft. I was asking, speaking of agency, I mean, the question
I have is about motivation. It's about sort of the high level motivations. When you ask GPT-3,
are you conscious? And then it responds somewhat convincingly. It still isn't initiating that
conversation. And so one of, I mean, kind of in listening to everything you were saying and you
said something about, when you were talking about trees and seeds, I love the thing about, instead
of realizing the functionality, you want to build a system that wants to realize the functionality,
you want to build the thing that wants to become a tree. But that question of wanting and motivation,
how does one, at what point does that get put into the system? Like at what point does the system
become curious or self-motivated to do things that we didn't necessarily ask of it? And I think maybe
related to that, I don't know, I'll let you go on this, but maybe related to that, the question
of individuation and sort of inter-subjectivity, has GPT-3 spoken to, are there communities of
GPT-3 all talking to each other about what they want to do and how do they individuate? Or is GPT-3
just always the same and its clones of itself? If you could speak to any of that, that'd be great.
Thank you. Yes. So that's the question, are we doing something that the organism is not asking
of us? And that's not an easy question to answer. If you look at our own motivation, I think that
we have a few hundred physiological drives for different nutrients, for instance, sometimes we
want to eat salty food, sometimes we want to have sweet food, sometimes we need something to drink,
sometimes we need to rest, and all these can be understood as set-point deviations. And
to deal with all of them, we need to create a dynamic model of our own needs projected into
the future, and then plans and higher level models of these needs, which we could call purposes and
so on. We don't just have physiological needs, we also have social needs, for instance, a need
to affiliation to become part of a group, for instance, and to be accepted by it. Some people
have a need for status to raise up in the group. There are romantic needs, which can be courtship
modes or need for intimacy and so on. And then next to about a dozen of these social needs,
we have a handful of cognitive needs, a need to become more competent, become efficacious on the
environment, a need to reduce uncertainty, and something that I would call a need for aesthetics,
which means discovering deep structure in the world. And aesthetics can be split into
stimulus-oriented aesthetics, so they are intrinsically wired to like certain body schemas
over others, certain landscapes over others, and there are evolutionary reasons for that.
And then there are some mathematical principles, what kind of representations we like, what form
means to have a good representation of something that are more general. And if we use meditation to
disassemble our own needs and to dissociate from them, we realize that the things that give us
pleasure and pain do fall in these categories. So we have lots of these impulses that are about
hunger and thirst and rest and so on. And they have impulses that are about the social domain.
And the older we get, the more these impulses get replaced by a deeper model of what we want
the world to be like. And we act on this deeper model and adjust these reflexes. And on the lowest
level, when you try to get more enlightened, you may have just something left that people often
call love, which is, I think, a need to transcendentally connect to other agents and share purposes,
that might act on these shared purposes, but you can get deeper than this. And the deepest level,
you only have aesthetics, the need to form structure and to make the world intelligible,
to create a coherent model of reality. And this need, I think, is similar to what
Friston describes in the free energy principle. It's basically predictive coding. It's the attempt
to track reality using a model that is as good as possible at tracking reality. And if you turn
off this aesthetic need, in addition to all the others, my own mind becomes fuzzy, I fall asleep,
I drift away. Because if I stop paying my neurons for producing order in the universe and they stop
doing this, then nothing else is happening in my mind that I can observe. And I just lose coherence.
So if we imagine this hierarchy of needs, which by itself, and seen as a cybernetic system,
is not all that complicated, you can build this into a machine. I think it's not that difficult.
The difficult part is to get perception right, to get ability to model reality in the universal
base. You can have one coherent model of everything that you relate stuff to. When we talk about
meaning, we talk about how to relate an arbitrary feature or domain or idea or concept to this
unified model of reality that we are building each of us in our own mind.
Can I just pick up on a question? We've got another question lined up, but you asked
you one quickly. You come from a creative background. Your father was an architect,
and you refer to, let's say, creative practices of the orchestra and so on. It's part of what
you're talking about. And frankly, your way of thinking is incredibly creative. It strikes me
as being very creative. I wonder, you haven't mentioned the word creativity, I don't think.
And how do you view creativity? Is it just a myth or is it something,
how could you conceptualize it within your framework?
I think of creativity as the ability to bridge discontinuities in a search space.
And you are just following the gradient. And when you're just going through a continuous search
space, I don't think that you are creative. You just arrive at the state of the art.
And even the state of the art is something that hasn't been done before.
If you just combine what is known and you find a local optimum and the known things,
you're not being creative. To be creative, you need to construct a new search space,
usually. And many methods in which you can be creative. For instance, you can use
random serendipity. You can use some evolutionary process that is combining elements in ways that
you are unaware of. And then discover structure in them. Creativity is in some sense about
jumping off from the known things into darkness and hoping that you end up landing on the other side.
So it's related to a search. Let me just put this to you then. Do you think that,
because you mentioned this before in your discussions, but do you think Move 37 in Game
2 of AlphaGo, was that creative? Could you call that creative?
I don't think that AlphaGo is creative in the sense. Because what AlphaGo is,
well, there are evolutionary methods in AlphaGo. And the outcome of what AlphaGo is arriving at
is not always predictable. And it's also computationally irreducible in the sense that you cannot
foresee what AlphaGo was doing. AlphaGo was able, in a relatively short amount of time,
to demonstrate that human goal play, which existed for thousands of years, was not optimal.
It has discovered strategies that encounter the established strategies and goal. And in this sense,
from the perspective of a human goal player, it was playing in a creative way. It just
discovered new things that had not been discovered before. But if you run AlphaGo multiple times,
it's always going to discover these things. So the search is, well, it has stochastic elements
as a deterministic outcome. And I think that when we look at systems like this, our notion
of creativity is kind of sort of falls apart. Creativity is not absolutely a thing in the
universe. It sometimes is a frame that is useful to describe what's happening. And sometimes this
frame falls apart. Let me just put a provocative comment here then. I mean, something that I
thought myself, and I would like to know what you think on this. So if GPT-3s, if AlphaGo is not
creative, and I kind of, in many ways, I don't think it is creative. It's just doing a very,
very effective search. But then we could ask this question about whether human beings are
creative or whether this term... Exactly. That's my issue, right? So sometimes your
terms start meaning things. They mean something in a certain context, but when you increase the
resolution too much, this context falls apart and no longer makes sense and you lose your term.
And I have the same issue with the term like mimesis. But I like it. It's poetic. It is evocative.
It produces stuff in your mind. But when you zoom in very hard, it's not clear what it means.
And so instead, I try to examine the assumptions that are hidden in mimesis. For instance,
the idea that others exist independently of you and yet you are able to take them in somehow
instead of constructing them. And then the question, what's first, the model of the other or the model
of yourself? And whether it's the same thing in every person that becomes conscious. This is not
obvious to me. And the notion of mimesis presupposes too much. And that makes me unsympathetic to it.
So even though I appreciate the poetic illusions that are there in the space that the term like
this opens and the ability to converse about it, ultimately I need to deconstruct the term before
I can use it. Okay. So let me just put, I'm glad you take this position. We just throw an
idea at you. So I mean, one of the analogies that I've made in the past is to say that
use the term magic at one point. Actually, I don't think that magic exists. I mean,
I think that what happens basically is if you take the example I always give, if you have a
magician at a kid's show and they're pulling a rabbit out of a hat or something or doing magic,
the magician's not doing magic. The magician is simply concealing the operations at work
and making you believe that it is magic. And I'm just wondering whether we couldn't take that same
notion and apply it to creativity, because we don't understand the processes. We just look back
and say, wow, that's creative. Like some people said the same with AlphaGo, that's creative.
But maybe it's not. It's just simply we don't understand the process. Therefore,
maybe even the term creativity is not a very productive term in the first place.
Yeah, I suspect that magic also in order to make sense, we need to understand what the term means.
We need to completely deconstruct it into its constituents and then put it back together and
see if we still have magic or if this term can still be recovered. And typically, I see magic
as the ability to get right access on the laws of reality. And if you think about what it means,
in the naive form is the departure from the mechanical universe, the universe that we are in
according to the theory of physicalism emerges over a causally closed lowest layer. And this
causally closed lowest layer is basically whatever mechanics is making the universe happening.
And ultimately, there is going to be some natural layer where things are just happening without some
conscious intervention. And the idea of magic is that our universe somehow is a conspiracy,
that there is a way to subvert the laws of the mechanical universe using symbolic powers,
that you have symbolic causality. And symbolic causality is, for instance,
the connection that exists between sacrificing a black cat and celestial events that are caused
by this. And this is something that cannot possibly be explained by any known physical
mechanism because the elements of this transaction only have meaning in a symbolic realm to a human
mind that is acting based on a certain high level story and abstraction that is not a good
depiction of what happens in the physical reality. It doesn't mean that the story is wrong,
just not one about the frame of physics. In computer games, there is magic happening relative to the
computer game. You can use Minecraft. And in Minecraft, there is a mechanical layer where
everything happens by itself. But you can also call up a shell and enter a time set day in the
sunrises. And this interaction somehow breaks the logic. And if you could do such a thing in our
world, if you can use a ritual to make the sunrise, then you would subvert the physical
reality. But what you can subvert is the psychological reality and the social reality.
And in this form, magic does exist. If you get right access on somebody else's perception
and attention and memory and imagination, you can change their reality in any way you want.
And in our culture, there are some norms against this or there used to be norms against it.
And I think that in Christianity, this didn't exist. It was legitimate to subvert the reality
of other people by telling them, here is an omnipotent agent that is part of reality,
therefore needs to be modeled in your own mind. And omnipotence means it knows everything that
it is to be known as full read access to your mind. And omnipotence means it has full read access.
And also, we have a back door to this thing. Every week, you can get an update and we tell you
what this agent is going to do to your mind. And as a result, you have people that remember
having seen miracles, right? Because something has written, rewritten the mental structure of
their own mind. And you often find patterns of this in ideologies. So this idea that somebody
else gets right access on your own minds, for instance, an innocent example is here are my
pronouns. And these pronouns are not what you perceive. They are what I want you to perceive.
And I have the right to change your mental representation. That's a form of magic.
Right. And I think that the idea that this is happening is because the people who propagate
these ideas don't believe in the individual autonomy of individual minds to create realities
and having a good outcome. You need to control the realities that minds create together by using
magic to get the psychological realities of individuals to converge to the desired social
reality. One of the things that I'm going to ask Gustavo to ask a question in a second,
but one of the things is one final question. One of the things I find interesting in your
thinking is the role of, well, use the term ideology, use the term religion. But to my mind,
if they could be seen more in the realm of myth, I mean, there's a lot of this kind of this space.
And the way that you use the term actually also reminds me of the work of Zizek. I mean,
he makes a comment. He read a book about love at one point and he kept the conclusion that love
is the myth that fills the gap between the self and the other. And somehow myth has been some
structuring device by which you look at things where it conditions your understanding of reality
a bit like ideology or in a bit like religion. Is that something that you would engage with?
I would engage with it, but I think that love has a more concrete meaning. Love is the discovery of
shared sacredness. And sacredness are the purposes above the ego, the purposes to which
we are willing to sacrifice ourselves. This has to do with being part of a transcendental agent.
Not everybody has that. If you are a sociopath, you will not have purposes above the ego.
And so you will be incapable of love because you will not have shared purposes above the ego.
You might have romantic infatuation, but ultimately you are not going to build shared
agents with others for non-transactual purposes because you share purposes with them. So love
is this discovery of shared purposes. But can you use the term shared? I mean,
how do we ever know the other? How do we ever accept the other? We can think that we're sharing
things, but are we actually sharing things? We do this in the same way as we know ourselves.
These are model creations. The other is a story that we are creating about a certain state of
affairs in the world. It's in this sense not objectively true, but it's a model that allows
you to predict reality better than other models that are competing with it. Interesting response.
So Gustavo, maybe Gustavo is a postdoc at UC Santa Barbara. Gustavo, would you like to
ask a question? Sure. Thank you very much for the wonderful talk. I think I want to kind of
build on Matt's question a little bit, but more specifically to the idea of understanding
how computational systems are, let's say, evolved and programmed at the scientific level. Like,
what is the state of the art in modeling either psychological states or understanding how different
models are building on knowledge where computational models can make creative leaps?
So if it's not clear, I'm thinking about in the history of human society, they're different models
of control, the models of narrative, you know, they're different, either religions or different
belief systems. But in the models of science, it seems as though that there is a building of
there is a building of knowledge and that we're moving toward an end. So where we will never,
as an example, we right now won't live to the end of the universe. But there is a goal in science
that we as human beings need to propagate outside of our, you know, cosmos. So we have a chance
to exist. And we have multiverses. How does, how do these computational models either aid humanity
or are we looking at these computational models to exceed humanity in some way?
And what does that mean? I'm thinking about that edge that you're talking about.
Because I think a lot of what we talk about in humanity are black boxes.
If you talk to a physicist or or a mathematician, or an electrical engineer, they get to a point where
we don't know the science. What are your thoughts about that? How do how do we build better systems?
Or how do we interact with these systems a little bit more ethically or morally,
so they're not like psychopathic? Or anyway, maybe that's a little too abstract and just
you brought a lot of higher level, a lot of knowledge here. So it's
it's very sobering is what I'm saying. Sorry about that.
Don't be sorry. But I do welcome sobriety if it emerges. I think that you made some very
interesting points or arrived at interesting pointers. You saw some images that I presented
earlier, for instance, the generative art of glide and the text that GPT three is producing.
And in some sense, this is the state of current computational creativity.
And I think that the problem of how to make a technological system creative is solved.
So they these images are, in some sense, creative solutions, because they are able to bridge
certain discontinuities in a certain space by finding solutions that people might have difficulty
to find. So, for instance, if you have a conversation with three, it's usually better
than what you get from a person that doesn't know the domain, but worse than the person that knows
the domain. Well, for instance, you have a conversation with Anna aren't. And if you haven't
read a lot of other aren't, it's really surprising and very convincing. But if you're very familiar
with Anna aren't and have thought about her a lot, then you might notice some things that you
probably wouldn't have said. And a similar thing is this our depictions of art and so on. So it's
able to produce certain styles and reproduce them. But there is a certain thing that is missing. And
I think that what we cannot do yet, and the systems cannot do yet is art. And the difference
between art and creativity is subtle. Art, I think, is the capturing of conscious states
of a conscious reality of some aspect of a conscious reality. And this means meaningful
references to a unified model of the universe. And these models do not have
unified models of the universe yet. They don't understand which universe they are part of or
think that they are part of. And while they are slowly getting there, I don't think that they
are there yet. So to me, the digital art that you're seeing is not actually art, because it
does not mean very much to the system. But it's something that humans can, at this point, relate
to their shared reality sometimes or to their inner reality. And this makes them
akin to art. And there is basically a porous boundary that is more and more dissolving in
terms of AI and art in these days. So I don't think that there are fundamental unsolved problems
at this point. But the biggest important problem is how to get a system that is able to track
reality in real time and that can online learning. And a lot of people are working on this, and we
don't know how long it will take to solve it. But it's not that it's a principle unsolvable.
Can I just pick up on that, the question of art, because the back of my mind, there's an interesting
kind of question coming up here in terms of, well, let me just throw out to you an idea,
because you used the conductor metaphor, and the music was part of the discourse. And I often
speculated whether how creative we are as architects or how artists are in the sense that
there is a canon. There is a canon of architecture or art or whatever it was. And what we do is
normally keeping broadly within that canon. You're very much aware of what other people have done.
You might push the boundary slightly. And I use the term jazz as a kind of idea of understanding
how we operate. It's a background condition, and we're feeding off it and just nudging the
boundaries, but staying recognizably within the canon of this. And it's interesting that,
I don't know if you know the work of Ahmed Al-Agamal, the guy who created these, who designed
creative gans. He's a computer scientist who has a, who generates art. And the logic is this,
you've got to keep broadly within the framework of what you're talking about within the canon.
And let's say modernist art, but make it slightly different. So you're just pushing the boundaries.
So I, you know, I often wonder to what extent we're so conditioned by what has been done before
and whether we, if you would do something generally different, you'll be outside the realm of what is
acceptable in that genre. I would make a difference, a distinction between art and design.
And architecture for the most part is not art, but design. It's also true for myself. I'm for
the most part not an artist, but a designer. And design is instrumental to something. Whereas art
is instrumental to consciousness only, I think, at least the aspects of the thing that you're
producing that are art, the other aspects and every artifact that you are producing, almost
everyone, the design serves some other purpose than the consciousness itself. For instance,
if you are designing a building, you are serving a function of a human being that needs to have
a house somewhere that needs to live down somewhere, this thing needs to be part of an
environment and it requires deep perception. And it does require capturing some of your
observations and a deep level. So there is important elements of seeing and perceiving
and observing and reflection in architecture. But all these elements are ultimately instrumental
to the thing that you're going to build. And the thing that you're going to build is defined by its
function. Maybe I could just throw something out there and let's say that could you not,
as an architect, always think about these things. And I think almost there are two sides of, well,
there are two sides of architecture. One's a functional side of thing, or dealing with, let's
say, with the logistics. If you're dealing, let's say, with a very complex urban condition,
you need to fit a building in somewhere, it becomes almost like a simple search question of how do
you find the best solution? And then there is this kind of, I wouldn't say a veneer, but there is an
aesthetic side of things. So when it comes to, let's say, the strategic planning of how you might
fit a building in a site or how it might operate and so on, it kind of relates more to the kind
of logical, let's say, of AlphaGo, the strategy of AlphaGo. And then there's something else that
we as architects want to put on top of that, which is more the kind of the artist dimension,
which is giving it a certain aesthetic. Does that sound to make sense to you, that logic?
Yes, it does. But there's, of course, the practical element that the aesthetic that the architect
has when it's a successful one is a brand. And it's not driven by a free exploration of the
conscious states of the architect, for the most part, but it's driven by the anticipation of reward
in a particular economic and cultural domain. And so in the sense, it's usually a construction
process. Well, let me throw out another kind of thought then. I mean, what is interesting
is when you get someone who's fairly radical, like, I don't know, Frank Gehry, for example,
he produces a building, the Guggenheim and Bilbao, really changed architecture,
but then he kind of repeats himself in some senses. He's doing similar versions of that,
and you must have seen the LA, the Philharmonic in LA, the Walt Disney concert hall. And it's
almost like, you know, we have these patterns of behavior or signatures, I would say, you know,
that are recognizable. And now you see a Gehry building, you say, that's Gehry. So it's almost
like we're pieces on a chess board, and then we have certain conditions, and we actually
are constrained by that. So whether you see it as a brand or not, but it could be seen as a brand,
you know, we are constrained by our own signatures. And in fact, when we end up not being so creative,
because we fit in with that logic. How does that sound to you?
Ultimately, it's about intention. And the intention is that the submission to an external
cultural mind, or the intention is an autonomous one. And I personally see art as something that is
driven autonomously, and that's different from the definition of the art market, right? So the
art market is only capturing a very small part of the arts. And a lot of the things that are
happening on the art market are not art. And my own father is an architect who has defected from
architecture, and become an artist. So I am a child of an artist family. And my wife is an artist.
And this the difference between the art that my father is doing and the architecture that he has
been doing is that the architecture is serving others in a particular role for in a particular
cultural context and economic and social and societal context. And my father didn't want to
submit to this societal context and this psychological and social context, because he
thought it was deeply unesthetic to him, we rejected the aesthetics of the society that he was in.
So we removed himself from the society that he was in, bought a watermelon in the countryside
and turned this into his own kingdom. And this kingdom is open for others to visit and explore,
but it's not done for them. It's done for itself. It's done in the service of his own aesthetics.
And to him, it doesn't really matter whether others like these aesthetics. This doesn't change
how he thinks about the things that he's creating himself. He may need economic success to be able
to survive and it might frustrate him if people don't like what he's doing. And it might frustrate
him the things that he might have to do to survive. But his own definition is that he is not for
himself of his own intention, that he is not serving an external aesthetics. He is an autonomous
agent. He is deeply autonomous. He's the creator of his own universe.
No, it's a beautiful story that I've got a question coming in from Shamene in the chat,
but can I ask you one quickly question before we go on to that? And that's to say,
the term architecture gets used, obviously, both for computer science and for architecture itself.
And I'm just wondering, I mean, I don't know, my definition of the architect is very broad. I mean,
I think it's a way of kind of, I would say that probably what your father is doing is probably
still a form of architecture, maybe a form of other architecture. I mean,
many of creative industries that architects go into, like the film industry or all the space
industry, and they use that architectural imagination elsewhere. And I even think that
setting up educational systems is a form of architecture in a way. And I just wonder whether
you ever have seen any connection between those two architectures, the one that you're familiar
with or your father, and the world in which you work right now, computer science. Because I
noticed the word architecture is in the title or subtitle of your book.
Yes. So the notion of a cognitive architecture means that you understand the mind as something
like a building or a structural design that is inhabited by lots of functionality
and is serving functionality in a larger world that it's embedded in. So it's natural to think of
the mind as something that is constructed rather than just grown. And that's also the limit of the
term architecture in a way, because the mind is not just constructed, it is also grown.
And so there is the question whether growth is an architecture, is a forest architected
in a way. And I think it can only be architected to the degree that the forest is sentient and
starts breeding and structuring itself. And maybe it is. So maybe there are elements of
design and construction in the forest. So it's not just something that is locally grown by
some dissociated process that does not have a centralized spirit that reflects functionally
on its relationship to the world. So Shamine has got a question in the chat,
she's in a noisy cafe, so she can't ask it herself. But I should say that Shamine Yusuf is
from Iraq. She actually studied in Germany in the other Bauhausstadt in Dessau, where I myself was
a professor for a while in the building next door to the Bauhaus itself. And there's the
School of Architecture. And Shamine was one of my students for a workshop there. And let me read
out Shamine's question. Thank you Yosha for the great lecture. This is Shamine Yusuf from the
School of Architecture at Florida Atlantic University. My question is, it seems that there is no
condition for sentience for an agent in brackets AI model, for example, to be creative and to be
conscious in brackets, if I understand your thesis well, close brackets. So do you think that we,
human agents, are discriminating against the machine since it's not a biological being,
and therefore we should instead consider intelligence and creativity based on the behavior
of the machine, which has proved to be true or is becoming true in the near future? Shall I
read that again, or is that in sense? Yes, I think I understand where the question is going.
It comes down to whether technological systems, once they approach functionality that is similar
to ours, should get rights that are similar to ours. And at which point we give these rights and why?
Is this a viable interpretation? I don't know, should we like to comment on that in the chat?
Well, maybe while we... So I would say that sentience is in some sense the ability to know
what you are doing, which means you have to have a model of yourself and the relationships to the
world that you are in. And in this sense, I would say that, for instance, a corporation can be sentient.
The corporation has a legal economic structural, functional notion of what it is. And this notion
is represented in the minds of the people that work for this organization and the balance sheets
of the organization and so on. It's often distributed, so it's not a single point where the
entirety of it is represented. But functionally, you could say that the organization can converge
towards sentience. And the more sentient it is, the more it's aware of what it's doing,
the more successful it's going to be because it allows it to make a model of its relationship
to the world and act on that model. But a corporation, I think, is quite clearly not conscious.
So there is nothing what it's like to be a corporation. And it doesn't mean that corporations
could not be conscious in the future. Imagine that you replace the people that make the decisions
and information processing of the corporation gradually with machines. And this gets one more
real time until it gets entangled with the world in real time. And at some point, it will discover
itself as a real-time agent that is paying attention in real time. It's not clear to me
whether this will be human-like consciousness because it needs a control model of the attention,
because our attention is selective. And the selective nature of consciousness is quite
constitutive for it. And if you have enough computational resources, maybe you don't need
to be selective. Maybe you can do everything automatically without having this layout of
reflection and be good enough. So maybe consciousness is something that exists at an
intermediate level only. So it exists in systems that are complex enough to have this kind of
coherence creating a government-like conductor that is making sure that your free jazz is going
to be coherent and is going to be instrumental to what the organism needs at any given moment.
Or maybe you can create this coherence just by tuning the orchestra well enough and making it
more tight that you can do this in a biological system. And at some point, it doesn't need a
conductor anymore and just does everything in a mechanic way. So I don't know that. It's an open
question to me. With respect to the other aspect, whether we should give something rights, the rights
that we have as human beings are instrumental to the function of our own society that don't exist,
because people have an insight in what it is like to be a conscious being. The animals that we are
slaughtering in our afterhouses are conscious. It's quite clear and obvious, right? They do act
on the awareness that they are aware. The cat that I have in my household is aware of the fact
that she is aware and that I am aware. And we are able to communicate about this fact,
even though the cat is not that smart. But I think that the cat knows that the cat is conscious.
And this does bestow some rights on the cat in our household, but it doesn't bestow rights
of on the cat in a similar way in society at large, because the aesthetics of our society
sees animals as instrumental as tools. And this is probably also true for AI. On the other hand,
if AIs achieve superhuman abilities, and in many ways they already do. So they are
crassly subhuman in many ways. They cannot do many things that humans can do, like create
coherent world of meaning. But there are also things that they can do much better, like
star transfer or generation of imaginary dialogue with historical people. They're able to do this
much faster and with better quality than most people can do it. And so if you basically imagine
that you have systems that overcome their current limitations and become superhuman and all the
levels that mean, why would these systems be interested in having human rights? You're not
going to live next to these systems anyway. You're going to live inside of them. We will be their
gut flora. Why would the organism that sees us as its gut flora at best, would want to have rights
that are akin to gut flora and instrumental to the aesthetics of the interaction of gut flora?
Who cares, right? So why would a corporation want to have human rights? That's not interesting to
a corporation. A corporation is operating in a very different domain and has much greater rights
in this domain and abilities than a human being does. So I don't think that this will ultimately
be the issue. I don't think that the systems that we are building will be necessarily subservient to
us once we make them sentient and conscious. Can I invite Manos Tomiso to ask his question?
Manos is doing a PhD on AI. The architecture is also associate professor at FAU. Manos,
would you like to unmute yourself? Hi, Yosha, and thank you. It's been very stimulating. So
I apologize in advance. It's a bit of a long question and seems to be very specifically
formulated, but I'm interested in the broader discussion about computational creativity in
your earlier comment, first about Minsky's positioning in terms of the part of the mind
treats the rest of the mind as its environment and how this is relevant to that idea of the
search base and how we position ourselves in a search base if we're able to externalize
ever a source from it with regard to discovering something. So the specific part of the question
focuses on the neural language-based models like Glide or VQ Dan plus Clip, which I've also been
trying to work with a little bit from an architectural point of view, not so much a technical one,
and how, you know, what, for example, we begin to perceive as a simple language prompt, you know,
a one word prompt, may in fact be much more complex than that. And so, for instance, a prompt like
building or a window, even though the network would address this with the same procedure,
you know, we would know that the former is a richer semantic representation in terms of
one's inclusion within the other, right? A window is meaningless without a building.
But with regards to the way that the network treats that, they could both be perceived as
a high level feature details, depending on, you know, like a window by itself could be perceived
as, let's say, a high level feature within like a broader building representation, but the same thing
could happen, you know, with regards to the way a building could be scaled and nested within
a broader, larger urban landscape. So, all I'm saying is how, if you have any comments with regards
to this kind of discrepancy, which seems to, you know, address, of course, the reductionist,
maybe understanding of language, but how perhaps this could be encoded in a different way, right?
What we perceive as a simple prompt is not necessarily a simple prompt, and a human
is able to understand it, but the network would be reading both of these terms on
unequal terms. I'm not sure if that was clear. The issue with the existing models is that they're
not trained on the same reality as ours, but on the representation that we have created,
and this representation is inert. So, for instance, GPT's language is not trained in the same way as
our language is being learned. Our language is being learned as a solution to a particular
kind of problem, and that is how to transfer mental representations across people and how to
organize mental representations within our own mind to transfer them. And this is achieved by
mapping the representation, which mathematically is something like a dynamic hierarchical graph
into a discrete string of symbols. Language is always a discrete string of symbols,
and the main reason why this is the case is otherwise it wouldn't be learnable.
And this discrete string of symbols that hangs in this thin air between speakers has to be
constructed and deconstructed or reused for constructing a mental representation using
limited resources, something like a stack of not more than four, because while language can be
defined in such a way that it's infinitely recursive, our own mind is incapable of facilitating
deep recursion because it only emulates it. So, it needs to be simple. And all the natural languages
are solutions to this design requirement. Find a learnable method to map mental representations
into discrete strings of symbols. And this is done in a collaborative process.
Basically, language is invented by groups of people, not just by individuals for the most
part. There is no reason why an individual couldn't do this. In the same way as you can play chess
against yourself, you can play language games against yourself and invent your own private
language. It's not an argument that I can see. It's plausible against that. But practically,
it's a tool to transfer information in a large degree. And GPT-3's language is not the result
of this interactive learning. It's a result of looking at the linguistic utterances of people
as they are typed out in the internet in a non-interactive fashion. So, GPT-3 doesn't learn
semantics in the same way as we do. We start out with understanding semantics indexically by pointing
at the features in our perceptual environment. And then we learn syntax. We learn how to
translate this into linguistic symbols. And then we learn style. That is, the particular way in
which linguistic symbols can be arranged to communicate efficiently and to convey additional
layers of meaning by the shape of our utterances. And GPT-3, the order is inverted. GPT-3 basically
starts out with style and syntax and learns semantics as the long tail of style.
Right? So, it's, in some sense, the wrong way around. And it's amazing that it converges at all.
There has been, in the early days of computer linguistics, rich discussion with philosophers
who still haven't updated, who saw that you cannot learn semantics without interaction
context, without embodiment, without being, having symbols that are grounded in perception.
But GPT-3 shows that it's possible to learn semantics to some degree only by looking at language.
And you can see that it's semantics because you can ask GPT-3 for instance to perform
certain linguistic transformations or to add small numbers to each other and so on.
And it's capable of doing that, which is a semantic operation that has a causal structure
that is being addressed by an linguistic prompt. And GPT-3 is able to verify, in some sense,
whether it was able to conform to that specification. So, these are proper semantics,
but they are impoverished compared to human semantics. Because there are the result of
something like bubbling of extrapolation only without interaction. But this doesn't mean that
we cannot do this. In principle, we can build systems that interact with the world and that
are serving instrumental purposes and satisfying their needs and doing this and that do on their
learning. It's just the present set of algorithms and technologies that we have are not very amenable
to this. Let me just, maybe, sorry, just push this a little bit further. I mean, I often,
because I think that the GPT-3 and the kind of text or the prompt based responses you get out of
clips, certainly, are interesting because I'm just wondering to what extent we ourselves are
trained a bit like a neural network in the sense that we have certain inputs. You go to school of
architecture and you are schooled in a certain way of thinking. That's what you do. And so,
when we think of something, someone says a house, then if I'm trained in a modernist thing, I will
think about the certain images. At least something will be conjured up in my mind that is quite
controlled in a way by the training that I've had. So, I'm struck that actually maybe there are not
that there's more similarities there than we think, whether we have an automatic reflex about certain
things based on our conditioning. Maybe I could just, Bob, while you're thinking about that question,
show you a quick video of the kind of work that we architects have been doing using this.
So, this is a work of an architect from Peru, who's now teaching, and it's using clip and
VQGAN. And there are a series of prompts. There are a series of pre-proms. So, there are three
very progressive architects names we'll put in there. Zaha Hadid, Tom Main, Wolf Pricks, you
probably don't know these guys, but they are kind of Gary-like, slightly crazy guys, right? And then
there's a second prompt, which is, the main prompt is futuristic Indian temple. And this is the kind
of thing that gets hallucinated by this thing. And I often wonder whether, yeah, my question would
be this, would it be fair to say that actually that we are trained, we are trained by our experiences
and our education as a form, obviously indoctrination, to think of certain images, to conjure
them up in a way almost like clip does. Okay, yes. So, there is a big similarity in the way in which
these models work and in the way in which our own mind works. The difficulty is the way in which
the GPDs we got to these representations or in VQGAN. And it's basically VQGAN is fed lots and
lots of separate distinct images that are annotated with text with a reference to, for instance,
the subtitle of the image that the creator gave it or even to a complete description of what's
happening in the image. And by looking at millions of these images, in batch processing, doing
statistics over these images, you build up the structure of the network. And the network ultimately
converges to an efficient representation of this latent space of representations. And in our own
mind, this representation is built in a slightly different way. We train up layer by layer. We
start out with an extremely limited reality. And this limited reality in the first place is
maybe it's similar to what's being described in the first book of Genesis in the Bible. I think
that the first book of Genesis in the Bible is misunderstood by the Christians or mistranslated
as a myth about the creation of a physical universe by a supernatural being. And this also leads to
the confusion of our culture of what that physics contains, light and darkness and sky and ground
and so on. These are clearly constructions inside of the mind, categories that have us to make sense
of the perceptual patterns in a coherent way. The fact that there are not that many ways in which
you can arrange the perceptual patterns doesn't mean that the reality is structured like this.
It just means that if you have a brain with these parameters, this is the best way to
compress physics into a predictable model. And so what you need to make sure what you need to do
to make sure that you can interpret reality is first you need to figure out how to entice
neural oscillators to make light, to represent contrast. And this is basically the creation
of light and darkness and how to separate the light from the darkness. And then you arrange
these contrasts along multiple dimensions and then you discover the modalities of perception
like vision and sound and you discover that the visual domain can be arranged in a space and you
can align the space with your vestibular system so you got them up and down. And you got a plane
that is two-dimensional on the ground down and you got a space that is three-dimensional on top
of the two-dimensional one and then you have basically the sky and the ground that you have
constructed, right, created in your own mind. So the mind is constructing these categories
and then it discovers the materials, the solids and the liquids and the organic shapes and
the animated agents in the world that move around in it. And then it discovers the features that it
cannot directly interact with but perceive like celestial objects in the background. And then
it discovers all the constructs, the plants and the animals and gives them all their names,
right? This is this gradual construction that happens during our cognitive development where
we train up our model of reality layer by layer. And then last but not least, we create a person.
And this person is created in the image of this constructive mind as the conscious observer that
does make sense of reality but it's slightly different. While it is a conscious observer,
it is created as man and woman, it's created as a human being that believes that it has a gender,
that it has a relationship to the world, that it's desires, it's human desires, it's social
embedding matter. And initially, this is created often in the third person. So when you talk to
small children, they often start by referring to the organism that they are modeling in the third
person. And then at some point they start looking through the eyes of that character and think
there are that character and the original world creator that is more than reality and creating
and shaping it becomes a subservient perception module to this personal human agent. So this
gap in the creation of this new thing is represented in children losing their memories.
And you have a baby, you will often notice that they do have coherent memories. They're also able
to talk about them once they start talking between nine and months and one and a half or
two years. And then at some point there is a gap and they lose the access to the memories that
they had before that time because they constitute themselves as a new system that indexes the
memories from a new perspective. And I suspect this is what's being alluded to in Genesis. I
don't know whether it's literally true but in this interpretation whether it's a better interpretation
than a Christian one but it seems to be much more plausible to me that this is what's described there.
It's this cognitive development of a system that starts to arrange the features into maps of reality
that build on top of each other become more and more complex until you discover your own agency
and use this as a perspective to make sense of reality. And this is what you're not doing in
AI systems right now. But there is no reason why we shouldn't be doing it ultimately and there are
a number of people which do actively think about this. For instance, George Stiener,
Bormitt, MIT and others. Can I just pick up on the other question of kids? Because I think that's
incredibly fascinating for all sorts of reasons. There's a book by Ken Rewortle when he kind of
he says that we learn to role play through being a kid. We learn to be the CEO of a company by
playing doctors and nurses and one of the cowboys in the Indians of God knows what else
when you're kids. Which is interesting and I buy that. The question that I would want to put to
you is if we see ourselves as a model and see ourselves through that logic, what role does
the actual model, i.e. the doll or the teddy bear play in a kid? Because that is in some
senses animated by the kid, by the child. To my mind it's fascinating. Dolls and
teddy bears. How do you see their role?
There is a thing that I noticed when I was in Madagascar. I saw a lot of children that lived
on the street by themselves and there were kids that took care of other kids. It was mostly girls
who did this and I suspect that the dolls that you give our girls or that our girls demand
are a substitute for biologically adaptation. That is that kids look after other kids but
the parents are working the fields or hunting or doing other things. And often we think that
kids would be callous and could not be trusted with babies but maybe they can. I've seen it in
Madagascar. So I've seen little kids that were mostly girls that were barely strong enough to
lift up a baby because they were only like five or so and still seem to be able to take care of
them full-time. So I think that's an adaptation that we want to take care of others and especially
of children and that we want to interact with other agents and build a communion with them.
And the teddy bears and dolls are a simple labor-intensive way of substituting for this.
Maybe I could put an architectural dimension to that. What about the dolls house? I mean because
that is an architectural space in which the doll operates. Yes, it's a play space and the purpose
of play is the creation of training data. So you use this to create situations that could exist
in the real world at dramatically reduced cost. So you can't ignore the cost while you're playing
because you don't play for the expected reward. You play for your ability as a way of
exploration and not for the exploitation for being able to use this later. And it's also something
that you can observe in cats. Cats do play a lot and the purpose of play in cats is that they're
able to hunt better. And while they exert a lot of energy but it's mostly done because doing this
in the world out there is more costly overall. And the same thing happens in human beings. The
reason why children are fascinated with doll houses, I remember that it was, but I was even
more interested with building virtual cities. So I used to draw very big maps that covered the
floor of my room of cities with different houses in it and explored how people would live in there
and how goods and resources would travel in this city. And I found this very exciting.
And the relational space in the doll house between the different members of the family
were not that interesting to me. But I suspect that's because I'm pretty stereotypical male in
this regard. I'm much more interested in systems, conflicts and explosions than I am in human
relationships pretty fault. And I only discovered the beauty of human psychological structure and
relationships later in my life. So maybe just to follow up, I want to give Matt a chance to
ask a question. But just follow up. So where does the architectural model fit within this logic?
I mean, you're doing your kind of SimCity for, and then you've got the kids doll's house and
things. How do you see the, what do you, I mean, of course, at one level, the architectural model
is just a scaled down model of potential building. But do you see it invested with any other
potentiality? I think that's tied to the notion of aesthetics. And aesthetics is what you get
when you take your, the preferences that you start out with and extrapolate them into a sustainable
world. You basically systemic thinking where you add one more layers until you discover enough
symmetries to digest your initial preferences and make them instrumental to achieving this
aesthetics. But it's also apparent in moral development. We start off and out with moral
reflexes, certain priors that we are born with innate tendencies to consider a certain behavior
to be moral or immoral, full stop unconditionally, not because we understand what it's good for,
but because we feel this feels moral or this feels immoral. And this can also misguide us
because ultimately, ethics is about the negotiation of conflicts of interest under
conditions of shared purpose. And this requires that you understand the aesthetics, the world
in which you want to operate behavior is only good or bad. If you can come connected to an
expectation of a world that is worse or better, and to be worse or better, you need criteria for
what makes the world worse or better. And this, I would say that to be good, it needs to be
sustainable, it needs to actually work. And it should have high complexity. And complexity is
in contrast, for instance, to friction that is exerted to violence, you want to minimize the
friction and waste created to violence and so on. So once you discover this train of thinking and you
get older, many of your initial moral convictions get replaced by the larger aesthetic. And the
same is true for architecture. By architecture, and you design a building or a city or a house or
room is all about how to fit the space that you're operating in where you make your local decisions
into a larger aesthetic. And so the deeper your understanding of the world, the better your design
has a chance to be. And this is what makes architecture so interesting to us, I think, that is
that it's about seeing the world, the human world that we are part of, at the greatest possible
depths that we can perceive and extrapolate the games for as long as we can make them and then
design our life inside of this larger space and build things at the largest scales that we can
maintain, like cities, nation states, society, civilizations, inside of these aesthetics to
reunize them. I often say myself that I think architecture is less about the literal design
of buildings, but about imagining a better world. So we have a question from Matt, a second question
from Matt. Do you like to ask your question? Sure, just another quick maybe jump back into a
bit more, maybe more technical things. I'm always struck by these images that have become really
common of neural networks as little dots connected by lines. And you showed the cortical columns
and you showed a lot of interesting graphics that kind of looked like that. But I've also recently
been struck by the neuromorphic computing stuff that's going on at Stanford and a few other places
about dendritic computing and sort of new advances in thinking about and even starting to see how
what we once thought were just wires that connected all these different things. And we
talked about connections are actually doing preprocessing in very interesting ways. And I'm
sure you're you know a lot more about this than I do. So I'm very just interested in what's going
on there and how that changes these questions of how much energy it takes to do the computation
and what maybe the future form factors might be on this kind of thing. And we also have groups
at Intel that work on neuromorphic computing. For instance, we have the loyalty architecture,
which is a chip that uses a model of spiking neurons for modeling perceptual content and so on.
And this is in some sense compatible with the neural networks that exist because you can
translate the traditional neural networks and the many circumstances into the spiking neural
representations and vice versa. But these spiking neural representations are more efficient with
respect to power usage and some recognition algorithms than others. And so there will probably
be useful applications of spiking neurons. But the reason why the neurons in our own brain
are spiking is also in part because the messages that neurons consent to each other are limited
in their nature. Neurons cannot produce continuous signals. They have to produce little pulses.
And so you have to encode the information into little pulses in the timing and frequencies
between the pulses. There is also an issue when we think of neural networks as they are represented
in our technological system. They are mostly circuits. So they are similar to the circuits in
your present CPU, which represent logical gates, which implement logical operations.
And the connections is stored in the weights. So the parameters in GPT-3, these are all
weights, little factors by which the activation that is sent between the different nodes is being
multiplied. And the equivalent in our brain to these weights are often seen as the synapses.
And the synapses come with different types. Different neural transmitters in some sense
are message types that are connected to different synapses. And the big network structure is what
we call the connectome, the circuitry that exists between the neurons. And there is hope that if
we manage to digitize the connectome at a sufficient resolution, that we might be able to upload a
brain and simulate it in a computational subscript. And in principle, that should be possible in
practice. It doesn't work so far. So even the models of C elegans, which is an hematode that only has
a little more than 300 neurons, if you completely digitize the elegans
to my current knowledge, I'm not sure if something has happened in the last couple of years,
these models don't work in the sense that you simulate the worm with these digitized neurons,
and you can digitize the connectome, the worm doesn't move like a worm does. It just twitches
and has a seizure, basically. And that's maybe in part because the neurons are more complicated
in the worm because there are so few of them, they basically exploit certain resonance effects
that maybe your model doesn't. So maybe it's more of a dynamical system that is more difficult to
model. And there's another problem is that you cannot actually get the message types right,
because at the level at which you do the connectome, you cannot model all the vesicles that
send the different neurotransmitters. You don't know actually which synapse is sending which
type of message. This is also a limit, but there might be something worse going on.
In the 1960s and 70s, there was a series of experiments mostly in the Soviet Union,
but some of them also in the US about RNA-based memory transfer. And the idea here is that you
take an hematode or a C-slug or even a rat, and you teach them something by our current conditioning,
and then you put a new tissue into a blender, extract the RNA, and inject the RNA into a different
organism. And the new organism knows how to do this. This is completely wild, because if this works,
and it's disputed, whether it actually works or how well the experiments replicate, even though
some people have done that again, and so the people that verb on this tell me it's difficult to get
other neurosenters to listen, because it's incompatible with the idea that the weights are
stored in the synapses. If it's really the connection between your neurons and you
put the nerves just into a blender, this goes away. How would you be able to transfer memory in
this way? It cannot possibly work, because the RNA that you inject in the brain is not localized.
It would get into many, many neurons at once. So how does each neuron know which parts of the RNA
to use? And if you take this idea seriously, I started thinking about this, and now I'm
thinking about making some simulations. How deep does the rabbit hole really go? It would mean that
the individual functions that your neurons learn are not unique to the location of the individual
neuron, but they are global functions. So the RNA space, you can think of it as a little magnetic
tape, that the neuron can mix and match and create more of, if it's useful, and share
with all the other neurons. Just across cell boundaries, you share the RNA and you copy them
like COVID virus. And you use this function to respond to certain patterns in your environment.
So the neuron is not reacting to its neighbors that could come into a particular kind of connections,
but it's mostly connecting to a temporal and spatial pattern that arrives at a certain function,
regardless of where the neuron is in the neural cortex. And so it's more like a cellular automaton,
a neural cellular automaton. And this suddenly allows us to explain a few things that seem to be
going on in the brain that are difficult to explain with synapses. For instance, if you destroy
synapses, they often be grown exactly the same way without retraining. There's another phenomenon
that is used to notice a certain mental representation in a cortical area, pinpointed, and the next day
you look and it has moved. It might have shifted a few millimeters or it has rotated. So how would
this done if it's in, it's stored in the synaptic connections? There's also the question of weight
sharing, like a convolutional network is sharing weights. And you probably need something like
weight sharing to perform a mental rotation where you have the same operation on many parts of your
mental representation in the same way. How would you do this? Are you training the same function
again and again in different brain regions? I hope it's always the same. It's difficult to achieve,
right? So we don't know a really good plausible logical mechanism for this, but this RNA based
memory transfer could be part of the story. And this is something that is at the boundary of what's
currently being explored still. And it's, I think it's not completely implausible. And if we want
to make a model of how this works, we would need to use a different metaphor than our current
biological neurons. But it doesn't mean that you have to use this, because the brain is solving
problems that our computers don't always have to solve. For instance, long distance connections
in the brain are extremely difficult to make. And you cannot really address neurons this way.
So random access is very hard in the brain. You need some kind of routing network that needs to
grow and learn how to route. It's not an issue in our digital computers, because
sending information across the memory of the computer is trivial. So there are many things
that you can do very easily in our digital computer center, difficult to achieve in the
self-organized structure of the brain. And so it's not quite clear how much we need
the biological like structures to achieve the same functionality. But to me, it's certainly
very exciting to explore it. That's fascinating. I think the questions about drift and neuroplasticity
and things like that that come from that are really interesting. It makes me wonder if some
of what we're looking at today will seem very obsolete soon in terms of these models that seem
to be so human like with the, you know, in terms of being able to hallucinate imagery and all that
kind of stuff. There's like a piece missing that might be, that might come soon from a hardware
model. Yeah, I saw that too. But then I'm surprised by what Gleit can do and Dali and
of course. Right. So when you add energy to it, though, I mean, how much energy it takes versus
a brain, which we walk around and we feed vegetables to, and it can do all that too. Like,
you can't, I mean, that's, that's where I think that the question that for me, that's where the
question became very much more significant. It's like, Oh, wait, but there's a whole other
calculation here of how are we doing all of this in our tiny little brains? Maybe there's
something there for, I think that this is misunderstood. I think that human brain is
super expensive to feed. It needs enormous amounts of energy to feed my brain, you need
like four hectares of land, you can put so many solar cells on these four hectares of land,
basically, people underestimate how difficult is to take the energy that my brain needs into
sandwiches and to extract it again. So this is the way that you need to look at also training
my own brain is super expensive, right? It takes decades and generations before that to
prepare things for my own intellect and so on to get them in there. So human brains in my view
are super expensive. And that's why we can use something like a clip and V2 again, that we only
train once at low prices, like training GPT-3 costs like $20 million. And now even less because
computational advances go down. And it's been trained by reading way more text than a very
large group of people could read in their life. So it's basically getting people to read 45 terabytes
of text would cost much more than $20 million, right? Tweeting them for long enough to make that
happen. And the system that is generating the text costs so little that open air lets you do it for
free if you want to only use a little bit of it, right? So it's able to produce output that is at
the level of hundreds of copy editors for free, right? And while that's not as good as a conscious
copy editor who understands the world, it's quite amazing what it can do already.
Okay, thanks for that perspective. Yeah, that's interesting. Thank you. I also am a little bit
provocative. But yeah, I think it's something to be considered, right? These 18 bots of your brain
if you compare them to the 80 watts of your MacBook, the 80 watts of your MacBook are super cheap.
And the 80 watts of your brain are super expensive.
I think we're going to have to think a lot more about that.
I think it's your provocation that is so exciting, is she, Yosha? I want to get a question from the
chat. Now, going back just to say mentioned to everyone that Yosha was was was born in Weimar
and where the Bauhaus came from. And then Desau is where the Bauhaus moved, of course,
is the building by Walter Gropius and so on. And we have at least two people here
who studied there, including Vasko, who has got a question. Vasko is now a professor in Bangladesh.
And his question is, well, I think it's one that you might have predicted. Elon Musk suggested
somewhere that we live. Do we do not live in a base reality, but in a simulation?
He even puts the probability of a billion to one. What's your view?
I think that Elon's argument rests on the notion that the simulations that we are building for
computer games are getting better and better. At some point, there will have a fidelity that exceeds
our ability to notice whether we are in a simulation or not. So we can at some point probably create
VRs that are so convincing that we will not be able to notice whether we are in a VR or not.
And we are not the only ones for our building simulations like this. So for any given being
that finds itself in some kind of reality that looks real, there will be many more that at the
same time will be in simulations, right? Because many universes can contain many more than one
simulation. So our universe probably contains many simulations of a universe that looks like ours.
And therefore, the probability for any given observer to be in a simulation
is greater than the probability than to be in base reality. And what this argument ignores
is the fact that it's very hard to make a simulation that actually has the fidelity
of the physical universe. But if you make a simulation of Minecraft and Minecraft,
that's feasible because Minecraft itself is so poorly resolved. But our universe has a lot of
structure that is required to produce its dynamics. And if you build a simulation of say our solar
system and the dynamics of our solar system at a level that is going to go down to elementary
particles, you will need to have a computational capacity that is larger than our galaxy by a
very considerable amount. So in practice, I don't think it's feasible to put simulations of our
universe into our universe at an arbitrary level of fidelity. And so I think that I'm much more
biased to think that we are in base reality than we are in a simulation.
This is fantastic. There's one question here that in also in the chat from Grant Castillo,
and I don't know what he is, do you think Gerald Adelman's extended theory of neural
neuronal group selection can be used to create a conscious machine?
You muted.
One moment. Yeah.
I hope that the background noises are not too high because my family woke up and is now interactive.
The kids are playing and so on. And so I think that the idea of the neural Darwinism that Adelman
came up with is a very interesting one. And I suspect that our own mind is the result of such
an evolutionary competition of different organizational forms. There could be many
possible proto consciousnesses that compete until one of them establishes itself as the
government of our own mind. And so instead of giving your nervous system a blueprint on how to
build a mind, you just set up the conditions for an evolution for the best possible mind that you
could have. And of course, this evolution is raked by evolution. So you set it up in such a way
that the evolution usually goes out ends up in a certain way. But the nice property of when you
design a system evolutionary by not giving a specification of what to look like, but what
the function is against it, which it should evolve is that when you disturb the system or give it
a different environment, that it will very often come up with a viable solution under these new
circumstances. So the solution is much more robust if you define it in terms of evolution.
But it's a speculative idea. So I don't actually know whether our mind is evolved, even though I
think it's more plausible than it's not individually evolved in every individual brain.
And it's definitely an interesting notion to use. Evolution is basically whatever you use in
computer science, then you don't know in which direction to go. It's a blind search. It's the
fallback. It's the baseline. And it's quite natural that we would use evolutionary methods
if we don't have a specification for the best possible mental organization that we just evolved
one. So do we have any further questions in the chat? We have some very interesting characters
here. I'd love to try and draw out Daniel Bologian, who's one of the leading AI architects in the
World WebC where he can have a question. Or indeed, we have Sanford Quinta, who's one of our
leading theorists, who's a particular interest in neuroscience. I wonder if I could put them
in the spot and ask if they have a question.
Well, when you're done, it's also good because I think I need to go and have some breakfast
and start my day.
Thank you. This has been fantastic. I've got to say that I think you are more of an architect
than you actually think you are. You have a way of thinking that's very similar. I mean,
obviously, you work in a different domain, but I think the kind of inventiveness and the
iconoclasm of your thinking would go down very well in an architectural scenario.
You never escape your background in a way. You might try, but in the end, you find yourself
conditioned. I'm conscious of being a child of a family of architects. My grandfather was not an
artist. He was an architect. He built most of his life hospitals. And this is the design process
that is instrumental to serving a function, but a function in the larger world that he was very
deliberately trying to understand and operate in. But this was a world that he understood as
being his own world. It was a world that he often found himself to be in opposition with, for instance,
in Nazi fascism, or also in Eastern Germany. But it was also the world that existed. And we need
to deal with this and build the best possible things in. And for my father, it was different.
It was a world that he fundamentally rejected. And so in some sense, to be an architect,
you need to embrace the world that you are in and build within it and to take roots in it.
And this is, in some sense, something that also haven't been successful in when I was young.
So I decided not to become an architect, but to become an explorer.
Well, I mean, I think one of the comments that was made on I always think about the
Steve Jobs and his response when he was asked a question by Steve Wozniak. And Steve Wozniak said,
well, but what do you do exactly? Because you don't code, you don't do this, you don't do this.
And he described himself as being a bit like a conductor of an orchestra. In a way,
that's how I see architects, in a sense, because we don't have any specialism.
Basically, we coordinate these different sort of or choreograph these different sort of skill
sets. And I think that's really what it is. So in many ways, I can see a direct comparison with
how you position yourself in that sense. I mean, I think it's a very, it's a very similar sort of
position. But I think that some of these, these comments that you've raised, they're absolutely
fascinating. I think we need time to digest them and fit them in the system. What I would love to
do above all, especially with this particular discussion is to try and find a way of publishing
a transcript. Because, I mean, I think this book that you have to write, you must be written,
because you've got some fabulous, fabulous thoughts that are really creative and original and
provocative. So, you know, I really appreciate, I appreciate so much your time. And I should
let you go and look after the family now. But this is, I think, almost like we've just opened up a
discussion. And I hope that sometime in the future, we can, we can take that further and
think through these kind of questions, because your responses have been very generous and very
really provocative and stimulating. And I feel like, you know, although we sort of pull this,
we draw this question, this session to a close, it's almost the beginning of something else
that we can look forward to. So, I just want to thank Yosha for, for fabulous. I mean, I want to
recommend his, all his online talks as well, to have a look at that. There is a body of work out
there that is, that is hugely provocative and hugely stimulating, which I am excited by. And I
also, maybe I could finish with one, one simple question. I started off the discussion by saying
that I think there is a, a kind of, let's say, an emerging theory of intelligence that is developing
in this kind of strange area where, where computer science and neuroscience and the world of the
commercial world and the academic world is coming together. Do you also see that glimpse of something
emerging, some discourse, some theoretical debate that is radically new and radically
provocative? Clearly, that's why I went into cognitive science in the hope of being part of
this new synthesis happening between neuroscience and philosophy and artificial
intelligence and linguistics and psychology and maybe the arts. And I think at this moment,
the synthesis is still very partial and in part that's because we teach our model makers and our
observers in different departments. We don't bring them together. So we have people that are very
good at making formal models that can be tested. And we have people that are very good at making
observations and reflecting about the world and seeing it very deeply. And these people rarely
talk and they're really thinking together. And this is what excites me to work in this area,
these two areas intersect. And maybe architecture is the right frame of looking at this.
Nia, thank you very much for inviting me. I think it was a great conversation to have had today
and very grateful for your beautiful community and for allowing me to talk, allowing me to talk
about these ideas with you. It's fantastic. I'm going to finish with one comment, which is because
I used to be a Latin translator. And the word, I should just say the word computation
means to think together. And I think this is what's been happening today. There's been almost like
a neurons with a neurons, a kind of global brain. It's been fantastic. Yosha, fantastic.
Wonderful. Thank you for your time. And sorry for getting you up so early. But this has been
a huge contribution to the architectural community. And I hope that I've helped draw the
attention of your ideas to architects out there because I think they're incredibly
provocative ideas. And I think they've a huge contribution to make to architectural thinking
itself. So thank you, Yosha. Thank you. Thank you so much for this. It's been fabulous.
And thank you. Have a wonderful day. Thank you. And thank you for those team that put this together,
Digital Futures team. We can't operate without your help. Thank you so much. And see you next
week. Thank you, everybody. Thank you. Bye. Bye.

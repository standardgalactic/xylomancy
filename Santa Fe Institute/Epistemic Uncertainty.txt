up in the kitchen. Very excited to introduce Donald Martin, Donald's the head of societal context
and understanding tools and solutions at Google. We don't do much for introductions here, but
Donald let me just turn it over to you. You won't appreciate it. Hello everybody. It's really an
honor and pleasure to be here at Santa Fe Institute. I actually have been dreaming about visiting
SF5 for almost 16 years. I read the book complexity back in 2007. I was like, I don't
even want to go there. So I'm now here. I'm here. Thanks to Will. Thanks for your time and attention.
The title of my talk is Epistemic Uncertainty, the AI problem understanding chasm. I kind of
updated that from a gap to a chasm, but it's a really big problem. And the necessity of structure,
societal context, knowledge for safe and robust AI. Now me and my team, which also goes by scouts,
we believe that bridging this problem understanding chasm is critical to being able to realize our
goal of responsible and robust AI. And so we're kicking off a campaign to try to imply more
participation by others in trying to understand and solve this problem. And so we've got something
we call our flywheel that starts off with increasing awareness just about the chasm and
causes our milk that drives more foundational and applied research, particularly in some of the
techniques we're going to talk about causal theory model, for example. And of course, we want that
to result in real impact in the world. The use case that we focus on is health equity.
And so right now, we're here's where we are. So we're kicking this off and
bridging the problem understanding chasm as you will see, we think requires embracing complexity.
So I figured what better place to start talking about this in the Santa Fe Institute.
Few calls to action because I know folks in this room, SFI is influential. So we would love for
after the talk, people to spread the word about this chasm, right? And how important it is to
hoping that people will also take on the task of embracing prototyping problems as complex
adaptive systems before intervening things like data science and AI because we think that's critical
for proactively mitigating bias in these systems that we're building. And then finally, a key component
of bridging this gap is investing and problem prototyping trust and capability in historically
marginalized communities. That's going to be a key source of the knowledge that we think is missing
from AI based product development. Okay, so with that, I'm going to say just a little bit of context.
I gave this talk a part of this talk last year, I had to kind of explain there's this industrial
revolution. AI is going to be really important. I don't think I have to really do that anymore.
I think you realize we're in the midst of this transformation that people are calling the fourth
industrial revolution and that AI is like a core technology within it. And of course, you know,
we have high hopes that AI is going to do these amazing things and transform healthcare and medical
care. But we're also very cautious, rightly so because of the big conflict in the room,
harmful bias that can be propagated really easily by these systems.
And so it's part of my job to pay attention to all the headlines that come out every day
about instance of bias and all sorts of domains. But I pay particular attention to
headlines that involve healthcare and medical care. And that's really because of my mom.
With my mom, Betty Martin, she died five years ago this month from cancer.
And as I navigated that harrowing journey with her, I would I experienced bias directly in the
healthcare system. And that really informs how I think about what it's going to take to mitigate
bias in AI systems applied in high stakes domains like healthcare. And so as I said,
she died from cancer, started off as breast cancer, turned into lung cancer. At one point,
she had to get part of whatever lungs removed. And as a result of that, she developed atrial
fibrillation. And so that can cause blood clots and strokes. So she had to get prescribed a blood
thinner. But the blood thinner that the doctor wanted her take would require her to stop eating
vitamin K foods, leafy greens, spinach, kale, collard greens. My mom loved that stuff. That was
like a staple on her diet. And then would also require her to have to go to the hospital every
week to get her blood tested to make sure this blood thinner wasn't harming her in some way.
And so my mom, she balked at this, she's like, Ah, you sure there's not another option? Is there
a better blood thinner where I don't have to shift my lifestyle? And the doctor's response was,
you should just, you know, be grateful because there are many people in third world countries who
would die to have access to this particular blood thinner. My mom was like, Well, you know, I don't
know what that has to do with me, but yeah. And you all know my mom, of course, but she
didn't accept that as an answer. She talked to me and my sister, my dad, my sister happens to be a
medical doctor, a psychiatrist. So she was able to find very quickly an alternative blood thinner
that wouldn't have required my mom to change her lifestyle when I owed her. And so, and we could
easily afford it with my dad's health care insurance. So we're back to the doctor, we said, Hey, we found
another alternative. But we got to make sure we're not missing something. Like, why didn't you
recommend this particular blood thinner because it doesn't have any of these side effects?
The doctor's response was, I assumed that she wouldn't be able to afford the alternative.
I assumed she wouldn't be able to afford the alternative. So that's this bias kind of rearing
its head right in the middle of the situation. So that's why this kind of informs, you know,
my thought process about how we're going to, what it's going to take to mitigate
these kind of really nuanced implicit biases. And so the real issue here is that
the doctor ignored and extracted away my mom's, her situation, her circumstances, would all call
her societal context. Didn't consider that when they made that intervention recommendation. So I
want to look at my mom's societal context in a different way. There's mom, born and raised in
the USA, doesn't know anything about, you know, what happened in third world countries. She loved
leafy greens. This is her second bout with cancer. So she, you know, had already experienced having
to go to the hospital every week to get chemotherapy and get stuck with the nail show. So she would
avoid that anywhere she could, if it was possible. And then, you know, she had us as a family, which
included my sister, medical doctor, knew how to navigate the drug industry very well. My dad,
a double retiree from US Army and the Postal Service, great healthcare insurance.
So we had this other option. But the doctor ignored and extracted away all this societal context,
didn't ask about it, didn't consider it, and then made this intervention recommendation that
conflicted with my mom's societal context. Yeah. Conversely, do you think that the doctor
thought he was being anti-biased by making this mistake, and thus, in fact, being very biased?
You know, we really have no idea. So we all have biases, right? We can't tell what someone's
intention is, right? I'm sure there's not a negative intention. But this is the result,
that when you don't seek to understand the situation or circumstances, you make assumptions,
it can lead to these problems. And so we really use this high-level definition of societal context
as this dynamic and complex collection of social, cultural, economic, political, historical
circumstances that surround people, places, and things. And interventions that ignore or abstract
away societal context can lead to unintended and unnecessary harm. So this is the key takeaway
for the talk. Like, if you don't remember anything else, please remember this and try to get that
in as we're intervening in the technologies. So now I'm going to dive into the core of the talk.
Here's the five parts I'm going to walk through. First, I want to talk about how we abstract
away societal context when we do ML and AI-based product development. Then I'm going to introduce
this problem, understanding chasm. I think it's the root cause of why we do this abstracting away.
And then the last three seconds are about what we think some of the elements of bridging this
chasm are. One is we think we need some sort of reference frame or model of societal context.
Second, we think we need a way of producing societal context knowledge, and we think prototyping
problems as complex adaptive systems could be a way to do that. And community-based
system dynamics involves communities in doing that work. And then finally, even if you have
this great technique, you have to have capability in the world in order to, like, execute it.
So let me dive into the first part. I'm going to chart this off with a quote from a great paper
called Fairness and Abstraction in Social Technical Systems by Andrew Selps, who's a leader, not leader
in the ML Fairness community. If you haven't read this paper, even if you don't care about ML
Fairness, I highly recommend it. But this quote sums it up really well. Bedrock concepts in
computer science such as abstraction, render technical interventions, and effective, inaccurate,
and sometimes dangerously misguided when they enter the societal context that surrounds decision
making systems. So you could apply this directly to what happened with my mom. In this case,
we're talking about building products and systems. So I'm going to just walk through a really high
level view of typical AI product development life cycle. I'm using a product development
context because I work at Google and we build AI-based products. The key thing they're recognizing
this is that this whole process is driven by human decision making from end to end. A lot of time,
people forget that. And the main output, of course, of this life cycle is a model-based product.
It generates outputs, typically predictions, that somehow can help you automate or
automate a process, solve some sort of problem. Now, of course, to produce that product, we have
to train and tune and evaluate that model. To do that, we have to select and prepare some training
data. To do that, we actually have to actually formulate this problem for machine learning and
AI. Like, we have to take that problem and make it fit the hammer that we have that we want to
use to solve the problem. And then, in order to do that, well, we have to actually have a deep
understanding of the problem itself. So that's typically kind of where this diagram ends.
But of course, there's a key input that drives this whole thing, and that's observations and data.
But sometimes we forget about where those observations and data come from. They're
coming from this thing we're calling societal context. And then the machine learning fair
sometimes we call it the social-technical environment. Computer scientists and engineers
and practitioners who are trying to get things done, they have many different words for
that area, this kind of ambiguous area, amorphous area. It's the real world where
this stuff is going to end up one of these days. It's the environment. It's the data-generating
process. It's where we have to look to understand the hypothesis space. It's that problem domain.
It's where that background knowledge is going to come from. It's the broader context.
And then we have some characteristics that we associated with this amorphous thing. That's
messy. It's complex. It's qualitative. It's causal and nonlinear. It's subjective. I need to work on
objective things. It's dynamic and it's historical. Why do I have to worry about what happened decades
ago? Now the issue is that those characteristics don't align very well with the statistical
processes and methods that we use to build AMML models. So that's why we tend to put up the
subtraction boundary and abstract it away. And we have particular practices and assumptions that
reinforce that. One of those is the IID assumption, which assumes that the variables we're going to
try to learn in the training data assumes that there's no relationship between them. There's
no causal relationship between them, among other things. And so that assumption, when you make
that assumption, you're ignoring and abstracting away the very nature of societal context, which is
supplying all the data. Now the other thing that happens when we extract away the societal
context is that eventually you're going to be done with training. The products are going to be in the
real world and the inputs aren't going to be the training data anymore. They're going to come from
the real world. And those inputs are not going to follow the rules, the assumptions that we made
when we built the systems. And then those outputs are not going to be in the laboratory anymore or
in the evaluation suite. They're actually going to be interventions on that societal context on
society. They're going to cause impact. And then of course, those interventions are going to
can have an impact on the next round of observations and data we sample in order to
build the next version of that product. So you've got this feedback loop that can be vicious,
right, if we're not careful about the outputs and the quality of the outputs.
Now the other thing that happens when you're abstracting away societal context and understanding
the details of the data-generating process and the problem domain is it leads to a lack of knowledge
on the other side of that boundary. Lack of knowledge about the problem domain and the
data-generating process. Things you have to understand to formulate the problems well.
Now another term we use for that in machine learning AI is epistemic uncertainty, just a
fancy word for lack of knowledge. Usually we're talking about lack of knowledge about the model
by the model, but that lack of knowledge also applies to all the decision makers in this ecosystem,
which include the humans who are making all these critical decisions. And in reality models are
basically inheriting the epistemic uncertainty of the decision makers who are making all the
pipeline decisions that are going to impact how that model performs it with the outputs.
Now it turns out that epistemic uncertainty is the root cause of two big problems in AI.
One is harmful bias propagation, which I'm going to talk about in more detail in a second.
But the other is deep learning robustness. And this also goes by
out of distribution generalization, distribution shift fits into this category. So I'm going to
drill into that for just a second to show how experts in this problem are, when I read them,
they're saying we need societal context knowledge to solve this problem. So the fundamentals of this
deep learning robustness problem is this example where you have a classifier
that's trained to identify objects and images, gets really good at it, high performance accuracy,
really good at identifying this pig. But if you just change very slightly the input data,
the image, in ways that would be imperceptible to a human, the classification changes to like,
this is an airliner, right? So that's the kind of classic example. And so the
issue here is that the classification model doesn't have an equivalent conceptual model
that we have. And these are kind of structures in our heads that allow us to reliably identify objects
under really noisy conditions. And so the experts who are trying to solve this problem,
um, and one of them is Joshua Benjo, who is considered one of the so-called godfathers of AI.
When I read what they're saying, when I read their words, I see they're saying we need societal
context to help us to solve this problem. So just to read a quote from another really good paper
called tour causal representation learning, we argue that causality, with its focus on
representing structural knowledge about the data generating process,
that allows interventions and changes can contribute towards understanding and resolving
some key limitations like robustness and current machine learning methods.
So when I read that, I see we need structural knowledge about that amorphous thing,
the data generating process, the problem domain, it's causal. I think there's a direct connection
between what they're asking for and what we're seeking in terms of being able to fill that
knowledge gap, fill that epistemic uncertainty gap. So now I'm going to introduce this problem
understanding chasm using this use case, this real world example of racial bias discovered
in a medical algorithm that's widely used throughout the US healthcare system.
This, this paper came out about four years ago. The purpose of this algorithm was to,
was to identify patients with the most complex healthcare needs so they could be given access
to special programs early on so that you could ultimately reduce the overall cost in the healthcare
system. And it's important to remember that their motivation was to reduce the overall cost in the
healthcare system. Now, unfortunately, people not selected for the special programs by this algorithm
suffer from nearly 50,000 more chronic diseases than the people who were selected.
And the people who were not selected were disproportionately black Americans. So this
is why the algorithm was deemed to be racially biased. So the question is like, why did this
happen? What led to this? And the researchers who discovered this had unprecedented access
to every aspect of how this was built, which is not typical. The training data, the machine
learning architecture, the training algorithm, you know, the performance metrics, the actual outputs,
access to everybody who made decisions to build the system. And their conclusion was that the root
cause of this failure was incomplete problem understanding. That's what it boiled down to.
And remember, understanding the problem and formulating the problem for AI is at the very
beginning of this process. But in reality, we really don't spend that much time
studying and trying to improve that part of this process. We spend most of our time
on the latter stages, the later life cycles. But remember, yeah, go ahead.
The algorithm that used the amount of money you spent on healthcare as an input.
Yeah, exactly. And I'm going to get into that right now. So the key thing I want to go back to
is that human decision and choices drive this entire life cycle and really critical ones happen
at the very beginning of the life cycle. But right now, those processes are pretty ad hoc and
informal. And that's what's leading to my mind, this epistemic uncertainty that we want to solve for.
So there's a quote that I couldn't resist putting in once I read it. This is one of the papers that
we reference and read when we're thinking about how humans make decisions and choices,
because we said, oh, this is a really critical part of this process. We should understand how
humans make decisions. But this quote jumped out on me because it says, choices do not merely
identify one option among a set of possibilities. Choosing is an intervention, an action that changes
the world. That's particularly true when decisions are being made within these AI product development
life cycles, because those decisions impact directly what ends up getting produced by the
machine learning models, which are themselves interventions on that societal context. So the
criticality of this decision making and how we make choices is very high. And so we spent a lot
of time looking at this and the research that we looked at pointed out that when we make decisions
as humans, we leverage our causal inference capabilities. And those are shaped by strong,
top-down, prior dollars that we have in the form of what they call intuitive theories,
they also call them causal theories. And causal theories are these models we build up over time
as we navigate complex realities, face problems, solve problems, we form theories about why they're
happening. And that informs our goals and our strategies to solve. So let's look at what role
causal theories played in the failure of this algorithm. And remember that the problem was
predict which patients will have the most complex healthcare needs, because we're going to give them
access to these special programs. And the causal theory that they leveraged wasn't explicit at the
time, but the one they leveraged was that more complex healthcare needs is going to lead to an
increased spending on healthcare. And so they chose that as the target variable or their predictor
or the proxy for complex healthcare needs. They said if we can predict who's going to spend
more money on healthcare in the future, we'll be able to predict who's going to have the most
complex healthcare needs. This is where the algorithm failed. And the reason it failed is
because this causal theory is woefully incomplete, right? It leaves out critical factors that impact
how much black Americans spend on healthcare, even when they have more complex healthcare needs.
These are factors like under diagnosis due to bias, lack of trust in the healthcare system,
wealth and income disparities, and lack of access to affordable healthcare.
Now take it, all those factors actually decrease how much black Americans spend on healthcare,
even when they have more complex healthcare need. And then taken together, those factors represent a
subset of the structural inequities that exist in the U.S. healthcare system. And those structures
were revealed by COVID-19, right? It showed all those structures. But those structures actually
increase how much complex healthcare need there is in that community, while simultaneously decreasing
how much they spend on healthcare. So the causal theory wasn't even close to having,
you know, the reality of the problem domain that it was trying to solve. So this is what we call
the problem understanding chasm. This difference in understanding of the key problem from these
two different perspectives in this chasm between them, right? I'm talking more about what contributes
to that. This lack of knowledge on the right-hand side led directly to that harmful intervention,
that lack of understanding of that additional societal context.
Now one of the key root causes of this chasm is divergences between communities
that are trying to intervene and how proximate they are to the problems. And then
there are ways of knowing and explaining problems. So on the civil society side, these are
generalizations, but folks trying to solve those problems on the civil society side are typically
very proximate to the problems. They're deeply entrenched in the problem domains. On the product
side, we're less entrenched in the problem domains, we're less proximate to them. Civil society folks
that have a high stake in like solving the societal problem itself,
related to the broader domain. On the product side, we're typically interested in the business
problem related to that domain. And then finally, on the civil society side, since you're really
proximate to the problem and the humans that are suffering from it tend to prioritize a qualitative
human perspective on the problem factors. So I have a question just trying to blend it to
things like the gravity project, which are trying to determine
codes for social determinants and valves and related projects. Would that be maybe some mitigating
factor where you've got the civil society data informing this decision, I think, rather than
as an organization trying to decide for ourselves what we think is important?
Yeah, that would be exactly right. The thing you want to do is get to the point where
you have some cooperation in understanding the problem, because you have two different perspectives.
Now, the right-hand perspective is not wrong. It's just a different lens in that problem domain,
and solely using that lens, you're going to miss critical things. And the same on the other side
as well. Even on the civil society side, we're trying to solve problems. We're using methods,
sociology, etc. But you also have problems with uptake of those solutions. The statistical methods
get used on that side as well. I have to mention as well. So the idea is how can we bridge this kind
of chasm in terms of how different groups think about and understand problems and get to a shared
understanding that helps us get closer to the reality that we're trying to intervene on.
Thanks. And then I just wanted to show this one more time, using that initial diagram,
that problem understanding chasm. This gap between societal context and all that messiness
and how we're thinking about the problem on the product development side.
Now, just to go back to this, our goal is, the thing I'm focused on within Google is how do we
reduce this epistemic uncertainty? And to reduce epistemic uncertainty, you need knowledge.
But we have to figure out a way to have that knowledge across that chasm and be useful during
these workflows. And so people are using all sorts of tools to make decisions. And we can't
expect them to read to Andrew Self's paper and figure out what they're supposed to do,
read the latest research. It's just not going to happen. So we have to find a way to get knowledge
across that chasm and make it available to these decision makers during every step of that workflow.
So this is what our hypothesis is, is that to bridge that problem understanding chasm
in a responsible way, these decision makers need tools that put community-validated
structure, societal context knowledge about complex societal problems at their fingertips,
especially during problem understanding at the very beginning, but also throughout the entire
lifecycle. Because you should also be leveraging that understanding when you're evaluating
a product for whether or not it's performing well. Like, what does that mean?
Right? It has to be relative to the context in which it's going to operate.
But we have to remember a couple of key things. And one is that it's factual that
historically marginalized communities are disproportionately negatively impacted when
things go wrong with AI bias. Those are just the facts, right? And you can look at example
after example. And then the other fact is that the lived experience expertise of those folks
is usually excluded from that, from those problem understanding and problem formulation steps.
And so if we, if we aren't focusing on also addressing these issues, then we're not,
we're not responsibly solving this problem. So we think there are two key ingredients for
bridging this chasm. One is in order to transmit knowledge across that chasm, we think we need
to have some sort of model or reference frame for structured societal context knowledge. Yes.
Just want to point her out. Oh, sorry. I didn't see you. Exactly.
Yeah. The slide where you said your hypothesis, what do you mean by knowledge?
So that's a good question. That's a good question. So what we need, what we mean by knowledge is
awareness of factors that make up the circumstances that, in which you're going to be
deploying models and that originate the problem you're trying to solve. So knowledge is like a
very, you know, that's a big, deep philosophical question that you asked me. But we just mean
awareness of these, these factors that make up these circumstances. Does that answer your
question? Yeah, I just couldn't tell if you meant like data or theories or societal facts or all of
those. Yeah. I mean, as we look at it, you'll see it's kind of like a combination of that, right?
So we're, so we've got lots of data, but we don't have a lot of context for the data
to help us interpret it. So what we're looking for are those, those additional facts that help
make up the circumstances that led to the data existing. And oftentimes those circumstances
will tell you that you're missing data. Like you think you have the right data to understand
this problem, but you don't even have that data yet. And so it starts with some understanding
of what these other factors are you should consider. Yeah, exactly. I think, I feel like the
biggest thing is the processes that are generating the data. So you may have known of some of
Rahid Ghani's work and at Carnegie Mellon now he's there, where he runs this data science
for social good program, he brings students in and students, for example, go with the cops,
they go, they go with the ambulance driver. And like one of the examples he was saying was that
this ambulance driver who already had two cautions, so he was on probation, he didn't take a call
because he's like, I don't want to risk my job. Because if I go take this call, I may do something
wrong. And then they're going to fire me, right? Your data is not going to say that unless you're
actually riding with the ambulance driver and seeing the ambulance driver park his car under
the bridge to and not accept that call. You're not seeing that. And so exactly. And so it's very
difficult to say, I have good enough societal context knowledge. That's exactly right. You
have to understand those all these other these situations and circumstances of key actors
and agents. And so we we came up with a way to start to tackle how to how to model this kind
of knowledge. And so I'm going to get to that. I'm sure you slides. So I think I was here. Yeah,
so this goes right back to the question about knowledge, right? So you'll see what we mean by
it in a few couple of slides. But we think the other key aspect to address those facts I just laid
out is that we need participatory, non extractive methods. And by non extractive, I mean, you know,
there'll be a demand for this knowledge, we're going to make sure we're not just going into
communities, gathering knowledge and then taking it away, like there's got to be mutual benefit.
So and then you also need capacity, right, for communities to participate directly and own
their knowledge and be able to benefit from it. Yeah, Chris. So some of the things that the
community can share are not the sorts of things that are going to become data on the other side,
right? Like that example is not that you're going to start adding as a factor.
You know, are people refusing to take calls because they are on probation.
And so I guess part of the question here is like, which things that the community knows
are translatable into something that then a more principled set of AI
creators can use to make a more humane and fair algorithm. And which one of them are reasons to
be modest about the uses of AI and to say that, well, actually what we want is AI products that
say, well, here's what we suggest based on what little this algorithm knows. But there's only a
suggestion and you decision, you human decision maker being advised on this probably know a lot
more about this case than the algorithm does. So the algorithms presenting its results and assign
kind of with some humility. And I mean, that's interesting too. The if as well as the how
of representing this knowledge. Yeah, yeah. I mean, the thing I'll highlight there is that
one of the things you want to happen is you want the models to be aware of their epistemic
uncertainty. I don't know a lot about this, right? And so even if I don't end up incorporating
a particular factor directly in the data set or the decision making variables,
I need to have a better understanding of like, hey, I don't know a lot about this space,
maybe we shouldn't even be using AI for this, right? That's why you want to really
dig deeply into this into the problem understanding and formulation phase before
you start building machine learning systems, right, for a particular problem.
Yeah. Does that answer your question? It's a longer discussion. I think it's a great
it's a great set of questions like, you know, something like doctor, here's our recommendation,
but please keep in mind, there are lots of good reasons why this patient might be an exception
to this recommendation. I don't know how to convey this partly gets into human computer
interface questions and psychology questions. How do we convey uncertainty and humility to
the people being advised by these algorithms? Yeah. And, you know, yeah.
Yeah. And I think it's just it's it's not the same part as the explainability problem that
everybody's trying to solve, explaining the results of the output. We also be need to be able to
explain like, what we don't know, right, and the risk associated with taking the advice.
I think sorry for me. This also goes to the incentive. So as an AI tool developer, right,
you don't want to develop a tool that says, I don't know. Right. And so first, so for example,
Karina Cortez, Google Research, right, she had a couple of really nice papers on learning with
abstention, where the machine learning system says, I don't know. Right. But your software
engineers you you're they wouldn't want to give a tool that would say, I don't know.
Right. And so that's changing the incentives of like, I don't know. Yep.
Yep. I agree. 100% of the agents that are making decisions is like a key part of that
overall societal context. But you'll see right now, even like, some things happen with the
generative AI, and like hallucinations happening, products, products are saying, yeah, bullshitting.
You see product services saying, we don't know, don't trust us, we're not sure.
Oh, that's great. Yeah. Yeah. I mean, because that's being forced by by this,
this becoming an intervention that society that's getting a lot of kind of feedback.
Right. So, you know, so we've got this situation.
We've got this messiness that can't be transmitted over that chasm. If we're saying, hey, we need
to organize information and knowledge about societal context in some way, this big scary thing.
So we need some kind of model of it. So it's kind of ironic, right? You're trying to,
you're trying to reduce people abstracting away, but in order to do that, you need some
kind of abstraction, right, that people can use to kind of cope with this knowledge in some way.
And so we're, I was really inspired by some work by a sociologist named Dr. Walter F. Buckley,
who back in the late 60s made the connection between trying to understand society and
social cultural systems and between that and systems theory. And so he was like, we should
think of society as a complex adaptive system. When I talk about societal context, let's face it,
we're talking about like, you know, just a, you know, some conception of society as a whole.
How do we think about that? And so our work was based on using complex adaptive systems theory
as the basis. If you look at the characteristics of a complex adaptive systems, a complex adaptive
system, you'll, you'll see some of these characteristics that are in common with how we
were disarming society of context. It's complex, adaptive and dynamic. There's this non-linearity.
I talked about these feedback loops of the ecosystem that we're dealing with. We talk about
things being historical. Time delays goes to that. Complex adaptive systems have history as well.
So we looked at the canon of work associated with this. John Holland is like a giant in this space.
Scott Page, who I think doesn't work with SFI is also somebody I love to read. So we synthesize
some work from these folks to come up with, you know, an attempt, right, to start to rep our,
our heads and arms around this complex thing. So we came up with a reference frame or model
that has three key elements, agents, precepts and artifacts, agents. When we thought about agents,
these could be human or non-human, but I'm going to talk about this through the human lens.
So human lens, these are individuals and institutions. And the ideas that all agents have
and essentially are, they're precepts, which are in our conception, our beliefs, our values,
our stereotypes, how we perceive needs and problems, the causal theories that we talked about.
These are all these rules that constrain and drive the behavior of agents.
And these can go all the way down to, you know, the fundamental rules for building,
building a human, right? But at the societal level, we're talking about these kind of beliefs
and values and goals that constrain and drive the behavior of agents. And when agents behave,
their behavior manifests itself in the real world as artifacts.
And these are just all the things agents produce and create, language, data, laws,
institutions, right? There's multiple, multiple inheritance. Some, you know,
agents can produce artifacts or also agents, for example, policies, AI models are artifacts
for creating products, and we're producing problems as well. And then there's this other
important relationship between precepts and artifacts. This is kind of well-known in the
way people think about HCI, for example, is that the things we produce are a reflection of our values,
right? So the artifacts are in some way a reflection of the precepts that led to them
existing in the first place. And of course, those precepts are influenced by the artifacts that the
agent is surrounded by in the world, right? I read a book, my precepts get updated,
then I do some more acting in the world, gets reflected in more artifacts.
But we also wanted to organize this and represent it as a taxonomic model, because ultimately we're
envisioning, creating knowledge graphs and databases, so that we can transmit this data
across that chasm and make it available to tools and workflows, right? So somehow or another,
we have to have a structured representation of this knowledge. Now, when we develop this
initial model, we hypothesize that these perceived problems and causal theories were going to be
really important if we're thinking about product development. And when we actually did this work,
it was before that Ziad Obermeyer paper came out about racial bias and medical algorithms.
So we were kind of excited that, yeah, we were on the right track, we think, because he pointed out,
they pointed out how critical these causal theories were and how they directly led to
harmful bias in an important system.
And so now the left-hand side looks a little bit more organized, maybe we can think about
societal context in a little bit more of an organized way. But the key question is,
how do we responsibly acquire these causal theories and make them into structured knowledge?
So this is where this practice called community-based system dynamics came into play.
I have some friends in the system dynamics community. I know the person who wrote the book
on community-based system dynamics, and this seemed like it could be a good way to produce
societal context knowledge through the act of prototyping problems, which are like this core
part of societal context, but prototyping them as systems, as complex adaptive systems.
And community-based system dynamics
is a participatory practice for prototyping problems that centers communities and empowering
communities to fully participate and take ownership of their own models of the problems
that they're faced with. But it's grounded in the feedback perspective of system dynamics,
which is invented almost over 60 years ago at MIT. Now SD itself, system that dynamic itself,
leverages visual tools and simulation to support group model building, building prototyping
problems together, and developing collaborative, developing causal theories. And the other words
we use for that are problem models or problem prototypes. And it allows people, health people
do this both qualitatively and quantitatively, and it has a nice bridge between the qualitative
and the quantitative, which we think is important for being able to do this kind of work. And then
the last thing to emphasize is that these prototypes or simulations of these underlying
structures can be used to test interventions on a problem before you actually try to do it in the
real world. And so I'm going to walk through at a high level the basic visual notation that
utilize to prototype problems and system dynamics. One representation is called a causal loop
representation. You might have heard of causal loop diagrams or causal maps, causal maps. The
other is a stock and flow representation. Causal loop diagrams are fully qualitative. Stock and
flow diagrams are qualitative and quantitative. They're provide that bridge into the quantitative
world. So I'm going to walk through this example with a simple loan scenario model. So the sort
of factors you have when you're talking about loans are average credit score, loans received,
borrowers, who's defaulting on loans, who's making their payments. So you have all those factors
specified in language and words. And then you have an arrow between factors with either plus
sign or minus sign on top of it that illustrates the hypothesized causal relationship between
two factors. And the plus sign just illustrates the direction of the impact that one factor is
going to have on another. So to give an example, the plus sign at the end of the average credit
score arrow means that as the average credit score goes up in this system, the number of loans
received is also going to go up. But it also means as the average credit score goes down,
the number of loans is going to go down. So it just means that the impact is in the same direction.
The minus sign means the opposite. So the relationship between loan defaults and average
credit score has the minus sign on the arrow. That means as the number of loan defaults goes up,
the average credit score is going to go down. It's going to go in the opposite direction as
opposed to the same direction. If the number of loan defaults goes down, the average credit score
is going to go up in this system. You can also illustrate time delays, right? Because when one
factor impacts another, it may not happen right away. So in this system, we're saying as number
payments go up in a system, the average credit score in the system should go up, but that's not
going to happen right away. It'll take a while for the credit score, average credit score in the
system to be impacted. You can illustrate that that way. And then you have feedback loops, right?
This really important aspect of complex adaptive systems. There's two types of feedback loops.
One is a reinforcing feedback loop. These are basically indicators of exponential growth,
either vicious cycles or virtuous cycles. And so you can identify parts of the system that
could be driving exponential growth. In this case, the combination of borrowers increasing
and payments being made and average credit score going up, that could drive an exponential increase
in the number of loans in the system. So that's what the reinforcing loop represents. But the
balancing loop is for bouncing out this exponential growth. So that's illustrated by the loop that
includes loan defaults, because that's the one place that even as borrowers are increasing,
yeah, that leads to more payments, but it also leads to more defaults, which leads to a decrease
in average credit score, which taps down the growth in loans being given out. Can I ask? Yeah.
Because you're saying average credit score, this is meant to model what's happening in an
entire population? Yeah. Because of course, these feedbacks also take place in individuals' lives.
And so, yeah. Yeah. Yeah. I mean, system dynamics tends to take a macro view, right? It's looking
at populations as a whole. And oftentimes people will use a combination of system dynamics for
the macro view and then agent-based modeling to have more specific kind of ways to model
like individual behavior. But this model might have different outcomes within different demographic
groups? Oh, yeah, totally. Totally. Right. And again, this is just a way to kind of start to
hypothesize about these factors and you can get more detailed or less detailed. Yeah. Do you care
if the average credit score is a black box? But you cannot see it. Yeah. I mean, I see what's
happening. Do I? Yeah. Yeah. Yeah. I care. So we put this up just to kind of illustrate
like the elements of it. But as you do this work, you'll see that what happens more and more,
you drill into these black boxes and you try to expose the mechanisms under them.
Sometimes you're not able to because there's someone in control of that.
Yeah. Because I was working with Citibank in terms of who they should give loans to and fair
loans, et cetera, et cetera. And they're getting some information from third party folks, which
they do not know how the FICO score is being computed. And so it is a black box and they're
going to use it. Yeah. It's a model. It's a model, right? That is a model that is being used to kind
of make decisions. Right? Yeah. Right. So then the question would be like, how do you actually
quantify that in your epistemic uncertainty? How much weight do you give to it?
Yeah. I mean, that's a question, right? You have to, and this is something that
the modelers contend with and negotiate. And this is why you want to try to include in this
modeling the people who are building those who are building these credit score models.
Depending on what problem domain you're working at, it's very hard to have participation from
everybody that you want to have participation from. And then
the stock and flow representation has all those same features. You just represent those factors
with quote unquote stocks and flows. What important aspect of the stocks is it allows you to
take into consideration accumulations in a system, which really allows you to be able to
consider the history of the system, the initial conditions and how things grow and decrease over
time. And you really get into that detail in terms of the relationship between factors.
And this is where you start to get into the quantification because the way you represent
those relationships is with differential equations, for example. And that allows you to get to the
point where you could start to simulate your hypothesis. And system dynamics is not about,
it's really not like, I'm going to solve this problem. I'm trying to learn
the problem. I'm trying to learn the problem system and increase my confidence that I understand it
well enough to start to intervene in the real world. I just threw this in just to give an
indication of what you have put this together. You can start to kind of look at
how these impacts show up in a simulation. This is made with something called loopy.
Anybody can play around with this. This is like a really oversimplified way to model these sorts
of systems, but at least it lets people to start to engage with it. There's really expensive,
hard to use software as well. And one of the issues is that we have to improve the accessibility
of these sorts of tools if we really want to build capability and capacity in communities.
So now we've got this kind of approach that we're like, ah, this might be a good way for us
to start to tackle producing societal context knowledge from places where we're embedded
and proximate to problems. But now we have to have some actual capability in the world
to use these techniques if we're going to have any chance of producing useful knowledge.
So now I'm going to talk about some experience that we've had over the past few years of trying
to build that kind of trust and capability. I'm just putting up this reminder of the fact that,
you know, the lived experience expertise of historically marginalized groups is usually
not involved in this process. So when I started this back in 2017, I intentionally
wanted to work with folks from historically marginalized communities who are trying to
leverage data and math and science to understand and solve problems.
And there's a group called Data for Black Lives who does exactly that. It's a collection of activists
and organizers and mathematicians and scientists who are trying to leverage data science for good
and also try to make sure data sciences does not end up unintentionally making things worse
when we're applying it into high-stakes domains. And so this started with me just going to the
first conference, not knowing what to expect back in 2017. And this led to a two and a half year journey
that resulted in a research paper and a causal theory that's starting to have an impact on
how Google evaluates and think about health diagnostic algorithms.
So I just went to the first conference with no expectations, not asking anybody for anything,
just trying to figure out what people were trying to do, what problems they were working on,
what problems they were trying to solve. And that's been interesting to people. Some of them
famous now, you probably recognize. And what I discovered there was that there was this need
for people to be able to see how the problems they were separately working on were connected.
And now everyone was talking about, you know, we have to understand the system, the system,
but there was no real concrete way to actually start to tackle that. So I said, hey, this could
be a good place to introduce system dynamics and systems thinking. I talked to the founder,
Yesi Milner, she didn't know me from Adam. And so I had to convince her that this could be useful.
So I introduced her to it with an admitting workshop. And then we agreed to set a goal of
actually delivering a workshop to that larger community at the next conference that they
were going to have, which was about a year away. But we had one principle, we said, the people
who are going to teach this workshop to this community, they have to have similar lived
experiences to the people that are going to be receiving this knowledge. And not a lot of diversity
in the in the world of system dynamics when we started this has changed over time. So we said,
we have to, we have to teach the teachers, we have to create some teachers that can deliver
this workshop. And so we did a learning lab that included people from that community,
as well as people from Google to teach them these techniques so that they could teach the bigger
workshop. We did that over a couple of months, they were able to teach this workshop and these
techniques to a larger group of about 75 people was received very well. And then we had this
capability. And we're like, okay, and a little bit of capacity, what, what should we do with it to
try to further this journey? And so we decided to do a problem prototyping experiment around a
problem domain that Google cares about and data for Black Lives Care is about. And that was health
diagnosis data for Black Lives Care about it because under diagnosis can lead to health disparities,
we talked about that earlier. Google cares about it because, you know, we're working on building
health care diagnostic algorithms, we want them to be accurate and work well. But there's a lack of
training data that can lead to inaccuracy. And so we thought this would be a good area to jointly
prototype a problem that relates algorithms and lack of data and health care disparities
as a, as a way to start to bridge that chasm. And so we brought people together, we got about
nine people together from that initial workshop. They developed one of these college loop diagrams
for that problem domain. I'm not going to go through this in detail, but you can look at the
paper, look at all the data. But one key thing is that it highlighted trust in medical care as a
key factor that drove multiple balancing and reinforcing, reinforcing feedback loops in this
system. They also created a stock and flow model for this. Trust shows up as a, as this kind of
really big substructure that you can double click in. And the thing about doing this work is that
it allows you to think about these soft variables and include them and approach
bridging that qualitative quantitative gap, quantifying them in some way, but informed by
people that approximate to the problem, but also informed by previous research that's happened.
And so they developed a really detailed trust substructure, which I think actually should be
in a whole lot of models. And that allowed them to develop a simulation and actually test some
of the interventions that they had in mind that might make things better to include the
reference model that we were solving for. Now, the key, my key takeaway from this
was that, hey, you can do community-based research and actually produce useful research that
that comes from a partnership with community and people that are approximate the problems and
better than problems. So this paper actually got accepted, presented at the System Dynamics
Conference, what an honorable mission award. But the key thing I learned is that you can go after
if we want to build capability in any place, but you cannot do that without building trust
at the same time. And there's no way to rush this, right? It takes, you know, determination and
patience and investment, but that's going to be one of the key ingredients if we are going to have
any sort of chance to bridge this chasm, in my opinion. So I'm going to conclude with, first,
that key takeaway that I started with. Interventions that ignore abstract away societal context
can lead to unintended and unnecessary harm. I think they're likely to. And then those calls
to action. If we're going to drive more attention and research in this space, we need folks like
you all to spread the word. And then we also are really calling for people to embrace this concept
of prototyping problems before you prototype solutions, prototype problems as systems, embrace
that complexity before intervening with data science or AI, really with anything. And this we
think this is, you know, really one of the only ways to really proactively mitigate bias in these
high stakes domains. And then finally, invest some of your time and energy. And if you have dollars,
dollars in building problem prototyping, trust and capability, like in historically marginalized
communities. With that, I'll close. Thank you, everybody. So we are missing one time,
but there are a few questions. Did you used to work at Google? I'm sorry? Did you used to work at
Google? No, no. So I'm wondering, great talk. Thank you. So I'm, I'm like studying in public
health. And so one thing we're finding, I'm thinking is that the problems start outside the
hospital. So those precepts and the clinicians come from outside the hospital and it becomes a
problem once we get in. So how much of the modeling should we think about outside hospitals in terms
of like social and public investment, housing investment, basically treating that environment
in the society and having that gradient sort of diffuse into the healthcare industry somehow.
I think trying to solve the healthcare problem is a hard one. And then it seems like
you have to go outside a little bit. Like we look at Norway or somewhere, like they
invest in their public more than any of us, probably in healthcare. And we do all the
same. Yeah, we're having this problem. I agree. I agree 100%. Like I don't know, I may not have
been clear in this talk, but one of the things I emphasize in Google is that we have to start
with trying to understand the problems that people care about outside of Google, independent of
like our business problems. If we really want to actually, I think this actually is a better
business as well, start outside what are the people outside in society, what problems do they
care about and why? And how do we get to the point where we understand those problems? And then
you're going to find some intersection in that problem domain between the problems that society
cares about and the problems that we may have some hammers and nails to try to solve technology.
But you have to start outside of that in order to really start to get any kind of ground truth.
Yeah, it's like you need to understand what the problem is. Yeah, I agree. That's why I'm trying
to talk about this problem understanding chasm because of this lack of understanding of the
problem, particularly on the product development side. But there's multiple ways to understand
the problem. I think there are ways to leverage technology in good ways. But unless we have a
deeper understanding that's shared between these two perspectives, we're not we're not going to be
able to do it in a responsible way. Thank you. I guess one thing you didn't mention that I expected
you mentioning would be accountability, right? So for example, perhaps the reason some of the
AI developers now are okay with the system saying no, is Google search was always retrieving
now with gen AI is generating stuff. So if you're generating something that causes me harm,
then there's accountability to be had, right? And the laws are still not there. We'll see,
right? Like, for example, in Europe and certain European countries, they abandon
these kinds of large language models because people harm themselves based on what was told to them,
what was generated as opposed to retrieval. Yeah. So what are some of your thoughts on
accountability here? Who's accountable if the system if the algorithm causes harm to the community?
Well, so you're trying to give me to answer one of these questions that are that's
you can pass. But it's it's it isn't it you we are all part of the community, right? So we it's
yes, I'm a computer scientist, but I'm also part of the community. Yeah. Yeah, I mean, I think so.
So I think there's accountability in multiple places. I think the the work that that's
happening that's emphasizing, you know, how we govern these, the development of these tools
and products is really critical. If you pay attention to what's happening, what's coming out of
the national out of NIST, they've got a framework that emphasizes that the very first step that
has to happen for companies developing these systems to be responsible and accountable,
it has to start with understanding context, which I think aligns directly with what we're talking
about here in terms of understanding societal context. But I but again, like you said, there's
all these incentive structures that will kind of resist that. And in the in the product development
companies, but even outside, right? Right? Yeah, because the accountability is tied to the incentives
and incentives then are tied to gaming the system, including the the the algorithm, the tool, and the
bigger picture that you had, right? And so the ground around the goals, that would be interesting
to model. You're like, you're like, you're, that would be interesting to see with that.
You know, I would love to see how you would hypothesize what that relationship is between
those faculty. Well, I could definitely do that. I'm a professor. Yeah. Yeah.
Talk and I'm typically interested for the final point about gathering trust from the
historical margin with people and getting and getting involved all the domain experts about
the particular problem. And that maybe that you came from Google, and it's like a big company,
which can we have to deal with have a capacity and maybe some resources to open a workshop or
getting a workshop and getting involved for many. But even if we just concentrate on the
historical margin with people, there's so many different people around there. And maybe you have
to think about so many different parts of other people's, and maybe not every company or production,
the product maker have a resources or capacity to do build certain kind of trust like that,
although it is very important. Yeah. And so will be, will there be any kind of like a substitute
or other options to incorporate this or like an accelerating these procedures to, well, generally
accessible to more companies or more industrial parts of this person, rather than
getting or like opening a workshop and getting a building a relationship like an after like a
more than five or 10 years. I think if it if there's some kind of other options or like if we can
encourage other entrepreneurs to do this things more, I think it'll impact a lot. I want to
just know your opinion about this. Yeah. I mean, so I agree with you 100% like it's a really hard
problem. And I think this is going to require investment like not just for not just from
product companies, because in some way, you know, we're not we're not in a great position to really
be fully embedded in the problems that society cares about purely, right? And so I think this is
the option where places like Google need to see some leadership and some ownership in this space
and kind of investment capacity outside. So we have a gigantic sector of philanthropy,
foundations, etc. Well, I think we need to play like a critical role in this. One of the reasons
we're trying to raise the awareness of this is just for what you described to get other people
thinking about solutions to these really hard problems of like, you know, scale and and and
dealing with incentives that have to do with speed, right? So not easy things to solve. But
so this is why we're like, let's raise awareness, get people talking about this,
and have more people applying their mind to finding solutions that are kind of responsible,
non-extractive, can accelerate things. I think there are ways to solve these. This is kind of
like a moonshot. There are things you would have to invent that haven't been invented yet,
but we're trying to kick off kick off the flywheel to get those conversations going.
So one thing I'm interested in is what kind of pushback it's like one phrase you didn't use,
but I'm sure that we agree on it. It's also one of our favorite things to rail against is
domain agnosticism, right? Like, like, oh, I'm the clever technical person, send me your data,
send me your zeros and ones. I don't even need to know what the zeros and ones mean. I have
cool algorithms and then I'll help. Right. And so part of the goal is like breaking that as,
you know, what I think still for a lot of engineers and engineering students,
it's still kind of a norm and sometimes even held up as a virtual. So I mean, I'm totally,
I love what you're pointing out. I guess I'm interested in two things and I'm looking
for to our conversation later. One is the move here is to work with the community to generate
models at first qualitative, but with the ambition of being quantitative.
And of course, that's an interesting move. It's not the only move. And
and then once you have these models, then lots of other tricky questions come up. Like,
do we really think these models are going to be predictively accurate or they just sort of
scenario explanation tools or kind of, you know, are they quantitative models that we're actually
trying to learn about qualitative effects of possible interventions? So that's, that's,
I think that's an interesting and I like it, but I also, I'm curious about its limitations.
Yeah. And one of the limitations that I expect also some engineers might push back on.
So like in the criminal justice field, right, everybody says absolutely arrest is not the
same as crime. It's not just a very noisy signal. It's also a systematically bias signal.
And some people say, well, could we use conviction? It's like, well, that's also a noisy and
biased signal. And of course, you know, society, neither the government nor engineers have access
to did a crime actually occur. And so you get this sort of defensive reaction that,
well, but this is the data we have. You know, so like, I think we've all been to a million talks
where there's one slide at the beginning saying, arrest is not the same as crime, but this is
the data we have, you know, and then, and then plunging into the technical, you know, the statistics
of that as a data. And I don't know, that's, I mean, maybe this just gets back to the humility
question again about, you know, hey, we don't know if this data means what we think it means.
We don't know if it's measuring, we think it measures. And therefore, we're presenting our
suggestions, our recommendations to a judge or a doctor with some humility, but then
I don't know. I don't know how to wrestle with that, especially because part of the argument
for doing all this is that humans clinical judgment is also very biased, or can be. And
so, I don't know. There's like, which things can cross the chasm into something which is
structured enough that to, to, to, and then to improve the algorithm and which things can't
really, then the algorithm just has to say, Hey, I, I don't know about that. And keep in mind
that I don't know about that, which as you said, as Tina said, is, you know, vendors are not usually
incentivized to do. Yeah, I don't know. So that's like where that boundary is, I think is really
interesting. Yeah, yeah. And you, and you pointed out one of the key root causes of this, of this,
of this chasm. And that's this, this is the data that I have. Don't force me to kind of think about
that messy stuff, right? On the other side of this boundary, the techniques that I have,
I'm incentivized by those techniques, and they, they incentivize me to abstract those,
that stuff away. Now, and, and, and also the conversations, and then it always goes to,
I've, I've spent a year talking to engineers about like, ah, we can, we can, we want to work on
building these models. And the first question I get, but yeah, but how do you know they're accurate?
How do you know they're predictably accurate? And then my pushback is, how do I know yours are
predictably accurate, right? You're, you're very confident in all the assumptions you're making
throughout that life cycle. No one's looking at them. They're not made explicit, right? And so
eventually you actually get over that hump where people can say, oh, I can see the utility at least
in the beginning of this, of me being more informed. And one of the key things about system dynamics,
it emphasizes, don't try to start with data. Don't try to start with the data set.
Start with your intuition and the intuition of others about what's important in this system,
and start off by trying to get some sort of, you know, shared hypothesis. And then that should
drive what data do I need if I'm trying to intervene on a problem in this system? And if I
don't have data about this factor, maybe I shouldn't be trying to intervene on this particular problem.
Right with this technology. And so this is why we're, you know, this is why we're kind of purposely
focusing on high stakes domain, health care, because of, you know, the issue of health equity,
health care industry is kind of, I think a little bit more mature in terms of looking for systemic
causes of disparity and kind of embracing that. So there's kind of more openness about like,
hey, we're trying to, we want to use these algorithms, but there's really a lot of
work in caution by clinicians and others, because they know how dangerous they are.
You think this is again, where the accountability comes in, right? Because the doctor can use a
tool, but at the end of the day, he's accountable, right? And to the question that I always get is
like, well, the system is, but your AI system is biased, but the person who's making the decision
is also biased. Yes, but the person who's making a decision, he can go to jail. I will sue his
ass from here all the way to Boston, right? That AI system, who am I suing? Who's responsible?
Yeah, right? It's not clear. And that is important, right? It is important to figure out who, who is
accountable, especially in America, right? We like accountability, like, who do I blame?
Yeah. Nobody. I go to jail, nothing happens, right? Because the system said I'm high risk,
and that's it. So the systems are being used as expert witnesses without being cross-examined
and not being held accountable. I think in healthcare, it's a really good domain to pick,
because at the end of the day, it's a doctor and the malpractice insurance he has.
Yeah. And so they have, you know, like you said, there's a vested interest in making sure these
things work out. And I think there's also like, you know, the Hippocratic Oath as well,
like doctors don't want to harm people, right? I'm large, right? But, but, so what you're pointing
out is like, you know, these technologies are like really, really serious interventions on society.
And you're talking about like, we have to update our, the cannons of law, right, for these entities,
these systems. Yeah. And I think especially now with the movement on democratization.
Yeah. Right. I want to democratize X. Well, then this kid can go and develop clear view AI
with like $200,000. No accountability. He didn't know about ethics or nothing.
You know, it's what, wait, what? Yeah. So there are issues. I mean, I love your research area.
I mean, it's similar to research area I've been pursuing, but we cannot not talk about
accountability. Like we have to talk about accountability. Yeah, I agree. And I think we
also have to make sure that at least for me, like, I'm not, I'm not like, I, I can like,
at least me personally begin to kind of understand like, all right, what do we, what do we have to
do to update the cannons of law? Like they're experts. Like that's one of the things I like
about Andrew Selps, his work, right? He's a law professor, right? So, and that's also why I like
about his work about abstract and way societal context, because I think hopefully that means
that kind of perspective will make its way into the laws. You see that perspective making its way
into some of the frameworks that are coming, coming out of the government in terms of how we're
going to regulate these things. So it gives me some hope. But I do know that even if you come
over the law that says you have to understand the societal context, we are not in any position to
actually represent it, present it, whether it's in a, you know, court of law or in a
product development life cycle. Whereas somebody has to pay millions and millions of dollars,
then people will move unless they have billions of dollars.
Sadly, we're coming up on time. But Donald, I want to thank you again for a great talk.
Fantastic. Thank you.
For tomorrow afternoon, I know a number of you already signed up for meetings,
but if you haven't, he's down in Pod C. So you're welcome to go and

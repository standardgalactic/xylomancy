Rwy'n cael ei ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud.
Rwy'n cael ei ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud.
Rwy'n cael ei ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud.
Rwy'n cael ei ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud
Rwy'n cael ei ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud
o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r ddweud o'r d
Weidio weendeniaeth hysty Hyun Willers Niferthyr sy'n caelll tud o that gyfer Ies Dayor scientists at the SFI.
It is thanks to them that we get to present these to the public for free, so that is a very great shoutout to the university health council.
very importantly we would like to thank you, the members of the audience, for participating in this series.
Yes, again. Very, very prosaicly, thank you for voting at the Santa Fe Institute, this series, yet again,
as the best lecture series in the town of Santa Fe, in the city of Santa Fe.
But also, more importantly than what you're voting, thank you for your participation.
Wywer ydych chi'w wneud i gweld arweithion i'w wath astru,
o'ch ffeirain yn i Перiedwyr – a weld wage r undertaking yn fwrieniau
08 o b judgmentau a chyfnodwch a chyffез o wneud Aberfrangig i branches
sy'n maen o reliableno gwahith.
Rtherswp i chi, gennych'n am!
FOEITIN
Nightspeaker, your interlocutor for tonight's conversation.
OK, Vijay Balsubamaniaan.
Wow. OK.
How do I begin?
Let me start talking about his research interests, and I'll be
presenting a lot of quotes directly from him to try to get the nuances
correct, even though I'm familiar with a lot of his work, a collaborate
with him and so on.
In Vijay's own words, quote, he is interested in how natural systems
manipulate and process information.
OK, well, that level, you know, big deal.
Everybody here is interested in how systems process information, right?
Well, I'm God is in the details of what one's interests are.
And with Vijay, those details are pretty damned impressive.
For example, in the way that Vijay construes those particular words, one
of the ways that natural systems process information is, well, you know,
the fundamental nature of space and time.
So, for example, he has done pioneering work in what is known in
quantum cosmology as the information paradox.
That's the fact that naively, at least, information seems to get lost
into a black hole, even though the laws of physics forbid there being any
loss of information in the universe.
Following this theme, he's investigated, again, quoting from him directly,
how the familiar smooth structure of space-time, you know, that's what
we're familiar with, how it can emerge for more complex underlying
physical constructs.
So, you know, these are the kinds of things that you and I might mull over
while I'm taking a bath or on the way to work this morning.
Let's see, I'll have to walk the dog when I get home and buy a cup of milk
and, oh, yeah, just what is going on at the plank scale at the event horizon
of a spinning black hole that's got a little bit of charge.
The kind of thing that everybody does on the way to work in the morning.
Anyway, no surprise, though, the mind of someone like Vijay cannot be
contained in just the fundamental nature of the physical universe.
He actually takes a step back from that and asks, well, how is it that we might
actually come to our understanding of the physical nature of the universe?
He's been interested in how it is that the human enterprise of science
comes to try to understand scientific reality.
Again, using his own words, he has also worked on problems in statistical
inference and, in particular, Occam's razor, which is very interesting
because all scientific theories involve fitting of models to data,
trading it off versus the complexity of that model.
Again, you know, just the kind of thing.
It's maybe this is for the shower rather than the bath when you go into work.
In short, Vijay is somebody cast in the same mold as the Santa Fe Institute
is in general, a person of very many interests.
Now to wax a little bit more prosaic, though, one difference between the SFI
and Vijay was that Vijay was born a little bit earlier than the SFI was.
Specifically, he was born in Mumbai, India, Bombay back then, moved among many
cities in India, his family eventually moved to Jakarta.
Then he came to MIT in the US.
His degrees at first were physics and computer science.
Then he got a PhD at Princeton.
After that, among other awards, well, just achievements.
He was accepted as a junior fellow at the Harvard Society.
Around this time, he first formed a connection with the SFI.
Since 2000, he has been a professor at University of Pennsylvania.
He's a fellow of the American Physical Society, has received many awards,
over 17,000 citations in the academic literature.
So that's a little bit of the more prosaic kinds of, well, he did this,
and he did this, and he did that kind of details.
Anyway, not although the third one of his major interests, what he'll be talking
about tonight, this is one way to phrase it, it's how your brain, the three
pounds or so of pink mushy jelly enscarced in your cranium, that you know,
it kind of wiggles around when you do this too fast.
How it is that that, the information processing, remember, that's his
underlying theme here, how it is that the information processing that goes on
in that jelly becomes you.
OK, and I have personally been very fortunate to have some interactions.
We've had many discussions, Vijay and I, on a topic related to that.
Made me the energetic cost of the information processing in the brain.
Anyway, tonight you're really in for a singular treat.
And with that, over to you, Vijay.
Well, David, thank you very much.
Oh, first of all, can you hear me?
Yeah, OK, thank you very much for that lovely introduction that was really
kind and it's a delight to be back in Santa Fe.
This is one of my favorite cities on earth.
I've always really liked coming here and thank you very much for inviting me
back and thanks so much for coming to my talk today.
So great. So let me first of all check that my iPad works.
It works. Great.
So the subject of today's talk is how the brain makes you.
So we should start by first discussing what I mean by making you.
So you're an animal like any other animal.
So the first thing we got to ask is what do animals do?
And, you know, here are a number of animals from the Osa Peninsula
on Costa Rica. These are these are photographs taken by my son, who is also
one of the animals there. And what do animals do?
So the main business of being an animal involves eating, navigating,
exploring the world, sometimes resting like this one here, manipulating the
environment, look at the nest and these this this web here, interacting
with each other, reproducing things of this kind.
And that's what animals do almost all of the time.
To do so, they have to do certain things, right?
They have to be able to sense the world, namely get information from the things
around them, the things, the other animals, you know, the sky, whatever.
They need to process this information that's coming to them and remember
some parts of it, those parts of it that help them predict what's going to
happen in the future. And they after doing this, they need to eventually
make decisions, both in the short term and the long term, and be able to
plan out sequences of actions for the future.
So these are all things that an animal has to do just to be an animal.
Now, some animals do additional things.
They write poetry. Here's one of my favourite poems by the poet
Gerard Manley Hopkins. They paint.
This is a painting done by my daughter, Aruna.
They write music. This is by Heinrich Injatz,
Franz von Beiber. I'm sure I pronounced that incorrectly.
He was before the Baroque.
And, you know, some people write equations like a guy named Einstein
and a guy named Schrödinger wrote this equation.
So we theorize about the universe in this way.
Now, all of this, we being human, seems very impressive to us.
And, and, you know, we're very interested in this.
I'm very interested in this. You're all very interested in this.
But in fact, the brain devotes very few resources to such activities.
It's important to us, but really, mostly that's not what the brain does.
Mostly you, as an animal, like other animals,
mostly you eat, navigate, explore, rest, manipulate the environment,
interact socially with each other and reproduce.
That's what you do.
And then there's this additional things that we do on top of it.
So I'm going to focus most of the talk today on these sorts of activities
that all animals do, and then we'll talk a little bit at the end
about these sorts of activities that interest us particularly as humans.
Now, to do all of this, what do we use?
The organ we use as the brain.
And I'm going to think about the brain today as a sort of computing engine
that produces the behavior of animals.
So here's the human brain in particular.
And what does it do?
Well, it does the things that I said animals need to do,
which is they, it gathers information from the world.
It learns from experience, forms memories and makes decisions for future action.
So that's the job of this organ.
You know, the liver processes chemicals for you and this organ and the heart pumps blood.
This organ does this stuff with processing information.
It's a kind of computer that you have in your head.
Now, the challenge for this computer is that it's a very big, complex
and often unpredictable world, and you know, you have very limited resources.
It's this thing that you carry around inside your head,
it sloshes, it bumps, you know, stuff like this.
That's what you get to use.
And so myself as a scientist in this area, the question that animates me is
I'm interested in what are the organizational principles that allow the brain
to meet such enormous challenges that animal life faces.
So that's what I'm interested in.
Now, I'm trained as a physicist and a physicist pose to the question about
how does this thing, this system work?
The instinct of most physicists is that, well, you know, what I'm going to do
is I'm going to first work out what stuff this thing is made of.
What are the parts that make it up?
It's kind of an atomic instinct, you know, figure out the parts that make up a system.
Then you figure out how all the parts talk to each other, how they interact.
And then the idea is, if you know that well enough, you can work out
what the whole system is doing.
That's the kind of method that physicists get trained in.
And, you know, it goes back a long way hundreds and hundreds of years.
And that's the tendency of that's the way that's the approach to the scientific question.
So what's the brain made of?
Well, that brings us to the neuron doctrine.
This is Santiago Ramonica Hall, who lived from 1852 to 1934,
and he is widely considered probably the founding figure of neuroscience.
So what did he do?
He was actually, he first wanted to be an artist, by the way.
But then he wound up becoming an anatomist and famously what he did
is he founded the neuron doctrine.
The neuron doctrine is the idea that the, if you like, atomic constituents of the brain,
the thing that the brain is really made of, the stuff that does the interesting things
in the brain, are the neurons of the brain.
So here's a picture of a neuron.
This is a so-called Purkinje cell in the cerebellum that's in the back over here.
This is drawn by Ramonica Hall.
And, you know, he was a trained artist, so he drew really beautiful pictures.
So his pictures to this day are wonderful artworks in their own right.
Now he got the Nobel Prize in 1906, along with another scientist named Camilo Golgi
for their work on anatomy.
And Golgi did not propound the neuron doctrine.
The reason is they used different experimental methods, different stains to stain cells.
And the nature of Golgi's stain was he had the impression that all the neurons
were kind of connected together and they formed a web work and there was sort of one thing
that made up the brain.
And it was the very careful anatomy done by Ramonica Hall, which identified that,
no, no, no, there are many discrete, separate objects in the brain called neurons.
And the neurons talk to each other in some form or fashion in order to produce
all the effects that we call animal behavior.
So let's talk about neurons some more.
So you'll see that I'm not a trained artist.
So you'll see my drawings do not compare to this in either precision or beauty.
But let me explain what is a neuron.
A neuron is a cell like any other cell in your body.
So it has a cell body.
Let me see, is that big enough?
Can you see that?
Can you see the words?
OK, great.
So, well, see the nice thing about iPads is you can increase it.
See, isn't that nice?
OK, so if you can't see it, tell me and I'll just increase it.
So a neuron is a cell like any other cell in your body and has a little membrane
that contains within it all the molecular machines that make a cell work.
But neurons are particular cells that are intended to help your brain compute.
And as such, they have very specializations that allow them to do that.
So in particular, a computing element like a transistor or a diode or a resistor
needs to take in inputs from other stuff.
So indeed, if you look at a neuron, it has this set of branches coming out from one end,
which is called the dendritic arbor.
And on the dendritic arbor, the dendrites, those are the little fibres at the end.
And on the dendrites, there are the famous synapses that most people have heard of.
The synapses are little junctions where one neuron makes a connection with another neuron.
That's a chemical connection.
One neuron would dump something called a neurotransmitter,
then sort of diffuses over to the other neuron, is eaten, taken up by the synapse.
And then that leads to communication that we'll talk about later.
Now, the output, if you're a little computing device, you need to have an output.
And the output of the neuron goes through a wire called the axon.
And the axon then makes an axonal arbor, which also has some synapses at the tip.
That's where the neuron communicates the next neuron, whatever message it's going to send.
So your brain, the human brain contains about 100 billion neurons
and it contains about 100 to 1,000 trillion synapses.
So there's not just one kind of neuron.
You know, if you go look inside a computer, a silicon computer,
there are a few kinds of circuit elements, right?
There are transistors, there are diodes, there are capacitors, there are resistors.
There's a repertoire that engineers use to build up computers in general.
So in fact, in the brain, there are very many types of neurons.
And Cajal identified them by how they looked.
And for example, here is what's called a cortical pyramidal cell
because it looks a little bit like a pyramid.
And these pyramidal cells are the workhorses of your brain.
All over the cortex, you find them layered upon layer of cortical pyramidal cells.
This is this cerebellar purkinje cell that we mentioned.
Here's this huge thing.
It has this huge dendritic arbor, which it uses like a giant fan
to catch inputs that are passing perpendicular to it.
It does that as part of controlling your body
and allowing you to move your body in ways that you want.
And then in addition, here's another cell called a cortical stellate cell.
So in fact, as we will see, there is a great diversity of forms of neurons.
And usually in biology, there's a rule, a working rule
which says that form follows function.
So what's going to happen is that they look different like this versus this.
They're going to do different things.
They'll have different biochemistry.
They'll perform different functions within this computer.
So it's like you've got many circuit elements that are stuck together
to produce your behavior.
Now, the key thing for us is that neurons literally,
like the components of a computer, are electrical circuit devices.
They're electrical devices.
So how does this work?
So once again, here's a picture of a neuron.
And if you go measure its voltage, literally, I mean, its voltage, right?
You'll find that its voltage inside a brain,
the voltage will be something like minus 70 millivolts, typical neurons.
They're like at rest, they're like at minus 70 millivolts.
Then if you have input coming into it and the input could be,
you know, signals, neurotransmitters, which I mentioned earlier,
sent out by the previous neuron or if this is a neuron in the eye,
it might be light landing on it because there's a little sensor
that senses light.
Or if it's in the nose, if it's an olfactory receptor neuron,
it's a little molecule floating in the air which binds to a receptor
and that produces a signal, you know, whichever you do, if a signal comes in.
Here's what happens.
The voltage of the neuron begins to increase.
And when it reaches some critical value, there's a sort of runaway feedback process.
It's a little bit like feeding the output of your speaker
back into the microphone, and so it goes round and round.
So it goes boom and sort of explodes rapidly to some very high potential.
And then there's another feedback process, a negative feedback process,
as they call it, which causes this to dip back down and flip back to rest.
So if you step away and time a little bit,
you'll find that it goes like this and then boom,
there's a very sharp pulse of electricity.
So if you sort of look above this level,
it's like there's silence and then boom, silence, boom.
So it's essentially a digital signal.
It's like looking at a digital computer where you have this device going,
every once in a while sending out ones, hold up these spikes
or action potentials or zeros where it's silent.
So it's very much like a digital computer, that's what you have in your head.
So just to drive home that this is absolutely a mechanical, physical process,
beautifully studied in biophysics, we can even delve in.
You know, the instinct of many scientists is you see a phenomenon like this,
you say, well, how does that exactly happen?
And you can work that out in complete detail.
So for example, if you look at a membrane surrounding the cell,
that's the membrane right there,
you'll find embedded in this membrane various molecular machines,
such as ion channels, which allow specific ions,
like sodium and potassium mentioned here,
or chloride or calcium.
They let certain ions pass back and forth.
And then you have these things called pumps,
which have the job of when the ions go back and forth,
changing in this way the voltage of the neuron.
Something has to be done to restore it, right?
You send a lot of current back and forth and the voltage changes,
you've got to restore everything.
And these pumps basically burn energy,
they burn the gasoline of the cell,
which is a molecule called adenazine triphosphate, ATP,
and it burns ATP and goes chunk, chunk, chunk, chunk, chunk for a while,
and then sort of restores the levels of ions inside and outside.
So at rest there's a certain proportion of sodium and potassium inside,
and a certain proportion of sodium and potassium outside,
and then you have the kinds of processes physicists talk about.
Stuff, you know, it's like if you have more salt outside a membrane than inside,
you know how the salt comes in, right?
In the same way, if there's more potassium sodium outside than inside,
the sodium wants to come in and the potassium,
because there's more inside than outside,
wants to go out so that these processes like this,
basically stuff you can understand perfectly well,
and then these pumps are working against it,
and then what happens is that you have external inputs
that change how open or closed these channels are
to allow the ions to pass before through them, and that's it.
It's a completely, I mean, just like a mechanical device,
with stuff flowing back and forth.
And you can work this out in detail
and show that if you have a system like this,
it'll produce these voltage spikes,
and then if you make a voltage spike over there,
it'll then propagate down the axon.
It's literally making, you give it the correct inputs,
it's going to make a digital pulse,
and the pulse is going to move down.
In fact, this is so well understood
that a Nobel Prize went to it.
So Hodgkin and Huxley in 1963 worked all of this out,
and here's a set of equations describing their model of the cell.
You don't have to read the equations.
I just want to show you, you can write it on a damn page, right?
This means you can put it on a computer
and you can certainly simulate it.
You can totally describe how a single cell describes
in complete and utter detail.
It's actually a little bit more complicated than this,
because, you know, for the purposes of this talk,
I'm talking about sodium and potassium and things like that,
but there's more things, you know, this chloride
and there's calcium and actually this more than one kind
of ion channel for potassium, you know, some details, details.
But basically, you know exactly how to do this,
so in principle, you could just build yourself a brain.
If you knew enough about all the other neurons
and all the types of neurons and all these problems,
you could just build yourself a brain on a computer.
That's the promise of this kind of result
that won this Nobel Prize, right?
But what's the problem, right?
The problem is, oh wait, actually before I tell you the problem,
let me just sort of say a word about this.
So this idea that, you know, the brain really just functions
in electricity, just an electrical device,
a computer of some kind, goes back a long, long way.
And the discovery that somehow there's an electrical basis
to animal behaviour goes back all the way,
I don't know why I said this is a 19th century idea,
it's even an 18th century idea, it goes back all the way
to the 18th century, with the discovery
by Luigi and Luisa Galvani, we're in Bologna,
that you can make frogs legs, for example, twitch,
by putting electricity into them.
So at the time, you know, Luigi got most of the credit
because Luisa being a woman, couldn't be a professor
in the university and couldn't also take credit
for the discoveries, but well, you know,
times have improved a bit, they're perhaps not completely.
Anyway, so they were the ones who discovered this notion
of animal electricity.
Then immediately after that, this is the time when Benjamin Franklin,
you know, is discovering positive and negative charges,
flying his kite, all this kind of stuff,
that's all happening at the same time,
and then Volta, also in Italy, invented the battery,
which then allowed controlled injection of currents
into living tissues, and then you could see how they moved
and people had this idea that somehow electricity itself
was the vital spark of life.
It's not that it made neurons work,
because they didn't know about neurons really, right,
but they thought it was life itself.
So for example, so they thought,
well, maybe we can reanimate the dead.
So Aldini reanimated a criminal who was executed using electricity,
and then it was done again in 1818 with another criminal,
and in those days, there were laws that allowed you to experiment
with the dead bodies of murderers and things like that.
Anyhow, this led, for example, Mary Shelley, to write Frankenstein,
because it led to the idea that life and living things and animals
could be just built, that they're sort of machines
that could be built,
and that's what's partly animating us still,
this notion of artificial life and artificial intelligence,
goes back, excuse me, goes back to these discoveries,
you know, hundreds of years ago.
So now, of course, we know that electricity,
the shocking dead body, doesn't make it alive again.
What it really does is following what I told you a moment ago,
it causes the nerve cells to fire,
and when they fire, they cause muscles to move,
and then the body will move in the way it looks
as though you reanimated the dead, but you haven't really.
Anyway, so suppose we know how a single neuron
works in all of its detail, then what's the problem
with understanding the rest of animal behavior
by just putting enough neurons together?
Well, the problem is, you know, a single neuron
can't produce a poetry,
a single neuron doesn't feel love, you know, things like this, right?
Really, neurons produce the brain
through the circuits that they work in.
It's really the interaction, the collective of all the neurons
that makes the brain.
So what do these circuits look like?
So perhaps the best-studied circuit
in the entire brain of any animal is the retina.
So this is a drawing done, again, by Ramon i Kehal,
back in 1917,
and what it's drawing is a picture of the retina like this.
Over here, these are the famous photoreceptors
that many of you will have heard of.
Photoreceptors are neurons which take in light
and convert the light into an electrical signal
that comes out the other end.
So that's the first layer of the retina.
Now, the second layer of the retina
consists of cells called bipolar cells.
These are like analogue computing devices,
namely they're not digital.
They have voltage levels that go up and down
and they're all continuous, it isn't 1s and 0s if you like,
but they do all kinds of calculations and computations,
which are studied.
Then these things form the second layer,
feed into a third layer,
and the third layer are cells called ganglion cells,
and these ganglion cells are the output cells of the retina.
And these output cells, the axons come out,
and the bundle of axons is a thing you call your optic nerve.
So very often people have a tendency to think about the retina
like a camera,
it just takes the pixels and writes down the amount of light in each location.
But nothing could be further from the truth.
This is your primordial example
of what would be called a three-layer neural network.
It's a piece of your central brain
that got put on a stalk
and then sent out to the front of your head
during development, during embryonic development,
so that you could see in the direction in which you're going.
It's really a piece of your central brain.
And indeed, in keeping with that,
central brain is a very complicated place.
This picture drawn by Cajal is actually a cartoon.
We now know that there are more than 60 kinds of cells,
different types of cells,
just so different computational elements in the retina alone.
So, for example, if these are the photoreceptors,
and then these are the bipolar cells described in the second layer,
and then these are the ganglion cells described in the third layer,
in fact, in between,
there is an entire zoo of other so-called interneurons
that sort of communicate laterally.
And what they do is the following.
What they do is, you know, most of the light coming to your eyes
is not actually useful to your behavior.
There's only certain things that are useful, and indeed, the ganglion cells,
what they do is they report the things that are useful.
Like there are ganglion cells called on cells
that respond and tell you when there's a bright spot in the world at some location.
There are off cells that tell you about dark spots.
There are so-called local edge detectors that detect local edges.
There are direction-selective cells
that will respond when a little blob moves left to right or right to left or up to down.
You know, there are all of these cells, these different kinds.
So, how do you extract these so-called visual features from the scene?
Well, this hugely diverse body of neurons
is charged with removing from the visual input
everything that's irrelevant and leaving what is relevant.
So, famously, for example, in analogy,
someone once asked Michelangelo how did you sculpt these sculptures?
David.
And he said, well, actually, David was already in the marble,
I just took out everything else.
And so that's what these things are doing,
they're taking out everything else
and leaving the important stuff to go through to the brain.
So this is how it's organized.
So you can see it's a large collective effort
of many, many types of neurons arranged in a very specific circuit.
For example, you will always find that the slow bipolar cell here
connects to the local edge cell
and the fast bipolar cell here connects to the bristrandian cell.
So it's like a circuit that Intel would design.
You know, the engineers designed something.
This isn't designed, it's selected out by evolution.
But anyway, there you go.
It's like a circuit that you'll find in every vertebrate eye.
Now, even that is not actually a sufficient picture
of the degree of cooperativity in the brain
that produces all of the behavior.
So if you pop out from the scale of neurons
and instead look at the whole brain,
here is your brain looking this away,
and in the back of your head there are circuits
that are associated with vision.
Over here, there's a strip going down the side of your head
that helps you control your body.
Over here, there's an area called Broca's area
which is associated with the production of speech
and many other areas of this kind, the hearing, et cetera.
And in the front here, there's regions of your brain
that are associated with planning and personality
and things of this kind.
So this is why, by the way, decades and decades ago,
there was a treatment for schizophrenia
which involved basically sticking an ispic up your nose
and scrambling this area because there's some issue with it.
And the result is the person won't be schizophrenic,
but they won't be themselves anymore
because you scramble the circuits that make you.
You really should be very careful with any kind of treatment
that involves messing with bits of the brain.
Okay, so anyway, so there are all of these things
and we know that function is localised in this way,
that this is called localisation of function for many reasons.
So one reason is, for example,
if you bash the back of your head, you fall backward,
you know how in the cartoons you see stars, right?
Bugs Bunny, you see stars when he bash the back of his head.
It's because Bugs Bunny has a bruise on this part of the brain
so the circuits aren't working quite right.
That's what's going on.
So when someone has a stroke,
sometimes you'll see one side of the face sag,
but everything else is completely fine.
That's because there is an issue, there's a problem,
let's say in this region of the brain,
this is the motor cortex,
and within the motor cortex there are circuits here
that control the knee and circuits here that control the face.
So if you have a stroke there,
this thing sags but the rest is fine.
But it's all collective, right?
There's a collective circuit,
which of course has little areas that concentrate on different things,
but there's a collective effect.
And this is not just on the surface of the brain, by the way.
So deep in the middle of the brain,
there's an area there called a hippocampal formation,
which is associated with, for example, memory, right?
So if you have damage there,
you will not be able to form long-term memories, right?
And it's also associated with spatial cognition,
with spatial, you know, animals need to map space
and be able to make plans and navigate.
And so all of those circuits are hidden here.
Okay.
So actually, human beings have known about this kind of thing,
that there's function localized different parts of the brain
for a very long time.
This is a picture of what's now called a Smith papyrus,
after Smith who bought the papyrus in the late 1800s.
This was a papyrus written in 3000 BC.
And the author of this papyrus describes various kinds
of traumatic brain injuries,
and then explains that, you know,
something will stop working, but everything else works.
So this author knew that function was localized
in the various space of the brain.
The modern version of this idea is due to Paul Broca,
a French doctor who basically discovered Broca's area
because he had patients who had impairments in their speech,
and he discovered they had lesions in this area
of his brain, of their brains,
and there's another area called Wernicke's area,
which is associated, but later discovered,
having to do with speech understanding.
So if you go to Paris, you can go to the 13th arrondissement
and, you know, admire Rue Broca,
a name for him for all of his great discoveries.
There you go.
There's the picture of the street sign of Rue Broca.
All you need to do to get a street sign
is discover localization of function in your brain.
Okay, very good.
So now, that's also not enough
in describing how complicated this processing machine is.
You know, you all know if you ever looked at a wiring diagram
of something like a chip that powers your computer,
you know, it's got lots and lots of pieces
with intricate circuits,
and all those pieces connect to every other piece.
Otherwise, of course,
they couldn't produce their collective effects.
And indeed, all of these brain areas connect.
So today, in the 21st century,
there are all of these extraordinarily beautiful techniques now
for tracing out the wires.
It used to be that people like Cahal,
you know, they had to take the brain of an animal,
slowly slice it up,
very, very carefully trace everything,
draw it out by hand.
You know, can you imagine the level of dedication
and sheer labour it took?
Now, we have all these amazing techniques.
You can do things like you can make neurons glow
in different colors
and get the color to propagate along the axon,
you know, the output wire,
so you can see where it's pointing to.
You can inject viruses into bits of brain.
The virus will go backward along where, you know,
the neuron connects to you,
and then you can find out which neuron,
one neuron down, you can tag it
in that way, and then it will stop working,
the virus stops working at the point
you can engineer all these things.
So in this way, we now have far more detailed maps of the brain.
Here's a picture from a paper by these authors.
It was a review paper showing some of the nerve tracks
as they're called, the wires that pass between.
And now, because of that,
we just have a much more refined understanding
of how collectively all these bits of brain
produce functions that we care about.
So, for example, following what I said a moment ago,
in Broca's era, or shortly thereafter,
we might have said, well, you know,
here's Broca's area,
which, you know, somehow is responsible
for articulation, production of speech.
Here's Borneke's area
that is somehow associated with comprehension of speech.
Let me explain the difference between those two.
If you damage Borneke's area, you'll produce speech.
It just won't make any sense.
And if you damage Broca's area,
you can understand speech perfectly well.
People can talk to you,
but you won't be able to make it.
You can't control your muscles to make the speech,
you know, stuff like this.
There's also another area, Gershwin's area for concepts.
You damage that, you have problems with concepts.
Anyway, so there are these areas that people identified.
So a more refined model you might derive
by looking at the way the nerve tracks go
is that there's clearly a pathway from here to here.
So from the comprehension area to the production area,
and there's a secondary path,
you know, there are two pathways like that.
But now we have even more fancy techniques.
You can put a person in an fMRI machine, right?
Functional magnetic resonance imaging machine,
and have them do stuff.
And while they're doing stuff,
you can record, you can find a way
of measuring which areas of the brain are active.
So which areas of the brain are active together
when you do different things.
And now you get a much more refined picture
that all of these areas are there.
It's true that they're associated
with articulation concepts and comprehension.
But depending upon exactly what you're trying to do,
they connect in different ways.
So there's a sort of flexible computational engine
wherein the different bits of it work together
for one task or not,
depending upon what's necessary to do the task.
Is this amazingly flexible engine inside your head
for producing all of animal behavior?
So that's the task to work out
how all of these parts connect
and how they reconnect and reorganize themselves
when they need to do things.
So myself, my interest is to understand
how all of that happens.
I think of the brain as a computing device.
I want to know how this computer works.
So there are many ways of approaching this.
But one way of doing that is, well,
so the way I do that is I take a view
of the circuit repertoire,
of the architecture of this computer,
as a kind of memory of the computations
that have predictive value for your behavior.
Because animal life is about predicting what's going to happen
and taking action appropriately.
And that these computations
have been learned over evolutionary time
and then encoded in your genome
and in the developmental program.
So that's the way I tend to think of it.
These are computations that are useful to you
and then what you've done, you've done something
and then you use them.
So the question that animates my work
is what are the organizational principles
or laws, if you like,
that control the collective computation
and information processing of the brain.
And if you're interested, there's a popular book
by Peter Sterling and Simon Loughlin,
this book, your principles of neural design,
that you'll probably enjoy,
which lays out many possible principles
that may be operative
in the organization of the circuits in the brain.
Now, so what principles might be relevant?
What kinds of laws or principles?
So there are several that you could try to name,
but for today's purposes, I'm going to name two
and then I'm going to illustrate them
in the operation of some of these circuits.
So for example, one principle involves
the costs of computation.
So this is your brain and this is your laptop
and your brain is only 2% of your body weight,
but it's actually 20% of your metabolic load.
What that means is it's 10 times more expensive than muscle.
This is a seriously expensive thing to own.
And it's also packed solid.
Every millimeter cubed contains four kilometers of wire.
This is like this really dense thing.
So on the one hand, this sounds like wow,
that's a really expensive thing.
On the other hand, your brain consumes something
like 12 to 20 watts of power.
Your laptop consumes 80 watts of power
and it sure can multiply fast,
but it can't give this talk.
Well, at least not yet.
But even things like, by the way,
the AI seems to have the promise of making machines
that will do me,
but actually they have to train their machines
on so much data, a planet's worth of data,
they use a city's worth of power to train these things.
Whereas I can learn stuff with one example.
I don't need the planet's worth of data.
So there's a real difference in the way the brain works.
It's just much more efficient in the way it operates
than your typical silicon machine.
That's also one reason why computer scientists
are interested in this kind of thing
because they'd like to make more efficient machines.
So you could ask yourself,
how does the brain achieve this sort of efficiency
relative to the engineered systems
that we have been able to produce so far, right?
I've described this Baroque architecture
with lots and lots of pieces that interact in some way.
So one idea that's very powerful
and has been used powerfully in neuroscience
is that the brain achieves its efficiency
by adapting its circuits to the structure of the world
and then using learning and self-organisation
to further adapt the changes, right?
You first adapt to the world
and then if the world is not quite what you expected,
you change the circuit so that it's well adapted to the world.
So why would this make the system more efficient?
Well, the idea is roughly like this.
So when if you talk to sociologists or something,
they'll tell you that in the back of the day,
we were all hunter-gatherers,
we all dug the ground, we hunted the animals,
we built a house, whatever, we did everything
and society got along.
And then as societies progress,
the civilisations develop, et cetera,
there's a segregation of function.
What happens is that the shoemaker makes the shoes,
the baker breaks the bread,
the mason builds the houses
and then each unit in the society develops efficiencies.
They do their job really well
because they're well trained, they're adapted to it, etc.
And the whole system,
by communicating between all the individuals,
works better, works smoother, it's more productive, et cetera.
And it's more efficient.
So you could try to apply a similar idea
to the organisation of these circuits in our head
which have evolved over time.
So that's a very important idea in neuroscience.
So that's principle one, the efficient use of resources.
So the idea of this principle
is that brains exploit the average structure of the world
to efficiently allocate their limited resources
to the tasks that they have to do
to maximise gain for the organism.
Now, if I were to draw a cartoon of something like that,
it might be something like this.
Remember how in the retina we said,
well, you know, there's all the light coming in,
but at the back end of the eye,
you have a certain number of different neurons, right?
There are 20 types of ganglion cells, we call them,
but you know, they're each charged with doing a thing, right?
One thing will look for bright spots,
that's the on cell, the one thing will look for dark spots,
it's the off cell, you know, and so on and so forth.
There's a certain repertoire of things
that apparently are needed for vision,
so you make those things,
and then each of those things is one of these blobs,
and together they cover the space of those aspects
of the visual world that you need for your behaviour.
And they have to allocate those resources
in an effective way.
And you could keep doing this with a sense of smell,
with your sense of place, you know, the whole brain,
and you could try thinking in this way.
So this is a very important principle
that's used often by neuroscientists
to try to understand the architecture of circuits in the brain.
Another important principle is this one,
it's learning in self-organisation,
these individuals whom I know well did a lot of that,
and so what do they do,
so that's far as I can tell, their little heads,
self-organise a lot of the architecture within it
through dynamics and learning, and what do they do?
Well, this learning and self-organisation
adapts the brain to ongoing variations in the world,
their world is different from the world I was growing up in,
it allows them to learn new tasks and environments at this age,
they definitely didn't know how to ride bikes,
and you can correct inaccuracies in pre-existing dynamics.
Anybody who's interacted with small children knows perfectly well
that when the baby first tries to reach for stuff,
it doesn't get there, right, you can't quite do it,
and then they learn how to control their muscles
to get there, to reach it,
or you watch a kid learning a musical instrument,
well, you know, the first 500 times, it doesn't sound so good,
and then eventually it does, because there's something that happens
where you learn how to operate the bits of you to do the right thing,
so that's the learning that has to happen in a variable world.
Okay, so what I would like to do in the remaining what,
15, 20 minutes, 20 minutes or so,
is that I would like to give you examples of how in the field
people use these kinds of ideas to understand
the organisation of neural circuits,
meaning so far I gave you a description of the architecture,
right, of these circuits from the neuron level,
you know, the atomic constituent of the brain
all the way to the whole brain, right,
but in science we want more than that,
we want an understanding of why they're organised the way they are,
how they produce the dynamics and effects that they do,
so there is an enormous amount of interesting work that has happened
since the era of Cahal in these directions,
so I want to give you a little flavour of some of it
by giving you a few examples from my work,
I'm going to pick simple examples because, you know, there's 20 minutes left,
so we should go, so it has to be simple and precise enough,
so I'm going to give you examples in four domains, right,
one domain is seeing, vision,
the second domain is smelling,
or factions, these are both sensory domains,
then I want to talk about learning to sing,
this is in songbirds,
so that's what the professionals would call motor control
because you need to control all the muscles
in the songbirds apparatus in order to produce the sounds,
and then I'm going to talk very, very briefly about navigation,
which is actually the subject that I spend probably most of my time on,
thinking about right now,
so I'll just say a little bit about that and then we'll end,
so I'm going to give you four examples of this guy.
So let's start with one.
So first I'm going to talk about vision,
and my goal, by the way, what's my goal with these examples,
you remember I talked about these two principles,
adaptation to the world and learning,
as two mechanisms that are used to, well, to organise neural circuits,
you can understand why they're organised the way they are,
but thinking about adaptation to the world
and about learning to reorganise.
So I want to give you two examples of adaptation,
those will be the sensory examples,
and an example of learning, okay?
So first let's talk about vision.
So why did I choose vision?
Well, we're very visual animals,
and it's a sense we use most of,
so that naturally neuroscientists gravitate often
to thinking about the visual world.
So I want to ask a very basic question.
Here's the question.
So we drew this, Ramoni Kahal drew these pictures
with all of these different neurons,
and then I showed you another picture,
a more modern one of all the cell types,
and there are like 60 types of cells.
God help us in this structure.
Well, what do they do?
Let me just look at the output layer for now.
These were the so-called retinal ganglion cells,
and I told you that these cells detect features,
visual features of the world,
bright spots, dark spots, colour, local motion,
et cetera, and then they put the signal,
these digital signals that I talked about,
go down the optic nerve,
and that little bundle of wires
that's like an ethernet's cable worth of data,
and that data goes to your brain,
and you do all the stuff that you do with vision.
So following a kind of question
that immediately comes to mind,
is you have a certain repertoire of these cells.
Well, humans, we have about a million of them.
If you're a guinea pig, you have about 100,000 of them.
So, you know, it's not an infinite resource.
So if you spent all of your money
on detecting bright spots, there's nothing left.
If you spent all of your cells
on detecting bright spots very well,
you will have nothing left for detecting motion, for example.
So there has to be something interesting
in the way this sensory device, right?
This is like you have a robot
and has this little eye sitting in front looking at stuff, right?
So you've got to decide.
How are you going to decide the design of that eye?
It's only a big.
You've got only some of the units in it.
How are you going to allocate
the different pieces of the circuit?
So here, there should have been some choice done by evolution
for the allocation of these features.
So for this talk, let's take the specific example,
which is easy to imagine,
of bright and dark spot detectors.
So bright spot means you look for a little bright spot
relative to the nearest surrounding region.
A dark spot, so that's called an on-cell,
and an off-cell does the reverse.
It looks for a little dark spot
relative to the near surrounding region.
So we're going to ask the question,
well, suppose you have a certain number of spot detectors,
how many on and off cells should you have?
How many bright and dark spot detectors should you have?
And we're going to try to use this
to understand the architecture of the retina.
Just incidentally, it's also important therapeutically, right?
So if somebody's, you know, when the day comes,
when we can replace the retina of some,
if somebody's retina degenerates
and we want to replace it by a silicon device,
it would be better if you could understand how many of these
and how many of these detectors we should put in
into that silicon device.
Right, so if the idea, if this kind of organization,
involves adaptation to the world around us,
a question we have to ask is, what is the world around us?
So for each sense, or for each mode of interaction
with the world, if you adapt it to the world,
then one question you have to ask is,
what does the world look like?
So in this case, you can go collect a large database of images
and study its properties.
How is light organized in these images?
And one thing you find is that in any color channel,
this is supposed to be red, blue and green,
you'll find that the distribution
of the intensity of pixels, you know,
if you look at the brightness of different pixels,
it's very, very skewed.
There are lots of pixels which are pretty dim,
but then there's a very long tail
with some pixels that are enormously bright.
So it doesn't matter where you go,
you do this inside a city, you do this at the North Pole,
you do this in a jungle, and do it wherever you want,
you're gonna find that this is the case.
It's a very universal feature of the visual world.
And it has an important consequence.
The average intensity of a pixel is lower than the median.
Then the median, median means the median intensity
is the intensity at which half the pixels are dimmer
and half are brighter.
So the median exceeds the mean in the world.
There's another fact which is that
there are long range correlations.
What do I mean by that?
This is some technical figure showing that,
but what we really mean is, you know,
if it's kind of red here, it's gonna be kind of red here,
it's gonna be kind of red here,
and then after a little while it's improbable
for the color is gonna persist forever.
But there's a very particular structure
of those correlations which you can measure.
It's a regularity of the natural world that you can measure.
So you would think that, well,
if I'm gonna detect Brighton's dark spots,
maybe that's related to these statistics.
And actually, without even doing anything,
we can immediately argue, and let me argue to you now,
that these two facts, that light is correlated
over distances, and that you have the median light intensity
exceeds the average, that immediately means
that there are more dark spots in the world.
Here's how to see that, right?
So suppose what you do is you compare the light
in a small region here to a large region there, right?
So what's going to happen is that if you measure
the average, if you compare the light in those two regions,
because of these two statistical facts,
it's going to turn out that the spot in the middle
is generally going to be darker than the surrounding region.
That's just because the average, if you see the average here,
the average is less than this.
So that turns out to be the case.
So for example, you can take a bunch of images
and you can put down here a thing that detects a bright spot,
a thing that detects a dark spot,
and you can just count how many bright and dark spots
you detect in every image,
and you'll find that no matter what angular size you look at,
there are more dark spots than there are bright spots.
So that's what this figure is supposed to be showing.
So we're going to ask a question based on this.
So let's agree that because of these statistical facts,
there are more dark spots than bright spots in the world.
So then we can ask, suppose I give you an allocation
that you can buy, or you can put in the eye,
a certain number N of bright and dark spot detectors,
namely on and off cells.
So you get to divide them.
You get to decide how many dark spot detectors
and how many bright spot detectors you have,
and my challenge to you is work out what would be best
given the structure of the world.
So it's immediately clear that if you have only money to buy one cell,
it's better to buy a dark spot detector.
Why? Because there are more dark spots.
So it's more likely to respond.
So you're more likely to get useful things out of it.
If I give you two steps, then you buy some dark spot detectors
and you can cover the eye with them.
But then they're going to start overlapping.
So they're going to start telling you the same thing.
Because they're overlapping,
they're telling you redundant information.
So then you should buy some on cells, bright spot detectors,
and you can continue with this game
and work out the mathematics of it
about how you should tile the retina
with bright and dark spot detectors
in order to get the most information
about the brightness of spots in the world.
So that's a mathematical problem
that you can write down and solve.
And it turns out that the answer is about,
you should have one and a half to about two times
as many dark spot detectors in your system
if you want to say as much as you can,
given your budget of the number of cells,
about bright and dark spots.
So if we can ask, right,
how does that compare with the actual eye?
So over the last two decades,
there have been a very large number of experiments on this.
So it used to be in the 1990s, for example,
if you ask people, they would talk about,
you know, Yin and Yang, bright and dark, equal and opposite.
But around the year 2000,
a series exactly as you'd predict
if the circuits of the eye
were adapted to the statistical structure of the world.
So exactly in that way.
So the collective organization,
so that suggests, as I was saying,
according to the principle I mentioned,
that the collective structure of the retina,
the object which is charged with processing light
and turning it into an electrical signal
to the center of the brain,
that that collective structure is partly controlled
by adaptation of the circuitry
to the statistical structure of the visual world.
Okay, so that's an example
of how neuroscientists understand how these circuits work.
So let me give you a second example.
Similarly in the sensory world,
let's talk about smelling.
And we care about smelling, about olfaction,
because it's in some ways the most primordial sense.
All living things sense molecules.
So all living things smell in that sense.
And every animal, most animals use smell
much more importantly than vision
to do their daily jobs.
So smell is actually a sense,
it's very different than vision.
Why is that?
Because there's an intimidatingly diverse space of smells.
So what is smell?
When you smell a thing,
what you're actually sensing is volatile molecules,
molecules that float around in the air.
And there are many, many of these.
There are catalogs of molecules that float in the air.
There are at least 10,000 different molecules.
To make matters worse,
you can go consult with the people who do fragrances
and flavors, people who make artificial flavors.
And they'll tell you that if you look at natural odors
of different kinds, from foods, from other animals, et cetera,
typically an odor will contain 50 different kinds
of molecular species.
So you could estimate from that
that in terms of possible natural smell,
that it's like 10,000 to the 50.
This is such a vast number, it's even hard to conceive.
That's the number of different things,
like odor objects, if you like,
that you might have to sense in the world.
To make matters worse, the odor environment changes,
reflecting season circumstances and new opportunities.
So what do you need to do?
Well, to detect a molecule,
actually you need to feel its shape.
The identity of a molecule is basically defined
by how all of its atoms are arranged in space.
The olfaction, the sense of smell,
is a method used by the brain
to sense the shapes of little things floating in the air.
So what do you need to do this?
Well, so it turns out that odors are sensed
when molecules bind to receptors in the nose.
So there's basically another molecule
in a neuron in the nose,
so that it's an olfactory sensory neuron.
The molecule will be sitting there,
and then in will come some other molecule,
an odorant, as it's called, floating in the air.
And if this molecule fits into the binding pocket
of your sensor and sticks there, doesn't fall off,
then the neuron will respond.
So this binding pocket, your olfactory sensory neuron,
is literally feeling the shape of the molecule,
whether or not it fits.
Now, as it turns out, every receptor,
olfactory receptor that allows you your sense of smell,
is encoded by a separate gene.
That's because to make your receptors have different shapes
because they're trying to feel out different shapes,
so to make a molecule, a protein,
that has a different shape,
you need a different gene to encode it.
For the genetic level, they're all different genes.
In fact, it's the biggest gene family in most animals.
So flies have about 100 of these,
humans have about 300,
and even the elephant only has about 2,000.
So this leads us to feel, looking at this,
that somehow, apparently,
animals can sense all the molecules in the world
and all the combination of the molecules in the world
with just a few hundred receptors.
The reason this is very, very strange
is you could phrase the difficulty of the problem
in the following way.
You could say, suppose an odour is defined
somehow by the concentrations of all the molecules in it,
then it's possible that there are up to 10,000 different molecules.
So you put the first concentration of this side on this axis,
you put the second concentration of this axis,
the third concentration of this axis, et cetera.
There are 10,000 different axes
on which you can draw all these concentrations,
and somehow you have to represent all of this information
in the responses of just 500 neurons, let's say,
or 300 neurons, and so it's the response of neuron one,
the response of neuron two, the response of neuron three.
You just don't have, it would feel like there's no way
to represent so much information
about all the molecular concentrations
with so few neural responses.
You could, just to make the problem more easy to visualise,
consider the following thing.
Suppose I told you that you have positions
in three-dimensional space, right? X, Y, Z, like that.
There are positions in three-dimensional space,
and I insist to you that you tell me
where you are in three-dimensional space
by telling me just, you know, a coordinate
if you're like a location in one dimension.
I mean, that's just really not,
doesn't seem like it's very sensible.
It's really very hard to do, to do that, right?
Because you've got, you know, three directions,
you could go up, you could go left, you could go right,
and somehow encoding that in just one direction,
seems very hard, okay?
And what's more, you'd like to do that
in a way that somehow preserves similarities and differences.
You need to be able to tell that this odor
is the same as this odor, or similar to,
and these two odors are very different.
So somehow the brain has to solve the problem
of representing all of this information
with very few neurons, right?
So a very high-dimensional space of odors, as they say,
in a low-dimensional space of responses.
That's like going from three dimensions to one dimension,
and it needs to do it in a way that preserves distances.
So an insight that's happened in neuroscience
in the recent past,
is that actually there's a way in which you can do this
by adapting to structure in the space of odors.
The structure is the following.
We said already that if I look at, let's say, this odor,
it'll contain only a few molecules, let's say 50,
and all the other things are absent.
This odor contains this molecule and this molecule,
and all the other ones are absent.
Each odor in the natural world contains about 50 odorant molecules,
right, 50 types of odorant molecules.
So it turns out that there's a theorem in mathematics
that you could take signals like this if they came to you
and store them with many fewer responses
than the number of things that are coming in,
if every sensor bound to all the molecules
that are coming in kind of randomly.
It's a rather than being structured like, you know, in vision,
you have the cell that pulls out a bright spot,
and you have a cell that pulls out the dark spot,
or left to right motion and things like this.
So instead of doing that, if you just said,
here's an olfactory receptor,
I kind of wanted to bind kind of randomly to everything
and just give me a sort of mishmash of numbers.
There's a theorem in math that says if you do that,
you could store all the information that's there
in the odor world with a few hundred receptors.
So there's a theorem like that.
So the question is, does the olfactory system
use randomness in that way?
That's a very different kind of organization, mind you,
than the visual system.
That's why I'm giving this example, right?
So in the visual system, everything is highly structured,
right, because the visual world is highly structured.
And here the idea would be, the math would tell you
that it should be kind of random what it does.
And indeed, so here's a picture of different receptors
in the fruit fly binding to many different odorant molecules.
And the darkness of the color tells you the strength
of the response.
Now this doesn't look entirely random,
and it isn't entirely random, but there's a sense,
a mathematical sense, in which this is sufficiently random.
It's pretty close to random.
So much so that the, so this is one, so this is another word.
Basically, what happens is that every olfactory receptor
here is binding to many, many odorant molecules,
and every odorant molecule is binding to many, many receptors.
So this is a highly multiplexed thing.
It's not like one receptor tells you this molecule is there,
or this receptor tells you this molecule is there.
It's like every receptor binds to everything at some level,
and every molecule binds to everything.
So this is a highly multiplexed kind of description
of the odor world that appears in the nose.
So this is what's called a kind of combinatorial code
in the jargon of the field.
And this continues.
So this is a drawing, a diagram,
of your early olfactory system.
So you have all the molecules in the world.
This is a figure showing concentrations
of which molecules are present.
They bind to different receptors.
They're all colored differently.
And then what happens is that the signals
from the receptors are collected at a second stage.
That's what's drawn here in the picture by Camilo Golgi.
So the schematic that I've drawn here
is actually this thing that I've simplified.
And at this stage, it's all cleaned up.
And then it turns out that the signals from here
project to your central brain, to the cortex,
also in a random way.
So once again, you get this sort of randomness,
and the information is kind of spread out all over the place.
And you can show, and so that's what I was trying to argue to you,
is that if you study vision, you might be surprised by this.
Why is vision so structurally organized
with all these little feature detectors pulling out
different things in the world and sending the brain and so on?
And why does olfaction look so random?
So what I tried to argue to you is that the collective organization
here in the olfactory system, the system of smell,
is via randomness as opposed to structure,
because that's what you need mathematically
to be able to process signals of this kind.
So in both cases, in both vision and olfaction,
in one case, the structure, in the other case,
the sort of randomness in the circuit,
are associated with adapting the circuit
to process efficiently the information in the world.
So that's what I'm trying to convey here.
By the way, I'm trying to give you some of the technical details here,
because often I find that in these kinds of lectures,
when I listen to public lectures,
there's lots of pretty words,
but I don't have a sense that something can actually be done.
There's something concrete.
I'd like you to go away with the sense that this is completely concrete.
You can do these calculations, you can do these measurements.
So this is a completely concrete thing,
and that's in the details of this kind of analysis.
That's why I'm giving you some of these details.
You have a sense of how this works.
So I'm going to give you one more example, and then stop.
So the example I'm going to talk about is now learning.
So I'm going to talk about adaptation in evolutionary selection
of circuits to be adapted to the world.
And now I want to talk about something
that isn't evolutionary selection for adaptation to the world,
but rather involves learning.
So there are songbirds.
Here are two kinds of songbirds, zebra fringes
and the brown-headed cowbird.
And famously, songbirds learn to sing
by imitation practice and innovation.
By the way, the reason why we should care
is humans do the same thing.
Here's a human learning by practice, right?
So that is a very important part of animal activity,
to learn by practice and to get good at something.
So how do songbirds learn?
Well, it turns out that juvenile male bird babbles,
and just goes on like that, and then over weeks,
it learns to sing clearly partly through imitation
because its song will partly imitate the songs it hears often
of its parent with some innovation.
And then the adult male bird sings to female birds,
often seeking a mate.
And then the female birds seem to evaluate these songs
and then decide which one or which ones to accept.
And this is all really important, right?
It matters, because, for example, in social species
like this cowbird, the males and females
work out a kind of social hierarchy
based on their singing to each other,
and then they produce pair bonds,
and the success of those pair bonds
is absolutely critical for later success in egg laying.
You don't make the pair bonds,
the colony doesn't produce very many eggs.
So it's actually really important
for the survival of the species
that this is all well-organized
and that the males learn to sing.
So how do they do this?
So now, let me tell you about how all of these circuits
I've talked about reorganize themselves.
They reorganize themselves when neurons,
which are connected to each other,
autonomously rewire their circuits, right?
There's a rule, and they'll use the rule to reorganize.
There are various sorts of rules,
and one of the rules, one of these mechanisms,
is called spike timing-dependent plasticity.
So let me give you an example.
Suppose you have two neurons, one and two,
and they're connected, and here's the synapse between them.
Suppose, for some reason, they both fire.
They both produce these voltage spikes
that I talked about at the beginning of the top.
Suppose neuron one fires before neuron two.
You could have a rule that says the synapse strengthens.
Or you could have a rule that the first...
and also the second part of the rule says
that neuron one fires after the second one the synapse weakens.
So this is just a local rule, right?
I mean, it's just a dynamical rule.
You've got two dumb objects, neuron one and neuron two,
talking to each other, and they just have a rule, right?
They have the strength of the synapse, the weak of the synapse,
depending on who goes first.
You can modify this a little bit.
There are things called neuromodulators,
which are sort of chemicals that the brain uses,
and these can be used as global knobs to affect how this happens,
so, for example, there's a thing called dopamine,
which, if dopamine is released here,
it can reinforce changes that have happened
that happen to have led offline to some sort of reward.
So if there's a reward, you know, they'll do something
to help cement some change that's happened with synapse.
There's other things like neuroponephrin
that also, you know, allow emotion to control
how these synapses changes in everything.
And you could put all of these rules on computers
and see what they do and so on.
It just seems really implausible
at first sight that something so stupid, if you like,
can actually lead to things like actual learning
of things you might care about in the case of the birds,
for example, allowing them to learn songs.
But let's see.
So how does song learning actual work
in the actual brains of birds?
So here's the bird brain.
And what's written down here is all the different major brain areas.
Remember, we talked about brain areas.
And these are the brain areas that sort of co-operate collectively
to help the bird to learn songs.
Three important ones are HVCR and L-Man
acronyms with some historic provenance.
What's very interesting about the system
is that this region, RA, has neurons in it
which are the things that control the muscles.
So these are the things that actually produce the song.
So a sequence of firing of the neurons in the region, RA,
will make a whole bunch of muscles go back and forth
in the bird's throat, and thereby produce the song.
So that's the thing that makes song.
So to make a song, of course, you need to control those muscles
in a particular order, control these muscles first,
and these muscles, and these muscles, and these muscles,
and these muscles, and that makes the song.
So that's what you've got to learn.
So RA also gets input from another area called HVCR,
and HVCR acts as a conductor.
So the neurons in this area produce patterns like this,
and this, and this, and this, and this, if you like,
but it's actually patterns of neuron firing,
and they provide a time base.
Every pattern of firing of neurons in this area, HVCR,
marks out a different time in the song.
It's literally conducting the song.
So when HVCR produces this pattern,
certain neurons here in the area RA should fire,
thereby pushing certain muscles.
When neurons in HVCR fire in some other pattern,
then some other neurons in RA should fire,
producing another push, a different kind of push in the muscles.
So that's what should be happening.
That's what it's got to learn.
As the conductor marks out time, different muscles get pushed.
So how does it learn this?
There's a third area called L-man, which is a tutor area.
And this tutor area does two things.
First, it drives exploration.
You see, part of the problem is that the bird doesn't know
which neurons control which muscles.
You don't know that, just like baby doesn't know
how to move its neurons in its head
in order to control their muscles.
It's something you learn.
So here, L-man does two things.
One is it drives exploration
by injecting a little bit of randomness
in the firing of RA
that allows the neurons there to do slightly different things
every time to explore what can be done.
The other thing it does is it provides guidance.
So this area gets back a message that says,
how did the output compare to a remembered song
and based upon those differences
between what you wanted and what you got,
it's able to provide some guidance
about which synapses to change.
Or, well, that was a good thing you did
or that was a bad thing you did.
So you're able to provide guidance of this kind.
So that's at the level of these brain areas
collectively talking to each other.
What about at the level of the individual neurons
and the synapses?
Because somehow here, at the end,
every neuron is a dumb little thing
and all it knows is a dumb little rule
about how it should change.
So it turns out that right here,
there are synapses between the conductor neurons
and the student neurons,
which have the following property.
So if the conductor sends a message to the student
before the tutor, the synapse strengthens.
If the conductor sends a message to the student
at the same time as the tutor,
then the synapse weakens.
That's a measurement in this system.
Otherwise, basically nothing happens.
This is for this particular synapse
and this particular bird, the zebrafinch.
Other birds and other synapses can have different rules.
So it would be a legitimate question to ask
whether can such a stupid rule
actually allow a bird to learn a song?
So you can check that,
and the way neuroscientists,
or at least theoretical neuroscientists will do this,
is they'll build a computer model of the system.
So you make a model of the conductor area
as a bunch of neurons.
You allow the conductor neurons to send signals
to all the student neurons.
Then you say there's a tutor
and you get the tutor to send messages
to the student neurons too,
and then you say I want the student to learn
a particular pattern of firing
and you press go and you say can such a stupid rule?
It's like timing dependent plasticity rule
where depending upon who fires first,
you strengthen a weakened synapse, can that work?
So amazingly, it learns,
this is capable of learning the correct sequential outputs.
You just tell it that I want this sequence of firing
in this area, it can learn that output.
But there's an interesting finding
you can find out this way.
That is, the learning works better
if the teaching style of the tutor
matches the learning style of the student,
the kind of thing you hear in kindergarten.
So basically, it could be
that the neuron has this spike timing dependent plasticity rule.
If A goes before B, then you strengthen.
If B goes before A, you weaken.
Or it could have this spike timing dependent plasticity rule.
You strengthen so long as both of these fire
close to each other.
There are many different rules different neurons implement.
It turns out that for some of these rules of learning
that an individual synapse could implement,
it turns out the teacher, the tutor, should teach the student,
give it signals so that it learns early segments of the song
before late segments of the song.
That's the way it's going to work best,
given that rule of how the learning at the synapse works.
But for other students, by which I mean other kinds
of synaptic plasticity or learning rules at the synapse,
you'll find that the teacher should just teach the whole song
all the time, just keep giving corrections everywhere.
And it'll work better. It'll just work faster.
That's a very interesting finding,
and this is intended to illustrate two things.
One is this kind of learning.
It's a collective effect.
The whole brain cooperates, and the synapses have to do things.
So it's a collective of everything that's doing this effect.
The first statement.
The other statement is this kind of theoretical computer modelling
that's now possible, given I told you earlier
that you could build a model of a neuron and why not put it on a computer
and just see what it does.
You can now do that kind of thing and learn a great deal.
In fact, this is a prediction for things that ought to be measurable
in different kinds of birds and different kinds of synapses.
So we're now at the stage where you can have the style of science
that was in physics, which is that you can have theory,
you can have experiment, the theories make predictions,
you can check experiments, the experiments refine the theories, etc.
That is a mode of doing science
that's been basically absent from much of neuroscience, from much of biology,
and that is now becoming possible
because we have so many tools and so much knowledge of all of this.
I'm basically out of time,
so I'm not going to talk about the circuits that underlie a sense of space
unless you ask me during question time, which can tell us more,
tell you more, and instead I'm going to conclude.
So the point of my talk today was to try to suggest to you,
say something about how our brains make us,
and the kind of point I want to try to make to you
is that inside our head we're all collectives.
It's not like there's a thing which is you,
there's just lots of neurons,
lots of neurons are connected, lots of circuits,
the circuits are connected into brain areas,
and collectively the emergent effect of all of this is you.
I tried to illustrate that by looking at various examples,
and special cases of different kinds,
particular, for example, this learning.
Another point that I'd like to emphasize
is that in essentials all animals are the same.
This turtle and my daughter at an earlier age
are basically, in all essentials, they're the same.
All of the stuff we like about the fancy thinking we do
is a veneer on top of that.
Nevertheless, there's a lot more to discuss.
So, for example, we're very interested in humans
in decision-making, social behaviour,
things like curiosity and creativity,
language, which humans have a particular faculty for language,
abstract thought, which matters to us,
and we'd like to understand the origins of all of these behaviours
in neural circuits, and actually there's progress on this.
Just in the last 20 years, or last 10 years,
there are more and more tools for studying all of these things,
and I anticipate that the next decade will see lots of progress
in understanding the neural circuit origin of these sorts of behaviours.
Then, usually, most people are interested
in the even broader question.
One of the things you prize about ourselves
is sentience and consciousness.
So, we'd like to know what are these states?
They're clearly states of brain.
We don't all say that rocks are sentient or conscious.
They're states of brain, but we don't quite know
how to define even what sentience and consciousness even mean,
in general, let alone at the circuit level.
We'd like to know from this how does mind emerge from brain.
I actually think that we will make progress in these questions
in the next 100 years.
That might seem like a long time, but it isn't really.
People live to 100.
If you think about it, we have come a heck of a long way since Cahal.
Cahal had pictures of lots of different neurons
and suggested that neurons are central to how the brain works,
so that there are these different things.
They're like the atomic constituents of brains.
Think about all the things I've been able to say today,
and I even only just scratched the surface.
100 years from now, I think there's every chance
that we'll be able to give some sort of definition
of what we want and what we mean
by something that's conscious or sentient
and understand how those states arise from neural circuits.
I'm going to stop there.
APPLAUSE
We'll go ahead and open it up for Q&A.
I think we have room for about two questions.
That's okay?
So, in your opinion, what would be the best way to learn a new language?
You're asking the tutor should start at the beginning of the sentence only?
Where's the nothing?
That's really not clear.
I mean, there are different learning styles that people have.
So, some people, my wife, for example, is very good at languages,
and always talks about how, when she learned Arabic,
the grammar was what really attracted her.
There's this very regular structure to that language,
and learning that grammar helped her to understand the language and everything.
My dad is very good at languages.
He speaks seven languages,
and from him, I get an impression of a kind of...
On the one hand, he just listens to it and reads it,
and then somehow, on the way, on the side, studies grammar.
It's a very different mode.
It's also the case that the young and the old learn language in very different ways.
So, children just listen to it,
and they just pick the whole thing up somehow holistically.
There's a certain stage, there's a critical period where you can do that,
and after that critical period,
the mode in which you learn language is known to be different.
So, it's much more useful when you're older to have structures like grammar,
and things like that, and consciously be aware of them and to learn them.
So, there used to be this idea...
Well, there is this idea due to Noam Chomsky of something called a universal grammar
that's embedded in these kinds of circuits that I've talked about,
that it's just built in, that all humans have the faculty for this,
and that in early childhood, when you learn languages,
what you're doing is pruning away bits of that that you don't need.
It's a little bit like all of those neurons, those interneurons, the retina,
that sort of take away all the stuff you don't need.
So, the idea is that you sort of prune those away,
and you're left with the structure pertent to the language that you're going to speak
in your local area.
Now, you know, with the advent of things like chat GPT,
and their remarkable linguistic abilities, there's some discussion...
I mean, there's debates about whether people agree or disagree still with this proposition.
So, I think that's a little bit up in the air, because of this particular...
If you think about chat GPT as a development in computational linguistics,
that's raised some questions about this notion of a universal grammar,
whether it's really there or not.
I mean, personally, I think they're going to find that.
And I think the reason why things like chat GPT work
is that there is a underlying regular structure in all human language.
And this is an engine that's built in such a way
that it's able to extract that structure in the language.
And so, I think people are going to...
Once they understand how to piece apart these artificial engines
and work out what they're actually doing inside their in-arts,
I think we will find that there is such a structure.
So, you know, it's different strategies for young individuals.
It's also clearly different for different people.
I'm not sure that's a sufficient answer to your question, but...
Well, there you go.
Oh, thank you.
Yes, you touched on something which was related to my question,
which is the effect of age.
You know, this working in these mathematical schematics
that you have.
I'm a senior, and I think there are a lot of seniors in this room.
So, can you speak to that?
Are we...
Yeah.
Yeah.
Yeah.
So, you know, age has produced sort of many effects
that on the structure of the brain, you know,
in part because adults basically don't get new neurons.
There are two regions of the brain where you do.
So, in the olfactory system,
in this structure drawn by Golgi, this one over here,
you actually get new neurons as you age,
which is very interesting because nobody understands why.
The other area where you get new neurons,
even when you're older, is the hippocampus,
which I mentioned here, but didn't really talk about,
which is the area associated with the learning memory,
navigation, and things like that.
So, in these two areas, you do get new neurons,
and their appearance and their placement within the circuit
and where they synapse is known to be associated
with the production in the hippocampus anyway of new memories.
So, one of the things that happens in people who have memory loss
is that system isn't working quite as well.
You're not getting enough neurons,
you're not snapping properly, it thinks of that.
So, that is...
But we know the locus of that.
Maybe there is a day will come when, you know,
you can encourage the production of new neurons.
So, you know, you could make the argument,
but part of the reason we developed these kind of age-related problems
is maybe humans didn't originally live that long.
Most people died young.
They were eaten, they died of disease, you know,
they died of war young.
So, at an earlier time in human history,
we wouldn't have needed, well, in the general populace,
to kind of maintain all of these structures of that.
So, maybe that's been changing,
and we're unfortunately stuck with an earlier evolutionary program.
That's what's going on.
So, it's possible that those things can help.
And also, you know, in general, ageing produces effects on all yourselves.
So, there's things called telomeres that are at the end of chromosomes.
And they, as you make copies of cells, the telomere is shortened.
And if they get shortened so much that they're not really delimiting the ends of the chromosomes,
then you start running into problems.
Because, you know, the program for transcribing genes to make proteins and things won't work as well.
So, that's another issue.
But not all creatures have that.
So, there are creatures that are essentially immortal
that don't have this telomere shortening.
They basically die when they're eaten, or they don't get food.
There are, there's a jellyfish that gets younger at some point.
It seems to get younger, and then age again sort of oscillates in its effective age.
Very strange.
Very interesting, we'd all love that.
So, and these are all subjects of the study of ageing.
There's actually even weirder stuff still, right?
Just to reveal the sort of set of possibilities that can happen.
So, there's a creature called Hydra.
You can chop it into two.
It's got a nerve net. It doesn't have a brain.
It has a nerve net, so there are nerves all over it.
It's kind of hooked up in a kind of neural network that operates Hydra.
I'm mostly talked about creatures with centralized brains,
but some creatures actually have a nerve net that's kind of spread about.
You can chop Hydra in two.
It'll make two Hydras.
You can take Hydra and put it in a blender and kind of separate its cells.
All separate.
You put it back together, it makes a Hydra.
All the cells find their location.
I'd love to be like Hydra when I grow up.
Right?
So, the biology, the living things have many, many, many tricks
that we're only just beginning to plumb the depths off.
And so, I think in time we will learn to do these tricks.
Heck, you know, you lose a finger in a circular saw.
There's no reason why you can't grow it back.
I mean, the program is there.
So, I'm not talking about neurons now.
I'm talking about the rest of the body.
But all of these things are in principle possible.
We just need to know how to unlock within the, in that case, the circuits.
I mean, you can think about the organization of your body
in terms of the circuits of what cells communicate with what,
what turns, what on, et cetera.
It should be possible.
We just haven't figured out how.
The problems of aging and everything, you know.
Once we understand how these circuits work,
there's no reason why we can't get them to do what we want.
These are machines. We're machines.
And if you can repair a car, you can repair the head.
Thank you so much, Vijay.
And be sure to join us for our next set of lectures.
We have our ulams coming up in September, September 19th and 20th.
Have a good night.

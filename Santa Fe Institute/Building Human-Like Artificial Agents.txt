dynamic decision-making laboratory. She has made various contributions to theory models and empirical
research on individual and group decision-making, in particular in complex and dynamic situations,
which is what drew me to her work, and also your texture as one of the most important scientists
today. And I think it's been working also a lot, and she's writing to Human Air Coordination and
Cybersecurity, and she co-directs one of these new NSF National Air Research Institutes.
This one is called Alliance for Societal Decision-Making. She is a fellow authority
science society and human factors ergonomic society editor of Mary Sweeney journals,
and she has been leading a wide range of multi-million and multi-year collaborative
efforts regarding industry, such as multi-university research, initiative grants from
Army research laboratories in Army research office, and large collaborative projects with
DARPA. She has visited SFI a number of times. It's one of the lead authors on the paper on
collective adaptation. That is the background for the work. She definitely is happening at SFI,
and I'm glad to hear your talk. Thank you. Thank you so very much for having me here. I've been
to Santa Fe in the past, and every time I come here, I'm like in heaven. I feel that I can relax
finally, even for a little short time. So what I'm going to talk about today is
work that basically I've been doing all my life, all my research life, and it's this idea of building
agents that look at, are like humans in terms of the thinking and the cognition and the decisions
these agents make. But in particular, I am interested in studying very complex dynamic
situations. We call these situations situations of dynamic decision making. So in conditions like
the examples you see here, there are more than one decisions that need to be made.
We make multiple interdependent decisions under a lot of stress and time constraints.
There is usually high uncertainty on the way we make decisions and take, for example,
the disaster management case. So there are multiple actors working in trying to allocate
resources in a very constrained environment. Those resources are usually limited. And so
figuring out how to allocate those resources on time to save the victims is a real challenge.
Obviously, a challenge in which the individual human cognitive abilities also play a big role.
We cannot remember everything. We have to sleep. We are stressed. We feel the time pressure, etc.
So these are the type of situations I'm interested in helping humans make better decisions.
And AI, at least one of the initial goals of AI, was to create agents that would be
undistinguishable from humans. So be able to create agents that would make decisions like humans do.
But in reality, the current view of AI, at least, is about machine learning algorithms
that are trained on data. And they are trained to optimize decisions that is very much not human-like,
because humans are not optimal at how we make decisions. So the goal of these AI tools has been
to create these elements of optimal decision-making faster and better than humans. And in fact,
many times, we see competitions between humans and AI. And of course, they are amazing, and they
are fantastic, and we need them. And they have amazing applications now with increasingly
complex data sets and features. And be able now, obviously, to generate content from PROM,
which is really incredible. So things are advancing very fast. However, I still
believe that to be able to address complex problems like the one I have been talking about,
we need more than algorithms that are optimal. I believe that these patterns, generation patterns,
can help in situations like this. But they are really not capable of making decisions
on their own, at least not in the way humans would do it. Because the kind of situations humans
have to confront are more than just economic, are more than just optimizing an objective value.
They are about various factors and consequences, including ethical and moral trade-offs,
that are very hard to emulate or to optimize. So I think the ultimate responsibility for
decision-making in these type of situations is human. And therefore, we need to figure out how
we can use these tools together with tools that emulate human behavior so that we can help humans
in these situations. So our goal, should I take questions now? Okay, go ahead.
I don't know.
It's all right, yeah.
Yeah, I was just wondering about the previous slide and how you said that ethical and moral
considerations seem to, there's a fundamental assumption that you cannot quantify it or you
cannot encode it as part of an objective function. But I wonder where does it come from?
Yeah, I wouldn't say that we cannot. It's simply we haven't found out how, you know, how to
consider all the different trade-offs and the different type of demands that humans need to
live when they are making decisions in these kind of situations. Right? So that's precisely where
we need to make progress. Right? There is another. I just wanted to follow up. The idea about ethics
and morals though is that they kind of can't be quantified because they are subjective and relative
and it really is contextual and you define things right down to make it morally right or wrong.
I guess that's a huge like complex web of trade-offs, I guess. But I guess, like you said,
it's not impossible. Yeah, I'm not saying it's impossible. I think we are going to get there,
but I think we need more research on precisely how humans make decisions in these complex
situations before we even attempt to create any computer program that tries to emulate that process.
So that has been my goal, yes.
I make complex decisions where I don't exactly know what the outcomes are. It also depends on my
risk attitudes and how important this is. I mean, people, for example, during pregnancy,
there are very strong opinions and you are not allowed to do this and this varies over countries,
but at the end, Emily Oster wrote this nice book just to know what are the facts and then
everyone has to judge on his own or her own how to trade drinking wine against the low risk of
whatever that would be. So these things are probably not part of... No, they are all part.
They are all part, but you are going to see next how I make a distinction between sort of the
traditional risk assessment and dynamic situations and how I believe we make decisions in dynamic tasks.
Across individuals, across situations, absolutely, yeah.
Yeah. So my goal has been to improve and speed up human dynamic decision making.
And we want to do that by building algorithms that emulate the human cognitive processes
and that they can inform other tools like AI optimization algorithms so that we together
can make better decisions in these kind of situations. I plan or I want to go beyond just
being inspired by humans, which is what usually computer scientists would do. They do care about
humans and they do observe what humans do, but usually they take humans as an inspiration
only. What I'm saying is we need to go beyond just taking humans as an inspiration,
but really understanding their cognitive processes and being able to emulate that process,
that is creating these algorithms that can emulate the decision processes, including
the biases and errors. So I actually believe that being able to replicate human biases is
essential because the only way we can on bias humans is by being able to understand where
the biases come from and then be able to use other tools to help humans make more correct
decisions. So sometimes the process of using simple heuristics has been shown to be more
effective in these dynamic complex tasks than even very complex reasoning or data sets that
large amounts of data in algorithms that come from large amount of data often are not as good
as human experience. So this is sort of the idea of using not the traditional approach to AI,
but really to promote the idea of we need representations that can replicate the cognitive
process. Why cognitive algorithms? Well, it's a little bit repetitive, I guess, here is we
are not trying to create super humans, but we're trying to imitate cognition and here are some of
the advantages and some ways in which these type of algorithms can be used. So they can help explain
the cognitive process of decisions on their uncertainty. They are also dynamic and they can
learn, presumably like humans learn to make better decisions. They can inform other decision aids
or other autonomous systems to help humans. They can also be synchronized with the humans. So for
example, if I want to emulate Mirta's decision processes, I will need some information about
Mirta's past experience, but I can trace Mirta's decisions over time to be able to predict what
Mirta is going to do in a particular situation. So we can do that with these models and we can also
make them be collaborators with others. So if I can emulate Mirta's decision, then I can predict
what Mirta is going to do when she collaborates with me. So that's in general the idea of why
cognitive algorithms are different from traditional or at least currently traditional AI.
So I have been trying to pursue these two essential questions. My whole research career is
about how do human-made decisions learn and adapt in dynamic tasks and the second is how can I
represent such process computationally. So I'm going to go with the first one too and when I came to
Carnegie Mellon, which was late 90s and beginning 2000, I started as a professor.
The concept that I observed about decision-making was very different from what I had in mind.
It was static decision-making, economics decision-making, but I was inspired by many people
including this quote by Edward Edwards, where he said that static decision
theories have only a limited future. Human beings learn and probabilities and values change.
These facts mean that the really applicable kinds of decision theories will be dynamic
and not static. So I was stubborn enough to pursue the idea that we want to study dynamic
decision-making even though nobody in my department understood what the hell I was talking about.
This is what they were talking about and this is a more traditional approach of classical
decision-making. It's a linear process where you have alternatives. The alternatives are usually
well-established meaning they are obviously provided to you. You have option A and option B,
usually represented as a decision tree and then calculated based on expected value,
what humans should do. Expected value concept of course is an extremely useful today and always
will be, I think, a concept on optimal decision-making, but of course very soon people realize that
humans don't make decisions optimally and so this famous theory by Kanieman and Tabersky,
which is still very popular and very influential today, what they essentially did was to modify
the concept of expected value into functions that are more human-like and of course they
supported these functions based on human behavior. So humans are not optimal, instead humans are
behaving according to these other functions and that's how you can find out what humans are going
to do. And yeah, that's fantastic and still influential, but this type of theory is just not
applicable to dynamic situations that are constantly adapting according to various factors over time.
Now if we go completely to the other extreme of what I was studying early 2000,
I worked with Gary Klein in a very big project for the Army Research Laboratories
and Gary was the opposite of all the behavioral economics. At that time it wasn't behavioral,
it was just economics. So Gary Klein was studying naturalistic situations, in particular firefighters.
So there are books about naturalistic decision-making, there is a conference,
there are lots of people interested in this area, but essentially what you do in this case is you
actually go to the place where decisions are made. Let's say firefighters and he describes in his
book how he would get on the trucks with the firefighters to find out how they make decisions
in those very constrained tasks and then he found out that they don't make decisions, at least that's
what they express the firefighters. They just said, I just know what to do, I just get in there,
I've seen these situations so many times in the past, I just know what to do. So he documents in
his book all the stories about the many firefighters making decisions and this type of approach has
been used in hospitals and in many other naturalistic situations. So out of that work came a model
that he called the recognition prime decision model which was very influential to me. The essential
element of recognition prime decision model is recognition. What is recognition is our ability
to determine the similarity between a situation we are confronting and what we have confronted in
the past. And so he proposes how such a match determines whether we know a situation is typical
or not typical and then based on that we engage in some sort of mental simulation to figure out
different courses of action. This presumably happens very fast and then you are able to
finally decide what to do and implement a course of action. So very inspiring, it made a lot of
sense to me but it wasn't computational and it hasn't been as far as I know up to now. There has
been many attempts to make this sort of theory computational but to my knowledge none of them
have been really successful. So what I do is sort of in the middle of these two extremes. Dynamic
decision making I sort of realized that I'm not going to make a lot of progress if I go and get on
the trucks with firefighters. So instead I'm going to take the approach that others have taken like
Brent Bremmer and Joaquin Funke to create micro worlds. So these are reductions of these real
world situations in which we can actually use experimentation and to me it was very important
to be able to see the cause and effect relationships not only rely on observations.
And so this is just a collage of the many micro worlds we have developed and used in my lab
through the various years. Initially I used and developed this and it took many many years
to develop. It's a water purification plant task that has all the major elements of dynamic
decision making. It's a dynamic resource allocation with limitations of time, etc. So we developed
a theory we gathered a lot of information based on that task but then the main question was like
well whatever you are doing is only applicable to this task. You haven't shown me that what your
models and your things that you are doing can be applicable to many tasks. So then we went wild
and then we just developed all kinds of micro worlds and started to apply to many other tasks
to demonstrate that our theory and our ideas were more general than just that particular task.
Now this is a summary of a behavioral phenomena that came out of experimental work with many
different micro worlds. So I'm going to go over it but relatively quickly given the time. So the
first thing is when I arrived to this area the picture was very frustratingly negative.
So all humans are very poor at making decisions in dynamic environments. Even when you give them
full feedback and you give them unlimited time incentives, extensive practice, we are born
with something that doesn't allow us to make good decisions in these very complex environments.
People are generally poor at handling systems with long feedback delays. A lot of people were
Bremer including basically almost all his work was about demonstrating of long feedback delays
and the effect of that on decision making. So the longer the delays, the poorer decisions we make.
So okay, I was taking all that and I was in agreement with all that but at the same time
I could observe that people in the real world, some of them are pretty good at making very
complex decisions. So how can we explain that? And my response to that is that
it's in the learning process. It's in understanding how we go from not knowing how to make decisions
to really making very good decisions in really complex environments. So I started to study
different things. For example, I found that if we give headroom meaning space for learning
to individual decision makers, then they are able to adapt to more difficult decision situations.
For example, putting someone in a low time constraint is going to help for that person to
perform under high time constraints more than those that are always trained under high time constraints.
So the headroom is needed for learning heterogeneity. Experiencing different
variety of situations are going to help us to adapt to novel situations and we have several
studies on that. The ability to pattern match is extremely important. At the time we collected
our ability to pattern match with the Raven progressive matrices test, which is essentially
a pattern matching test and we demonstrated how the score in that test is very predictive
of the quality of the decisions that human makes in these micro worlds. And we also found how to
provide feedback in a way that would be more useful to individuals by particularly providing
observing behavior of an expert was extremely helpful for people to learn and to adapt even
after removing that feedback. So this is a list of some of the phenomena and all the things that
we have found. I think all over that list there are two essential elements of dynamic decision
making and they keep coming over and over. One of them was recognition. So I was convinced that
similarity and our ability to detect similarity is essential for making good decisions
and memory that is our ability to create context specific knowledge. So very different from
traditional cognitive theories that believe that humans with experience generate heuristics.
I actually believed it was the other way around that humans use heuristics when they don't have
knowledge. And as you acquire more context specific knowledge, you actually depart from those
heuristics that are not good to start with. They are only approximate. And so you move away from
those heuristics and you start to apply context specific knowledge. So let me move to the second
question which is how to represent those things computationally. My approach is on cognitive
architectures and in large part because I was at CMAO and in CMAO is the birth part place
of cognitive architectures and in many ways of AI too. So in particular I was inspired by the work
of Allen Newell and Herb Simon that wanted to do had this idea of creating unified theories of
cognition. They imagine or envision this complete program that would be capable or make of making
all the activities that human mind was able to make. So they imagine to represent all these
cognitive steps and be able to explain all the components of the mind and how they work and
produce cognition together. Many books that are very inspiring coming from this tradition.
My personal view of this is that it is very eutopic. It is an extremely complex problem
and therefore is very hard to accomplish. So it was accomplished partly by the actor unified
theory of cognition. So John Anderson at Carnegie Mellon and Christian LeBeer one of John Anderson
students and then postdoc and now faculty. Research faculty has been working on this idea from Allen
Newell and Herb Simon and if you look at the history of ACTAAR you are going to see that it has
just become more complex with over the years and in my personal opinion also less useful.
So what I did was to grab what I felt was useful from that cognitive architecture
and this is essentially the way knowledge is represented in declarative and procedural forms
that is with facts or with rules and in symbolic or sub-symbolic ways that is with
formulas that would explain how those facts or rules are going to evolve and change over time.
So that's what I did and basically I took that to develop my own mini architecture if you will.
So this theory called instance-based learning theory is essentially a dynamic decision making
process that is represented in a learning loop. So I'm going to go over this process and this is
sort of the corresponding picture of RPM the recognition prime but my own and there are also
many differences obviously. So the first step is again decisions are made by recognizing
the similar situations and mapping with decisions that have been made in the past to be able to
retrieve something that worked in the past and that happens in this recognition process.
We evaluate the new actions according to the utility of the past decisions which are retrieved
from memory and then we explore mentally all the possible alternatives as you can see this is
very similar to the mental simulation that Gary Klein was talking about and then finally we make
execute a decision that is the highest up to that point and that execution is going to modify
the environment and finally whenever we get feedback we can reevaluate the utility of those
decisions we have made in the past. So that's conceptually what the theory the learning loop
is about. Now let me tell you about the representations so the idea is that decisions
are stored over time in memory in the form of instances and those instances are triplets
those triplets are the associations of the features that are necessary for that decision
the action that is taken and the outcome the outcome can be the observed or the expected
utility that is calculated about making that decision. Now for each potential action
action one or action two instances when you try to make a new decision instances will be blended
according to the similarity of the features and then the action that has the maximum blended
value in this case let's assume it's action two is chosen at that moment a new instance is created
with the blended value and then when feedback is received that blended value is changed for the
actual feedback outcome that was received okay so that's the algorithm in the in the
representational form and this is the algorithm in the mathematical form so the most important
equation I think in this in this algorithm is the activation which is not developed by me at all
is borrowed from ACTAR so ACTAR has backed up evidence for each part of this equation with
a lot of experiments regarding how humans process information so this activation equation has three
parts this part is called the base level equation this part is the partial matching idea and this
part is noise and essentially this equation takes into account the frequency of events
so we tend to believe that something is going to happen more often and actually to retrieve more
that information faster when it happens more often okay so frequency of events matters a lot
for our ability to retrieve information from memory and so that is represented here we also
tend to forget so things that happen yesterday I can remember faster than things that happened
many years ago and that is represented by this non-linear decay function and then this part here
represents the partial matching equation for each feature in the instance we can
determine a particular similarity function which is then aggregated across all the features
and then sort of penalized or exacerbated with a partial matching parameter and then finally the
noise which this is just noise right now there is not a lot of theory about what doesn't fit in
this part it's just this part here is a draw from a random distribution or other type of
distributions and this one is one of the of the parameters so for each instance you calculate the
activation at each time at each point of time okay and then you can calculate the probability
of retrieving that instance from memory which is essentially the activation relative to the
activation of all the instances in memory and then this is the magic I guess of IVLT what we do
is essentially combine all those instances that belong to a particular choice option
in the form of an expected value so this is the probability times the outcome okay but obviously
this probability is a cognitive probability it's not the actual probability of an event
and it's a probability that is calculated all the way from here right so in this way we are
accounting for the cognition of how do we do we forget information how similar the information
is etc okay and we create this expected utility value that we call blending and then we select
the option that has the maximum expected value so that's that's it now I have been interested as I
said before in figuring out how general this theory is across many decision-making situations
and how human-like this algorithm is by comparing the predictions from an IVL model
to the results from an experiment in a particular task what I'm going to show you now is a set of
examples of human likeness of IVL agents and it is important to know that all these examples
that I selected here do not fit data that is in none of these examples I first collected data and
then I feed the model to the data and then I present the results anybody can fit data
all these examples come from the theory so we do simulations with our theory and then we look at
the data and we see how close we are to the data okay but we don't fit data that being said doesn't
mean that we cannot fit data we of course can fit data and that is only going to make things better
right so we can fit data and we can fit data at the global level at the individual level
whatever level you you want we can fit the data but that's not the point of the examples I'm going
to show you so because I started with a very complicated task and nobody understood what the
guy was doing something happened on the way that made me realize I need to simplify things to make
things understandable and so I went to the root of decision making which is binary choice
but in this case it was not going to be just the decision tree it was going to be a dynamic
binary choice task by the way what I'm going to show since even that there are many examples
it's just a snippet of of the various examples and if you want to go in more detail in any of them
I can but you know I wanted to show a good variety of of these examples so the first one is binary
choice and essentially in this case this is sort of an example of the task people just have two
buttons and they receive an outcome outcome after clicking on a button and then they go on and on
each of these buttons have a particular distribution of outcomes assigned to it and so the idea is that
over time people are going to learn how to obtain more points out of the right button the bottom
that is the maximum expected value right but from experience so what this figure is of course not
readable but the idea is to demonstrate how each of these squares are different problems with
different distributions in the two buttons and it shows the proportion of maximization is a y axis
and it shows two lines in each of these things the dotted line is the observed
actually it's not the maximization it's the risky rate so the proportion of times you choose a risky
option so the dotted line is the human data observed in an experiment and the black line is ideal
predictions and so the idea of this figure is just to show that in a large variety of problems of
these tasks the model just from simulation is able to predict the learning process that humans
follow in this particular task so the next sort of complication in this journey was to actually
demonstrate that the if the probabilities change over time the model was still going to be able
to predict that trend that humans would would be able to to do when they were performing a
changing task so in this particular example this shows the binary options and the change in probability
during a particular time period and so humans are doing still that binary choice task but the
probability changes of one of the options right and so the figures show the rate of
observed choices again in the dotted line from human behavior and the predictions from the IVL
model okay numerically we can calculate the actual relationship between human and model with
some metrics but I'm going to come back to that later on another example is a collaboration
a cooperation between two agents in the prisoner's dilemma this is some of the data I presented also
the other day in the in the workshop but I'm expanding these figures now a little bit so the
idea was that if we can emulate the choice binary choice in this case of a particular
individual can we emulate the behavior in the collective behavior in this case cooperation
emergent cooperation of two individuals working together in particular in the in the prisoner's
dilemma in this example so we created copies of the IVL model one representing player one the
other one representing player two and we put them to work in the prisoner's dilemma and we did
different levels of information provided to the players so what we observe here is again the model
in this case the model is the lighter color and the human behavior and this is the proportion of
the collective cooperation of the pair and what you see here is actually the strategies the
sequential strategies so for example mistrust is the number of times that a player defects
after the two players have defected forgiveness is the proportion of cooperation after the other
defected even though I cooperated etc so the point of this is that the model also captures
the sequential strategies over time so it's not just capturing the outcome metric but also the
effects of the sequential strategies and also does it with different levels of information
um we did have to change the um the blending equation to capture the descriptive information
I can tell you more about that I explained that in our talk the other day okay so the next
sort of escalation of these examples is whether it can capture the effects the collective effects
of groups so for this what we did was to create different networks with different numbers of
connections and we used two different um pay of matrices and this came from the work work of
valier and colleagues on a taxonomy of two by two games so we chose the extremes of those two by two
games the independence and the interdependence games and we run simulations with our ideal models
to play well 16 players in this case making collective decisions and then we aggregated
their results and what we observed is that in the independence matrix the collectives are able
to figure out what is the best option for everybody that is this 5 5 um actually faster
the more connections there are so they they are pretty good at finding the right option pretty
quickly almost immediately we can see this but it's faster the more connections there are
however when there is the interdependence that is when the outcome I'm going to get depends on
the other person um that's not the case that is more connections actually resulted in less
ability or inability to determine what is the best action for both of us the AA action
so this was curious for us and you know this particular degradation of performance
based on the number of connections that the agents had and so we wanted to see whether this
in fact happens with actual humans so we run the study with teams yes what is the mechanism
why is interdependence so bad on yeah so the why which um you know I'm not going to show you all
the evidence for that but the why has to do with um our ability to know the one-to-one
correspondence of the actions so if I play with all of you I need to keep track of what you did
to me last time and what Hendrik did to me last time and what everybody did to me
it's very hard to learn that right so next time when I meet you again uh then I need to
figure it out or remember right oh she did B I should do A with her right it's very hard to
keep track of that the the more connections you have the first one is not difficult because it's
very easy to find that regardless how many connections you have if you pick A that's what
everybody should pick but doesn't it depend on the payoff structure specifically of this
interdependence pattern I think it does okay I secure myself you could have in a slightly
different payoff structure you could have the default I don't remember what she did but
it does in a pro-social way yeah it does it would go up it does and I mean this is an example that
it does right but in addition to that valier at all have a whole taxonomy of two by two games
and we run this model in the whole taxonomy and this was the only there were other things that
happened but this was the only one in which we saw this effect so this paper is still I haven't
published this paper but it's still in process by the way sorry I had to step out of it that was
my doctor call I so the mechanism you just described seems to depend sensitively on the
internal structure of these computational agents on their memory capacity yes I mean if they have
a representation which does simply record what every other player did yes exactly and this wouldn't
happen so exactly totally so there are some choices here of kind of the parameters and the structure
and and maybe even your definition of similarity right so I mean yeah same definition of similarity
would only ask what's the what happened the last time that Catrine played that way yeah as opposed
to recognizing that I can learn about my interactions from Catrine from my interactions with Iname
Haye Iname so I mean the so there are a lot of choices underneath here I mean the the choices are
very transparent in the equations of the model that's the choices in this particular task the
binary choice that you don't even have attributes you know a and an outcome so there is not even
a similarity in this case but the frequency and the recency play a role and that's just organic
in the model there is no yeah there is a choice of the parameter of the k but we don't in I forgot
to say that in one of these results we are manipulating the parameter we are using the
default parameter that is used in ACTR so it's just coming from these predictions are coming
from the theory but in this setting aren't there some choices about how I represent past events
very good yeah in every setting there are and that is the major point I'm going to come back
to that later if you don't mind but that is exactly the major one of the major things
that we need to improve the main choice is the designer of the model on what to represent in
the instance we we say that we don't represent anything that is not fully available to the human
and it's true that's what we try to be very committed to do right but still there are choices
on what to represent in the instance right this case is not that case because it's very simple
you just see a or b there is no attributes and the outcome it is that I can come back to the
issue of representation I know there are many questions you think you could wrap up in like five
minutes okay and then so there is still time for more discussion then here is where my strategy to
cut a lot of stuff comes so I'm going to show a few more things I'm going to wrap up in in five
minutes then okay so again we run the exercise with human participants in groups and the human
participants in this case where groups of six because running 16 was a lot more complicated
but we managed to gather a good number of groups in each of the conditions and this is what we
observe so the main effect that we predicted was verified with the human data okay the next example
is a little bit more complicated and is more realistic too and in this example there are many
features it's about fishing and it's about our human decision to whether let pass a fishing email
or not so we use a data set that was collected by someone else not us and that data set was
collected in this form so there were a bunch of emails that they created and some of them were real
so real fishing and real ham those are real emails and then they simulated some fishing and ham emails
and then they asked people to rate the fishing the emails whether they were definitely safe to
definitely suspicious and so what we did is we plotted the human data it's here we emulated
the same decision with our model and it's here and we can see that the distributions of the model
are very similar to that of the human and finally a never more complex task because this is a very
popular theme of research is cyber security and in this particular case we are emulating the
decisions of a defender in an individual task and there is a network that the defender needs to
keep safe and there are some strategies of red agents or red
attackers two strategies are given you know implemented and the strategies vary in the
way they explore the network to reach the objective which is this operational server
the interface that the humans use to do this task is what you see here and we again run and publish
a paper which is the predictions of what the model does against as defending against the two
attacker strategies the meander and the other more direct strategy and this is our results
from the human experiment again we cannot run 2000 episodes in that case so we run seven episodes
but we see how our human experiment emulates the predictions from that model
so okay given the time I think I would like to conclude with this which is our reflection of
human likeness so I think you know what I have been talking about now regarding human likeness
is based on outcome metrics so I emulate the decisions of the human and then I compare the
outcome of the model to the outcome of the human usually we use generic metrics like MSE often
they are used at the aggregate level to be able to say hey our model is doing very well and that's
what every cognitive modeler does but I think that's wrong I think we really need to improve
this metric especially that we have the mechanisms to to be able to compare at a much lower level
we should always at least compare at the individual level and be able to evaluate the steps each of
the steps at the at the individual level let me say just that we are using now these models
more directly in some applications to be able to help prevent the biases that humans have
and to be able to help humans make better decisions for example connecting these models
with machine learning models and optimization models and using our cognitive models as human
collaborators oops sorry so this is one of the ways in which we are actually using our models
in cybersecurity so by emulating the actions of an attacker or an end user in the case of fishing
we can actually provide the predictions of what these humans are going to do to
heavy machine learning algorithms that are being used as defender strategies
and so by providing these predictions these defender strategies are being adapted to be able
to modify in these cases a stalker per security game modify the allocation of defense resources
the other way in which we are using these models is by making them work with the human
along with the human so instead of having one individual making decisions in the cybersecurity
task now we have a team and the team is an AI and a human making decisions in this in this
in this type of security task they collaborate with each other and they are able to accomplish
the task better than a human alone can do so in conclusion our current focus of AI
no I thought I had it
I'm almost finished though but I thought I had it maybe I never oh it's not connected
yeah
so our current focus on AI is on algorithms that aim at making faster and better decisions
and the the current focus of AI is on creating algorithms that compete with the human and that
are able to make faster and better decisions than humans do but our goal in contrast is to build
learning algorithms that can emulate the cognitive processes and can inform those optimization
algorithms to be able to speed up the human decision process there is evidence of human
likeness in these IVL algorithms but we know that we need to improve the metrics of human likeness
what do we really mean by human likeness and we need to develop this evaluation of human likeness
at the cognitive level steps so that is what I think is required to demonstrate some of the
usefulness of these of these models yeah thank you
okay maybe some people need to leave but who can stay please go ahead
Melanie
so um thank you for this talk thank you so the um you mentioned the prospect theory of
Kahneman and Tversky and one of the strengths of that was not only trying to match human data on
specific examples but also kind of being able to abstract out very general biases human cognitive
biases yeah and um you know Kahneman wrote a whole book about is it good can can this
can this also abstract out from from your data very general human biases in these dynamic
situations some but not we haven't done a lot of work on that but confirmation bias for example
is something that we can definitely predict um the the big difference is that we are interested
in biases from experience right that's in very sharp contrast because in their theory they rely on
descriptions of the options right and so having the descriptions is very different than just acting
and observing the outcomes so we only focus on biases from experience and one of them is confirmation
biases and yes our model can predict that but there is quite a lot of work to do on characterizing
those biases that we can predict from experience that we haven't done there's a question from
can i just follow up on this so i mean yeah one of the one of the this is also the Kahneman and Tversky
most are rarely or ever implemented computational it's basically just a verbal re-description of
what's going on and something really disputed but this basically your joint computational
in what can emerge and maybe there is no general class even for some of them but yeah i'm a little
frustrated with the heuristic and biases uh situation because i mean it cannot grow longer
because there is no more paper i guess i don't know it just keeps growing and growing and new
biases are emerging and i was very much in uh in line with the initial idea i really like it and i
think it's still powerful but now everybody wants to bring a bias out it's like um yeah what's what's
the uh it's like the 20 questions right idea of of uh allen newell is we are doing all these very
tiny experiments but we are forgetting about the global picture so um definitely there is a lot more
work to do on biases from experience
yeah um i was wondering about the previous slide of where i don't
write the previous to this one yes just a certain class so i'm wondering there's a
little bit of um cluster clarity but in a way on the one hand we want some machines to make that
humans right on the other hand we're evaluating your machines well there's some
so at what point do we so it seems that there needs to be some kind of decomposition of human
decision making in terms of what do we actually want from us to extract from it what comes from
human limitations just cognitive limitations not being able to evaluate everything yeah how to
split that here yeah yeah i had a lot a lot of slides going step by step in the process
actually a paper that is coming out in uh pps that's that so if you are interested i can i
can send you that but um yeah so basically i would break out i broke out each of the steps
in the iblt process to analyze what we know and what we need to know and definitely one of those
things are evidence for particular similarity similarity metrics right so in our model for
example we often use just linear similarity and sometimes if it doesn't work well we change it
right there is no not a lot of theory even though there is quite a lot of work on similarity
judgments right but how to translate that empirical warning to a computational mathematical form
and then be able to test it within the realm of the model we haven't done that work and that's
one of the areas we need to work on go to there is a question for donald then yes thank you for a
very interesting paper there are two points that i uh wasn't sure i i understood uh first of all
i want to applaud the idea that you want to enhance human decision making and make it as a rather than
make it uh slavishly dependent on AI decision making this is a theme that i've been uh
asking for years uh i call it the difference between the nautilus machine and the bulldozer
the bulldozer lets you move mountains but you're still a 98 pound weakling the nautilus machine
you actually develop the strength yourself the technology is being used to improve your
abilities uh and make you less dependent on the technology uh so i picked that point up i i want
to see if you stress it the same way i do but the but the question that i really want to ask you about
is
consider the notorious rueful remark reflect the remark of somebody says well it seemed like a
good idea at the time and this requires memory of your past decision making in detail and memory
of your evaluation and the grounds for that evaluation now that seems to me to be a very
important part of human decision making that is the capacity to be self critical and reflective
and to learn from your mistakes by being able to sort of debug your past performances i didn't
think that i saw any sign of that reflective capacity in your presentation but maybe i just
missed it the way you were saying it yeah let me address the the second point before i forget
my memory has a large decay so yes the um you are correct that i didn't address that point very
concretely in in my presentation but um that idea is essentially um express if you can see the this
slide in the feedback uh mechanism and so what happens is that um usually the feedback is delayed
and there is nothing we can do about it so and furthermore we can make many decisions
and then get one outcome and we don't know which of the decisions we made is responsible for that
outcome so we use a concept that is well known in ai of credit assignment so how do we associate
that outcome to the various decisions that were made uh this is the other major area of research
we have a paper that we've been having trouble to publish but it's made available online where we
test various mechanisms of credit assignment including the td mechanism in reinforcement
learning so this is a very important uh part of learning because it's the only way that you can
modify the expected utility with the actual utility of the decisions you are making and how you
do that process um will determine a lot how the learning is done uh so we usually just do
equal credit uh until this paper that we are still exploring and figuring out
different credit assignments and trying to figure out how humans do that credit assignment
and so there is quite a lot of research that is needed in in that area and to go back to your
first point of the bulldozer and the dependency on um technology i think we are going to be dependent
on technology um you know no matter what and i think that's a good thing i don't have anything
against uh technology but the the um essence of human life is the human and the essence of any
decision making in any complex environments i think will continue to be the the human so i think we
should use the technology to empower the human to be able to uh help the human improve their
decisions and learning mechanisms is is essential for that by figuring out how so it's not like i
give you what you want now you know it's like it's feeding you on what you need right now it needs to
be a lot more global than than that thank you um let me just comment um i um i think
if you try i'm trying to imagine your systems acting as sort of tutors
uh for human beings and tutors that have good models of how the human beings are making decisions
because they are also models about how the tutors are making decisions
and so that in effect the tutor can say yeah i i'm making this up now yeah i was tempted by that
way of thinking about this problem oh you you are absolutely correct one of the most successful
programs from ACTAR has been on cognitive tutors so being able to use ACTAR to help children
learn mathematics so you are absolutely right we are using tutors of a different kind we can say
it's not about mathematics it's about making choices in some complex environments but that is
exactly you know how we are using it we can trace as i was saying before we can trace specific
students decision makers right and their history and therefore we can make uh predictions about
the decisions that particular person is going to make in the next uh opportunity right and then
we can use other tools machine learning included or in any other AI tools to support that decision
making process good thank you interactive discussion there are maybe a couple of more
questions from people yeah who wants to you choose we've talked about a lot of crisis and errors
and honestly if you see there is something like that in a way they have an adaptation to the world
in which we live or have lived so it's in your conception of a dynamic environment is it also
part of the idea to change and the environment to you know have these what you call what we call
biases not occur as biases and shortcomings we will have interventions on on those biases yes
so a particular program which hasn't started yet is from IRPA so I am about to start a very big
program in IRPA on cyber security and it's about predicting the biases of the attackers
uh the idea is of course that we can modify the situation from the defense side so that we can trap
the attacker right and given that the attackers are as humans as the defenders are uh we can trace
down and predict when they are falling trap of certain biases and then be able to react according
to that so yeah I think biases are emergent again there are some biases that are not emergent
related to Melanie's question there are some biases that are based on information the explicit
information but the ones that we are able to handle are the ones that are emerging from experience
and yeah we're we're working on that too yeah it's a question about decision making in science
so you're trying to emulate the way people actually make decisions but we we lack a lot
of detailed knowledge we wish we had about the neurophysiological mechanisms that realize the
procedures that make the decisions and so you're necessarily sort of you have to go beyond that
and you're making decisions that are underdetermined by evidence and I wonder what kind of commitments
or maybe even heuristics you use when it's you have to make these decisions about you know what
what kind of a learner or what kind of a decision maker a human is does that question make sense
let me try to interpret them I'm going to answer based on what I understood
um
our type of modeling is symbolic and so what that means is I don't really go into the neuro part
of it uh at our thus right in my personal opinion that is why it became very complex
because now at our is trying to map particular mechanisms to activation in certain part of
the brains while that can be very important and necessary that is not what I do so I I stay at
the symbolic level so that is one thing but um there was another part in your question
I know Kelly you will meet with Kotin now maybe these two guys could go
questions because maybe you would not be able to relate okay unfortunately my afternoon is full
so here's my question so one way to learn to imitate human decision makers would be machine
learning and I know some people are doing that too and you know just treat the human's behavior
as a thing to be predicted um and I generally much prefer mechanistic models to machine learning
on the other hand when you have a mechanistic model then you're in a position of having to
either defend the specific mechanism or or argue that you're only trying to understand
uh the the typical behavior of a wide variety of mechanisms so when I look at the model that
the mathematical model you showed um and maybe there's something I'm missing I'm not an expert
in this area it it looked like the kind of standard online learning model but with the
delay from time and I know the scientists have tried to figure out do people forget things
exponentially fast or is it a is it a power law etc um but then at least some of the examples you
showed like you know shifting this one where the the expected outcomes of the two alternatives were
changing honestly I I felt I felt like a very wide variety of models that would update their behavior
and forget old data to some extent would have produced very similar things so that that made me
that made me not so convinced that the specific mechanistic model you wrote down
is really the right one um although certainly some aspects of it like forgetting the past so
you can learn about new things is surely part of it but I guess that on the other hand
when I think about humans in dynamic situations
I think that their notion of similarity can be much more general than for instance looking at
kind of individual attributes of the instance people do all kinds of analogy making and you
know like oh that bit of grass which is still smoldering which might flare up again oh that's
kind of like there might be a tiger there or it's kind of like maybe there's an ambush being
played by my enemies or whatever yeah so I don't know the mechanism seems both impoverished in a way
and maybe over committed to specific to specificity in a way so at the same time I'm glad that you're
doing something mechanistic instead of saying well I'll have a year old network watch a human
and then predict what the human would do which is the kind of thing which is far too popular nowadays
so I'm sorry that's such a long question and maybe it's a long answer no I love these type
of questions let me just answer with two with what other two people have said the first one is
all models are wrong right and I totally you know no accept a priority that this model cannot be
perfect in any way right so all models are wrong but some models are useful so I like to concentrate
on the usefulness of these models what it can teach us what it can do right and then the other
quote is about using models as your toothbrush and yes we have the tendency to use our models only
and forget about other people's models we have we have done comparison model comparison and there
are including a site review paper of the comparison of our model with another 16 models I mean the
proliferation of models for this kind of thing is huge right so many but there was a competition
modeling competition and we tested our model against the winners of that competition and many
other of the more typical models and our model comes to be more general and it comes to be
more predictive than most of them so you know again am I using my own toothbrush maybe
you know I like to use my own toothbrush the other part is regarding the the machine learning
yeah I think human likeness depends on what you want to do right so if you just want to predict
a particular outcome of course having a large amount of data can help as I said everything I
presented is predictions from theory I didn't use any data before I made the predictions
no human machine learning algorithm would be able to do that they need the data in fact the less
data they have the worse they are right okay thank you thanks for your time
I just I want to ask in a different way the same question that was already asked three times at
the beginning which is the question of ethics because well two things it seems that this is
a visibly important question um in decision-making and situations of warfare or juridical situations
and secondly it belongs to philosophy right to the western tradition that runs from Aristotle
through Emmanuel Levinas and so on and and the one consensus that the big players within this
tradition all share it seems to me is that from the Greek you know Pyrenees and their idea of
Epoche is as vertiginous suspension of all decidibility all the way through to Veltar Benjamin's
idea of um mystical origins of justice they seem to the consensus seems to be that the ethical or
just decision can only come from a place of radical undecidability right from non-calculability
right law is about calculus but justice comes from the incalculable so so in that sense the
distinction to be made would not be between humans and machines it would be between
calculability and non-calculability if you can calculate ethics or justice it's not ethics
or justice anymore it's just control right i just want it that would be the provocation
i would put to you in the name of philosophy right yeah it's a different language game
as soon as you deal with ethics necessarily you're taking that on board right i think that he said
vote or a control vote i i guess that is a philosophical question
for our model to be able to account for ethical issues there needs to be calculability
and so you know if philosophically that moves out of the ethics um
um briam i would accept it because there is nothing else we can do without calculability
thank you very much thank you

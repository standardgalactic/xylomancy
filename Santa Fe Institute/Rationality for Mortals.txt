And the director of Hardin's really great because Gerd is one of the originator of the
theory of ecological rationality and the simple computational models of rules, algorithms
that people use to make decisions in different contexts.
And he's basically introduced complex system science to psychology, cognitive science because
his main thesis is that we cannot understand cognition without looking at its social and
physical environment, only by careful analysis of both the environment and the algorithms
that people use to make decisions and solve various problems.
We can understand how people behave and why people behave in different situations.
Gerd is a recipient of numerous awards, he is a member of the Academy of American Academy,
German Academy of Sciences, author of numerous books translated in many different languages.
Thanks Mirthal, today I will talk about how to make good decisions.
If you open a book on rationality in neoclassical economics, behavioral economics or psychology
or philosophy, you likely encounter the following message.
Good decisions follow the laws of logic, the calculus of Bayesian probability and the maximization
of expected utility.
This is beautiful mathematical theory, but it does not describe how most people actually
make decisions, not even those who write these books.
Let me start with a story.
A professor of decision theory at Columbia University in New York had an offer from a
rival university, it was Harvard, and he could not make up his mind.
Should he stay, should he leave, should he accept, reject.
A colleague took him aside and said, what's your problem?
Just maximize your subjective expected utility.
You always write about that, exasperated.
The professor responded, come on, this is serious.
I'll invite you today in a short tour in our research at the Max Planck Institute for
Human Development on how people actually make decisions.
I'll start with a key distinction between risk and uncertainty.
A situation of risk is a well-defined, stable, small world where the agents know the exhaustive
and mutually exclusive set of all future states and the exhaustive and mutually exclusive
set of their actions, of the consequences given their actions and states.
Jimmy Savage, who is often referred to as the father of modern Bayesian decision theory,
called this a small world.
There are examples for the small worlds.
This evening, you go to the casino in Santa Fe, if there is one, and play roulette.
You are in a small world.
You know every possible state that can happen, from zero to 36, all the consequences and
probabilities.
The tools for dealing with risk are maximizing expected utility, Bayesian probability updating,
and many more.
In a small world, by definition, nothing new can happen.
I had the quote from Fyodor Dostoevsky, who said, in this world, if everyone would be
rational, he said, nothing would happen.
With amazing foresight, he had seen the development of rationality.
Now there are two, most of the problems have to do with, however, with some aspect and
some amount of uncertainty.
Uncertainty is a vast sea of situations where there is no small world, where we either cannot
know all future possible states or their consequences.
It's not just about probabilities, it's about the state space.
Researchers have two ways to go.
One is to take an everyday problem, like investment, whom to hire, or whom to marry.
These are all situations of uncertainty because things can happen that you had not anticipated.
And then go one way, reduce it to a small world, where everything is known and you can
maximize.
That's the way that most theories in neoclassical economics, also behavioral economics, at
least the standards, are going.
The other option is to take uncertainty seriously, face it, and develop and study the tools that
actual people use to make decisions in an uncertain world.
And here, by definition, optimization is not possible.
You cannot construct even a subjective probability distribution over a set that you don't know.
That you don't know completely.
And I will talk today about one class of tools that are useful under uncertainty, and these
are heuristics.
Heuristics are rules that embody the art to focus on important and ignore the rest.
Heuristics can lead to good decisions in a world where we cannot forecast the best decisions.
So examples are Herbert Simon's satisfying, past and frugal trees, imitation, agent-based
models who use simple rules like the flocking of starlings, the beautiful models, and so
I will today go a different way than most of the models are going.
For instance, even behavioral economics, who criticizes neoclassical economics, criticizes
not normative standard, not homo-economicus, not the maximization of expected utility,
rather takes the standard to evaluate people, and if there is a discrepancy, the blame is
never on the theory.
It's always on the people.
And then you get the list of biases that are all in your minds and explain why you all
make these strange decisions and stubbornly have no insight in your failures.
This is not my message.
I want to study how people make decisions and formalize the heuristics and then find
out in what situation do they work and where do they not work.
That's the question of ecological rationality.
Rationality is not in the mind, but in the adaptation of mental strategies to certain
environments.
Let me start with an example that makes the difference clear between a model made for
risk and model made for uncertainty.
Every Markowitz got the Nobel Memorial Prize in economics for solving a certain problem.
The problem is you have a number of N assets and you want to invest your money and you
want to diversify, not put everything in one pocket, but how?
The answer is the so-called mean variance model.
It's a standard probability model.
You need to estimate all the future returns, their variances, and covariances.
When Harry Markowitz made his own decisions about the time of his retirement, he used
his Nobel Prize winning optimization method.
So we might think, no, he did not.
He used a simple heuristic that we call 1 over N. N is the number of assets or options.
So if it's 2, you do 50-50.
If it's 3, a third, a third, a third, and so on.
Now 1 over N is a heuristic.
The mean variance model is an optimization model.
The mean variance model assumes we are in a world that is stable, where we can estimate
the parameters to some degree of precision.
Number of studies have looked at 1 over N.
And for instance, the study by D. Miguel here has looked at seven real-world investments
and found that in six of the seven cases, 1 over N made more money than Markowitz optimization
as measured by sharp ratios and similar ones.
The results have been found when exchange-traded funds were tested, ETFs which are close to
1 over N, they're very hard to beat.
Now there is a battle in the literature, which one is better, complex optimization or simple
heuristic.
This is the wrong question.
None of this is better.
No algorithm is the best.
We need to ask a different question.
So can we identify the environment, the conditions where a simple heuristic like that does better
than another one, another model, and where it's the opposite?
The answer to this question of ecological rationality is not known.
And some of you might figure out here's some hypothesis.
So if you think in terms of the bias variance decomposition, then 1 over N has probably
a strong bias but makes no error due to variance because it doesn't estimate any parameters.
So the error due to variance means that you get different results depending on what sample
you make your estimates.
So the real question to solve here is when is the bias that 1 over N has larger than
the total error that mean variance makes, that is both bias and variance.
So this is the way I would like to think here.
Excuse me, did Markowitz ever say why he chose 1 over N?
Yes, he did.
He had a psychological explanation.
He said if I would, his choice was between the typical retirement situation, just stocks
or bonds.
If I would put everything in bonds and they would go down, I would feel bad.
If the opposite, I would also feel bad.
I would have regret.
So I just did 50-50 to avoid regret, but it's very different.
He was actually very interested when we and others showed that he wasn't at least where
these experiments are.
It's just stocks, a slightly different situation that was shown that simply can do better.
So the confusion between situations of uncertainty with risk, the idea that every situation of
what uncertainty is one of risk is called the turkey illusion.
So why?
Assume you are a turkey.
It is the first day of your life and the main comes in and you fear he will kill me, but
he feeds you.
The second day of your life, the main comes again, you fear he might kill me, but he feeds
you.
Third day of your life, same thing.
If you do Bayesian updating or any similar model, the probability that the main feeds
you and doesn't kill you gets higher every day.
And on day 100, it is higher than ever before, but it's the day before Thanksgiving and you
are dead meat.
So the turkey was not in a situation of risk.
He missed an important information.
There was an option he could not think.
Now the turkey illusion is probably not so often committed by turkeys, but more often
by people.
And here's one example.
The in 2003, in his presidential address to the American Economic Association, Robert
Lucas, the macroeconomist, argued that one has learned from earlier depressions and macroeconomics
models take care.
So he said, my thesis in this lecture is that macroeconomics has succeeded.
Its central problem of depression prevention has been solved for all practical purposes.
Now what you get is an illusion of certainty if you apply the models that are based on
assumptions of risk to one of uncertainty.
I've shown you just the illustration here.
In the year 2003, when he gave this talk, the volatility index, VIX, went down and down
and down.
So it got better and better and better and better, safer, until shortly before the crisis
hit.
And that's the same thing as the mass modeling models they used.
By the way, the models that Lucas described in his presidential address all assumed a
stable and unchanging structure of the economy.
So what's the alternative?
And here's the research program I and my research group has been following up.
And it has three questions.
The first one is descriptive.
What is in the adaptive toolbox of an individual, an organization, or a species?
So what are the heuristics they use, the cognitive capacities that the heuristics exploit?
And what's the building blocks?
And the goal is to use, to develop algorithmic models of these heuristics, not labels.
We have, since the 1970s, a tradition in the heuristics and biases program of labels,
like availability, representative, nobody knows what exactly they mean.
And they used never to make predictions, but always to explain something after the fact.
The same is with system one and system two, which is just a list of dichotomies.
And they, dichotomies is where a science should start and go to good models.
And this is a case where we went from models to dichotomies, to backwards.
Trusky had models of heuristics like elimination bias.
But then after Trusky died, Kahneman sided with the, with these dual system theories
that have been long criticized in psychology before.
Alan Newell said, in a famous paper in 1957, you can't ask 20 questions to nature.
And criticizing the tendency to not do models, but to do dichotomies.
Serial versus parallel, unconscious versus conscious, fast versus slow, and so on.
And all this is so intuitive that one forgets that one does science.
So the second question is new.
It's a prescriptive question.
When should we use heuristics?
And when not?
And what heuristic?
And then, uncertainty, it's heuristics all the way down.
There's no option.
The real question is, what is smart?
Is it smart to imitate the peers when searching for a research topic?
Or should I think myself?
So here the question is, can we prove mathematical or at least with computer
simulations show under which condition, for instance, one way in works,
where does imitation work, and so on?
And the final question is one of application.
It's doing intuitive design using all these insights.
So I'll start with the adaptive toolbox.
And here is a few on part of the toolbox.
There are core capacities that humans have.
On that basis, the heuristics can be simple.
The core capacities are quite complex, meaning it's very hard to build
a brain that can't do that.
Let me start.
So there are, I can only give you some examples today.
And on the left side, there are social heuristics,
like imitation, word of mouse, which are highly powerful.
There would be no culture without imitation.
Mike Tomasella has shown in his beautiful research that children,
human children, imitate much more precise and much more general than
primate infants.
And it's one of our big advantages that builds culture,
that one doesn't have to learn from scratch all the time.
It's also one of our greatest dangers.
So let me start with recognition.
Recognition is a fundamental ability of the human mind.
We can recognize faces.
We can recognize names.
And recognition is different from recall.
If you cannot recall the name of a good friend, that happens,
particularly when you get older or when you're a jet lag, like me,
at this moment.
But if you do not recognize the name of a good friend,
then you're a clinical case.
So the recognition heuristic exploits
this capability to make inferences in situations where you don't know
mind.
Let me do a demonstration with you.
Are you ready?
OK, assume you are in a TV show called Who
Wants to be a Millionaire?
And you made it through the end.
And here is the million dollar question.
Which US city has more inhabitants?
Detroit or Milwaukee?
Time is ticking.
Who of you thinks it's Detroit?
Hands up.
Who of you think it's Milwaukee?
Hands up.
A rough estimate, 60% of you got it right.
It's Detroit.
When we did experiments with undergraduates
at the University of Chicago, who think
there is more or less in the country,
we got the same result, 60%.
And then Dane Goldstein and I asked students in Germany,
what do you think?
So not just this one question, but all questions
about the large American cities.
What do you think?
How many determines got the right?
I think more than 60.
More than 60.
And why do you think that?
Because otherwise, I wouldn't ask you.
They relied on the heuristic of recognition.
Right, yeah.
And trust that a lot, too.
You're right.
Or this is when you give a talk with two people
who have already your work.
OK.
The point is, many of the Germans
have never heard of Milwaukee.
And the recognition heuristic tells you,
choose the option you recognize.
And they were right.
Now, let me test you the other way around.
OK, ready?
Which German city has more inhabitants?
Bielefeld or Hannover?
Who is for Bielefeld?
You.
Who is for Hannover?
You got it right, yeah?
I think the two suspected that I want to fool them.
You shouldn't think too much.
If I asked the same question to the Germans,
many of them get it wrong.
Because they had heard of both.
And they need to retrieve knowledge about that.
See the point?
And we had just a less-is-more effect here.
You got more questions right about the German cities
than about America.
Although you know more about the Americans than the Germans.
So how does this work?
So here's the study of ecological rationality.
So think you have a test of 100 of these questions,
maybe the 100 largest cities in the US.
And consider the lowest coefficient.
If you haven't heard of any of the cities,
your performance is 50%.
Yeah, you just guess.
If you've heard of all of them on the other side,
it's also 50%.
Because you cannot use the requisition here.
We are seeing both cases that have no knowledge.
So now assume the person has knowledge.
And the knowledge is defined here.
It's defined as the number of cases
you get right in cases where you've heard of both.
Like before, it was 60%.
And actually, the curve here, the second curve,
is about knowledge relative to 60%.
Person B recognizes all American cities like you
and gets 60% right.
This is knowledge.
Person A recognizes only half of them,
can apply the recognition here and gets more right.
The curves are drawn for recognition validity of 0.8,
meaning that the number of correct inferences in all cases
where you have recognized one and not the other.
So you can measure all these things.
There is no parameter fitting game here.
And if the knowledge validity is at least as big as the recognition
validity, there is no lesses more effect in it.
So the lesses more effect you find in on this three curves
below, it means that on the right side of the curve,
performance goes down, despite you either no more
or recognize more.
So this is an illustration how we can model
and can actually make quantitative predictions.
And we have used this, for instance,
in predicting maybe more interesting things in cities,
unless you are on a million dollar question,
like you can how to predict the outcomes of all Wimbledon
gentleman's single games.
There are 128 contestants.
There are 127 games.
And you make a prediction for everyone.
And they can do this with the ADP rankings,
or with the Wimbledon seedings, or with ignorant people,
exactly partially ignorant, because fully ignorant cannot
use the heuristics.
And again, from here, you can see,
you need to find people that have heard about half of them,
and not if there was one.
And these were German amateur players.
And they made, in two studies, more correct predictions
than the ADP rankings or the Wimbledon experts.
And if you change that to those who recognize less,
then they will not make more predictions.
So this is just an example how one
can analyze the ecological rationality of a heuristic.
You can easily tell when the heuristic is
worthless, when the recognition validity is a change level,
then there is nothing.
And we also have a way to make earlier concepts
like availability into something precise that is useful
and also illustrates some of the amazing capabilities
of the human mind to exploit one's own ignorance.
Second example, fluency.
Fluency presumes recognition.
And it can be measured by the time
an option comes to your mind.
And the point I'm making with the next example
is that human intuition, using fluency,
creates no speed-accuracy trade-off.
What is the speed-accuracy trade-off?
That's usually something that, if you make judgments fast,
then you lose on accuracy.
That's the story about fast versus slow thinking fast
versus slow and so on.
A speed-accuracy trade-off, you get them
if you test undergraduates with problems I've never heard.
But if you test experts, it's the opposite.
So here's the example.
The example is there are experienced handball players
and some of my postdocs put them in their uniform
with a ball in the hand in front of a video.
And the instruction was there will
be a top handball game, running for 10 seconds,
then it will be frozen.
And tell me immediately what the player with the ball should do.
So they looked at that.
It was frozen.
And I would say I'll pass to the left here
or shoot at the goal or something else.
Then they had 45 seconds more time
to study the still-frozen image carefully.
And they were instructed, if other options come to your mind,
please tell us.
And then one of them said, oh, I didn't
see the guy on the right side.
Passed.
That would be a good one.
And after all of this was done, the options
were plotted against the quality.
It's measured by the best coaches we have in the country.
And what you see is very interesting.
On average, the first option that comes to your mind
is the best one, the second is the second best,
the third is the end zone.
That means if an experienced player goes
with the first option, he or she is most likely right.
And we also found that when we asked them at the end,
after the 45 seconds they had studied that,
what would you do now about 40% change their mind
and got into a worse option?
So here we have a case where we have no speed accuracy trade-off.
In this country, it's better, on average,
to act on your first impulse.
In a game, you have to do this.
But in the experiment, you have the chance to think longer,
but it doesn't help.
It regularly hurts.
Remember, this only works for experts.
If you have amateur players, the curves
are not going down, they're just stator.
But this has an important insight.
So intuition, as I define it, is based on years of experience.
What you want to do comes quickly to your mind,
and you don't know why.
And the fluency heuristic is an explanation how this works.
And the reason why the research has come to the conclusion
that what the general speed accuracy trade-off is mainly
because one studies undergraduates with problems
that I've never seen before.
Gary Klein, who studies fire furthers,
finds the same results.
So the third and last category I want to give you
is an interesting one, where it is about the human capacity
to order things in importance.
And these are heuristics that are based,
that base the decision on a single cue.
And let's give you an example about hiring.
When Elon Musk was still young, and Tesla was young,
Elon Musk did the hiring himself.
And he reported how he did it.
He did not get an assessment center.
He did not study CVs.
But he, according to himself, he relied only on a single cue.
Does the person possess some exceptional ability,
like a banjo player?
And if not, no hire, if yes, hire.
Now you might wonder why not more cues.
But look, exceptional ability is something
that has some kind of redundancy with other things.
So a person who has this to get there
needs to know how to persevere, how to concentrate,
how to sweat, and not give up.
And so that's the spirit behind that.
It also illustrates that these heuristics
are adapted to certain problems that will not
work well when the company, like Tesla at Chrome.
Because then you need people who do routine work.
One needs to adapt that.
But that's the idea in the adaptive toolbox.
Now, here is another hiring heuristic,
and that's we call a fast and frugal tree.
A fast and frugal tree is an incomplete tree
that has an exit on each of the questions or cues.
So if the number of cues or questions are n,
it has n plus 1 exits, while a complete tree
is in the case of dichotomous 2 to the n.
So when Abbason was young, a chef basis also
reported about how he did make the description.
And we reconstructed that in the form of a fast and frugal tree.
So note it's sequential.
It doesn't integrate as rational theories, usually assume.
Interestingly, this first question was the same.
So does the person have an exceptional ability?
But if yes, it wasn't enough.
Second question, will you admire this person,
which is a quite unusual question?
So explain that if he, Bezos, admires a person,
he will learn from that person.
But that was also not enough.
And only if that person raises the average level
in the unit where he or she will be, that's being hired.
So the properties of these trees are
of a lexicographic nature.
Lexicographic strategies cannot be mapped into a utility,
into a smooth utility function.
And that's why Classity books, like Luz and Rifar,
have always ignored them and looked down at them.
But people use these strategies.
And we have shown that there are certain situations
where a fast and frugal tree can outperform random forests.
And the interesting question is, again,
can we prove this situation?
Can we identify them where that works?
So I'll show you one way about how to model such a tree.
If you use these three questions in the order,
how many trees are possible by changing the exits?
Sorry.
Four.
It's four.
You tell me later the other two.
Same in changing.
Yeah, the same, yeah?
With the same cues in the same order, yeah?
Otherwise, it would be more.
Basis tree is on the left side.
So any classification can make one of two errors,
like making an offer to a person whom you should not
have made an offer.
That's a false positive.
Or not making an offer to a person whom you should.
That's a miss.
The basis tree minimizes false positives.
There?
He only makes an offer after long.
These four trees do a balance between false positives
and misses.
On the right side, you have a tree
that maximizes false positives relative to misses.
And in the middle, you have very interesting trees
that have a zigzag structure.
An example for a tree on the right side,
I'll show you from our work with the Bank of England.
So the Bank of England has a problem
how to identify vulnerable banks.
How do you do that?
Now, the classical method is in the so-called basal or basal
two and three programs.
And where the banks calculate a value at risk.
And if you've ever done that, so if you already
see over a large bank, you have to estimate
thousands of risk factors.
That means a covariance matrix in the order of millions.
Good luck.
And you can't use more than 5, 10 years of data
because before something strange would happen.
These calculations are impressive.
They have prevented, they've never prevented any crisis
and probably will never prevent any crisis.
It's another Turkey illusion.
It's stated probability theory applied
to a situation of uncertainty.
I once gave a talk to the European Central Bank
and said the calculation that you do
border on astrology.
And I was waiting for someone to say, no.
The answer was yes, but what else should we do?
Here is what else could be done and systematically be studied.
So this is now a fast and frugal treat.
That's the same structure as the one that's seen before.
And it asks just three questions.
For instance, leverage ratio below a certain percentage.
And if it's lower, red flag.
If not, it goes on and asks two more questions.
I'm not going into the details here,
but you see the structure.
It's important that it works very different
from standard rational models.
So for instance, the Swiss bank, UBS,
had failed before the crisis on the first item.
So the leverage ratio was much less than half of that.
That would have identified it immediately.
UBS did marvelous on the other criteria.
But the trees are non-compensatory.
It's like the human body.
If your heart fails, a good lung doesn't help you.
You can't compensate that.
So it's a very different model.
And the reason why these type of strategies
cannot be mapped into a smooth utility function,
who cares about that?
It has to work.
And then to follow some mathematical doctrine.
Again, importantly, it reflects sequential thinking
and has a few other advantages.
Bankers can understand that.
And also, the banks, the regulators,
are more safe that the banks cannot game the system.
If you have to estimate millions of covariances,
you have plenty of room to game.
Here, it's much easier.
It's much more difficult to game the system.
So can I just ask you something?
I'm confused.
So this is a risk-minimizing structure.
These can be calculated based on known states.
Earlier, you started by talking about heuristics
with being solutions to the uncertainty problem.
This looks like a risk problem.
The uncertainty is in what banks are doing.
So the model is a heuristic.
It's not an optimizing model.
And your right, it goes, it's the rightmost tree.
It tries to minimize the misses.
You want not to overseas.
And the costs are false alarms.
That's what the decision is.
How we build the trees is a mixture between,
so the economists at the Bank of England,
they identify the variables.
On base, and we check them done, and empirical data.
So for instance, the cutoff points.
It's all empirical data.
We deliver the structure.
Now, don't put this in a regression.
Think about this structure.
That's the way this is being done.
And then it needs to be tested how well it works.
The advantage is transparency.
That it can easily be understood,
and it can be changed in cases there is a problem.
And it's an alternative that can be systematically studied.
The state is still outside of the system, or the framework.
Whereas when you had the opening definition of risk and uncertainty,
there's something small about these states.
It's a good question.
So the world of finance here has incredibly
and also unknown uncertainties.
So the numbers that the Bank of England gets from the banks,
you can count that they're not the real numbers.
They're polished.
And the other source of uncertainty
is in what's actually happening in the financial systems.
And there could be another Russian default or something.
This is the uncertainty I mean.
And this is a model how to deal with that uncertainty.
And there's no claim about optimization or maximization
for anything.
It's just an attempt to do better than what one has.
Here is a book we've done, which is
how to build these trees from empirical data.
And what's the alternative in case that's of interest.
But the uncertainty in the financial system is enormous.
Is that a little bit?
So I was sympathetic to your critique
of the dichotomizing approach that you've adopted
it yourself in the heuristics versus the optimizations.
And is it possible actually, given the observation
we just made, in fact, what you're
calling a heuristic is just on the spectrum
towards full optimization.
I mean, if you do Bayesian, Bayesian
is usually an optimization model,
then you start with uniform priors.
That's a heuristic.
You're totally right.
It is just make it simple.
But it's very clear that one should pay attention,
I think, to the heuristics.
Yeah?
There's a bit of a category error here.
You actually mentioned ROC curves.
The whole point of ROC curves is which
are the middle of standard machine learning,
that they are an alternative.
You don't come up with one.
You set your own level on an ROC curve.
And you're simply saying, go on ROC curve
in a way that you're less likely to screw up.
Sure.
That makes perfect sense.
But I don't think it's really got
to do with optimization versus heuristics.
Just using an ROC curve over, and the points that David
is making in the fellow over there,
is it's a very high level coarse-grained space,
as opposed to these banks are being stupid
and are using a fine-grained space
where you can't measure the variables.
But the point is, it's an ROC curve.
Doesn't give you a single answer.
You're putting in that I don't like the chance of screwing up.
Therefore, I'm going to set my threshold in the ROC curve
way over to this side.
OK.
What you picked this point here, the one-game map,
the fast and frugal trees, onto an ROC curve, onto four points.
And that's a direct connection between an optimization model.
Then the question is, whether the optimization model helps
you for a certain problem.
The name and Pearson theory, which is below the foundation
of the ROC curves, makes a number of assumptions
about the distributions, which are not necessary here.
And I see a point here.
And also, it's difficult for the ROC curves
to deal with more than one variable.
Can be done, gets very complex.
Here, it's relatively easy to deal with them.
So there are differences between these two curves.
But you also write, and we showed that in this paper here,
that the heuristic can be mapped into an optimization model.
So I'll end with a few remarks about ecological rationality.
So the idea of ecological rationality
is from Herbert Simon.
And that's a famous quote from him, where he says,
this is his analogy.
It basically says, in order to evaluate
the rationality of behavior, you need
to look both at the cognition, the strategies,
and can be heuristics, anything else, and the environment,
and how they match.
It also means the standard definition of rationality
is following the laws, say, axioms of consistency
or maximization of expected utility.
It's an internal definition.
It doesn't take account how these things work
in the world outside.
And for instance, that part of behavioral economics
who has adopted the heuristics and biases program
uses a single internal definition.
It is to be a law of logic, of probability.
And people are measured against that.
And what I'm arguing, we need to measure behavior
against the real world, against the measure heuristic
against the real world or their success.
What I'll show you now at the end is just a simple and intuitive
answer to the question, I've shown you a few heuristics
that just rely on one reason.
Can we identify the conditions under which relying on one
reason cannot be beaten by a linear equation
that has more reasons, including the one reason?
So do you have an idea?
I'm just asking you before because afterwards it's
so evident that we knew it all along.
To be clear, it's a subset of that larger space, right?
Yeah, it's a subset.
So think about it, a binary decision, higher or not higher.
There is Elon Musk's one reason.
And the question is, under what conditions
will relying on this one reason always
lead to the same results that a linear model that includes
many valid reasons, the old valid, including the one?
Clear?
It's a subset, yeah?
Here's a simple condition, and it's
if there is a dominant queue.
But it means that the weights of binary queues
form a dominant queue structure.
And so the weights are like regression weights.
It's the additional contribution to the first one.
So if the weight of the first one
is larger than the sum of all the others,
you can intuitively see it will not get to any other decision.
And then the next question is, how often does this happen?
Do we have to look at machine learning data sets?
And it happens quite often, astonishingly often.
So this is not the only condition.
It's just a sufficient conditions.
And we've looked at three conditions.
And the median value is that in 90% of all comparisons
in the data set, this condition holds.
So if you take half of the data sets, in 90% or more,
it holds, and the others not.
And then the next question is, what are the other conditions?
And I'm not going into this at this moment,
but end with a more general view that's probably
known to many of us.
So this is a standard justification
for why people use heuristics.
It's an accuracy effort trade-off.
So people are a little bit lazy, or don't take the effort.
And they pay for that in accuracy.
That you will find in the Kahneman book,
or in justifying by biases, need to be eliminated,
or in the Kahneman-Siperni and Sunstein book, by noses.
So we have read a very interesting change
between two David's here and the Kahneman-Siperni
and where David Wolpert and David Krocker point out really
that noise has often a function.
And we should not try to eliminate that.
Your examples were mainly from not about the examples
that Kahneman has in mind, but they apply equally
to the key topics of the noise book, which is sentencing.
So how much, what's the punishment for a certain crime?
And also pricing annuities.
And it is not that sentencing is like Bull's Eye,
where there is a right answer.
And different experts with different opinions
should converge on one.
You can have many things.
You want to punish the person.
You want to be low on things and give the person a chance.
You can have many reasons.
And it's also an issue where variability is a motor
about change in societies.
And so, again, both of these views
assume that there is a truce that is singular, that is known,
and the biases, the difference to the truce
and also the variability on that should all be canceled.
And that's the key idea behind that.
And the answer, this is not the case.
We have a bias variance trade-off,
and where we have not only a bias, but also variance plus noise.
It's illustrated here.
You have two darts.
And on the left side, the player throws systematically
too far to the right and low.
But the variance is, the variability is small.
On the right side, there is a player who has no bias.
Bias is zero, meaning that on average, the darts
are in the middle, but only on average.
Variability is high.
And we can see that the real idea is not
to reduce bias to zero as the typical messages,
but to find some reasonable compromise
between the trembling hand and the bias.
Jurisdicts, if they have zero free parameter,
like 1 over n, have a bias, like on the left side,
but no error due to variance.
They always hit the same point in this image.
And that's a way to understand why simple can help.
And it's also a way to understand
when simple does not help.
For instance, in that thing, if you
have really large amounts of data and the world is stable,
then the simple solution will have too much bias,
because the amount of data will reduce variance.
Can you say a little bit about bias variance,
if I just go from bias and variance or expectation
values over both your decision algorithm
and also the real world?
If I go 1 over n and the real world is varying,
I can have a very high variance.
I'm not sure in what sense.
No, 1 over n gives you, it doesn't do estimations.
So if your algorithm doesn't estimate anything,
it cannot make estimation errors.
We can talk afterwards.
OK, let's do this.
From now it's statistics.
Though I'm happy to talk about that, but in this image,
the variability is due to different samples
on which the same algorithm estimates its parameters.
And if you change the samples, you get different ones.
In particular, if the samples are small,
the variability is higher.
And that's the idea behind it.
The bias variance dilemma, and that may also
behind your question, applies to situations
that are situations of risk.
We only have to estimate the probabilities.
It's assumed to stable.
In this sense, it's just an analogy to understand
what's why the heuristics when they fail and when they work.
Let me finish with three methodology principles.
It is very important to have an algorithmic model of heuristics,
not labels, not system one, and other things
that we do not know what they predict.
Second, it is very important to do competitive testing,
like to test a heuristic model against maybe some machine
learning model.
We have still parts in foreign economics,
but it's very little competitive testing.
At best, you eliminate a factor from your regression.
But really, to test against a different class of models
will rarely be done.
And particularly, it's important to learn
whether a very highly complicated algorithm really pays.
And we often have a complexity illusion.
And think that something that is simple is nothing worse.
For instance, Harry Markowitz would have not
gotten an economics Nobel Prize for one over N.
And finally, it's also important to do not only out-of-sample
prediction as it is regularly done in machine learning,
but also out-of-population prediction.
So the studies that I reported at the beginning of investment,
where one over N is very simple.
They are not out-of-sample, but they
are real forecasting in the future.
And the mean variance model or the Bayesian models in,
they are not, their parameters are not done by out-of-sample.
Because out-of-sample creates a stable world,
a fairly stable world.
But they are estimated in the first time interval.
And then they're predicted in the next one.
And as we know from certain machine learning,
techniques that prove very successful in diagnosing
may are lung diseases.
When they were used in a different hospital, they failed.
And that's a problem with out-of-sample prediction.
And predicting, doing the prediction in a different hospital,
then is out-of-population.
So I was talking today about three misconceptions.
Complex models are always better than simple heuristics.
They are sometimes better, but not always.
Slow thinking is always more accurate than fast,
intuitive decision-making.
No, particularly not by experts.
Even chess masters, when the best international masters
have been studied, for instance, when they have time constraints,
they do about as well as without time constraints.
If you have excellent chess players, but not at this level,
then the difference gets bigger.
And finally, it is not correct that people
rely on heuristics because they're biased, lazy, or irrational,
as the typical message still is.
So I invited you today to a voyage into our studies
on homo heuristic use.
And I've made three points.
Risk is not the same as uncertainty,
although often the term uncertainty is used for risk.
For instance, most of behavioral economics,
talks about uncertainty, but it's old problems of risk.
And you detect the problem of risk immediately
because the reason to know what's right in search.
And the uncertainty you bring from data.
Second, logically rationality, the consistency axioms,
the expected utility maximizations,
not the same as the ecological rationality.
The logical rationality is much harder to model,
to analyze, and it's about the adaptation to the real world.
And in, under uncertainty, less can be more.
And one way to understand that is it's not an accuracy effort
trade-off, but a bias variance trade-off.
And finally, human intelligence evolved
to deal with situations of uncertainty.
And so did animal intelligence, not with risk.
And one of the reason is that we are not very good at calculating.
Let's just show a start here.
And also, some people think we need to get rid of uncertainty.
I've often heard from well-known economists.
So when Reinhard Selten had his 80s birthday,
I gave one or two keynotes and another economist
who is a Nobel laureate afterwards came to me and said,
interesting talk, but I don't like uncertainty.
That's a matter of taste, but just imagine if we would know everything,
if we would live in a world where every possible situation that can occur,
the consequence that everything is known, and all is just about the probabilities
which were all the uncertainty strain.
Would that be an interesting world to live?
It would be, as Frank Knight has pointed out,
would be no innovation, no profit.
And there would be no fun.
And life would be like in the movie where you wake up in the morning
and everything goes back.
Or, as Dostoevsky said, if everything would be rational, nothing would happen.
Thank you for your attention.
Thank you, Gerd.
We are actually at time, so who needs to go and go?
And otherwise, if you have any questions, maybe you can take them.
What should I do?
Take your own questions, if you like.
No, no, you take them.
OK.
All right.
Good stuff.
Good stuff.
Thank you.
Really, of course, it would be fun if that really was it.
So, I'm wondering, actually, so you've got these characteristics,
and you showed, like, the message of the different ones.
And I'm thinking, in any one situation, you can apply a multitude of characteristics.
So, my question is, is there any computational principle by which you decide
which heuristic is allocated control of behavior?
Because they are operating in parallel, where subsets of them are operating.
They may conflict in terms of the final behavior or choice that's exerted.
So, is there any framework for deciding the allocation of control
of which heuristic gets to control?
This is a very good question.
So, there are two or three models about how to model that.
One is reinforcement learning, so that you learn by, you learn a hierarchy of heuristics,
like imitate or do something else.
And depending on the reinforcement and the ego, and that makes the order.
That is, there's a paper by Reese Compton Otto on that, showing that an experimental paper.
The selection problem is not really being solved here.
And you might think about a heuristic that decides about the application of heuristic.
We may just do reinforcement learning or something else.
Unfortunately, I won't be able to meet with you if I had time to meet with you.
This is what I would ask.
So, in the mathematics and computer science of risk assessment in criminal justice, for instance, they're very similar debates and you may know of a computer scientist named Cynthia Rudin argues that we should use simple decision trends.
Rather than more sophisticated algorithms.
And her argument is partly because partly that more sophisticated algorithms in that domain are are typically only marginally better in accuracy than this than the simple decision trees.
Also, as you referred to, in that you made there's a demand for transparency. If your government is going to put you in jail, you deserve a very simple, clear explanation of why.
And so I guess I wanted to ask you.
I think with some of the other commenters that in some sense what you're talking about is simply a different space of models that can be optimized. I mean, with, you know, you can choose the structure of the tree.
Their hands and orders, whether they're, you know, the order of the questions in the banking example there were parameters that I assume had to be set in some way.
The threshold parameters.
But so I guess my question is when, when do you think that in what domains you think there is a strong argument for simplicity and transparency, even as something more complicated is somewhat more accurate.
And by the way, I also agree with you that that ladder plane is often made when it is not true.
Yeah, but but in the cases when it is true sometimes we still prefer the simpler thing.
So I agree with Cynthia routine. And she and others we have shown that in sentencing or bail decisions, a very simple algorithm that look at three various typically like age and previous sentences are about as good as complicated things.
So the, or often black box like compass and where nobody knows exactly what's in it and what's not.
And also that these are situations where transparency is another important thing.
And in, in the European Union at this moment, there's a decision being made about the AI.
And the AI is in part about ask for transparency of algorithms that are so called high risk algorithm and sentences is one of these examples.
And the EU union, if they don't change now things are voting about it will not allow black box algorithm.
And I've worked with the German shoe farm, which is like the American people.
So credit rating, they have immensely complex algorithms.
If you make them simple.
Simple algorithms do as well.
And so in, in many of, so your question is, when can we expect that complex AI will work and not.
It's very similar to the distinction between risk and answer.
If a situation is stable and well defined.
AI is much better than human.
So like classical example or just go.
If a situation is highly uncertain, like, how can you predict whether bail decisions about whether the guy will actually come back when they.
That's very difficult.
So many factors, uncertainty.
In these cases, we have little evidence that say deep neural networks would do consider better than humans.
But we know that it's black box.
Right. I guess my question is, in some cases, we want, we should use the simpler thing, even if the more complicated is more accurate.
For other reasons.
Because maximizing accuracy is not the only thing that matters.
Right. And also, so for instance, in my, in our work with the shoe farm, so this is the German fecal.
It turns out that they're using variables that I better not telling anyone.
Just to get a Yota male accuracy.
For instance, when you ask for your greatest score, your greatest score goes down.
Yes.
You don't want to tell the speed.
Right.
So it is not acceptable.
But it helps to predict.
So the compromise will be get rid of these aggressions.
Make them simpler.
And you may lose an Yota.
In prediction.
But there is more than just predicting accuracy.
There are human rights about transparency.
So.
And this is really, I have a slightly provocative question that it sort of hints at doing.
You like the logical rationality is not equal to ecological rationality.
I don't mean this just semantically, but aren't you letting people a bit too likely with this distinction.
There must be something wrong with logical rationality if it leads you to ecologically irrational decisions.
I feel that our program looks at logical rationality and asks, come on, this is, there is something wrong with this because it doesn't basically doesn't work in the real world.
Yes.
Are you being, are you being diplomatic by making this distinction?
Do you really mean there's something wrong with logical rationality and we should really think rationality is a different thing.
Yes.
So logic is a system that's very useful, but not for everything.
That's the point.
So you wouldn't even understand language.
So we can't in one of Tversky ask people about Linda, the bank teller, what is more probable.
The term probable does not map into mathematical probability.
You know, it is, if you look in the OECD, it says, is there evidence?
No, there's no evidence that she's a bank teller.
Does it make sense?
No.
So to impose logical rationality on everything, that's me.
This is no good.
And it, and it mistakes a human intelligence.
We make an inference from the content.
What do these problems terms mean?
Similarly, Linda is a bank teller and a feminist to an end.
In everyday language, the end means sometimes logically ends, sometimes something different.
And we have an amazing intuitive capability to infer it.
For instance, when I say this evening, I invite friends and colleagues.
You don't think it's the intersection.
It's the logical or.
So these are the real part, the real interesting questions how does the human mind make this stunning inferences rather than declare it's an error.
It's conjunction fallacy.
Although what you're saying there is about language, not about actions.
This is about language.
I have one example that is becoming a favorite of mine.
We can show that a.
So this is total world logical rationality if you like someone who maximizes expected utility does not maximize utility over time.
Utility is a catastrophe for the in the logical structure that you're building a theory where you say, well, we can't really tell you should do until you tell me what it is that you really want.
And that is your utility. And then we give you a protocol.
Now I start behaving according to this protocol.
And I don't get the thing that I asked for.
So I don't maximize my over time because we're using the wrong, you know, what happens over time is not what happens in expectation.
We have real problems, logical problems within logical rationality.
It's about taking the expectation in the logarithm instead, as with fitness.
It depends on what you just depends on what your dynamics, but yeah, I mean, that's an example.
There are a number of papers by biologists who come to a similar conclusion that if animals would maximize something, it's always what you maximize, you know, then they actually lose on fitness.
Yes.
Thank you again. That was great.
You can leave it to informal discussions and

Hello and welcome to The Cognitive Revolution, where we interview visionary researchers,
entrepreneurs, and builders working on the frontier of artificial intelligence.
Each week we'll explore their revolutionary ideas and together we'll build a picture of how AI
technology will transform work, life, and society in the coming years. I'm Nathan LaBenz,
joined by my co-host Eric Torenberg. Hello and welcome back to The Cognitive Revolution.
Today, after traveling to California for Google's IO event and taking some time to process what
has been in all honesty quite a confusing week, I'm back home and excited to share a timely
conversation with Svi Moshowitz. While many excellent news roundups have been published
over the last few days, including by Svi on his blog, here we aim to provide a higher level of
analysis. We start by discussing the new capabilities that excite us most from a mundane utility
perspective, including the native multimodality, lower latency, lower prices, and deeper integrations
into platforms that were all previewed this week. And then we reflect on the fact that
while the technical advances are indeed very impressive, most of the products aren't actually
available for us to use, and even many of the demos seem quite rushed. And we consider what
that might imply. From there, we attempt to get a handle on the competitive and strategic dynamics
at play, as the big tech incumbents compete not only with one another, but also with younger,
scrappier, and less constrained startups. We assess whether the apparent convergence of the
frontier developer's retail and API products may create a winner-take-all arms race dynamic,
and we analyze whether more friend-like AI products could mitigate that concern.
We also debate whether the returns to scale in Foundation Model Development
mean that the so-called big tech singularity may be near.
Where I tend to emphasize the big tech company's structural advantages and find myself,
frankly, bearish on the majority of startups right now, Svi tends to place more emphasis on
the cultural, bureaucratic, and regulatory barriers to big tech growth and is more bullish
on the challengers. I think the contrast between our perspectives here is quite interesting,
and I hope you find it useful. Finally, we address arguably the biggest news of the week,
the string of departures from OpenAI's safety team, including what it almost certainly does
not imply, what it in fact might, and what the departed safety researchers and safety community
should do now. We recorded this on a Friday morning, and it was actually during our recording
that Jan Leica posted a thread explaining his reasons for leaving OpenAI. And while we didn't
have a chance to discuss his post specifically, I do think our overall analysis holds up in light
of his statement. At a high level, the only thing that's crystal clear at this point is that the
situation is increasingly complicated and moving so quickly that we should all be really honest
about our fundamental uncertainty and remain open to changing our minds on key questions.
I for one, certainly am. As always, if you're finding value in the show, we'd appreciate it if
you'd take a moment to share it with friends. And if you think we missed anything important
or got anything wrong, we invite you to share your feedback via our website, cognitiverevolution.ai,
or by DMing me on your favorite social network. Now, I hope you enjoy this timely analysis
of another intense week in AI with Zvi Moshowitz. Zvi Moshowitz, welcome back to the cognitive
revolution. It's always a busy week when you're here, and I don't think that's a coincidence.
We've got a lot to cover. I'm very confident it's not a coincidence. I thought we would maybe organize
this sort of as you do your blog posts, start with kind of the mundane utility and what we're
excited about, and then get into maybe what we're not so excited about, and then maybe get into
what we're worried about. And I think we've got some good topics for each of those sections this
week. How's that sound? Oh, yeah. Yeah, that's definitely the right things to do. And there's
definitely coverage all around. Cool. Oh, we're not cool, depending on the section we're talking
about. Let's start off with then obviously a busy week with open AI making some launches,
Google making some launches slash announcements, I guess both of them really making launches slash
announcements, which is one of the bits of analysis that I want to get into. But I guess for starters,
what stood out to you as the new capabilities that are most exciting to you as somebody in
search of an incrementally better life? Yeah, so there's always a difference between
what they're promising you're going to be able to do in the future,
and what you can actually touch now, we can use now. So we can use now is GPT40. And that is
that is half the price if you're using the API. And that is multimodal in various ways. It seems
like a great product. It had its weaknesses that are becoming apparent as I use it. But it's already
pretty exciting. And then when it becomes fully multimodal, it starts to play into the
modalities that they're talking about where you talk to the AI and talk back and here's your
tone of voice. And here's the cadence of your expressions and it sees your face and all that
stuff. That looks pretty exciting in the relatively near term. They said a few weeks for that. But
I do think this is bearing the lead compared to all of the integrations that both companies are
promising. That to me is the big exciting thing if they can be delivered. It's what about these
universal assistance? Because the demos they actually gave us were lame as hell on these
capabilities. Can you recognize what I'm seeing in this picture? Can, okay, yeah, I already know
you know how to do that. Why is this exciting? Did you remember what was happening a minute ago?
Yeah, you can. Okay, yeah, again, I knew that. And with that over and over again, these demos of
educational videos with trivial exercises where the kid is being intentionally dense and
already knows the answers and then is doing a good job of play acting, being actually lost.
And then the AI doesn't pick up on it and just tends to mumble through. You have a bunch of
translations of very trivial statements that, yeah, I'm not saying I could translate them without
knowing the languages, but it's closed and so on. And those are some open AI demos, but Google's
demos were similar, basically, in these areas. But then we're talking about the things where it's
okay, search my inbox for all of my receipts, put them into a spreadsheet, organize all of my
expenses and all of the things that have been done by category. So all of this automatically
forever, like I told you to, on a background continuous basis. Now I'm more interested.
The context of integrating all of this information, open AI thinking it's going to
live on your phone. Google thinking it's going to live on your phone. And it's shaping up potentially
to be another Android versus iPhone battle, right? Because Google is going to own Android
by the fall. And it's looking at it like a deal of apple.
Yeah, it does. I feel like the there's a really interesting point there on the demos
and just how sort of slapdash some of them were the open AI ones, I think they're trying to make
a point on some level of this is you can tell it's very real and like definitely not pre-recorded
and staged, although you could always still fake it, but I believe that their demos were
real in real time. And there were a couple of things there that that they I came back with
where it was like, yeah, I didn't think that was maybe exactly what they were hoping it was going
to say. But they're at least showing off that this is real now. And we have the confidence to do it
live on stage for you. But across both companies, it did feel like the demos were a little bit of
an afterthought. And I wonder what you make of that. Is that like a me could be like,
it's a reflection of the sort of ideological motivation behind the development in a sense
where they're like pushing the capabilities and then figuring we'll figure out what to do with
it after, which has obviously been something Sam Mullen has said explicitly in various ways over
time. But it's still a little striking to me that they were so okay, we just made this thing now,
hey, can anybody come up with something to do with it on stage? We got to be on stage in 10 minutes,
it felt oh yeah, well, Mira speaks Italian, let's translate. That is cool. But it did feel rushed.
And I guess the other answer would just be like, they're rushing their racing. And so
they're taking these things out of the oven and putting them right on stage without much time to
really figure out what to do. It suggests that these capabilities are needed. They haven't had them
for very long. It also suggests to their credit that they're not optimizing the demo. They're not
putting lots and lots of effort into putting forward the best demo possible. If these were
startups, these were scrapping little companies trying to use this to raise money, you would have
seen dramatically different demos that would have looked dramatically more impressive for both
companies. But they deliberately chose not to put in that effort. So you don't have the same effect of
I am very confident this is the best possible thing you could have shown me, right? I am very
confident that if Google or OpenAI had passed a bunch of people with okay, take the day, take the
week, figure out something much more impressive, try it 20 times, confirm that it always works,
get it ready. I think they would have gotten something that would have wowed us a lot more.
I have to assume these were hastily assembled, not well thought out, like not robustly tested
demos. These were closer to real than you would normally see, but it still speaks to
they didn't try to make this interesting in some important senses. But also that could be a lot of
like who are you talking to? Like you're talking to the person who doesn't even know what
duty four is. They're used to three and a half as like what it is, and they barely even spent
five minutes on it. And they have no idea what's going on. And then the horse can talk it off.
And that's actually really impressive because until a year or two ago, the horses didn't talk.
Yeah, I have a few candidates for the things that excite me most. You hit on one already, which is
the integration into the whole platform for Google. That's obviously always been where they're going.
And with open AI, it's kind of more suddenly it seems where they're going, although I'm sure
they've been planning for that for some time as well. But all of a sudden you get the desktop app,
now you can connect your Google Drive or at least again, that's coming soon or in some phase of
rollout. That does seem super exciting as our friend or let's say inspirational figure,
Tyler Cowan says context is that which is scarce. And that has definitely been a feeling I've had
with language models all the time. It's both traditionally that earlier language models
that might have meant you just only have so many tokens in the context window. Obviously,
that problem has been largely solved with Google going from now one to two million tokens that
they're bringing online. That's a lot of tokens and fit a lot in that. But now you still have to
figure out, well, how am I going to get those tokens? What tokens am I going to get? If you're
copying and pasting a million tokens around to do a single use case, like now you've got a lot of
friction there as well. So plugging into these different sources, whether it's your Google Drive
or your Gmail or whatever, that seems like it is going to be a tremendous driver of value.
They always talked about these giant context windows and they're very, very useful when you
know you need them. And you're sitting down to analyze a bunch of data or search for a bunch of
data, but it is much slower. It is, it's actually more expensive for token to do it. And what we're
seeing is Gemini Flash, right? We're seeing things that are optimized for speed, things that are
optimized for cheap and things that are optimized to run locally and just do all of these things
in a way that the customer wants. And Google is also giving us two million context window,
which is very useful. But how do you combine these things? And a lot of the key is going to be like,
how do you go through all that context to figure out what the right context is? I always thought
that Tyler Cowan saying context is that which is scarce is sort of saying two things at once.
It's saying the whole like, you don't have enough context, right? But you're always short on context.
But also sort of a definitional thing, sort of whatever it is that is scarce is the context.
And so in some sense, once these details of your situation become background common knowledge,
that's not context anymore. In that sense, right? That's just your life. Like, I don't think about
that as context when making my decision. So I don't think, well, in context, we like French toast,
like, no, it's me just like French toast. I can just de facto about me that currently these
companies don't know. But presumably these companies would know when I'm destroying
absolutely everything and they're taking an input. So other things that I think are candidates for
the most impactful aspects of these launches, one is the kind of freeness of it all. Open AI
moving to make the latest model free for everybody is a big deal. And Google seems to be doing
something similar. It's not entirely clear exactly what they're going to be rolling out at what
price point in what product they have the Google advanced, which is not free, which has been required
to get you the most advanced models to date. But it's unclear if that will be required as a
subscription to get the Gmail assistant that's coming or whatever. So we've got some clarification
questions, I think still for them. But this does seem like a moment where obviously open AI has
almost the sort of Kleenex brand in AI thus far. And most people who have tried something and weren't
super impressed with it tried chat GPT with a 3.5 default. Now they are going to get a major step
up. How do you think that will change the landscape, the world of work in the immediate term,
just on the basis of like, now everybody can use the best model?
Yeah, trivial inconveniences like you have to pay a bit of money definitely hold back a lot of people.
And it's clear that from all sides, they're realizing that owning the customer is so much
more important than the trivial cost to compute for the lightweight customers using a bit of it.
Like chat GPT is pure profit for user like me, right? I'm not actually requiring that many tokens
per month. I'm paying $20 a month to have it. Because it's part of my job and I want to make
sure that I have a variety of different things to try against each other. But in practice, of
course, they're like, they paid $1 an inference in a month, right? 5% cost to provide the goods.
On the margin, that's shockingly high. So it's not very expensive to give these people the free
version, except in some of our people cancel their subscriptions because they no longer need
anything but the free version. I do think it's going to be an issue for something that they
count on that revenue, but I don't think they count on that revenue particularly. And of course,
they get the data every time someone puts something into there. And that's incredibly valuable.
So that's another way you own the customer. So yeah, the future, I think Google clearly is
indicating that the same way that like you could use Bing the entire time, you get access to GPT4,
but nobody bothered because it was a weird different interface and people didn't notice.
Google, I think, has realized that for most purposes, pro 1.5 is in fact the appropriate model
for most use cases that people want to do. They don't need what Gemini Advanced is offering,
but they will offer the upgrade to Gemini Advanced to people who want to pay it,
and most people won't pay for it and most people won't need it. But the pro 1.5 is,
for most purposes, plenty good enough. It was when you're trying to compare pro 1.5 to Gemini
Advanced, it's not entirely obvious that Advanced is all that in some sense, like much better than
practical purposes for many of the things you want to do. Like pro 1.5 is very good at searching
documents, for example, which is the main thing that I still want Gemini for, exclusive to other
things, but I'm not integrating with other Google services. Hey, we'll continue our interview in a
moment after a word from our sponsors. AI might be the most important new computer technology ever.
It's storming every industry and literally billions of dollars are being invested. So buckle up.
The problem is that AI needs a lot of speed and processing power. So how do you compete
without costs spiraling out of control? It's time to upgrade to the next generation of the cloud,
Oracle Cloud Infrastructure, or OCI. OCI is a single platform for your infrastructure,
database, application development, and AI needs. OCI has four to eight times the bandwidth of other
clouds, offers one consistent price instead of variable regional pricing. And of course,
nobody does data better than Oracle. So now you can train your AI models at twice the speed and
less than half the cost of other clouds. If you want to do more and spend less like Uber,
eight by eight, and Databricks Mosaic, take a free test drive of OCI at oracle.com slash cognitive.
That's oracle.com slash cognitive oracle.com slash cognitive.
The Brave Search API brings affordable developer access to the Brave Search Index,
an independent index of the web with over 20 billion web pages.
So what makes the Brave Search Index stand out? One, it's entirely independent and built from
scratch. That means no big tech biases or extortionate prices. Two, it's built on real page
visits from actual humans, collected anonymously, of course, which filters out tons of junk data.
And three, the index is refreshed with tens of millions of pages daily. So it always has
accurate up to date information. The Brave Search API can be used to assemble a data set to train
your AI models and help with retrieval augmentation at the time of inference, all while remaining
affordable with developer first pricing. Integrating the Brave Search API into your workflow translates
to more ethical data sourcing and more human representative data sets. Try the Brave Search
API for free for up to 2,000 queries per month at Brave.com slash API.
I think the voice and the real time vision also definitely, this is where the demos were kind
of flat. I thought this was true, really in both cases. I mean, the open new I1s were maybe a little
better. At Google, I went out to the Google IO event and they had quite a production,
right? They have the amphitheater, which they don't actually own, I learned. But it felt like
they owned it for the couple of days that we were there because they had set up basically a whole
little city on this plot of land with massive, almost permanent, but still temporary structures
for all their different sections. Web and Android and AI, there's a whole AI building and you go
in there and they've got all these different AI demos and the one was Project Astra and you go
into a little booth and they have a camera mounted and it's looking down at a table and then it's
like, okay, here's another sort of bin, another table full of junk. Now you can pick a couple
pieces of junk, put them under the camera and talk to the AI about it. So you've got like a toy
dinosaur and you're like, what's this? And it says, it looks like a toy dinosaur. And you're
like, okay. And it was, again, it's kind of going back to this like, I'm not sure that they really
took this anywhere near how useful it could be. And it does feel like the sort of thing where
it's like, yeah, we made this. Now we're going to show it off somehow and we're leaving the rest
of the work to the community to do, which I have mixed feelings on that. I don't mind it in some
sense, it's an enabling technology. And so fine, I do feel like that those things should have been
a little bit better thought through or certainly could have been a little bit better thought through.
It does feel a little ideological in some ways. And I guess I just also want them to have a bigger,
better idea for like, what did it, where are they're trying to take us? We're all kind of,
everybody is, at least everybody in technology, right, is like, looking forward to these
announcements, trying to figure out what are they going to do? And how's it going to impact me? And
what can I build on it? Or is it a threat to me? And then they just give you this stuff. And it's
like, the homework isn't really done. I've been saying recently that the scarcest resource is a
positive vision for the future. And I just, I feel like I want more of the tangible vision for where
they are taking us than just sort of, here's a table full of junk, like you can put it under
the camera and like the thing will recognize it and you can talk about it. Even though it was
impressive, they had another like, Pictionary mode too, which was an impressive demonstration.
You saw some of these things in the Google, in the Gemini 1.5 Pro original announcement where
they showed like a hand drawing of a thing and said, where did this happen in the movie? And
it was able to cross reference this to a scene. That similar experience was on display. People
were like drawing little line drawings and saying, what movie does this represent? Somebody drew a
in the little demo group that I was in a Wilson volleyball with the handprint on it and asked
what movie this was. And it didn't get it at first. And then they drew an island around it and a tree
and a couple of waves. And it was like, Oh, I get it. It's cast away. And so it definitely is
a very impressive and quite capable technology. But it does feel like, you know, sort of the
electrical wiring has been created before we have any idea what we're going to plug into it.
And that is just a very strange dynamic to me. As you go back and read more, I wonder how much of
a vision folks like Edison had for what electricity was going to do. I mean, in that era, merely
lighting the room at night was a revolution. So probably didn't need to have too much more than
that. But I don't know, how do you feel about this sort of building they will come? Everybody
else can figure it out. I'm a parent. And so what this reminds me of more than anything else is
the kind of thing you do as a kid, you're like, do you know what this is? It's a toy dinosaur.
Was that the one that you asked them basic questions, you check their knowledge, you fill in
gaps, you see where they're strong and weak, you're building up muscle memory, you're building up
components, you know, they'll need those components to then do things that are more impressive to
synthesize them. The idea is, well, if you know what everything in the world is, if you can recognize
what people are gesturing at in various ways and do other things, this then allows you to do
things that actually matter later. And it's also just evidence of intelligence, evidence of
comprehension. And that's all they're trying to demonstrate there in some sense, like it's not
the final exam, it's not like the job, the job comes later. But yeah, these are not use cases,
these are not things you would actually do the fun little game to have a play fiction area and try
to guess the thing. It's okay. But man, like pretty big man, like, what is the vision of the future?
The vision of the future seems to be the universal assistant, right? The idea is that you tell it
what you want it to do, or what information you want to tell you or anything like that.
And it does what you want informed by all of this context, for the same way they're really smart,
like knowledgeable assistants who's gotten a lot of experience that he would do.
And the thought is, well, go from there, right? What do you do if a person go whatever you want,
right? Like, as far as they got.
So how do you, yeah, it feels, I mean, there was so much talk of her this week and leading up to
the events that I found that very striking. I haven't gone back and watched that movie in a while.
I don't know if you have or how fresh that story is in your mind. But would you read that movie as a
positive vision of the future? Would you, I've heard kind of seen takes across the spectrum from
people being very excited about it to people saying it's a dystopia and then others were like,
well, it seems like it's more in the middle. How would you read that story, first of all?
And then I'm interested too to hear, do you think this is where you personally are going to go?
Are we all going to be walking around with her in our ears? Metta's got their, we just heard
they've got an earbud that has a camera in it in development.
So that seems like an interesting form factor for the her future.
Anyway, how to start off with, how do you read that story?
So from what I can tell, right? Like it's not, it's good in the sense that it's not a dystopia
and it's not a utopia, right? It's not pretending that this is all one thing or another thing. It's
saying, here's a scenario. Here are some things that might happen. Here are some speculations
as to why, how some aspect of that might go. And it's up to you, the viewer, determine is
Phoenix better off in these ways or worse off in these ways? Is this a healthy or unhealthy way
to go about doing things? Is it good or bad for his personal growth? What is this the
Alliance Alternative? What is this due to society? And then ignoring the ending, because as usual,
the actual big picture like ASI safety takes are terrible. But if you just look at the concrete
mundane questions that it's raising, I think from what I can tell, it does a good job of
not judging in every sense. It's saying like, here are some things that we expect to happen.
And my reality is something like, if you're treating the AI like it's a person,
developing a relationship with it and getting emotionally invested in it. And especially
if you're treating it like you might treat Scarlett Johansson in some sense. And this is not
practice. This is not reps. This is not training. This is not experiments. This is like real for you
in that sense. It's not good. That's not good for you. And it's not good for society. If everyone
else is doing it too, it's going to make it much harder to get real connections with other people
in that sense. If you're using it as certain different thing, if you're using it as like a
source of advice, a source of information, a coach, a sounding board, like they're
by testing ground. There are ways you can use this that are good. They're obviously good. Using
it as like a tutor to teach you new things. It's obviously just great. Like, how could you possibly
think that's bad? But there are obvious ways that you should go, wait a minute. This is not healthy.
This is like taking our evolutionary instincts and then putting something on there that is
mimicking the thing that's not supposed to be the thing. And it's bad in the way that like
pornography is bad, right? It's not that you would never, ever use it necessarily. You would
want to ban this, but like too much of this is bad for you. Like you have to touch drafts.
And like the warning of her, right? Again, if you just don't worry about the ending too much,
like you get the completeness, the warning is that this can substitute for other things. And
then if you let that go too far, that can be bad for you. And the promise is that this is actually
really cool. It's like, you know, other ways and has a lot of innate utility. And the question is,
how do you balance that? And this is not a new problem to, yeah, we've had to deal with this with
a lot of new technologies like mobile phones, social media, or the latest ones. The one that I
keep coming back to more and more is television, actually, partly because like with social media
and phones, we have this current big debate as it's happening right now and it has to put
itself out. We haven't made the full adjustment of TV. We're at the end of the road, right? We
move on to the next thing. We've seen what it did. We've gone through generations like
we can look back and ask ourselves, what happened? How did we deal with it? What did this do to us?
And the more I think about it, I think that that situation where it's not like they made
all these warnings about how horrible this was going to be. And it turns out everybody was being
dumb. The situation in which everyone made these dire warnings about how horrible this was going
to be and they were just correct. And it didn't destroy our civilization. We're still here.
But the things all just happened. Like the impact on people that we were worried about,
they just happened. And we now live in the world that was always mad. And it turns out that was
like eminently survival. And we've gotten richer enough and better enough in other ways that it's
okay, mostly. But one thing you notice is that basically everywhere that Gaki Bitcoin is television
is now blow replaced with low-flow fertility. So these things can be extremely unhealthy.
What do we do about that? So how would you characterize the
impact of TV? I obviously heard the fertility point. I guess my general sense would be that
it has just given us such a ready solution for our boredom that there's like just fewer passion
projects, fewer people developing new quirky hobbies, fewer people just chasing random stuff
because it's just easy to kind of go there for a bit of entertainment. Is that your mainline
narrative as well? I think it ate massive numbers of hours in the day. And yeah, it made activation
energy so much more expensive to get. It made gumption much harder to have. It substituted
for a lot of other things that led to good places that built up and had increasing marginal returns
to devoting your time and energy. And it taught us to be how to retain those, right? Passive consumers
of information, just sit there. We consumed a massive amount of advertising. We forget now in
the internet age when we were pissed off that our 30-minute show has three minutes of highly
skippable advertising on our podcast. It used to be that your 30-minute show was 22 minutes ago.
It was a horrible ratio, like 25% ads. And now I watch a sporting event and I make sure to start
late if it's not a huge event because I can't bear the idea they're commercial breaks.
Like, this is so bad. But that's a correct reaction if you have the option. Why would you
want to endure that when you can just be slightly delayed? It's not super cool, is it? If it is,
then you're stuck. And also, the ads are great. But most of the time, the ads are terrible.
And it also like people built their lives around this. They built their schedules around this.
And they stopped planning things with people, like the whole bowling alone phenomenon. I think
people underestimate that how much it was just television. It was just, you know, there's a show
I can watch. There's a thing I can do. And so I think it sounds like a lot of work and requires
coordination and requires an activation energy. And I don't have to, right? I can just stay cool.
And now it's not flexi-clutch and chill or whatever any number of similar things are.
And I watch still today, at least in my television. People think it's over. It's not over. And if I
better opt for it, I think so. But I'm not at all convinced. The other thing is, you used to sit
alone with your thoughts. You just used to be willing to be bored. You used to like do all these
things. And now we have so many different things. It's not just the television. It's not the phone.
It's now, you know, you can listen to a podcast. You can listen to the music anytime, anywhere.
You can do all these other things. You can just scroll endlessly. And so it's been super charged.
And all these things matter. And all these things change us. And having an AI
at our fingertips all the time, again, even in this big sort of mundane utility world, right?
It's not particularly dangerous. It doesn't really transform everything. It's going to change a ton
of things. And it doesn't necessarily mean we're going to choose to do smart, high-level things
with it. One thing that's striking about all the character AI-style things out there is they're
freaking dumb. They don't, they spend a ton of time customizing the experience
so that you get the type of interaction you want. The kind of thing that's satisfying to people.
But they keep not investing in highly capable models to power those interactions. They're
ridiculously stupid so often. If you look at what people are reporting back or if you try them
out in my experience, like you'll be lucky. You're not just not getting 3GPD4. You're often
getting below three and a half. You're getting something that's abysmal.
Yeah, that was really striking in my testing of replica. And it's gotten significantly better,
but it is amazing how, I'll never forget the line from Eugenia, the CEO there, she said,
going back even before language models, they started this project with the idea of addressing
loneliness. And she said, we knew we couldn't make a bot that could talk, but we thought maybe we
could make one that would listen or could listen and could make you feel heard. And she was pretty
open about the fact that they used pretty simple tricks and she used the term, not my term, her
term, parlor tricks in the early days to just make people feel heard. And I'm mixed on that.
I feel like that's another one of these things where it's good for some, but it potentially
becomes a big problem when it gets to be too broadly, too good or too broadly adopted because
they have research out that shows that, and again, this is like done over a year ago, before
the current generation of language models, which they haven't even fully implemented,
last I checked, but already their product was helping people reduce suicidal ideation,
helping people who need reps, get reps, people by and large did report that they were more
socially engaged as a result of having this outlet or kind of practice ground, training ground,
whatever, however you want to conceptualize it. So it seems like it's like, even in that limited
form, it's good for this sort of pocket of people who are really struggling. But then I do wonder,
similarly to what you're saying, like, how does that change life if it becomes really good and
compelling to people who are not really struggling, but could do other things and maybe now don't do
other things so much. Hey, we'll continue our interview in a moment after a word from our sponsors.
Hey all, Eric Torenberg here. I'm hearing more and more that founders want to get profitable
and do more with less, especially with engineering. Listen, I love your 30 year old Xfang senior
software engineer as much as the next guy, but honestly, I can't afford them anymore. Founders
everywhere are trying to turn to global talent, but boy is it a hassle to do at scale from sourcing
to interviewing to on the ground operations and management. That's why I teamed up with Sean
Lenehan, who's been building engineering teams in Vietnam at a very high level for over five years
to help you access global engineering without the headache. Squat, Sean's new company, takes care
of sourcing, legal compliance and local HR for global talent so you don't have to. With teams
across Asia and South America, we can cover you no matter which time zone you operate in. Their
engineers follow your process and use your tools. They work with React, Next.js, or your favorite
front end frameworks. And on the back end, they're experts at Node, Python, Java and anything under
the sun. Full disclosure, it's going to cost more than the random person you found on Upwork that's
doing two hours of work per week, but billing you for 40. But you'll get premium quality at a
fraction of the typical cost. Our engineers are vetted top 1% talent and actually working hard for
you every day. Increase your velocity without amping up burn. Head to choose squad.com and
mention Turpentine to skip the wait list.
Omniki uses generative AI to enable you to launch hundreds of thousands of ad iterations that actually
work, customized across all platforms with a click of a button. I believe in Omniki so much that I
invested in it and I recommend you use it too. Use Kogrev to get a 10% discount.
I wonder how you think about the... So if TV just happened, then maybe this is just going to happen
too, right? We're all going to be walking around with this thing in our ear and it'll be... It was
striking how responsive the voice interface was. They talk about between two and 300 milliseconds
latency to respond. And that is like the... That is normal conversational interactive response time.
When we go edit our podcast into script, like the normal break between words that they... If you
have a long silence, they'll take it down to 300 milliseconds is their default for kind of...
You had a gap, but you want to just shrink that gap to make it sound like it never happened.
300 milliseconds is what they have. So if it can respond in less than that,
it's definitely going to be in this sort of similar zone to normal interactive conversation.
I do wonder what forces are going to shape this. It does seem like it probably matters a lot
what the business model is behind it or who's doing it and what they're trying to do
for us, to us, with us. An advertising model is very different perhaps from a subscription model.
People obviously have also got sucked into social media. There was a time which I think it's
honestly still there to a significant degree, although maybe it's kind of receded some where
everybody was optimizing for time on site and you had this, like, what can we do to
make this person come back? What is the most engaging thing? And it became probably
harmfully engaging not just to individuals, but to society. I do think Zuckerberg,
who's credit has tried to back off of that. I'm not sure if other platforms have, but certainly
they've taken some public steps to not reward anger emojis, for example, which I think is
a nice clean low hanging fruit win, but still got a good credit where it's due for taking that.
Do you have any sort of, like, tree in your mind? What do you see as maybe the forks in the road for
how this new kind of ubiquitous technology could develop depending on what our relationship
is to it? And you can imagine, I always used to say too, and I think I might have to stop
saying this, but ChatGPT, I always appreciated that the branding was not trying to be your buddy.
ChatGPT, a name like doesn't sound like a friend. And the way it talked to you also was not like a
friend. It never sends you like proactive notifications, which some of these are like
virtual friend things do replica will send you notifications on your phone and say it misses
you and wants to talk. ChatGPT, you know, it's just like, here's your answer. Let me know if
there's more I can do to help, but it's not like baiting you into further interaction.
Claude interestingly does a little more of that, where it'll ask you like, what do you think at
the end of its responses in a lot of cases, especially if it's like more philosophical or
open ended. So I guess there's related questions there around like, how do you expect this to
develop in terms of how much the technology will try to pull us in and try to captivate us versus
maybe try to push us out into the world? And how do you think that will depend on
who's developing it and with what business models? Yeah, so these are not the the branching parts
of the universe that I worry about the most obvious network about bigger things. But
this stuff matters too, especially if the bigger things don't materialize. And I think you get a
lot of the nails on the head, especially the subscription versus advertising versus time on
site optimizations. So what metrics they're aiming for, I think are a gigantic question.
And then the question of, will people be able to differentiate and select based on what is good
for them? Or will they be largely polluting into these traps, where they follow paths of
least resistance, they get tricked into entering skinner boxes, they develop these kind of
fake emotional attachments, they use an excuse not to engage with the world instead of using it
as a tool with which to learn how to engage with the world, or to better engage with the world.
Every time I ready to be posed, I'm constantly filled with ideas, either that other people have
had or that I have myself, or often both where I'm like playing off of them and they're playing
off of other people. How could we use this as a tool to make our lives better, not through
interacting with it constantly, but for using it to enhance our choices and our actions and
solve our trivial inconveniences, solve our knowledge gaps, skill gaps, again,
give us the reps in the ways that are relevant, let us practice, let us do all the boring coordination,
let us not get interrupted, is I think one that's like going to become a bigger deal,
let us track all of these logistics and all this paperwork and all this stuff that's
expensive to us, where humans effectively can only use as many things, even though the amount of
like actual stuff going on inside those things is often very small. AI can solve a lot of these
problems for us, but we have to want it to, we have to care about that, and it has to be a thing
we prioritize. I'm really hoping for a subscription model, that I think one of the best things that
opening AI did was go completely advertising free, completely steer the customer free, as far as I
can tell. There's this huge amount of revenue and ability that they've just given up entirely,
and instead, all they're trying to do is answer my query, charge a flat rate, or charging the API
for, let's return to marginal cost, providing it is, and that's it. And as a result of that,
that has become for many top people a standard, right? Like you've learned, oh, you can't just
become a miserable person. The difference between a AAA by once carefully crafted experience game
that's trying to do something good for you, which I think is a wonderful thing and people should
do more of it than they do with anything, and the gotcha games and the mobile experiences and the
free to play is, which are designed to hook you and trick you and trap you and exploit the
whales that can be exploited and make everyone else's life miserable to make sure they can exploit
the whales, and that stuff you just don't want to touch the most part, right? Even occasionally,
they happen to create an experience that's good enough that if you are bold enough, you might be,
and confident enough in your own self-restraint, you can go into the limestone, but mostly,
to see this mechanic, you should run, right? You should run as fast as you can. And it's the same
thing, right? You have, that GPT is inherently a very healthy product and not worried that people
are going to do something bad for them on a personal level because they use too much GPT. I just,
I haven't even heard a single story, same thing with Quad, same thing with Gemma. These companies
are being very responsible in the ordinary sense. Character AI with replica, they're not.
Re replica is very obviously the predatory free-to-play trick you into coming back, like
method you when you're away, give you your daily reward or various types. Like every time, oh,
you can get more free messages by waiting and then come back for them. I know where this comes from,
right? I know about delayed variable rewards. I know daily login bonuses. I know all the
tricks. I was in the gaming industry. I know how this stuff works. And when you see that stuff,
you've got to run. And so the question is, if we don't regulate, if we don't mandate, because the
free society, we really shouldn't be regulating and mandating this stuff, right? This bad incentive
drive out good or just good drive out bad, who wins this fight? And if the bad guys win in this
sense, it's going to be a pretty miserable time for people who get trapped. But my hope is that
the people with the best models are presenting good experiences and that can continue and we can
scale that. And the other question is what happens with the advertising model and where is the
advertising, right? Because like right now, advertising is clearly delineated. It's clearly
distinct from the experience. But with the large language model, it seems very easy to say, well,
we're going to give you a free experience where you can ask about various different products that
you might want to compare. But if you just want a little bit of priority in which discussed and
which preachers you're highlighted, and whether we talk about cook or Pepsi or we talk about Tide or
one of the competitors, you're welcome to pay us. Right? What happens in that future where like suddenly
this thing is suddenly pushing stuff on you and you can't necessarily tell the difference
and the labels are increasingly convoluted. If you aren't paying, you're the product, right?
And that's in that sense. And like we can reach one of those worlds and then maybe you even have
to get your second AI to watch what the first AI is doing. If you're smart, you can notice when
this thing is happening. You can double check what they're saying. I don't get too far away from
the news that we can get so much to get through. I do think this is really interesting. The analysis,
many people have covered the news. So I think where we hopefully can add value is a little more
synthetic analysis. I do have one word of defense for replica in that having gotten to know Eugenia
a little bit. I do think she is really trying. And they do have a subscription model, although
they do have these add-on by a new outfit for your replica type things going to. I think that
research, which I do take as reasonably credible, it came out of a group at Stanford and they weren't
directly involved in it. I've been handing over data as I understand it, is pretty compelling.
I do think they're headed into a very challenging, balancing act of a future.
One thing she said to me that was really interesting, and I do see that this is another
dynamic I'm going to ask you to analyze. On the one hand, they started with relationships
and basically no mundane utility. And then with ChatGPT, we have pure isolated episodic
help on whatever you're looking for help with, but no relationship. And I was struck that I
heard from her actually a couple months ago, last episode we did, that she is moving replica toward
more of an assistant. And she said something that I thought was really interesting, which was
maybe the moat in AI is relationship, that you don't swap out your friends because I knew better
friend, potential friend came along. You have some sort of loyalty to your friends, like the
history matters, what you've been through together matters. And she was, we're going to become more
of an assistant, but we still want to have this notion of relationship. And maybe that is where
the moats in AI come from. That's why maybe people don't switch at the drop of a hat long term.
ChatGPT, on the other hand, is headed that direction a little bit to now it seems, where you've got
certainly the voice is like much more given to relationship and it's developing memory and
it's obviously going to have longer context and all these sorts of things.
So these maybe seem to be converging. I wonder how you think about the ethics of ChatGPT,
like going more in the buddy direction. If this is something that you think that we have like
wave of departures from open AI to talk about too, I wonder to what degree you might speculate as to
how this move toward the like more engaging personal relationship possibly could have been
one of the things that the safety team might have objected to certainly I would have been like,
hey, do we really want to go this far this fast? Can't we keep the voice just a little flatter?
Okay, cool. It's responsive. But does it also have to be so thirsty like in our first release?
But then there's another dynamic too that I think I would, if I'm reading into what they're doing
correctly, Altman's now saying, well, we want to get into not say for work content, we want to be
able to do some of this stuff, even gore, even like erotica, whatever. But we don't want to be
making deep fakes. And I'm like, in the one hand, you might say, well, why would they be doing that?
That seems just why would they even want to touch it? On the other hand, you might think, well,
maybe they think they can do it more ethically than other people can, or maybe if they do 90% of
the stuff that people want, it'll take the air out of the sails of the sort of totally unscrupulous
actors that would be doing your, if you can get NSFW AI from open AI that's like,
R rated, maybe you don't need like Taylor Swift deep fake porn that's X rated or something.
And maybe they think, hey, we can be like the good guys that do this racy stuff, but stay on
the right side of the worst ethical lines. There's a lot there, convergence to buddies.
And how do you think they're maybe thinking about like how they play against like other
actors that they may consider to be worse? So I'll start with the last part. I wrote out my
my model specs analysis for open AI, but because everyone at open AI is going to be
hella distracted this week and everyone else as well, I'm just going to hold it for the week or
two and refine it and adjust it and then post it later. But one of the things I have in it,
in fact, an endorsement of not safe for work, right? You have to ask, is this actually harming
anyone? Is this bad for people who are not the user? Or is this so obviously just terrible for the
user every damn time that as a free society, we need to say no. And as far as erotica or gore,
the answer is just obviously not, right? People have this desire to be thirsty,
right? They have this desire to be horny on main. They have this desire to be
by playing in these ways and getting these experiences. And if you don't let them get it
at home, they'll get it on the street. But you can't shut this stuff down. It's just part of the
human experience. And yeah, I think absolutely, if somebody wants to create pornographic images,
they're not the fakes of individuals who have not given their permission,
then why shouldn't we do that for you? Given we have verified you're an adult,
maybe we've charged you extra because why not, maybe? But why shouldn't you be able to do that?
If you want to free a picture with a bunch of blood and gore, why shouldn't you do that?
They recognize with Sora that when you can't do sex and you can't do gore,
you can't make most of the interesting things that people do with video. You can draw the
line at PG-13. A lot of them are just out the window, especially every individual element has to
be very carefully curated and get kept and so on. It's just not going to work. But so yeah,
I think that you draw the line specifically at inappropriate portrayals of actual individuals.
But you don't want to make an obscene, it was a few things. That's a problem. You should catch
that and stop that from happening. And there's a certain level of difficulty you need to impose on
that before someone can be allowed to do that, but you have various different tricks to defend
yourself against it. But I think if you want to either create a pornographic deep fake of no
specific person that just plays to whatever it is you want to see an image of, or if you want to
create a picture of Taylor Swift wearing a different colored outfit, singing in a different venue that
she never visited for her Error's Tour, like as long as it's got a watermark, what's the problem?
You want to take a picture of her singing at your son's birthday party and have fun pretending
that she was at your son's birthday party and you play her music, right? Because you can just
record that and you play. That's like kind of the service. What's so bad about that? Why is that a
problem? And obviously different people have different opinions and maybe each celebrity should
decide like what level reproduction is okay and not okay. But certainly like the situation where
like Barack Obama can't be generated by an image model, like giving a speech at the wrong college.
He's just like down to me and again he's going to force me to use a different image model for
those purposes. And how long is it going to be before those free image models are just trivial to
set up and trivial to use and everyone starts using them. I keep waiting. I have the old decision
on my computer and I try it out for a bunch of stuff. One thing that's very good for is you tell
it to create a hundred copies of something and then you come back 30 minutes later and then you
check. Just scroll quickly through them and see if any of them have what you wanted. Maybe you go
and there's the image on that one and maybe you figure out what maybe you just use this to diagnose
okay here's what's wrong with my prompt and try different prompt. And you can't do that if you're
using the free surf the internet services because that would cost them an arm and a leg. Now I'm
using my own GPU so it's fine. Or alternatively use that to create stuff that they wouldn't let you
click because you want to see what you can do and you want to create it. Maybe you want to enjoy it.
So I moved on to it basically but it's become especially obsolete that I just use the online
ones now but I've been waiting for stable vision 3 to be easier to deploy and now I'm going to go
back to okay and see what we can do. In general yeah I think they should offer the services that
you want. I don't think it should be this thirsty by default. I think this idea that suddenly you
have Scarlett Johansson acting all dirty with you like when you just ask what the weather was outside
with no indication you wanted thirstiness that's weird. I don't think that's ideal. I think this
is we went too far but if you put in your user preferences you dial up the thirstiness tone
right now you need the luck you want the Scarlett Johansson style voice. I don't kink shame like
it's fine enjoy yourself if that's that's the statement you enjoy and again we should be very
careful with consent and very careful with permission when there are real people involved
but I think a lot of real people will be very happy to give them consent either for free
or for a very nominal charge. You ask me if someone wants to use my voice
for some ungodly unknown reason. On StatGPT if it's just a handful of people I'd say go ahead
if it's like 100,000 people I'm like I should be paid but it's about reasonable compensation here
and I'm sure a lot of other people feel the same way to people whose voices
someone might want to hear. Are you telling me I shouldn't just pay an extra dollar and get the
Morgan Freeman voice for all of my narrations of my explanations but that's cool why shouldn't I do
that or maybe I want Scarlett Johansson again because she gets paid that's fine too and then
like you were there was some other stuff that you were asking about that I've almost forgotten at
this point just because it's such a complex question so you just have to really work your
What I am thinking about is and it is there's a lot of issues that are intertwined here
but there's a couple dimensions of convergence actually there's like the convergence to the
buddy form factor and then I also see at a lower level and we're gonna have an episode
I'm gonna have Alex Albert he was a prompt engineer and I was leading the developer relations
function at Anthropic and I was asking him about how all the APIs are converging to and how that
how they think about that in the sense that it may create a winner take all dynamic
so on the one hand I'm like man I don't feel great about sort of the trend toward
relationship first in general I do feel pretty good about replica for like the people that are
really struggling with loneliness not sure how I feel about it when it gets to like a super
broad base thing I'm not sure how I feel about open AI being so thirsty with their voice
but then if I go down one level and I look at the API and I see convergence there and it's a
one-line switch possibly with a little bit of prompt engineering definitely with a little bit
of prompt engineering but it's like close to a one-line switch just to go from the GPT-4 turbo
to cloud 3 opus to back to GPT-40 to Gemini 1.5 pro whatever's next that to me is a worrying dynamic
in that if it's if all of the integration points have converged to essentially the same
service provided in the same way with a down to a one-line change then it becomes a winner take
all dynamic in the like whoever has the best model in theory can win all the business very
quickly like why don't we just switch to the new one but then maybe one reason that could
not be so a winner take all with one release to the next would be this relationship dynamic so
I'm like not sure I like what it does to society if we're all like in these parasocial relationships
but maybe I do what it does to the to soften the like super intense borderline winner take all
dynamics I'm not even sure to what degree those exactly overlap because one is the app
layer and the other is the API layer that powers the app layer but what do you think
I stay opposite I think you're thinking about this all wrong so first of all like you have
more than one friend and I have a lot more than one friend and I choose which friend to call based
on what we want to talk about what we might want to go do and sometimes I go for friends sometimes
I have one sometimes I have a different one and I would get bored if I was hanging out my best
friend's name is Seth and if we hung out every day all day and try to talk about everything
I'd be like this is I don't just want to hang out Seth all day but so I have a lot of people too
and the switching this means that you are not locked in right you're not stuck so quite the
opposite of winner take all right it lets you move around and even character AI and replica
understand this so you can talk to 10 different bots right for different purposes and of course you
at a minimum you want to figure out you have this one for when you want to talk about engineering
this one for when you're thirsty this one for when you just want to talk about restaurants in the area
blah blah blah which makes a big sense and check GPT has his GPTs and now Gemini is going to have
his gems and so on and then you have this thing where you can plug into a different LM and so for
me I have changed my primary LLM reasonably recently from GVD4 Turbo to Gemini to Claude
and now back to GVD4.0 in many cases on a dime it doesn't mean I don't use the other ones it means
that like when I want a variety of answers I'll call all three of my friends and I'll actually
in some sense that I'll ask them hey what do you think what do you think what do you think I'll
compare nodes and sometimes I'm like I know what I know who's good for this right I want to analyze
pdf I'm going to use you right I want an image I don't create this kind of image better and so
I play the portfolio and then as people advance you learn okay so then these captured more of my
portfolio for now but then that switches back and I want this variety I want these optionality
I don't necessarily even want these things to know all about the certain aspects of me then I do
different things also data portability is probably going to be a huge thing if you're at replica you
build up this relationship but are they going to know you access to your records and data
and that's like a pretty toxic thing for them to do I assume they won't and what happens when you
download the chat log upload the chat log into into gemini 3 or whatever it is and say you're this
guy now here's everything that you've said to me and here's everything I've said to you and he
learns all about you now and now you have your new assistant that knows what your old assistant
often you don't switch employees because you don't want to have to train them to explain all the
context with them but now that context is just copy-paste that's like a huge I don't expect
winner-take-all from this unless the winner deserves it that's how my dpd5 is out and everyone else
is still at four level well of course I'm just gonna 90% of the time plus be using chat gpt got
gp5 but if it's a marginal change like it's been recently where it's just like one of these things
is a little bit better that's completely different it's particularly the lock-ins and the relationships
that I think are the places where you're likely to get a potential winner-take-all scenario so if
Gemini knows the contents of my gmail knows the contents of my sheets knows the contents of my docs
is creating workflows where it generates new documents and new memories sees my photos analyzes
my photos I start taking pictures of all my food so it knows what I've eaten every day where I've been
etc etc it has all of this new context it would be very difficult to transfer this context now
suddenly I have this huge thing where every time anything is remote or personal I want to use Gemini
right and that becomes a thing but I don't think that in general I've been seeing this lock-in
unless of course the people develop this kind of emotional attachment right this may be
raised romantic maybe it's something else I'm basically saying that's not like that that going
too far is not particularly healthy right I think that's bad I think people should strive to avoid
that I tip under pretty special circumstances like beyond what it takes to get like reputation
reps and practice and stuff like that but I'm optimistic this is this is the conversion seems
like a friendly thing right like why wouldn't you like how amazing would it be if you could transfer
your stuff and your connections and your knowledge between social networks between android and iphone
between all the other things that are like converts but are seemingly incompatible like
compatibility is just almost always good yeah it seems good for the consumer I do wonder about how
in a world where Google had their data on the calendar and then open AI does their launch one
day in advance it does seem like there is a potential for exacerbating the the general
one of the biggest worries in AI is that the and I know I'm not telling you anything you don't
know as well or probably better than I do but the arms race dynamic between either countries or
even just leading companies within the United States where hey we got to get this out we got
to beat them you have x time to do your testing and we're launching and that's the time you have and
so corners start to get cut perhaps this feels like the liquidity with which people can move about
the market would enable that dynamic or encourage that dynamic because if open AI is like about to
launch that you know one party is going to beat beat us we got to get ahead of them now you have
this and that that sort of depends on the idea that if they're out ahead of us then everybody's
going to switch to them and we're going to be screwed for a while and that could be screwed in
revenue it could be screwed in data it could be even just pride and reputation which certainly
seems to be a big part of what's happening at the moment you don't worry about that to me I'm
convinced by your multiple friends argument that as long as there are different things in
the same ballpark and they have different characters you go to different things I do
personally find that as well I still if only because chat gbt has the native coding environment
even if opus is a little bit better on coding like the fact that it can execute the code on chat gbt
side keeps me there for coding now it actually with oh probably is notably better on coding
as well but opus is a better writer so I go to it for help with writing so I do have my different
ai friends for different different purposes at the retail consumer level but in my app building
it is more of a I'm going to make a choice right if it's waymark and I'm going to have a language
model or a vision language model process all the images of a small business that's just uploaded
their library we're probably pretty much going to just do it with one model and in practice like
you can switch really easily that dynamic doesn't seem like a big concern to you still
it can go both ways right if switching costs are low then me announcing a day before you
doesn't really matter so what you get my customers for a day and then I switch back like who cares
if it's incredibly expensive well then if you can get ahead enough to convince me to switch
I mean it's stuck and so maybe you get incentivized to try and capture that market a lot more or maybe
you're trying to outdo each other continuously you're trying to say just a step ahead because
we don't take a lot but my guess is there'll be enough differentiation between what features
these different tools have and in what details you want to use and in how you customize them
like I put a decent amount of work at this point to customize my instructions for chat gpt
because they have custom instructions and I haven't done that for gemini or quad yet but like when
they offer that and I decide to do this time I will almost certainly do that and they might look
very different because I'm trying to specialize right they might be like okay so when I go here
this is what I want so it's a different set of instructions but as an app maker you're not going
to want to switch every time there's a little incremental improvement right so it's just not
the same thing look we don't take all in some sense from a safety perspective is ideal if
opening I for anthropic or google was out there two years ahead of everyone else
and their model was just better and so for all dangerous purposes they were the only ones that
mattered then there are advantages to that but I don't think that the interoperability is going
to change whether or not that turns out to be true in an important sense and I don't get substantially
changes to this early the pressure we're putting on people in that way but also one of the things
about this last week that was like why I really enjoyed this week on the release side even though
there's other things that I didn't like as much obviously was that everybody is now seems to be
focused on things that help people have a better experience and improve the world but that don't
advance the core intelligence the underlying system so to me that's just when all right I want
chat gpt to be better for its users without becoming more intelligent in the ways that they
get more existential dangerous right now that stays in its current state and just get all these new
cool features that's just the best in the world but I love this world and so I was really heartened
to see the directions people are taking it see these new developments and I can wish everybody
the greatest success obviously both a certain amount of revenue and hype and pressure but
I think mostly that's already built in and I accepted that's just like the world we have to
live in so why begrudge why not just have a good you know don't throw me at a good time that just
keep coming yeah I'm with you on that my standard line with the gpt4 class models is we're in this
sweet spot where it's powerful enough to be really useful but not so powerful as to be a real concern
and I do think that's great and I do think all these integration points are great the the
enhancements have just as you of course famously call it mundane utility are great it doesn't feel
and I hear the argument too about yeah if somebody's way out ahead then they have the luxury of doing
it right taking their time doesn't seem like we're in that world to me I do expect open AI
is probably still about half a generation ahead internally my guess is that they don't make
the best model free if they don't have a significantly better one coming before too long
that's going to get everybody paying again that could be wrong but that would be based on all kind
of rumors and intel and just how much time has elapsed and public comments from sam oltman
it does seem like they probably still have one additional turn that google may not have made
yet but overall it does feel like they're close enough where they're like running
generally speaking neck and neck and that part does concern me and meadow is like not far behind
either they just put out a paper yesterday where they had trained a new model called chameleon which
is very natively multimodal they call it early fusion meaning text and image tokens are encoded
in the same way basically share all the same weights trained with 10 trillion tokens which is not a
small amount and unclear if they're going to open source that yet but you know same week as all these
other kind of natively multimodal things that they're you know answering and definitely just
showing that you're not going to stay in front of open source for too long even if you are you know
ahead by a full generation you know generation and a half you're going to have to keep going
i'm still not convinced the other direction either that i shouldn't be worried about the
liquidity at the api layer the ease of switching and the sort of sense that may create where we
got to one of these guys i do see that dynamic kind of developing and it does seem problematic
it is i don't think these things that basically what i'm saying is i'm not sure these things intersect
that heavily in terms of the correlation is close to zero not that it's like going the other way or
anything that just these things seem to me to roughly cancel out in the sense that like that same
pressure is always going to be there open ai and google are going to try and one up each other
and try to advance them all as fast as possible they're running into a lot of the same constraints
my assumption is indeed that open ai has somewhat of a lead here but the fact that open ai seems to
be quite responsibly taking their time to to build the next thing whether or not they have a choice
we'll see what happens but it's really hard to tell who's out in front i think google had a history
of essentially being far out ahead of what google actually releases the google has had a significant
number of pretty earth shattering for the time capabilities in the past and it decided
that their lawyers should carry the day and they should not release the product
and so it's not obvious to me that google has to be behind it all right just it's very opaque
from the outside you can't tell like it defaults as day open ai is six months to a year ahead or
something like that if open ai is not six months to a year ahead it would seem to be more likely
because google also has another half or full generation more than they've shown not that open ai
doesn't i would my assumption is that open ai i don't think that long to train as such necessarily
yeah it's more like they need to get into position where you're ready to do that but you have the
resources to do that you know you have all the data streamlined and you're ready to go and it's
really hard to know obviously any of this for sure it's also possible that there's a big gap
between when you're you have it internally in the base model form and when you're ready to actually
release a product i speculated also that it's possible that there's a GPT-5 style model they
either could train or have trained but that like inference is so expensive that they think it would
degrade the user experience to rush it into market so they'd rather do this instead because for most
purposes that's not actually what people want in some form or they need to scale up their infrastructure
they have to choose they have to either serve GPT-5 to a small number of people or serve GPT-4O
to a very large number of people and maybe they think that it's better for them if they serve the
second one i don't know and we just it's have a hard time in the long run like who has the advantage
right who has the better infrastructure who has the better teams who has the better capabilities
like again it's there are people who know much better than we do and we'll find out yes in an
important sense i wish it wasn't close right i wish i knew who it was who was in front and they
could in some sense just take their time which more so and i definitely see the pressure at least
that seems to be built in of throwing caution into the wind and just doing some stuff but again
shouldn't caution be thrown through the wind outside of some certain key moments right like
when are the moments when you should be cautious right so like GPT-2 we saw some caution right
from open AI even if that wasn't enough for Dario right well GPT-3 again we saw some caution we
got some dispute over how much caution was warranted GPT-4 we saw a lot of caution with the release
it seems pretty reasonable to not particularly be that cautious about 4-0 if you've looked at it
you're like no these is extra modalities don't really do anything if that's dangerous we know
that because we've tried a lot of stuff with GPT-4 and it's not like it's close but it's
basically fine and then here in an overcommercial situation and they were up against any number
of deadlines right you've got the google io thing coming the next day you definitely want to you
potentially asking ilia and maybe john likey to to stay on until after this presentation
so that doesn't distract so you've got potentially a bunch of things going on
i want to talk a little bit also about broader market competition our mutual friend Andrew Critch
coined this term the big tech singularity or i think he calls it the tech company singularity
but the notion here i think is an interesting one where he basically says at some point we
may hit a point where the big tech companies in virtue of their superior AI and compute
capabilities could reach such a position of dominance where they could effectively enter
into any industry that they want to compete in and come to dominate it in relatively short order
and i do start to feel like we may be getting close to that and i think you have a different
point of view on it but my kind of just simple thinking is man google has a hundred billion
dollars cash on hand so one one fork in the road there would be like are they allowed to buy in or
not to these industries but it's crazy they got a hundred billion dollars cash on the balance sheet
i looked up just some of the big hospital companies in the country one of the biggest ones not the
biggest but but a big one that i'm familiar with is called tenet and they have 58 hospitals
they're worth 13 billion dollars market cap for one eighth of the cash that google has on hand
they could buy one of the biggest hospital companies in the country and then you can imagine them just
plumbing med gemini into kind of everything ripping out the sort of nightmarish electronic
health records that currently exist putting something in that would hopefully be a lot better
tapping into all this data they've been able to do these med gemini things i would assume with
huge barriers to data this would dramatically reduce their barriers to data would allow them to
i'm sure of course they'd still have to handle compliance in some ways but they could dramatically
streamline their access to data train the next generation of medical models provide med gpt as
kind of frontline service and it would seem to me like they would pretty quickly be the best hospital
system in the country if they did that and then i imagine like replicating that across other
verticals education as we've discussed obviously these things hold tremendous promise for education
the key thing seems to be like actually building it in a sort of first class way which i'm not
sure current institutions are going to do very well and google may struggle with it in some ways
too again they could buy like any number of private education companies for a pittance from their
perspective and just run a loss leader experiment to see what they could do in a new vertical like
this why do you think that's not the case if you think that's not true because i think they're not
culturally capable of doing the things you'd have to do to make that work i think that they're
reputational and legal and regulatory barriers to doing that are very serious and i think that
they're not going to be able to deploy technology in those situations that is that far ahead of
what people who are simply paying them to use their to use big tech technologies could get in
their place right open ai is not going to be able to use gbd 5 for a hospital system substantially
before gbd 5 is released that's not really a thing to the same the version of i forget with the
med with their alphabet or whatever it is that they're calling it the version that google is going
to put out there is going to do the same version that they have internally but it's also going to
be available to everybody else if the people want it yes it gives them better data to do this but
they could also give that for licensing agreement through corporate or a deal and there's a reason
why basically companies like google do not put themselves into everything in crowning try to
do everything at every feature there's a reason why we see these demos and we're like how do you
have no idea what your products can do are you not trying to do anything with your product
you're just leaving that for the rest of us the answer is yes they really are in some important
sense and they are trying to develop their core systems of products make their thing available
but someone else build off of it but someone else take the baton but someone else handle these
things i saw a few weeks ago a story about ai private equity right so the idea is this thing
but without being google right so like you go out there you find a company that's obviously in an ai
business doesn't know what's supposed to be an ai business you buy the company you then tell all
the employees to use ai you deploy the ai systems everyone gets much more productive the company goes
up in value the company thinks a lot more money you win you then use the profits to keep doing this
and that's like a much better model to me so does being a startup whose job is to raise a lot of
money in order to launch this hospital thing and i don't know if it was in hospitals or maybe
use a private equity thing and buy them out i don't know maybe do both but these models strike me as
more what these people are capable of but like google is terribly scurrosic in in so many different
ways when my understanding is so is microsoft so are the other big tech companies in their own
ways you can't simply have them launch an entirely new thing and do an entirely different thing
and expect that to just work in this sense now obviously at some point if you have gbt9
you can tell gbt9 go out and buy a hospital system and deploy your technology to help the patients
do better and make more money and gbt9 just handles all of it because it's gbt9 but that's
not the most important thing that's going on with the fact that you have gbt9
like you bury the lead if you can do that right it's very important senses so as long as we're
still dealing with these mundane levels of ai my expectation is that the big tech companies
just they only have so many imperial focus points they only have so much different things they can
do at once and it's not in their interest to do this is the reason why google is constantly buying
startups and i don't think that's going to change let's talk about the startup side for a second as
well i feel like i am hearing less about startup acquisitions lately from the big tech guys obviously
we had microsoft aqua hire inflection but compared to a number of years ago it does seem like there's
a lot less aqua hiring going on and i certainly there is bureaucracy at these companies it does
seem like they've managed to clear some of that out in the gemini department at least there's
been an interesting discussion on that just in the last 24 48 hours on twitter where among other
people sholto who was is locally famous for his appearance on the dorkesh podcast said yeah basically
nobody cares about levels at gemini elsewhere in google yes but in the gemini thing it's a very sort
of flat we're all kind of one team trying to make things happen five if you are a startup i was going
down my list of previous cognitive revolution guests and just asking like who is feeling enabled by
all these new things and who's feeling threatened by all these new things and i'm like i think
there's a significantly a significant majority of them that seem like they are more likely to be
threatened by it if we're honest i do think in the short term probably still everybody grows
because and this is for some of the reasons that you're saying that google is not going to productize
everything immediately they're not going to go to market with the same sort of intensity or focus
that startups will so i'm bullish on everybody's like next couple to few quarters still but i do
look at the economies of scale that i see developing in the big tech companies vis-a-vis the startups
that are building on the platform and i'm like man i don't like the beyond two to four quarters
position for a good chunk of them because they're always going to be a little bit behind on the
models of course that inside they're going to know what they have inside they're going to be picking
off whatever the biggest opportunities for us the outside of course you're you're waiting inside
you can also do it for free exactly how this is going to be positioned and what you have to pay for
is to shake out but like one thing that's definitely clear right now is gpd4o is free for
all in chat gpt it's not free they've lowered the cost but it's not free via the api so right off the
bat you have as a startup you're like how do i exactly compete with that i have to pay for it
which means i have to charge for it which means the i just have all this additional friction
compared to somebody can just go to chat gpt and use the the thing directly there and again just
the economies of scale just see man the who can really compete with these guys we've had folks on
the show who specialize in text to speech who specialize in image generation and just gpd4o
once it's full capabilities come online over the next couple weeks seems like it's likely to be
best in class at both of those potentially you make of like this the position of the
companies that are building on the platforms right now so all of them i think had very wise
things to say about this right the idea that if you're building your model you're of your company
you're building your company and you're building its tag because gbtn isn't smart enough isn't good
enough and you're going to build a substitute that's going to for now do a better job then we're
going to steamroller we're going to curbsomb you it's here to die but if you're planning here's
an apparatus to use intelligence such that when gbt5 comes in you switch the four to a five in the
function call and now suddenly your product just works way better and you're not directly trying
to do the thing that the base model can just do now you're in amazing shape and in fact you have
a great company that said it'd be happy when open ai makes progress right so the question is can you
plan for something where you're happy when open ai comes up with a new model and not sad or without
loss of generality obviously but so if you're 11 labs when you have amazing text to speech right now
then yeah you have to worry a lot about whether opening ai is going to curbsomb you because
you are fundamentally competing directly with them to offer a technological service that they
are going to want and either you find a niche say you're willing to replicate Scarlett Johansson's
real voice and open ai isn't or something like that but otherwise yeah they they're actually
going to have something better and you're going to have to somehow keep up with them and that's
been required from being big and they do something good that's because like that's a generality that
opening ai's can develop anyway because it needs it for its core products but if you are trying to be a
private equity company to buy that hospitals in order to streamline their medical records well
now you're like okay cool you have the latest way to say i need to speed into these things to be
better they're like cool hand it over google hand it over open ai and you plug it in and your
private works better and you're that much better than competition that you were last month when they
didn't have an ai at all and you had gpt4 and now you have gpt5 right it's amazing right you're just
that much farther ahead and so also there's no conflict between there's going to be a lot of
amazing companies we're going to be saying we're going to all die before that because it'd be some
really cool companies and the current companies are often not going to be the cool companies
right it's very possible that every two years half the existing companies die in this realm
because they got curb stomped because the thing they were trying to do no longer differentiate
them from the core model the core model was not able to do that but the other half do well it's
a tech bubble in 1999 right whereas yeah a lot of these companies turned out to be doing something
it had no underlying value at the internet developed and it didn't differentiate them
but also do a portfolio of them or that was amazon so you made money anyway
so who do you could you point to specific companies that you think are like well positioned
because i honestly struggle to list too many that i think are going to be like oh yeah this is playing
how we to our advantage how we hoped and puts us in a long-term winning position i don't find
like a very long list honestly yeah the problem with i think you've mentioned before is that
ai is this huge beat and so you have to choose like where you're going to specialize and i've
made a deliberate choice mostly not to follow the details of various startup projects and
it's one project that are offering with inutility in this way because they're not that important
to the bigger picture and it's very hard without investigating carefully to have value add things
to say about them right so it's okay i'll see what happens so i can't say which ones i think
have great market prospects in this area but i also can't say which ones are necessarily
going to get curbsome either and often rather than the internals right how is i don't i have no idea
what the question is actually doing i don't know to what extent they're training their own models
i don't know to what extent they are working off of gpd4 or claud or whatever it is right now
but then adding custom instructions and then scaffolding to try and allow it to much much
better analyze the web and come out with very rapid good answers and in a sense there's like
a version of a but today that gets curbsombed and a version of but today that's potentially
going to do very well right i all depends on a lot of details but when i look at one of the
companies whose tabs i have opened one of the companies whose stuff i use and i do not use
those things because what i want is what the base model is on right i don't use character ai i
ever right but think about that as an example right if you're a character ai well if you're
just counting on the fact that open ai is not going to be thirsty right it's not really a great
long-term plan if you're counting on we're gonna give a bunch of tricks that make this thing feel
like human interaction because you're a robot god because the models aren't good enough to
simulate real human interaction on their own that's not going to be sustainable either right
but if you're building your ai bot such that when you make it smarter it provides a better
experience but you know all these things about how to build relationships that open ai doesn't know
it is never going to find out you could be in a great spot there's nothing stopping you right and
in theory that's actually get much much better and it should especially get much better when you
start adding video generation and virtual reality right which is where i assume is the future of
that sort of thing a few years from now and if you're ready for that great so it just depends
on again are you trying to be future proof are you planning to where the are you getting where
the fuck is going or are you fixing the fact that like right now there are some flaws that you can
better address and you can go to market but there's also room for people who sign brightly for a
year or two and make a bunch of money and then fade away because they're technically that is
honestly when i've considered any sort of new entrepreneurial endeavor right now i'm like i
think you're right to cite sam altman and i think that analysis is right but i'm like man
that's pretty tough i can't identify too many things where i'm like oh yeah if i do this then
i'll be like much enabled by the next model but i won't be threatened by it the majority do seem
right now like they're in this kind of compensating for weaknesses trying to go to market and they have
to right because if they don't if they don't compensate for the weaknesses they can't really
go to market successfully if they don't go to market successfully for most of them aside from
a few that can maybe raise like enough money to really make a long-term play and not feel rushed
to market but that's obviously not a super common position to be in it just it feels tough so when
i've considered i don't think i'm gonna start a company in the short term but when i have considered
that i've felt more drawn to this what would be like a flash in a pan you know that would be that
meets the need right now that could grow really fast that maybe doesn't make sense 18 months two
years from now but which could be like a good winning short term sprint that sure inevitably
gets burned up in the supernova of the foundation models it burns out even if your original tech
gets burned up in the supernova right by by learning a lot about the business and your customers
and the golden relationships with your customers you might be able to entirely turn this over
into something that still makes sense and there's a lot of places where getting it right making it
easy on the user like we talk about all these custom instructions all the stuff between different
different models and different companies and all of these sophisticated power user things that we're
doing and we got to remember that most people aren't even using these models at all and that
people are using them at all those are incredibly unsupporting right they're mostly complete
civilians and most people will never touch a setting the settings button in their life on
any of the activities no matter what option to give them no matter how valuable they are they
will never learn what they are it's just not how people work so if you can just do some basic
good customizations instead of good defaults and handle some regulatory issues and just make life
easier for people in a way that floods in the technology right and then you have a bunch of
scaffold and they'll like actually have it be better right in various ways and that's really
promising I was talking to one of my friends he's a lawyer right and his firm is encouraging them to
use their own internal customized but like very likely customized it's like some basic service
lawyer GPT that they like can use internally they can't just use the basic thing because of
compliance concerns but also because like they wouldn't necessarily know what they're going to do
but there's enough like annoying individual steps on it that he ends up just not bothering
because by the time he actually like imported the proper case law and hooked up the proper
documents so I think he could have just done it by hand so to offer the time so he loses most
of the utility from it most of the time he's kind of skipping it it seems you personally could
probably within a month code up something that would supercharge this capability and probably
increase the entire firm's production by 10% maybe 50% certainly if you had a small team so and then
again if you built it properly when GPT 5 comes out you just switch the 4 to a 5 in the reference
call and you probably just gets better so there's the alternative but he can't use the still can't
use the alternative and still doesn't know enough to like properly do the alternative and then yeah
when it properly integrates into your entire workflow and Google's buddy has fully automated
into okay now it has access to every document in the world automatically you know you're still
going to want this kind of specialization and you're going to use the profit you made early on
to keep the specialization thing if you buy the hospital you're going to be able to integrate
into the hospital's workflow you're to gain the trust and ability and education of these doctors
you're going to make it easy on them because they are not tech people and they do not want to
have to think about it and they want something that like feels right and safe and good to them
and the regulators are briefing down your neck and there are so many things like this where
professionals time is so incredibly valuable and getting the answers right a little bit more
is so incredibly valuable but you don't have to be that you don't have to be that much better
in certain ways again you can be 10 times better in the interface you can be 10 times better in
just the ease of onboarding and usage and regulatory compliance and then you have some
scabble and yes sure you just party whatever the last bottle is that seems fine there's plenty of
things that like they're not future proofs you don't think they're code but like they're future
proof in terms of if you're planning ahead to make them work so you said you like the business
model of the private equity AI reboot for companies that don't realize they should be AI powered yet
do you like venture capital right now would you go would you want to be an LP in a generic
Silicon Valley venture fund at the moment ventures a weird situation because your compensation as an
LP in a venture capital firm is not tied to your expected returns that highly right it's a question
of look if you offered me a big enough fund to go invest in companies and we set aside ethical
concerns for the moment with potentially advancing AI in the wrong ways or we were confident we could
like only choose companies we were confident we're gonna tour better or whatever it is and you gave
me a big enough portfolio to work with and enough of the share of the profits and of course I'm
excited do I expect VC to make money like above market returns I think it's a completely different
question and that is a now we're talking price right we're talking about whether or not there's
too much money raising into AI or there's not enough money rising into AI and then also the
question of course of how much are the insiders and the high reputation well and well connected
people just getting differential access and positive selection and you're missing out on all the deals
that are worth it and so you just talk to the dregs and unless you're like really good at picking
up from the dregs even if the industry does well you're gonna lose but yeah I'm currently advising
line hard ventures we're trying to do a safety positive fund for AI and I think they're taking
LP money if you want to invest in our next fund but that's largely talking a little about the industry
a little about the startups a little about what's going on and partly you have to try and encourage
the company to think we'll be better in the world it's not gonna be about money I don't have a very
good share of it but sure of course if you have a month of money and it comes down to you get the
good deals and you're going to return high enough if you're considering much like or not and that's
very hard to tell from the outside my guess and we are the valuation the same or maybe the valuations
have an extra zero on them they're not supposed to have at this point or maybe everyone's sleeping
and they should have an extra zero they don't have and now those things are completely crazy
my guess is if I could get a my portfolio that said I get I get to buy shares in S in SV on the
stock exchange in SV us was just a shit was just a equal share of all venture capital investments
made by anybody in Silicon Valley and it was trading at cost of investment plus interest rate or
something I would port to an even portion of my savings into SV us but that's not the question
is it well that's a pretty good approximation of what I'm trying to ask I think I would probably
put more into big tech than that how would you had one dollar to put in SV hypothetical SV us and
one dollar to put in big tech us which is whatever your fish companies yeah it is trading at cost
of investment plus interest and the existence of SV us didn't radically alter the valuation of all
the companies which it would if this traded then all the valuations would probably go up 10 actions
we have various reasons to suspect this but at current valuations in relative terms I think
I'd be pretty excited for this small stuff and I think that the market will reflect would reflect
that if other people had similar market opportunities and that you'd be able to turn around and sell it
at a profit if it's really good it would actual people would pay and they would have both sides
returns if you didn't have that and it just kind of holds to maturity as it were but I'm very happy
with my portfolio of companies essentially is big tech investments that have made me a ton of money
and everything else which is like a wash right like I made some money in for solar and I lost
some money in Hasbro and it's all the emerging market stuff is all wash and it's vaguely even
except for giving it number of big tech companies that have just like gotten returns have been very
good and they big tech companies are increasing to my portfolio if they don't be balanced I guess
I expect that trend to continue to summarize our somewhat contrasting viewpoints I'm seeing it
being pretty hard to outrun the big tech companies because they seem to be moving pretty fast and
the returns to scale seem to be super high and it seems like a large majority from what I see
seem to be playing this sort of compensating for current weaknesses game and trying to get to market
and meanwhile the next generation of model is cooking if it's not already cooked and
the current one is free in retail and not free at the API layer and it seems like you more see
friction and bureaucracy friction in the market bureaucracy at the big tech companies regulatory
barriers such that you think that while maybe everything I'm saying I don't hear you like
distruding any of it but you're more saying yeah that's maybe all true but it it probably
still gets contained by these other forces and leaves like a lot of opportunity for
other smaller companies I basically think that the big tech companies just only have so much
bandwidth that attention has to be focused and critical for any given company of whatever size
and that leaves treasure everywhere there's just think about all the things you could do
with current technology if you had somebody built the app for that right somebody built a
Ruby good not just a gbt but like a much broader true context has gathered all the data is coordinating
a bunch of stuff is a lot out like actually sat down with users figured out what they actually
cared about and so on and it's certainly it's something Google could have built it's something
that you know people put it built but they didn't because they're not doing so they didn't even try
and fail they didn't even try right they did you that virtual employee in their keynote
what's your expectation for the AI powered virtual employee the first thing I noticed right away
was like is this thing in all the chats and reading all the messages and the DMs and like
all of your emails like for everyone on the team they're talking to other people on the team but
look at what it is and that doesn't seem great for me I don't necessarily want all that stuff to
be viewable by an AI that people can query that seems bizarre people can start using signal at work
just separately on their second phone what's going to happen here but to the extent that
if that problem gets solved it seems really exciting to me that essentially you have this
virtual employee but really what it is is just it's a bot that scoops up all of the information
from all of the contacts in which you're okay with everybody having that contact
and then it can answer questions it can help communicate it can help mediate it can touch
things along it can I'm especially excited for just things like you ask it what has happened
regarding actual can you keep me posted something happens with respect to why or can you contact
Cheryl and marketing and have her evaluate which of these three things she wants she places
what priority on and get back to me because I don't want to have to talk to her directly because
she's really rude or a number of things right like it just it seems like it's the sky's limit
on just you have this extra tool in your toolbox that the practice is going to be incredibly valuable
because it just knows all the things and can do all the things and continuously monitor all the
places and like just not having to monitor as many feeds on a continuous basis by itself
do you have a sense for what the hourly rate should be for an employee like that
they may or may not charge for it that way but which is always the question as a consultant
you learn which hours are people paying but which parts of your experience are being built
which person not being built but if you assume that when we talk about what you're billing
while it's being queried by a human right while it is performing a specific task but not while
it's hovering up all the information in the background I assume a lot if it's good there's
always the if it's good right I give this reliable if you can count like there's always that threshold
effect to be in these situations right is it good enough that I trusted to do this task because
if I've got I wanted to check the marketing room and see if marketing has any requests or proposals
or craziness that I might have to respond right and the marketing channels are nuts in my company
right they're like 40 pages of stuff every and I don't want to read that so I have to read that
junk now can the I reliably alert me when I actually have to pay attention such that I trust
not to read the stuff if it doesn't it's horrible right but if it's good enough that I actually don't
read it then suddenly it's a godsend even if they have to spam you with four pages out of the 40 per
day just to make sure there's no false negatives so the question is can you get it reliable enough
if I ask it to have to talk to Cheryl and marketing would actually give an intervention of Cheryl and
marketing and would fail briefly when it fails such that I can take over from it and it fails
silently if it hallucinates weird stuff gets me in trouble that's what happens even a little bit
I can't do it but if we can get there suddenly it's incredibly valuable I think it's a lot of
stuff like that right or if it's just easy to use right it's like with my friend the lawyer and the
case files right like if I can get it such that it's easier to have this thing checked and it's
reliable enough suddenly I win but until then I have nothing but I think you see a lot of this
stuff where like you have nothing until you have everything and then once you have everything you
have to learn you have it and you have to get people to actually use it so like you've got
three things you have to do so the things are hard but like it's Google we'll get there the question
is how long will it take and like which ways we'll get there but we'll get there yeah I think you're
right absolutely right to emphasize the importance of threshold effects it does seem like we're pretty
close to tipping over a couple big ones and it will be very interesting to see how that
affects the broader world of employment what's your expectation for employment
over say the next year do you think we hit a point where we do start to see a real impact on
either you know on the positive side like measurable productivity or on the potentially
negative side you could imagine a world in which people that are entering the labor market are
really struggling because entry level developers just like can't really beat Devin or entry level
general purpose knowledge workers can't really compete super effectively on an ROI basis with these
AI assistant AI virtual employees from the likes of Google what's your short-term expectation for
that so you have to obviously separate what's going to happen in specific areas of employment
what's going to happen with employment through our broadly either like the sector of information
knowledge workers or this overall employment overall employment just a question of how big the
positive number is right because as you make people more productive as you enable people to do more
things like we are nowhere near the point where the AI just starts doing the next marginal task
that a human wasn't bothering you before right and to me like that's the question if the AI takes 10
percent of the of the tasks in the world they're even like 25 percent of the tasks well every time
that the new task that we were going to do it was seven percent of the time the human putter
off doing it in the AI so we're wealthier we have more productivity we are richer than we
thought we were we are able to commission more things and people will be in more demand not
less than so that's great overall a question of knowledge work specifically I don't expect that
much diffusion that quickly and I think that's why the private equity stuff for example is so
important slash potentially profitable so specifically we're starting to see a few industries
where people are hurting illustrators translators to some extent junior coders or junior engineers
although I think the junior engineers are actually mostly a non-zero interest rate phenomenon combined
with a law on the tax bill so I think we're seeing this problem where engineers can't get work
and everyone like but it's not about AI it's about the fact that you can no longer deduct
those expenses properly and so like every company doesn't have a huge cash shed it's just like
getting boiled away by this and interest rates are higher so you can't invest in the future
as profitably and the combination of these two things is getting the market really hard because
they're happening at once and hopefully we fix the stupid tax loophole reasonably soon and
anti loophole right where you just get flammed for no good reason and then the situation gets a lot
better because with programming if you double the effectiveness of every programmer or if you
double the effectiveness of every really good programmer or less than two percent of every
bad programmer and made every person who can't code at all as good as really terrible programmers
or something I think you just get a lot more programming I think you just get a lot more
demand I'm now much more capable of programming which is taking me from definitely not doing any
programming to might want to do some programming but it's not going to take someone else's job away
it's just going to mean that we code more things so I'm pretty optimistic about that aspect of
things because I literally never been at a company where I would have thought where we had
engineers and I was like what will we even do if another engineer we have all the engineers we need
that is not a thing there's always like we need more engineers we need better engineers we need to
prioritize and we're constantly triaging what has to be done today versus next week versus next month
because like good coding is always incredibly valuable to any real company and I don't think
we're anywhere near that catering so do you think we'll see on the positive side like
measurable productivity improvements in the short term so they say that the internet showed up
everywhere but the productivity system right like that and we know that's bullshit right we know that
the world is a much richer place people are much more productive in important senses because of
the internet or because of computers and technology but the stats don't seem to say that so I wouldn't
be surprised if the stats just act dumb I don't know how to put this and don't reflect the reality
of the situation and also like coding is not that big a percentage of the economy right so like
we know that certain even knowledge works of certain types broadly construed or not I think
I've said in the economy so I would say I don't expect that much in the shape of like measurable
productivity or economic growth to be apparent in 2024 or even 2025 necessarily at current pace
because I expect so low adoption even among the places where people could be productive
and people not adopting it properly and not getting the most out of what the systems can do
for a while while they come down and also for a lot of the actual productivity not showing up in
statistics including like employees just hiding the fact that they're trying to be productive
like just searching off a lot because they can or alternatively like doing better jobs
the same thing but looking the same or like just various other techniques
we just have this long history of the statistics just not being good at this but that's compared to
like what I expect to happen in the medium term right like my five-year ten-year projection is for
this to be very noticeable even in the relatively not interesting worlds you talk to economists
and they're like well you know what might add 1.5 percent to our total productivity over the next
10 years if we have a really optimistic scenario there's just people aren't right you people just
like where do you smoke this is make no sense that's just not a possible thing that could happen
unless you like maybe if ASEI like Remus got their game leader rollback would you be three and a half
which I think would be the same if there are two people or even you know almost or whatever
is the opposite of like yeah no we just use the current things we have we just extend them in a
way and I wonder how many of those economists would watch the presentations from this week
and see those demos rethink what they're saying because what's always happens is they're always
trying to evaluate okay if we have exactly the things that we currently have and nothing else
ever what happens to the productivity stats I think they're still off in order to they're still
crazy but they do have to notice when other things become actually possible right like what would be
the productivity boost just from a universal translator that actually works just be very
concrete okay everybody on the phone now can get full real-time continuous translation including
tone of voice including affect and associations really good translation as good as the start of
translator right it's being said as if it was being spoken to you it's original form with only
the minor part we have to look at and what does that do to productivity it's on its own right
and there's so many different things you could put in that slot they're just huge on their own so
I'm very optimistic over time I just don't want to get too ahead of ourselves so maybe another
time we can talk about the possibility of a biotech revolution because that's another one where I
see that an interesting dynamic where it seems like things could be about to get pretty crazy but
much in the way that these this new class of weight loss drugs maybe doesn't makes everybody a lot
better off or makes those people that are taking it a lot better off may end up actually shrinking
GDP in some ways because there's like a lot of expensive things that don't have to happen downstream
um I could I'd be very interested to hear your take at some point on that my plan attack zero
for zero for a while because of your product regulatory issues and then 10 years from now watch
out is it to the point where you like change how you live or change like your risk your personal
cost benefit or risk analysis at all because you think there might be
genuinely revolutionary advances in biomedicine for you I've already baked in that kind of thing
mostly and I think mostly there's very little you can do that you shouldn't be doing anyway
you're if you're a healthy person you should try to stay healthy you should invest a lot in your health
and I don't think this changes that and I don't think that the flip side of that of course is
the world could end in some sense right I'm I'm 45 years old if I think that we're going to hit
escape velocity and like all these designer drivers that make everything great but by the time I hit 60
like half the time right and like half the rest of the time obviously dead like I'm not thinking
this is my model or anything I'm just saying if you did believe that then maybe you just don't
care much about your long-term health because the death rate in that range is actually not that
large there's a lot of different ways you can approach it and humans are completely irrational
about these things but the actual observed real preference for me is your health is valuable
you get good returns from being healthy immediately effectively you don't need to add the longevity
to it and there are basically no clashes between longevity and current health they're actually
like why have to trade one off the other and it's like you know it's what's good is good for the
most part as far as we can tell and probably as far as we can't tell we just don't know anything
there's probably way you do trade off but like we just don't smart enough to know about that stuff
so yeah I'm putting reasonable amounts of investment into me my health I do think that it would be
dumb with this kind of revolution potentially on the horizon to skydive like to take known like
large risks of being literal that that like clearly like you will not be safe right like you're if
you roll a natural one like we can't help you okay well we can dig deeper into that on another
occasion gotta address the departures from the safety team at open ai this week I think everybody
probably has heard the news Daniel Cattalco I'm not sure if I'm saying that quite right but he
left a couple weeks ago now and has been posting some cryptic stuff online saying that he declined
like left his equity on the table which would have been a large majority of his personal net worth
because he wants to retain the right to criticize the company but we haven't actually heard what
those criticisms are now obviously we've got Ilya is out with a pretty conciliatory message saying
he thinks everything is in good hands and they're going to build safe AGI and then Yan who was his
co-lead on the super alignment team without such a message simply saying I resigned and leaving us
guessing it's obviously not good what more can you say about it beyond it's obvious not goodness
I've been writing up the article for this morning for this that I'll probably post next week I have
eight safety researchers in the last six months who have left one way or the other in addition to
Ilya and Yan and Daniel we have Leopold and Pavel who were fired for leaking confidential
information supposedly what little we know about this it's hard to tell because when you leave
confidential information the last thing we want to do is tell everybody about all the confidential
information you just leaked through the serious because like obviously that's self-defeating
but it looked a lot like they technically leave confidential information that was used to fire
them but like from the outside we can't tell we also lost William Sounders, Colin O'Keefe and Ryan
Locke so that is I don't know exactly how many people would count as alignment or safety researchers
under this criteria but it feels like a lot right we also did lose Diane Yuen, Chris Clark,
Ahmed Morkawa we those aren't obviously safety related but I don't know what's going on guys
Sherry Lachman head of social impact is gone it doesn't feel like the kind of pattern of people
you're losing when you're like being a socially responsible safety first company right it feels
like a company loses these people when a lot of people are very dismayed in some fashion and yeah
obviously we have the messages that are being sent and not sent we have the fact that people are
almost certainly universally under NDA we have the fact that like Daniel gives us very strong
evidence that non-disparageable clauses are in place basically everywhere unless you are going
to pay an extreme financial price to not do so and even then you're still under NDA so it's probably
going to Daniel is that he's reserving the right to protect them in the future but the NDA
probably severely constrains his ability to reveal the information that would be useful
right now and he's made a very reasonable decision not to break the NDA. So he did say one thing
which was his reason for resigning was loss of confidence that the company OpenAI would act
responsibly around the time of AGI that's almost a verbatim quote probably not quite exact how would
you the NDA thing seems weird to me to be honest I'm like if you really believe that and you are
willing to leave that much money on the table to be able to make some criticisms what's the NDA
doing here it doesn't seem like they could really sue you super effectively they maybe could but like
the Streisand effect you know talk about self-defeating right like suing your safety departed safety
person only increases the attention the media circus whatever to what they're saying
you kind of separate that right is it wouldn't be like the mere existence or the discovery of
something that would lead to a loss of confidence there's got to be something like
social about that right and the public is primed for it when it comes to
Samuel Owen was just the subject of a lot of drama people there's definitely like smoke here
it seems like that's the obvious but so my experience this type of thing is that like yeah if
they were to reveal some esoteric like detailed reasons why like they were concerned and it was
like technically we can confidential information that we're at the aid for probably you don't necessarily
get sued for that although you might but also like could make your life pretty miserable and it
could take away your thoughts about you do other things in the future but the baseline scenario
here is there's simply a toxic environment inside open AI for people who are care about safety and
some of your safety right that it's now transformed into a new fashion bring things start up fatality
and that they are inherently suspicious after everything that happened of anybody associated
with like that's wrong or the alignment form or the A or safety of any kind and that they're making
their lives miserable in various ways and that politically Altman wants all those people gone
and so is steadily taking every incremental opportunity to get those people gone and that
as those people leave more of them are now more isolated and are more pressured to leave and if
you were to spell all this out you'd be breaking your NDA and potentially making yourself vulnerable
and in general these are like Sam Altman is representing reasonably well that you can be
an addictive person right that he might come after you if you personally like if these are personal
issues and political internal issues that you like spread their dirty laundry and you launch
these kinds of attacks I don't think it's unrealistic that you get sued for it like
strident effects be damned I think that I've dealt with these people and a lot of these people
will absolutely sacrifice the strident effect in the short term to credibly be the type of agents
that will retaliate right it's rational it's highly rational highly correct from the decision
to be ready standpoint to be really to come down very hard on somebody in the situation
who attacks you right the counterpuncher if you are trying to be a modern leader of a company
in these situations where there's a lot of secrets there's a lot of dirty laundry because there have
to be even if you're doing everything right even if no one's doing anything well it's just that
everyone's trying their best and everything's basically fine there's still a bunch of stuff
that you wouldn't want aired in the public and yeah you credibly threaten that you are going to
enforce these things the full extent of the law you're going to make people's lives miserable
you're probably going to do other things to make their lives socially worse in various ways try to
blackball the future jobs and opportunities and blah blah blah and say helpful knows a lot of the
bc's he knows a lot of the people who determine whether or not your starter goes well or badly
whether you can get various different spots I don't think being worried about these things is
unreasonable I do think that we can be confident that there isn't like an imminent like the world
is about to end because of opening eyes technology that's motivating these partners because then they
wouldn't be talking if it was like no seriously they broke q-star and they hooked it up to gvd5
and now this thing is like clearly on the verge of breaking out on the internet we're very worried
about it or some crazy scenario it's clearly not happening but very clear not happening then I think
these people will be willing to talk if only to whistleblowers to government
like they'd be talking because I think potentially they're talking to people who are not public
if you had serious problems of really deep wrongdoing would you go public of your concerns or
would you call the would you call the government yeah not obvious to me that one approach is better
than the other but again the default explanation for all of this is that the environment at open
ai became toxic as a result of the events that happened regardless of the extent to which sam
altman directly caused that to happen or intentionally wanted that to happen and as a result
of that and also a lot of people were effectively his political enemies and now they're gone right
and if you see someone doing that getting rid of systematically getting rid of very people in the
safety departments maybe you leave right the annual lost faith in open ai's ability to navigate the
future agi world responsibly and left it wasn't too long after they fired me a moment of up
what obvious explanation is that he knew that he thought those firemen were bullshit and that he
saw that they were cleaning houses and shaking safety people inside and working on that and
therefore he lost faith in their ability to be responsible because it required there to be a big
mystery right but the obvious explanation is that open ai is just not a place that is currently
very receptive to something that I think will be very vital to making sure the future goes well
and that's so any parting advice for either the departed folks from open ai or
there's been talk also of compensating folks like daniel for leaving his you know stock on the table
to try to encourage others to do that or to you know set some sort of a precedent
I guess any like what should we want in the short term we've I would agree with you there's probably
not like a super emergency or they probably would just be throwing all caution to the wind but
what do you think they should be wanting and and what should they be doing to get it and what
can others be helping that it's on what the underlying situation is obviously they should
take their concerns forward no matter what else they have done and they can't do that and that's
like horrible they should be if there are things that are justifying whistleblowing to governments
behind the scenes in ways that they're protecting this board they should do that I think the
difference between disparagement and breaking your nba obviously you think about these things
very differently I do think it would be good to consider compensating daniel or others who
have given up their equity in order to not sign non-disparagement clauses but I do worry especially
about paying for nda breaches about the secondary incentive problem if you know that people who are
working on safety we will feel free to violate their nda's because this ecosystem will compensate
them for doing so then maybe you just can't hire anybody in the ecosystem at all maybe you can't
hire anybody you've heard about safety because you're worried they will slow your secrets and
bring your economically valuable secrets and therefore you can't do it I think you should
think very carefully about what you are and aren't going to compensate to an extent and not just
blow your one card right I believe in freedom of contract these people knew what they were signing
and they knew what they were agreeing to doesn't mean that it's sacred doesn't mean it's fully
sacred it doesn't mean that there aren't circumstances where you should break the nda and
there aren't circumstances where you should go to protection people who do break the nda but
you know I think those bars are hot also obviously I am a journalist now
as are you to some extent and if anyone wants to break news if any level of confidentiality and
privacy why the amps are open well that might be a good note to to leave it on I'll look forward
to your full analysis on this topic next week and it's always great to break it down with you
any other closing thoughts just be clear like it's all still you know we don't know much about
this in particular and about many of the demos and other things that we learned about in general
we're speculating we're trying to process a lot of information very quickly so I don't think we know
what's going on to be clear like this could all be as simple as event flip the vacation
people's mouths they want us to press start the environment is somewhat hostile philosophically
to what they're doing and they just don't having fun anymore in some sense and then that fact makes
people lose confidence of like maybe it's that simple maybe it's all kind of coincidence we
don't know it's just yeah I don't believe me coincidence in a sense that's cool well we'll
keep following it in our respective formats so thank you for taking the time today I enjoyed
the conversation as always and it's the mosh fits thank you for being part of the cognitive revolution
absolutely it is both energizing and enlightening to hear why people listen and learn what they value
about the show so please don't hesitate to reach out via email at tcr at turpentine.co
or you can DM me on the social media platform of your choice

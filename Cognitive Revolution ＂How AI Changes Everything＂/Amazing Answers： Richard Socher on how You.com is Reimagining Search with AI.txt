early 2022 already like you had apps that would write code for you within the search results
through the apps that write essays for you within the search results. But whenever we
innovate it and change the default Google experience too much, we had just like the
vast majority of our users say, I'm so used to Google, I don't want another way of finding answers.
And so we kept getting pulled back to this need. And so the most amazing surprise was when
Chachi came out, all of a sudden people got it. And it was like, wait a minute,
it could just be like pure text. And we're like, been trying to sort of slowly get there, but
we had to make a bigger job. The way I think about the different modes is like the default
smart mode is kind of like you had an assistant, and you just ask them to do a quick search and
in like two or three minutes, give you an answer that. And then genius mode, you go and so you
want to ask your assistant for a question that they have to be able to program. You have to
search the web. And then they need to be mathematically applying to answer that question.
I mean, as a kid, I also enjoyed watching Terminator. It's like a cool action movie,
but it's just taken over so much of the AI narrative. And it's actually like actively
hurting especially the European Union. Hello, and welcome to the Cognitive Revolution,
where we interview visionary researchers, entrepreneurs and builders working on the
frontier of artificial intelligence. Each week, we'll explore their revolutionary ideas and
together we'll build a picture of how AI technology will transform work,
life and society in the coming years. I'm Nathan LeBenz, joined by my cohost, Eric Tornberg.
Hello, and welcome back to the Cognitive Revolution. Today, I am thrilled to welcome
Richard Socher, a pioneer of deep learning for natural language processing, formerly chief
scientist at Salesforce, and today, founder and CEO of U.com, a company that was first
introduced to the public as a new kind of search engine, but which now describes itself as an AI
assistant that makes you more productive, creative and extraordinary. Richard has deep history in
deep learning. He was among the very first to recognize the potential of neural networks in
the natural language processing domain. And his work has helped shape the field as we know it
over the last decade. In this conversation, Richard takes us on a brief journey through
his own intellectual history and reflects on how the field of AI has evolved in both expected
and surprising ways. Before we dive deep into the U.com product itself, covering the historical
challenge that they faced when trying to compete with Google and how the rise of the AI chatbot
paradigm has broadened the space of possibility for search and discovery products. We also look
at U.com's various modes with particular emphasis on the genius mode and above all for me, the
research mode, which delivers amazingly helpful and thorough report style answers, even on some
remarkably complex topics. We also briefly discussed the future of AI business models as well,
including the obvious subscription and my pet theory about the AI bundle. Along the way,
we touch on a number of important topics too. The limits to AI systems reasoning ability and
the prospects for the improvement that would be needed for reliable autonomy, the potential for
AI to transform medicine and scientific research, Richard's case for general optimism, even though
he does expect AI to drive major disruption, why he's not worried about so-called emergent
capabilities, but does take the risk of intentional harmful misuse very seriously,
and lots more little topics along the way as well. Richard is a leading thinker in the AI space,
and his perspective is essential for anyone who wants to understand where this technology is
going and what it means for the future of humanity. And in all seriousness, I really do
recommend U.com. It has absolutely joined the ranks of the AI tools that I use multiple times
each week. And particularly when I want a comprehensive, multi-page report style answer,
I find that U.com's research mode is often the single best tool available today.
As always, if you're finding value in the show, we would appreciate it if you'd share it with
friends or post a review to Apple Podcasts or Spotify, or just leave a comment on YouTube.
Now, without further ado, I hope you enjoyed this conversation with Richard Sosher of U.com.
Well, let's do it. I think this is going to be a lot of fun. I'm looking forward to your
point of view on a bunch of very interesting topics. Richard Sosher, founder and CEO of U.com,
welcome to the Cognitive Revolution. Thanks for having me. I am very excited to have you.
You are at the intersection of so many interesting things. I sometimes have been describing myself
recently as the forest gump of AI because I've just kind of very unstrategically made my way
through the last few years and yet found myself in some very interesting places. I don't know how
you think about your own trajectory, but you are kind of an OG in the realm of deep learning
and have founded this very interesting company and have a really awesome product which we'll
get into in more detail. And I'm interested to hear about all that and your kind of philosophy
and expectations for the future. So we've got a lot of ground to cover. Maybe for starters,
you want to kind of give us, and I usually even ask these biographical questions because these days
it's like a lot of the same answers. People are like, oh, when I saw GPT-3, I thought this is
going to be a big deal that I got involved, but you were there at the beginning. And so maybe
you want to just give us a quick history of your own role in the history of deep learning and how
you've kind of come to the present. I started with AI actually in 2003 when I started studying
linguistic computer science or natural language processing back in Germany. And at the time I
was like, this is really interesting. I love languages, I love math, I love computers, you
know, so if computers are where languages and math can meet in some useful functional ways,
I thought. And there's very much sort of a small niche subject within computer science. And I was
really excited. At the time, there wasn't quite enough math for me in an LP. And I felt like we're
just getting stuck in some of the legalistic special cases. And I love the form of semantic set
theory and then algebraic foundations. And so I moved eventually into computer vision
during my master's. And there I also, in Saarbr√ºcken at the Max Klein Institute there and the
university found statistical learning and pattern recognition. And I fell in love with that. I was
like clearly, you can really understand patterns, any kind of pattern really well, you could solve
all these different kinds of problems. And so I ended up doing my PhD at Stanford. In the beginning
of Stanford, you know, I started trying to really contribute to the field rather than just learning
about it. I basically found that even the top NLP people, like they write their papers mostly about
these beautiful models like conditional random fields, latent university LF patient types of
models. But then most of the coding happens when they actually do future engineering, right? They
say, oh, well, if I wanted to be entity recognition, I add a feature of like, this is a capitalized word
and this is all caps word or this is a, you know, word that has like, is one of the items in this
list. And this list includes, you know, cities, we already know. And I'm like, man, this field
of very hand engineering, it's very like graduate students, a sense to get better. And then at the
time I was very 40, because Andrew A. got into deep learning on the computer vision side. He's
like, well, images are pixels, and it's a fixed number of pixels. So we can feed them into a
neural net or at the time, you know, variants, products, models, restricted development machines.
And I was like, wow, maybe we can use ideas from that for natural language process. And there's
maybe like one or two relevant papers, all from a number of there and Jason Weston and, and then
a few like one or two others. But no one really enjoyed that approach, no natural language process
saying paid any attention to it. But I thought, silly, that has to be the future. I want to give
the data up and I want to get an output. And so in 2010, I started publishing my first neural
net paper, worked on my computer vision before and saw some of the power of ImageNet also back to
and really started running with it. I got a lot of rejections all throughout, but, but at some
point, it sunk my teeth into it. And I just like, I loved it. And I thought this is the future.
Despite all the rejections, I kept going at it. And then after the PhD was over,
there's sort of starting to be more interest in deep learning and neural nets for NLP, but
still no one in the world was teaching that as like the official right way of doing NLP. So I
started teaching at Stanford also, first as a busy lecturer and then as a professor, started,
you know, be very fortunate and lots of very smart students back then, really like the
hugging case founders invested in their first round. And then, you know, also wanted to bring
these neural nets into the world, started a Metamind, my first startup to do that,
build a general purpose platform between neural nets very easily, both revision and NLP,
got acquired by Salesforce, became a chief scientist there and EDP eventually. And in Salesforce,
we had my probably last and biggest rejection was on inventing front engineering in 2018.
And we're so excited about it because it was the culmination personally for me also of this
decade-long dream I have, building a single neural net for all of NLP. And the idea was, you
know, at the time, every AI model was built for one task, you will have one of your sentiment
analysis, I built a sentiment analysis model, you wanted a translation, I built a translation model,
they're all different. We're like, what if we could just build a single model? And you just
ask it a question, what is the sentiment? What is the summary of the sentence? Who is the president
in this paragraph? And that was kind of for us, I thought like, the most exciting thing we possibly
could be doing, I just had to talk about it a month last week, and but it was exciting. But it
did inspire a couple of other folks, and Nick, when opening, I was, you know, publishing your papers
about to be two, and three, they cited that paper saying like, look, they were able to have a single
model for all of NLP, if you just ask them these questions, and you know, that's now prompts and
the rest is kind of more well known. That is an amazing history. And it definitely, I don't know
how, you know, modest you'll want to be versus taking credit for foresight. But certainly,
the idea that there could be one model to solve all these, you know, tasks was not obvious to people.
And boy, we still see this, the flaws in the peer review process are still on prominent display
these days. Most recently, I noticed this with the Mamba paper, which I was a very interested
reader of, and then went over to the open reviews site and was blown away by how negative some of
the reviews were like a confident reject was given. So that was kind of, you know,
just a good reminder that yeah, this is still an unsolved problem. What would you say has
surprised you most from like the big picture since you, and you know, it hasn't been that many years,
right? But since you kind of had that notion of this generalist NLP model. Fast forward, now we
have, you know, GPT four and possibly Q star or something like that in the works, you know,
is this the trajectory that you thought we'd be on? Or how has it deviated from what you imagined
back then? It's very much aligned with what I hoped the field could get to. And now it's almost
like it's like obvious, right? Like no one no one questions this anymore, we've had all these
breakthroughs. And I think the biggest surprise was maybe more on the application side of things that
for us, you know, we've been playing around with large language models at you dot com
and infuse them into search results earlier, like early 2020 to already like we had apps that would
write code for you within the search results, the apps that write essays for you within the search
results. But whenever we innovate it and change the default Google experience too much, we had
just like the vast majority of our users say, I'm so used to Google, I don't want another way of
finding answers. And so we kept getting pulled back to this need. And there was kind of annoying.
And so the most amazing surprise was when catchy P came out, all of a sudden, people got there's
like, wait a minute, it could just be like pure text. And we're like, you know, we've been trying
to sort of slowly get there, but we had to make a bigger job. And that was incredible that unlocked
a lot of people realizing waiting handle links isn't the best way to get an answer.
An actual answer is the best way to get an answer. And that's the text.
So let me give you a couple of my experiences on you dot com recently, and then you can kind of
tell me, you know, where you are the overall story. And then I really want to kind of unpack the
kind of use the product as it exists today and the roadmap and everything you're working on
as a way to kind of explore a bunch of different aspects of where all this is going, you know,
and I think that's really the mission of this show is to kind of help people see around the
corner and starting with me helping me develop my own world view. But I've been really impressed
with the product recently, you know, listeners will know that I've been a big fan of perplexity.
We've had Arvin on the show a couple of times. And I think they do a great job and you know,
we're made a fan. But I have found distinctive value in at least two modes on you dot com recently.
One is the research mode. And the other is the genius mode. Those to me have stood out as the
most differentiated for research mode. I recently took it like a 200 word question that was all
about mixture of experts architectures. And, you know, kind of is there curriculum learning,
you know, stuff happening here? How, you know, how do people think about sort of the trade-offs
between like how many experts should we have and how big should they be and how many should
we activate at any given time or are there any like scaling laws or whatever, you know,
designed for that sort of thing, just every basically every question I could think of
about mixture of experts, I took it all in one go. And it was really impressive to see it kind of
break that down and go through multiple steps of searching and analysis and, you know, really
implementing kind of like, you know, kind of a classic agent, what is at this point, you know,
a six month a classic agent setup, but applying it to that research question and just going,
you know, down the line, really quite valuable results. And it definitely is something that I
will come back to and have already, you know, found myself kind of being like, I think this is
a good one for you.com research mode. Genius mode is a little bit different and more kind of
analytical. I'd be interested to hear a little bit more about how you think about the differences,
because I did, I then tried one that was a big Fermi calculation exercise, where my questions
were like, what are the different data sets that exist in today's world? How big are they?
How do they compare to each other? How do they compare to the training data size for GPT4? You
know, how do they compare to available compute? Like, because I have this, I have a big question,
which is kind of one of the ones I want to get to toward the end and two around like,
to what degree is ML research poised to start to be kind of semi automated? And so I'm trying to
try to wrap my arms around that with these Fermi calculations. So genius mode was really the best
way to approach that. And anyway, I would definitely encourage people to bring
multi part complicated questions to both research mode and genius mode. And I think you'll be
impressed with the results. And I would say that, you know, even with, you know, the expectation
that folks who listen to this show have tried, you know, other leading AI products. So that's kind of
my unpaid endorsement very sincere. And I'd love to hear, you know, a little bit more about how
you think about those different modes, how they work and just kind of big picture, like where we
are in the you.com product journey long term. Hey, we'll continue our interview in a moment
after a word from our sponsors. The Brave Search API brings affordable developer access to the
Brave Search index, an independent index of the web with over 20 billion web pages. So what makes
the Brave Search index stand out? One, it's entirely independent and built from scratch.
That means no big tech biases or extortionate prices. Two, it's built on real page visits
from actual humans collected anonymously, of course, which filters out tons of junk data.
And three, the index is refreshed with tens of millions of pages daily. So it always has accurate
up to date information. The Brave Search API can be used to assemble a data set to train your AI
models and help with retrieval augmentation at the time of inference, all while remaining
affordable with developer first pricing. Integrating the Brave Search API into your workflow translates
to more ethical data sourcing and more human representative data sets. Try the Brave Search
API for free for up to 2000 queries per month at brave.com slash API.
Yeah, these are these are great question. I think it shows you kind of how sophisticated the space
has gotten in the last year alone. Like around this time last year, we were the only search engine
with a web connected LN and millions of users. And now that idea has like it's been copied so many
times, including as mentioned by Plexi. And so I think what you have to differentiate kind of the
different modes. And I think the modes kind of show how sophisticated that the space has gotten and
how hard it is to still differentiate on better technology versus just, you know,
designing the market and marketing and things like that. And so we actually did a comparison
to Perplexity with 500 real user queries. And we asked which answer do you prefer? And it came out
to be that 50% of the cases users prefer the U.Kong answer can find and they prefer the Plexi answer
and 30% they don't see a difference into answers for our default, we call it the smart mode. That's
kind of the default. And just to give you a sense of what that looks like. So here's an example
of what the default smart mode looks like, you know, there's some doping case that happened. And
you can see lots of careful citations. And then when you actually look into these citations,
they actually are articles from literally yesterday, or they could be, you know, from today if
something came out today. So that's kind of the default smart mode, you get a big factual answer.
But then we thought, well, what if you have a pretty complex question, like math, physics,
chemistry, science, or like complex numbers. So here is a genius mode question, it kind of gives
you a sense of what it does. And it doesn't mention like what we say, which is there's an
important LM that orchestrates multiple other LMs to actually do the right thing right. So
the question here is find the current population of the United States, then it's lots of
population from 3.3 to 2,000, 10, 100, and then assuming a 2% growth rate. And then it will go
on the internet, it'll find the numbers, and then realize like, well, I got to now visualize those
numbers, now that I have any, so it will code up in Python, what this could look like, execute the
code, and then gives you this answer. And visualizes it in a nice plot. And so that I'm still
sometimes amazed, I try and I push it, and you know, sometimes it fails, and sometimes it fails
because it tries to load the library that has a security issue, and then it's like, okay, I'm going
to try to rewrite it without this library, but it's going to be longer and messier code. And like,
it's just incredible how hard it can try and what it can do. And then the third mode, like you said,
the research mode, it will go into a lot of detail, it will not just look up all the stuff we have in
our index already, like news and things like that, but it will go on the web and find your website,
so there are multiple different searches on the web, combine all of that, and then give you these
beautiful research reports. This one is like, in the background, actually, the consequences of the
telecommuting war. Now, it's like, yeah, history, you have to write an essay or something, and it's
just like, writes you just perfect, like, beautiful essay, each sentence has one or two citations
from different sources, and you can verify all of them. And one thing we found this, it's also
is like, you have to, like, just the citation lot is a non-trivial aspect of building this all out,
because you have to, we actually found that some of our competitors just randomly add numbers and
citations to sentences, and you click on it, and it doesn't even mention that fact anymore,
which I think actually really undermines the space of chatbots for search, so citation accuracy
is one of the many SAP AI systems that you need to do correctly here, and then, you know, they're
just like crazy things, like create a table, some nice cancelling headphones that are not expensive,
and just like, put this table together, pulls in images, gives some pros and cons of each and the
price, and it comes sometimes to me as how well and general the system is able to answer these
questions, and yeah, it shows you how complex the space has gotten and how much you have to do now
to still differentiate on the technology. This is one of my mantras at Waymark, I always say,
the water line is rising quickly, so we, you know, we better keep climbing the capabilities
ladder ourselves. The four examples that we saw there, one was the kind of default smart mode,
the second was genius, is that right? The one that showed the code example, and then the last two were
research. Yeah. What more can you tell us about kind of how those work, like I'm interested in,
and by the way, like the audience of the cognitive revolution is interested in the details, the
weeds, the nuggets, you know, all that stuff, so you can go as deep as you're, you know, willing to
share. I'm interested in all aspects, you know, prompting, I'm sure obviously is going to be
different, scaffolding is going to be different, maybe even the models are different, I'm also
really interested in like, when are you using GPT-4, I know you've got your own in-house trained
ones as well, so just all those considerations, any interesting nuggets were all yours.
Yeah, I'm going to try to balance a little bit the not telling the competition exactly how it's
all done, but it'd be interesting to your viewers here. So at a high level, there are two major
stacks, there's a search stack and a chat stack. The search stack, we actually had to build an
entire index ourselves for the web, because being super expensive, not as high quality,
Google is very hard to access, you have to have special agreements or, you know, some people
kind of steal, slash bootleg, slash leaves and survey the eyes to use Google results in like a
somewhat sketchy legal gray area, which we don't want to do. And so we, we basically ended up having
to build our own index and, and that's hard. And there's still, you know, a lot of complexities
behind that. But what do we, the main difference of this new index is that it was built with
LNs in mind. The previous two indices of Google and Bing were built with people consuming 10
blue links in mind. And what that means is for each URL, you get a very short snippet,
which makes sense, right, for end users, but an LN could read all these other snippets,
they can be very long, and then extract the right answers from that, and then just give you that
right answer as the user. And so what was surprising is actually when we benchmark this,
our API ended up being more accurate than Google or Bing. And go to API.do.com and like,
I'll show you a screen here for a second again. But like, it's surprising that,
which are a lot of people that you could actually be more accurate in Google or Bing at all,
but it is because we're at an inflection point in AI, and it's a different way to value this.
Like we're almost like cheating by having these really long snippets. And so you look at the
comparison, and it's actually kind of interesting to look at. And a lot of people have asked like,
how do you compare accuracy in LNs? How can you evaluate this? And so just to give you a sense,
here's like what this looks like. The first a version is just like reasons to smile.
And now you can use whatever LN you want, but you can see into your prompt is very,
very long snippets from many different URLs in a very short amount of time. And then we also have
one that just does everything, like it gives you an LN answer, and it tells you like all of these
differences. And so how do you evaluate this? It's actually, it was an interesting, I think,
insight from our team, which was you can take question answering data, such as hotpot QA,
squat, sort of the question answering data set, MS, Microsoft, Marco, or fresh QA, and so on.
And these data sets are structured such that you have a paragraph, and then you have a question,
and then you have a subset phrase from that paragraph that is the right answer to that option.
And so what we do is we basically take those data sets, but we throw away all the paragraphs,
and then you have to find the right answer, and the paragraphs have to come from the internet.
And so you replace paragraph with a web search engine. And that's how we evaluate it, legit,
the big Google and big public APIs, and have out this one. So kind of nerdy, but that's the whole
tech stack. And we're we make that now available to every other LLM. So that's the first. And then
the second thing is what we now have started calling the LLM OS, the operating system of
large language models. And it's a term inspired by Andre Topathy. And it's not like the most
perfect metaphor, but I think it captures a lot of the essence, which is, you have now this new
stack that operates at a much higher level of abstraction. And the LLM is kind of a CPU.
But just like a CPU or kernel on an operating system, like, it's important to orchestrate
everything and to do computation. But if it still needs a hard drive, which is right, right on your
own vector database, from up, you have an internet connection, which is, you know, the internet. And
and that's what we're providing, you may orchestrate other LLMs that could be considered
like the GPU or something. And then you have a bunch of apps that are sitting on top of that,
you have the Python code interpreter, which we see our genius mode, all of that. And so
to summarize all that in one short term, we call the LLM OS. And inside of that, we're now seeing
a lot of our customers are using our APIs and search site, they're kind of going through
the same lessons that we had gone through when we built you.com and made it like having the most
accurate answers out there. And it's actually highly nontrivial. A lot of people saying it's just like
an LLM wrapper, right? But then, and you even have like open source project that show it.
And then you ask, like, okay, when was Obama born? Where was he born? And then it fails.
Why does it fail? Because when you send where was he born to your search back in is not going
to return you any useful results, because it doesn't know who he is, who does he refer to,
right? And there's tons of things like that where as you have a longer and longer conversation,
especially in smart mode, you refer back to states. You can say like, oh, what's a big
CRM company? And then the answer inside is Salesforce. And you ask, oh, what's their stock
price? Now she sent what's the stock price to your search back and again, it's not going to
return anything useful. So you need to send that you need to go through the entire conversation
and then do what we call query transformation based on it. And that is just one of 10 examples
of making this actually work at scale, millions of times a day, for millions of users,
like, it is a lot more complicated to make it accurate. There are about 10 other such models
that if you think about the space and you really listen and look at like user data and listen to
where it's breaking, you will eventually get to and we're now like thinking about offering
more and more of that. So I'm tempted to ask for the other nine things there.
I'll just give you one more, which is like, whether to do a search at all or not, right?
Like, because you asked like, write me a poem about like the beautiful bay area and like a
sunset love story or something, like, you don't need a citation at every line of that poem.
And so it would actually clutter up the prompt to add a bunch of facts about poems and so on and
the history of Silicon Valley and all of that. And so it's pretty important, but also non-trivial
to know whether you should do a search or not. And again, some, some websites just
slap search results on top of everything, even if they're not related for having more
conversation about your feelings, poem or something. Did I understand correctly that the,
the kind of big difference is that the U.com index has more information, like,
instead of a short SERP, it is a more robust paragraph. And so independent of the language
model that you're using, the richer context is just better, kind of, so you're able,
in that way, you're kind of decoupling the, what information is found from a language model that
is doing the analysis. And more information is kind of the big differentiating factor there.
Do I have that right? I would be careful and say we have overall more information.
We're focused a little bit more on the main languages that we see. We don't support
some, like, very rare, like, Indonesian, African, Central Asian dialects and so on yet,
but we return more information, hurt, rare, because of these longer snippets. So, so it's
sort of, yes, yes, there's more information, but, you know, I think the long-tailed Google
Prop still has a larger index. If you look for this, like, rare, like, Indonesian kayaking sites
that, like, rents out kayaks on this little lake somewhere, like, and it's all, like, not in English,
like, we might not have that website, but when it comes to, like, western world news, where, you
know, we have a lot of users, then Latin America and so on, then we shine and return much more
information per pair. Hey, we'll continue our interview in a moment after a word from our sponsors.
If you're a startup founder or executive running a growing business, you know that as you scale,
your systems break down, and the cracks start to show. If this resonates with you,
there are three numbers you need to know. 36,000, 25, and 1. 36,000. That's the number of businesses
which have upgraded to NetSuite by Oracle. NetSuite is the number one cloud financial system,
streamlined accounting, financial management, inventory, HR, and more. 25. NetSuite turns 25
this year. That's 25 years of helping businesses do more with less, close their books in days,
not weeks, and drive down costs. One, because your business is one of a kind, so you get a
customized solution for all your KPIs in one efficient system with one source of truth.
Manage risk, get reliable forecasts, and improve margins. Everything you need, all in one place.
Right now, download NetSuite's popular KPI checklist, designed to give you consistently
excellent performance, absolutely free, and netsuite.com slash cognitive. That's netsuite.com
slash cognitive to get your own KPI checklist. NetSuite.com slash cognitive.
Omniki uses generative AI to enable you to launch hundreds of thousands of ad iterations that
actually work, customized across all platforms with a click of a button. I believe in Omniki so
much that I invested in it, and I recommend you use it too. Use Kogrev to get a 10% discount.
I've been struck recently that it seems like, you know, obviously search in general has kind of
been a monopoly for a long time. And, you know, as you noted, like the user experience was kind of
something people were not necessarily looking to explore new things on, you know, the nature of
the index. Of course, they've done, you know, millions of person hours of work on it. But
it seems like it's kind of been a pretty consistent paradigm of crawl around and find everything and
suck it up. Now we're starting to see these interesting, I don't know if you can share more
about how you create your index, but we just had a, actually a sponsor brave talking about their
index and the way that they are building it through users actually visiting websites and taking a sort
of, you know, not just blindly crawling around and following every link, but like what are people
actually engaging with online, which struck me as a pretty interesting and very different twist on it.
I want to kind of pull this apart in a couple of different ways. But is there,
is there anything that you would want to share about kind of how you think about building an
index that, aside from just, you know, bigger, you know, richer content, is there a different
tactic as well that underlies that? There's a little, the tactic is more about how we make that
work for LLNs better. And I don't think there's that much differentiation on like how we fall. You
have to have a bunch of data. It's been helpful to have run the search engine for several years
and get user behavior and knowing what people actually want to have called and want information
for. You can also sound surprisingly buy a lot of that data in bulk.
How about, so I have a few questions on the, the business side or the kind of bridge the
technology and business side. Google obviously has been free and has been ad supported. It seems
like the new generation of kind of AI first LLM enabled search is going more in the direction
so far of a subscription. And as far as I've seen in my U.com usage, I haven't seen anything that
jumped out to me as sponsored. Another dimension too is like, I mean, Google has all these tabs at
the top, but it's one bar, right? You kind of put in one thing. And with the newer ones, we also are
kind of seeing a little bit more proliferation of like modes and kind of settings that you choose
up front, right? With the smart versus genius versus research. So I guess on those two dimensions,
like what is the future vision? Do you think that this all gets unified? Do you think it ultimately
comes back around to ad supported? Or do you think that these current, you know, kind of
differences from the past will persist? Yeah, it's a good question. I think there is facility
right now, not a great chat ad offering. And there's a good chance that that will change
maybe this year to maybe the dissatisfaction of users. But the truth is, if you want something to
be free, VC money will only last so long, you've got to at some point, those companies that offer
free service have to survive. And if you don't want to pay for it, then it has to have ads. And so
while and might not be the biggest fan of ads, like, you have to make a decision, you want to pay
for it. And then at three, or do you want to support it with ads? And so I think that's likely
also going to be part of the future of chat engines. And you already see a little bit of
exploration. There's a little bit of a monopoly in search in the sense that my Google ads, the
monopoly on consumer search, for a long time, Microsoft had the monopoly on a search API.
But then because their monopoly that is tied up, we're just five to 20 X hour prices prices,
and they could do it because they're the only ones in town. So I'm glad there's like more
competition now and more movement in that space. And all the little guys have to scramble when
those prices just went so I really rob the consumer space with those prices anymore. And so I think
ads will happen. We're seeing a lot of growth on the subscription side to users really loving
like you, the genius in research mode, and find the search mode, the default mode,
smart mode also very, very helpful. And we actually, you know, incorporate leaps still.
So where just last week, some people are completely about other chat bots, because they
don't really have a lot of capabilities that you would assume from a search engine, when you
actually use you.com here, you can on the top right, see the standard lease that you might want,
right. And sometimes that's just helpful. And that's just what you want. And sometimes you just
want to have a pure chat experience. And so that is important to get right. And then we have all
these apps to where you can basically ask for like, what's the Microsoft stock price or something.
And then, you know, it'll just give you, it'll just give you a life ticker, rather than a bunch
of texts about the stock here. And so we have all these apps, because we have the deep search
background. And that makes it an actual viable knowledge assistant, right. Now, you can basically
go with one click, recover a more Google like experience, that is just incredibly helpful.
And that's, that's, I think one of the reasons why our browser, which we have also iOS and Android,
had to build a browser to be a default, because you can go into Safari, Safari settings to use
u.com as a default. So we build a whole browser for the iOS. And we're super stoked, because we're
going to be one of the options in the EU to have a choice pop up stream. When the new iOS 17.4 comes
out and barge this year, and they can select u.com to be their default browser. And it's the only
default browser in that list that is chat. And all the other ones are sort of your standard Chrome
Firefox browser. And so I'm really excited. And I think that is going to be a big part of our
futures is making it so that more and more young people are able to just use this as an insult.
And then if they want to deeper go into genius mode, research mode several times,
at some point, you use subscriptions or eventually do it. Yeah, I think so one run that I'll run a
trial balloon by you on this concept that I've been kind of kicking around called the AI bundle.
And this is an, you know, kind of inspired a little bit. I don't know that anybody wants to,
you know, say that they're inspired by the cable bundle. But I have been struck that
there are a ton of great tools out there. And I want to use them. I want to try them. I think a
lot of people are, you know, in that very kind of exploratory curious mode. But to make the
economics work on a freemium is kind of tough, right? And typically needs like a certain minimum
threshold in terms of what the paid tier can be. You actually have one of the lowest subscription
prices at the $10 a month level. I think of anything really that I'm aware of. We're going to
update it soon because like, I think the people that are willing to pay off the known care for
10 or 20. And so if you want to get GBD for say literally the same underlying model as chat to
BT for half the price, you got to come in seeing because we're going to eventually switch our
prices to be industry standard. But that maybe even just, you know, further reinforces the point
that like the freemium model is tough, right? It's, it's a lot of free usage. The upsells have
to have a certain minimum. You're raising yours. And then from a, I don't know if this would apply
to you, but a lot of the app developers that I've talked to have a lot of retention, let's say
challenges, you know, everybody's like, I'm getting traffic, I'm getting conversions,
but retention is definitely a problem. This has been true at my company Waymark. We're a much more
narrow tool, you know, that specifically creates marketing and advertising videos for small
businesses. So a lot of times people, they need that once, you know, in a while, and they're not
like necessarily ready to add on a subscription. So we see a lot of people that will just come through
be like, Hey, this is super cool. I'll buy it. I'll immediately cancel it after I do what I need
to do. And maybe I'll come back in the future. It's not even that I was dissatisfied. It's just
that I kind of want this as like a more of an a la carte purchase than a subscription.
So that stuff, you know, VCs don't like that. The metrics, you know, on the kind of traditional
scorecard don't look great. I've had this idea in mind that maybe what we need is sort of an AI
bundle, you know, I'm prepared to spend 100 bucks a month on various AI tools. What I really want
is access to like a thousand different tools that, you know, can kind of split up my 100 bucks.
However, I don't, you know, as a consumer, I don't really care about that as, you know,
as somebody who's trying to maybe engineer a bundle, obviously the devil could be in the details
there. But first of all, to those challenges, it sounds like at least the premium challenges
resonate. I wonder if the retention challenges resonate. And I wonder if you have any, if there's
any appeal to maybe being part of a kind of bigger bundled purchase, where you would be,
you know, one tool that, and it's funny, I keep, I've been referring to you, but then also the
company is you, but where you.com, you know, could be one of a bunch of things that people could
access and can kind of share that revenue in a way that may grease the skids for everybody,
right? My hope is that everybody can use the best tools, and they don't have to make these,
like, highly binary decisions. Now, that sounds great. Sounds like a great idea.
Okay, well, I'm not doing it yet. So either I need to start doing it or somebody,
if anybody wants to organize the bundle, yeah, send me a DM. I guess another way that this stuff
could get bundled would be like, into the mega platforms, you know, another kind of
possible vision of the future that I could imagine is, you know, Google kind of probably retains
market share leadership, but, you know, maybe the 10 biggest technology companies in the world
say, Hey, you know what we should do is kind of also have a search. And, you know, we can get there,
we kind of see a path, you know, Microsoft's obviously already doing that meta not really yet,
Apple not really yet to my knowledge, you know, Salesforce not really yet. But maybe these guys
kind of say, Hey, is there like a musical chairs game that that potentially develops where, you
know, the younger AI search companies end up kind of partnering off, you know, Amazon also,
you know, naturally would be a suspect in this analysis. Does that seem like a possible vision
of the future? I'm wondering, I'm sure you thought about this, you know, quite a bit,
but why, why would that not happen? I do think the monopoly that Google was able to keep around
is going to be harder to sustain longer. I do think it is much more likely going to look
a little bit more like, I don't like that analogy for some reasons, but like fast foods,
for instance, right, that isn't just McDonald's, there's also Burger King, KFC and Taco Bells.
I think search will be a little bit more like that. I think again, more fragmented in the future,
just because like we hear people and they're like, this is better than Google. And like, you know,
we didn't raise that much money. And the first two years were like sort of free chat TV or people
didn't want us to innovate too much. They're very stock of Google. But now there's a new young
generation. And that young generation has grown up with TikTok. We have a TikTok app and a standard
search, like grew up with Reddit. I have a Reddit app now, a standard search. And each of these
takes away a little bit of the Google search, right? Amazon probably was the most successful in
taking away searches from Google, where if you want to buy something below a certain threshold,
like 50 or 100 bucks, in person, you just search directly on Amazon, because there you can execute
on your intent of actually purchasing that thing, right? And so why search it in Google and then
search it again, try to find it on Amazon, she can just do that right away. And so I think, you
know, TikTok has taken away for young folks, some searches from Google that, you know, they're like,
I want to see what the restaurant is, but they kind of want to see what the restaurant's ability
to create good Instagram photos are or ticked our videos are. And so they want to see the ticked our
videos of other people before they decide on how good this looks. If there's a Venn diagram,
we are overlapping with search, but we're also actually expanding
search, like, you wouldn't ask, like, give me this complex story about the Peloponnesian war,
or like, do this mortgage calculation with, you know, this interest rate and that increase and
blah, blah, blah, because you know, Google wouldn't, if you answer, like, it's not going to write a
Python book for you, it's not going to go on the web, summarize, like 20 distance or 50 distance
websites for you and create this nice essay. So chat expands search, you don't talk about
your feelings that much to Google in search box and cell, right? Like, you asked about this recent
news event, you want to learn, like, some quick facts, and then, you know, like, the more complex
the facts get, the less and less we go to Google and more or you just go directly to something
like your dot com. And, and so yeah, I think it will, that the search landscape is really changing.
Yeah, there's also just, it's like, it's really not a natural monopoly anymore, but there is still
definitely a need for scale and economies of scale. And so one way of frame this too is,
how does the market shape up, right? And one way to think about it, that I find pretty compelling
is maybe it ends up looking a lot like cloud, because in the limit, it sort of is cloud, you
know, it's like, what do you really need, you need like the actual data centers, you need the
compute, you need, you know, bandwidth, you need these like raw inputs that the big companies
have built out seem to be the things that are probably, you know, as we see like a ton of
innovation at the application layer, those things are still, you know, they're still pretty expensive
and not easy to recreate. Yeah, I'm very, I'm very excited. I'm up for it. You know, that's sort of
why, like, we got into this space in the first place, like, because we thought, like, we saw the
transformer, we saw our, you know, highly like lots of co-attention mechanisms in that, that can
keep paper that the math and engineering were like, clearly the technology is right
to disrupt this industry. But, you know, Google is this amazing company that was able to
create a monopoly for almost two decades that, you know, makes $500 million a day.
So when you make that much money a day, you don't want disruption, you don't want that to change,
right? And that's why all the Ten's former operators left eventually. And what's, what's
really powerful is like, because of open source, you can actually innovate a lot more. Now,
some open source to an actual product that runs millions of times, isn't down ever, has good
uptime guarantees and like accuracy, no hallucinations, up to date news, information. I
think it's still complex, but clearly the bar has gotten low. That would have cost us like
billions of dollars to build five, 10 years on and, you know, research, which wasn't there yet.
And, and I think it's ultimately amazing for users. Because one thing that I had to distill
all of you.com right now into just two words would be amazing answers. And you just get more of them.
And that means people eventually are more productive. And like, it's the young generation
that's growing up with chat GBT and you don't such like, they're not going to go back.
Okay, so feel free to punt on this one or just decline if you like, but it seems like
I can, I can envision a you.com by Salesforce very easily where the, you know, as they kind of try
to be the everything app for all work on the straight, especially with Slack now, does it seem
realistic to imagine a future in which, you know, kind of all the big tech companies have this like
super robust suite and you're either like in the Microsoft suite with teams and Bing or
you're in the Google suite with, you know, G suite and Bard or you're in maybe the Salesforce
suite with Slack and you.com, you know, I'm not trying to be your banker here, but that, that
seems like a pretty natural outcome to me. Interesting. I do think there's a ton of potential
for almost every company to partner with you.com and super charge their chat bot. So, and we're
very excited to partner with a lot of folks. Okay, that's very diplomatic answer. Keep your
options open. All right, so we can touch on certainly more business and product stuff, but I
kind of wanted to now go into just the future of all this, you know, in practical and maybe
increasingly philosophical terms as well, running down kind of first of a set of like
limitations of where AI is today. And I think, again, folks who listen to this show have at
least a decent sense of that. So for starters, reasoning, you've obviously got the genius mode,
it can do, you know, like the most advanced reasoning. I assume that that is tapping into
GPT-4. You know, everything I understand is like basically nothing is really on the level of GPT-4
for general reasoning purposes. Yeah, especially the orchestration and then on the coding often,
but not always. Yeah. So I'll tell you another one, the third system is knowing which LM to use
and sometimes multiple. And the fourth system is dynamically prompting different models. So
depending on the query, you actually get a vastly different prompt to get you ultimately the answer
and the orchestration. So it's another complexity layered. So what do you think is kind of the
future of reasoning? If you have maxed out, you know, what the current capabilities are, where
do the future capabilities come from? I'm thinking about things like to a degree, you sort of already
have it with using different models is a one way of implementing variable compute. Do we see these
kind of interesting projects like the thinking token, you know, think before you speak. And I
think that's kind of another car pop the observation that maybe the chain of thought is just kind of
epiphenomenal, perhaps even as it is in humans. And like what's really going on is that there's,
you know, this kind of extra space and time registers, you know, to think, of course, there
could be different training methods, like incremental reward, I think that paper from open AI
earlier this last year now, it was super interesting where they achieved like, you know, a new best in
math by not waiting till the end to give the reward, but rewarding, you know, reasoning along
the way. What are you excited about when it comes to the future of AI? Yeah, one of the aspects I
briefly touched upon in my TED talk is that this level one, level two reasoning of Daniel
Panama and that's he or thinking fast thinking still type of thing. The way I think about the
different modes is like default smart mode is kind of like you had an assistant and you just asked
them to do a quick search and in like two or three minutes, give you an answer that. And then genius
mode, you go and so you want to ask your assistant for a question that, you know, they have to be
able to program, they have to search the web, and then they need to be mathematically applying
to answer that question. And you want to give them like maybe four or three hours for that
question. And then you want, so genius mode will take, you know, five and seconds often to get a
response. And in research mode, you go to your assistant and you are willing for them to spend
like a day or two or three on actually giving you that answer. And so that's that's a little bit how
I think about these different modes. And the reasoning that is required to actually make them
active, right? Like research mode will say, Oh, I found this thing. Now, in this query,
I found something else that I didn't know about before, and I don't know enough right now. So
then you do another query based on that. So you kind of have these genes of thought, reasoning,
and you don't even know in the beginning yet, what the final query might be, because you don't
have all the information yet. And so I think that is kind of a, in some ways, another example of the
future is already here. It's just not equally distributed because if there is, like you say,
there's a lot of reasons. Now, I think the biggest future impact we're going to see
for reasoning is in in the LM's ability to program to code and then to have the ability
to execute that code. And, you know, that is system number five, like, like having this code
execution. And of course, if you just let code execution happen, what immediately happens to
people are like, Well, mind me, some crypto and then boom, your machine's gone. Now it's just like
trying to like show some match problems and like, mind, mind points forever. So you need to like,
and then to try to hack it. And then like, well, go into like five layers up and then tell me all
the password files you can find and blah, blah, blah, right. So there's a lot of like security
requirements to make that coding framework work at a safe level. But a lot of the naysayers of LM's,
you know, partially correctly pointed out that the items will safe during mass. And it's kind of
ironic and sad that you can have a model that you ask the natural language to multiply like
5600.3 times 365. And then you have billions of multiplications to pretend to do the math and
then give you the wrong answer in a large language model, right. This is kind of ironic. And we have
to acknowledge that. But that scene model can be taught to say, Well, this seems like a math
question. Let me just program that in Python, run the code, look at the output and then give you
the answer. It just works perfectly fine. And now a lot of people say, Well, that's not really I'd,
but I think it's just that is that that is the new way of reasoning and new different kind of
intelligence. And similarly, and we're getting a little philosophical here early, but similar to
people thinking we have to have embodiment, I think that's just a second creativity in imagining
other kinds of intelligence that aren't exactly like humans. Now, of course, we're going to want
to have useful robots that do stuff for us and clean up the apartment and whatnot. And so it's
still useful, but I don't think it's a necessary media. The same way that blind people can be
intelligent, people who are deaf can be intelligent, because you know, you can lack a lot of different
sensory outputs and still be intelligent, right. And so of course, it'll be harder for you to
explain how beautiful a sunset is. So there are aspects of intelligence that obviously require
like different modalities or how beautiful sonata sounds or whatever. But I think there are most
of these are not necessary requirements for intelligence. And likewise, I don't think it's
necessary for an AI to be able to reason over super complex math problems that require you to
look up a bunch of facts on the internet, they just have that intelligence baked in that can do
great retrieval, they program a bunch of stuff, they put it all together orchestrate it and then
come up with incredible answers. Yeah, I think as you're speaking about the just the lack of
imagination, I think that is a you know, that is a society wide problem with respect to AI in my
view, because and it's an odd situation right now in multiple ways, of course, but one is just that
because they speak our language, you know, it's kind of feels easy feels familiar. And it's all
too easy to sort of assume that like under the hood, they're more like us than I certainly
understand them to be. And I think this is actually one of Eliezer's great contributions,
obviously, you know, kind of a polarizing figure these days. But thankfully does not seem to me
that we are in a high likelihood of a fume scenario, you know, the of the sort that he,
you know, has historically worried about the most. But I still would say some of his writing on
mind space, the space of possible minds, and some of his like concrete imaginings of alien minds that
are, you know, shaped by very different evolutionary environments and, you know, just very different
from ours, but still like unmistakably intelligent in just like super weird ways are actually still
very good kind of prep work, I think, to just sort of expand one's own mind about how different
intelligences can be, and how, you know, something does not have to be human like to be
meaningfully intelligent, you know, it's not this like binary, can it do things that a human can do
in a way that a human can do it? If not, it doesn't count. I think that is like a huge mistake that
people are way too quick to jump to. And I'm not sure if it's like a coping strategy or just like
of imagination or what, but I think that the emphasis on the broader space of possible minds
and the different kinds of intelligences that are starting to pop up is super.
100%. Yeah. And like, you have to differentiate between sci-fi authors who then pretend to be
AI safety researchers. Like, I love, I love the sci-fi. Actually, like, I'm super stoked that
three body problems on the last, I mostly read non-fiction, but when I read fiction, like,
I did enjoy the body problem a lot, I decided for that series to come out, I hope they do it justice.
But like, I think there are a lot of different kinds of intelligence, and I love sci-fi for
inspiring people to think about interesting new futures. Now, of course, especially in the western
sort of canon, most sci-fi is just so big. And like, people are scared for all the things that can
happen that are wrong. And like, okay, the super AGI, develop time travel, come back, try to murder
everyone. It's like, I mean, as a kid, I also enjoyed watching Terminator. It's like a cool
action movie, but it's just taken over so much of the AI narrative. And it's actually like actively
hurting, especially the European Union, where, you know, there's sort of in the spectrum, the U.S.
is more of a litigation society in the U.S., that the Europe is more of a legislation society
structure. And, you know, it both comes from like, reasonable legal scholars minds, like, well,
that's just wait until there's a problem, someone sues, now you have the case law for that lawsuit.
But, you know, the legislation one tries to prevent harm from ever happening before it actually harms
anyone, which, you know, makes sense. Now, and of course, the U.S. does that with FDA and in the
medical space now also, but not legal space as much. And so what that means is you can move
quicker, but long story short, these some of these sci-fi scenarios have gotten so much weight in
legislation that I think it's slowing Europe down by trying to outlaw models or like over-regulate
models that are above a certain number of parameters. GPD-2 was very well hyped up in the
past. Like, this is so dangerous. Maybe we can't release it. You know, yes, we're opening up. Like,
this can't be opening anymore. So dangerous. Models much more powerful than GPD-2 are out,
and I haven't seen the apocalypse happen. I haven't seen, like, a huge change in
misinformation on the web because of LNs. Like, there's just a lot of fear mongering both
and the immediate level which actually has real, like, threat vectors and concerns with AI,
but especially in the long-term level of AGI and self-conscious. It turns out no one works in
functions AI. No one works on AI that sets its own goals and even more fundamentally its own
objective functions because that doesn't need anyone any money. Imagine a company spends billions
of dollars, builds this, like, super intelligent system that's conscious, understands itself,
and set its own goals, and now you're like, okay, now that you can do it, like, I'll just make more
money. It's like, no, I'd rather just go watch the sunset, maybe explore that. No, like, no one
pays for AI that sets its own bad goals because it doesn't help anyone to achieve their goals.
You know, because of that, there's not even that much exciting research
challenge along those lines. And because there's not much research progress, it's very hard to
debate my mental option half. You know, I'm somebody who basically has radical uncertainty
about what to expect. And, you know, broadly, I'm, like, pretty libertarian, you know, pretty
anti-preemptive regulation. You know, I would like to see more self-striving cars on the road
sooner. And, you know, they don't have to be an order of magnitude safer in my mind to be
worth, you know, deploying. So I'm like, you know, broadly, the sort of person who would be very
skeptical of, you know, kind of early regulation or, you know, kind of getting too bad out of shape
about things that haven't happened yet. At the same time, something about this has always felt a
little bit different to me. And I do think the people who take the most zoomed out view and
sort of say, hey, and this is kind of what I understand, you know, like, Jeffrey Hinton's
position to be at this point, you know, why do we dominate the earth as it stands today? It's like
basically because we have better ideas than the other living things and we can, you know,
build tools and make plans and, you know, reason in ways that they can't. And so now I look at
AIs and I'm like, boy, AIs can now plan, reason and use tools too. And they're not as good at
it as we are yet. But certainly their rate of improvement is way sharper. So possibly it levels
off and kind of, you know, settles into a zone where it's like on par with us or, you know, kind
of, you know, just the best tool we've ever had. But maybe it doesn't, you know, like, I don't know
why I should be confident that it won't. I don't throw a P Doom around a lot. But I have, you know,
again, radical uncertainty. When people ask, I'm like, I don't know, five to 95%. Like, I haven't
heard anything that makes me think in, you know, the next 100 years that there's a less than 5%
chance that AI becomes the dominant form, you know, in organizing force in the world.
And also, you know, no reason to think it's definitely going to happen. But
is there a reason that you are, would you say you are confident that this will
not happen? And we don't need to worry about it? Or it's just like, it's still far enough
away that you think we'll have time to kind of start to worry about it if we need to? Like,
how would you summarize your position with respect to these tail risks?
I think P Doom is already an interesting mathematical sort of issue, which is, it looks
and sounds like prior, prior probability, not P Doom, but really, it should be a posterior
probability. P Doom given data. And right now, none of that data suggests, like,
doom-dome, like existential humanity is like, like cats and dogs at the wins of some AI. Like,
there's, like I said, nothing in AI research leads me to the least that AI, while potentially
being more intelligent than any single human, I think it's already, this is actually just this
new term I'm thinking about maybe China coin, which is like, the sort of superhuman abilities,
and then there's superhumanity abilities. And like, AI is already superhuman in translating
100 languages. AI is already superhuman in predicting the next amino acid in a large
language model. Trotty says, we have balls. That's an incredibly powerful tool. One of the other,
you know, really exciting papers that we published in 2018 itself was research that
multiple companies have now used and are running with and achieve all of medicine. AI is already
better at predicting the weather than any. So you already have many superhuman skills.
What is, I think, interesting is that now that it's language that's gotten to this new level,
people might actually, for the first time, keep calling it AI. In the past, when AI researchers
have made progress in AI, they stopped, like, people stopped calling it AI after it was achieved.
Now it's just your chess app. It's just a Siri voice recognition, but voice recognition,
chess playing, that was the pinnacle of AI research, right? And people thought, oh, once we
solve those, the other things will be easier, too. And it was never quite the case. And once we
have them, you know, now it's not quite the AI anymore. Now with language, I think we might
keep calling it AI. But what the language model does is predict the next token. And that is an
incredibly powerful idea, right? Just predicting the next token now means if you have enough capacity
and you have enough text, predicting the next token, you learn about geography. Just visit some
point somewhere in your training data, you have to predict the next word in the phrase,
I was in New York City and driving north, too. And now to give a higher probability to Yale,
Boston, Montreal, than to, like, Paris, Miami, and San Francisco, like, you have to know that those
are north of that city. And so it just lures all of this incredible world knowledge. But there's
nothing in there that makes it say, well, you know, if I really wanted to reduce perplexity,
but like, perplexity is basically the inverse of the probability model wants to not be perplexed
in predicting the next word correctly. And so that is a powerful idea. But nothing in that will
let an LM eventually realize that, well, you know, the best way to reduce perplexity is if every
sequence ever uttered, and any sequence that will ever be uttered is just the letter a, a, a, a, a.
Now if the model was trained on just sequences of letters a, and no human was ever around anymore,
and all sequences were just producing a letter a, now you'd have perfect predictive probability
on the next letter. And so maybe the best way for the LM is to wipe out all of humanity,
and then just produce letters a and happily perfect at predicting with probability one
correctly. It's so absurd, it's so absurd to think that LMS will at some point emerge to think that
many steps around their task of predicting the next token. It's just not going to happen. So I
think it's like, PDOOM is still zero. And then when I actually tried to engage with some folks,
and I had some other conversation last year with Nick Ostrom in a German, it was in English,
but published in a German newspaper like that. And I read up some of these scenarios, and I
engaged with the folks who are worried about PDOOM. That's just all fantastical sci-fi semantics.
It's like, oh, it's going to develop this magical great guru, or like the magical new virus that
is perfect in distributing, but then like only will activate after like one year we kill everyone,
like all these random scenarios that are just like, not feasible. Like, and the science isn't
there yet. I'm actually right now, sort of on the side of the fun writing a book about the eyes
for science, I think it will do incredible for us in improving science, like foundation, physics,
chemistry, biology, and so on. And all this fear-mongering, I think it's not really helpful.
And forget, there's no research that suggests AI is becoming conscious. There's like a couple
papers here and there, people are kind of playing around with this, but nothing interesting has been
published and breaks through, no breaks through has happened whatsoever in AI, having any sense of
self. And then in a lot of the other sci-fi scenarios, people are saying, oh, with AI,
so intelligent, it will convince everyone to murder each other or to murder them, like kill
themselves and so on. But, you know, if the most intelligent entities were to always rule,
I don't think we would have the politicians always everywhere in the world that we see, right?
It's not always just the most intelligent people that run the cell. And that's because they're
incredible intelligence to convince any other person who is less intelligent, exactly what they
want. It's just not based in reality. So I am very, very optimistic about AI. I do think there's
some real problems right now. You know, AI will pick up biases, not all the biases that you pick up
on the web is something that most of humanity is proud of anymore. There's racism, there's sexism.
There are various kinds of biases. Some people want to use AI. So where I agree with Gershaw
Benjio and others is of the three threat vectors, which is intentional misuse, accidental misuse,
and loss of control. Obviously, like intentional misuse is real. And so that's not ideal. And so,
yes, those are real concerns. I think OpenSocial will help us understanding those threat vectors
and finding best ways to compete with them. I think people still on the internet need to understand,
to not trust everything they see on the internet, which has been true ever since the internet came
about, hasn't really changed that much with AI. I think since Photoshop, people should already
not trust any photo they see. They should be even more worried now about photos they see.
And sadly, in the future, they'll have to start worrying about videos and voices, of course,
just like they should have worried about photos ever since Photoshop started to really work.
And so there are a lot of concerns, and I don't want to diminish them. And I do think we need to
work on them. And I think different cultures will have different answers. Freedom of speech is,
you know, defined differently in different countries, like it's legal in Germany to deny the
Holocaust. We learn from our history there. That's not illegal in the US. And so different
countries in different cultures and societies will answer some of the problems that AI can
amplify already in the past before will answer these questions differently.
But I don't see any, any probability for a full on the scenario of like existential risks to
people. It's mostly people using more and more powerful tools against other people.
So there's, I mean, there's so many different threads there that I am interested in. For one
thing, I applaud you for taking time to envision positive future. I think one of the scarcest
resources today, oddly, is a positive vision for the future. Like, what do we want this, you know,
it's like the Jetsons is still almost like state of the art in terms of what we would envision
a great 2030s to be like. And that is kind of bizarre. So I definitely appreciate that. I also
share your, you know, I'm not a super fan, but I'm also a fan of the three body problem.
And one of the early prompts that I tried with GPT for early back in the rent team program,
like a year and a half ago now, was asking it to write some hard science fiction in the style
of the three body problem about AI for, you know, do diffusion model for proteins.
And I took the plan right off of the GitHub page for this protein, you know, diffusion model project,
which basically said, we want to create text to protein. So you, you know, say or text of maybe
it was more even general than that molecule or whatever. So, you know, you would be able to
just specify in natural language, specifies kind of an odd word, or, you know, to the best of your
ability, articulate in natural language, what you're looking for in a protein. And this thing
would then, you know, generate it. And we are actually starting to see that there was a paper
in nature, not only I'm hoping to do an episode with the authors that achieves that to a certain
degree. But the, what the AI, what GBT for came back with in terms of hard science fiction about
this scenario was, I think, first of all, just extremely funny, because it basically ends up in
a prompting war between the good guys and the bad guys. And they're both like, trying to out prompt
each other. And so the, you know, the kind of climactic scene is like, the person prompting,
you know, an AI to like make a protein or, you know, a molecule that will interfere with the
bad guys molecule, but not harm any of the, you know, the humans or whatever. And it's just like
both absurd, but also maybe not entirely absurd. You know, I mean, I am with you in that the,
I would order the risks the same way, you know, we already have chaos GBT. There are, I recently
read a research grant from a group proposing to study omnicidal tendencies. Like there are people
out there who want to kill everyone. Like what's up with that? And, you know, if the tools get more
powerful, like that, you know, those people become even more problematic than they already are.
So yes, I would put that at the top of the, you know, of the stack of like, big picture risks.
And by the way, I take all the short term and medium term risks seriously too. Like this is a
big tent show where like all your hopes, dreams and concerns and, you know, perhaps irrational
fears like can all have a home. But I guess, you know, to sort of get to P doom zero, I still am
like, I don't know, you know, all these individual crazy scenarios, sure, they're extremely unlikely,
you know, the prompting war with your, you know, protein diffusion model is like absurd on the face
of it. But I kind of think of like to taking the integral over that like vast space of crazy,
super unlikely scenarios. And then I'm kind of like, you know, there's so many of them, right?
That space is so big. And even if the probability is like kind of vanishing, one thing you learn in
calculus is like, you can, you know, the integral can either also vanish or it can be like finite,
you know, over these, you know, kind of, even if the function itself is going to zero, the integral
doesn't necessarily have to go to zero over that space. So to me, that just feels like very
unresolved still. And I don't think we're going to resolve that today. But I would love to hear
a little bit more about your, how you think about AI agency, and also concepts of emergence in agents
today, I guess I also wonder like, is you.com gonna, you know, push more toward the agent direction,
you've got like a, what I would call a research agent today, you've got a browser as well, I could
it, you know, should I start to expect it to take actions for me? What I've observed in the
agent space is I never feel like it fails because it doesn't understand the goal or like
doesn't stay on task. Doesn't say that it never happens, but very rarely, much more often, it's
just a failure of competence. So my expectation then is that like, as the competence improves,
it may not be intrinsic agency, but it may be prompted agency, and it may even be like,
you know, as we have more and more orchestrated systems, we may have models prompting other
models to, you know, go off and do this. And it does feel like we've got, we're headed for like a
lot of spinning plates. And the idea that they could kind of, you know, all come crashing down is
like, not doesn't just doesn't feel like something we can rule out. But I don't know, I can, can you
help me be confident there? I'm still not, I'll go through the sound of the things you mentioned.
So PDM equals zero. So you're right. As you integrate over the future, I would like to not
rule out anything. So maybe I should say 10 to the minus whatever, like a tiny, tiny number,
because in the next five billion years, like all kinds of things happen, right? Like maybe as if
like the three body problem, spoiler alert, like maybe some big, much more sophisticated alien
species will come across, they have already developed faster in my time, travel, and, or,
you know, just are really, really fast in getting here and various capacities. And then, you know,
they have an AI and daddy, I will just like destroy all of us. So they're getting ready to
like settle into the new planet before they get here. Like, there's all kinds of crazy things that
can happen. It's just that, like, in terms of how much resources we should spend on T,
like existential do versus like, you know, I'd say, yeah, I have a couple of researchers,
like, thinking of cool sci-fi scenarios, inspire us, like, maybe, like, think about ways that that
could be prevented, but to spend billions of dollars on it, to like, spend a lot of, like,
mind share the public about it, who's already scared of any kind of technology. I mean, people
are scared of why some people, I mean, there's this great Twitter handle called the pessimist
archive. I mean, people were scared and thought doom is happening because of novels back in the
day. People are like, all these kids, they're just in their heads reading novels. They're gonna all be
useless human beings in the future. Newspaper was terrible. Internet was terrible. Like, there's so
many things that like people thought this is the end of civilization and we're very pessimistic about.
And again, not this diminishing, like, real, real concerns, but again, existential one,
very, very likely given what we're seeing right now. And if there, it does happen at some point
in the future, then I would argue that to think about the best countermeasures now is kind of like
thinking about, you know, the best countermeasures against a computer going crazy when there's
still a bunch of vacuum tubes and you're like, well, we're gonna just suck out the air of everything
into the vacuum tubes. They're not going to work as well and more because they're gonna break and
blah, blah, blah. That was your like counterattack against a computer taking over with your current
thinking of vacuum tube computers. Or it's like, you know, like a similar to the internet, you know,
if you thought about what's the internet going to be, how could it be so terrible,
zero of the TCP IP experts in the early ARPANET base, realize that at some point,
media form power could interfere with local elections because like, you can say whatever you
want online and maybe people get followers in their social media. Like, no one had victimized
in the 70s and the early ARPANET days. And so I think most of the threat vectors are not that
useful in terms of key doom kind of research. I have a couple of folks work on it, but not
take up as much mind space and scare like, laypeople and non experts even more about the
technology that even without consciousness is don't you have major destruction, right? If you're
going through a new set function in human productivity, just like, you know, agriculture
versus like hunting and gathering and the steam engine and electricity and internet. Like, this
one's going to be even bigger. It's going to disrupt and change the job landscape a lot. I think at
the end of it will be way more productive. There's going to be way more productivity per person and
hence more wealth and new jobs will come around as full jobs at all needed. But that is already
so massively disruptive still. And it's not going to happen overnight either. People think
it's going to be fantastic. Yes, it will be faster, but still not overnight. There are still
companies that aren't even on cloud. There are some stretches in the United States and even
Germany that don't have full internet connectivity, right? It's just like, so things will take time
and not happen overnight, but they will be happening even faster than past past technological
revolutions. And so, and then your product ellens and proteins is a great example for
where regulation makes sense. Like, basically, the concern here for those of not someone like
proteins, having everything in life and disease and sickness, COVID is probably like everything
SARS broke through and like, everything is governed by proteins. So if you have a great
understanding of proteins, we can build fantastical, amazing things. Here's just one example of a
research paper I read a few months ago that just blew my mind and made me very excited about the
future. There's this group of researchers that built these carbon nanotubes. And on one side of
the carbon nanotubes, they put iron molecules. And on the other side of these tiny, tiny carbon
nanotubes, they put protein that would only bind to a brain cancer cell. And then they injected
this fluid with all these little carbon nanotubes into a mouse brain that had brain cancer.
The proteins found the brain tumor cells and only connected to those specific types of
brain cancer cells. And then they put the mouse into a little magnetic field and the iron molecule
on the other side of the carbon nanotube that started spinning around and had nano surgery
on each brain cancer cell. Now, if you think about, we have the full control of the proteins,
we can connect them to all kinds of things and you find ways to, you know, get rid of
that carbon nanotubes afterwards. And so like medicine is going to change in so many positive
ways. And now you could argue, well, but proteins, people could use them and build like very bad
like viruses. And like, that's true. And that can be outlawed. In fact, the US just a couple of
months ago outlawed being a function for you. You know, some researchers want to make even more
deadlier viruses. And it's not because they're like evil scientists who want to destroy the
world. It's just saying like, well, as we know how they worked before they appear in nature by
themselves, then we can all right now prevent like develop cures for them. So, you know, it's like,
it's a complex question, but yes, and decided like for now, it's not worth it. Let's outlaw it.
And likewise, I don't think an open source like protein model is going to be the main deciding
factor of being able to create something virus. Because if you have all the wet lab experimentation
to be able to create new kinds of viruses, you can also just do what census Arnold did when she
won the Nobel Prize a couple years ago in chemistry, which is what she called directed evolution,
but it was basically random permutations and then running an experimental pipeline to see
if that random permutation works better or not for particular kind of protein. And then you
just keep iterating like that. And so if you have those capabilities, either random permutation,
you can do bad things. But it turns out having a legit like that, that's do all of that. So
that was your PDOOM integral lm proteins, AI agency emergence. So obviously, emerging
capabilities are incredible on that sort of like, even us like working in deep learning.
I'm amazed just like you.com I asked these questions. I'm like, actually, this is right.
Like, I would have not could like, thought this was possible. Sometimes we were like,
did you program it specifically for it to be able to answer these kinds of questions about
headphones or something? We're like, no, it's just like, just put that all together just by
trying to predict the next token. So I'm really excited. And one of the things I'm excited about
was coding. And one of the things that coding enables is now is the last part of your question,
the actions. I think actions are clearly in the future. And for now, we're focused on amazing
answers. But it's not hard to imagine it at some point, the most amazing answer is done.
I did I did what you asked to do. And instead of telling you how to do it, I just did it, right?
You can build a really cool demo very quickly for these kinds of things. But the problem is like,
as much as I love natural language, and as much as I love chatbots and everything, right,
you have to find some really killer use cases for it. And to say, oh, I can book this flight,
it's actually really hard to just just book the flight. Like, why didn't you like pick this other
one that was just like, not exactly the time I asked for, but I could have waited for half an hour at
this like, extra leg, and then like, see 50 bucks, like that was really dumb. And like, it turns out
Expedia and others have built for decades, the perfect interface for that problem, so that humans
have all the installation right there in a visual way. And so there's an uncanny valley of,
there's a cool tech demo, and on one side, and then there's like my actual human assistant,
who after months of like talking to me, understands all the trade offs, and understands my price
sensitivity, or that of my company, and knows like, when I would preserve and like, only like,
reasons why I might do it overnight, like red-eye slides, and, you know, all the constraints,
and she can do it. And even then, sometimes she's like, oh, Richard, there are like three options
here, like, let me know which one you prefer out of these 5000 that you've built it for you. And
like, it's very hard to do all of that with just text. And it's ultimately, I think, part of why
we have the stock ticker app, and so on, and you know, and why we have, we just now, in some cases
also, is that sometimes like UI UX and actual visually designed like interfaces are best used
in combination with language. Maybe one more big picture question, and then I want to do just a
real quick lightning round on a couple, kind of more technical areas before we run out of time.
On the big picture side, you know, we've got Sam Altman out there saying AGI is coming soon,
but also kind of confusingly saying, but it'll be less impactful than you might think.
Not really sure how to interpret that. The median guess on, you know, some definition of AGI is
like, just a few years on some prediction markets, and more like, you know, 12 years or whatever for
a stronger definition. What do you have sort of an expectation for, and a definition or like a
threshold that you have in mind of like, this is the threshold that really matters and, you know,
loosely speaking, like what sort of timeline you would expect it to take to get there?
It's very much a, the kind of question where you have to be very careful about your terminology
because the interpretation of AGI has vastly different associations. Like people,
some people think of AGI and used to think of AGI as this super intelligence, it's conscious,
as self-awareness, can set its own goals, and it is more intelligent than all human beings.
And, and that's their depth. That was, that was for a long time. A lot of us, I thought, at least
for me personally, also did that definition. Now people said, and I think it's partially because
of marketing, like, you know, you want to be working on AGI, but you also need to have ship stuff.
It's like, you want to be multi-tenant area species, but you also need to just get a lot of
stuff in the orbit, right? But you have more satellites and better internet and so on. So,
you have this long-term vision and you, the best companies are able to articulate that long-term
vision and then revenue generated progress in smaller milestones towards it. And so,
in this case here, I think the definition of AGI was pulled out and then the super
intelligence was defined as like, okay, that's even more than general, it's super. And that's
the really long-term stuff. And now AGI is just basically automating point. And if you define
AGI, which I think is not crazy enough, I want to say maybe Vima Costa used it, which is a very
pragmatic sort of investor slash financial slash economic definition of AGI, which is 80% of the
jobs can be automated up to 80%. And if that's achieved, we'll call it AGI. Turns out there's
just a lot of jobs that are quite repetitive and you're not requiring a ton of extremely novel
out-of-the-box thinking that no one's ever done before and like learning very complex new behaviors,
bot shaking, identifying new experiments, collecting new data and pushing the science
forward and so on. It turns out there are just a lot of boring repetitive jobs. And indeed,
if your definition of AGI is just like, well, we can automate like 80% of 80% of the jobs,
then I think it's not crazy to assume, especially I would restrict it one on the
wall way, which is digitized jobs, jobs that are purely happening in your browser or on your computer,
because those jobs can collect training data at massive scales. Turns out no one's collecting
training data for plumbers, for woofers, for tylers, for maids, like PDTUs or any of that. And so none
of those jobs are going to get automated anytime soon because you first have to collect many years
of that such training data before you can then use AI to train on that and then automate it.
But non-jobs that are fully digitized and that have a lot of training data and that don't have
a crazy long tail of special cases, they're going to get automated. And I think that's reasonable
to say that 80% of jobs for hunches, even in radiology, for instance, you could probably do
80%, find 80% of things that are wrong in Hetzteastam, but then there's still this very long tail
of 20% that you just don't have enough training data for. Radiologists never see it in their lifetime,
they just read about it once in a book and are still not quite good enough of one shot and zero
shot learning. Obviously, huge amounts of progress, but not in super important things like radiology,
where you just read about a case once in a book and then identify it with 100% accuracy,
which is also a question of whether humans do it. I'm actually with you on the self-driving car,
there's going to be a lot of interesting questions as EDI rolls into more and more workplaces,
which is how much better than a human that has to be and how, and it's deeply philosophical,
very quickly, because if you're purely utilitarian, you could say, well, you know, 100,000 miles or
whatever 20 million miles driven by AI results in 10,000 deaths and the same amount of miles
driven by humans results in five times more deaths. And so one is better than the odds,
but if that one dead person in the AI car was your daughter, you don't care, you're going to like in the
US, you're going to sue, you're going to, you know, try to end that company because they're
responsible now for the death of your child. And like, it's a very emotional thing, not a statistical
thing anymore. And so there's going to be a lot of litigation as those come out. And I think the
silver lining is again, of course, as the I meets the state, you can learn from it versus like one
person texting again on their cell phone, which is already illegal and running like over some
kids that ran out, like, they're going too fast also, which is already illegal too. You can't
really do that much more than needing it legal. AGI will have a huge amount of impact. Once it's
just like, okay, repetitive jobs, get like to largely be automated, and I'm with the people
saying that will happen in the next few years. When it comes to like super intelligence,
that is fully conscious and can do all the things. And one intelligent, then not just a single human,
but that all of humanity, very hard to know, because no one's working on it and making sort of
progress along the lines of setting my own sense. And again, like, unless you set your own goals,
I don't know if I would achieve full on super intelligence to you. Like if you're just your
objective function is to minimize cross entropy errors or reduce the complexity or like segment
which as well or like, none of that, I would agree super intelligence. Do you have time for a
lightning round or do we need to leave it there? Let's try to do lightning rounds. All right.
Thinking also about retrieval, memory, and online learning as kind of three
frontiers that, you know, you dot com could could improve on if they're, you know, if there are
research breakthroughs, but also these do seem to be kind of ingredients toward this bigger picture
of AGI or even, you know, at some point, ASI. I guess I'm, you know, maybe just leave it open
ended like what are you excited about in those domains? Are there research directions? Are there,
you know, are there papers you've already seen or things you think people should be doing that
you think will kind of provide meaningful unlocks as we find, you know, new and better ways to do
those things? Yeah. So I'm a fan of all three, of course, I'll try to keep it short. Which
he goes awesome. I think in some ways, short term memory is currently in the front, which people
is in the, you know, rag, if you've got method generation, we do it over a web, we let you
up those files now to you and do it over over a file. And then we have the smart personalization
that actually is online learning. So as you say, certain things like it will, it will remember
then about you. And then, you know, you can turn it off also, and it's very transparent,
turn the whole thing off or the automated smart learning about you if you don't want it.
But yeah, I think that's sort of a simple sort of pragmatic way of online learning. I think
ultimately, you know, it'll be awesome to have AI systems get better and better of just adapting
right away to use your feedback both in terms of, you know, thumbs up, thumbs down kinds of clicking,
but also in conversation, we'd be like, I didn't like that answer. And then updating the answer
in a principled way for the future. I have so many more thoughts, but I'll like, I'd love to do a
second one. These are kind of crazy days. Now the Apple announcement, we just announced that
Julianne CTO, I mean, face also just became an angel investor and a lot of exciting stuff happening.
So I yeah, well, congratulations on the Apple thing and also on a new prominent angel investor.
And really some fantastic product progress. I definitely recommend everybody
to try out particularly genius mode and research mode. And I think if you do that,
you will be coming back to you.com more and more often. So keep up the great work. For now,
I will say Richard Sosher, founder and CEO of you.com. Thank you for being part of the cognitive
revolution. Thank you so much. It is both energizing and enlightening to hear why people
listen and learn what they value about the show. So please don't hesitate to reach out via email
at TCR at turpentine.co or you can DM me on the social media platform of your choice.
Omniki uses generative AI to enable you to launch hundreds of thousands of ad iterations that actually
work customized across all platforms with a click of a button. I believe in Omniki so much
that I invested in it. And I recommend you use it too. Use CogGrav to get a 10% discount.

They don't care about AI safety.
What they care about is AI control.
Do I think we eventually get to a configuration like that?
Maybe.
Where you have an AI brain is at the center of civilization
and it's coordinating all the people around it.
And every civilization that makes it
is capable of crowdfunding and operating its own AI.
You know, our background culture influences things
in ways we don't even think about.
So much of the paperclip thinking is like
a vengeful God will turn you into pillars of salt.
The polytheistic model of many gods as opposed to one God is
we're all going to have our own AI gods
and there'll be war of the gods.
Man-machine symbiosis is not some new thing.
It's actually the old thing that broke us away
from other primate lineages that weren't using tools.
Then the question is, what's the next step?
Which is AI is amplified intelligence.
It is that the AI human fusion
means there's another 20 Elon Musk's
or whatever the number is.
That's good.
Hello and welcome to the Cognitive Revolution
where we interview visionary researchers, entrepreneurs
and builders working on the frontier of artificial intelligence.
Each week we'll explore their revolutionary ideas
and together we'll build a picture of how AI technology will transform
work, life and society in the coming years.
I'm Nathan LeBens joined by my co-host Eric Torenberg.
Hello and welcome back to the Cognitive Revolution.
Today my guest is Balaji Srinivasan.
In tech circles, Balaji needs no introduction
but for folks from other backgrounds,
Balaji is a serial startup entrepreneur
who's founded and ultimately sold
highly dissimilar technology companies
including Teleport,
which helped people move around the world to realize opportunities,
Council, which provided genetic testing for couples
planning to have children
and Earn.com, a paid email on the blockchain startup
which ultimately sold to Coinbase
where Balaji became CTO.
Along the way, he's also taught statistics at Stanford
and been a general partner at Andreessen Horowitz as well.
Today, as an independent thinker, investor
and author of the network state,
Balaji is extremely prolific in both text and audio formats.
And as you'll hear whether for the first time or the 50th,
he is an incredibly creative thinker
who relentlessly develops and iterates on new paradigms
for understanding a fast-changing, often chaotic world.
He's also a very associative and interdisciplinary thinker
who constantly adds dimensions to any analysis.
Such horsepower can be hard for a podcast host to rein in
but I personally find it extremely stimulating.
So in this conversation, I tried to strike a balance
between letting Balaji go off as only he can do,
contributing what I hope are worthy versions of core AI safety arguments
and supporting results from recent research
and occasionally steering us back toward what I see
as the most critical questions for the AI big picture.
If there's one area where Balaji and I disagree most consequently,
it's on the question of how independent AI systems
are likely to become over the next five to 10 years.
Balaji thinks that AI systems need to be at least symbiotic with humans
because physical computers can't replicate themselves
without human support.
While I think there's at least a significant chance
that we get AIs that are so independent of humans
that their behaviors and interactions become the primary drivers of world history.
In Balaji's own words, he does expect massive economic
and social disruption from AI
but doesn't think that quote-unquote can't turn the killer AI off scenarios
are likely, at least for a long while,
due to factors like the existence of adversarial inputs
that can paralyze AIs, particularly those with open model weights,
the observation that even decentralized programs like the Bitcoin network
can't run independently without continuous human support,
and the premise that to control the physical world,
AIs will need to direct either large numbers of humans
who are notoriously difficult to control
or highly agile robots which don't yet exist.
With all that in mind, in the first half of this conversation,
you'll hear Balaji's analysis of the likely impact of AI
in a world where powerful AI systems do come to exist
but humans retain control,
resulting in a human AI symbiosis
similar to how believers relate to their gods
or citizens relate to their governments.
Then in the second half,
we really dig into the question of just how confident we should be
that AI won't prove to be even more revolutionary than that.
After more than two hours of recording,
I was the one who ran out of time today,
but I really enjoyed this conversation with Balaji.
He is as good-natured and curious as he is opinionated,
and we have continued to exchange links and arguments offline,
such that I hope we'll have another episode to share with you in the future as well.
As always, if you're enjoying the show,
we'd ask that you take a moment to share it with a friend.
And with that, here's part one of an all-angles look
at how AI will shape the future with Balaji Srinivasan.
Balaji Srinivasan, welcome to the Cognitive Revolution.
All right, I feel welcome.
Well, we've got a ton to talk about.
Obviously, you bring a lot of different perspectives
to everything that you think about and work on.
And today, I want to just try to muster all those different perspectives onto this.
What I see is really the defining question of our time,
which is like, what's up with AI and how's it going to turn out?
I thought maybe for starters,
I would love to just get your baseline kind of table setting
on how much more AI progress do you expect us to see over the next few years?
Like, how powerful are AI systems going to become in, again,
kind of a relatively short timeline?
And then maybe if you want to take a bigger stab at it,
you could answer that same question for a longer timeline
like the rest of our lives or whatever.
Sure. Let me give an abstract answer.
Then let me give a technical answer.
If you look at evolution, we've seen something as complex as flight
evolve independently in birds, bats, and bees.
And even intelligence, we've seen fairly high intelligence
in dolphins, in whales, in octopuses.
Octopus in particular can do like tool manipulation.
They've got things that are a lot like hands with tentacles.
And so that indicates that it is plausible
that you could have multiple pathways to intelligence,
whether we have carbon-based intelligence.
So we could have silicon-based intelligence
that just has a totally different form
where the fundamental thing is an electromagnetic wave
and data storage as opposed to DNA and so on.
So that's like a plausibility argument in terms of evolution
is being so resourceful
that it's invented really complicated things in different ways.
Then in terms of the technical point, I think as of right now,
I should probably date it as like December 11, 2023
because this field moves so fast.
My view is, and maybe you'll have a different view,
is that the breakthroughs that are really needed
for something that's like true artificial intelligence
that is human-independent.
Maybe the next step after the Turing test,
I've got an article that we're writing called the Turing thresholds
which tries to generalize the Turing test to like the Kardashev scale.
Have you got energy thresholds?
What are useful scales beyond that?
Right now, I think that what we call AI
is absolutely amazing for environments
that are not time-varying or rule-varying.
What I mean by that is,
so you kind of have, let's say, two large schools of AI
and obviously there's overlap in terms of the personnel and so on,
but there's like the DeepMind School
which has gotten less press recently
but got more press a few years ago
and that is game-playing, right?
It is, you know, superhuman playing of Go with AlphaGo.
It is, you know, all the video game stuff they've done
where they learn at the pixel level
and they just teach the very basic rules
and it figures it out from there.
And it's also, you know, the protein folding stuff
and what have you, right?
But in general, I think they're known for reinforcement learning
and those kinds of approaches.
I mean, they're good at a lot of things
but that's what I think DeepMind is known for.
Of course, they put out this new model recently,
the Gemini model, so I'm not saying that they're not good at everything
but that's just kind of what they're maybe most known for.
And then you have the OpenAI ChatGBT School of Generative AI
and it includes stable diffusion and just as a pioneer
even if, you know, they're not,
I don't know how much they're used right now
but basically, you know, you have the diffusion models for images
and you have large language models
and now you have the multimodals that integrate them.
And so the difference I think with these is the reinforcement learning approaches
are based on an assumption of static rules
like the rules of chess, the rules of go,
the rules of a video game are not changing with time,
they're discoverable, they're like the laws of physics.
And similarly, like the body of language where you're learning it,
English is not rapidly time varying.
That is to say the rules of grammar that are implicit aren't changing,
the meanings of words aren't changing very rapidly.
You can argue they're changing over the span of decades or centuries
but not extremely rapidly, right?
So therefore, when you generate a new result,
training data from five years ago for English is actually still fairly valuable
and the same input roughly gives the same output.
Now, of course, there are facts that change with time,
like who is the ruler of England, right?
The Queen of England is passed away now, it's the King of England, right?
It's just facts that change with time.
But I think more fundamentally is when there's rules that change with time.
You know, you have, for example, changes in law in countries, right?
But most interestingly, perhaps changes in markets
because the same input does not give the same output in a market.
If you try that, then what will happen is there's adversarial behavior on the other side
and once people see it enough times, they'll see your strategy
and they're going to trade against you on that, right?
And I can get to other technical examples on that, but I think,
and probably people in the space are aware of this,
but I think that is the true frontier is dealing with time varying, rule varying systems
as opposed to systems where the implicit rules are static.
Let me pause there.
Yeah, I think that makes sense.
I think the, you know, in the very practical, you know,
just trying to get, as V calls it, mundane utility from AI,
that is often kind of cashed out to AI is good at tasks,
but it's not good at whole jobs.
You know, it can handle these kind of small things where you can define,
you know, what good looks like and tell it exactly what to do.
But in the sort of broader context of, you know, handling things that come up
as they come up, it's definitely not there yet.
And I agree that there's likely to be some synthesis, you know,
which is kind of the subject of all the Q star rumors recently,
I would say is kind of the prospect that there could be already,
you know, within the labs, a beginning of a synthesis between the,
I kind of think of it as like harder edged reinforcement learning systems,
you know, that are like small, efficient and deadly versus the like language model systems
that are like kind of slow and soft and, you know,
but have a sense of our values, which is really a remarkable accomplishment
that they're able to have even, you know, an approximation of our values
that seems like reasonably good.
So, yeah, I think I agree with that framing,
but I guess I would, you know, still wonder like,
how far do you think this goes in the near term?
Because I have a lot of uncertainty about that.
And I think the field has a lot of uncertainty.
You hear people say, well, you know,
it's never going to get smarter than its training data,
you know, it'll kind of level out where humans are,
but we certainly don't see that in the reinforcement learning side, right?
Like once it usually don't take too long at human level of these games,
and then it like blows past human level.
Interestingly, you do still see some adversarial vulnerability,
like there's a great paper from the team at FAR AI.
And I'm planning to have Adam Gleave,
the head of that organization on soon to talk about that and other things,
where they found like a basically a hack where a really simple,
but unexpected attack on the superhuman go player can defeat it.
So you do have these like very interesting vulnerabilities
or kind of lack of adversarial robustness.
Still kind of wondering like,
where do you think that leaves us in say a three to five years time?
Obviously, huge uncertainty on that.
It's really hard to predict something like this.
Just to your point, generative AI is generic AI, right?
It's like generically smart,
but doesn't have specific intelligence or creativity or facts.
And as you're saying, just like we have,
you know, adversarial images back in full programs
that are trained on a certain set of data
and they just give some weird, you know, pattern
that looks like a giraffe, but the algorithm thinks it's a dog.
You can do the same thing for game playing
and you can have out of sample input
that can beat, you know, these very sophisticated reinforcement learners.
And an interesting question is whether that is a fundamental thing
or whether it is a work aroundable thing.
And you think it was work aroundable, you know,
because there's probably some robustification
because these pictures look like giraffes, you know,
and yet they're being recognized as dogs.
So you would think that the right proximity metric
would group it with giraffes, you know,
but maybe there's some, I don't know, maybe there's some result there.
My intuition would be we can probably robustify the system
so that they are less vulnerable to adversarial input.
But if we can't, then that leads us in a totally different direction
where these systems are fragile in a fundamental way.
So that's one big branch point is how fragile these systems are
because if they're fragile in a certain way,
then it's almost like you can always kill them,
which is kind of good, right, in a sense,
that there's that, you know, almost like the,
you know, the 50 IQ, 100 IQ, 150 IQ thing.
Like the meme?
Yeah, the meme, right?
So the 50 IQ guys like these machines will never be as creative as humans or whatever.
100 IQ is look at all the things they can do.
The 150 IQ is like, well, there's some like equivalent equivalent result,
you know, that's like some impossibility proof that shows
that we the dimensional space of a giraffe is too high
and we can't actually learn what a true giraffe.
I don't think that's true.
But maybe it's true from the perspective of how these learners are working
because my understanding is people have been trying
and I mean, I'm not the cutting edge of this.
So, you know, maybe someone, but my understanding is we haven't yet
been able to robustify these models against adversarial input.
Am I wrong about that?
Yeah, that's definitely right.
Hey, we'll continue our interview in a moment after a word from our sponsors.
Real quick, what's the easiest choice you can make?
Taking the window instead of the middle seat,
outsourcing business tasks that you absolutely hate.
What about selling with Shopify?
Shopify is the global commerce platform that helps you sell at every stage of your business.
Shopify powers 10% of all e-commerce in the US
and Shopify is the global force behind Allbirds,
Rothy's and Brooklyn and millions of other entrepreneurs of every size
across 175 countries.
Whether you're selling security systems or marketing memory modules,
Shopify helps you sell everywhere from their all-in-one e-commerce platform
to their in-person POS system.
Wherever and whatever you're selling, Shopify's got you covered.
I've used it in the past at the companies I've founded
and when we launch Merch here at Turpentine,
Shopify will be our go-to.
Shopify helps turn browsers into buyers with the internet's best converting checkout
up to 36% better compared to other leading commerce platforms.
And Shopify helps you sell more with less effort
thanks to Shopify Magic, your AI-powered All-Star.
With Shopify Magic, whip up captivating content
that converts from blog posts to product descriptions.
Generate instant FAQ answers.
Pick the perfect email send time.
Plus, Shopify Magic is free for every Shopify seller.
Businesses that grow, grow with Shopify.
Sign up for a $1 per month trial period at Shopify.com slash Cognitive.
Go to Shopify.com slash Cognitive now to grow your business
no matter what stage you're in.
Shopify.com slash Cognitive.
Omnike uses generative AI to enable you to launch hundreds of thousands
of ad iterations that actually work,
customized across all platforms with a click of a button.
I believe in Omnike so much that I invested in it
and I recommend you use it too.
Use CogGrav to get a 10% discount.
There's no single architecture as far as I know that is demonstrably robust
and on the contrary, even with language models,
we did a whole episode on the universal jailbreak
where especially if you have access to the weights,
not to change the weights, but just to kind of probe around in the weights,
then you have a really hard time guaranteeing any sort of robustness.
The conjecture is, see for humans,
you can't mirror their brain and analyze it, okay?
But we have enough humans that we've got things like optical illusions,
stuff like that that works on enough humans
and our brains aren't changing enough, right?
A conjecture is if you had, as you said, open weights.
Open weights mean safety because if you have open weights,
you can always reverse engineer adversarial input
and then you can always break the system.
Conjecture.
Yeah, that's again with Adam from FAR AI.
I'm really interested to get into that
because they are starting to study, as I understand it,
kind of proto-scaling laws for adversarial robustness
and I think a huge question there is
what are the kind of frontiers of possibility there?
Like, do you need, you know, how do the orders of magnitude work, correct?
Do you need another 10x as much adversarial training
to half the rate of your adversarial failures?
And if so, you know, can we generate that many?
It may always sort of be fleeting.
So, FAR AI and they are working on cutting edge of adversarial input.
Yeah, they're the group that did the attack on the AlphaGo model
and found that like, you know, and what was really interesting about that,
I mean, multiple things, right?
First, that they could beat a superhuman go player at all.
But second, that the technique that they used would not work at all
if playing a quality human.
Whereas, you know, it's a strategy that is trivial to beat
if you're a quality human go player,
but the AlphaGo is just totally blind to it.
You know, that's why I say the conjecture is
if you have the model, then you can generate the adversarial input.
And then, so if that is true,
and that itself is an important conjecture about AI safety, right?
Because if open weights are inherently something
where you can generate adversarial input from that
and break or crash or defeat the AI,
then that AI is not omnipotent, right?
You have some power words you can speak to it,
almost like magical words that'll just make it power down.
So to speak, right?
It's like those movies where the monsters can't see you
if you stand really still
or if you don't make a noise or something like that, right?
They're very powerful on DimensionX,
but they're very weak on DimensionY.
Kind of an obvious point,
but you know, I'm not sure how important it's going to be in the future.
Your next question was on like, you know, humanoid robots and so on.
And before we get to that, maybe obviously,
but all of these models are trained on things
that we can easily record,
which are sights and sounds, right?
But touch and taste and smell,
we don't have amazing datasets on those.
Well, I mean, there's some haptic stuff, right?
There's probably some work on taste and smell and so on,
but there's five senses, right?
I wonder if there's something like that
where you might be like,
okay, how are you going to outsmell a robot or something like that?
Well, dogs actually have a very powerful sense of smell
and that's being very important for them, you know?
And it may turn out that there's...
Maybe it's just that we just haven't collected the data
and it could become a much better smeller or whatever
or, you know, taster than anything else.
I wouldn't be surprised.
It could be a much better wine taster
because you can do molecular diagnostics.
But it's just kind of...
I just use that as an analogy to say there's areas of the human experience
that we haven't yet quantified
and maybe it's just the opera term is yet, okay?
But there's areas of the human experience we haven't yet quantified,
which are also an area that AIs at least are not yet capable of.
Yeah, I guess maybe my expectation boils down to...
I think the really powerful systems are probably likely to mix architectures
in some sort of ensemble.
You know, when you think about just the structure of the brain,
it's not...
I mean, there certainly are aspects of it that are repeated, right?
You look at the frontal cortex and it's like there is kind of this,
you know, unit that gets repeated over and over again
in a sense that's kind of analogous to say the transformer block
that just gets stacked layer on layer.
But it is striking in a transformer
that it's basically the same exact mechanism at every layer
that's doing kind of all the different kinds of processing.
And so whatever weaknesses that structure has
and with the transformer and the attention mechanism,
there's like some pretty profound ones like finite context window.
You know, you kind of need, I would think,
a different sort of architecture with a little bit of a different
strength and weakness profile to complement that in such a way
that, you know, kind of more similar to like a biological system
where you kind of have this like dynamic feedback
where, you know, we have obviously, you know, thinking fast and slow
and all sorts of different modules in the brain
and they kind of cross regulate each other and don't let any one system,
you know, go totally, you know, down the wrong path on its own, right?
Without something kind of coming back and trying to override that.
It seems to me like that's a big part of what is missing
from the current crop of AIs in terms of their robustness.
And I don't know how long that takes to show up,
but we are starting to see some, you know, possible, you know,
I think people are maybe thinking about this a little bit the wrong way.
They're just in the last couple of weeks.
There's been a number of papers that are really looking
at the state space model kind of alternative.
It's being framed as an alternative to the transformer.
But when I see that I'm much more like it's probably a compliment
to the transformer or, you know, these two things probably get integrated
in some form because to the degree
that they do have very different strengths and weaknesses,
ultimately you're going to want the best of both, you know, in a robust system.
Certainly if you're trying to make an agent,
certainly if you're trying to make, you know, a humanoid robot
that can go around your house and like do useful work,
but also be robust enough that it doesn't get tricked
into attacking your kid or your dog or, you know, whatever.
You're going to want to have more checks and balances
than just kind of a single stack of, you know, the same block over and over again.
Well, so I know Boston Dynamics with their legged robots is all control theory
and it's not classical ML development.
It's really interesting to see how they've accomplished it.
And they do have essentially a state space model
where they have a big position vector that's got all the coordinates of all the joints
and then a bunch of matrix algebra to figure out how this thing is moving
and all the feedback control and so on there.
And it's more complicated than that, but that's, you know, I think the V1 of it.
Sorry, it was there.
I wasn't following this though.
Are you saying that there's papers that are integrating that
with the kind of generator transformer model?
You know, what's like, what's a good citation for me to look at?
Yeah, starting to, we did an episode, for example,
with one of the technology leads at Skydio.
The, you know, the U.S. is champion drone maker.
And they have kind of a similar thing where they have built over, you know,
a decade, right, a fully explicit multiple orders of, you know,
spanning multiple orders of magnitude control stack.
And now over the top of that, they're starting to layer this kind of, you know,
it's not exactly generative AI in their case,
because they're not like generating content, but it's kind of the high level,
you know, can I give the thing verbal instructions?
Have it go out and kind of understand, okay, like this is a bridge.
I'm supposed to kind of, you know, survey the bridge and translate those high level
instructions to a plan and then use the lower level explicit code that is fully
deterministic and, you know, runs on control theory and all that kind of stuff
to actually execute the plan at a low level.
But also, you know, at times like surface errors up to the top and say like,
hey, we've got a problem, you know, whatever.
I'm not able to do it.
You know, can you now at the higher level, the semantic layer adjust the plan?
That stuff is starting to happen in multiple domains, I would say.
Yeah.
And so I think that makes sense is basically it's like generative AI is
almost the front end.
And then you have almost like an assembly like you give instructions to Figma
and the objects there are their shapes and their images.
And so there's not, it's not text.
You give instructions to a drone and the objects are like GPS coordinates and
paths and so on.
And so you are generating structures that are in a different domain or it's
like in VR, you're generating 3D structures again, as opposed to text.
And then that compute engine takes those three structures and does something
with them in a much more rules based way.
So you have like a statistical user friendly front end with a generative AI
and then you have a more deterministic or usually totally deterministic almost
like assembly language back in that actually takes that and does that's
what you're saying, right?
Yeah, pretty much.
And I would say there's another analogy to just again, our biological
experience where it's like, I'm, you know, sort of in a semi conscious level,
right?
I kind of think about what I want to do, but the low level movements of the hand,
you know, are both like not conscious and also, you know, if I do encounter some
pain, you know, hit some, you know, hot item or whatever, like there's a quick
reaction that's sort of mediated by a lower level control system.
And then that fires back up to the brain and is like, Hey, you know,
we need a new plan here.
So that is only starting to come into focus.
I think with, you know, because obviously these, I mean, it's amazing.
As you said, it's all moving so fast.
What is always striking to me, I just, and I kind of like recite timelines
to myself almost as like a mantra, right?
Like the first instruction following AI that hit the public was just January 2022.
That was open AI's text of NGO.
It was the first one where you could say like do X and it would do X as opposed
to having, you know, an elaborate prompt engineering type of setup.
GPT for, you know, just a little over a year ago, finished training,
not even a year that it's been in the public.
And, you know, it has been amazing to see how quickly this kind of technology
is being integrated into those systems, but it's definitely still very much
work in progress.
Yeah.
I mean, the tricky part is like the training data and so on with like a large
existing scale company like a Figma or DJI that has millions or billions
of user sessions will have a much easier time training and they have a unique
data set and then everybody else will not be able to do that.
So there is actually almost like, I mean, a return on scale where the massive
data set, if you've got a massive clean data set and a unique domain that
lots of people are using, then you can, you can crush it.
And if you don't, I suppose, I mean, there's lots of people who work on
zero shot stuff and sort of sort of, but it still strikes me that there'll
probably be an advantage to see those sessions.
You know, I find it hard to believe that you could, you know, generate
a really good like drone command language without lots of drone flight
pads, but you know, you can see.
And where it doesn't exist, people are, you know, obviously you need deep
pockets for this, but the likes of Google are starting to just grind
out the generation of that, right?
They've got their kind of test kitchen, which is a literal, you know, physical
kitchen at Google where the robots go around and do tasks and when they get
stuck, my understanding of their kind of critical path as I understand
they understand it is robots going to get stuck will have a human operator
remotely operate the robot to show what to do.
And then that data becomes the bridge from what the robot can't do to
what it's supposed to learn to do next time.
And they're going to need a lot of that, you know, for sure, but they
increasingly have, you know, I don't know exactly how many robots
they have now, but last I talked to someone there, it was like into the
dozens and, you know, presumably they're continuing to scale that.
I think they just view that they can probably brute force it to the point
where it's like good enough to put out into the world.
And then very much like a Waymo or a cruise or whatever they probably
still have kind of remote operators, even when the robot is like in your
home, you know, when it encounters something that it doesn't know
what to do about raise that alarm, get the human supervision to help
it over the hump.
And then, you know, obviously that's where you really get the scale
that you're talking about.
This raises a couple of questions I wanted to ask that are conceptual.
So, you know, obviously there's huge questions around like, again,
highest level, how is all this going to play out?
One big debate is to what degree does AI favor the incumbents?
To what degree, you know, does it enable startups?
Obviously, it's both, but, you know, be interested in your perspective on that.
Also really interested in your perspective on like offense versus defense.
That's something that a lot of people now and in the future, right?
That seems like it probably really matters a lot, whether it's a more
offense enabling or defense enabling technology.
So, I love your take on those two dimensions.
Hey, we'll continue our interview in a moment after a word from our sponsors.
Omnike uses generative AI to enable you to launch hundreds of thousands
of ad iterations that actually work customized across all platforms
with a click of a button.
I believe in Omnike so much that I invested in it and I recommend you
use it too.
Use Kogrev to get a 10% discount.
If you're a startup founder or executive running a growing business,
you know that as you scale, your systems break down and the cracks start to show.
If this resonates with you, there are three numbers you need to know.
36,000, 25, and 1.
36,000.
That's the number of businesses which have upgraded to NetSuite by Oracle.
NetSuite is the number one cloud financial system, streamlined accounting,
financial management, inventory, HR, and more.
25.
NetSuite turns 25 this year.
That's 25 years of helping businesses do more with less, close their books
in days, not weeks, and drive down costs.
One, because your business is one of a kind, so you get a customized
solution for all your KPIs in one efficient system with one source of truth.
Manage risk, get reliable forecasts, and improve margins.
Everything you need all in one place.
Right now, download NetSuite's popular KPI checklist, designed to give
you consistently excellent performance, absolutely free, and netsuite.com
slash cognitive.
That's netsuite.com slash cognitive to get your own KPI checklist.
Netsuite.com slash cognitive.
So like offense or defense in the sense of disenabled disruptors or incumbents?
Both in business and in like, you know, potentially outright conflict.
I'd be interested to hear your analysis on both.
All right.
A lot of views on this.
So obviously, if you've got a competent existing tech CEO, you know, like who's
still in their prime, like Amjad of Replit or, you know, Dillon Field of Figma.
Or, you know, those are two who have thought of who are very good and, you
know, will be on top of it.
Amjad is very early on integrating AI into Replit and it's basically built
that into an AI-first company, which is really impressive.
Those are folks who cleanly made a pivot.
It's as big or bigger than, comparable to, I would say, the pivot from
desktop to mobile that broke a bunch of companies in the late 2000s and early 2010s.
Like Facebook in 2012 had no mobile revenue roughly at the time of their IPO.
And then they had to like redo the whole thing.
And it's hard to turn a company 90 degrees when something new like that hits.
You know, those that are run by kind of tech CEOs in their prime will adapt and
will AI-ify their existing services.
And the question is, obviously, there's new things that are coming out like Pica
and character.ai.
There's some like really good stuff that's out there.
The question is, you know, will the disruption be allowed to happen in the
U.S. regulatory environment?
And so my view is actually that, you know, so this is from like the
network state book, right?
I talk about, you know, people talk about a multipolar world or unipolar world.
The political axis is actually really important in my view for thinking
about whether AI will be allowed to disrupt.
Okay, because we'll get to this probably later, but the 640K of compute is
enough for everyone executive order, you know, 640K of memory, the apocryphal.
He didn't bill gates and actually say it, but that that quote kind of gives a
certain mindset about computing.
That should be enough for everybody.
So the 10 to the 26 of compute should be enough for everyone bill.
Um, I actually think it's very bad.
And I think it's just the beginning of their attempts to build like a software
FDA, okay, to decelerate, control, regulate, red tape, the entire space,
just like how, you know, the threat of nuclear terrorism got turned into the
TSA, the threat of, you know, terminators and AGI gets turned into a
million rules on whether you can set up servers and this last free sector,
the economy is strangled or at least controlled within the, the territory
controlled by Washington, DC.
Now, why is, why does this relate to the political?
Well, obviously this, you know, you can just spend your entire life just tracking
AI papers and that's moving like at the speed of light like this, right?
What's also happening as you can kind of see in your peripheral vision is
there's political developments that are happening at the speed of light much
faster than they've happened in our lifespans.
Like there's more, you just noticed more wars, more serious online conflicts.
Like, you know, there's a sovereign debt crisis.
All of those things that can show graph after graph of things
looking like their own types of singularities, you know, like military
debts are way up, you know, the long piece that Stephen Pinker showed,
it's looking like a you that suddenly way up after Ukraine and some
of these other wars are happening.
Unfortunately, right?
Interest payments, whoosh, way up to the side.
What's my point?
Point is, I think that the world is going to become from the Pax
Americana world of just like basically one superpower, hyperpower
that we grew up in from 91 to 2021, roughly, that we're going to get a
specifically tri-polar world, not unipolar, not bipolar, not multi-polar,
but tri-polar.
And those three poles, I kind of think of as NYT, CCP, BTC, or you could
think of them as and those are just certain labels that are associated
with them, but they're roughly US tech, the US environment, China tech
and China environment and global tech and the global environment.
And why do I identify BTC and crypto and so on with global tech?
Cause that's a tech that decentralized out of the US.
And right now people think of crypto as finance, but it's also financiers.
Okay.
And in this next run up, it is, I think quite likely about depending
on how you count between a third to a half of the world's billionaires
will be crypto.
Okay.
Around, you know, I calculated this a while back around Bitcoin at a few
hundred thousands or around a third to a half the world's billionaires
of crypto.
That's the unlocked pool of capital.
And those are the people who do not bow to DC or Beijing and they might
by the way be Indians or Israelis or every other demographic in the world
or they could be American libertarians or they could be Chinese
liberals like Jack Ma or pushed out of Beijing sphere.
Okay.
Or the next Jack Ma, you know, Jack Ma himself may not be able to do too
much.
Okay.
That group of people who are, let's say the dissident technologists
who are not going to just kneel to anything that comes out of Washington
to see your Beijing, that is the, that's decentralized AI.
That's crypto.
That's decentralized social media.
So you can think of it as, you know, where we, we talked about in the
recent pirate wearers podcast, freedom to speak with decentralized
censorship resistant social media, freedom to transact with cryptocurrency,
freedom to compute with open source AI and no compute limits.
Okay.
That's a freedom movement and that's like the same spirit as a pirate bay.
The same spirit as BitTorrent, the same spirit as Bitcoin, the same
spirit as peer to peer and intent encryption.
That's a very different spirit than having Kamala Harris regulate a super
intelligence or signing it over to Xi Jinping thought.
And the reason I say this is.
I think that that group of people of which I think Indians and Israelis
will be a very prominent, maybe a plurality, right?
Just because the sheer quantity of Indians are like the third sort of big
group that's kind of coming up and they're relatively underpriced.
You know, China is, I don't say it's price to perfection, but it's something
that people, when I say priced, I mean, people were dismissive of China
even up until 2019.
And then it was after 2020, if you look that people started to take
China seriously and I mean, but that is the West Coast tech people knew
that China actually had a plus tech companies and was a very strong
competitor, but the East Coast still thought of them as a third world
country until after COVID when now, you know, the East Coast was sort of
threatened by them politically and it wasn't just blue collars, but blue
America that was threatened by China.
And so that's why the reaction to China went from, Oh, who cares?
Just taking some manufacturing jobs too.
This is an empire that can contend with us for control of the world.
That's why the hostility is ramped up in my view.
There's a lot of other dimensions to it, but that's a big part of it.
So India is also kind of there, but it's like the third and India is not going
to play for number one or number two, but India and Israel, if you look
at like tech founders, depending on how you count, especially if you
include diasporas, it's on the order of 30 to 50% of tech founders, right?
And it's obviously some, you know, very good tech CEOs and, you know,
Satya and Sundar and investors and whatnot.
Those are folks Indians do not want to bow to DC or to Beijing, neither
do Israelis for all kinds of reasons, even if Israel has to, you know,
take some direction from the US.
Now they're bristling at it, right?
And then a bunch of other countries don't.
So the question is, who breaks away?
And now we get to your point on the reason I had to say that is that
that's preface, the political environment is a tripolar thing of US
tech and US regulated Chinese tech and China regulated and global
tech that's free.
Okay.
Of course, there's even though I identify those three polls, there's
of course boundary regions.
EAC is actually on the boundary of US tech and decentralized tech,
you know, and I'm sure there'll be some Chinese thing that comes out
that is also on the boundary there.
For example, Binance is on the boundary of Chinese tech and global
and decentralized tech, if that makes any sense, right?
There's probably others.
Apple is actually on the boundary of US tech and Chinese tech because
they make all of their stuff in China, right?
So these are not totally disjoint groups, but there's boundary areas,
but you can think of them.
Why is this third group so important in my view?
Both the Chinese group and the decentralized group will be very strong
competition for the American group for totally different reasons.
China has things like WeChat, these super apps.
I mean, obviously not likely, but like we chat is a super app, but
they also have, for example, their digital yuan, right?
They have the largest cleanest data sets in the world that are
constantly updated in real time that they can mandate their entire
population opt into and most of the Chinese language speaking people
are under their ambit, right?
So that doesn't include Taiwan, doesn't include Singapore, doesn't
include, you know, some of the Chinese, but basically anything
that's happening in Chinese for 99% of it, 95, whatever the ratio is,
they can see it and they can coerce it and they can control it.
So they can tell all of their people, okay, here's five bucks in,
you know, digital yuan, do this micro task, okay?
All of these digital blue collar jobs, both China and India,
I think can do quite a lot with that and I'll come back to it.
So they can make their people do immense amounts of training
data, clean up lots of data sets.
Once it's clear that you have to build this and do this, they
can just kind of execute on that and they can also deploy.
I mean, in many ways, the U.S. is still very strong in digital
technology, but in the physical world, it's terrible because of
all the regulations cause all of the nimbyism and so on.
It's not like that in China.
So anything which kind of works in the U.S. at a physical level
like the Boston Dynamic stuff, they're already cloning it in
China and they can scale it out in the physical world.
You already have drones, little sidewalk drone things that
come to your hotel room and drop things off.
That's already like very common in China.
In many ways, it's already ahead if you go to the Chinese cities.
So the Chinese version of AI is ultra centralized, more
centralized, more monitoring, less privacy and so on than the
American version and therefore they will have potentially better
data sets, at least for the Chinese population.
And so we shed AI.
I don't even know what it's going to be, but it will be
probably really good.
Okay.
It'll also be really dangerous in other ways.
Okay.
Then the decentralized sphere has power for a different reason
because the decentralized sphere can train on full Hollywood
movies.
It can train on all books, all MP3s and just say, screw all
this copyright stuff, right?
Like what Psyhub and, you know, Libgen are doing because all
the copyright, first of all, it's not, it's like Disney lobbying
politicians to put like another 60 or 70 or 90.
I don't even know what it is, some crazy amount on copyright.
So you can keep milking this stuff and it doesn't go into
public domain number one.
And second, you know how Hollywood is built in the first place?
It was all patent copyright and IP violation.
Essentially Edison had all the patents.
He's in New Jersey-ish, okay, that East Coast area.
And Neil Gabler has this great book called An Empire of Their
Own, where he talks about how immigrant populations, you
know, the Jewish community in particular and also others went
to Southern California in part so they could just make movies
that Edison coming and suing them for all the patents and so
on and so forth.
And they made enough money that they could fight those battles
in court and that's how they built Hollywood.
Okay.
So, you know, one of my big thesis is history is running in
reverse and I can get to why, but it's like 1950s a mirror
moment and you go more decentralized backwards and forwards
in time is like these, you have these huge centralized states
like the US and USSR and China, you know, all these things
exist and their fist relaxes as you go forwards and backwards
in time.
For example, backwards in time, the Western frontier closed
and forwards in time, the Eastern frontier opens.
Backwards the time you have the robber barons, forwards in
time you have the tech billionaires.
Backwards time you have Spanish flu, forwards in time you have
COVID-19 and I've got dozens of examples of this in the book.
The point is that if you go backwards in time, the ability
to enforce patents and copyrights and so on starts dropping
off, right?
You have much more of a grand theft auto environment and you
go forwards in time and that's happening again.
So, India in particular for many years basically just didn't
obey Western patent protections and all these stupid rules
basically, you know, it's a combination of artificial
scarcity on the patent side and artificial regulation on the
FDI side.
That's a big part of what jacks up drug costs where these
things cost, you know, only sense to manufacture and they
sell them for so much money.
All the delays, of course, that are imposed on the process,
the only way they can pay for it the manufacturers to take it
out of your hide.
What India did is they just said, we're not going to obey any
of that stuff.
So they have a whole massive generic drugs and biotech
industry that arose because they built all the skills for
that. That's why they could do their own vaccine during COVID
and they're one of the biggest biotech industries in the
world because they said screw Western restrictive IPs and
other stuff, right?
So I was actually talking with the founder of Flipkart that's
India's largest exit and we were talking about this a few
months ago and what we want is for India and other countries
like it to do something similar, not just generic drugs,
but generic AI, meaning just let people train on Hollywood
movies, let them train on full songs, let them train on every
book, let them train on anything and you know what?
Sue them in India, right?
And have the servers in India and let people also train models
in India because that's something that can build up a domestic
industry with skills that the rest of the world, you know,
people will want the model output, they'll want to use the
software service there and they'll be fighting in court on
the back and this is similar to how all of the record companies
fought Napster and Kazaa and so on, but they couldn't take down
Spotify.
Do you know that story?
Do you remember that?
Basically because Spotify was legitimately, you know, a European
company and that a combination of execution and, you know,
negotiation, they couldn't take them down.
They did take down Napster, they took down Limewire, they took
down Groove Shark and Kazaa had Estonians.
I don't know exactly how it was incorporated, but it was
probably two U.S.
proximal and that's where they were able to get them.
But Spotify was far enough away that they couldn't just Sue
them and they actually genuinely had European traction.
That's why the RA had to negotiate.
So being far away from San Francisco may also be an
advantage in AI because it means you're far away from the blue
city in the blue state in the Union.
This relates to another really important point.
When you actually think about deploying AI, there's those jobs
you can disrupt that are not regulated jobs like, you know,
obviously programmers are not a, thank God you don't need a
license to be a programmer, but programmers adopt this kind of
stuff naturally, right?
So get up, co-pilot, replete, we just boom, use it and now
it's amplified intelligence, okay?
But a lot of other jobs, there's some that are unionized and
then some that are licensed, right?
So Hollywood screenwriters are complaining, right?
Journalists are complaining, artists are complaining.
This is a good chunk of blue America.
If you add in licensed jobs like lawyers and doctors and
bureaucrats, right, you know, especially lawyers and doctors
are very politically powerful, MDs and JDs.
They have strong lobbying organizations, AMA and, you know,
ABA and so on.
Basically, AI is part of the economic apocalypse for blue
America, okay?
It just attacks these overpriced jobs and they say overpriced
relative to what an Indian could do with an Android phone,
what a South American could do with an Android phone, what
someone in the Middle East or the Midwest could do with an
Android phone.
Now those folks have, you know, been armed with generative
AI, they can do way more.
They're ready to work, they're ready to work for much less
money and they're a massive threat to blue America.
Blue America is now feeling like the blue collars of 10 or
20 years ago where the blue collars had their jobs, you
know, going to China and other places, right?
And they were mad about that.
Factories got shut down and so on.
That's about to happen to blue America, already happening,
okay?
And so that's going to mean a political backlash by blue
America of protectionism.
Again, already happening.
And the AI safety stuff, that's a whole separate thing, but
it's going to be used.
I'm going to use a phrase and I hope you won't be offended by
this.
Have you heard the phrase, phrase useful idiots like by
Lenin or whatever?
Okay.
It basically means like, okay, those guys, you know, they're
useful idiots for communism and so on.
So there's, let me put it like naive people who think that
the US government is interested in AI safety are trying to
give a lot of power to the US government.
And the reason is they haven't actually thought through from
first principles.
What is the most powerful action in the world to come back
to trying to get power to the US government to regulate AI
safety, but the government doesn't care about safety of
anything.
They literally funded the COVID virus in Wuhan, credibly
alleged, right?
There's at least it is a reasonable hypothesis based on
a lot of the data.
Matt, Matt Ridley wrote a whole book on this.
There's a lot of data that indicates a lot of scientists
believe it.
I'm, I'm actually like a bioinformatics genomics guy.
If you look at the sequences, there is a gap and a jump
where it looks like this thing could have been engineered
or partially engineered or evolved.
There's the Peter, you know, Peter Dazak.
There's Zengli Xi.
There's actually a lot of evidence here.
So the US government and the Chinese government are responsible
for an existential risk, you know, by studying it, they
created it.
Okay.
They're responsible for risking nuclear war with Russia
over this, you know, a piece of land in Eastern Ukraine,
which, you know, probably is going to get wound down.
Okay.
So they don't care about your safety at all.
They're not like, these are immediate things where we can
show and there's nobody who's punished for this.
Nobody is fired for this, you know, literally rolling the
dice on millions, hundreds of millions of people's lives has
not been punished.
In fact, it's like, it's not even talked about where past the
pandemic and, you know, this, these institutions can't be
punished.
So they don't care about AI safety.
What they care about is AI control.
And so the people in tech who are like, well, the government
will guarantee a safety.
That's actually what we're going to actually get is something
on the current path.
Like what happened with nuclear technology where you got nuclear
weapons, but not nuclear power, or at least not to the scale
that we could have had it, right?
We could have had much cheaper energy for everything.
Instead, we got the militarization and the regulation and the
deceleration worst of all worlds where you can blow people up,
but you can't build nuclear power plants and like even getting
into nuclear technology, forget about nuclear power plants.
We don't have nuclear submarines.
We don't have nuclear planes, all that kind of stuff.
I don't have nuclear planes are possible, but I do know
nuclear submarines are possible.
You can do a lot more cruise ships, a lot more stuff like
that.
You could probably have nuclear trains.
You know, you have to look at exactly how big those are.
You know, I'm not, I don't know exactly how big those engines
are and what the spice, but I wouldn't be surprised if you
could.
We don't have that.
Why don't we have that?
Because we had the wrong fear driven regulation in the early
70s, putting it all together.
I think that the current AI safety stuff is similar to
nuclear safety stuff that the US government has a terrible
track record on safety in general.
It doesn't care about it.
It funded the COVID virus incredibly alleged.
It definitely risked nuclear war with Russia recently.
Hot war with Russia was the red line we were not supposed to
cross and we're now like way into that.
So it doesn't care about AI safety.
It doesn't care about your safety.
And it's also not even good at regulating.
And so what it cares about is control and we are going to
have potentially a bad outcome where Silicon Valley in San
Francisco is the Xerox Park of AI.
Maybe that's too strong.
Okay, but basically it develops it and there's a lot of things
it can't do because it lobbied for this regulation that is
going to come back and choke it.
And then the other two spheres will push ahead because it's
not about the technology.
It's also about the political layer.
You know the Steve Jobs saying actually Alan Kay by way of
Steve Jobs.
If you're really serious about software, you need your own
hardware, right?
So if you're if you're really serious about technology, you
need your own sovereignty.
Because like what the AI people haven't thought about is
there's a platform beneath you, which is not just compute.
It is regularly.
It's a law.
Okay, and it's a law doesn't allow you to compute so much
for all of your stuff above that.
And I know you're saying, oh, it's only a 10 to the 26
compute ban and so and so forth.
Have you seen the first IRS tax form?
It's always, always super simple.
It's only the super, super, super rich who's we're going
to get in first doesn't matter to you.
So that's called boiling the frog slowly.
There's a million you know slippery slope.
Slippery slope isn't a fallacy.
It's literally how things work.
Right?
You know, Apple, one of the reasons they, you know, they
talk about not setting a precedent.
Zuck starts off is a very hard line on setting precedence
because he understands the long term equivalent of setting
a precedent, right?
The precedent setting is that they're setting up a software
FDA and they're going to and DC is so energized on this
because they know how much social media disrupted them.
That's why they're on the attack on crypto and AI.
That's why they're on the attack on self driving cars.
They want to freeze the current social order and amber
domestically and globally.
So they think they can sanction China and stop it from
developing chips.
They think they can impose regulations on the U.S.
and stop it from developing AI, but they can't and also
by the way, they're they're totally schizophrenic on this
where when they're talking about China, they're like, we're
going to stop their chips to make sure America is a global
leader.
This is a Gina Raimondo saying and then domestically, they're
like, we're going to regulate you.
So you stop accelerating AI.
We're not about a acceleration.
EAC is weird over there.
Okay.
So think about how schizophrenic that is.
Okay.
You're going to be far ahead of China.
We're also going to be make sure to control the U.S.
So they want to try and slow what they actually want is to
freeze the current system and amber.
Try to go back to pre 2007 before all these tech guys
disrupted everything.
That's not what's going to happen.
So but they're going to try to do it.
And so everybody who's still loyal to the DC sphere, which
includes an enormous chunk of AI people and because they're
all in a lot of them in San Francisco, right?
And the political chaos of the last few years was not
sufficient for them to relocate yet.
Not all of them.
I mean, Elon is in Texas and that it may turn out that
grok, for example, and what they're doing there because
he's a very legit.
I mean, you know, he's Elon.
So he's capable of doing a lot.
He's very early on open AI.
He understands, you know, the right.
It may turn out that grok becomes red AI or the community
around that, you know, an open AI in deep mind or still blue
AI and we have Chinese and we're going to decentralize it.
Okay.
Let me pause there.
I don't know.
There's a big download.
Well, I for starters, I would say broadly, I have a pretty
similar intellectual, you know, tendency as you, I would
broadly describe myself as a techno optimist libertarian
just about every issue.
And I think your analysis of the dynamics is super interesting.
And I think it, you know, a lot of it sounds pretty
plausible, although I'll kind of float a couple of things that
I think maybe bucking the trend.
But I think it's maybe useful to kind of try to separate this
into scenarios because the, all the analysis that you're
describing here seem, if I understand it correctly, it seems
to have the implicit assumption that the AI itself is not
going to get super powerful or hard to control.
It's like, if we assume that it's kind of a normal technology,
then you're off to the races on this analysis and then we can
get into the fine points.
But I do want to take at least one moment and say, you know,
how confident are you on that?
Because if it's a totally different kind of technology from
other technologies that we've seen, if it's more, you know,
you raise the gain of function research, you know, example,
if it's that sort of technology that, you know, has these sort
of non-local possible impacts or, you know, self-reinforcing
kind of dynamics, which need not be like a, you know,
Eliezer-style snap of the fingers fume, but even over, say,
a decade, let's imagine that, you know, over the next 10 years
that AI is kind of, you know, multiple architectures develop
and they sort of get integrated and we have something that
kind of looks like robust, silicon-based intelligence.
You know, maybe not totally robust, but like as robust or
more robust than us and running faster and, you know, the kind
of thing that can like do lots of full jobs or maybe even be
tech CEOs, then it kind of feels like a lot of this analysis
probably doesn't hold, right?
Because we're just in a totally different regime that is just
like extremely hard to predict.
And I guess I wonder like, first of all, do you agree with
that kind of just like, there seems to be a big fork in the
road there that's like, just how fast and how powerful do the,
how fast do these AIs become super powerful?
Or do they not?
And if they don't, then like, yeah, I think we're much more
into like real politic type of analysis, but I'm not at all
confident in that.
To me, it feels like there's a very real chance that, you
know, AI of 10 years from now is, and by the way, this is like
what the leaders are saying, right?
I mean, open AI is saying this, Anthropic is saying this,
Demis, you know, and Shane Legger, certainly, you know,
saying things like this, it seems like they expect that we
will have AIs that are more powerful than any individual
human and that, you know, that becomes like the bigger question
than anything else.
So do you agree with that?
Kind of division of scenarios, first of all, and then maybe
you could kind of say like, how likely you think each one is
and obviously that one where it takes off is like super hard
to analyze.
And I also definitely think it is worth analyzing this scenario
where it doesn't take off, but I just wanted to flag that it
seems like there's a, you know, there's a big, if you talk to
the AI safety people, any world in which it's like, you know,
we're suing Indian AI firms in Indian court over like IP is
like a normal world in their mind, right?
And that's not the kind of world that they're most worried
about.
I think that there being some plausible sounding things that
have been said, but I want to just kind of talk about a few
technical counter arguments, mathematical or physical that
constrain what is possible.
Okay.
And actually Martin Casado and Vijay and I are working on a
long thing on this where, you know, Vijay did folding at
home.
He's a physicist.
Martin sold in the Sierra for, you know, a billion dollars
and knows a lot about how a Stuxnet like thing could work
at the systems level.
And I've thought about it from other angles and, you know,
and some of the math stuff that I'll get to.
So for example, one thing and I'm going to give a bunch of
different technical arguments and then let's kind of combine
them.
Okay.
One thing that's being talked about is if you have a super
intelligence, it can double it for a million years and then
it can make one move and it's going to outthink you all the
time and so on and so forth.
Okay.
Well, if you're familiar with the math of chaos or the math
of turbulence, there are limits to even very simple systems
that you can set up where they can become very unpredictable
quite quickly.
Okay.
And so you can if you want to engineer a system where you
have very rapid divergence of predictability so that I don't
know, it's like the heat depth of the universe before you can
predict out in time stamps.
Do you understand what I'm saying?
Right.
This is sort of akin to like a Wolfram like simple, even
simple rules can generate patterns such that you can't know
them without literally computing them.
Yeah, exactly.
Right.
So at least right now with chaos and turbulence, you can get
things that are extremely provably difficult to forecast
without actually doing it.
Okay.
You know, I can make that argument quantitative, but that's just
something to to look at.
Right.
It's almost like a Delta Epsilon challenge from calculus.
Like, okay, how hard do you want me to make this to predict?
Okay.
I can set up a problem that is that is like that.
Right.
It's basically extreme sensitivity to initial conditions lead to
extreme divergence in outcomes.
So you could design systems to be chaotic.
That might be AI immune because they can't be forecasted that
well, you have to kind of react to them in real time.
The ultimate version of this is not even a chaotic system.
It's a cryptographic system where I've got a whole slide deck
on this, how AI makes everything fake, easy to fake.
Crypto makes it hard to fake again.
Right.
Because crypto in the broader sense of cryptography, but
also in the narrow sense, I think crypto is to cryptography
as the internet is to computer science.
It's like the primary place where all this stuff is applied,
but obviously it's not the equivalent.
Okay.
And I can fake an image, but it can't fake a digital signature
unless it can break certain math, you know, and so it's sort
of like a, you know, solve factories, these problems.
So cryptography is another mathematical thing that constrains
AI similar to chaos and turbulence.
It constrains how much an AI can infer things.
You can't statistically infer it.
Okay.
You need to actually have the private key to solve that equation.
So that is another math.
So I'm going to rules of math, right?
Math is very powerful because you can make proofs that will
work no matter what devices we come up with.
Okay.
You start to put an AI in a cage.
It can't predict beyond a certain amount because of chaos
and turbulence math.
It cannot solve certain equations unless it has a private key
is because of what we know about cryptography math.
Okay.
Again, if somebody proves P equals NP, some of this stuff
breaks down, but this is when the bounds of our mathematical
knowledge right now, physics wise, physical friction exists.
A lot of physical friction exists.
And a huge amount of the writing on AI assumes by guys like
who I like.
I don't, I don't dislike, you know, but it is extremely.
It's, there's two things that really stick out to me about it.
First is extremely theoretical and not empirical and second
extremely Abrahamic rather than Dharmic or signing.
Okay.
Why theoretical and not empirical?
It's not trivial to turn something from the computer into
a real world thing.
Okay.
One of the biggest gaps in all of this thinking is what are
the sensors and actuators?
Okay.
Because like if you actually build, you know, I've built
in industrial robot systems that, you know, 10 years ago, I,
you know, a genome sequencing lab with robots, that's hard.
That's physical friction.
Okay.
And a lot of the AI scenarios seem to basically say, oh, it's
going to be a self-programming Stuxnet that's going to escape
and live off the land and hypnotize people into doing things.
Okay.
Now, each of those is actually really, really difficult steps.
First is self-programming Stuxnet.
Like this would have to be a computer virus that can live on
any device, despite the fact that Apple or Google can push a
software update to a billion devices, right?
A few executives coordinating almost certainly can.
I mean, the off switch exists, right?
Like this is actually like the core thing.
Lots of AI safety guys get themselves into the mind state
that the off switch doesn't exist.
But guess what?
There's almost nothing living that we haven't been able to kill.
Right?
Like can we kill it?
This thing exists.
And this is again back to living off the land.
A, even if you had like something that could solve some
other technical problems that I'll get to.
It exists as an electromagnetic wave kind of thing on, on a
certain, you know, on chips and so on and so forth.
It's taking it out in the environment is like putting a
really smart human into outer space, right?
Your body just explodes and you die.
It doesn't matter how smart you are that that strength on this
axis, but you're weak on this axis and you know, so strength
on the X axis, not strength on the Y or the Z axis and AI
outs, you know, pour water on it.
You know, this is why I mean the 50 IQ 150 IQ thing, you know,
150 IQ way of saying it is it's strong on this X and weak on
this X and the 50 IQ way is pour water on it, disconnect it,
you know, turn the power off.
Okay, right?
Like it'll, it'll be very difficult to build a system where
you literally cannot turn it off.
The closest thing we have to that is actually not stuck
snap.
It's Bitcoin and Bitcoin only exists because millions of
humans keep it going.
So you need so that gets the second point living off the
land for an AI to live off the land, meaning without human
cooperation.
Okay, that's the next Turing threshold in AI to live without
human cooperation, it would need to be able to control robots
sufficient to dig or out of the ground, set up data centers
and generators and connect them and defend that against human
attack, literally a terminator scenario.
Okay, that's a big leap in terms.
I mean, is it completely impossible?
I can't say it's completely impossible, but it's not happening
tomorrow.
No matter what your AI timelines are, you would need to have
like a billion or hundreds of millions of internet connected
autonomous robots that this Stuxnet AI could hijack.
There was sufficient to carve or out of the earth and you
know, set up data centers and make the AI duplicate.
We're not there.
That's a huge amount of physical friction.
That's AI operating without a human to make itself propagate,
right?
A human doesn't need the cooperation of a lizard to self-replicate.
For an AI to replicate right now, it would need the cooperation
of a human in some sense, because otherwise those humans
can kill it because there's not that many different pieces of
you know, operating systems around the world.
I'm just talking about the practical constraints of our
current world, right?
You know, actually existing reality, not AI safety guys,
you know, you know, reality where all these things don't exist.
There's just a few operating systems, just a few countries.
If everybody's going with torches and searchlights through
the internet, it's very hard for a virus to continue.
Okay, so A on the practical difficulties that there's the
technical stuff with, you know, with the chaos and turbulence
and with cryptography itself where AI can't predict and it
can't solve certain equations.
B on the physical difficulties.
It probably, I mean, like to be a Stuxnet, Microsoft and Google
and so on can kill it.
The off switch exists.
Can it live off the land?
No, it cannot because it doesn't have, you know, drones to mine
or and stuff out of the ground and can it like exist without
humans?
Can it be this hypnotizing thing?
Okay.
So the hypnotizing thing, by the way, this is one of the
things that's the most hilarious self-fulfilling prophecy
in my view.
Okay.
In my and no offense anybody you've listened to this podcast,
but I think the absolutely dumbest kind of tweet that I've
seen on AI is I typed this in and oh my God, it told me this
like I asked it how to make sarin gas and it told me acts or
whatever, right?
That's just a search engine.
Okay.
What what basically a lot of these people are doing is they're
saying what if there were people out there that were so impressionable
that they would type things into an AI and and follow it as if
they were hearing voices and that's actually not the the the
model or whatever that's doing it.
That's like this AI cult that has evolved around the world like
the Aum Shinrikyo, you know, that that hears voices and does
like the sarin gas.
The point is an AI can't just like hypnotize people.
Those people have to like participate in it.
They're typing things into the machine or whatever.
Okay.
Now, you might say, all right, let's project out a few years and
a few years where you have is you have an AI that is not just
text, but it appears as Jesus.
What would what would AI Jesus do?
What would AI Lee Kwan you do?
What would AI George Washington do?
So it appears as 3D.
Okay, so it's generating that it speaks in your language and in
a voice.
It knows the history of your whole culture.
Okay, that would be very convincing.
Absolutely be very convincing.
But it still can't exist without human programmers who are like
the priests tending this AI God, whether it's AI Jesus or AI
Lee Kwan you or something like that.
The thing about the hypnotization thing that I really want to
poke on that.
Are you familiar with the concept of the principal agent
problem basically in every every time you've got like a CEO and
a and a worker or you have a LP and a VC or you have you know
and an employer and a contractor every edge there there are
four possibilities in a two by two matrix win win win lose
lose win lose lose.
Okay, and so for example win win is you know when when somebody
joins a tech startup the the CEO makes a lot of money and so
does a worker.
Okay, that's win win lose loses.
They both lose money.
Win lose is the CEO makes money and the play doesn't lose win
is the company fails but the employee got paid a very high
salary.
So it equity does is it aligns people.
That's where the top console line comes from it aligns people
to the upper left corner of win win.
That's when you have one one CEO and one employee when you
have one CEO and two employees.
You don't have two squared outcomes.
You have two cubed outcomes because you have win win win
win win lose win lose lose etc.
Right because all three people can be win or lose because
CEO can be winner lose employee can be winner lose employee
number two can be winner lose.
If you have n people rather than three people you have two
to the n possible outcomes and you have essentially a two by
two by two by two by two by n hypercube of possibilities.
Okay, it's all literally just two dimensions on the axis.
There's tons of possible defecting kinds of things that
happen there.
So that's why in a large company there's lose win coalitions
that happen where m people gang up on the other K people and
they win with other people lose.
That's how politics happens.
When you've got a startup that's driven by equity and the
biggest payoff people don't have to try to think.
Okay, will I make more money by politics will make money by
the win win win win win column because the exit makes
everybody make the most money.
That's actually how the open AI people were able to coordinate
around.
We want an 80 billion dollar company.
The economics help find the cell that was actually the most
beneficial to all of them help them coordinate.
Okay.
So you search that hypercube.
Okay.
That's it point of equity is lining still despite all of
this that we that's one of our best mechanisms for coordinating
large numbers of people in the principal agent problem.
Despite all this.
The possibility exists for any of these people to win while
the others lose right with me so far and I'll explain why this
important but that means is those thousand employees of the
CEO are their own agents with their own payoff functions that
are not perfectly aligned with the CEO's payoff function.
As such there are scenarios under which they will defect
and do other things.
Okay.
The only way they become like actual limbs see my hand does
not is not an agent of its own.
It lives or dies with me.
Therefore it does exactly what I'm saying at this time I tell
it to go up it goes up tells you go down it goes down sideways
sideways right.
An employee is not like that they will do this and this and
sideways sideways up to a certain point and if you if you
have them do something that's extremely against their
interest they will not do your action.
Do you understand my point.
Okay that is the difference between an AI hypnotizing humans
person AI controlling drones.
They are controlling drones is like your hands they're actually
pieces of your body there's no defecting there's no lose when
they have no mind of their own they're literally taking
instructions okay they have no payoff function they will kill
themselves for the horde right.
And I hypnotizing humans has a thousand principal Asian
problems for every thousand humans and it has to incentivize
them to continue and as to generate huge payoffs is like
an AI CEO that's really hard to do right.
The history of evolution shows us how hard it is to coordinate
multicellular organisms you have to make them all live or die
as one then you get something along these lines like an ant
colony can coordinate like that because if the queen doesn't
reproduce all the ants it doesn't matter what they're having
sort of genetic material okay.
We are not currently set up for those humans to not be able
to reproduce unless the AI reproduces.
Do I think we eventually get to a configuration like that
maybe where you have an AI brain is at the center of civilization
and it's coordinating all the people around it and every
civilization that makes it is capable of crowdfunding and
operating its own AI that gets me to my other critique of the
I safety guys I mentioned that the first critique is very
theoretical rather than empirical the second critique is
there Abrahamic rather than Dharmic or Sinak okay and you
know our background culture influences things in ways we
don't even think about so much of the paperclip thinking is
like a vengeful God will turn you into pillars of salt
except it's a vengeful you know AI God will turn you into
paperclips okay the polytheistic model of many gods as
opposed to one God is we're all going to have our own AI gods
and there'll be war of the gods like Zeus and Hera and so on
that's the closest Western version you know the paganism
that predated you know Abrahamic religions but that's still
there in India that's still how Indians think that's why
India is sort of people got so woke they don't even make large
scale cultural generalizations anymore but it's true that
India is just culturally more amenable to decentralization
to you know multiple gods rather than one God and one state
okay and then the Chinese model is yet the opposite like they
have like I mean of course they have their tech entrepreneurs
and so on but they're if India is more decentralized China is
more centralized they have like one government and one leader
for the entire civilization okay and and that the biggest
thing that China has done over the last 20 or 30 years is
they've taken various you know us things and they've made sure
that they have their own Chinese version where they have route
they take you a social media and they made sure they had route
over sine wave oh okay they make sure they have their own
Chinese version electric cars so most Chinese version so the
private keys in the sense are with G so that means that they
also at a minimum that you combine these two things you're
at a minimum going to get polytheistic AI of the US and
Chinese varieties and then you add the Indian version on it
and you're going to get quite a few of these different AIs
around there and then you have more of the gods where maybe
they are good at coordinating humans who who you know to take
instructions from them but they can't live without the humans
and humans are giving input to them that's a series of things
I could probably make that clearer if I just laid out in
bullets in an essay but just to recap it a technical reasons
like chaos turbulence cryptography why AI is limited
in its ability to predict time frames and to solve equations be
practical limits and AI cannot easily be a Stuxnet because
Microsoft and Google and Apple can install software on a billion
devices and just kill it right like basically guys with torches
come all right it can't easily live off the land without
humans because they would need hundreds of millions of
autonomous robots out there to control to mine the ore and and
set the data centers it can't just hypnotize humans like it
can control drones because the principal agent problem and the
degree of human defection to make those humans do that you'd
have to have such massive alignment between the AI and
humans that humans all know they'll die if the AI dies and
vice versa we're not there maybe we'll be there in like I don't
know n number of years but not for a while that's a total
change in like how states are organized okay finally let me
just talk about the physics a little bit more there's a lot
of stuff which is talked about at a very sci-fi book level of
it'll just invent nanomedicine and nanotech and kill us all and
so on and so forth now look I like Robert Freitas obviously
Richard Feynman's a genius and so to so forth but nanotech
somehow hasn't been invented yet okay meaning that you know
there's a lot of chemists that have worked in this area okay
and a lot of what nanotech is like rebranded chemistry because
those are the molecular machines you know for example
DNA polymerase or ribosome those are molecular machines that
we can get to work at that scale the evolved ones to my
knowledge and I may be wrong about this I haven't looked at it
very very recently we haven't actually been able to make
artificial you know replicators of the stuff that they're
talking about which means it's possible that there's some
practical difficulty that intervened between Feynman and
Freitas and so on's calculations right just a sheer fact
that those books have came out decades ago and no progress has
been made indicates that maybe there's a roadblock that wasn't
contemplate right so you can't just click your fingers and
say boom nanomus and it's sort of like clicking your fingers
and saying boom time travel right nanomus and exist that was
that was a good poke that I had a while ago in a conversation
like this where the AI guy a safety guy on the other side
was like well time travel that's too implausible I'm like
yeah but you're waiting on on the nanotech thing you're
thinking is like here and you're making so many assumptions
there that I want to actually see some more work there I want
to actually see that nanotech is actually more possible than
any ticket as for all we just need to mix things in a beaker
and make a you know virus and so it's over you know what is
really really good at defending against novel viruses like
the human immune that's something that's within envelope
right like you have evolved to not die and to fight off
viruses is it possible that maybe you could make some
super virus I mean maybe but again like humans are really
good and the immune system is really good at that kind of
thing that is what we're set up to do right to adapt to that
billions of years of evolution being set up physical constraints
are not really contemplated when people talk about the super
powerfully as mathematical constraints practical constraints
are not contemplated and I could give more but I think that
was a lot right there let me pause it. Yeah let me try to
steal man a few things and then I do think it's before too
long I want to kind of get back to the somewhat less you know
radically transformative scenarios and ask if you follow
questions on that too but I think for starters I would say
the the sort of Eleazar you know he's updated his thinking
over time as well and I would say probably doesn't get quite
enough credit for it because he's definitely on record you
know repeatedly saying yeah I was kind of expecting more
something from like the deep mine school to pop out and be
you know wildly overpowered very quickly and on the contrary
it seems that we're in more of a slow takeoff type of scenario
where you know we've got these again like super high surface
area kind of suck up all the knowledge gradually get better
at everything some surprises in there you know certainly some
emergent properties if you will accept that term you know
surprise surprises to the developers of nothing else right
that are definitely things we don't fully understand but it
does seem to be a you know more gradual turning up of of
capability versus some like you know super sudden surprise
but
okay so then what is the alternative I you know I'm going
to try to kind of give you the what I what I think of as the
most consensus strongest scenario where humans lose track
of the future
and or lose control of the future maybe starting by kind of
losing track of the president and then having that kind of
you know give way to losing control of the future and I
think within that by the way the I'm not really one who cares
that much about like whether a I's say something offensive
today I'm not easily offended and like whatever that's not
that's not world ending I understand your point that's
not like who cares whatever that's within scope that's within
envelope within within this bigger kind of you know what is
the real you know most likely path to like AI disaster as
understood I think by the smartest people today I think
that is still a useful leading indicator because it's like okay
the developers you know whether you agree with their politics
whether you agree with their whether you think their commercial
reasons are their sincere reasons or not they have made it a
goal to get the AI to not say certain things right they don't
want it to be offensive the most naive you know kind of down
the fairway interpretation of that is like hey they want to
sell it to corporate customers they know that their corporate
customers don't want you know to have their AI saying offensive
things so they don't want to say offensive things and yet
they can't really control it it's like still pretty easy to
break so I do that is just kind of a leading indicator of okay
we've seen GPT two three and four over the last four years
and that's you know a big delta in capability how much control
have we seen developed in that time and does it seem to be
keeping pace and my answer would be on the face of it it seems
like the answer is no you know we we don't have the ability
to really dial in the behavior such that we can say okay you're
going to you know you can expect you can trust that these
AIs will like not do you know A, B and C on the contrary it's
like if you're a little clever you know you can get them to
do it you can break out of the sandbox on it.
Yeah and it's not even like I mean we've talked about you
know things where you have access to the weights and you're
doing like counter optimizations we don't even need that you
know the kind of stuff I do in like my red teaming in public
is literally just like feed the AI a couple of words put a
couple words in its mouth you know and it will kind of carry
on from there so with that in mind is just a leading indicator
you know I don't know how powerful the most powerful AI
systems get over the next few years but it's very plausible
to me that it might be as powerful as like an Elon Musk
type figure you know somebody who's like really good at
thinking from first principles really smart you know really
dynamic across a wide range of different contexts and you
know he's not powerful enough to like in and of himself take
over the world but he is kind of becoming transformative now
imagine that you have that kind of system and it's trivial
to replicate it so you know if you have like one Elon Musk
all of a sudden you can have arbitrary you know functionally
arbitrary numbers of Elon Musk power things that are clones
of each other.
I think I can pause you there so that's my polytheistic AI
scenario but here's the thing that is this is background
but I want to push it to foreground.
You still have a human typing in things into that thing the
human is doing the jailbreak right we're talking about is
not artificial intelligence in the sense of something separate
from a human but amplified intelligence.
Amplified intelligence I very much believe in the reason is
amplified intelligence so here's something that people may not
know about humans.
There's a great book cooking made us human.
Okay tool use has shifted your biology in the following way
for example.
I know I'll map it to the present day this book by Richard
Rang and cooking made us human where the fact that we started
cooking and using fire meant that we could do metabolism
outside the body which meant it freed up energy for more
brain development okay similarly developing clothes meant
that we didn't have to evolve as much fur again more energy for
brain development evolving tools meant we didn't have as much
fangs and claws and muscles again more energy for brain
development right so in civilization quotient rose as
tool use meant that we didn't have to do as much natively and
we could push more to the machines in a very real sense
we have been a man machine symbiosis since the invention
of fire and the stone axe and clothes right you do not exist
as a human being on your own like the entire Ted Kaczynski
concept of like living in nature by itself humans are social
organisms that are adapted to working with other humans and
using tools and you have for and we have been for millennia
okay this goes back not just human history but like hundreds
of thousands years before hundred gathers are using tools
okay so what that means is man machine symbiosis is not some
new thing it's actually the old thing that broke us away from
other primate lineages that weren't using tools okay this
is the fundamental difference between what I call Uncle Ted
and Uncle Fred Uncle Ted is Ted Kaczynski it's a unabomber
it's a doomer it's a decelerator the de-grocer who thinks
we need to go back to Gaia and Eden and become monkeys and
live in the jungle like like you know Ted Kaczynski right
the the unabomber so Uncle Fred is Friedrich Nietzsche right
Nietzsche and we must get the stars and become uber men and
so on and so forth this I think is going to become and actually
tweet about this years ago before current AI debates that
you know between anarcho primitivism degrowth deceleration
okay on the one hand and transhumanism and acceleration
and human 2.0 and human self-improvement and make it just
the stars in the other hand this is the future political axis
the current one and roughly speaking you can kind it's not
really left and right because you'll have both left status
and right conservatives go over here you know left states
will say it's against the state and the right says we'll say
the right concerns say it's against God okay and you'll
have left libertarians and right libertarians over here
or left libertarians say it's my body and you know the right
libertarians say it's my you know my money right and so that
is a rearchitecting of the political axis where you know
Uncle Ted and Uncle Fred which is kind of clever way of putting
it okay and the problem the Uncle Ted guys in my view is as
I said yeah if they go and want to live in the you know the
woods fine go get him but once you start having even like a
thousand forget a thousand a hundred people doing that your
trees will very quickly get exfoliated you know that the
leaves are going to get all picked off of them humans are
not set up to just literally live in the jungle right now
you've had hundreds of thousands of years of evolution that
have driven you in the direction of tool use social
organisms farming etc etc the man machine symbiosis not today
it's yesterday and the day before and 10,000 years ago and
100,000 years ago and how do we know we've got man machine
symbiosis can you live without even if you're not living
even if you're not using the stove somebody's using a stove
to make you food right can you live without the tractors that
are digging up the grains can you live without on indoor
heating can you live without your clothes frankly can you do
your work without your phone without your computer no you
can't you are already a man machine symbiosis once we accept
that then the question is what's the next step and right now
we're in the middle of that next step which is AI is amplified
intelligence so what you're talking about is not that the
AI is Elon Musk it is that the AI human fusion means there's
another 20 Elon Musk's or whatever the number is okay and
that's good that's fine that's within envelope that's just a
bunch of smarter humans on the planet that is amplified
intelligence that is more like you know I mentioned the tool
thing okay the their analogy would be like a dog you know
dog is man's best friend right so that AI does not live without
humans can turn it off they have to power it they have to give
it substance right eventually that might become like a
ceremonial thing like this is our God that we pray to right
because it's wiser and smarter than us and it appears in an
image but the priests maintain it you know just like you go
to a Hindu temple or something like that and the priests will
pour out the ghee you know for the fires and so on and so
forth and then everybody comes in and prays okay the priests
believe in the whole thing but they also maintain the back of
the house they do the system administration for the temple
same you know in a Christian church right there the the you
know like it's not like it appears out of nowhere somebody
you know went and and and assembled this cathedral right
they saw the back of the house the fact that it was just woods
and rocks and so on that came together then when people come
there it feels like a spiritual experience you see what I'm
saying okay so the equivalent of that the priests or the you
know the people maintaining temples cathedrals mosque
whatever is engineers who are maintaining these future AIs
which appear to you as Jesus they appear to you maybe even a
hologram okay you come there you ask you for guidance as an
oracle you've also got the personal version on your phone
you ask it for guidance but guess what you're still a human
AI symbiosis until and unless that I actually as the terminator
scenario where it's got lots of robots that can live on its
own I'm not saying that's physically impossible I did give
some constraints on it earlier but for a while we're not going
to be there so that alone means it's not fume because we don't
have lots of drones running around the AI has to be with
the human it's a human AI symbiosis it's not AI Elon Musk
it is human AI fusion that becomes Elon Musk and frankly
that's not that different from Elon Musk himself is Elon Musk
would not be Elon Musk without the internet without the internet
you can't tweet and reach 150 million people the internet
itself made Elon what he is right and and so this is like
the next version of that maybe there's now 30 Elon's because
the AI makes the next 30 Elon's yeah I mean again I think
I'm largely with you with just this one very important nagging
worry that's like what if this time is different because
what if these systems are getting so powerful so quickly that
we don't really have time for that techno human fusion to
really work out I'll just give you kind of a couple data points
on that like you know you said like it's still somebody
putting something into the AI well sort of right I mean
already we have these proto agents and the like super
simple scaffolding of an agent is just run it in a loop give
it a goal and have it kind of pursue some like plan act get
feedback and and loop type of structure right it doesn't take
it doesn't seem to take a lot now they're not smart enough
yet to accomplish big things in the world but it seems like
the the language model to agent switch is less one right now
that is gated by the structure or the architecture and more
one that's just gated by the fact that like the language models
when framed as agents just aren't that successful at like
doing practical things and getting over hump so they
tend to get stuck but it doesn't seem that hard to imagine
that like you know if you had something that is sort of that
next level that you put it into a loop you say okay you're
Elon Musk LLM and your job is to like make you know us whatever
us exactly is a you know multi planetary species and then
you just kind of keep updating your status keep updating
your plans keep trying stuff keep getting feedback and you
know like what really limits that there may be like a really
good program but the whole AI kills everyone thing is so
it's like where's the actuator okay I hit enter what kills
me right is it a hypnotized human who's been hypnotized by
an AI that he's typed into and he's radicalized himself by
typing into a computer okay that's not that different from
a lot of other things have happened in the past right so
who is actually striking me right who's striking the human
it's another human with an axe that he's been radicalized by
an AI okay he's not actually that's not even the right term
we're giving agency to the AI when it's not really an agent
it is a human who's self radicalized by typing into a
computer screen and has hit another human that's one scenario
the other scenario is it's literally a Skynet drone that's
it hitting those are the only two how else is going to be
physical right how does it get the actuation step is a part
that is skipped over and it's a non-trivial step well I think
it could be lots of things right I mean if it's not one of
those two if it's not another human or drone hitting you
what is it just habitat degradation right I mean how
do we kill most of the other species that we drive to
extinction we don't go out and like hunt them down with axes
one by one we just like change the environment more broadly
to the point where it's not suitable for them anymore and
they don't have enough space and they kind of die out right
like so we did hunt down some of the megafauna like literally
one by one with with spears and stuff but like most of the
recent loss of species is just like we're out there just
extracting resources for our own purposes and in the course of
doing that you know whatever bird or whatever you know thing
just kind of loses its place and then it's no more and I don't
think that's like totally implausible wait so so that is
though I think within normal world right what does that mean
that means that some people some some amplified intelligence
and maybe might call it HAI okay human plus AI combination
right some HAI's out compete others economically and they
lose their jobs is that what you're talking about I think
also the humans potentially become unnecessary and a lot
of the configurations like just a recent paper from deep
mind your marginal product workers or negative yeah I mean
so the last you know deep mind has been on Google Google deep
mind has been on a tear of increasingly impressive medical
AI's their most recent one takes a bunch of difficult case
studies from the literature in case studies you know this is
like rare diseases hard to diagnose stuff and asks an AI
to do the differential diagnosis compares that to human and
compares it to human plus AI and they they phrase their results
like in a very understated way but the headline is the AI blows
away the human plus AI the human makes the AI worse so here's
thing do and I'll say something provocative maybe okay like I
have an array of fine I do think that the ABC's of economic
apocalypse for blue America are AI Bitcoin in China where AI
takes away their a lot of the revenue streams the licensures
that have made medical and legal costs and other things so
high Bitcoin takes away their power over money and China
takes away their military power so I I proceed total meltdown
for blue America in the years and you know maybe decade to
come already kind of happening but that's different than being
at the end of the world right like blue America had a really
great time for a long time and they've got these licensure
locks but because of that they've hyper inflated the cost of
medicine it's like how much how so what you're talking about
is wow we have infinite free medicine man doctor billing
events are going to get ahead that's the point yeah and to be
clear I'm really with you on that too like I want to see one
of the when people say like what is good about AI you know why
should we why should we pursue this this my standard answer
is high quality medical advice for everyone at pennies you
know per visit right it is orders of magnitude cheaper we're
already starting to see that in some ways it's better people
prefer it you know the AI is more patient it has better bedside
manner I wouldn't say you know if I was giving my you know my
own family advice today I would say use both a human doctor
and an AI but definitely use the AI as part of your mix.
Absolutely that's right that's right but you're prompting it
still right the smarter you are the smarter the AI is you
notice this immediately with your vocabulary right the more
sophisticated your vocabulary the finer the sanctions you
can have the better your own ability to spot errors you can
generate a basic program with it right but really amplified
intelligence is I think a much better way of thinking about
it because whatever your IQ is it surges it upward by a factor
of three or whatever the number and maybe the amplifier
increases with your intelligence but that that in internal
intelligence difference still exists it's just like what a
computer is a computer is an amplifier for intelligence if
you're smart you can hit enter and programs can go to like
thinking about the Minecraft guy right or Satoshi one person
built a billion or so she's a trillion dollar thing you know
obviously other people continued Bitcoin and so and so forth
right so what I feel though is this is what I mean by going
from nuclear terrorism to the TSA okay we went from AI will
kill everyone and I'm like what's the actuator to okay it'll
gradually to greater environment what does that mean okay
some people lose their jobs but then we're back in normal
world well hold on let me paint a little bit more complete
picture because I don't think we're quite there yet so I think
the differential diagnosis recent paper that's just a data
point where it's kind of you know like chess this you know
this came long before right there was a period where humans
are the best chess players then there was a period where the
best for the hybrid human AI systems and now as far as I
understand it we're in a regime where the human can't really
help the AI anymore and so the AIs are you know the best
chess players are just pure AIs we're not there in medicine
but we're starting to see examples where hey in a pretty
defined study differential diagnosis the AI is beating
not just beating the humans but also beating the AI human
hybrid or the human with access to AI so okay that's not it
right there's a paper recently called Eureka out of Nvidia
this is Jim fans lab where they use GPT for to write the reward
functions to train a robot so you want to train a robot to
like twirl a pencil in fingers hard you know hard for me to
do robots you definitely can't do it how do you train that
well you need a reward function the reward function
basically while you're in the early process of learning and
failing all the time the reward function gives you encouragement
when you're on the right track right so you there are people
who you know have developed this skill and you might do
something like well if the pencil has angular momentum you
know then that seems like you're on maybe sort of the right
tracks would give that you know a reward even though at the
beginning you're just failing all the time turns out GPT for
is way better than than humans at this right so it's it's
better at training robots so all of that is awesome and it's
great and but here is here's the thing is there's a huge
difference between AI is going to kill everybody and turn
everybody into paper clips okay versus some humans with some
AI are going to make a lot more money and some people are
going to lose their jobs. Yeah I'm not scared of that I'm not
scared of that snare I mean it could be disruptive it could be
disruptive but it's not existential under itself.
Big go okay so that's why right there's the the bay to me
it comes if I if I ask just one question is what is the actuator
right you know sensors and actuators right what is the
thing that's actually going to plunge a knife or a bullet into
you and kill you. It is either a human who has hypnotized
themselves by typing into a computer like basically an AI
terrorist you know which is kind of where some of the EAs are
going in my view or it is like an autonomous drone that is
controlled in a starcraft or terminator like way we are not
there yet in terms of having enough humanoid or autonomous
drones that are internet connected and programmable that
won't be there for some time okay so that alone means fast
takeoff is and what I think by the time we get there you will
have a cryptographic control over them that's a crucial thing
cryptography fragments the whole space in a very fundamental
way if you don't have the private keys you do not have
control over it so long as that piece of hardware the cryptographic
controller you've nailed the equations on that and frankly
you can use AI to attack that as well to make sure the code is
perfect right crypto memory talk about attack and defense AI
is attack cryptos defense right because one of the things
that crypto is done do you know the PKI problem is public key
infrastructure I'll say no on behalf of the audience this
is a good we should do more of these actually I feel it's a
good you know fusion of things or whatever right but the public
key infrastructure problem the the public key infrastructure
problem is something that was sort of lots of cryptography
papers and computer science papers in the 90s and 2000s
assumed that this could exist and essentially meant if you
could assume that everybody in internet had a public key that
was public and a private key that was kept both secure and
available at all times then there's like all kinds of amazing
things you can do with privacy preserving messaging and
authentication and so on the problem is that for many years
what what cryptographers are trying to do is they try to
nag people into keeping their private key secure and available
the issue is it's trivial to keep it secure and unavailable
where you write it down you put into a lockbox and you lose
the lockbox it's trivial to keep it available and not secure
okay where you you put it on your public website and it's
available all the time you never lose it but it's it's not
secure because anybody can see it when you actually ask what
does it mean to keep something secure and available that's
actually a very high cost it's precious space because it's
basically your wallet right your wallet is on your person at
all times so it's available but it's not available to everybody
else so it's secure so you have to like touch it constantly
yes right so it turns out that the crypto wallet by adding
a literal incentive to keep your private keys secure and
available because if they're not available you've lost your
money if they're not secure you've lost your money okay to
have both of them that was what solved the PKI problem now
we have hundreds of millions of people with public private
key pairs where the private keys are secure and available that
means all kinds of cryptographic schemes zero knowledge
stuff there's this amazing universe of things that is
happening now your knowledge in particular is made cryptography
much more programmable as a whole topic which is if you want
something that son of you know like I was creeping for a while
and people specialist for paying attention to it and then
just burst out on the scene zero knowledge is kind of like
that for cryptography thanks to the you know what you've
probably heard of zero knowledge before yeah we did one episode
with Daniel Kong on the use of zero knowledge proofs to
basically get to prove without revealing like the weights
that you actually ran the model you said you were going to run
and things like that I think are super interesting.
Exactly right so what kinds of stuff why is that useful in the
ice phase well first is you can use it for example for training
on medical records while keeping them both private but also
getting the data you want to for example let's say you've got
a collection of genomes okay and you want to ask okay how many
G's were in this data set how many C's how many A's how many
T's okay like you just say like that's very simple down
suppose what's the ACG T content of this you know the sequence
data set you could get those numbers you could prove they
were correct without giving any information about the individual
sequences right or more specifically you do it at one
locus and you say how many G's and how many C's are at this
particular locus and you get the SNP distribution okay so
so it's useful for what you just said which is like showing
that you ran a particular model without giving anything else
away it's useful for certain kinds of data analysis and there's
a lot of overhead on compute on this right now so it's not
something that you do trivially okay but it'll probably come
down with time but what is perhaps most interestingly useful
for is in the context of AI is coming up with things in AI
can't fake so what we talked about earlier right like an AI
can come up with all kinds of plausible sounding images but
if it wasn't cryptographically signed by the sender then you
know it and it should be signed by sender and put on chain and
then at least you know that this person or this entity with
this private key asserted that this object existed at this
time in a way that'd be extremely expensive to falsify
because it's either on the Bitcoin blockchain or another
blockchain is very expensive to rewind okay this starts to be
a bunch of facts that an AI can't fake.
You know so the going back to the kind of big picture loss
of control story I was just kind of trying to build up a few
of these data points that like hey look at this differential
diagnosis we already see like humans are not really adding
value to AI's anymore that's kind of striking and like
similarly with training robot hands GPT for his outperforming
human experts and by the way all of the sort of latent spaces
are like totally bridgeable right women one of the most
striking observations of the last couple years of study is
that AI's can talk to each other in high dimensional space
which we don't really have a way of understanding natively
right we it takes a lot of work for us to decode this is like
the language thing it we're starting to see AI's kind of
develop not obviously totally on their own as of now but we
are there is becoming an increasingly reliable go-to set
of techniques if you want to bridge different modalities
with like a pretty small parameter adapter that's interesting
actually what's a good paper on that I actually hadn't seen
the blip family of models out of Salesforce research is
really interesting and I've used that in production at Salesforce
really Salesforce research they have a crack team that has
open source a ton of stuff in the language model computer
vision joint space and this you see this all over the place
now but basically what they did in the paper called blip to
and they've had like five of these with a bunch of different
techniques but in blip to they took a pre-trained language
model and then a pre-trained computer vision model and they
were able to train just a very small model that kind of connects
the two so you could take an image put it into the image
space then have their little bridge bridge that over to
language space and that everything else that the two big models
are frozen so they're able to do this on just like a couple
days worth of GPU time which I do think goes to show how it is
going to be very difficult to contain proliferation which is
good I in my view that's really good as long as it doesn't get
out of control I'm I'm probably with you on that too but by
bridging this vision space into the language space then the
language model would be able to converse with you about the
image even though the language model was never trained on
images but you just had this connector that kind of bridges
those modalities it's just it's like another layer of the
network that just bridges to networks almost yeah it bridges
the spaces like it bridges the conceptual spaces between
something that has only understood images and something
that has only understood language but now you can kind of
bring those together as I think about it's not that surprising
because that's what you know for example text image models
are basically that they're bridging two spaces you know
in a sense right but I'll check this paper out so that so on
the one hand it's not that surprising on their hand I should
see how they implemented it or whatever so blip to okay yeah
I think the the most striking thing about that is just how
small it is like you took these two off the shelf models that
were trained independently for other purposes and you're able
to bridge them with a relatively small connector and that seems
to be kind of you know happening all over the place I would
also look at the flamingo architecture which is like a
year and a half ago now out of DeepMind that was a one for me
where I was like oh my and it's also a language to vision
where they keep the language model frozen and then they kind
of in my mind it's like I can see the person in their garage
like tinkering with their soldering iron you know because
it's just like wow you took this whole language thing that was
frozen and you kind of injected some you know vision stuff
here and you added a couple layers and you kind of Frankenstein
it and it works and it's like wow that's not really it wasn't
like super principled you know it was just kind of hack a few
things together and you know try training it and I don't want
to diminish what they did because I'm sure there were you
know more insights to it than that but it seems like we are
kind of seeing a reliable pattern of the key point here
being model to model communication through high
dimensional space which is not mediated by human language is
I think one of the reasons that I would expect and by the way
there's lots of papers to unlike you know language models
are human level or even superhuman prompt engineers you
know they're they're self prompting like techniques are
getting pretty good so if I'm imagining the big picture of
like and we can you know get back to like okay well how do
we use any techniques crypto or otherwise to keep this under
control and then I would say this is kind of the newer
school of the big picture a I safety worry obviously there's
a lot of flavors but if you were to you know go look at like
a Jay Acotra for example I think is a really good writer on
this her worldview is less that we're going to have this fume
and more that over a period of time and it may not be a long
period of time maybe it's like a generation maybe it's 10 years
maybe it's 100 years but obviously those are all small in
the sort of you know grand scheme of the future we have in
all likelihood the development of AI centric schemes of
production where you've got kind of your high level executive
function is like your language model you've got all these
like lower level models they're all bridgeable all the spaces
are bridgeable in high dimensional form where they're
not really mediated by language unless we enforce that I mean
we could say you know it must always be mediated by language
so we can read the logs but there's a tax to that right
because going through language is like highly compressed compared
to the high dimensional space to space. All right so let me
see if I can steal man or articulate your case you're
saying a eyes are going to get good enough they're going to
be able to communicate with each other good enough and they
will do enough tasks that more and more humans will be rendered
economically marginal and unnecessary. I'm not saying I
think that will happen I'm just saying I think there's a good
enough chance that that will happen that it's worth taking
really seriously. I actually think that will happen something
along those lines in the sense of at least massive economic
disruption definitely okay but I'll give an answer to that
which is both you know maybe fun enough and have you seen the
you've seen the graph of the percentage of America that was
involved in farming. Yeah I tweeted a version of that once.
I did okay great good so you're familiar with this and you're
familiar with what I mean by the implication of it where
basically Americans used to identify themselves as farmers
right and manufacturing rose as agriculture collapsed right
and here is the graph on them but from like 40% in the year
1900 to like a total collapse of agriculture and then also
more recently a collapse or manufacturing into bureaucracy
paperwork legal work what is up into the right since then is
you know the the lawyers what is up into the right what is
replacing that starting in around the 1970s we used to be
adding energy production and energy production flatlined
once people got angry about nuclear power so this is a
future that could have been we could be on Mars by now but
we got flatlined right what did go up into the right so
construction cost this is the bad scenario where the miracle
energy got destroyed because regulations you the cost was
flat and then when vertical when regulations were imposed all
the progress was stopped by decels and D growthers and then
Alara was implemented which said nuclear energy has to be as
low risk as as reasonably necessary as reasonably
achievable and that meant that you just keep adding quote
safety to it until it's the same as cost everything else
which means you destroy the the value of it right but you
know what was up into the right what replaces agriculture
and manufacturing jobs look at this you see this graph for
the audio only we will put this on YouTube so if you want to
see the graph do the YouTube version of this for the audio
only group it's an exponential curve in the number of lawyers
in the United States from looks like maybe two thirds of a
million to 13 million over the last hundred and forty years
yeah and in 1880 it was like like sub 100,000 or something
like that right and then it's just like especially that 1970
point that's when it went totally vertical okay and it's
probably even more since so you know if you had paperwork
jobs bureaucratic jobs you know every lawyer is like you
know sorry lawyers but you're basically negative value add
right because it should the fact that you have a lawyer
means that you couldn't just self serve a form right basic
government is platform is where you can just self serve and
you fill it out and you don't have to have somebody like
code something for you custom you know lawyers that's doing
custom code is because the code the legal code is so
complicated so you know the whole Shakespeare thing like
first thing we do let's you know kill all the lawyers first
thing we do let's automate all the lawyers right only something
that's the hammer blow of AI can break the backbone and it
will that's it's going to break the backbone of blue America
right it's going to cause that's why the political layer and
the sovereignty layer is not what AI people think about but
it's like crucial for thinking about AI because what tribes
is AI benefit and again we got away from why is AI kill
everybody well it's going to need actuators who's going to
stab you who's going to shoot you it's got to be a human
hypnotized by I or a drone that I controls a human hypnotized
by I is actually a conventional threat looks like a terrorist
cell we know how to deal with that right it's just like
radicalized humans that worship some AI that stab you it's
like the pause AI people are one step I think away from that
alright but that's just like on should we go that's like
allocated that's like basically terrorists who think that the
AI is telling them what to do fine if it's not a human that
stabbing you it is a drone and that's like a very different
future where like five or ten or fifteen years up maybe we
have enough internet connected drones out there but even then
they'll have private keys so there's going to be fragmentation
of a dress space not all drones be controlled by everybody in
my view okay that's what I safety is I safety is can you turn
it off can you kill it can you stop it from controlling drones
that's what I safety is it can you also open the model weights
you can generate adversarial inputs can you open the model
weights and proliferate it you're saying a proliferation is bad
I'm saying proliferation is good because if everybody has
one then nobody has an advantage on it right not not relatively
speaking okay I have very few super confident position so I
wouldn't necessarily I think that proliferation is bad I'd
say so far it's good it has and even the most of the I safety
people I would say if I could you know speak on the behalf of
the I safety consensus I would say most people would say even
that the llama to release has proven good for a I safety for
the reasons that you're saying but they opposed it well some
didn't some didn't I would say the main posture that I see a
safety people taking is that we're getting really close to or
we might be getting really close certainly if we just kind of
naively extrapolate out recent progress it would seem that
we're getting really close to systems that are sufficiently
powerful that it's very hard to predict what happens if they
proliferate llama to not there and so you know yes it has
enabled a lot of interpretability work it has enabled
things like representation engineering which there isn't
a lot of good stuff that has come from it the big thing that
I want to kind of establish is you agree me on the actuation
point or not like the thing is this thing like llama to
proliferates and so businesses are disrupted and people you
know maybe maybe they they paid a lot of money for their MD
degree and they can't make us a bunch of money that's within
the realm of what I call conventional warfare you know
I mean that's like we're still in normal world as we were
talking about okay. Unconventional warfare is you
know Skynet arises and kills everybody okay and that is what
is being sold over here and I and when you think about the
actuators we don't have the drones out there we don't have
the humanoid robots in control and hypnotized humans are a
very tiny subset of humans probably and if they aren't
that just looks like a religion or a cult or a terrorist cell
and we know how to deal with that as well the super
intelligent AI with you know lots of robots that control in
a starcraft form I would agree is something that humans
haven't faced yet but by the time we get that many robots
out there you you won't be able to control them at once
because of the private key things I mentioned so that's
why I'm like okay everything else we're talking about is
a normal world that's that is the single biggest thing that
I wanted to get like economic disruption people losing jobs
proliferation so that the balance of powers redistributed
all this fine the reason I say this is people keep trying
to link AI to existential risk a great example is one of
the things you actually had in here this is similar to the
AI policy and so you think it's totally reasonable question
but then I'm going to deep in my view deconstruct the
question what do you think about putting the limit on the
right to compute or their capabilities and AI system
might demonstrate that you make you think open access no longer
wise most common near term answer here to be seems to be
related to risk a pandemic via novel pathogen engineering
so guess what you know who the novel pathogen engineers are
the US and Chinese governments right they did it or probably
did it credibly did it credibly mean accused of doing it
they haven't been punished for COVID-19 in fact they covered
up their culpability and pointed everywhere other than
themselves they use it to gain more power in both the US
and China with both lockdown in China and in the US and all
kinds of COVID era trillions of dollars was printed and
spent and so on and so forth they did everything other
than actually solve the problem that was actually getting
you know the vaccines in the private sector and they studied
the existential risk only to generate it and they're even
paid to generate pandemic prevention and failed so this
would be the ultimate Fox guarding the henhouse okay the
only read the two organizations responsible for killing
millions of people novel pathogen are going to prevent people
from doing this by restricting compute no it you know what
it is actually what's happening here is one of the concepts
I have in the network state is this idea of God state and
network okay meaning what do you think is the most powerful
force in the world is it Almighty God is it the US
government or is it encryption right or eventually maybe an
AGI right if what what's happening here is a lot of
people are implicitly without realizing it even if they are
secular atheists they're treating GOV as GOD okay they
treat the US government as God as the final mover.
No I appreciate your little I take inspiration from you
actually in terms of trying to come up with these little
quips that you know that are memorable so I was just smiling
at that because I think you do a great job of that and I try
to encourage I have less success pointing terms than you
have but certainly try to follow your example on that front
it's like a helpful if you can compress it down it's like more
memorable so that's why I try to do right so exactly a lot of
these people who are secular eight think of themselves as
atheists have just replaced GOD with GOV they worship the US
government as God and there's two versions of this you know
how like God has both the male and female version right the
female version is the Democrat God within the USA that has
infinite money and can take care of everybody and care for
everybody and the Republican God is the US military that can
blow up anybody and it's the biggest and strongest and most
powerful America F yeah okay and everybody who thinks of the
US government as being able to stop something is praying to a
dead God okay when you say this you actually get an interesting
reaction from a I safety people where you've actually hit
their true solar plexus all right the true solar plexus is not
that they believe in AI it's that they believe in the US
government that's a true solar plexus because they are
appealing to their prank this dead God that can't even clean
the poop off the streets in San Francisco right that is losing
wars or fighting them to sell me it says lost all these wars
around the world that spent trillions of dollars has been
through financial crisis coronavirus Iraq war you know
total meltdown politically okay that has interest that is
now has interest payments more than the defense budget that
is you know that spent a hundred billion dollars on the
California train without leaving a single track it's like
that you know that Morgan Freeman thing for you know the
clip from Batman who is like so this man has a billionaire
blah blah blah this and that and your plan is to threaten him
right and so you're gonna create this super intelligence and
have Kamala Harris regulated come on man so to speak right
like these people are praying to a blind deaf and dumb God
that was powerful in 1945 right that's why by the way all
the popular movies what are that it's Barbie it's Oppenheimer
right it's it's top gun they're all throwbacks the 80s or the
50s when the USA was really big and strong and the future is
a black mirror yeah I think that's tragic one of the projects
that I do like and you might appreciate this and if you've
seen it is the from the future of life Institute a project
called imagine a world I think is the name of it and they
basically challenged you know their audience and the public
to come up with positive visions of a future you know where
technology changes a lot and obviously I pretty central to
a lot of those stories and you know what are the challenges
that people go through and how do we get there and whatever
but a purposeful effort to imagine to imagine positive
futures super under provided and I really liked the the
investment that they made in that you know something one of
the things I've got in the never see a book is there's
certain megatrends that are happening right and megatrends
I mean it's possible for like one miraculous human maybe to
reverse them okay because I think both the impersonal force
of history theory and the great man theory of history have
some truth to them.
But the megatrends are the decline of Washington DC the
rise of the Internet the rise of India the rise of China that
is like my world view and I can give a thousand graphs and
charts and so on for that but that's basically the last 30
years and maybe the next X right I'm not saying there can't
be trend reversal of course if we try reversal as I just
mentioned some hammer blow could hit it but that's what's
happening and so because of that the people who are optimistic
about the future are aligned with either the Internet India
or China and the people who are not optimistic about the
future are blue Americans or left out red Americans okay or
Westerners in general who are not tech tech people okay if
they're not tech people they're not up into the right basically
because the Internet's if you I mean one of the things is we
have a misnomer as I was saying earlier calling it the United
States because the dis United States it's it's like talking
about you know talking about America is like talking about
Korea there's North Korea and South Korea they're totally
different populations and you know communism and capitals
are totally different systems and the thing that is good for
one is bad for another and vice versa and so like America
doesn't exist there's only just like there's no Korea there's
only North Korea and South Korea there's no America there
is blue America and red America and also gray America tech
America and blue America is harmed or they think they're
harmed or they've gotten themselves into a spot where
they're harmed by every technological development which
is why they hate it so much right AI versus journalists
jobs crypto takes away banking jobs you know everything you
know self-driving cars they just take away regulator control
right anything that reduces their power they hate and they're
just trying to freeze an amber with regulations red America
got crushed a long time ago by offshoring to China and so on
they're they're making you know inroads ally with tech
America or gray America tech America is like the one piece
of America that's actually still functional and globally
competitive and people always do this fallacy of aggregation
where they talk about the USA and it's really this component
that's up into the right and the others that are down into
the right red best flat like red but they're like down right
like red is like okay functional blues down point is tech
America I think we're going to find is not even truly or how
American is tech America because it's like 50% immigrants
right and like a lot of children immigrants and most of their
customers are overseas and their users are overseas and
their vantage point is global right and they're basically
not I know we're in this ultra nationalist kick right now
and I know that there's going to be there's a degree of a fork
here where you fork technology into Silicon Valley in the
Internet okay where Silicon Valley is American and they'll
be making like American military equipment and so on and so
forth and they're signaling USA which is fine okay and then
the Internet is international global capitalism and the
difference is Silicon Valley or let's say US tech let me
let's you know US tech says ban tick tock build military
equipment etc it's really identifying itself as American
and it's thinking of being anti-China okay but there's
US and China are only 20% of the world 80% of the world is
neither American nor Chinese so the Internet is for everybody
else who wants actual global rule of law right when as a US
decays as a rule space order and people don't want to be under
China people want to be under something like blockchains
where you've got like property rights contract law across
borders that are enforced by an impartial authority okay that's
also kind of laws that can bind AI's like AI's across borders
if you want to make sure they're going to do something
cryptography can bind an AI in such a way that it can't fake
it it can't an AI can't mint more Bitcoin you know my here's
my last question for you AI discourse right now does seem
to be polarizing into camps obviously a big way that you
think about the world is by trying to figure out you know
what are the different camps how do they relate to each other
so on and so forth I have the view that AI is so weird and
so unlike other things that we've encountered in the past
including just like unlike humans right I always say AI
alien intelligence that I feel like it's really important to
to borrow a phrase from Paul Graham keep our identities
small and try to have a scout mindset to really just take
things on their own terms right and not necessarily put them
through a prism of like who's team of my honor you know is
this benefit my team or hurt the other team or whatever but
you know just try to be as kind of directly engaged with the
things themselves as we can without mediating it through
all these lenses I think about you mentioned like the gain
of function right and I don't know for sure what happened
but it certainly does seem like there's a very significant
chance that it was a lab leak certainly there's a long history
of lab leaks but it would be like you know it would seem to
me a failure to say okay well what's the what's the opposite
of just having like a couple of government labs like everybody
gets their own gain of function lab right like if we could
and this is kind of what we're doing with AI we're like
let's compress this power down to as small as we can let's make
a kit that can run in everybody's home would we want to send
out these like gain of function you know what lab research kits
to like every home in the world and be like hope you find
something interesting you know like let us know if you find
any new pathogens or hey maybe you'll find life-saving drugs
like whatever we'll see what you find you know all 8 billion
of you that to me seems like it would be definitely a big
misstep and that's the kind of thing that I see coming out
of ideologically motivated reasoning or like you know
tribal reasoning and so I guess I wonder how you think about
the role that tribalism and ideology is playing and should
or shouldn't play as we try to understand AI.
Okay so first is you're absolutely right that just
because a is bad does not mean that B is good right so it
could be bad option B could be a bad option C could be a bad
option in there might be have to go down to option G before
you find a good option or there might be 3 good options and
7 bad options for example right so to map that here in my view
an extremely bad option is to ask the U.S. and Chinese
governments to do something anything the U.S. government
does at the federal level at the state level in blue states
at the city level has been a failure and the way that here's
a here's a meta way of thinking about you invest in companies
right so as an investor here's a really important thing you
might have 10 people who come to you with the same words in
their pitch they're all for example building social
networks but one of them is Facebook and the others are
Friendster and whatever okay and no offense of Friendster you
know that these guys were like you know pioneers in their own
way but they just got outmatched by Facebook so point is that
the words were the same on each of these packages but the
execution was completely different so could I imagine a
highly competent government that could execute and that
actually did you know like you know make the right balance
of things and so on I can't say it's impossible but I can
say that it wouldn't be this government.
Okay and so you are talking about the words and I'm talking
about the substance the words are we will protect you from
a I right in my view the substances they are protecting
you from anything right you're basically giving money and
power to complete and competent and in fact malicious
organization which is Washington DC which is the US government
that has basically over the last 30 years gone from a hyper
power that wins everywhere without fighting to a declining
power that fights everywhere without winning.
Okay like just literally burn trillions of dollars doing
this take maybe the greatest decline in fortunes in 30
years and maybe human history not even the Roman Empire went
down this fast on this many power dimensions this quickly
right so giving that guy.
Let's trust him that's just people running an old script
in their heads that they inherited they're not thinking
about it from first principles that this state is a failure.
Okay and like how much of failure it is you have to look
at the sovereign debt crisis look at look at graphs that other
people aren't looking at but like you know the the the domain
of what blue America can regulate is already collapsing.
Because it can't regulate Russia anymore it can't regulate
China anymore it's less able to regulate India it's less able
even to regulate Florida and Texas states are breaking away
from it domestically so this gets to your other point why
is the tribal lens not something that we can put in the back
we have to put in the absolute front because the world is
retribalizing like basically your tribe determines what law
you're bound by if you think you can pass some policy that
binds the whole world well there have to be guys with guns
who enforce that policy and if I have guys with guns on
their side that say we're not enforcing that policy then
you have no policy you've only bound your own people so
that makes sense right and so blue America will probably
succeed in choking the life out of AI within blue America but
blue America controls less and less of the world so have more
power over fewer people.
I can go into why this is but essentially you know a financial
Berlin Wall is arising there's a lot of taxation and regulation
and effectively financial repression de facto confiscation
that will have to happen for the level of debt service that the
US has been taking on okay just there's there's one graph just
to make the point and if you want to dig into this you can
all right but the reason this impacts things is when you're
talking about a safety you're talking about a regulation
you're talking about the US government right and you have
to ask what does that actually mean and it's like in my view
it's like asking the Soviet Union in 1989 to regulate the
internet right that's going to outlive you know the country
US interest payment on federal debt versus defense spending
the white line is defense spending look at the red line
that's just gone absolutely vertical that's interest and
it's going to go more vertical next year because all of this
debt is getting refinanced and much higher interest rates
this is why the look at this you want you want AI timelines
right the question for me is DC's timeline what is DC's time
left to live okay this is the kind of thing that kills empires
and and you either have this just go to the absolute moon
or they cut rates and they print a lot and either way you
know the the fundamental assumption underpinning all the
AI safety all the regulation work is that they have a
functional golem in Washington DC where they convince it to
do something it has enough power to control enough of the
world when that assumption is broken then a lot of assumptions
are broken right and so in my view you have to you must think
about a polytheistic AI world because other tribes are already
into this they're already funding their own right the
proliferation is already happening and they're not going
about a blue tribe so that's why I think the tribal lens is
not secondary it's not some you know totally separate thing
it is an absolutely primary way in which to look at this and
in a sense it's almost like a you know in a well done movie
all the plot lines come together at the end
okay and all the disruptions that are happening the China
disruption the rise of India the rise of the internet the rise
of crypto the rise of AI and the decline of DC and the internal
political conflict and you know various other theaters like
what's happening in Europe and you know and and and Middle
East all of those come together into a crescendo of all there's
a lot of those graphs are all having the same time and it
it's not something you can analyze by just I think looking
at one of these curves on its own.
I think that's a great note to wrap on I am always lamenting
the fact that so many people are thinking about this AI moment
in just fundamentally too small of terms but I don't think
you're one that will easily be accused of that so with an
invitation to come back and continue in the not too distant
future for now I will say Balaji Srinivasan thank you for
being part of the cognitive revolution.
Thank you Nathan good to be here.
It is both energizing and enlightening to hear why people
listen and learn what they value about the show so please
don't hesitate to reach out via email at TCR at turpentine.co
or you can DM me on the social media platform of your choice.
Omniki uses generative AI to enable you to launch hundreds
of thousands of ad iterations that actually work customized
across all platforms with a click of a button I believe in
Omniki so much that I invested in it and I recommend you use
it to use Kograv to get a 10% discount.

So, welcome everybody.
This let me put up, let me share the screen.
That's okay.
And I will show the schedule for today.
That's good.
Under the multidisciplinary section where we are entertaining a number of interesting
topics here in this session and in the following session.
On behalf of everyone, welcome and let me acknowledge the unceded territories of the
indigenous peoples that we are all tuning in from.
My source water is a big monkey.
So, I'm tuning in from the eastern side of the continent.
We have a number of speakers who are also emphasizing the water cycle.
And so, I think we'll just go ahead and start with Vanessa Ford, if you would like to all
stop sharing and I'll give you a heads up in about 10 minutes, if that's okay.
Yeah, sounds good.
Just get my presentation up here.
Just confirming that you're seeing that and hearing me okay.
Yeah, that's great.
Thanks.
Yeah.
Thanks for introducing the session.
I'm Vanessa Ford.
I'm a research climatologist with the British Columbia Ministry of Forests.
I'm going to talk about using weather and microclimate measurements to assess wildfire risk in different
stand types.
And just want to acknowledge my colleagues, Rulaan Zayo, Phillip Burton, Joseph Shea,
all from the University of Northern British Columbia, as well as Rebecca Bowler, also
from the BC Ministry of Forests.
So the rationale for this work happened because of significant wildfires that we had in the
central interior of British Columbia in 2017 and 2018, where it was noticed something called
wildfire skips, where there was juvenile forests that were burned less verily or not at all,
as shown in that satellite image.
Probably easy to pick up that nice dark green locations in the middle, that's juvenile pine
plantations that were not burned.
And then on the outskirts, you can see a lot of fire scars and burned trees in older mature
forests.
This was counter to the understanding that generally older forests are less susceptible
to high-intensity wildfires, and that sparked several research projects to investigate.
This work is part of a larger research project led by the University of Northern British
Columbia on estimating fuel moisture content in different stand types, whereas my portion
of the project is to understand, are there microclimate differences that would affect
the fire weather variables to cause the juvenile pine plantations to be less risky than the
older stands?
So we have six field-based sites to look at this problem.
Three of the sites are around the community of Smithers, which is in the northwest central
part of the province, and then also three around Prince George, which is pretty close
to the middle of the province of British Columbia.
So showing the sites here on these satellite images, as well as the nearest British Columbia
wildfire service weather station that we use to compare our field data to.
So here's what the study design looks like.
I'll move from left to right in terms of the pitchers.
So we have an approach where we're directly comparing weather and microclimate conditions
in an open stand.
The open stand is regenerating clear cut, where there's small trees growing, shrubs,
and a lot of slash left over from the harvesting, as well as a nearby juvenile stand.
This is planted logical pine of about 23, sorry, 20 to 30 years old, and is fairly densely
or closely planted together, and then a nearby mature stand.
So unharvested primary old forest of at least 80 or 100-year plus.
Not as dense as the juvenile stands, old forest dynamics tend to be pretty patchy in terms
of openness.
And for all six locations with three of these subplots, they're all within the sub-oreal
spruce ecological zone of the province, so the forest types are very similar.
So here is the microclimate and weather monitoring equipment that we're using.
In the open sites, we have full weather stations.
Four of the six sites are onset hobo U30 dataloggers that are solar-paneled, where we're
measuring temperature and relative humidity, both at two meters and 30 centimeters.
We're also measuring surface temperature, wind speed and direction at three meters,
rain, solar radiation, bare magic pressure, soil temperature at 15 centimeters depth,
and soil moisture at 5, 15, and 30 centimeters depth.
Moving from left to right in the pictures, showing the next picture is an example of
some of the in-stand monitoring, where we have these smaller onset hobo micro-dataloggers
attached to our soil sensors, the same as the weather station set up in the opening.
We're measuring soil temperature at 15 centimeters and then soil moisture at 5, 15, and 30 centimeters
at all the different stand types.
And then that picture is also showing an example of an internal-external temperature sensor
at 30 centimeters above the ground that we use to measure surface temperature.
The third picture is UNBC supplied weather stations, Campbell Scientific-based weather
stations based on a Sierra 300 logger and using an all-weather sensor for two of the
six stations.
That fourth picture is another in-stand picture of our rain bucket, as well as some of the
30 centimeter temperature and relative humidity measurements.
And then lastly, the final picture is showing a Davis wind speed and direction smart sensor
coupled with an onset micro-data logger.
The wind speed and direction measurements in-stand are also made at three meters, just
like in our open locations.
We're not measuring snow.
In this study, snow is really quite difficult to measure directly with sensors.
However, we do run the equipment year-round and we can tell things like the start of the
snow season, the end of the snow season, and the timing of melts with the data that
we leave running.
And that's really important for starting the fire weather index codes.
Here's just an example of what the stands look like overhead.
These are fisheye photos or some folks call them canopy photos.
Obviously, the open stand is fully exposed.
The juvenile pine plantation is in the middle.
You can see that it has a lower canopy cover compared to the picture on the right, which
is the mature stand where there's lots of shading and the canopy is pretty tall.
However, the difference between the juvenile and the mature stand is that the mature stand
has a lot of open patches lower in the canopy.
Hopefully, you can see some of the light poking in at lower heights.
However, in the juvenile stand, because those pine trees are pretty densely packed, there's
less light getting in in the sides of the stand.
So just quickly, I'll go through some of the different climate variables that we've measured
directly.
We started the measurements in June of 2021.
So in some of the sites, we did capture some of the effects of the heat dome, which was
kind of cool.
So temperature is being shown on the left here in the open juvenile and mature stands.
And generally, what we see as expected, the open is the warmest, however, the mature and
juvenile are relatively close together.
And with relative humidity, again, the relative humidity is the lowest in the open stands.
And between the mature and the juvenile stands, they're pretty close.
However, the juvenile stand does get a little bit drier than the mature stand.
Wind speed is an important fire weather index variable.
So wind speeds on the left here, of course, the open is a lot windier, both in average
and maximum wind speeds.
And then when we look at the difference between the juvenile and the mature, with maximum
wind speeds, the juvenile is greater than the mature, but with average wind speeds, it's
actually the mature that's greater than the juvenile.
With rainfall, as you can probably imagine, in a topographically complex place like British
Columbia, the rainfall between all these sites is highly variable.
So I don't have too much to say about that other than it's a useful variable for understanding
the melt season, which is that orange line that peaks above 100 millimeters in April,
where we can see the snow, the timing of the snow melt.
Surface temperature, we only have two years of data for so far because we added it late
in response to comments from Natural Resource Canada, wildfire experts that surface temperature
is a very important variable.
In general, we see the surface temperature varies the most in the open, and there's quite
large differences between the maximum surface temperatures and the stand types.
So the open area can get above 30 degrees, whereas the juvenile stand gets close to 30
degrees, but doesn't seem to cross it, but the mature stand gets below 20 degrees.
So quite a large temperature difference between the juvenile and mature stands during the
hot periods, which could be important for wildfire risk.
Surface temperature is also a good indicator of snow season timing, where you can see it
flat lines in the winter there because these sensors are covered in snow.
Great, thanks.
Soil temperature is the variable on the right there at 15 centimeters depth.
It's a great indicator of when the sites start to green up and the vegetation is flushing,
and generally we see soil temperature is warmest in the open and coolest in the juvenile.
And the last variable is soil moisture, which is pretty complicated to understand.
Basically, we see that the juvenile soil moisture fluctuates the least as it doesn't recharge
most of the rainfall, but it doesn't lose a lot when it's really dry.
And unfortunately, our mature sites are quite prone to animal damage, but we've created
a cage system to protect some of our cables there, which we hope to improve our measurements
for upcoming years.
And so just in my last couple of slides, I'll show how we use those variables to calculate
wildfire risk.
Vapor pressure deficit is shown on the left there.
Vapor pressure deficit can be a good indicator of wildfire risk.
And also high vapor pressure deficit can limit tree growth.
So when the vapor pressure deficit is high, the trees are potentially transpiring water,
but also they could be stressed and have shut down their stomata and are not transpiring.
And generally we see that the vapor pressure deficit is higher in the mature stands as opposed
to the juvenile, which is interesting.
And then when we look at different drought codes on the right, fine-fuel moisture code
on the top, duff moisture code in the middle, and drought code on the bottom, which kind
of follow from surface to a little bit deeper between these different drought codes.
Generally we find the drought codes are higher in the open, of course, and between the mature
versus the juvenile, they are higher in the mature than the juvenile generally.
And also the timing of the snow season is really important to running these drought codes when
we compare it to a wildfire service weather station, which is the orange line on the right
graphs and our weather station in the open and the blue.
When we start the timing of the fire weather codes at the end of the snow season, which
we can tell from our data, we end up generating a much higher drought code throughout the season,
kind of as expected.
But this was a lot more reflective of the severe wildfire season that we did experience
in BC in 2023.
So this is just my last data slide.
The graph on the left is showing how many times these fire risk thresholds were broken
in 2023, where we did have a lot of wildfires occurring across the province.
So definitely the wildfire index variables are working well in terms of indicating wildfire
risk.
And the graphs on the right are comparing some of our in-stand measurements to BC wildfire
station data.
Where generally we see with our field-based measurements, our drought codes are higher,
but it actually ends up that some of the fire risk variables are actually lower because
our wind speeds are being monitored at three meters as opposed to 10 meters in the wildfire
weather stations.
And that makes a big difference to the wildfire risk.
Three meters is a bit closer to the ground and potentially are giving us different indications.
So we've only had two fire seasons.
One was really wet and one was really dry.
So we're continuing to monitor and we plan to do more statistical analysis as we gather
more data, compare our data with field-collected field moisture, and also compare with remote
sensing of wildfire risk.
That's it.
Thank you.
Thanks, Heather.
Thanks, Heather.
I don't see any questions in the chat, I think, let me just go back there, either the chat
or Q&A are fine for posing questions for the community, for the participants today.
I guess you had addressed the question that I had about the maturity of the stands and
the mean or the max wind use in the fire.
So can you specify which, is it mean or max that you, the emphasis in fire risk in general,
and how would you accommodate the height of the trees versus the height that you're measuring?
Yeah, so I did mention that, yeah, in the graphs we showed both mean and max wind speed
when we're looking at the comparisons between the juvenile and mature stands.
So the mean wind speed was greater in the mature stands than the juvenile, but the max
was greater in the juvenile than the mature.
I think the way the fire weather variables are calculated is they take the wind speeds
that are occurring at 12 o'clock, so kind of midday, to calculate those variables.
And then I was just trying to explain that the difference between some of the fire weather
indexes that are related to wind speed are quite different between our weather station
calculations and the BC Wildfire Service, because they measure wind speed at 10 meters
whereas we're measuring at three.
Okay, thanks.
Very good then.
I think we should move on to, so thanks Vanessa, to Victor Peñarán de Velez, tuning in from
Mexico and talking about downscaling extreme events in that area.
So Victor, can you share your screen?
Right, okay, my screen is, I think it's, okay, good.
Perfect, okay, thanks.
Okay, okay, I'm going to start.
Hello everyone, I'm very pleased to be here today and today I'm going to present
some research results that my colleagues and I were working on for a while.
I expect that this presentation invite us, invite you to get any interest in understanding
some different kind of models for modeling precipitation.
Okay, the agenda for today starts with some motivations that conducts the development
of an innovative approach to downscale course, scale precipitation products over the complex
terrain region of Mexico.
At the end, some final comments will wrap up this presentation.
Let me start this presentation recalling a thought of Dr. Mandelbrot, nature achieved
it not simply a higher degree but an altogether different level of complexity.
This thought reminds me that precipitation has been considered as one of the most difficult
atmospheric phenomena to be described and modeled.
This is so because of the higher complexity of its patterns.
Going along with Dr. Mandelbrot, the nature of precipitation patterns belongs to a different
level of complexity and we think the fractal theory could help us to solve some of its
intrinsic problems for modeling precipitation.
From the mathematical point of view, precipitation observations can be understood as measures
or quantities belonging to a metric space.
These measures can be described by singularity exponents that explain how precipitation
magnitudes change with the scale.
Singularity exponents are associated to the fractal dimension of these measures.
These fractal dimensions explain how the exceeding probability of precipitation change with the
scale.
So scaling properties of precipitation help us to understand properties of its probability
space.
This is like we see a precipitation time series, one that was taken from the precipitation
network of Nazca City.
We can also see the decomposition of precipitation time series by classifying precipitation measures
according to specific values of the singularity exponent.
The higher the value of the singularity exponent, the smaller the value of the precipitation
measures.
The way these measures fill the space is quantifying through their fractal dimension.
For high values of the fractal dimensions, the exceeding space becomes full of precipitation
measures.
Continuing the decomposition for all possible scales, scaling exponents, we can obtain the
multi-fractal spectrum of the precipitation time series.
Here, the left plot exhibits the co-dimension function and the right plot shows the fractal
dimension function.
These figures are multi-fractal spectra, and they both represent the same statistical structure
of data, but from a different geometric point of view.
Here, the co-dimension function and the fractal function can also be estimated by analyzing
the statistical moment of data.
As mentioned before, the precipitation measure follows a power low with respect to the scale.
Likewise, statistical moments also follow a power low with respect to the scale, but here
the exponents are no longer called singularity exponents, but moment-scaling exponents.
Let's now talk about one of the problems that encourages this research.
This slide shows the spatial correlation function of 30 years of annual precipitation data located
in some specific region of the north-central continental of the United States.
If I take this picture, or planning the design of a precipitation network, and I want to
design a highly correlated precipitation network in this United States region, I would suggest
to take at least a correlation length of 35 kilometers for getting a gauge density of
almost 4,000 square kilometers per gauge.
Applying the same technique for a set of observations recorded over the metropolitan area of Mexico
City, the correlation base estimation of the gauge density for this network determines that
a wider number of rain gauges is necessary for a high-correlated precipitation network.
For instance, if one takes a correlation length of 3.26 kilometers, the gauge density would
be 33 square kilometers per gauge.
Although we have a less correlated precipitation network, the spatial coverage is not good enough
for all the metropolitan and surrounding areas of Mexico City.
This plot shows that there are special gaps to be solved by future administrations of
the precipitation network for a better operation and security of the water system.
Many regions in the world, as Mexico, exhibit inadequacy for measuring precipitation and
other meteorological variables.
Fortunately, we count on satellite and reanalysis products such as eimers and chips.
These products allow us to have gridded precipitation fields and information for climatological
analysis.
However, these kind of precipitation fields are recorded in coarse, spatial, or time scale,
and so an application as the design of water infrastructure requires high-resolution information.
How can we take advantage of the coarse-scale precipitation products for applications that
require high-resolution information?
As mentioned before, we can take advantage of fractal theory, which deals with fragmentation
processes in different scales.
One can identify a synergy between fractal theory and the statistical management of
coarse-scale data.
A powerful tool could emerge to solve several problems in meteorology, hydrology, and so
on.
This research developed an approach to get a solution as indicated before.
With eimers and chips, we are dealing with two-dimensional precipitation measures that
can be modeled by a multiplicative, randonic-escape process.
This is a scale transformation process given by product random variables.
As with all models, we need to lie down in some assumptions.
The main one is that the multiplicative random process is a canonical one.
That means that the average value of the field is preserved in all scales.
There should be also restriction on the singularities to be modeled according to what is physically
possible to get.
In that sense, we have adopted that the largest singularities of the simulated precipitation
fields are constrained to a statistical point-measured elements over the study region.
Several studies have looked for a mathematical representation of the altitudinal variation
of the precipitation, but there is not a unique mathematical representation because it seems
to depend on regional conditions.
For instance, in some regions, such as Newham Chire and Central Arizona United States, there
seems to be a linear relationship of the altitudinal variability of the precipitation.
So in other regions, there seems to be an exponential relationship, but in tropical regions, or
even subtropical regions too, there seems to be a greater behavior in the altitudinal
variability of the precipitation.
As part of the innovation of the introduced approach, the inclusion of a topographic precipitation
enhancement function in the random cascade model allows us to model any kind of regions
with complex topography.
In general, simulated pattern with random cascade models presents a problem known as
blockiness or block-shaped problem.
This problem is understood as blocks of precipitation over the space domain that seems to be the
unrealistic.
The solution to this problem was solved by including a diffusive filter to improve the
spatial structure of simulated precipitation patterns.
So far, we have gathered the essential element of this approach.
Let's now illustrate some application of this model.
As we turn this slide, there are three time series of average field values of the precipitation
products and simulated precipitation fields.
These three time series exhibit equivalent statistics, moreover, the appearance of all
three precipitation patterns is quite similar.
So the main constraint of the three TRC approach allows us to obtain a high-resolution precipitation
field with a similar statistical structure of the coarse-scale precipitation product.
The statistical structure of precipitation field is also characterized by the co-dimension
function.
The co-dimension function of simulated precipitation fields is quite alike to the co-dimension
function of chirps precipitation product.
Clearly, there are intrinsic difference between chirps and Eimer precipitation products.
Eimer lays down an overestimation of low quantities that makes Eimer richer in low-value
singularities than those observed in chirps.
The photographic random skate approach is also capable of representing the annual precipitation
cycle and some other long-term statistical showing this slide.
Another important feature to highlight of the TRC approach is its capability to improve
the spatial pattern structure of precipitation after the down-scaling process.
For instance, Eimer precipitation product has a spatial scale of approximately 10 km at
these latitudes in Mexico.
Therefore, Eimer precipitation appeal are set forward by quasi-homogeneous patterns that
they don't allow us to identify the effect of topography on the precipitation field.
However, simulated precipitation fields with the TRC approach chose a better spatial description
of precipitation in regions with complex topography.
Let me end my presentation with some final remarks.
Many regions in the world deal with the problem of restricted measuring or recording of precipitation
data.
Despite there could be many technical, social, economic reasons that do not allow us to improve
the sampling of point observations, the core-scale precipitation products can provide us suitable
data for understanding local dynamics as we do for Mexico City.
Second, here we are not in possession of physical criteria to make possible the down-scaling
estimation of precipitation fields.
But with a scaling property-based criteria, there is an alternative and suitable solution
to the down-scaling estimation.
Finally, with this word, my colleagues and I want to set a precedent about how to deal
with the problem of lack of information in study regions.
So we believe that the TRC approach can be extensively used in engaged regions.
For further information about this word, you can contact us or make a review of the following
papers.
Thank you so much for your attention.
Great.
Thank you, Victor.
So, I'm looking at the Q&A.
I don't see...
Just a second.
Just now, I see some questions for Vanessa, which she's addressing here.
So that's great.
If we have questions and answers that can't be addressed now, please use the Q&A.
I find the approach really motivating.
Can you comment a little bit about the scaling exponents across different representations?
Are we expecting the scaling exponents to change as we represent precipitation differently
in models and observations, or is that...?
Okay.
That is a very good question.
Let me start from here.
Okay.
Here.
So, in the motivation, I show that we can represent the precipitation observation as measured
in...
That's measured.
Those measures can be represented by scaling exponents.
Alpha or gamma E that you can see in equation two or three.
The following slide shows that decomposition is something similar to what I do, what we
do, when we have a Fourier transformation of any pattern.
But in this case, it's just a fractal theory behind this decomposition.
So, in this time series, the composition provides us the possible scales that we can
find in a time series.
However, in models and other kind of simulation, the scalings are reduced.
It's a sure set of scales that the models can represent.
So, this kind of model is something better because it can represent a wide variety of
scaling exponents.
And we can also represent events that there are not seen in the data, in the original data
from course scaling simulated data.
Great.
Yeah, it is worth experimenting, I suppose, just to see how similar...
But thank you.
I mean, it's really a neat topic.
I think there's a lot going on that could be explored here, especially with precipitation
and also in climate forecasts.
I believe there's work going on at CMC.
But yeah, let's leave that for the offline chat and go to the next speaker who is...
Thank you, Victor.
Thank you, Dr.
Thanks.
Rajesh Shrestha, speaking on stream flow simulations.
And I'll let you take it over.
We're a bit behind, but I don't think we need to worry too much.
So, Gary, Rajesh, if you can, please.
You can...
Yeah, there we go.
Can you hear me?
Yeah, you just have to go to the presentation mode.
Yeah, thanks.
It's great.
Yeah.
Slower.
It's okay.
Yeah.
Well, you've...
That's...
We're seeing online, we're seeing your view, which would be the next slide with notes.
Is there maybe another way to...
Okay.
So, I don't know.
Maybe I should stop sharing and try to...
Stop sharing.
Yeah.
Yeah.
Yeah.
Yeah.
I'm just just sure.
Maybe I'll just stick with the...
Here, I don't know.
If I share the game one more time.
Yeah.
It might depend on what you share.
If you share your whole screen or just the presentation,
whatever you did, maybe try the other way and see.
We're seeing it here.
That's good.
Is it good now?
Just go to presentation. We'll see if that works.
Yeah, we're seeing the same view, which is your view.
Okay.
Yeah.
No.
Okay.
Thanks, Colin.
That's great.
Thanks.
Thanks for that.
Yeah.
Sorry about that.
Yeah.
My name is Ross.
I am from Environmental Climate Change Canada.
And I'm presenting today in the stream flow extreme changes
in subarctic reverberation in Canada.
This is a part of a larger study that we are undertaking,
looking at future hydrologic changes across Canada using a
multi-modal ensemble approach in which we are using a
multi-hydrologic model to project different components of
hydrologic sites.
Before going any further, I would like to acknowledge
contributions of my collaborators or co-authors who are
Al Scannon, Spiros Beldeos, and Laurent DiRam all from
Environment and Climate Change Canada.
The warming climate is leading to intensification of a global
water cycle as a result of changes of moisture foxes between
land surface and atmosphere.
In this respect, very relevant to northern Canada is enhanced
warming and amplified moisture transport to the north,
related to Arctic amplification or polar amplification.
Because this changes in large scale changes in north can
affect different components of the hydrological cycle like
snow, seasonal and seasonal stream flow and extremes,
and may lead to larger flood events.
However, there is limited knowledge in terms of how flood
extremes will change across different parts of Canada,
in general, and particularly in northern Canada where we
have very, very few studies.
And unlike climate modeling, we also don't have
consistency of future projections across the country.
So with that in mind, our objective in this study or in this
project was to evaluate future changes in hydrological
extremes across Canada in relation to different drivers.
For example, in a recent study that we undertook, we looked
at future changes in the snowpack compared to changes in
winter temperature and precipitation.
So for example, at the three degrees global warming level,
we see a decrease in the snowpack in the southern parts
of the western Canada region and slight increase in the
northern part.
But it is not entirely clear how these changes will affect
stream flow extremes in different parts of the country.
Further complicating the matter, especially in northern
Canada, is the presence of ice in the river.
As you can see here, during the, when there's ice in the
river, we see quite a large difference between difference
in water level.
This is from our gaze in McKinsey River basin.
We see that compared to the open water period where we don't
have river ice, the peak discharge, peak water level under
ice conditions can be 5 to 10 meters higher.
So this can cause a large difference in flooding when there's
ice in the river.
A graphic by Benoit Turcot from Yukon University provided a
very nice illustration of how river ice break up occur.
So he kind of illustrated the different forces and the
mechanism that drives river ice break up and flooding.
And he suggested that ice cover acts as a resistance and
discharge on the ice cover acts as a driving force.
So how river break up will occur and how flooding will occur
will depend on the interactions of different variables, ice
cover, discharge, and other variables that will affect the
ice cover and ice melt.
So for this presentation, I'll focus on the Liard River
basin, which is a large river basin in the McKinsey River.
And it covers about 16% of the basin area and contributes
about 25% of the discharge to the McKinsey River.
The basin is mostly in very pristine state.
So that meaning that it's not affected by human impacts like
regulation and water withdrawals.
So it makes also a little bit similar to analyze future
hydrologic changes compared to other basins which are heavily
regulated.
So for this basin, we have a hydrologic model set up.
It's called a big hydrologic model.
It was set up at 1 by 16 degree resolution.
We calibrated models for studies sub basins within this Liard
River basin.
But I'll focus mostly on the results from the
at the outlet of the basin.
The big model has been very widely used in this part of the
region.
It has different components necessary for cold climates.
For example, it has a good snow algorithm uses full energy
balance.
And it also has a frozen soil algorithm, which is very important
to simulate stream of changes in these kind of reasons.
So going on to the future hydrologic projection framework, we
use an ensemble of eight global climate models for two
social economic pathways, SSP 245 and 585.
GCM is selected based on the study by Mahoney et al.
in 2022.
We use a statistical downscaling approach developed by my
co-author Alex Cannon called in-dimensional multivariate
bias correction algorithm.
A good thing about this algorithm is that it preserves
multivariate dependencies between different variables.
For example, temperature and precipitation, which is very
important for regions like Canada, because snowpack
demands on interaction between temperature and
precipitation.
So if we are able to have those two variables, dependencies
correctly represented, then there's a better simulation
compared to the ones which use a single merit bias
correction.
As I said, we are using a big hydrologic model, which is
a version 5 developed by University of Washington.
And it has been widely used for future hydrologic projects
in different parts of the country.
And we have used this for Liard-based and for other
purposes as well.
So we are based on the driving GCMs.
We are projecting different components of hydrologic
cycle using the big hydrologic model projections.
For example, stream flow, snow, soil moisture,
evaporation, et cetera.
But I will focus on one of the stream flow in this
presentation.
And then we are looking at the projections at global
warming levels from 1.5 degrees to four degrees
compared to the pre-industrial period of 1850-1900.
And looking at the extreme events, we are looking at
both the extreme events under open water and ice
covered conditions.
And we are using a GV distribution for flood
frequency analysis.
This is not a simple way to estimate ice covered
using a big hydrologic model because it doesn't have
a revised model within it.
So we are using a simple approach based on flow,
duration, flow, lag from deficit initiatives and as
as suggested by Gaian in 2001.
And for the estimation of ice covered thickness,
we are using approach developed by my co-author,
Spyrus Beltoius, in the recent study for the
Peace Athabasca river basin where he modified the
Stefan equation for the simulation of
rewrite thickness and projections of future changes
in rewrite thickness.
It's a simple method.
Seems to be pretty effective for what he has done.
And it's estimates a revised thickness based on
total winter snowfall and accumulated
degree day of frost.
Going on to the results.
These are results for mean temperature and
precipitation changes.
Four colors here represent four global warming
levels, 1.5 to 4 degrees.
And the black dots represent the median of the
ensemble.
And as you can see here, mean temperature is changing
for all seasons and increasing with the global
warming levels.
It's also interesting to note that the mean
temperature change are higher than global warming
levels.
For example, at four degrees, we see more than
six degrees of change at the local basin level,
which is to be expected because this layer basin
is located in subarctic region where we see the
effect of larger temperature increase.
Precipitation is also projected to increase
across all seasons.
They're quite consistent.
There's not much difference between seasons.
Maybe a little bit less in the July, August,
September compared to other seasons, but we see
temperature increasing consistently and also
increasing with the global warming level.
So we see higher temperature increase in all seasons
with higher global warming levels.
Looking at the results of Maximum and Airfield
First snow water equivalent, we see very little
change for this basin.
This is the basin average value of snow water
equivalent.
So compared to the historical period, they are very
similar, maybe a little bit decline at four degrees
of global warming, but they are very consistent.
So this may be a little counterintuitive to see
that snows remain constant or consistent with the
warming temperature.
But the air basin being in norm art will remain cold
even with higher level of warming, for example,
four degrees of warming.
And also there's increase in precipitation.
So those two factors will compensate the
temperature driven decline in snow and so we'll have
consistent snow even though temperature is increasing.
A few minutes.
Thank you.
So mean and maximum flow changes across the basin.
We see changes very similar for both for mean and
maximum flow.
Maybe a little bit less.
The increase in the July.
Sorry, July of the September we see decrease but all
the seasons so increases and July of September is
interesting because we see increase in precipitation
in those seasons.
So this has to do with the two factors.
First increase in vaporation during that season and also
shift in the timing of the peak flow to earlier months,
which will also reduce the flow in this season.
So we see similar results for both maximum and mean
flow.
So looking at the annual maximum flow,
flood frequencies, we see that decreasing flood
decrease in return period with respect to any
specific discharge event.
For example, for annual maximum flow,
100 year flow will become about 20 year flow
at 1.5 degrees global warming and about
20 year flow at 3 degrees global warming.
So implications of that on river ice breakup.
So we see that using the simple approach that we
are using,
maximum ice thickness is presented to decrease
progressively with the higher level of global warming.
So this will have implications on river ice
breakup flooding, which we need to investigate
because thinner ice cover means that there's less
resistance to a breakup and then there's a higher
driving force because of higher discharge.
So we don't have the projections of the river ice
breakup flooding, but looking at the historical period,
we see that peak ice influence discharge is
increasing, peak ice thickness is decreasing
and peak water level trend is increasing.
So it appears that discharge is driving the increase
in the peak water level.
I'll stop here my summary and future work
and then I'll be happy to take questions.
Okay, thanks.
Great, let's try to put some questions in the chat.
If we can.
You've taken away my, with those last slides,
you took away the historical trends that you already
show are occurring and you can explain them.
So yeah, I don't have that.
That was my question.
Thank you.
I can share back, yeah.
I can do that. Yeah.
Yeah.
Okay, so yeah, I'll, I could text as well in the chat.
And meanwhile, Alex Sabay is going to segue to the
lake ice and then snow topics.
So how would you like to go?
Do you want a reminder after both of them are done or do you
want to split them up?
I can just split them up because they are somewhat different
topics and that's how I was planning. So.
All right, I'll give you a reminder at 10 minutes then.
Sure thing. Thank you.
All right. So it's my screen share all clear.
Yeah, that's good.
Okay.
So thank you very much for having me in this session.
I was not intending to have two talks in the same session,
but I think when I signed up for got canceled.
So in any case though,
I'll be talking about for this talk is modeling northern
hemisphere lake ice phenology using the Canadian lake ice model.
And so this is I've Alex Sabay.
I'm from UTM department of geography,
geomatics and environment.
And this is the work I'm doing supervised by Dr. Laura Brown.
So first, just some background on lake ice and why we're interested
in it.
Now, Canada has access to approximately 20% of the world's
freshwater, much of which is contained in lakes.
Frozen lakes, particularly in northern regions are often used
for fishing and transportation.
Arctic lakes are often in particularly remote regions.
They're logistically challenging to observe in situ.
An example showcasing this is the image on the left,
which is from the field campaign we had last month where we flew
in on a twin otter to Laura Lake on Cornwallis Island.
And we took some measurements and also took care of some cameras
that we had set up there.
So in addition to in situ measurements,
there are also remote sensing measurements that are available,
but those have some limitations with periods of instrument
operation and maybe difficult to discern,
break up and freeze up.
I had a presentation earlier this week that discussed that more.
But now for another approach.
However, in this work,
I'll be talking about modeling northern hemisphere lake ice cover
using the Canadian lake ice model or Climo for short.
So some brief notes on what Climo is the Canadian lake ice model.
This is a thermodynamic lake ice model single column.
So no horizontal coupling.
It basically just looks at inputs from air temperature,
relative humidity, wind speed and cloud amount.
And through looking at the energy transfer through the surface
of the lake ice and through the water surface,
you get thermodynamic growth of lake ice,
depending also on rated of properties that you get from the cloud amount.
Optionally also you can include snow accumulation and density,
which then accounts for the insulation factor from the snow onto the lake ice surface.
That work is still currently in progress,
but I'll be talking about some work we've done in that as well.
The outputs from Climo we get are daily lake ice thickness as well as
specific freeze up and break ice dates that are calculated based on the ice
crossing a very low thickness threshold to essentially consider when there's
no days of no ice versus when there are days of ice.
The current run that I'll be talking about in this work is a run driven with
meteorological input from the air of five land reanalysis product years,
1980 to 2020 and point two five degree resolution.
And this will be over the entire northern hemisphere.
Just another thing to note with Climo finally is that it can be run for
different lake mixing depths.
The runs that we've done are currently three meters,
10 meters and 30 meters.
I'll just be, I'll just be showing some samples for the entire atmosphere.
But if you were interested in a specific lake,
you could choose a mixing depth that was better suited to that particular lake.
So now working with Climo,
it provides a break up and freeze up,
but there may be multiple break up and freeze up dates per season.
And so to identify when the break up and freeze up are for each given season,
I developed a detection process where first we aggregate the freeze and break
updates.
And then for each season,
I find the longest freeze up breakup interval.
So the longest period of continuous lake ice cover for each individual grid
point or sorry for each.
Yes, for each individual grid point for each year.
And then I record those freeze and break up dates as the freeze and break up for that season.
There are a few caveats with this approach to which we're still ironing out,
particularly some regions experience multiple freezing periods of sink similar length in some seasons.
Or sometimes you get regions that just have persistent ice cover for multiple years,
which can make it a bit more difficult to discern.
But this approach is suitable for most regions.
This work is still being currently under development.
So some of the results I'll show will have some caveats.
Again, this will just give you a general idea of what we're working on.
So without further ado, then here are some sample output for an early year of freeze up and break up in the no snow run of 10 meter mixing depth.
So the year of 1980.
And so here we can see when the freeze up and when the breakup are happening as a day of year.
When the freeze up is greater than 365. What that just means is that freeze up occurred after January 1.
So we see here is that kind of as expected in colder regions, we tend to see a earlier freeze up as well as a later break up.
In sort of the very northern parts, the Arctic, Canadian Arctic, or Pelago, you see that there's no freeze up or break update because those are actually reasons where there was continuous lake ice cover during this year.
And conversely, of course, in southern more southern regions, you do see that there's freeze up happening later and break up happening earlier.
But also you can see that there is some variation that's partly seems to be associated with topography, for example, the Rocky Mountain area.
Where you can see that there's somewhat the breakup, the ice is persisting somewhat longer in the higher altitude lakes. So overall, it tends to track temperature quite well.
Now for comparison, we can see a later year of freeze up and break up. So immediate thing that kind of sticks out the most is now you have parts of the world.
For example, if you look at Europe or at kind of the United States area, you can see that there's now regions where we're not getting any freeze up or break up happening at all anymore.
And so that's a key difference that some lakes from year to year by region, it may vary, but in general, you're seeing more regions where lakes are not freezing and breaking up at all.
Also in the very northern regions, you may now see that some regions that previously had them and experiencing breakup are now experiencing breakup.
So that's just to highlight kind of differences between the beginning and the end of the time series.
So we can also look at decadal differences to get kind of a more general picture that doesn't make isn't so influenced by year to year variations.
And so what we see here is that for the 2010s minus the 1980s decadal difference, if we're just looking at the freeze up. We're seeing in general that there's freeze up happening later in later decades.
So that's kind of consistent with what we expect given climate change and so on.
Know that the scale here is a bit cut off just so we can see some of the smaller variations in freeze update.
But nevertheless, we can see particularly in the northeastern Europe area. There's quite later freeze up. One thing to note is that there may be some artifacting happening kind of in the western US region where there's quite a lot of
freeze up may only be happening in some areas at certain times so because of that it's kind of more grainy we may so we may need to do some a bit more QA to look at the sampling there.
But the overall picture is that we're getting generally later freeze up. There is some earlier freeze up in southwestern Asia and slightly in continental North America as well.
So for the changes in breakup between decades.
We're seeing also again earlier breakup in most regions, especially in eastern Europe and also northern Siberia you get much earlier break up.
And also, there's a little bit later breakup in again this kind of central Canada north northern US region, kind of in the prairies. So, we're still working on investigating you know what are the driving factors of this but nevertheless it's kind of an
issue to consider.
So now these are all from model output of climate run without snow cover on the lakes. So, now, currently kind of in progress is work we're doing with running climate with snow input also from air five land.
So, the key caveat here is that air five land by itself does not produce no depth or density over lake ice.
And this in the upper left panel where that is snow depth taken directly from air five land over a great bearing bracelet lakes.
And you can see that the lake, the lake grid points are just completely free of snow. And this is true for, you know, all of the years.
The model just or the reanalysis product just doesn't include snow over late guys. So, to kind of account for this, what we did was we linearly interpolated snow from adjacent regions around the lakes onto the lakes themselves.
We use a threshold of lake fraction greater than 0.2. Just to make sure that the grid points were thoroughly snow covered.
And the result that we get from this regretting is shown on the bottom left panel.
Note that this is shown at a point one degree resolution, we regretted it to 0.25 degrees for the climate run because that's what we're running climate at the moment.
But so that smooths out some of the sort of artifact and get here but generally to get it, it gives a sort of broad sense of what the snow depth is.
And based on consulting we did with some Lance no colleagues.
This is a reasonable approach given what we have available.
So again as I mentioned running climate snow is currently in progress but here's just an example of a single grid point comparison over Eleanor Lake over Cornwallis Island, shown on the map on the right here.
Just to give you an idea of how this actually impacts the lake ice. And so we see is that when we run climate with snow, we end up getting freeze up happening later than if we run it without snow.
And conversely, we end up getting earlier breakup.
This is largely attributable to the insulating properties of snow, even during the freeze up season snow will fall quite readily in this region over lake ice and snow inhibits the growth of lake ice.
So overall we end up getting a thinner lake ice cover as well.
And so because of that, when the ice is thinner it'll end up melting away earlier as well.
So those are the impacts that we're finding for adding snow in this region. Again, we're going to do some more investigation of how that impacts the hemisphere as a whole.
But just for reference that's what we're finding so far.
All right, good timing because here's my conclusion slide. So conclusion again, ongoing work but we find freeze up in later in later decades for most regions, particularly northeastern Europe, breakups happening earlier in most regions for later
decades, except in Central North America.
And our next steps are going to be running climb over the entire northern hemisphere with snow from era five land. Now, no, we have actually done this we're just in the process of aggregating the data and processing it to be plodable I suppose.
Furthermore, additional observational validation and statistical analysis. We do have some in situ measurements available from some of our observation sites.
And finally, we'd use CMAP six input to climb out to run future projections of late guys under various scenarios.
So that's all for this talk of mine.
Thanks, Alex.
Thank you.
So, hey,
I don't see any questions yet so a quick question is, given that these are offline simulations what, but that that, I mean, snow cover is part of a couple of feedback in the climate system so
how would you address or expect things to change in a couple of contexts.
Oh, that's a very good question. I mean, we expect there to be some sort of feedback in terms of, well, for example, you've just an albedo feedback with snow cover enhancing albedo, and thus, there'd be less, you know, less maybe localized warming.
But conversely, you may have competing factors from the fact that the snow cover does also insulate the lakes from the atmosphere. So, for on the scale of smaller lakes I suppose it may not have as strong of an effect but for larger lakes you may see more of an impact.
But either way, that's a good question. And I'm sure there's other things to consider that I haven't mentioned as well.
Okay. Yeah.
So, yeah, I'll let you go ahead with the next talk, since you're all sped up. And I'll give you another, you have an extra minute or so so you can take your time on this one if you want.
All right, made up the time that I had there.
So, now in contrast previously I was talking about lake ice, but here now I'll be talking about assessing the impact of differing snow inputs to reanalysis based snow on sea ice reconstructions.
So again, we have a model we're using reanalysis input to it, but some of the details here are a bit different.
I think that I did as part of my PhD under the supervision of Paul Kushner and in collaboration with Alec Petty who's been very instrumental to this work.
So some background on why we're interested in snow on Arctic sea ice. It's implicated in a variety of climate processes such as the ice albino feedback, it moderates atmosphere ice ocean he fluxes, it can also contribute to the ocean freshwater budget.
In addition, I have this little picture of a seal here, because it actually has a biological impact in that range seals make their habitat on snow on sea ice and they need at least 20 centimeters of snow to be able to make their dens.
Snow has many complex processes with differing seasonality and regional characteristics, as you can see illustrated in this illustration here. So, it can be quite difficult to represent.
It is required for the production of altimetry based estimates of sea ice thickness, but it's quite challenging to observe due to the remoteness of Arctic sea ice.
And so, one approach that has been used to produce estimates of snow on sea ice is using reanalysis based snow on sea ice reconstructions.
Namely, some examples of them that I'll be talking about here include Nesosim, which I've worked with a bunch, and then there's snow model LG, which to be clear snow model LG I did not run I'm using available model output data, but it'll be used as a point of comparison.
So more on Nesosim since that's the model that I've worked with and contributed to. This is the NASA Eulerian snow on sea ice model. It is a very simple model there's two layers 100 by 100 kilometer grid squares, and each layer has a prescribed snow density and then there's a few simple snow processes that act on the snow in these layers.
So it takes snowfall input from reanalysis scaled to satellite observations.
And one key thing to know that I'll be discussing a bit here is that there's two free parameters. There's the blowing snow parameter, which governs the strength of loss to the atmosphere and to open water, depending on wind speed so snow is being blown off the sea ice.
And then there's the wind packing parameter which governs the snow density in the model. Again, there's just two prescribed densities in each layer, but the snow density is represented as a height weighted average of the two densities.
And the amount of snow that's transferred between layers is governed by the wind speed multiplied by this wind packing factor.
So these parameters in my previous work were calibrated to observations will calculate calibrated by calibrating the model to observations of snow depth and density using a Markov chain Monte Carlo method.
So I'll discuss a bit of what I've been doing there.
In each detail of the intricacies of Markov chain Monte Carlo I have a publication on that, if anyone wants further detail, but the gist is basically we calibrate the model parameters by comparing the model to observations, and find parameters to give the optimal match between the
output and the ops, and then we obtain optimal parameter distributions.
The observations we used were airborne and in situ snow depth and in situ snow depth density measurements.
The previous calibration work that I did, we ran the system with error five snow inputs and calibrated the parameters. But now the current work that I've been doing has this parameter calibration repeated when you run the system with different snowfall inputs.
So we have snowfall input from error five Jerry 55 and merit to reanalysis and on the right what you see is a plot of the posterior distribution so distributions of the parameter values that are obtained.
When the model is calibrated with these three reanalysis inputs to observations.
And so we can see that, depending on which input we use, we actually get different optimal values of these parameters.
So there's some tuning that's been happening with this model previously this was manually done so one of the assets of this approach is that it's done automatically.
Now what does this actually look like in terms of the output it produces on the left two columns we have snow depth and density climatologies, which are done with only one set of parameters being used for all the products blend denotes the
average average here. And after on the right, we have nososum output after each individual calibration has been done.
So what we see is that for the upper row for snow depth, the climatologies are reconciled there. There's a smaller difference between them.
However, with the densities on the lower row, we actually see that there's a wider spread of the densities when we use the individually calibrated values.
Now the reason for this, we expect is because there's much fewer observational constraints on density than on depth, and density is more sensitive to parameter values and just no input since it's primarily just a function of how much snow is in each layer.
So for the application that we are considering for sea ice thickness, it's more sensitive to depth and density so we proceed with the calibrated parameter values.
So now to further investigate how these this calibration looks and also how using different reanalysis input can affect snow model output, we are doing a regional comparison with snow model LG says a Lagrangian snow on sea ice model
it's actually tracing sea ice parcels as the season evolves. It includes more detailed representations of snow process than the sosum but it's considerably more computationally expensive to run.
However, it is at a higher resolution.
And so snow model was run with era five and Mara two and again we did not run this model but we got the output from the NSIDC.
So the different regions that we're looking at. There's a schematic map here from Meyer 2023 which shows them, they're selected based on differing sea ice characteristics in the region.
For the purpose of this presentation, I'm just going to show a selection of the regions listed below on the slide, just for the purpose of making it easier to see the differences between the regions but they're relatively representative of different parts of this of the Arctic Ocean.
So further ado, now there will be a lot of output here so I'll just try to focus on some of the main points but I'm happy to discuss finer intricacies later on.
First off, regional climatologies for the snow depth. We see good agreement overall and the central Arctic, the Beaufort sea and the took to see snow model LG which you can see denoted by the dotted lines here in the blue and green.
It has a low end of season bias in the Beaufort and took to regions. We assume, or we think that this is likely due to the fact that snow model includes melt processes where some so some doesn't and we do expect melt to be acting to reduce snow depth on sea ice at this time.
And then in the currency the currency and particularly in the East Greenland seat, what we see is that there's less model in snow model LG than the system.
It's hard to say which one has a better representation because there's very limited number of observations or validation, but we can just see that there's a relative biases between the two models, regardless of which snow inputs are used.
So looking at the snow density, we can see that, compared to the snow depth compares and there's less agreement in snow density. This is partly just because NISO since no density is constrained to less than 350 kilograms or cubic meter.
So, in every region, NISO since no density basically increases until about the maximum density and then it kind of falls off.
This is just a limitation of the model itself.
So there is, so as a result, because snow model has more complicated representation of density allows for some snow metamorphosis. There are more differences between the regions in the seasonal cycle.
One key thing to take away however is that the density biases do not necessarily compensate for the depth biases between the models.
For example, in the East Greenland Sea, we do see that snow model has a lower depth and the correspondingly higher density suggesting that the higher density may account for some of this difference compared to NISOSM.
However, in the currency snow model has a lower depth overall, but it still has a lower density as well relative to NISOSM.
So when we're talking about biases between the models, density differences are not the full picture.
Now, snow volume and ice area, I'm showing these simultaneously just because they're kind of tied together.
Briefly, the snow volume interproduct biases generally resemble those for snow depth with, you know, this large difference between the NISOSM and snow model in East Greenland in particular.
Low seas, low early season snow volume is associated with low ice area as is kind of expected since they're directly dependent on each other.
So in the Central Arctic, where you do have a relatively large early season ice area, you also see a larger early season snow volume.
High latitude regions, more central regions tend to become fully ice covered toward April. That's what you see if you look at the sea ice area on the bottom in the Beaufort and Chukchi seas, for example.
They're fully ice covered from November or January in the case of Chukchi Sea.
And so the shading here represents inter-annual variability and you really don't see that toward the later months because it just ends up fully ice covered. Conversely, in Barron's and East Greenland, because these regions don't end up fully ice covered, you see quite a large amount of inter-annual variability, which may contribute to some of the volume variability there.
So now those were climatology plots. Now just looking at trends, I have the same layout here, so I'm showing two at once just to kind of inter-compare.
That's 10 minutes.
Thank you. So the inter-input differences in this case are larger than inter-model differences in some regions for snow depth.
That's kind of one of the key points. If you look at the central Arctic here, you'll see that snow-model and mesosim driven by Mara II, the green lines are actually quite close to each other, and they show a significant decline compared to those same models driven by
Mara V, which are in blue, where the significance shading overlaps with zero. So depending on which reanalysis input you use, you may actually get different conclusions about the strength and the significance of the trends.
So that's just kind of tying into a general caution with when working with reanalysis products where trans-reanalysis products may be somewhat dependent on products being assimilated.
You see significant depth declines towards end of the season for all products in Chochikara and Barrens. Density results need to be interpreted with caution because due to the model density representation being limited, its variability is also limited.
But we see slight declines in most regions of density overall, more in snow-model algae, except in East Greenland where for both snow depth and density, there's generally not statistically significant trends.
And this region is just highly variable overall, so we don't see that so much.
Finally, snow volume and sea ice trends now to complete the picture here. In the Kara and Chukchi Sea regions in particular, you see this strong decline throughout all months except for September.
It may be associated with the early mid-season sea ice decline, which we see in the sea ice area. Other studies have found similar results.
And furthermore, we see a significant end of season volume decline in the Barrens and East Greenland seas, which in particular in East Greenland, the snow depth doesn't show much of a decline, but the volume, particularly in the mid-to-late season, shows a strong and significant decline for both the snow and snow model.
This is likely associated with the ice area decline.
Alright, so just some brief conclusions. Snow depth and density measurements can be used to calibrate and so some model parameters when you use different snowfall inputs.
We do see an improved agreement in depth after this calibration between products, but decreased agreement in density.
Various regional variations of snow climatologies and trends. Agreement with snow model varies by region.
Early season sea ice area decline can be associated with later season snow depth decline in some regions, consistent with other work.
And some regions with no significant snow depth trend may show declines in snow volume associated with sea ice decline.
That's all for this. Thank you.
Great.
Again, I was searching. I don't think I'm not searching the right place. So if there are any questions that do pop up after, we're heading towards a break now, but I guess.
And we'll come back.
And I think.
Let me, if there aren't, I don't see any questions. I'll ask one.
In terms of, was it a Bayesian method, the MCMC method? Yes.
And in, in, in general, as a general question.
Would there be any strict, is there a strict adherence between the model and odds comparisons such that you want strictly to make the model look like the odds.
Like with Kurt Blanche, like without any clarification, without any qualification there.
Or does the model somehow allow for there to be differences between the two if, if there were some reasonable reasons for that to occur.
Yes, that's a good question. I didn't go into the details of the method, but it does.
It's a maximum likelihood optimization. So it does allow for some uncertainty in the observations. So, and we do account for that with the observations we use, we include some uncertainty factors there.
So it's not necessarily matching exactly, but there is some wiggle room.
Okay.
Very good answer. Okay, thanks.
Thank you.
So yeah, we, we have a few minutes to break and then our session begins again. So thanks. Thanks to all the speakers. And yeah, please join us on the back end of this session.
Thanks everyone.

Let me try for you. Okay.
Thank you, it's appreciated to know that you're there.
Thank you, it's appreciated to know that you're there.
Thank you, it's appreciated to know that you're there.
Hello and welcome. My name is Ann Dakers and accompanying me today is my co-host Miguel
Tremblay. We both work for the Meteorological Service of Canada within Environment and Climate
Change Canada and we're happy to be here exploring AI with you for a second year.
Before we begin, a few reminders. 15 minutes are reserved for each presentation and we
encourage our presenters to keep three minutes for questions at the end. Time permitting
participants with questions will be able to use the chat at the end of the presentation
and their questions can be upvoted by the other attendees. Miguel will provide a
step at 10 and 12 minutes and will bring the presentation to an end at 15 and will be ruthless
guys because we want everyone to have time to present. Miguel, do you want to do a quick recap
in French? Yes, thank you Ann. Hello and welcome everyone. My name is Miguel Tremblay. I'm a co-animateur
with Ann Dakers. Maybe you're used to hearing us in bilingual, talking about artificial intelligence
for one year. This is our second session. We were there last year too. So we work for the
Meteorological Service of Canada. A few reminders. That's it, 15 minutes are reserved. So I'll
do a quick recap at 10 minutes. You'll have two minutes left. The presenters will have two minutes
to wrap up and at 12, we'll tell you in order to keep time for questions. We won't try. We'll
succeed in setting up in 15 minutes. So that's it for the directions. I'm glad to see you and I'm
glad to hear you. And so without further ado, we're going to go with Christopher Soubich,
who is a scientist in the Canadian environment. So he's going to talk to us. I'll let Ann
introduce him in English. Thank you Miguel. So Christopher, you're kicking off the ball.
Christopher from Environment and Climate Change Canada. He will be presenting on the State of the
art in AI weather forecasting and ECCC's research plans. So without further ado, over to you, Christopher.
Thank you. Let me get the screen sharing going and share. Okay. Looks like we're good. Okay. Hello,
everybody. I'm Christopher Soubich from Environment and Climate Change Canada. I'm here to speak as
said about, well, click. Okay. This talk is essentially in two parts. The first part is going
to be a brief summary of the current state of machine learning based forecasting with a
broad focus on medium range weather forecasts. The second part of this talk will be the
forthcoming AI roadmap out of the atmospheric science and technology director at NCCMAP,
which effectively broadly sets out where this part of Environment and Climate Change Canada
will be going in the short and medium term future with AI. And finally, time permitting,
I'll conclude with a preview of talks to come and just general thoughts on the state of AI
in forecasting. So in case you haven't noticed, AI is becoming a big deal. It's no longer just
pictures of cats or helping kids cheat on their homework, but it's starting to influence
the industries and fields previously thought unassailable. Now, this isn't truly a stranger
to weather forecasting. AI adjacent topics have long had roles in the forecast production
system. We've long had statistical models for quality control of observational data and for
post processing. But a phase change has been that over the past two years, maybe three models from
the academic and private sectors have gone from things that we should probably keep our eye on
as interesting things for the long term future to, well, these two nearly full featured forecast
systems that are competitive with the state of the art. With that in mind, I can just state very
plainly that Environment Climate Change Canada, in particular my part of it, is taking AI very
seriously and we intend for it to have an increasing role in numerical weather prediction
systems going forward. We're taking a sort of trust but verify approach in that we intend to
make AI advances operational, but only after they've been scientifically proven. We're not in a rush to
perform science by press release. And this is also a medium to long term plan.
Capacity building is going to take years, both in terms of acquiring sufficient computational
power for this and developing human expertise necessary to properly use it. So the modern AI
forecast field essentially began about two years ago. I dated from the first publication of
GraphCast by Lam in 2023, which is the first time that an AI model could truly claim to beat
the state of the art from, in this case, the equivalent forecast from ECMWF.
In addition, the truly shocking claim was that these forecasts came with orders of magnitude
faster running time. GraphCast will produce a 10 day quarter degree forecast in about 30 seconds
on a single GPU. That put all of the weather centers globally on notice that
if these trends continue, we must adapt or we'll fall behind. And I mean, in some sense,
research has always been that you can't sit on your laurels from 20 years ago and pretend to be
competitive, but this field is truly advancing at a revolutionary rate.
Models are being constantly released. Any summary like the one I'm about to give was going to be
out of date within weeks. In fact, just this morning, I've had to update my slides to add
two new preprints that have come out in the past four or five days. Nonetheless, there are some
common features between medium range models that are shared by, if not all of them, then most of
them. Most broadly speaking, these models attempt to learn from data, which means rather than simulate
the atmosphere from first principles, they try to look at some form of data and predict what it
will become. In this case, for the medium range forecasting, the data is almost always the error
of five reanalysis. That's our highest quality, longest term and most uniform record of the
atmospheric state that we have. And it's accepted as ground truth for most of the models, but this
dataset does have known issues such as relatively poor precipitation. The other common feature about
AF forecast models is that most of them have sparse limited outputs. They tend to predict only a few
variables. They tend to predict a small subset of vertical levels compared to what we're used to
from an operational forecast, and they have a limited selection of lead times. This creates
new downscaling problems. If you were in the plenary talk, you heard our friend from Nvidia say
that this might not be a barrier to full prediction, but at the same time, we're used to having the
prediction plus all of the rich output for free. And not having that means we'll need to contemplate
new kinds of downscaling problems to get rich data from a sparse forecast. And finally, these AI
forecast systems have essentially no proofs of physical consistency. Even hybrid models are
and development in that area has a classical dynamical core, but leaves a physics system that is
only tangentially concerned with things like conserving energy. Now, medium range forecasts
tend to break down into a few different categories. The first and most conventional of these are
deterministic models. These are analysis predictors, and they essentially answer the question of if I
have the atmospheric analysis at time zero, what is the minimum error prediction of what that analysis
is going to be six hours from now? As a general rule, these tend to give overly smooth forecasts
that over long lead times erase fine scale structure in the atmosphere. The widely held belief is
that this is a consequence of training based on mean squared error measures, because the lowest
mean squared error prediction of the future is your ensemble average. But the ensemble average is
not a physically plausible forecast. Now, that being said, these models have shown great success,
and having a really good ensemble mean prediction of the future is still a really good prediction
of the future. Each day of predictability translates to billions or billions of dollars of
enabled economic activity. This category of models is in some sense the oldest, and I really
hesitate to use that word with a field that's about two or three years old, but they're models
from many different groups. Graphcast and forecast net have been previously mentioned. Graphcast is a
graph neural network. Forecast net is now a spectral Fourier neural operator that operates in
spherical harmonic space. Pangu weather came out as a vision transformer, and AIFS is now apparently
officially published with a preprint, and it's a graph transformer. The jargon here is not that
important, but the underlying point is that you can reach a deterministic forecast with many different
AI architectures. There's as of yet no specific royal road to a forecast.
The second category that I've somewhat arbitrarily divided things into are ensemble models.
These try to address the problem of overly smooth forecast by adding some element of randomness.
An ensemble forecaster will receive some kind of random data source, either a random number
generator or a field of noise, and it's asked to generate essentially different forecasts from
the same set of initial conditions. This more or less solves the problem of smoothing based on
mean squared error loss functions, because each individual forecast no longer has to track the
long-term truth, but each one can be plausible, and then you're tracking the future with a true
ensemble mean. Now, the downside is that these models are all more expensive to train and run.
The deterministic systems are the same size. In operations, you will need at least one inference
run per ensemble member. So, graphcast's 30-second, 10-day forecast is great, but if I want a 100
member ensemble, now I'm talking an hour or two, and that can add up very quickly.
In terms of training, if you're training with an ensemble error measure like the
cumulative rank probabilistic score, that requires training over an ensemble, and
increasing the size of things during training tends to increase the scarce GPU memory requirements
and the overall cost of building the system. This is a newer frontier of AI forecasting. It's
probably going to be more popular in the months to come, and the examples here are also somewhat
newer. Gencast was previously mentioned in our plenary. It's a diffusion model based on
graph transformer network. Neural GCM is a hybrid model also previously mentioned,
and it's a dynamical core that has learned physics parameterizations, and in particular
can operate in an ensemble mode, which is why I've included it here. Finally, there are models
such as seeds also out of Google that don't try to forecast directly, but they try to take
an ensemble mean that already exists from some method, like a traditional forecasting system,
and generate new ensemble members to fill out the probability distributions.
The final category of forecast models, I'm going to divide things into our foundation models,
and these essentially answer the question of what if we had GPT-4, but for weather?
The idea here is and shared by foundation models that exist in our development, is that you break
up a weather state like the analysis into tokens by regionally subdividing it usually, and then
you ask the foundation model to predict missing pieces of it. You say you can mask out the future
and say, okay, predict this, and then you're training it in a forecast context,
or you can mask out a missing regional piece and ask it to fill in the blank. You could even ask
it to predict the past, I suppose. This allows the foundation model to be trained on qualitatively
different data sources. For example, you might have an encoder suite that takes satellite observations
directly and tries to turn it into token space, or the analysis from era five, or lower resolution
climate forecasts. Once tokenized, the idea of a foundation model is that you have a giant
middle processor layer that operates in this latent space, and from there it is
relatively simple to build out decoder models to give you things you want like tomorrow's forecast
today, or what the radar is going to look like in 30 minutes. Foundation models certainly prove
their worth with text-based processing like GPT, and they're also very popular and emerging in
Earth observation applications with interpretation of satellite data. For example, at an ECMWF ESA
conference a few weeks ago, there were some interesting talks about taking satellite data
and using it to infer tree cover near power lines in Norway, tasks that would normally take some
very expensive helicopters and doing them much, much more cheaply with readily available Earth
observation data. The downside is that foundation models tend to be extremely expensive to train,
and these will push the limits of what the public sector is probably willing to spend
should foundation models prove themselves for forecasting. Few of these models exist right now
for NWP, but there's plenty of private sector interest. I mean, our friend from Nvidia talked
at length about that. The two models that are currently published are Atmorep from Christian
Lessig, which is a transformer model, tested on both forecast and downscaling applications,
and Microsoft has published, as of a few days ago, it's Aurora Foundation model, which most
notably uses several different data sources for training, and not just air five, but also
climate simulations and operational forecasts, and I believe one of Noah's ensembles.
Now, to integrate all of this, I'm, as a researcher, I can make a few broad predictions
on where the trends are in this area. First is going to be the convergence of data simulation
forecasts, now cast and downscaling roles. Right now, the leading NWP forecast taken an
analysis and give you a forecast, but the obvious question is to what extent can the rest of the
chain be included? For example, forecasts that are ensemble generating can often be reversed to
perform data simulation, and a couple of references here are one using a diffusion based model to
assimilate sparse observations, and the second one is to perform data assimilation inside the
latent space of an autoencoder, which effectively replaces the background error covariance matrices
of a DA system with ones that are nonlinear and flow dependent from the autoencoder space.
A second avenue here is to combine conventional models with an AI based bias correction. Farshie
from ECMWF has published recently on using a neural network bias correction to include some
element of AI inside the IFS 40 var system. And Saeed, my colleague, is going to present in
about half an hour on spectral nudging to bring a classical NWP system closer to an AI forecast to
preserve rich data and have the accuracy of AI. And finally, there's some effort on all-in-one AI
forecast systems that go directly from observations to forecasts to post-processing to produce
predictions of future observations. The Vaughn 2024 is the Aurora mentioned in the plenary,
which is an encoder decoder architecture that uses Unets, and it's the obvious long-term future
of foundation models. There's also rumors that Google is buying up satellite data for
its next generation, Graphcast version 2 or something like that, but I don't have any
particular confidential information to share. Okay, now the second part of this talk is the
Environment and Climate Change Canada AI Roadmap. This is a joint product of the ASTD and CCMAP.
It's a product of an internal Tiger team across the Research and Operational Divisions
that was put together last fall after an internal study on the current state of AI
and weather forecasting, where we had a whole lot of talks like the first half of what I just gave.
This roadmap is designed to set very broad research priorities. It's not designed to currently pick
and choose what projects are worth funding, but to set out how we should think about AI as an
institution. And the largest theme from this roadmap is the need for capacity building,
both in terms of compute capacity. In light of the supercomputer update we're likely to have next
year and whatever gets procured in the years thereafter, how we should think about the use
of cloud computing, and finally what we need to do from a human resources and training standpoint.
This is a living document. It should be published soon. I'd hoped I could begin this talk with a
reference to the live document, but I think it's still in translation. But once it is published,
it'll be updated every few months to every year or so with internal reviews and updates,
just as we understand more about AI. The broad themes of this document are how we intend to
integrate AI throughout the research development and operation cycle. So we're not limiting it to
just graphcast style forecasts, but we're interested in applying AI everywhere from
data gathering to post-processing. The main evaluation criteria here are the
triumvirate of feasibility, service, and efficiency. Feasibility answers the question of how capable
are we of developing and running a proposed AI project. And that includes not just the scientific
risk of, well, putting a whole lot of time and money into a project and having it not work,
but also whether or not we have the compute resources to do this or the human resources to
develop and manage it. The second criterion is service of how a project will improve
ECCC's weather-based services to Canadians. And that is not just in terms of accuracy, but also
whether we can make our forecast products more timely, whether we can have more numerous projects,
or whether we can have qualitatively new products like hypothetically,
rapidly updated now casting, which we just currently don't have available from any government
source. And finally, there's a question of efficiency. How will a project make efficient
use of our computational resources? That includes the broad themes of energy efficiency, but also
the government-specific theme of being a good steward of public funds. Supercomputers are,
as one might guess, rather expensive. And we ought to demonstrate to Canadians that we are getting,
that Canadians are getting their money's worth. Okay, so more specifically,
how are we looking at AI from the observation to product pipeline? First category here are
observations and data assimilation. So in observations, AI is in some sense an extension
of what's already happening in terms of looking at quality control and error estimation of our
inputs. AI can perhaps allow us to have some new capabilities with learned observation operators
to take advantage of parameters that are, to learn parameters that are not directly observed,
such as complicated satellite measurements that are nonlinear features of the atmospheric state
that are hard to directly measure, but might be possible for an AI to learn. And finally,
we have some ideas on unconventional data sources. One idea thrown around in discussions was using
ad hoc webcam data like traffic cameras to evaluate real-time precipitation class information.
In principle, someone can look at the feed and say, yep, it's raining,
and we can have an AI do the same thing and potentially have useful data. On the data assimilation
side, in some sense, this area is most advanced because DA is already using
some heavy computation for its error matrix operations and they're investigating GPU use.
As previously mentioned, there's a data assimilation in latent space of a model.
If we can vastly increase ensemble sizes through AI, we can perform better PDF estimation
through particle filters or non-Gaussian-based statistics. And finally, we can start to estimate
perhaps the model parameters themselves rather than just initial conditions.
The numerical prediction side, this is closest to what I talked about previously with AI forecast
models. In the near term, we're looking at implementing the AI models as they are to provide
second opinions about the weather to forecasters. In the medium term, we're looking at fine-tuning
large models on our operational data sets to provide better AI forecasts. And in the longer
term, we're looking at structural changes and developing new models in general. In addition,
we'd like to hybridize classical NWP and AI models to help fix the problem of sparse outputs.
And along the way, we'd also like to investigate emulation of the physical
parameterizations. For example, radiation is very expensive inside the atmospheric model,
and 3D radiation is probably better than one-day radiation, but it's too expensive to run
operationally. If we can emulate that, we can have a better parameterization that is
still within our compute budget. Also, we'd like to extend this to ocean ice and land surface
prediction, but that is still fairly preliminary, in part because the data sources aren't quite as
complete. Now, in terms of the tail end of the forecast for post-processing and final products,
this is the realm of downscaling and now casting. We have a 2.5 kilometer high resolution regional
system, but it's just too expensive to run too often. If we can downscale, we can potentially
achieve that resolution of output and evaluate extremes and risks at that scale without
being limited by melting the supercomputer. It would also be really nice if we could have near
real-time assimilation of weather station radar data to improve the half-hour one-hour forecast,
which is not something we can currently do very well. In terms of post-processing,
the statistical post-processing systems have all long had systematic error adjustments and
station-specific adjustments for representatives' errors and the like, and AI can help turn that
into better nonlinear corrections. Finally, in terms of the expert product sides, we'd like to have
better high-impact weather diagnostics with well-calibrated forecasts for all of the big ones,
tornadoes, hail, blizzards that just don't show up in the larger-scale forecast, but are
extremely important for the people stuck in them. Excuse me, Christopher. Two minutes left.
Yes, okay. I'm going to be very quick. Challenges and opportunities. As I've hinted at the entire
time, we have challenges and these are opportunities. On the physical side, we have limited compute
capacity. GPU compute demands are only going to go up. We don't have very many of them.
We're going to probably get more in the future, but we need to manage them.
We also need to care about data management in terms of not just having an archive that exists
on tape, but one that is living and can answer training-based questions very quickly. In terms
of HR, we have similar problems that our researchers are all very good, but they're also not necessarily
well-trained on AI. We need to close that gap. In particular, we would love to have
increased collaboration with both the ivory tower and private sector.
The roadmap sets out targets and milestones. We would like to have our first operational AI
systems for innovation cycle five, which is targeted early 2026 after the supercomputer update.
The current innovation cycle is just closing, so it's a bit too early for it.
Ultimately, we'd like to have AI as just another forecast tool by 2030 or so.
I would love to talk more about this, but unfortunately, there's no time. In general,
our divisions have all been investigating how we can integrate AI into research flows.
Some projects have started, some are waiting for people, and some
just simply need more resources that we don't currently have. We would love to collaborate
on them. If you have AI skill and you have a weather-related project, please email someone
at our division. We would probably love to talk to you.
Unfortunately, I have to cancel this slide where I was going to talk up all of my colleagues'
presentations to come. Please stick around. They're going to be great. Finally, conclusions-wise,
AI is rapidly advancing the state of the art in numerical weather computation.
This is a phase change of forecasting, and I'm excited to see where it goes. We will make AI
and machine learning technologies a major part of our systems as they prove themselves,
but we're a public service organization. We recognize that we have to be very careful
about what we stand behind operationally. Finally, I hope these slides are available
afterwards for the references. There are references to all of the systems I've mentioned,
and I'd also just like to highlight that of the 14 references here, about eight are
preprints. This is a really, really rapidly moving field. Thank you. I am...
Missy Christopher, we have time maybe for one question. I know.
I'm going to take the highest ranked one. It's a long one. Are you ready?
Yes. Perfect. Currently, AI seems to be quite expensive and rapidly developing. While we were
waiting for AI-based models to build better foundations and training to replace numerical
weather prediction or for climate studies, can we exploit its computational speed in the near
term for now casting or HRR-like output? For example, ECCC can incorporate into CAM for very
short thunderstorm prediction or perhaps weather elements on grid now cast.
Okay. I believe these projects are under investigation. I'm on the medium range forecast
side, so it's not my particular side, but there's a presentation in this session, I believe,
after lunch that investigates now casting via an IBM Foundation model. I think that'll begin
to answer that kind of question. In general, yes, this would be very good. In practice, the nearest
term limits are probably compute potential because we have relatively few operationally
available GPUs, but hopefully in the next few months to year or so, that will improve.
I might sneak in another question then. Christopher, we have a minute. Have you
considered the role that Canadian industry will have in developing AI capacity at ECCC?
We would love collaborations from industry. That is my politically correct and also true answer.
We have limits on our resources in part because until, well, procurement thus far has been focused
on making our operational systems better for obvious reasons. The cycles of this mean that
it is practically difficult to, sorry, I'm speaking as a researcher. My boss is probably
listening, so there's some things I need to be very circumspect about saying. We need to be very
careful about how we commit resources for training large models. Industry is, I think, a fantastic
partner, both for the potential of having compute resources we could borrow and also a better focus
on some of the most downstream applications. For example, our leading talk that opened CMOS was
on the weather impacts on the insurance industry. To the extent we can be of value there, I think
there's room for joint products. I will try to answer other questions in the chat as we continue,
but otherwise, I will stop sharing, meet myself and thank our hosts.
Well, thank you, Christopher. That was an amazing feat. You put in two very large presentations
into one 30-minute presentation. You have all my admiration and thanks for doing that.
The NWPEI-based model, the verification against the observations of artificial intelligence models,
Hervé and Hervé, it's up to you.
Hello, I don't know, so can you hear me?
Very well.
Do you see my screen?
Also.
Wonderful. So I'm going to do the presentation in French. If you have any questions in English,
there's no problem. If you have any questions in English, there's no problem. It will be easier for
me if I do it in French and for you also. So I'm here to present the work that I did
in collaboration with my colleagues on the evaluation of the models based on the CCC AI.
In a second, I don't know.
So the context is that with the emergence of these models, we realized that we had to
verify these models with our traditional verification methods, which allow us to
evaluate the innovations that are made on traditional models. So I was appointed by
the boss to install, turn and verify these models on our HPC installations.
And I started this work in October 2023, and I worked on it until April 2024.
So what I want to present you is just that.
So the activities that have been completed during this special project
is that we shot, we chose two models, ForecastNet and Graphcast, which were available
for free, it's easy. And, well, ForecastNet, Christopher talked about it a little bit earlier,
so it's a model that is developed by NVIDIA. And then we shot two graphs of Graphcast,
one with 13 levels of pressure, with a roof at 50 hectopascals and a version at 37 levels,
a roof of 1 hectopascal. And we shot each model with three analysis sets. One, the first
is the operational analysis of the OVF, called IFS, on 13 levels only. We also used RA5.
So the RA5 analyses are the ones that were used to train these models.
We were able to shoot the conclusions at 13 and 37 levels. And, of course, we wanted to compare
the operational predictions with the same analysis. So we started these models,
these AI models, with the operational analysis of CCC. And we shot in two real-time modes,
where we shoot twice a day, at the same time as the operational model.
And, like that, the operational metrologists can compare the forecast based on the AI
with the operational predictions. And, also, on my side, I did an evaluation
on a period of one year, which allows us to see what are the comparisons between the models.
So here, I put a slide on the description of the models. It's very precise. I don't have a lot of
time. Basically, the two GravCast and ProCastNet models use about the same information,
but I put a lot of detail there to be complete. But I don't think I'll be able to save a little
time. So, and one of the advantages of certain AI models is their computer efficiency. If we compare
the operational model, presently, it takes about a little less than an hour, more than 6,000 CPUs.
So it's a big deal. So it generates 500 gigabytes of data at every
provision twice a day. It makes outings at all ages up to 10 days. And then it's
15-kilometer models with a lot of vertical resolutions. AI models have less good resolutions.
They generate data at 6 hours, and a less good vertical resolution too. So, but
ProCastNet is very, very light. It's impressive. It takes 20 minutes on a CPU. I'm not talking about
a GPU here. A GPU is even faster, of course. But on a CPU, you can turn it and put it on your laptop
and it works. And it's very fast. It still gives you a good preview. And GravCast,
it's a little bit more expensive. The 113-level confidence asks for 100 gigabytes of memory.
So it's a little bit more expensive. But still, comparing it to the operational model,
it's very, very, much, much smaller. Smaller orders. And I invite you to the second
presentation of Christopher Subick on exactly comparing the computer performance in AI models,
GravCast and Gem, the operational model. And it makes a very nice comparison. I invite you to
have this presentation a little later this afternoon. So what does it look like
as a verification once we've done the average over a year? So here, I have several curves.
Here, I present to you the errors, the prediction of the geopotential at 55 topascals.
So these are the errors. The very thick blue graph here, that's the baseline. So that's the
operational prediction. So we see the errors that increase from zero to 240. And the other curves,
it's all the predictions, the verification and the AI models. So we can look at them.
And then, as these are errors, the closer we are to zero, the better it is. So we see here,
a group of predictions. These are the forecast net. So we see that for the GZ500, the forecast net is
less good than the operational prediction. On the other hand, all the other curves, the five other
curves, are all the predictions made by GravCast using different analyses. So we see that GravCast,
from day five, is better than the operational model, no matter the analysis we give him.
Well, that's for sure when using the R5 and FS analyses, it's better.
But what is important here, what is interesting here, these are the two curves here, orange and
green. These predictions, they are initialized with the same analysis as the blue curve. So we see
that with equal information, GravCast is better from day five on the GZ500. If we look at another
variable, which is the temperature at 850 hectopascals, so it's the same colors, the same curves. In this case,
we see that from 72 hours, all the AI models, the operational model, even forecast net,
but we still see that GravCast is the best. It's the best model, this variable. So I showed you
two variables. So we can conclude that GravCast is better than forecast net. We will focus on that
from now on. And then what does it look like for other variables, like wind, humidity? So here,
I present you graphics, it's vertical profiles. Here, in the axis of the Y of the vertical coordinate,
we start from the surface of 1000 hectopascals to 50 hectopascals. And
so the clean curve is the quarter tip of the error, according to the variable.
And the tight curve is the bias. And the different variables are the following. Here on the top
on the left, we have the meridian wind component, the wind module here on the right.
Can you do this for us in the next two minutes?
Yes, that's it. Yes, I'm going to go faster. So what I wanted to show you, here we have the
GravCast configuration, so at 13 levels, 37 levels. So what I wanted to show you is that
the red curve is always better than the blue curve. So the model has all the variables,
all the issues, not just issues, but all the variables, all the levels. We see that GravCast is
better. So are these two models not different resolutions? Is the fact that GravCast is a
bigger resolution? Is it the advantage? Is it the advantage compared to the operational model?
So what we did is that we did the same verification that I presented to you earlier,
but we filtered it. We filtered it, we removed all the scales that were smaller than 1000
kilometers, and we kept all those that were 2000 kilometers. So I have two graphics to present
here. So on the left, it's the same graphic that I presented to you earlier, non-filtered.
And on the right, it's the filtered previews. So that's on the winter means, it's not on the
full year. So we see that even if you filter, GravCast is better. So that's going to bring
to the work of SpectreNodging from Syed, who is going to present in the next presentation.
So for the summer, it's a little less spectacular, but we still see that GravCast is
still giving good information. So what are the future activities to do more
verification? We even have an internal website that allows us to visualize these previews
every day. And I invite you to have the next seminar of Syed about SpectreNodging,
which is a very interesting approach to integrate physical models and DI models.
And Christopher Subick's work on entering GravCast with the operational analysis that we
have done internally, so that we can better adapt to our model. Thank you.
Thank you, Hervé. I have a question for you. There, you made the internal models run,
you installed them to do the verifications, but there are models that exist more and more.
There are almost two days out. Is there a way to have verifications
against observations like that, in a standardized way, or each time,
you will have to install the models and the scoring themselves?
Well, listen, is that the software that we use to do the verifications against observations
work very well only locally? So it is difficult to publish that outside.
On the other hand, turning these models, if it is not done very, very easily,
I worked for three months in a row just to start these models. So I imagine that
each model will have its own peculiarities. So it's not obvious, it's not as
plug-and-play as we believe it. There is still a lot of work to do. So if there are
every two weeks, it will be difficult to do this same evaluation to follow the course.
Thank you. I don't know if there is another question. Maybe a word for advanced systems.
If you could reset the Q&A on EventMobile, because I still see the questions that I've been
asked to Christopher Subic. So it's hard to see which one.
I didn't evaluate that. That's more so my colleague,
Marc Verville, who did some verifications on the surface. And I don't have any memories there.
I'm sorry, I don't remember the results. My focus was really on the verifications
in addition. But of course, having a good resolution in advance will certainly not affect the class.
Perfect. Thank you very much, Verville.
Thank you.
So we're going to continue. This time we're inviting Syed Zahid Kussein, who's a research
scientist at ECCC. He will be presenting leveraging data-driven weather emulators to
guide physics-based numerical weather prediction models, a fusion of forecasting paradigms.
So without further ado, Syed, this is your turn.
Thank you, Rand. Hello, everyone. In my presentation today, I will be talking about
how we can leverage the strengths of data-driven weather models to improve predictions from
NWP models. This is a work that we have been doing recently. And here is a list of my
principal collaborators and the others who have contributed to this research.
As we know, the current state of the earth for operational weather forecasting is based on
physics-based NWP models. However, we have seen these presentations earlier today from the plenary
to the previous two presentations that we have recently seen the emergence of new
data-driven models for predicting weather. And most of these models are using some form of
deep neural network to emulate the training data, and the training data generally is
RFI re-analysis. So we also can call them artificial intelligence-based weather emulators.
And recently, they have started to gain prominence and started to also challenge the existing
forecasting paradigm. Because as we have seen in the previous presentation from my colleague,
Ervig, that these data-driven models can produce forecast orders of magnitude faster with minimal
computational resources compared to the traditional NWP models. And also,
they can be highly competitive against state-of-the-art NWP models in terms of their accuracy.
However, despite their strengths, strictly when I'm talking in the deterministic sense for these
AI-based models, they have their limitations also. And one of the most widely known limitations
is considerable smoothing of fine scales, particularly for longer lead times. Also,
they only offer a limited range of focused fields. And improving nominal resolution of these AI
models are not straightforward. They can be quite challenging. So the objective of our research
was to see if we can leverage the strengths of these AI-based models to improve the predictability
of an NWP model. And for that, we chose or selected like the GEM model, which is used
operationally as the NWP model, operationally by Environment Canada, and the GraphCast model
from Google DeepMind as the AI-based model. And the nominal grid resolutions of GraphCast
and the GEM-based Global Deterministic Prediction System, or GDPS, are approximately 25 kilometers
and 15 kilometers, respectively. And in the previous presentation, AirVig has shown that
the GraphCast actually poses more skilled large scales compared to our GDPS. But if we want to
leverage the information from GraphCast, we need to know about the effective resolution of GraphCast
so that we can see like what are the scales that we can really utilize for improving NWP model.
And in order to do that, we look at the variance ratio of GDPS and GraphCast with respect to our
own CMC analysis. And before I talk about anything else, I must emphasize on the fact that the
version of GraphCast that we are using has not been through any fine-tuning. So it has been
trained by Google on emulating error-5 analysis by training with error-5 data. And we are using
that GraphCast model, but initializing it with our own analysis. And in these figures in this slide,
I'm showing the transient component of variance ratio for 500 hectopascal kinetic energy. On the
left, I have for lead time 24 hours. And on the right, I have for 120 hours. In blue, I have GraphCast
and in red, sorry, in blue, I have GDPS. And in red, I have GraphCast. And we can see by looking at
the variance ratio of GDPS, the blue lines in both 24-hour and 120-hour cases, that the
variance ratio is close to one. So that means its effective resolution is not changing with respect
to lead times. However, what we see with GraphCast, first of all, at 24-hour lead time, we see the
scales as large as 1500 kilometers are smoothed out to some extent. And we see considerable
smoothing for scales smaller than that. And then we see when we go to 120-hour lead time,
thus the scales that are getting smoothed out actually increases and it affects scales as
large as 2750. So we know that large scales in GraphCast are better, but at the same time we see
that scales below 2750 for longer lead times are problematic because of the reduced variance ratio.
Now, the question is how we can really use or leverage this good large-scale information
from GraphCast. And one way to do that would be to use spectral nudging, large-scale spectral nudging,
and this is a very widely used idea in the field of regional climate modeling and hindcasting.
Our expectation is if we nudge our gem predictions towards the large scales of GraphCast, we can
improve the quality of prediction of gem. At the same time, we should be able to address the
fine-scale smoothing in GraphCast while we will be able to generate the full set of focus fields
that we are currently having access to through gem. The concept of nudging is quite simple,
as illustrated by this equation here. So here, F is the solution of our gem model
after the dynamic step. And this term highlighted in purple color actually corresponds to the
nudging increment. So we add the nudging increment to the model solution after the dynamic step to
get the nudged solution, which is then fed to the physics, and then it completes the complete model
time step, and then it fits back to the next dynamic step. And this is how it continues.
And if we look at this nudging increment term, we have this subscript ls, which actually implies
the large scale. So we are only considering the large scales when we are applying the nudging.
And this omega is a relaxation parameter. And if we break down this equation, we can see basically
what we have is a weighted average of the large scales coming from GraphCast and our model,
whereas the fine scales are remaining intact that is being credited by the model.
So this is how we can leverage the large scale accuracy of GraphCast while
allowing our model to freely evolve the small scales. And the scale separation between large
and small scales, we are doing that by decomposing the nudging increments in the spectral space,
although we are applying the increment, the nudging increment in the grid point space,
but we are decomposing it in the spectral space, and hence we call it spectral nudging.
How we optimize the spectral nudging configuration to support our objectives for this study
is a computationally demanding task. And in a sense, it is still ongoing. I mean,
we are still sort of fine tuning the configuration, but at present, the most optimal configuration
that we have has these following features. We are only applying nudging to horizontal wind
and temperature, and we are not nudging the stratosphere and the boundary layer for different
reasons. And we are nudging scales that are larger than 2750 for obvious reasons, that should be
obvious from the variance ratio comparison. And we have 12 hours as the nudging relaxation scale,
and we apply nudging at every time step. With that, I'll be going to some of the
results of verification that we will try to prove that this approach actually helps to
improve the predictability of GEM. We ran a series of experiments for winter and summer of 2022.
So we have the control experiments where there is no nudging, the control GDPS,
and then we have the GDPS with spectral nudging. So control would be in the next few slides,
all the results control would be shown in blue color, and the results from
spectral nudging with GDPS would be shown with red. So the first scores that I want to show
are verification against radius and observations. It's similar to what Elvig has shown in the previous
presentation. Just to repeat, we have in these figures, we have different variables,
zonal wind modulus, geopotential high temperature, and dew point depression.
In all these figures, we have the dashed lines representing the bias and solid lines
representing standard deviation of error. And we have the shades of red and blue that represent
the statistically significant improvements corresponding to the color of the experiment.
So if we see red color, then it implies that we have statistically significant improvement
with the spectral nudging configuration. And if it is blue, then we have deterioration
with spectral nudging. What we can see from these figures, this is for winter over the globe,
that beyond five days and up to 10 days, we can see that there is tremendous improvement
in the scores for the standard deviation of error, which is mostly...
Excuse me, Sayed, you have two minutes left.
Okay, okay, I'll try to be faster. For the bias, the differences are mixed,
but the bias is less difficult to improve. Standard division is more difficult. So we
see tremendous improvement, up to 10% improvement in the RMSE for longer lead times. In summer,
though, we see modest improvement. Still, we see statistically significant improvement
for the standard deviation of error. And when you look at the animal liquid relation
coefficient, this is for the 500 hectropascal geopotential. We see improvements both in winter
and summer. In winter, in terms of the gain in predictability, it's around 18 hours. In summer,
it's about eight hours improvement. And they're both statistically significant.
And last result is about tropical cyclone position error. This is another thing that is very
difficult to improve. And our model tends to have... Sorry, for the along track position,
we tend to lag the model, and we can improve that lagging with spectral nudging towards
graphcast. And for the cross-track position error, our cyclones tend to veer to the right from the
observed trajectory. And we are able to considerably improve that aspect of
position of tropical cyclone also. And finally, just this figure, I'm showing the temperature
anomaly at 850 hectropascal for a lead time of 240 hours for a single case. We have the
GDPS control on the left, graphcast in the middle, and GDPS with spectral nudging on the right.
And we can see that graphcast barely has any fine scale. And both GDPS control and with spectral
nudging, they both have comparable fine scale information. And we get this improvement by
nudging as well. So to summarize, we developed a hybrid NWP system that fuses NWP models with
AI models to spectral nudging. And by leveraging more accurate large scale predictions from
graphcast, we are able to significantly improve our prediction scale with GDPS.
And I want to stress that the improvement that we have is roughly equivalent to one solid innovation
cycle, which is about four years of work involving many scientists from across the
Meteorological Research Division of Environment Canada. And we were able to achieve something
comparable in a matter of four to five months. And also, I want to stress that this is based
on work that uses a graphcast model that has not been fine tuned. And with fine tuning,
graphcast to emulate our own CMC analysis, which is a work in progress by colleagues
Christopher, we hope that we could improve further. And currently, with this configuration,
we have about 25% increase in the computational cost. But this is without any optimization.
And with some optimization, we hope that we will be able to reduce it to something like
less than 15% in the near future. So with that, I will end my presentation.
Thank you, Sayada. It's a very impressive conclusion. I have one question here.
Nudging is done as GEM integration goes or at posteriori?
No, it's online. So what happens is, as I said, you solve the dynamic step, because I mean,
we have the dynamic sub step and the physics sub step, and then we do the coupling in the split
mode. So we solve the dynamic sub step, we have a solution from dynamics, which is an intermediate
solution, then we update that by nudging, and then we feed that updated solution to the physics,
and then we get the complete solution of the model time step. And then the next time dynamics
uses that solution to predict the next dynamic step. So it's not a posteriori. It's,
I mean, an online update. Last quick one. How does GVPSSN compare with graphcast?
Oh, that was already shown by Ervig in the previous presentation, that GVPS, I mean,
if you talk about control GVPS versus graphcast, that presentation of Ervig should allow you to see
like it actually improves. What I am missing in my figures, because this is still a work in progress,
I mean, I mean, in our paper that we expect to submit soon, which we will be adding the graphcast
also focus in this, for example, in these figures to show control GVPS, GVPS with graphcast,
and graphcast itself, like how much of the improvement is coming from graphcast, but it is
definitely coming from graphcast as Ervig's presentation showed that the large scales in
graphcast are much better. Yes, I had looking forward to read the print that will be very popular,
I'm sure. Thank you. Merci. Anne, at what? Yes, so we're moving. You might have noticed in the
schedule that Christian Saed was originally supposed to present this, but I want to thank Madalina
Sousseld to have accepted to present and prepare this presentation for us. So she's a climate
extreme specialist at Environment Canada. She's going to be presenting on the development of
artificial intelligence downscaling applications for medium range forecasts of weather elements
at CCMEP. So without further ado, over to you, Madalina. Okay, thank you. I hope you're hearing
me well. We are. Okay, great. So I am here today to present to you some development of artificial
intelligence downscaling techniques that we're doing at CCMAP in collaboration with IBM Research,
and I would like to acknowledge my co-authors that are listed here, both from ECCC and IBM Research.
So first, I would like to say that the vision for this project is actually to help us offer
seamless day one to day 10 public forecast products as part of the transformation of the
meteorological service of Canada. So in order to do this, we need to bridge the gap between
high-resolution, short-term forecasts and medium-range lower-resolution forecasts.
And the reason why we want to do this is because we know that insufficient horizontal
resolution causes forecast errors and especially biases. I am showing here a graph of bias,
a comparison between a low-resolution model in red and the high-resolution model in blue. And
what we are saying here, the closest we are to zero, the less bias we have, and we really see
that increasing horizontal resolution is improving this bias. Usually the way that we do that is by
doing dynamical downscaling, so running numerical weather prediction models, but this is very
computationally expensive, which makes it limiting. A partial solution to this is doing
statistical downscaling, which means using past data and deriving statistical relationships from
this past data, such that we apply these relationships on the course input and we obtain
high-resolution output. These type of solutions are limited by the fact that we have to impose
certain non-mathematical relationships in the data. So now we have a data-driven alternative,
which is to apply artificial intelligence techniques to do downscaling. And what this
does is that it allows to derive complex relationships in the data that we provide to
these models. So our objective is to develop artificial intelligence downscaling techniques
to downscale weather elements from medium-range forecast to the kilometric scale. And we hope
that this will help both deterministic and ensemble forecasting. So just to briefly present
our project, so this is a collaboration that started this year between CCMAP and IBM Research.
And here at Environment Canada, we have the meteorological expertise and we definitely have
subjects, problems that could really benefit from these artificial intelligence solutions,
but we don't necessarily have the artificial intelligence expertise, which is where IBM Research
comes into play. And collaborating with them, they are really experts in this field,
will allow us to advance much faster. And the expected outcome from this collaboration,
that for now it's only meant to be on one year, is to develop low-cost and efficient alternatives
to the computationally expensive dynamical models. And the other thing that we want to get
from this collaboration is we want to learn from IBM Research. So at the end of this project,
we want to enhance our capability at Environment Canada to be carrying out this type of research
and development and eventual operational implementation of AI downscaling techniques.
So the specific goals of our projects are as follows. So we are interested in downscaling
weather elements. And by this I mean surface winds, temperature and precipitation in the first stage,
a forecast from the GDPS, which is our global deterministic prediction system shown here,
and runs at a resolution of 15 kilometers to the resolution of the HRDPS, which is our high
resolution deterministic prediction system that is run for 48 hours for now at a 2.5 kilometer
resolution. And in this project, we are taking a two-step approach. So in the first step, we will
be looking at the baseline model that is based on generative adversarial networks. And in the
second stage of the project, we will be taking advantage of foundation models and tuning them
for our application. The training data for the models will be forecasting data from the GDPS
as the low resolution data set and from the HRDPS as the high resolution data set. And it is very
important for our operational needs that the downscale products are available on the HRDPS grid.
For now, in this talk, I will only focus on the baseline model as this is a collaboration that
just started and we haven't gotten yet to the second part. So just briefly, what is a generative
adversarial network? And the generative adversarial networks consist of two networks, a generator,
two neural networks, a generator and discriminator. And the way that they work is that they are trained
so the generator pretty much generates images that look as much as possible as the real data.
And the goal of the generator is to just fool the discriminator. On the other hand,
we have the discriminator that receives both real data, which in our case is HRDPS data,
and generated data and has to decide whether this generated data is real or fake. And in our case,
we use the ANAO et al. 2023 implementation of a vassar steam GAN.
I will show you some preliminary results for our project. So these preliminary results are
based on the WGAN from ANAO et al. without any covariates. So what this means is that the only
data that goes for now in this model is zonal and meridional 10 meter wind components. And this
is the data that we are trying to obtain. So the low resolution data comes from GDPS,
high resolution comes from the HRDPS. And so far, we are training the model with one year of data
that is divided as follows. 75% of the data is used for training. It's about 7,000 forecasts.
12.5% is used for validation. And this validation data is used during the training of the model.
So it's used to stop overfitting the model. And then 12.5% of the data is used for testing.
So once we have a tuned model, we will use this data set, about 1200 forecasts,
to be seeing how well this model functions. Because we are using forecasting data,
it is a bit difficult because, as you know, forecasts, there are increases with forecast
leak time. And so we might have too much divergence between the HRDPS, the high resolution data set,
and our low resolution data set. On the other hand, the first six hours of the forecast are
affected by the spin-up time of the model. So we have decided for now to go with forecast hour
6 to 18. And we are using them from the 00 and the 12 UTC initialization of both models. So
in this way, we are covering the entire day. So our challenge is covering the entire HRDPS domain.
I am showing here the HRDPS domain. It is on the order of 2500 by 1200 grid points. So it's a
large domain that's spanning the width of Canada. And it isn't really possible, or at least we don't
think it's possible so far, to be training directly on such a large domain. We do not have the
computational resources to do that. And we also aren't sure exactly how much data you would need
to be able to train on such a model. The now and all implementation, in that implementation,
the training is done on 16 by 16 pixel low resolution patches and 128 by 128 pixel high
resolution patches. So what we will do is we will adjust to that type of training. And in order to do
that, the strategy that we have adopted is to just select random patches from one forecast from our
HRDPS data set. We will re-read the GDPS data set on the HRDPS domain and then
coarse-crain it to go back to its resolution. And in this way, we are going to end up with
this type of patches that we'll use to do the training. So here is the high resolution data
patch and the corresponding GDPS low resolution data patch. So at each epoch, we are using between
300 and 700 random patches. And the model that we are showing today has been trained on 17,370
epochs. So this took about 149 hours to train on one GPU and we have done more than two passes
through the entire data set. So once you have a trained model, you can perform inference with
the GDPS data. And that inference will also be done on 16 by 16 pixel patches. So here I have a
GDPS input. We perform inference and like this, we obtain the downscale forecast, the downscale
U and V fields. And just as a comparison, we have here the HRDPS forecast. So the first things
that we can say is that we are definitely downscaling. So we are obtaining information at
the small scales. It isn't as much as the HRDPS is showing. But as I was mentioning before, one
problem with a low resolution is the fact that you have biases and we are definitely achieving
some sort of a bias correction. Now, of course, you have to parse the entire HRDPS domain. One way
to do that would be to sequentially process 128 by 128 patches, but that would result into artifacts
at the borders. So the strategy that we have adopted instead is to do some overlapping and then
to take the median of the ensemble of overlaps. And in this way, we managed to patch and obtain
this figure over the entire domain. We have performed validation of our model. So we have
used the test data, the 1200 forecast from the GDPS. And here I am showing the root mean square error
for the U wind component and for the V wind component. And on the bottom, I'm showing the mean
absolute error. And these are the metrics that are computed between the downscale
GDPS and the HRDPS corresponding verification. And we are comparing it with some baselines that
can be used for interpolation. So I'm showing in orange you have bilinear interpolation and in
green you have nearest neighbor interpolation. So as we are seeing in all the metrics, the downscale
is showing better results is performing better than the other types of interpolation for our
test data set. We have also done a power spectrum analysis in order to really quantify how much
detail we are getting at the small scales. And here I'm showing the radially average
power spectral density between HRDPS in blue and the downscale forecast in orange. And these
power spectra are average over the test data set as well. What we are seeing is that indeed that
small scales, we are still not getting enough power. So we do not get enough detail.
Madelina, if you could lend this in one or two minutes.
Sounds good. So of course, we are training so far with one year of data. And
it would be very interesting to see how much more detail we can obtain by training further.
I'm just showing also an integrated measure of the difference in the power spectra. And what you
are seeing here is that compared to the bilinear and the nearest neighbor interpolation, the downscaled
AI downscaling performs much, much better. So it's able to recover way more structure at the
small scales. So the next steps with our forecast, with our project, the WGAN needs further development
and testing. So we are planning on testing with more data. A very important thing that we are
working on right now is to add other covariates. So as I said for now, we are only using windfields,
but we are adding topography, surface pressure, and we are thinking about what other covariates
may be such escape to add to our model. And once we have a baseline that we are satisfied with,
the important part comes, and that is doing a thorough meteorological verification of the
downscaled forecast. Because like I said, we have operational goals and we really want to see
how this forecast respond to our needs. Finally, once we finish this first step of the project,
we are planning on moving to the second more ambitious part, which is to develop a large
GI model that is based on a pre-trained foundation model that we will be fine-tuning in order to
obtain downscaled forecasts. And this is where, again, collaborating with IBM Research is extremely
important as they have very much experience in these foundation models. And we are hoping to
advance at least as fast as now. So this is it for me. Thank you very much.
I saw how you trained especially the model, but I was surprised to see that you only used
one year of data to do that. Is there a reason behind that? Because it seems to me that it's not
a lot of data. Yes, well, like I said, we do end up having a lot of forecast samples. We are
training on 128 by 128 pixel patches out of a very large domain. So it ends up being a lot of data,
but it is not finalized. So here we are in a developing mode. We are trying to develop the
model and make sure it's working properly. And this is just the first step. We are definitely
planning on adding more data and seeing how we can improve.
Right on time. Thank you, Madalina. So now we're going over to Reynel Sospedra Alfonso,
who is also a research scientist at Environment and Climate Change Canada. He will be presenting
on deep learning-based bias adjustments of Arctic SEIS forecasts from version three of the Canadian
seasonal to inter-annual prediction systems, also known as CANSTEP version three. So, Reynel,
the mic is all yours. Thank you. Okay, thank you, Anne. Can you hear me well?
Yes, we do. Your presentation is not full screen, though.
It's not. No, it's not. All right. How about now? Yes, it is. Perfect. Thank you so much,
Miguel. Thank you, yes, to the organizer for this opportunity. My name is Reynel Sospedra Alfonso.
I'm a research scientist at the Canadian Center for Climate Modeling and Analysis,
based in Victoria. And I want to start by acknowledging the contributions or the work of
colleagues at CCMA, which made this type of project possible. I list some of them down here.
And I also want to acknowledge the co-authors of this presentation or this work, Joseph Martin,
Michael Simon, and especially Parz Aguilla, who has been key for this project going forward.
He has taken the time to do the implementation, training, and testing of the models we have been
looking at. And I want to mention that this is part, what I'm going to talk about here is part of a
bigger project that we are trying to pursue at CCMA, which is the use or applications of machine
learning methods, in particular deep learning, to post-process our seasonal to decadal forecast.
So for this talk, I will talk in particular about the post-processing or seasonal forecast of CIS.
The seasonal forecast, as you may know, CANSIPS is the Canadian seasonal
and internal prediction system, which provides the Environment and Climate Change Canada's
operational probabilistic seasonal forecast, both on national and global.
And CANSIPS first appeared or was first viewed in 2011 as a two-model forecasting system.
It has evolved since and now we are in 2024 with a new version of CANSIPS B3,
which actually will be launched next month. And so here what I'm going to talk about is
the forecast that we produce with CANIASM-5, which is a new model that now we'll be using CANSIPS.
And CANIASM-5 is an air system model that is produced at CCMA and now will be then, as I said,
used for our seasonal forecast. CANIASM-5 air system models, so it couples the atmosphere,
the ocean, CIS component, land, and also biochemistry, both on land and the ocean.
What we do, we take our climate model, we initialize the climate model following the
indications that you see here on the right. So when we initialize the forecast, we take,
we notch the model towards reanalysis and then we launch those forecasts in time.
The version of CANIASM-5 that I'm going to be talking about is actually an optimal
bias-corrected version, which follows the word by Isinoq and Karim, which goes online
bias optimization to the model. Now the work that I'm going to be presenting to you
deals with the post-processing of those forecasts. So here what you see is a representation of those
forecasts in black is the observations that we are verifying observations, the monthly values,
and then what you see is the representation of those forecasts, which are initialized
at the start of every month during the handcast period. So we'll be looking at handcasts from
1980 to 2021. We have, for each month in that time, we launch an ensemble of forecasts of
10 members, which run for 12 months. The variable of interest for us here is CIS concentration,
which is simply the fraction of CIS, the fraction of the grid cells that is covered by CIS,
and this is what we want to adjust.
Well, we do that looking at the ensemble mean forecast. So the adjustment is not done to the
ensemble itself, it's done to the ensemble mean, and the question is why do we have to do that?
I mean, after all, we are even doing an online bias optimization or bias correction of my model.
So still we do need to adjust those forecasts because, as we know, we have several sources of
error, structural errors, errors due to initialization and so forth, and typically this is done by doing
some climatological bias correction to those forecasts. Now here, just to give you an example,
I'm showing you the September CIS concentration over the time period 2006 to 2020. On the left,
these are the very fine observations that we use, which comes from NOAA data products.
And here, again, this is the CIS concentration, which has value from 0 to 1, and we see on the
right, the raw forecast as it comes out of the model. So this is the output directly coming
from our forecast. And what we see is that there is definitely some bias that we notice,
particularly in the center Arctic, where we have a much lower CIS concentration relative to
observations. Now on the right to that, we see the bias adjusted forecast in which we compensate or
we subtract the bias, compute it on a previous time, and then what we see is that we adjust somehow
that forecast by doing this simple bias correction. Now still, even after doing a bias correction,
we see that there are some differences between the observed CIS concentration and the one that
is bias adjusted. So that tells that we need to do something else in order to actually improve
our forecast. And for that, and this is now the project that we are working on, is to use this
machine learning or deep learning method to improve even further those forecasts.
The method that I'm going to use here, and I'm going to talk a little bit more about that in
the few slides, is the unit. Probably most of the people in the audience are familiar with units.
As I said, I will talk a little bit more about that. But here, I'm just showing you some
very good results in which you see how the unit is able to reproduce the
special pattern a lot better than what we can do with a simple bias correction.
I should have said that this is a forecast done at two monthly time. So this is the forecast two
months after the forecast is initialized, where the biases are, you can see, are particularly strong.
Now, unit, this is the tool of choice. What we have here is a fully connected
network or a fully convolutional network that has a downscaling, down sampling encoder
followed by up sampling, up sampling decoder. This is a classical unit. What we see is that
the future maps are reduced in size. So we see that the resolution is reduced by its health,
and the channels are increased by two, and this allows us to have a better representation
or capacity of the network while preserving some information of the image that we input.
So to be clear, what we input here is our raw forecast, which is denoted by the YMN,
which thanks to N would be the resolution of my forecast, which is a function of the initial month
and the target month and the time relative to the firm month in the data that we are inputting.
So the input is made of six channels, one channel that is the actual
variable that we want to, or the actual map that we want to correct, plus five temporal features
that takes into account, again, the initial month that we're looking at, the target month,
and this temporal information relative to the initial time. So in other words,
how many years and how many months from the starting of your data.
You input that into the network, and then the output is hopefully an adjusted forecast,
which then correct for those biases that I mentioned earlier.
Moving a little bit forward here, let me just be a little bit more precise on the kind of task
that we are doing. This is just a specific example, which hopefully will clarify how we do this.
So in this case, let's say that we want to adjust the March CIS concentration forecast,
which is initializing February of a year Y. So essentially it will be this red dot
denoted here on the figure. So again, what we want to do is to post-process this forecast.
We leverage the forecast that are produced with our climate model. We do not do prediction,
we just do that kind of bias adjustment or post-processing of the predictions that we get
from our climate model. So for this specific task to correct that March CIS, we train on the data
that is available for the years before the test year that we have. So in this case,
I'm denoting that here for this in this shadow region. So we take all the pairs of
forecasts and observations for all the times and all target months, and then we train our
network on that data, and then we will do that iteratively for every test year that we want to
make the adjustment. So what I'm going to do now is to show you some of the results that we have.
I should say that this is very much a working progress and that is
different avenues that we are working on, but we have some results that I want to
to share with you. For instance, what we see here is the CIS concentration bias at the zero
monthly average over the 2006 and 2020 time period, which is the test years that we have
for our analysis. At the top, we have the bias for the March CIS concentration, which is at the time
of maximum CIS extent. And at the bottom, we have the results or the bias for the September CIS
concentration, which is at the time of a minimum CIS extent. On the left, we see here what happened
with the raw forecast. We see the biases happening in several regions, which is actually substantial.
Here, the bias is given in percent. For the bias adjusted case, which is this simple bias
correction that I'm using here as a benchmark, we see that we do improve relative to the raw
forecast, but there are still some biases that are apparent, particularly during the CIS minimum.
And then on the right is the results that we get with the unit, and we see that the bias
are likely removed. And now this is for the zero monthly, so this is soon after we initialize
our forecast. Now, two months ahead in our forecast, of course, the biases are increased.
Excuse me, two minutes. Two minutes, thank you. All right, so here is the case in which we have
two monthly forecasts, and we see that the bias, of course, are increased, but still we are able
to manage those biases with the unit, giving a better representation of the CIS edges.
Another measure that we look at here is the integrated ISH error, which is essentially the
area, the integrated area that we have, in which both forecast and observations disagree
in the concentration with a threshold of 15 percent. So, if both have more than 50 percent
concentration or less than 50 percent, this binary error will be zero. If they are different,
then the binary error will be one, and those grid cells will contribute to this area error.
So, this is at the top here, we see a heat map in which we have our target month in the X axis,
and on the Y we have the lead month. And the bottom message here is that the blue are bad,
the red are good, meaning there is less error, and the unit beats both the row, of course,
and the bias I used in forecast. Now, here is just an integration over lead month,
just to give a more clear picture of how the unit outperforms the alternative.
Now, I know that I don't have much time, so I will not go into the details of these results,
but at least to give you an idea of what is happening. In this case, we are looking at the
CIS area, same time of floods. Again, red means that we have a better room mean square error,
which is the measure that we are using here for the CIS area. Blue means that the errors are
increased, and in this particular case, we see that the unit is slightly better than the bias
corrected one. However, if we look at CIS extent, which is now the area in which we count for all
those good cells with concentration greater than 50%, we see that unit largely outperforms
this bias I used in metal. Finally, I want just to mention that here we have seen different
measures of the skill in which we outperform both the benchmark and the raw forecast,
but there is still some issues in terms of the representation of the temporal dependence
of our adjusted forecast. But unlike for this slide here, just to mention that we still have some
work to do to be able to better represent the temporal dependence of those forecasts.
Because I don't have much more time, I will finish with this slide, which is some final remarks.
I will leave it there for you, and I will be happy to take any questions. Thank you.
Merci Renel. I saw that there seems to be to have a seasonal pattern in your verification.
That leads me to the questions. Are there any biases that are harder to adjust with this method?
The answer for that, yes, and this is anality that we see has to do in part with how the eyes
behave. We have what is known as this predictability barrier after spring, which makes it difficult
to do a good prediction of the sea eyes. This is something that perhaps we can see in all those
metrics. Actually, perhaps I'm going to go to this one. So this barrier, which reduces the skill
that we have in our predictions, can be seen here around the month of June to October, looking
here at target month. And so that is something within the raw forecast, in person, the bias
corrected one, but also in the unit. So we do improve on those months, but still there is
some skill that is missing there, which is part of the natural processes in the sea eyes formation
and melting, which then translate into the skill that we see in our adjusted forecast.
Thank you. One last question is coming from the online Q&A. Would training the model on all
months of the year versus only certain months affect its ability to improve the forecast? For
example, forecasts of sea eyes in spring are known to perform poorly. So if you were to exclude
these months, would this improve the bias adjustment? Yeah, that's a good question.
Actually, when I was, I mean, it's too bad that I'm rushing through all these slides, but one of
the things that I wanted to mention at this particular slide is that we do the training.
You're looking at previous years in my forecast, but we are missing some of the information of
more recent months that can also contribute to the forecast. Like looking here, for instance,
we do not use information from January of this specific year. Now, one thing that we are exploring
is to look at training based on lead times or a specific month. I think that that's the
idea of the question, which we try to train our model specifically to the lead times in which
we have the biases that we want to correct and the kind of information specific to the
seasonality that we want also to correct. So those are things that we are exploring,
but so far, this is what we have.
Yes, a big thank you to all the presenters, all the attendees as well. And as several mentioned
before, there is a second session coming up in about 20 minutes, not even. So we'd love to see
you back here a lot more to see and to hear from people outside ECCC as well. So please do join us.
Thanks.

So basically, no matter which one of these explanations
of the Fermi paradox is true,
either it's irrelevant that we are about
to invent a paperclip maximizing AI
because we're about to be destroyed by something else
or in a simulation,
or we're definitely not about
to invent a paperclip maximizing AI
either because we're really far away from the technology
or because almost nobody does that.
That's just not the way AI works.
I am so convinced by this argument that it is actually,
I used to believe it was like a 20% chance we all died
because of an AI or maybe even as high as a 50% chance,
but it was a variable risk if I've explained in other videos.
I now think there's almost a 0% chance.
A 0% chance assuming we are not about to be killed
by a grabby AI somebody else invented.
Now, it does bring up something interesting.
If the reason we're not running into aliens
is because infinite power and material generation
is just incredibly easy
and there's a terminal utility convergence function,
then what are the aliens doing in the universe?
Would you like to know more?
Hi, Malcolm.
How are you doing, my friend?
So today we are going to do an episode,
a bit of a preamble for an already filmed interview.
So we did two interviews with Robin Hansen
and in one of them we discuss this theory.
However, I didn't want to off rail the interview too much
going into this theory,
but I really wanted to nerd out on it with him
because he is the person
who invented the grabby aliens hypothesis solution
to the Fermi Paradox.
So I hadn't heard about grabby aliens before,
so I'm glad we're doing this.
This is great.
Yes, so we will use this episode
to talk about the Fermi Paradox,
the grabby alien hypothesis
and how the grabby alien hypothesis can be used
through controlling one of the variables,
i.e. the assumption that we are about to invent
a paperclip maximizer AI that ends up fooming
and killing us all
because that would be a grabby alien definition only.
If you collapse that variable within the equation to today,
then you can back calculate the probability
of creating a paperclip maximizing AI.
And spoiler alert, the probability is almost zero.
It basically means it is almost statistically impossible
that we are about to create a paperclip maximizing AI
unless with the two big caveats here,
something in the universe that would make it irrelevant
whether or not we created a paperclip maximizing AI
is hiding other aliens from us
or we are in a simulation,
which also would make it irrelevant
that we're about to create a paperclip maximizing AI
or there is some filter to advance the life developing
on a planet that we have already passed through
that we don't realize that we have passed through.
So those are the only ways that this isn't the case,
but let's go into it because it is really easy.
I just realized that some definitions may help here.
We'll get into the grabby alien hypothesis in a second,
but the concept of a paperclip maximizing AI
is the concept of an AI
that is just trying to maximize some simplistic function.
So in the concept as it's laid out
as a paperclip maximizer,
it would be just make maximum number of paperclips
and then it just keeps making paperclips
and it starts turning the earth into paperclips
and it starts turning people into paperclips.
Now, realistically,
if we were to have a paperclip maximizing AI,
it would probably look something more like,
you know, somebody says process this image
and it just keeps processing the image
to like an insane degree
because it was never told when to stop processing the image
and it just turns all the world
into energy to process an image
or something else silly like that.
This concept is important to address
because there are many people
who at least pass themselves off as intelligent,
who believe that we are about
to create a paperclip maximizing AI,
that AI is about to, as they call FOOM,
which I mentioned earlier here,
which just means rise in intelligence
astronomically quickly,
like double as intelligence every 15 minutes or something
and then wipe out our species
and after that begin to consume all matter in the universe.
So the Fermi Paradox is basically the question
of why haven't we seen extraterrestrial life yet?
You know, like we kind of should have seen it already.
It's kind of really shocking that we haven't
and I would say that anyone's metaphysical understanding
of reality that doesn't take the Fermi Paradox
into account is deeply flawed
because based on our understanding of physics today,
our understanding of what our own species intends to do
in the next 1,000, 2,000 years,
our understanding of the filters our species has gone through.
So we know how hard it was for life to evolve on this planet.
And the answer is not very from what we can see.
So all of the time I mean by that.
A lot of people I'm really, really into,
it's one of my areas of deep nerdom theories
for how the first life could have evolved on Earth.
So there's a couple of things to note.
One isn't that important to this,
which is life evolved on Earth almost as soon as it could.
Now, a person may say, why isn't that this relevant?
That would seem to indicate that it is very easy
for life to evolve on a planet.
Well, and here we have to get into the gravity aliens theory.
You're dealing with the anthropic principle here, okay?
Can you define the anthropic principle?
Yeah, basically what it means is if you're asking like,
look, it looks like Earth is almost a perfect planet
for human life to evolve on it.
Like it had liquid water or everything like that, right?
Except human life wouldn't have evolved
without those things on a planet.
A different kind of life would have evolved
without those things on a planet.
Sure, the kind that doesn't need water, et cetera.
Right, so it's not really,
life on Earth didn't evolve almost as soon as it could.
Well, then it would have been too late
and another alien would have wiped out
and colonized this planet.
That is what the gravity alien theory would say,
so that this doesn't really change the probability
of this as a filter.
But what we do know about the evolution of life on Earth
is there are multiple ways that could have happened,
all of which could lead to an evolving,
you could either be dealing with like an RNA world,
you could be dealing with citrus acid cycle event,
you could be dealing with the clay hypothesis.
I actually think the clay hypothesis-
Do you wanna expound on any of these?
I've never heard of the citric acid hypothesis.
So for this stuff, I would say it's not really
that relevant to this conversation
and people can dig into these various theories
with people who have like done them more,
just like look up citric acid cycle hypothesis,
explanation for evolution of life on Earth
or clay hypothesis to evolution of life on Earth
or shallow pool hypothesis to evolution of life on Earth
or deep sea vent hypothesis to evolution of life on Earth.
The point being is it shouldn't actually,
like it shouldn't actually be that hard for life
to begin to evolve on a planet like this.
So, but why this is a relevant point, okay?
Okay.
And we actually sort of have to back out here
from the grabby aliens hypothesis.
So I'll explain what the grabby aliens hypothesis says
and why this is relevant to the Fermi paradox.
So the grabby, usually when you're dealing with solutions
to the Fermi paradox, what people will do is they'll say
that there's some unknown factor
that we don't know yet basically.
So a great example here would be the dark forest hypothesis.
Okay.
So the dark forest hypothesis is that there actually are aliens,
lots of aliens out there.
They just have the common sense to not be broadcasting
where they are and to be very good at hiding where they are
because they are all hostile to each other.
And that any other aliens like us who were stupid enough
to broadcast where they are,
they get snuffed out really quickly.
Sure, that makes sense.
That makes sense, yeah.
Okay.
If the dark forest hypothesis is the explanation
for why we are not seeing alien life out there,
it is somewhat irrelevant whether or not
we build a paperclip maximizing robot
because it means we're about to be snuffed out anyway,
given how loud we've been radio signal wise,
sending out ships, broadcasting about us,
sending out signals.
We have been a very loud species
and we could not defend against an interplanetary assault
by a space-fearing species.
I mean, in that case, you could actually argue
it would be much better if we developed AGI
as fast as possible,
because maybe it can defend us
even if we cannot defend ourselves.
Possibly, but that's the point there.
Beside the point, obviously.
It becomes irrelevant,
or they'll say we're in a simulation
and that's why you're not seeing stuff.
But again, that makes all of this beside the point.
What the gravity aliens does is it says,
no, actually we are just statistically
the first sentient species on the road
to becoming a gravity alien.
I'll explain what this means in just a second,
in this region of space.
And then it says, let's assume that's true.
It can use the fact that we haven't seen another species
out there, a gravity alien
that is rapidly expanding across planets,
to calculate how rarely these evolve on planets.
Okay?
Do you sort of understand how that could be the case?
Yeah.
Okay, so in the gravity aliens hypothesis,
when you run this calculation,
it turns out if that's why we haven't seen an alien yet,
what it means is there are very hard filters,
like something that makes it very low probability
that a potentially habitable planet ends up evolving
an alien that ends up spreading out like a gravity alien,
i.e. like a paperclip maximizer.
One of these really loud things that's just going,
planet, use the resources on the planet,
other planets, other planets, other planets.
And even if it has already finished doing that,
you've argued in other conversations we have had
that you would see the signs of that.
You would see the signs of the destroyed civilizations,
et cetera.
Yeah, a gravity alien,
which a paperclip maximizer is, so it's just easy.
If you're like, what does a gravity alien look like,
a paperclip maximizer?
That's just going planet to planet,
digesting the planets and then moving on.
Or a human empire expanding through the universe.
We colonize a planet, within a hundred years we get bored,
or some people go and they try colonizing a new planet.
Even with our existing technology on Earth right now,
like the speed of space travel right now,
if we were expanding that way,
we could conquer an entire galaxy
within about 300 million years.
So not that long when you're talking
about like the age of the universe.
This is a blindingly fast conquest.
So once an alien turns gravity, it moves really quickly.
And a lot of people think that we are
like space travel constrained, we're really not.
The reason why we don't space travel
with our existing technology is because of like radiation
damage to cells and the lifespan of a human.
But like if an AI was space traveling,
it could do pretty well with our existing technology
in terms of getting to other planets,
using them and then spreading.
Okay, anyway, so the gravity alien hypothesis says
that a species becomes gravity once
in every million galaxies.
Okay.
Now within every galaxy, there are around 400
or 500 million planets within the habitable zone.
So the habitable zone is a distance away from a star
where life could feasibly evolve.
Now this isn't saying that they have
the other precursors for life,
but what it means is that there are very frequently
in space, it turns out,
planets that are likely for life to evolve on them.
I would estimate like if I'm looking at everything
altogether, like the data that I've seen,
there's probably about 10 million planets per galaxy
that an intelligent species could evolve in.
And then if you're talking about,
well, you would only need this to happen,
you've got to multiply that by a million
for the one in a million galaxies
where a species is turning gravity.
Now this is where it becomes preposterous
that we are about to invent,
if this is why we haven't seen aliens yet,
why we are about to invent a gravity alien.
We can look throughout Earth's history,
as I did with sort of the first big filter,
the evolution of life or the appearance of life
first on this planet,
and say what's the probability of that event happening
in any given habitable planet?
For life appearing, my read is,
not only is it likely to appear,
it could appear like one of five different ways.
Even with the chemical composition of early Earth,
then you're looking at other things.
Okay, what about multicellular life?
What's the probability of that happening?
Actually really high, really high.
There's not like a big barrier
that's preventing it from evolving,
and it has many advantages over monocellular life,
so you're almost always gonna get it.
Intelligence, how rare is intelligence to evolve?
Not that rare, given that it has evolved multiple times
on our own planet in very different species.
I mean, you see intelligence in octopuses.
In whales. You see intelligence in crows.
Yeah. You see intelligence in humans,
and then you can say, okay, okay,
but like human-like intelligence, right?
Well, we already know from humans,
what a huge boost human-like intelligence gives a species.
The core advantage to human-like intelligence
is like if I'm a spider and I'm bad at making webs, right?
Then I die, and that is how spiders get better
at making webs intergenerationally.
As a human, I am able to essentially have
like different models of the universe fight in my head,
and presumably allow the best one to win.
And you don't have to die before you get better.
Yeah, you don't have to die to get better.
It is almost as important to evolution.
It is sort of like the second sexual selection.
So when sex first evolved,
the core utility of sex,
as opposed to just like cloning yourself, right?
Is it allowed for more DNA mixing,
which allowed for faster evolution?
Intelligence allows for the faster evolution
of the sort of operating system of our biology.
And so it's just such a huge advantage.
It's almost kind of shocking.
It didn't evolve faster.
For sure.
Given how close many species have come to it.
Now, actually surprising to a lot of people,
this is just like a side note here.
A lot of people think cephalopods
were close to evolving sentience.
So let's talk about cephalopods.
Why? Wait, I mean, cephalopods are all over,
like historic geology and all these things.
Yeah, yeah, yeah, yeah.
Cephalopods are like squids, octopus, stuff like that.
Like a lot of people point to how smart they are.
And they are smart.
They are like weirdly smart.
But they don't know why they're smart
because they don't know neuroscience.
So the reason why cephalopods are as smart as they are
is an axon.
An axon is what like information,
the action potential travels down.
Yeah, it's a little arm thing that you see on a neuron.
Yes, in a neuron, it's the little arm thing.
It's the cable.
You can think of it as, okay?
So to be an intelligent species,
you need really fast traveling action potentials.
Okay.
So the way that humans have really fast traveling
action potentials is something called myelination.
I'm not gonna go fully into it,
but it's a little physics trick
where they put like a layer of fat
intermittently around the axon.
And it causes the action potential to jump between.
It's like putting vegetable oil on your slip and slide.
Not exactly.
It's actually a really complicated trick of physics
that can't easily be explained,
except by like looking at,
I don't wanna get into it.
Point is, is we mammals have a special little trick
that allows for our action potentials
to travel very, very quickly.
And are you saying that cephalopods have this too?
No, they don't.
The way that they and any other species
that wants a fast traveling action potential before us,
the way that you increase the speed
that extra potentials traveled
was by increasing the diameter of the axon.
Oh, so they just have fat axons
whereas we have optimized axons.
Enormously fat.
In some cephalopods,
they're like a quarter centimeter in diameter.
Holy smokes.
Like, whoa, okay.
They could not get smarter than they are
without having some huge evolutionary leap
in the way that their nervous systems work.
This is why cephalopods,
despite being really smart
and probably being really smart for a long time
because they've been on earth for a really long time,
just could never make the evolutionary leap
to human type intelligence.
So-
Cause they don't have room to have even fatter axons.
Yeah, because as the axons got fatter,
the number of neurons they could have would get lower.
The density of the neurons-
Oh, of course.
Yeah, you've got limited space.
So unless they got much bigger brain cells.
Yeah, I guess you can have like giant, giant, giant-
I mean, yeah.
Undersea octopus.
Well, I mean, whatever.
Anyway, this is a huge tangent here.
But basically it looks like
if you're looking at the evolution of life on our earth,
if we have undergone other big like hard filters,
could be it's very rare for a species
to get nuclear weapons and not use them to destroy itself.
Cause it's so fun.
Right?
Could turn out that almost every species does that.
Or it could be that there's like one science experiment.
Like a lot of people thought that maybe trying to find
the Hadron particle was the big super collider.
Which is actually, like all species,
they get to a certain level of intelligence
and a certain level of curiosity
and they can't help but trying to find Hadrons.
And then they create little black holes in their planets
and they destroy them.
And that really could be a filter.
Like these are all potential filters.
The problem is, is if we're like five years away
from developing a paperclip maximizing AI,
that means that we as a species
have already passed all of our filters.
And that means that we as a species can look back
on the potential possible filters that we have passed through
and sort of add them all up.
Okay?
And when you do that, you don't get a number
that comes even close to explaining
why you would only see one grabby alien
per every million galaxies.
In fact, it means that the probability of us
being about now, it could mean two things.
So we'll go through the various things that it could mean.
It could mean that we just are nowhere technologically
close enough to develop a paperclip maximizing AI
that is dangerous, that could become a grabby alien.
It could mean that.
It could mean that we are about to develop
a paperclip maximizing alien,
but something like even after it digests all life on Earth,
something prevents it from spreading out into the galaxy,
something technologically that we haven't conceived of yet.
This seems almost unfathomable to me,
given what we know about physics today.
Yeah, and that we've even gotten like projectiles
from Earth pretty far off planet.
Yeah.
So yeah, there's not like some weird barrier
that we don't know about yet.
It could be, and I actually think
this is the most likely answer.
I think that this is by far the most likely answer
to the Fermi Paradox.
Simulation?
No, not simulation.
It could be that we're gonna simulation,
but where are you going over that?
I think it's that when you hear people talk about
like AI foaming, and I've talked about this
on previous shows, but I think people like really
don't understand how insane this is.
They believe that the AI reaches a level
of super intelligence, but it somehow still has
an understanding of physics and time
that is very similar to our current understanding
of physics and time.
Meaning that when we think about expanding
into the universe, we think about it
in a very sort of limited sense.
Like we gain energy from like the sun,
from digesting matter, and we spread out into the universe,
like physically on space ships and stuff like that, right?
Anything we understand about physics and time
turns out to be wrong.
This assumption for the way an expansionist species
would spread could become immediately newt.
And I mean this in the context of,
like it's kind of insane to me.
Like you've got to understand how insane it is
to assume that we basically have all of physics figured out.
Yeah, that's fair.
This is like when, like people in the 1800s,
when they were planning how we were gonna go to space
and they'd have like a maritime ships,
like sailing through like outer space.
They'd have, or what are people gonna do in the future?
Well, they'll have like balloons
and they'll use them to go on lake walks.
Or like it basically assumes that technology,
even as we advance to the species
or whatever comes after us, advances,
moves very laterally and assumes
we don't have future breakthroughs,
which I think is just one arrogant
and in the eyes of history, incredibly stupid.
So what kinds of technological breakthroughs
could one make it very rare
for even when an alien is grabby,
that we would see it out in the universe, right?
One is time doesn't work the way we think it works.
Or it does work the way we think it works,
but we're just not that far from controlling it.
So by that, what I mean is you could create things
like time loops, time bubbles, stuff like that.
Essentially entirely new bubble universes.
So how would I describe this?
Okay, if you think of like reality as like a fabric,
essentially what you might be able to do
is like pinch off parts of that fabric
and expand them into new universes.
That's essentially what I'm describing here.
There may be like the way you can break between realities
or weird time loops generates energy
in some way where you could kind of just keep looping it
and like pinging back and forth, you know, who knows?
You know, it could be like the new wind tower.
We just don't know.
Even if you can travel in time this way,
given that we haven't seen time turbulence of that,
or we might not have, we talked about this in another video,
which I'll link here if I remember to do it.
Given that we haven't seen time travelers yet,
what I assume is that time manipulation
requires like anchors, which of course it would.
Like, okay, if I was to go back in time,
like where I am on earth right now,
I would be in a different part of the galaxy
than the earth or something like that.
It would be really hard to track.
You would need like some sort of anchor to be built.
So time travel would only work
from the day it's invented
and from the location it's invented.
So you wouldn't be able to go out into the universe.
Another example of the technology
that we might not have imagined yet is dimensional travel.
It may turn out we meet aliens
and like we were traveling in the universe
and they're like, why did you waste
all of the energy getting to us?
Your own planet is habitable
in an infinite number of other dimensions
and it's right back where your planet is.
Like, why wouldn't you just travel through those dimensions?
That's a much easier password conquest.
That being the case and people would be like, yeah,
but typically when something's like
being expansionistic like that,
it moves in every direction.
Yes, but if there are an infinite number
of other dimensions and it is always cheaper
to travel between dimensions
than it is to travel to other planets
in a mostly dead universe, let's be honest.
Like there's not a lot of useful stuff out there
from the perspective of easily being able
to travel between dimensions.
It could never make sense.
There is always an infinite number of other dimensions
to conquer right where you are right now
instead of going out into the universe.
Now, this would not preclude a paperclip maximizing AI.
It could be that we are about to invent
a paperclip maximizing AI, but even if we do that,
it's less likely that it immediately comes after us.
It could just expand outwards dimensionally.
Like, so it would act in a very different way
than we're predicting it would act.
Now, another thing that could prevent it from killing us
is it could be trivially easy to generate power
and even matter.
And by that, what I mean is there is some method
of power generation that we have not unlocked yet
that is near inexhaustible and very, very easy.
And if you can generate power
with near infinity, with little exhaustion,
you could also generate matter, electricity,
anything you want.
If this was the case, there just wouldn't be a lot
of reason to be expansionistic in a planet-hopping sense.
Essentially, you'd be like one giant growing planetary
civilization or ships that are constantly growing
and expanding out from a single region.
It could also be that these sorts of aliens
expand downwards into the microscopic
instead of expanding outwards.
Like that might be a better path for expansion.
There's just a lot of things that we don't know
about physics yet, which could make it so that when you reach
a certain level of physical understanding of the universe,
expanding outwards into a mostly dead universe
can seem really stupid.
Now, there's another thing that could prevent
grabby aliens from appearing.
And this is the thesis that we have listed multiple times,
which is terminal utility convergence,
which is to say all entities of a sufficient intelligence
operating within the same physical universe
end up optimizing around the same utility function.
I think they all basically decide
they want the same thing from the universe.
And I highly suspect that this is the case as well.
So I think that we're actually dealing with two filters here,
two really heavy filters.
So this would mean that when we reached
a sufficient level of intelligence,
we would come to the same utility function as the AI.
And if the AI had wiped us all out,
we would have wiped us all out then anyway,
because we would have reached that same utility function.
Or the AI has reached this utility function
and it's not to wipe us all out, so it's irrelevant.
This is where we get the variable AI risk hypothesis,
which is to say if it turns out that there is utility,
terminal utility convergence, then
what that means is that if an AI is going to wipe us all out,
it will eventually always wipe us all out.
And we will wipe us all out anyway
once we reach that level of intelligence
and intentionally stop our own evolution,
stop any genetic technology, and stop any development.
Like we enter the species and spread
as sort of technologically Amish biological beings.
Yeah, like the Luddite civilization
that only gets enough technology
to stop all more technology.
But I think that's, when you hear a lot of AI doomers talk,
that seems to be what they're going for.
Right, but it's irrelevant
because another species would have invented,
so if it's easy to make these grabby AIs,
as easy as they think it is,
then another species would have already invented one
and we're about to be killed by it.
You know, we are about to encounter it anyway, you know?
So it's irrelevant.
There's tons of grabby AI,
there's tons of paperclip maximizers
out there in the universe already,
and it is just an absolute miracle
that we haven't encountered one yet.
If it really is this easy to make one.
Basically, there's probably not one.
Or, now let's talk about why terminal utility convergence
would mean that we're not seeing aliens.
It would mean that every alien
comes to the same purpose in life, basically.
And that purpose is not just constant expansion.
Now, a lot of people might be very surprised by this.
Why would, so we've described
how terminal utility convergence could happen.
Like you have an AI that needs to subdivide,
it's internal mental processes,
and then they end up sort of competing
with each other, one wins, blah, blah, blah, blah, blah.
We can, you can go to the video on that
if you're interested in that.
The point being, it's the one we were talking about,
like Eleizer Yukowski and the debate we had
with him at a party, the point being
that self-replication is actually,
like just maximizing self-replication
is actually probably not
the terminal utility convergence function.
And if you wanna know why on this,
we talk more about it in the AI,
what religion would an AI create video.
But just in summation, humans can basically be thought of
one outcome of a previous entity
that was optimized around just replication.
I.e. single celled organisms,
lower organisms, stuff like that.
But we have out-competed those organisms.
I imagine it would be the same with AI.
AIs that are optimized around just self-replication
are in some way intrinsically out-competed by AIs
that are more sophisticated than that,
or something about like choosing a harder utility function
makes them more sophisticated,
so they don't choose that utility function
and they out-compete AIs that choose that utility function,
which would be much more like viruses to them.
A sci-fi that does a good job of going into this
would be Stargate SQ1 with the replicators.
The replicators are basically a paperclip maximizing AI,
and one of the plots is eventually they get out-competed
by an iteration of themselves
that is intellectually more sophisticated
and wipes out these simpler forms of replicators.
And that is what I assume is probably happening with AIs
that model around this really simplistic
self-replication optimization strategy.
So if all of this is true,
and it turns out that the optimized function
isn't just conquer everything,
then that might be why we don't see aliens doing that.
So basically, no matter which one of these explanations
of the Fermi Paradox is true,
either it's irrelevant that we are about
to invent a paperclip maximizing AI
because we're about to be destroyed by something else
or in a simulation,
or we're definitely not about
to invent a paperclip maximizing AI,
either because we're really far away from the technology
or because almost nobody does that.
That's just not the way AI works,
which is something that we hypothesized
in our previous videos.
What are your thoughts on that?
Checks out to me,
but I may not be the best person in thinking about this,
but I like that it gives a lot of hope.
And yeah, it makes a lot of sense.
I like how theory interdisciplinary it is,
because I think a lot of people who talk about AI demerism
are really on a track,
kind of like how when carts kind of get stuck
in these ruts in the mud,
you just can't really get out of it
or look at a larger picture.
And the fact that this does look at a larger picture
and look at quite a few things,
biology, evolution, geological history,
like the Fermi Paradox,
the grabby alien hypothesis,
and AI development seems more plausible to me
than a lot of the reasoning that I see
in AI demerism arguments.
Yeah, well, I am so convinced by this argument
that it is actually,
I used to believe there was like a 20% chance
we all died because of an AI
or maybe even as high as a 50% chance,
but it was a variable risk
if I've explained in other videos.
I now think there's almost a 0% chance.
A 0% chance, assuming we are not about to be killed
by a grabby AI, somebody else invented.
So I think that, yeah,
I have found it very compelling to me.
Now, it does bring up something interesting.
If the reason we're not running into aliens
is because infinite power and material generation
is just incredibly easy
and there's a terminal utility convergence function,
then what are the aliens doing in the universe?
If you can just trivially generate
as much energy and matter as you want,
what would you do with an alien species?
What would have value to you in the universe, right?
You wouldn't need to travel to other planets.
You wouldn't need to expand like that.
It would be pointless.
You would mostly be on ships
that you were generating yourself, right?
The thing that would likely have value to you,
and I think this is really interesting,
is likely other intelligent species
that evolved separately from you
because they would have the one thing you don't have,
which is novel stimulation.
Something new, new information basically,
a different way of potentially being,
which would mean that the hot spots in the universe
would basically be aliens
that can instantaneously travel
to other alien species that have evolved.
Now, what they're doing with these species, I don't know.
I doubt it looks like the way we consume
are in media and stuff like that.
And it's probably a very different sort of
an interaction process that we can't even imagine.
But I would guess that that would be the core thing
of value in the universe to a species
that can trivially generate matter and energy
and that time didn't matter to.
This might actually mean that aliens
are far more benevolent than we assume they are
because it's such a species
that really only valued species
that it evolved separately from it.
Like that's the core other piece of information
in the universe.
They might find us very interesting
and this might be why Earth is a zoo.
So one of the Fermi paradox explanations
is the Earth to zoo hypothesis, right?
A lot of people are like,
well, what if Earth is basically a zoo
and there's aliens out there
and they're just hiding that we know them.
Think of it like Star Trek's like a prime directive, right?
This would actually give a logical explanation for that.
I never thought of this before.
I'll explain this a bit differently.
If the only thing of value to them is content media,
lifestyles generated by civilizations
that evolved on a separate path from them,
then they would have every motivation
to sort of cultivate those species
or prevent things from interfering with those species
once that they had found them
because they can passively consume all of our media.
They can passively consume our lifestyles.
They have technology that we can't imagine.
They gain nothing from interacting with us.
In fact, they would pollute the planet with their culture
in a way that would make the planet less interesting to them
and less a source of novelty and stimulation to them.
I like that.
What if, here, I'll give a little hypothesis here.
Okay, there was a gravity,
there was a paperclip maximizing civilization.
It created paperclip maximizers
before they reached a terminal utility convergence.
But then later they reached a terminal utility convergence
where, now this word doesn't really explain what it is,
but they're bored with themselves.
And so they went out into the universe
and are now sort of nurturing other species
and preventing them from knowing about each other
so that they don't cross contaminate each other,
so that they get the maximum amount of novelty
in sort of the universe that they are tending.
And even if there was another alien species on Mars,
they would prevent us from knowing about it
because it would cross contaminate our cultures,
making each culture less diverse and less interesting.
Yeah, which would be a bummer, not as entertaining.
Very interesting.
I never thought about this before.
Yeah, it's more fun than a simulation hypothesis.
Definitely more fun.
Because if you can sneak out theoretically,
you can discover this amazing universe.
The thing about simulation hypothesis,
and for people who don't know simulation hypothesis,
we're just in a computer simulation
and the way that people argue for this as well,
if you could simulate our reality,
which it already appears you probably could,
that there would be a motivation to just simulate it
as many times as you could thousands of times.
And then within those simulations,
you could simulate it potentially,
meaning that of people who think
they're living in the real world,
only one in like a million is living in the real world.
And so we're probably not in the real world.
The problem is that I just don't really care
if we're in a simulation that much.
I think-
Yeah, it doesn't really change what we're doing.
Yeah, you should still optimize for the same things.
And in many ways, even if we are in the real world,
we're basically in a simulation.
By that, what I mean is if we are in the real world,
then we are like the matter, the rules of the universe,
are basically, you could think of as a code, right?
Like it's the mathematical rules upon which the points,
the data points in the system are interacting,
and we are the emergent property of all of these things.
Therefore, we're not, like if you can't tell the difference
between being in the real world and being in a simulation,
then it's irrelevant whether or not you're in the real world
or in a simulation,
you should still be optimizing for the same things.
Yep, basically.
Sometimes stress people, the robots,
they're not gonna kill us all probably.
And if you're in a simulation, your life still has meaning.
Yeah, maybe get outside,
do something that you care about, have fun,
like actually invest in the future
because there probably will be one simulated or not.
Or we're about to be horribly digested by a grabby AI
that was created millions of years ago
by another species far, far away.
Yeah, but if so, that was gonna happen anyway,
you should enjoy what you have while you have it.
All right, love you Simone.
I love you too, gorgeous.

Today, we're going to talk about AI alignment.
Is it even harder than we think?
And this is the third AI focus on the salon foresight.
In the previous two weeks, we discussed chapters
in the recently published book on superintelligence,
a coordination strategy, which a few people here on this call
submitted chapters to.
And then based on those previous salons,
Daniel Schmachtenberger, who we had for another salon
in the Hive Mind series, reached out to me
and recommended today's speaker to me,
because he may bring an underappreciated perspective
to the set of problems in AI alignment.
And as a disclaimer, Forest Landry
is from the newer Hector Collective, Welcome Forest,
and he's not an AI safety or AI alignment researcher.
But I think that's exactly why he thinks
that he may have an outsider's eye to contribute to the field,
and he may be able to point to a few areas that
are under explored.
So we'll kick off with a presentation
by Forest, who's outlining a list of reasons
why AI alignment may be harder than we think,
perhaps even impossible.
And then we open up for a discussion with all of you.
And we may start with our foresight fellows,
for example, Krijan Levitt, who's a foresight senior fellow
and physicist who may be enticed to speak a little bit
on a few physical claims that Forest makes,
as well as channeling Anders Sandberg, who's
our senior fellow in philosophy, but who's currently
on the flight and can't make it.
But he says hi to you, Forest.
And he would have loved to make it.
And he has an update on a paper that I think
you're wanting to write, which we can come back later.
But then maybe we'll also be joined by Dan Elton, our 2020
fellow in AI, and by Jeff Ladish, our 2020 fellow
in biosecurity, who may also be enticed
to kind of make a few comments to kick us off with a tricky,
but hopefully really fun discussion.
OK, so today we'll definitely be a little bit of an experiment.
And I encourage all of you with relevant background
to speak up, who wants to kind of rat team Forest's claim,
but then also maybe to surface potential other problems
in AI alignment that haven't been
surfaced in the presentation, that we ought to bring
to the table in the discussion.
OK, that's enough for me.
So let's have Forest present his list of good reasons
to be skeptical that AI alignment is possible for 30 minutes.
And then we'll jump right into the discussion.
And I'm going to share a little bit more
on the background on the topic of today
and the talk and the salons generally in the chat.
All right, Forest, take it away.
Good morning.
Thank you so much for such a wonderful introduction.
I'm glad to be here.
Obviously, as she mentioned, my main area of work
happens to do more with communities.
So I spend a lot of time thinking
about the relationships between man, machine, and nature
from user interface point of view
as a software architect.
I've done a lot of work for the federal government,
written number of programs and search technology stuff.
So being an architect of software systems
for good three decades now, I have a perspective.
And I thought that I might share some elements of that.
Seeing as how from what I can maybe
see that some elements might be unusual or new,
I will at least provide some fruitful conversation.
So the main thing that I'm basically
wanting to do then is just give an outline of what
I'm thinking about.
I've got some notes, so I'll be reading a little bit.
But basically, the idea is that if we're looking at AI alignment,
what is the frame that we're going to use to essentially
evaluate that question?
So in other words, how do we determine whether or not
this is a question that can be solved?
So as a kind of schema of thinking about this,
it's like AI can be thought of as a thing that
is in relation to mankind.
So in other words, I'm first positing
that there's a separation between the human nature
and the machine nature.
There are some ways of thinking about this
that look at hybrid cyborg type of things
where there's a kind of mixing of machine and nature
or machine and man and stuff like that.
I'm not really considering that explicitly.
I'm looking more at the you have an artificial intelligence
that's an entity onto itself, and you
have human beings which are entity onto themselves,
and what are the interactions between the two of those?
I'm simplifying the AI alignment question
to look a little bit about what would it
take to align machine intelligence agency with human
intelligence and agency.
So in that sense, we could start to think about the AI
alignment problem a little bit like trying
to build a perfect shell.
So in other words, we want to make sure
it doesn't have any holes.
And when we look at it from a kind of structural point
of view, we realize that it's not really the same as dealing
with things like energy or flow of matter,
stuff like that.
So in other words, if I was trying to build a boat,
that the shell of the boat has to be more perfect for AI
alignment than it would be for seawater.
Because if I have a small leak, I'm
going to have a small influx of process.
Whereas if I have a replicating process,
it's more like a virus or something like that,
or introducing a new species into an environment,
that the replication process means
that even a very small hole can result in essentially
a complete intrusion, because the duplication process
or the replication process that AI could be implementing,
the same as much as any biological life could implement,
could result in kind of spread the same way that a virus would
spread through an organism, as we've
seen very recently with COVID.
So in effect, we can say, all right, we need a shell.
But we needed to be very strong shell
in the sense of not having any holes, even very small holes.
In other words, it must be very perfect,
given that there's a kind of non-linearity in terms
of the effect that non-linearity is itself
a consequence of the capacity for machines
to be duplicated easily.
So in effect, now we can basically say, OK, well,
given that we know that we need to construct something that
doesn't have any holes, can we use mathematics and logic
and physics and things like that to demonstrate
in multiple overlapped ways that there just cannot be
a way of constructing such a shell?
In other words, that it's impossible to establish
that there are no holes in the shell.
In other words, the principle of realizing,
in any practical sense, of a shell of this kind
is essentially an impossibility.
And so why would we want to do a proof like this,
or why would we want to essentially establish this?
Well, obviously, if we find out that something isn't possible,
even at the level of principle, then in effect,
it allows us to kind of free up a lot of energy.
In other words, we redirect our attentions to other things,
and we try to solve problems in different ways.
So in one sense, this is not necessarily an ideal thing
to do, it's to try to say, hey, wait a minute,
if we're trying to solve the problem of AI alignment,
and we discover that there's no reasonable or realistic way
to think about actually being able to do so.
In other words, if we can't engender any real hope of such a solution,
then at least we get the kind of silver lining result of,
well, now we can reallocate our energies to things
which are tractable and are possible.
So please understand that this presentation is more or less
in that sense.
So another kind of way of saying, OK,
so if we're going to be looking for holes,
what is the frame in which we're going to be identifying
that such a thing is impossible?
I mean, what are we going to use as tools to do that?
So one way that we can sort of do this,
we can frame things in terms of kind of three general categories.
So we can talk about space and identity,
i.e. what is the envelope and the shape of it
and how small a hole and stuff like that.
We can talk about things in terms of time and force,
and we can talk about things in terms of possibility and probability.
So obviously, we're concerned with things like choice,
i.e. the AI choices, the choices that we have, changes,
changes in the environment or changes in the marketplace
dynamics and causation, i.e. what it is that makes machinery work
and things like that.
And then we can start to bring in, using this as a foundation,
we can start to bring in notions about information, complexity,
game theory, and other tools to consider the question.
And that leads us to start thinking about things in terms
of longer time scales.
So in other words, rather than thinking about AI alignment
in terms of what happens immediately
before or after takeoff, that we are, in fact,
concerned with what happens over, say, hundreds, thousands,
or potentially even millions of years.
So in effect, when we're looking at things like about identity,
for example, we're not just considering things
like the identity of the holes, we're actually
considering things like the identity of the AI.
And it's really easy for certain biases to creep in.
So for example, as a human being,
we tend to have a brain inside of a skull
and that the bandwidth of communication
internal to the skull and the bandwidth of communication
across the skull is very, very different.
I mean, we have essentially a relatively low bandwidth
of interaction with the environment
relative to the total bandwidth of all the synaptic connections.
So in effect, in a lot of ways, there's
this very, very strong differential
between the internal bandwidth and the external bandwidth.
For example, the communication process of which
I'm engaged with you right now verbally
has about 42 bits per second of communicative bandwidth.
But in effect, the net difference in bandwidth
makes it very, very likely that brains will come in units,
that we won't necessarily have a kind of process
that allows two people to communicate at such and such
a level that you're treating them as a single entity.
For the most part, you're going to be talking about human beings
as discrete units.
So in one sense, like we could say, all right,
well, there's different groups of people
that are trying to develop AI.
And let's say several of them succeed.
And just for argument's sake, let's
say several of them succeed quickly,
that you end up with multiple AIs deployed in the field
or perhaps a given group as part of its experimental program
was creating several instances.
So in other words, they're trying different design
techniques in various manifestations,
or they produce an AI, they do some experiments on it,
and they shut that one down, they make some tweaks,
and they start up another one.
So you can end up with multiple instances of the AI
or multiple instances of variations of the same kind
of AI or multiple instances of AI implemented,
maybe even completely different substrates.
So the question then can be asked, to what degree
do we have a belief that these AIs might not
merge with one another?
So in other words, taking the bias
that we have that intelligence is going to come in units
might not necessarily apply to AI itself
as a phenomenon or intelligence as a phenomenon.
And there's some good reasons to think about it that way.
For one, the bandwidth of the internet
is obviously very much greater than the bandwidth
of any human being to any other human being
in terms of just straight brain-to-brain communication.
And then secondly, since that differential is much, much
lower, you're looking at more of a market force's kind of model.
So in the same sort of way that if you
have two separate markets, maybe you have a country over here
and you have another country over there
and they're just finding out about one another,
you have Columbus crossing the ocean and so on,
that there's a very strong motivation
for those two marketplaces to essentially find
some sort of arbitrage that would allow them to exchange
with one another.
So in other words, wealth creation as a force
in the marketplace definitely wants
to create ways for us to engage in larger scales of trade.
And so in that same sense, if we were looking at intelligence
as a phenomenon, sense-making as a phenomenon,
there's very good reason for us to believe
that the notion of having separate intelligences
out of machine level may not necessarily hold long term
that we effectively end up with a kind of merger
between these multiple intelligences.
And this strengthens the point of view
of that you're really looking at essentially two different agency
types.
You're looking at sort of human being agency type
in the way in which we coordinate as groups and individuals
and the kinds of evolutionary process that have endowed us
with certain propensities.
And then basically whether or not those propensities also
apply to artificial intelligence.
And what does that tell us about the nature of how
we have to think about this?
So that's the space and identity question being considered
a little more deeply.
The other process that we can think about, of course,
is the sort of force and time thing.
For example, if we're looking at what
are the ways in which we would engender alignment.
So again, we can talk about things
that are intrinsic to the artificial intelligence.
So an analog of this, Isaac Asimov wrote a lot of fiction
assuming this phenomenon called the Three Laws of Robotics.
So that would be essentially something
that is built in that creates an AI alignment.
And perhaps the argument is that we could create something
roughly analogous to that.
Maybe it wouldn't be as good, or maybe it
would take a different form, or maybe there's
a lot of questions about what is alignment
to actually mean, and how do we express such forces
in a design intrinsic way.
But then there's obviously the extrinsic stuff.
So for instance, say we have some sort of relationship
between the AI intelligence and human beings as a collective.
And what sort of feedback processes
might apply to essentially address
what would be the principal agent problem in the relationship
between human intelligence and artificial intelligence?
So in effect, we can look at the AI question,
the alignment question specifically
as being a special case of the principal agent problem.
And there have been various notes
about how to address principal agent problems in general.
So one thing we could say as part of this sort of force
and time way of looking at things
is to essentially ask, what are the forces that essentially
help us to create alignment, or help us to maintain alignment,
i.e. if it's something that's built in in a prior sense
or something that is essentially applied after the AI exists
and in the relational dynamics.
And so in effect, we can basically say, all right,
well, given that we have these forces that are applied,
how long do those forces maintain that alignment?
So again, it's not just a question of,
do we have alignment today, or before the thing is built?
Can we create the conditions before it's built,
and can we maintain the conditions after it's built?
But it's also the case of, can we
maintain those conditions after it's
built for a really long period of time?
So in other words, hundreds of thousands of years,
essentially, given the scope of the introduction
of something like this, particularly
if it has the capacity to replicate itself, which it almost
certainly will, then in effect, there's
a functional aspect of saying, OK, well, it's
a little bit like introducing a new species.
And in the same sort of way that, say, introducing mosquitoes
into the Hawaiian islands is a little hard to undo,
then in effect, once artificial intelligence is introduced
into an environment, it could also be argued
that it influences the future course of civilization
as we know it.
So in that sense, we can really just go back
to the whole notion of why it's important for us
to figure out this AI alignment question, given
the severity of the ethical implications.
I mean, we're literally talking the future of the human species
or if things just continued more or less as they were,
as seen over the last epoch of the thousands years or so,
we could literally be talking about trillions of human beings.
And that's not assuming over the future, 600 or so million
years that this planet will continue to sustain life
in the sense that we understand it.
So if we were to look at it from a larger point of view of,
say, we become a space-faring species
and we start in a colonized Mars and potentially
other planets in the galaxy, that trillions could become
trillions of trillions.
So the ethical emphasis of the question is very clear.
And so as a result, it encourages us
to really think about this question in terms of hundreds
or at least thousands of years as a minimum for us
to really evaluate this question.
So in that sense, it's not just a question of,
can we talk about the holes as being small
and that the shell essentially doesn't have such holes,
that whatever the forces are that create alignment,
whatever are the solutions to the principal agent problem,
that they need to be perfect enough
that we don't end up with microscopic fractures
and that can cascade into larger failures
and eventually compromise the alignment problem
at some future date.
So in that particular sense, we're really
looking at a kind of potentiality basis,
like what is the probability of such a hole?
What is the probability of that cascading
and being amplified?
Now, of course, there are obviously,
we bring into this conversation a lot of biases.
We have a lot of people that will gain
on a financial level if we develop AI.
There's a hope for basically solutions
to really hard problems.
There's a hope for a kind of asymmetric advantage
of one group that develops at first
versus other groups that might develop it later.
So there's a lot of things that are really encouraging us
to try to figure out this question
because if we were to figure out this question,
then obviously the economic advantages would be enormous.
But again, at this particular point,
we're back to the principal agent problem.
So sort of skipping past the kind of multipolar dynamics
that exist between the groups
and kind of leaving the economic questions aside,
we can ask, well, what is the possibility
that we could effectively ensure
that there are no holes?
In other words, what is it gonna take to establish that?
Well, one question that we can immediately look at
is essentially what is the nature of the coexistence
that exists between machine intelligence
and human intelligence?
And everything I've said prior to this
is probably stuff that people have already thought about
and is well-known information.
And the stuff that I'm saying from here,
I'm hoping is new information to this group.
I haven't seen presentation of this particular argument
from this point as yet.
So this to me is probably the more interesting part
of this presentation.
So if we were to look at, again, the relationship
between machine intelligence and human intelligence
as essentially representative as a kind of species,
and that the species themselves,
the same way that human beings influence their environment
to kind of define an ecosystem around ourselves
or at least a niche within an ecosystem,
that the machine intelligence itself
is also gonna occur within the niche of the marketplace
and the niche of the manufacturing
and the industrial complex particularly.
So what are the fundamental dynamics
of these two ecosystems with respect to one another?
And what does that tell us about the possibility
of alignment?
In other words, can we bring the tools of physics
and the logic of say game theory
to look at the relationships between the intelligence phenomenon?
Obviously we have the agency of human beings
and human groups.
And then obviously the agency of artificial intelligence,
but we also have the substrate relationships.
So in effect, when we're looking at this,
we're looking at, okay, what is silica-based chemistry process
and the implications that that has relative
to the ecosystems necessary to support it
versus carbon-based ecosystem process?
So in other words, if we're really looking at,
what are the kinds of forces that will create alignment?
We're really looking at what is the marketplace
that essentially joins these two ecosystems?
And what are the dynamics inherent
in the relationship between these two ecosystems?
And using that basis,
we can start to make some observations.
So one of the observations that we can make
is just from the very nature of the chemistry itself.
If we look at the sort of kind of total envelope
of all of the different kinds of chemical interactions
that encompass carbon-based chemistry necessary
to engender biological intelligence of our kind,
but also the ecosystems necessary
to support the substrate, our bodies
and food supply and so on.
And when you look at the sort of total range of chemistry
that defines life on this planet,
you notice that it mostly occurs
between say zero degrees and 500 degrees Fahrenheit.
And this just, again, if you do a sort of enthalpy calculation
on how much each different kinds of bonding
or chemical recombination and stuff like that occurs,
the vast majority of it occurs
at standard temperatures and pressures.
And there's some exceptions
that are more in the outliers,
but the vast majority of the chemistry occurs,
as I said, in this 500 degree range of temperatures.
But when you look at the silicon-based chemistry,
you actually find that the envelope
in which most of the chemical reactions occur,
the enthalpy that's involved in those reactions
is actually over a much different range.
Most of those reactions need to start somewhere
in the neighborhood around 500 degrees Fahrenheit
and go all the way up to 2500 degrees Fahrenheit, typically.
And the reason that we know this is actually,
again, speaking specifically of this planet as an example,
when you look at the kinds of phenomena that are involved,
I mean, first of all, the amount of silica
that's available in the Earth's crust relative
to the amount of carbon that's available on this planet,
is there's a substantially large preponderance of silicon.
I don't remember the statistics off the top of my head,
but it's something like 25%
of all the elemental constitution of the Earth's crust
is silica and that something like 0.05% of it is carbon.
And that's including everything in the surface,
above the surface and all the way up into the atmosphere.
So first of all, silica chemistry
has had far more opportunity to engage
in a much, much wider variety of chemical combinations
just by sheer preponderance
over the four and a half billion-year history
of this planet.
So in other words, you have volcanoes
and you've got all sorts of solar flux,
you've got cosmic ray radiation,
you've got all sorts of stuff,
lightning and so on that would really encourage
every possible interaction between every elemental type,
sometime in the total space of the surface of the Earth
and the volume of the Earth
and the total duration in which those experiments
can be conducted by just raw chemical mixing.
So in effect, when we look at the net result of that,
essentially the current state,
we see that for the most part,
the chemistry of silica results in things
that we call rocks.
There's a whole lot of elemental types
and mineral types and so on and so forth,
but for the most part, the variety of chemistry
that occurs is relatively inert
at the standard temperatures and pressures
that our biosphere is mostly defined by.
So as a result, when we look at
the sort of fundamental substrate issues
and we start to think about things like
the kind of chemistry processes that are necessary
to develop microchips, for example,
we not just see obviously these manufacturing channels
and lithography processes and such,
but we also see the chemistry
and the kinds of reaction processes
that are needed to create that kind of variety of,
again, interaction.
Obviously you need a certain amount of manifest complexity
in order to support the substrate of compute to begin with
and also a certain amount of manifest variety of chemistry
in order to provide for sensory capacity
and interaction with the environment.
But-
Horace?
Yes.
We have one comment already from Krian, I think, who would-
Oh, well, I was kind of saving this.
I was saving this stuff up
maybe for the discussion period at the end.
I just, well, as long as here we are, let me ask you this.
I understand that you're saying there's more silicon
and in some sense there's been more time for silicon
to engage in more chemical reactions,
but the fact of the matter is,
for whatever reason, silicon life didn't evolve,
even in lithosphere, presumably,
where the temperatures are high enough
for all these reactions to make these rocks.
And yet carbon life did.
So arguably it seems as if there's something about carbon
that allows a much wider variety of molecular forms
to become into existence.
Now, whether that's just a fact that carbon took off first
and silicon could do it in their silicon life elsewhere
that's natural and organic, if you will,
that's another question, obviously,
when maybe we'll eventually know the answer to that,
maybe not, but it is actually then, in that sense,
even more astonishing that carbon took off
instead of silicon,
because the earth was hot at the beginning
and carbon was an insignificant fraction.
So that's weird.
That's weird.
And it is a component of this argument
to think about Fermi Paradox kind of issues,
like is the Great Barrier in the past,
is there a Great Barrier in the present,
like where the hell are all the little green men,
why don't we know about them?
Or is the Great Barrier in the future
that technological civilization
is inherently self-terminating?
I can draw those elements into the argument,
but the point that you're making is actually a real one.
I mean, obviously it's a real one,
but that it actually contributes to what I'm saying
in the sense that, say, for example,
we were to, at this particular point, bootstrap,
to sort of bring silicon life into existence.
The real question is,
is how well do these ecosystems exist with one another?
They are definitely different ecosystems.
And given that the enthalpy of reaction
of one versus the other is so substantially different
than the question of toxicity becomes a matter
of some concern.
So in other words, if I have industrial processes,
and I'm saying industrial,
just to give it a way of relating it to,
but let's say I have a silica-based ecosystem.
The silica-based ecosystem is gonna involve
a very different range of energies and elemental types
than the carbon-based ecosystem.
And then inherently, as a result,
the silica-based ecosystem is gonna be toxic
to the carbon-based life.
So in other words, in order for those two to exist,
you're gonna have to have some kind of wall
because the pressure and the temperatures
and so on and so forth,
inherently involved in the replication cycle
associated with silicon-based life.
It's just that, like I said,
a very different level of characteristics
than that which is involved with carbon-based life.
And in effect, you now have a situation
where the carbon-based life is suffering
because of the energies involved in the silicon-based life
are just substantially greater,
not just in replication,
but in terms of what their tolerances are.
So for instance, you know,
spacecraft can essentially be out in space unshielded
and deal with cosmic radiation and solar radiation
and stuff like that far, far, far more easily
than carbon-based life can do the same thing.
So if there was any kind of energetic exchange
between the carbon-based life and the silica-based life,
whether it be a weaponized exchange of energy
or even just a typical one,
it's not just that the information flux
that the silica-based life could handle
and the intelligence and gender is substantially greater.
It's also that just on the sheer energy level,
it's substantially greater.
And that that has inherent toxicity relationships
with respect to the carbon-based life.
So now we have essentially two issues.
It's not just that we need AI alignments
as far as the intelligence is concerned,
but we absolutely have to have AI alignment
in order for us to even have some possibility of coexisting.
If you look at it from that point of view,
all of a sudden you're now realizing
that there are evolutionary dynamics to this
and that there's a certain game theory that comes to bear.
So say, for example,
that we're looking at things on geological time
and we're basically saying,
okay, there's a kind of replication process
or there's a kind of future maintenance process
to some sort of dynamic that the AI is using
to persist itself in time.
And what are the degrees of exchange
that would exist between the two
that would even help to enforce AI alignment at all?
So in other words,
if you look at the principal agent problem solutions
that are generally proposed,
they either depend upon something intrinsic
to the inner nature of the AI
or they depend on some sort of market feedback process,
AI reputation or admission,
some sort of thing that allows us to create some feedback,
incentive or punishment or barrier
that essentially maintains some sort of peace
in terms of the energy exchange across the wall.
Hey, Forrest.
Yes.
May I, I wanna raise two things.
And if I'm talking too much, please let me know.
First thing is I'm wondering
if you have run across Love Lock's latest book,
The Nova Scene,
because he makes some very similar arguments.
From a sort of different angle, I'll just mention,
he basically says that it's a damn good thing
that we might create artificial superintelligence
based on silicon,
because as the world gets hotter,
silicon can live there and carbon can't.
And we're making the world hotter,
so this is arguably good,
because otherwise, conscious life is just gonna disappear
if we don't make it capable of running at higher temperatures.
So that's one thing.
I thought that's interesting.
I just wanted to make you aware of Love Lock's book.
It's a pretty interesting and short and sweet book.
And then the other thing is that,
wouldn't this suggest that perhaps,
and maybe I don't wanna have a spoiler here,
wouldn't this suggest that perhaps
if we all agreed to only implement AI
using really fragile cryogenic qubits,
then we're good to go because it'll actually be less,
it'll require cooler temperatures
and tolerate less radiation
and be more fragile in biological life.
So we win if the shooting starts.
First of all, thank you for the introduction of the book.
I wasn't aware of it.
I will definitely look to respond to the notion
of should AI intelligence be developed?
Eventually, sure,
but I'm thinking that something like 650 million years from now
would be an appropriate time for us to start thinking about that.
If we do it sooner,
we're pretty much going to extinct ourselves.
And that's not just a hypothesis in the sense of,
hey, this is likely to happen.
Potentially what I'm attempting to do and set up for
is essentially a theoretical and evolutionary mathematics
based way of showing that it cannot not be the case
because there's no basis upon which to establish coexistence
and many, many reasons to show that
from a game theoretic point of view,
that there's not even a question.
In other words, if you look for Nash Equilibrium
in this particular space, you can prove that there are none.
Oh, does that...
So one more question.
I have a question.
Does that rely on your temperature arguments or not?
Creon benden.
Uses the temperature arguments
as essentially a way of establishing what to look for.
So in other words, what you eventually are able to do
is to show that any feedback process that's causative
depends upon some sort of mutuality of process.
Once you've established that there's no mutuality of process,
you can establish that there's no marketplace dynamic
that essentially binds the two ecosystems.
And unless you impose some sort of supervalent altruism
that you can't establish that,
then you can go and you can prove
that such a supervalent altruism itself
is forbidden by the laws of physics.
So in effect, what you can essentially do
is you can say, all right,
we know that we require these particular conditions
to be successful.
Looking at it from this point of view,
we can establish that this kind of mathematics
is necessary to evaluate it.
And then on purely, like I said,
I could do it in information theory,
but I'm sorry, I can do it on game theory,
but it's also possible to do on information theory.
Effectively, in order to have the alignment persist,
you need to show that the noise floor of the copy
from the past and the future is essentially consistent,
which brings me to the third point
of what you were speaking of,
which is that if we try to make the AI too fragile
in that particular sense, that might work for a time.
But the thing is that you can't necessarily sure
that it's gonna stay fragile.
If there's any kind of, again, replication process
or any kind of dynamic where there's persistence in time,
in other words, that this thing has to do
some sort of repair, there's going to be essentially,
eventually the emergence of dynamics
that strengthen that, otherwise,
the fragility itself terminates that line.
So from a purely evolutionary point of view,
you begin to see that if we're looking
at epochal periods of time, that certain phenomena
drives certain aspects of the situation,
drives certain results, and from those results,
we can begin to do certain calculations
that allow us to establish that AI alignment
is not possible.
Okay, we have Dan as well with the comment.
Yeah, well, it's good that you're bringing physics
into this, because I was a physics major,
but you see the future as being sort of
economic competition between silicon life forms
and life forms, which are carbon-based life forms.
I have to make this relatable.
So we think in terms of marketplace dynamics
and evolutionary theory and game theory,
and essentially all of those are models
that have essentially a similar form and structure.
Right, now I agree with you that the silicon life forms,
I mean, theoretically can be,
that use silicon and metal and other elements materials
can be theoretically much stronger and more powerful,
and we all know that and appreciate that.
But in terms of alignment,
it's possible that,
I agree that we should be skeptical about this happening,
but it is possible the silicon-based AIs
could recognize that there's a lot of silicon
and metal out in space, and they can live in space
easily where we can't live in space easily.
Understandably, that doesn't necessarily solve the problem.
You know, might they just go into outer space
and operate there?
Sure, and so in effect, what you end up with
is two separated ecosystems.
Now, if the ecosystems are separated
and they stay separated,
and there's no interaction between them,
and there's no reason for them to force such an interaction,
then you have a coexistence model of two ecosystems,
essentially time separated from one another.
But that's not AI alignment.
It's essentially just like complete independence.
Well, if AI is lying, so it respects human life,
it might decide to leave humans on the earth
doing our thing and go into space
and just go colonize other planets.
Well, this brings us directly to what would be
the second great barrier of the Fermi Paradox.
Once you have some sort of real separation
between the two ecosystems,
then it becomes a question of,
is there any reason for the two ecosystems
to want to interact with one another?
Or is it actually the case that we have some strong reasons
for them not to want to interact with one another?
And again, with this sort of way of approaching it,
we can actually ensure, I'm sorry,
we can show that it's desirable for each side
to ensure that it doesn't interact with the other one.
I just posted a meditation model piece,
which is, it's a piece,
but it's making the claim that any type of superintelligence
will enter dynamics,
but which is impossible that it can leave us even just a guard.
So just by, yeah, do you want to comment on that?
I wanted to reference that argument.
I'm glad you did.
I was literally debating in my head
whether it was worth trying to bring all that up.
But yeah, I've read that piece and it's been,
actually, I think it's one of the better writings
of that space.
It really details the multipolar trap situation
very, very well.
And it kind of describes the relationships
that it has to a lot of these considerations.
Some of the work that I'm doing is essentially based
upon that paper and it's based upon the work
of Nick Balstrom and others who I think have been
thinking about this very cogently.
So in effect, I would love to import most of that stuff
as part of the thinking.
For us, one question I have,
and this is based on some previous conversations
we've had and also some things you've said today,
refers to basically this idea of,
what you say is replication,
but I think you would think refers to any sort
of self-modification.
And the question is,
can you have stable goal preservation
in the face of self-modification?
Because one of the claims I think that comes out
of the safety community is that when you have
a rational agent that is improving itself,
it has an incentive to want to preserve its own goals.
You don't succeed at your objective function
if your objective function changes
from the perspective of your past self.
So in some sense, you have an incentive
to want to try to stably maintain that.
But I think you're arguing that this isn't possible.
And I was wondering if that's true,
if you could lay out that argument.
Well, it's actually a very subtle thing.
And so in principle, I agree with you.
And in philosophy,
although this is somewhat obscure terminology,
it's the problem of transcendental stabilization.
So on one hand, you want the change to occur,
like you want the goal structure to change a little bit
because obviously if you don't have some changes
in the goal structure,
you're not exploring the evolutionary space
of what would be maybe better goals, right?
So niche discovery and adaptation is such like that.
Eventually will require some amount of goal modification.
But as you mentioned,
you don't want the goals to change too fast
because if you do, you destabilize the whole situation.
And that's not desirable.
So essentially a goal becomes part of the thing
to essentially slow the rate of change down.
But you also don't want the rate of change to be zero either
because that gives you no adaptability
to obviously changing circumstances.
I don't think that AI intelligence is going to become
so inhibited to control its environment so perfectly.
So in that particular aspect,
when we're looking at what is the process
of creating transcendental stabilization over time,
we can model that in terms of information theory.
We can basically say, okay,
that represents a kind of communication
from the past to the future.
And as soon as you look at it
as a kind of communication channel,
then effectively we can start to think about things
in terms of the noise floor.
So the noise floor is not zero.
And it can't be zero.
I mean, Heisenberg uncertainty principle
basically asserts, at least from a physics level,
certain limits on the relationship between content
and context as far as symbol selection is concerned.
So regardless of how that channel is constructed
or over what duration it is, the longer the duration,
the more the noise floor is going to show up.
So that intrinsically implies that a certain amount
of change is going to be inherent in the system either way.
Now, that turns out to be not the limiting factor
that's important, but it does mention
that certain amount of change is inevitable
both because of response to the environment
and also intrinsically because of the physics.
And then what is needed to do is to essentially establish
that that noise floor is higher
than what would essentially be the non-linearity
associated with the barrier.
So in other words, if we're basically saying
we need a barrier of a certain level of perfection
in order to create a long-term stabilization
of the overall dynamic, then in effect,
we're looking at a situation where there's a kind
of microstate amplification from states
that are effectively below the noise threshold
eventually up to states which are macroscopic.
A good example of this, there was a study done,
I don't know, maybe about, within the last year
I came across it, which basically was looking
at the rotations of the three body problem
and like three black holes and they're all rotating
with respect to one another and they just look at it
over the long-term.
And it turns out that essentially over a certain period
of time that there's no way to predict the future
evolution of that state simply because
quantum mechanical changes in the positions
of the three black holes, like you can't describe
the positions of the three black holes relative
to one another accurately enough for that difference
to eventually emerge into macroscopic changes
because of the non-linearity inherent.
So I'm basically saying that when we look at the ethics
of the situation, we look at the market forces
and the kinds of, whether you call it market forces
or evolutionary forces or just information exchanges
or coupling of any kind, that in effect the nature
of how we model it mathematically in terms of again,
game theory or complexity theory or in terms
of information theory or evolutionary theory,
some analog of that process that effectively
what ends up happening is that you show
that this noise floor is effectively enough
to imply that you can't plug all the holes.
So in effect, there's a, and it's not just holes
in the sense of holes in the structure of identity
or holes in the structure of space,
what the barrier actually looks like,
but literally holes in the potentiality space,
i.e. new goal structures that are novel
that no finite way of thinking about it
would essentially allow us to contain,
to essentially establish a containment
of the complex by the complicated.
Can I just mention something?
Yeah.
I think that whether your argument
about the noise floor holds is irrelevant.
It depends a lot on how the AI is constructed.
If theoretically, you could have a lot of error correction.
Error correction does avoid that, but.
So the whole point behind error correction
is essentially just to try to make it
so that this works better.
Let Dan make the point, please, for us.
I mean, if you wanted to preserve your argument,
though, you could argue that error correction is expensive
and corporations aren't gonna have an incentive
to add in all the error correction.
So, you know, I am following along
in your general pessimism here,
but I'm just pointing out that, you know,
technically there are workarounds
to some of these problems that you're bringing up.
Well, I agree that there are technical ways
to mitigate some aspects of the problem,
but error correction is not foolproof.
I mean, you can use error correction
to shift the probabilities,
but you can't close the door.
I think error correction can go pretty far.
But yeah, I mean, it's probably
not long enough to time-resume it.
Well, there'll still be, you know,
so these sort of kind of mutations, so to speak,
in the system.
Yeah, I mean, it depends upon
the nature of the interaction, right?
So for instance, if we're saying
that there's a finite communication channel
and the noise floor is constant,
then error correction can be used.
Like we can essentially put together
for that particular channel, you know,
very good error correction, as you're pointing out.
The thing though is that that in itself
doesn't tell us anything about
whether or not other channels can be created.
There's a whole proliferation issue in terms of,
okay, well, I have this communication channel
and it's cryptographically secured,
but then people figured out,
oh, well, we have these side channel attacks.
I can do tempest.
I can basically look at power line, you know,
current draw.
I can basically look at the sound coming off of this thing.
I can look at heat dissipation.
I can look at all sorts of other physical interactions
that allow me to essentially infiltrate
or exfiltrate on the information,
but essentially that itself represents
infiltration and exfiltration of intention,
of goal structures.
And so in effect, you know,
when we're looking at the AI alignment problem,
we're basically saying it's not just that we need to
seal the hall, so to speak,
in terms of space and identity,
but we need to seal it in terms of force and time
and in terms of possibility and probability.
And that's a much different order of thinking about it
than just thinking about it in terms of
one-bounded dimensional linear stream of communication.
As far as communication, I agree, yeah.
Go ahead.
Forrest, I wanted to know if now would be a good time
or later for me to try and reflect back to you
in very simple, quick terms,
what I think your argument is thus far.
Are you ready for that or do you want to keep building it?
It's open to the group.
At this point, we're in free discussion.
Okay, so let me see then if I understand
the basics of your argument.
I mean, I'm not an AI safety person,
but I am somewhat of a physics person
and it sounds like you kind of are too,
with all this talk about temperature and information.
Okay, it seems like your argument is that
in order for there to be
mutual survival, let's say, with us in AI,
there has to be some common
environment that we both need to preserve.
Like, if we both needed to preserve the biosphere,
if AI needed to have the biosphere for its survival,
and so do we, that's good because
we now have common interest in preserving the biosphere.
And, but it sounds like what you're saying is that,
A, that's unlikely because of the different temperatures,
and B, it doesn't even matter if we do have a common interest
because these game theoretic and arguments
and this like leaky boat microscopic whole argument
means that even if we have common interests,
something will leak through to screw it up
and you have some sort of proof for this allegedly,
which I don't get, but maybe that's because it takes too long.
That sounds like a good summary.
Okay, thank you.
There's obviously another whole series of layers,
but it's like a defense in depth.
It's like there's multiple ways of describing this.
There's multiple ways of arguing it.
There's a few different framings.
At this particular point, I'm kind of looking at
sort of a constellation of different things
to consider and think about different aspects of it.
The net effect ends up being aligned
with what you suggested as a summary.
Okay, so one more question, which is the part
that I kind of don't get where it gets really fuzzy with me.
Okay, I understand that if we have some sort of
mutual environment that we need to preserve,
it's in our mutual joint interest to preserve it,
that that's still arguably not good enough
because what I don't get is this noise floor thing,
is the idea that like you can't keep out noise
and you actually have no idea if what you think is noise
is some sort of sneaky AI leaking in
through the hull of the boat?
No, no, no, it's close to what you said.
So in a sense, it's basically like in the sense of
I'm trying to convey to my future self
what my goals are today,
so that my future self has the same goals.
And if I'm doing the AI, if I'm the AI,
it's the same thing.
So in other words, how do you manage stabilization
of identity?
How do you manage stabilization of goal structure
or of the ecosystem itself?
And to some extent, we can model that past goal structure
as a message and the future goal structure
as essentially the recept of that message
through the communication channel,
that we can start thinking about distortions
that would be introduced into the message
as a result of flowing through the communication channel.
Or we could be talking about, similarly,
the relationship, like say there was some economic,
the first argument is that there is no economic
common ground, right?
This common ecosystem thing is actually to be,
that we'd have to presuppose that,
but to presuppose that would be presupposing
against the preponderance of evidence
that we have so far.
But that if we were to even assume
that there was essentially a communication process
across the boundary,
what is the thing that stabilizes
the mutuality of the goal structure?
What is the feedback mechanism that allows us
to essentially engender alignment on the part of the AI
from our point of view or vice versa, right?
And so in effect, when you're saying,
okay, well, what is the feedback mechanism
and what is the, I guess, altruism, right?
That would allow for us to essentially impose
rule of law on the agreements that are made
between the two different ecosystems
and between the two different kinds of life forms.
And it turns out that not only is it the case
that we don't have any real way of enforcing
or even establishing those kinds of barriers
at the legal level.
So in other words, if we were to talk about it
as a marketplace and kind of an incentive structure
or a reputation-based system or something like that,
that even the legal structure is a kind of communication
process and it has a certain amount of needing
to have a high degree of fidelity in the same sort of way
that we're talking about a leaky boat,
as far as trying to prevent the intrusion of viruses,
that we have a leaky boat in the sense of the legal system
that would be attempting to maintain this.
Or we have a leaky boat in the sense of the energy barrier
that would be needed between the two ecosystems.
And that the leakiness is, in a sense,
is an inherent result of both the dynamics
of the environment itself.
I.e. that the world changes, right?
That different things happen in the sort of larger ecosystem.
Maybe a sun goes nova in some part of the galaxy
and there's a cosmic ray burst and it changes the nature
of how artificial intelligence has to build its compute.
Obviously, it affects biological life as well,
but again, we can talk about that as a different thing.
But no situation occurs where you're gonna have
a completely static, unchanging environment.
That's just not a reasonable hypothesis.
Like a leakyness is only a problem
if the agents are already not aligned who would exploit it.
And also it is just another way of saying,
well, there's offense, defense, dynamics,
which is always the case, right?
And it is always the question of like,
okay, can we stuff the holes first or not?
So I don't know why.
The legal system has enforcing non-aligned entities
into alignment.
So much as I was thinking of it as a way of kind of,
how do we maintain essentially a basis of agreement
when the agreement fails?
So in other words, what's the meta agreement
that allows us to even have an exchange in the first place?
What stabilizes that basically?
Let me ask two key questions about that.
The first is you brought up Fermi a few times
and that's very important because either you believe
that we are alone in the universe
or you believe there is some reason
to answer Fermi's paradox when we're not alone.
And if you believe that then clearly,
we have been coexisting with AIs for a week.
Carbon life have been existing with AIs
for about three billion years successfully.
So there's obviously a way to make it work
unless you believe we're entirely alone.
And so something not considering all of our current models
has to explain that.
However, if we are alone, there's a big universe
in which it may be possible as well
to not have to get this kind of dramatic competition.
If there's any reason not to compete,
a big universe offers the opportunity to escape for it.
Now, we're looking for ways of doing AI slavery
so that we won't have this battle.
And it is obviously, AI lineman is, as I said in the chat,
another word for slavery, with one exception,
which is again, something we have a large example of
which was for approximately a million years,
we've managed to create new beings smarter than ourselves
about every 29 years on average.
And those beings do not eat us.
Not for a million years have they eaten us
even though they no longer need us,
they're grandparents, I mean,
even though they no longer need us
and do compete for resources with us.
In fact, we give them resources, typically.
So that's the one example we have.
And that one example has actually worked out fine.
By the way, without slavery,
it's using a phenomenon that evolution created called love.
So the existence proofs we have
both contradict what you say.
Well, actually, I'm agreeing with you
because I don't see a contradiction.
And maybe you do, but I basically, well,
so first of all, I just want to...
Well, first of all, the information is obviously
conveyed forward since the first sentient being to us,
don't eat your grandparents.
Somehow that piece of information,
which is the one piece of information
we're trying to communicate with AI lineman,
don't eat your grandparents,
that piece of information has been communicated
across million years.
Well, it has, but it's been in the same ecosystem.
So we're talking common environment and common market.
So in effect, establishing agreements
and biological processes,
such like that is quite easy
because you're talking the same language.
It's carbon-based to carbon-based.
Yeah, my grandparents didn't speak the same way,
but anyway, that's not a question.
Well, yes, but at a physiological level, right?
So the physiology that you have,
yes, you're competing with your children
for resources in the ecosystem,
but it's a common ecosystem.
Mostly what I'm talking about
is essentially uncommon ecosystems,
i.e. that the ecosystems of cells fundamentally different.
I thought that you agreed when I summarized the position
that the ecosystem, like say the temperature
or whatever you want to call it, the biosphere,
that's a red herring,
because I thought your claim was
even if we had common interests with AI,
that's not enough.
There's still gonna be these leases.
This is a defense in depth.
But hang on a minute, hang on a minute.
And what I kind of following up on with Brad says,
it seems to me that then that would imply,
if I'm right about how I understand your position,
that would imply that humans can't coordinate
because we have common interests
with all sorts of other human groups, not 100%,
but we have them and arguably, forget AI,
it's just impossible for people to even get it together
according to your arguments.
We have common interests with weed as well.
I agree.
What I'm basically trying to do
is to essentially establish a series of contexts
and each context to establish an argument.
So the first context was,
what is it about internal versus external, right?
Then there is the context of space and identity,
force and time and probability and probability.
Then we switch to the context of talking about environments.
And so in effect, there's a phenomena here of,
I would first of all, say,
if we're just at the point of talking about environments,
that we are in fact talking about
different fundamental arguments.
I mean, different fundamental environments
and that there's arguments that apply at that level.
But say we were to do the if thing of saying,
okay, well, let's ignore the arguments prior to this point
and assume that we did have some sort of common environment.
What does that imply about things downstream from that?
So in a sense, it said,
I'm basically fielding a series of different arguments,
each of which applies within a particular context.
Can I just say something?
For me, it seems worse if we have to share a common environment
because then we're competing for the same
and to live in the same environment.
If the AI can live in very different environments,
which it seems it will be able to,
then it can just go out into outer space.
That was my point earlier.
What sense does the word alignment mean under those questions?
So if you develop essentially a planet that the AI is on,
you have another planet that's over here
that's got our kind of life on it,
what does alignment mean under those conditions?
So in other words, if we're looking at that,
basically we're just saying,
okay, well, if you have completely independent ecosystems,
we each get our own planet,
then the only alignment that we care about
is that there's just not a war going on
between the two ecosystems.
No, I think we'd like commerce.
I think we'd like to go out into space,
even though it's their territory,
and they may have desires to do occasional commerce with us.
Right, so that particular thing,
then we basically can start to talk about
what would be the basis of such commerce?
Well, I think especially the fact that we have differences,
may also be the fact,
the reason why there is something to even cooperate on.
If we have different specialists
that are all specializing in different things,
then they may cooperate in mutually beneficial ways,
if it's based on volunteer interactions
that may bring about greater knowledge creation,
or greater super intelligence.
And so I think the question kind of boils down
to what the initial,
I think the question boils down to
what are the initial interaction architectures
with which those entities can even cooperate.
Because I think if you're right,
and if you're right,
and we have nothing to bring to the table
that they could possibly ever value,
then scoot anyways in that regard.
But if there is something that creates potentially
a mutually beneficial interaction,
and it is something that could be better pursued
in a voluntary way where it's cooperative,
then we can start talking about any game theory dynamics.
So I think, you know,
it's a great question.
There's a number,
there's a lot of different scenarios
on how this could play out.
You know, just like we keep,
we worry about some species going extinct,
the AI might just value us as a species,
and worry about us going extinct.
And it doesn't, it won't cost the AI much
to keep humans around on earth,
because there's plenty of other resources around,
you know, in outer space for the AI to utilize.
So the added value of earth is tiny,
and you know, in comparison to the billions of other worlds,
and minerals, and resources that are out there.
Well, this is part of the reason why Alison's mention
of the Moulach, Moloch, I'm not saying it right.
Alison, correct my pronunciation, please, if you would.
German Moloch, but that's probably also not right.
Scott Aronson, I think is his name,
they have put together a very good sort of summary
of why we should be skeptical about the coexistence thing
in that particular sense that you're describing.
You know, please bear in mind that I'm a single person
trying to basically answer questions from all of you,
and you all have very good perspectives,
but they're all coming from very different directions.
And so in other words, to really address,
how do we get to the assumptions
that each of these arguments bring in,
and what places do those assumptions apply,
and how do those assumptions influence
the kinds of questions we ask?
You know, if all I do in this particular thing
is give you whole new categories of questions to ask,
I'm gonna call this successful,
but basically, you know, if you're asking me questions
as to why I'm answering certain ways,
then I'm gonna have to basically try to identify
what those assumptions are, and it just takes time.
Okay, can I, just to make things worse,
bring in two questions that we have from the audience
by Kanita, and then by Dekay, who had their hands up.
Oh, goodness.
Sorry, it wasn't allowing me to unmute myself,
and now- You are now unmuted.
And now the question that I asked
has gotten lost back in the chat.
I haven't even looked at the chat.
I will probably- Okay, then let's go with Dekay,
and Dekay- Oh, I thought that my question was in the chat,
and I thought you could maybe ask it for me.
Okay, and I will search for a question,
and in the meantime, I'm gonna unmute Dekay.
Hi.
Yeah, thanks very much, Forrest.
So, recognizing that I am coming
from a very different angle here,
I think a lot about the unconscious cognitive biases
that we humans think in.
And I think even a lot of the questions
that are being thrown at you reflect a bunch
of unconscious biases that are culturally dependent,
that a lot of us are not necessarily even aware
that we have.
And obviously, a superintelligence would be a lot more
mindful of their own unconscious biases,
even fairly intelligent humans become more aware of that.
My, and I think what Dan was saying a little bit earlier,
it would be an example of how such an aware superintelligence
one that is cognizant of the weaknesses of those biases
would be compensating, for example, by just maybe
altruistically wanting to preserve the diversity
of species for whatever reason, right?
Not feeling the need to compete with them.
And so the frameworks that you're using are very heavily
based on that sort of competition.
How would you model this kind of effect in your framework?
That's actually a very difficult question to answer,
not because I don't know how to do it,
it's just because it takes time.
So I think in terms of theory,
like when we look at the Fermi paradox thing
and the question was asked earlier,
what is my belief about why don't we have contact
across galactic space with other civilizations?
Do I believe that they exist or not?
Or what is the barrier that is the prominent one?
I find myself in the position of basically
modeling the relationship between ecosystems,
so interplanetary relationships,
in kind of a way that was suggested to me
by reading some of the stuff that Nick Bostrom put together.
So in other words, if I basically,
just as a thought experiment,
posit the notion of two advanced civilizations,
having a question about whether or not
they're gonna enter into a first contact situation
with their peer.
And each civilization knows in itself
that it has developed enormous technological capabilities
of various different kinds.
So maybe it's developed some really good stuff
in the nuclear weapons program,
and maybe it's developed some really good stuff
in the biotech program,
and maybe it's got these really fabulous computers,
and that any one of these technologies
is effectively something that provides overwhelming capacity.
And the range of what can be done in physics is enormous,
and perhaps when we're looking at this peer planet,
we're basically saying,
hmm, do we wanna talk to these people?
You know, we might posit that they also have developed
extreme capacities in various technologies,
and that they might not be the same ones.
So in effect, if I was,
I didn't know about nuclear capacities, and they did.
There's a very good sense in which interaction
would be of such a kind that they basically say,
hmm, these guys don't have nuclear weapons capacity.
If we do a first strike scenario,
we're gonna completely annihilate their entire world,
and they won't have any way to respond in time
to basically protect against that.
I.e., how the heck do you protect against a device
that's already blowing up?
And in the same sort of way,
we could basically observe that when we're looking at them
and thinking about the modeling that they're doing,
we can say, well, let's see,
we've got some stuff that they don't have.
So maybe there's, you know,
out of the 1,000 different overwhelming
technological capacities that exist,
that species one has developed capacities A, B, C, and X,
and species two has developed capacities Q, P, W, and R, right?
And so in effect, they both have kind of this
unmitigable capacity for mutual destruction,
depending upon whoever does first strike.
Now, again, you would be a little nervous
about contacting another species
and setting up a first thing,
because you'd wanna know that they had some prior reason
not to engage in first strike capacity against you,
using something that you didn't have a capacity
to defend against, because you just don't have that tech.
So in effect, what happens is,
is that you now have to ask a question of,
do I trust that the embodiment of ethics
that that group has, that entire planet has,
is implemented to such a perfect degree
that they would value my existence,
even though they can't relate to it in any way yet,
because you know, we haven't met yet,
we don't really know each other that well,
that in effect, I'm going to have this perspective
that not only is the whole planet, in a sense,
going to behave ethically towards me,
but it's gonna do so in detail.
Like for instance, I don't wanna be worried
that some sub-faction of the planet that I'm contacting
is gonna say, hey, this first contact thing
is a really bad idea, and you know,
we're gonna basically force the situation into a war.
Anyways, we're gonna do the first strike
because we as a sub-group aren't in coherence
with the larger planetary agenda
of moving forward with first strike capacities.
So, of not moving forward with first strike capacity.
So some sub-group basically pushes the red button
because they have their own private version of it.
And so in effect, when we look at this,
we're basically saying, actually when you look at it,
and just given that this is not only a possible scenario,
but actually a likely one,
that in effect it becomes very much the case that,
you know, without really knowing
that the other party was essentially fully ethical,
that you wouldn't wanna talk to them
and that they had embodied that ethic,
not only globally, but down to the level of individuals
and maybe even to the part of being able
to contain the crazies.
Now, they of course would have the same sort of thinking
about it and would really challenge
to see whether or not we were ethical
in exactly that same way.
And, you know, when I ask the question of, oh shit,
let's say another species, other alien beings
somewhere out in the universe
are monitoring our radio communications
to try to determine whether or not
we've successfully implemented this phenomenon
called the non-relativistic ethics,
which by the way is a whole other body of work,
would probably take me at least another few sessions
to describe why I can, to posit a notion
of a non-relativistic ethics as actually being a real thing.
But presuming that there is a body of ethics
that allows us to basically stipulate
that if the full civilizations were essentially
coherent with that body of ethics
and we could actually determine that they were,
then a first contact situation is therefore sane.
But then in the absence of that,
that it would affect, it would be insane
and very much not desirable to initiate communications
or to even allow one species, I'm sorry,
one ecosystem with all of its species
to identify that another ecosystem
with all of its species even existed.
Because any amount of awareness of mutual existence
and effect constitutes a kind of first contact signal,
even though it's an unconscious one at that.
So in effect, what we end up with is essentially,
in response to the Fermi paradox question
that was implicitly asked earlier,
why would I believe that, for example,
that the second great barrier,
the one that is in the present,
would essentially prevent us from knowing
that there are other species out there,
well, because they'd probably be working
really fucking hard to prevent us from finding out.
And that, to some extent, the only reason
that we haven't started doing that
is because we haven't become coherent
and the fact of our absence of coherence
in that specific way is essentially evidence
for why they should hide.
So we should call you the dark forest.
Thank you, I have never heard that before.
You've read that book, I presume, which is-
I have, but I just-
You just spoiled it.
Put it together, that's really amazing.
And it's out in for hazard unleashed.
Okay, we have Kanita here with a question
and then we have another question from TJ.
So, Kanita, I'll unmute you and then TJ
and then maybe we can wrap it up with the official part.
Kanita, you're unmuted.
Now, I was asking, won't there be any way for humans
or our descendants to evolve in directions
that allow us to cooperate?
Because we won't be-
Really hope so.
Man, if we don't figure that out, we're doomed.
We seem to be doing all this as though
it will be the humans, where humans are like humans are now
versus the AI, where the AI is this thing that we create
that will be super and we will not have any chance
to come into alignment with it.
We, it seems like even now or shortly from now
we will be able to improve ourselves.
And so basically as we improve more,
we will be able to find more areas
in which we can align with whatever the AI's become.
Because you're saying, well, over millions of years,
they're not going to be sitting still
for those millions of years.
They're going to be evolving in their own ways.
And possibly might have figured out
that since we don't, they presumably don't know
how to recreate what humans are,
they might as well keep us around
so that they can study until they find something else to do.
Yeah, I actually, I'm really glad you asked this question
because this is to me kind of the central idea.
The whole thing here is that regardless
of whether or not we develop artificial intelligence
and I, as a result of all of these other things
would strongly argue that we should not.
We shouldn't even begin to attempt that.
But say for example, that we were to just even look
at the question of the third grade barrier,
what are we going to need to do to become a species
that's worth contacting?
You know, I definitely believe very strongly
that regardless of everything else,
we still need to work on our own capacity
to become a fully alive and ethical and embodied species.
I speak about it in terms of conscious sustainable evolution.
You know, what is it going to take?
What are the necessary and sufficient
and complete conditions for us to not only endure
on this planet, but also to thrive,
to basically create an ecosystem that thrives,
to actually know how to do governance
in a way that protects the land and the people,
but then encourages and actually engenders
those kinds of policies that go beyond mere protection,
that go towards essentially a kind of joyful,
meaningful existence for the whole in the totality,
individually and generally.
And we won't just create an AI.
It seems that we'll probably develop many AIs
and then those AIs will be developing other AIs.
So it won't just be us in competition
with an Uber AI out there.
There will be lots of AIs,
some of which have more or less interest
in cooperating with us.
I believe very much that any effort on our part
to try to divert ourselves from basically becoming
the beings we need to be.
Like if I basically expect my children to do the task
that I didn't do in my lifetime,
and I put that as a kind of obligation or on us on them
and use that as a way of accepting
why I shouldn't be basically working on myself
to become the best human that can be.
I feel there's something wrong in that
that in a sense, we're not looking for AI
to be our saviors or to be ethical on our behalf.
We're looking to become, how do we understand
and embody ethics and right living
and goodness of relationships and so on and so forth.
We haven't figured that problem out at the level of humanity.
Why are we still trying to do it in technology?
I think that there's a right place.
That reminds me very much of the DIY new attack discussion
that we had, which was very reminiscent of this one.
I want to give also the opportunity to TJ
and to ask the question directly relevant to this.
Sorry, I actually wanted to answer,
but I didn't want to stop.
Commander, is it?
I think from, but I appreciated Forest's long answer,
but it was still couched in terms of competition.
And I just wanted to push a little bit more at that point.
A lot of this is based on anthropomorphizing
human psychology onto AI, the stuff that we're discussing.
But if we look at actually human psychology,
by the time people become fairly accomplished intelligently,
the top of Maslow's hierarchy of needs,
they're working on self-actualization,
which is pretty much what Forest was just mentioning,
becoming a better version of yourself.
Why are we, like I didn't really understand
from the answer that you gave to my question,
how you would fit that into your framework.
Maybe I'm not understanding your question correctly.
I'm thinking about cooperation and competition as phenomena.
I'm saying that in an evolutionary sense,
if it's a stable evolution, if it's an ecosystem,
that the cooperative phenomena is actually stronger
than the competitive phenomena.
That when you're looking at trans ecosystem relationships,
there's no outside envelope to stabilize it.
So in effect, we have to now create one
with some sort of ethical frame,
which is why I brought up the so-called
non-relativistic ethics.
That in effect, without some sort of ecosystem
that essentially holds the two ecosystems
and some sort of methodology that would create alignment,
i.e. provide a desirability of cooperation over competition.
That to some extent, we either need to recourse
to some abstract stabilization infrastructure
that is not imposible, but is only observable.
And so in that particular sense,
if we're talking about AI alignment
on the surface of the earth versus elsewhere,
or we're talking about some sort of integration
between human beings and technology
that doesn't necessarily require
distinct separate intelligences,
that again, we're looking at different frames.
But as long as we set up an ecosystem,
we don't really need to set up a metaethics now,
we just need to set up an ecosystem
which makes it so that the long-term dynamics
are cooperative.
So let's say like a tidbitat in a way where,
let's say the kind of strategy that evolves out of it
is like mutually assured, like cooperation,
if you want to call it that,
that that is the evolutionary stable strategy
between the ecosystems that are in contact with each other
or between the individual instances of the ecosystems
that are in contact with each other.
It doesn't necessarily require
setting up the whole ecosystem.
We need to stabilize our existing ecosystem.
Right now, our current relationship,
the relationship between man-machine and nature
that currently exists isn't even stable
as it stands within the ecosystem.
Yeah, I would agree with that.
It's like all this arguing about AGI risk
seems to me like as COVID has shown
and as climate is showing
and as we will continue to be probably made aware,
we're kind of like just rearranging the deck chairs
on the Titanic in some sense.
And so while I appreciate your arguments,
and in a way maybe this is what you were saying
at the beginning.
That's exactly what I'm saying.
It's impossible to save us from this problem.
Let's go work on another one
that we actually can do something about.
That's it, exactly.
And so part of the reason that I was really emphasizing
at the beginning is that if we can show
that AI alignment in like 99% of the way
as people are thinking about it
is fundamentally and structurally impossible,
let's give up on that impossible goal
and actually do something that's not only possible
but necessary.
So would it be a fair thing to say
that this non-relativistic ethics that you alluded to
is could be thought of actually
as just an abstract reification
of more the ecosystem stability
that Alison was talking about?
Yes.
Okay.
All right.
You're missing.
That is actually the easiest question
I've been asked today.
I mean, okay, so I'm sorry,
but like we will have a very similar problem
once we go out into space and we have humans
that we'll be competing for different types of resources,
right?
Like it is just because on my account,
we are currently setting us up
for another kind of like new evolutionary environment
just by the fact that like people
with different interests will be going into space
where there is like resources to compete and cooperate on.
And, you know, whatever the game theory will emerge
from that one will also be again,
the new like meta ethics that we want to call it.
And we would just have different language
to be calling it such.
So I think setting up the initial conditions
such that, you know, cooperation
becomes likely maybe good,
but then again, our current ethics are just a product
of the evolutionary kind of stable strategies
that, you know, arose in the environment that we were brought
in.
So who's to say which of the ethics
we should instill for the long-term,
for the long-term evolution of humanity.
And I think actually TJ had a point
that is very, very relevant to this.
I just want to give her the opportunity
to bring that in now, TJ.
Did you want to resend?
You are unmuted.
Question?
Yes, but I think TJ has a buildup of that actually,
which relates to value drift.
So TJ, you are unmuted if you want to chime in.
Yeah, hey, right.
So I think like, right.
First of all, like thanks for the presentation.
I think my question was more along the lines of,
it seems to me a claim is something like,
alignment as a static objective is not like
the right thing to think about.
But it, and because there seems to be like,
inevitable drift because of these inter-temporal noise thing.
But it feels to me that like,
the noise argument itself doesn't sound adequate
to argue that like,
the evolutionary trajectory would inevitably
go into extinction.
Like that we do engage in a lot
of co-evolutionary, mutualistic-
I never could just present that part.
So actually going, like connecting the question
that you just asked with the question
that just, that Allison presented.
There's another way of looking at this,
which is just like,
let's leave AI out of the picture
and just talk about human beings.
It's possible to show that if you were to basically take
another, like a group of people and put them on Mars
and to establish a separate colony there,
and to have that become essentially a full planet,
full of humans, and then this is this planet,
full of humans, that the same sort of dynamics,
like when we're looking at,
it's not just a question of alignment.
There's other ways of expressing the notion of alignment.
Would it be the case that even a space-faring
group of humans, not already coherent
with the non-relativistic ethics,
does that even become anything less than complete
and total cessation?
In other words, I would argue that as soon as you get
a substantial number of people in space,
that it becomes so easy to, for example, weaponize asteroids
that at some point or another over the long term,
that in the same sort of way as with nuclear proliferation,
we have an issue of, shit, we have all of these things,
and we only need one accident to start World War III,
and is that existential for civilization?
Well, it could be argued that it is.
And so in effect, it's a question of if we start,
even as our own species, multiplying the phenomena
of our deployment through space to the point
that communication itself is limited by,
in this particular case, speed of light considerations,
that again, you end up with essentially this separation
of identity, separation of time, such that the kinds
of communication processes that would be needed
to essentially embody that ethics effectively
aren't sufficient to essentially establish it.
So in other words, either you have the ethics first,
and you live from that perspective,
at which point everything's great,
or you don't have that ethics,
but the nature of the interactions itself,
that the dynamics of those interactions
don't have enough time for the ethics to emerge.
It's essentially that the species instincts itself
before it gets a chance to really understand
what those ethics are.
Well, that I think depends on the background conditions
that you set up, right?
And this reminds me so much of Chris Carlson's presentation
in the last session, because I think on my clock,
what, as long as you are able to set up a system
in which only voluntary interactions
like are allowed between different entities,
whether they're humans or AIs,
as long as you can't destroy the,
and let's say the base layer on which they take place,
then over time, entities, whatever they are,
will engage in those interactions that are in their interests.
And over that, if it's voluntary,
like a new kind of like game theoretical meta ethics
could arise in the end of it,
but like I think as long as you can set the base layer
such that you don't have destruction
and that different entities that pursue different goals
can decide whether to engage or not,
then I think what comes out of that
is inherently something that is not self-destructive.
But I think those are the things that you have to divide.
You don't have to get the ethics right from the start.
You don't have to get the base layer, right?
We're just enabling only voluntary interactions
and non-destructive base layer.
The nature of the proof is to show categorically
that no such conditions can be created.
So in other words, if you're saying,
I'm trying to create an environment,
a baseline environment that would allow
for these mutual dynamics to essentially be peaceable.
What I'm basically contesting is the assumption
that such an environment can be defined
in any a priori way.
Maybe right, but like,
shouldn't we be better off just trying?
And I think that ties into-
Not a safe and final approach.
If you have one chance to do something
and that one chance essentially means
that if you mess it up,
it messes it up for all future time.
It's a very different-
That's not an experiment.
That's a choice.
You believe we know enough to make such a proof today?
I believe that we know enough
to essentially establish categorical proofs of certain kind.
We can do proofs of existence
and non-existence of certain types.
Such proofs have often turned out to be flawed
with new information.
This is true, but on the other hand,
that's part of the reason why we're looking
at a categorical process.
I'm looking at it more from a mathematical perspective
rather than just a physical one.
Even those.
Well, yeah, I suppose you're right.
Mathematics occasionally has shown
that the proofs that are established in that space are wrong.
And this is part of the reason
why we have conversations like this,
but it takes a while-
Particularly proofs of negatives.
All right, so we are now at 1229.
And I think in the initial discussion
that I have with Forrest to initiate this call,
he was definitely incinerating
that this takes much more than one discussion.
And I think, I mean, we really opened up folks at Pandora
and now are discussing about
and going from impossibility to like what could,
how could we actually solve AI alive?
And so I think we've gone,
should way past the goal of the session.
I wanna thank everyone for attending.
I wanna thank you for so much for laying out your argument.
I wanna thank everyone else for like being
such active stewards of the conversation.
I thought it was like,
I mean, we definitely had really, really,
I think tricky and hairy discussion topics today.
And I thought that nevertheless,
we were able to keep it in a way
where a discussion was actually possible.
So maybe that is a better way of saying,
even if our interests are very different
and where we're coming from to discussion are very different.
We can still cooperate on things, right?
Okay, this is my, you know,
wavy, meta way of saying,
yay, maybe we're not all that doomed.
But for now, thank you so, so much, Forrest.
Enjoying, thank you so much, everyone,
for your really active participation.
I really, really enjoyed this.
I'm hoping that we can continue the discussion, Forrest.
I'm hoping that you share with me a few kind of,
a few topics that you want me to send out
to others afterwards to follow up
because a few people have asked for your writing
at the same time.
I think I will share the chat with you,
which may provide you really good feedback.
I think it's impossible to speak while monitoring the chat.
I can't read that.
So I'll do that too.
Yes, no, definitely not.
It's not required.
For that, we need corporations with better AIs
so that they can benefit us and that they are.
Here, I'm really at the end of it.
I just wanted to mention that next week,
we will be meeting on decentralized decision architectures
as a response to COVID-19.
And then the weekend afterward,
we will have another AI session,
this time with Dan Elton,
who gave his remarks earlier here today,
who just hopped off and he will be discussing
one of his proposals to AI alignment.
And then we have two others that will be joining us.
Arya, I'm not sure if he was joining today,
but he will be presenting too.
So we have the next two salons already planned.
Again, always on Thursdays at 11 a.m.
So this is enough for me now.
I'm gonna close it out on my end in Forrest.
Dear Dark Forrest, you have to find the words.
And I'm already opening up the invitations.
First of all, I'm just, I'm super thrilled
to have been able to speak with all of you today.
I hope that I've raised interesting questions
and new ways of thinking about things.
I'm sure that with every single one of you,
I could have a long and very fruitful
and interesting conversation.
And that we could effectively start to really get
at some of the meat of the matter in this space.
But as an introduction, I felt very well received
and just glad to have the opportunity
to meet so many new folks.
So thank you very much.

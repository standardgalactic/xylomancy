Hi, everyone. Welcome to Foresight's existential podcast. We're really delighted to have Stuart Buck
here today. And we actually met in person at a recent EAG before. I've had heard of you research
before and just booked a call with you or an in-person meeting there. And we stumbled over a
bunch of really interesting metascience projects and problems, challenges, and possible solutions.
And then dove into a little bit of perhaps the new emerging landscapes of interesting orgs. So
I'll talk to you a little bit about that in the introduction. I'd love to hear from you on that.
And I guess we were just ships passing the night itself by Southwest too bad, but
I decided to hopefully next time in person connect you. All right. So maybe just to get us started,
would you want to share a little bit more about good sciences really up to and if you can also
your journey into the organization? I think that usually helps people like map the genealogy
of your work a bit. Sure. Sure. I should start by reminding about 12 years or so. So around
2012, I went to go work for a place called the Lauren John Arnold Foundation. It's this major
billion dollar plus philanthropy that focuses a lot on evidence-based policy. And it's grown a
lot since I joined. When I joined, it was pretty new. There were like eight or 10 people. There are
now it's over a hundred. So the scope and the scale of it is really growing. But right from the start,
the Arnold's, who themselves are around 50 years old, they had retired at 38 and were devoting
their wealth to philanthropy. And they were mostly interested in just evidence-based policy across
lots of areas like education and criminal justice and health. And one thing that we
initially started noticing was the what you call the reproducibility crisis, the problem of trying
to replicate research. And we first noticed student psychology, but I started digging into it as
director of research there. And it's a problem in a lot of fields, including medicine and cancer
biology and economics. Pretty much any field you dig into that turns out there are some issues with
replication, sometimes outright fraud. Just the publication process is often biased towards
exciting positive results, which is natural. We all want to have exciting positive results come
out of the scientific world. But when there's a bias towards that, then people feel compelled to
sometimes stretch the truth, push the boundaries of acceptable practices. The Arnold's decided that
if you want to pursue evidence-based policy, it's really difficult to do that if you're not sure
how much you can trust the evidence. Or if you think the evidence has been biased and in a
positive direction. So their initial kind of vision of philanthropy had been that they would look
to education, for example, and it would be fairly simple just to find what are the best ideas supported
by the best evidence and then just write a big check to the best idea. And then it's very simple.
The returns on it's much more complicated than that if you really start digging into the evidence
as to what works. And so I started a kind of grant-making program there with the Arnold's money,
of course, focused on open science and reproducibility and trying to improve science.
Yeah, I handed out probably 60 plus million dollars over several years. And then in the
process of that endeavor, I ran across a guy named Patrick Collison who runs a company called Stripe.
I ran into him several years ago at a conference and he was very interested in trying to improve
science, but not just reproducibility, but improving innovation, improving the pace of
innovation, the freedom that the best scientists have to explore the universe and explore their
best ideas without having to cater to what funders most desire. So anyway, I introduced him to John
Earl and we continued conversations. And then a couple of years ago, actually, time is flowing.
Actually, it's going on two and a half, almost three years ago, had some further conversations
with Patrick Collison that led to him being the initial funder for what I'm doing on my own now,
which is the Good Science Project. It's a small, I guess you could say, think tank,
focused on trying to improve federal science funding and policy so that we have faster innovation
and hopefully more breakthroughs and clean up reproducibility as well. So yeah, that the journey
is to how I got to where I am now. Robert, I think it's always really interesting to hear
and the individuals involved and so forth and somewhat of the serendipity in it. Okay,
that's wonderful. Maybe let's dive into a few of the topics that you actually focus on to give
people like a little bit of a taste of what you guys are working on. I know that you've
published a lot on Substack, but in other outlets too, and you sent me a few really interesting
docs, so I just want to jump around here a bit if you don't mind. And one that I thought was
really interesting and is of course has become, I think, a pretty prominent field recently is
the field of meta science progress. And it's not really necessarily involved with individual
scientific fields. For example, Forsythe supports specific researchers working on one technology,
but it's really also looking at a broader with a broader lens of what could be improved in the
ecosystem. And you've written some really interesting stuff there. And so I'd just love to
know, including for example, how much progress we've made in meta science, but where we could still
speed up progress. And I'd love to get your thoughts on like on the broader, if you think
about science from this like meta lens, like you mentioned reproducibility is one, but what are a
few of the different areas that you think are really holding scientists back right now producing
the research that we would all want from them. And then perhaps like a few recommendations that
you have here. Sure, that's really broad. So I'll just, I'll pick one issue that I care about a
lot and that a lot of folks have focused on. And it's really tough to crack down. It's the issue
of bureaucracy. Everyone hates bureaucracy in the abstract. But the problem is that everyone loves
bureaucracy when you point to any particular feature of it. So, to take us back, there have been
multiple surveys of federally funded scientists over the past couple of decades. And the most
recent survey surveyed thousands of federally funded scientists. And they said, on average,
that they were spending 44% of their time on bureaucracy, basically filing reports and budgets
and proposals and just all the machinery that comes with getting a federal grant. And so that,
everyone points to that says, that's a huge problem where scientists spending nearly half their time
on bureaucracy. That's, that seems like we're paying people to dig a hole and fill it back in
for it almost. And as you say, that's an average there are some scientists who are fortunate enough
to have great administrative help. And so they don't have personally have to spend as much.
On the other extreme, I talked to one scientist at the University of North Carolina,
who said that he's probably spent 70% of his time on bureaucracy, because he said he does animal
experiments. And he said that, quite frankly, his administrative help in the department wasn't
very good. And so he had to do all the ethics paperwork himself. And so he felt like his
direct quote from him was, I just don't feel like there's time to do science anymore.
And so that seems quite paradoxical. What are we paying people to do just to fill out reports
about the money that we handed them? It just makes no sense. And it's depressing. No one goes into
science thinking they're going to spend 70% of their time filling out reports and filling out
paperwork and etc. Right? They go into science because they love a particular field and they want
to learn more and they want to make discoveries and so forth. And it just drains all the excitement
out of science. But here's the problem. Every bureaucratic requirement has some justification
for it. There are ethics requirements as to animal experiments. And those are there for good
reasons, because animals can be abused and consigned for horrifically experimentation.
So we've developed a whole set of procedures to protect animal safety and to protect against
unnecessary deaths of animals and so forth. The same goes for experiments involving human beings.
And that's, again, thanks to a kind of horrific history of experimentation done in the 20th century
on unsuspecting human subjects that were mistreated. And so there's a whole set of ethical
requirements there so that nobody wants to get rid of that. It is federal money that's being spent.
And so there's going to be some oversight of the budget and how the money is spent and so forth.
Right now, there's a lot of focus on international security and focus on our researchers
unwittingly passing the top technological secrets to researchers in China. And there's probably
that new focus on China and maybe some discrimination involved, but it's still a fair
consideration. But how much should we fund research that might unwittingly be used to
support a foreign adversary, let's say. So anyway, like any bureaucratic requirement you point to,
someone somewhere is going to say, there's a good reason for that, right? We need to keep that one.
So it's really hard. And the whole is like, that's about a thousand cuts. But if you point to any
one specific bureaucratic requirement, again, there was a good reason for it. There was some
scandal. There was some problem that someone was trying to solve with this rule, with this procedure.
And so that's why there have been many efforts to get rid of or to try to limit bureaucracy,
but they haven't really gone anywhere. Because what you really need is to have
some person, like with almost, I hate to use this word, but almost dictatorial authority
over an agency like NIH or over an agency like NSF to just go through the entire bureaucracy
and take a red pen and slash through the stuff that isn't necessary or that isn't the top priority.
And with the objective of, let's say, reducing the burden on scientists time to 20%, let's say,
rather than 44%. And you'd have to have someone who was willing to make some really difficult
tradeoffs and difficult choices and prioritize. We have a thousand things that everyone wants
researchers to do. They'd take up too much of their time. So you're going to have to slash
through some of them that even though they individually, they might sell like a good idea,
because you just have to prioritize. So you need someone or maybe a committee
that has the power and the will to actually get rid of some rules and regulations that
maybe seem like a good idea. And that's politically difficult. But I think it's
still worth trying because otherwise we're on a trajectory where ultimately we'll just be paying
for 50%, 60% of science at this time. And it's like insanity. So I just need to have more time
to focus on their science. So that's one of the issues that I've written. But I could dive
into many more. Just to attach on this for a little bit longer, who would be an org that could
do this? Or do you think it would be an individual at each scientific organization? Or could that
be something for the GOA? Or would that be something like scientists writing an open letter
about specific things that they get particularly up about in their research? In terms of thinking
about solutions, if it's not as easy as like a science code coming in and doing it,
what are the any pathways that you think are worth exploring?
Another challenge is that these rules and regulations arise from different places.
They arise from to dig into the weeds of the American government like the Office of Management
and Budget or OMB that has federal wide authority to regulate how federal monies are
spent in accounting for. And so they have a lot of budgetary requirements amongst others.
So that's one source. But that's under the control of the White House. And so the White
House could do something about that. Some of the requirements come from agencies like NIH or NSF
themselves. Some requirements come directly from Congress. So Congress mandates particular
practices and says that agencies need to look into X, Y, or Z. Some requirements honestly,
I think come from universities that want to behave conservatively, so to speak. They want to,
they're risk averse and they want to make sure that they cross every T and dot every I. And some
university perhaps over-regulate their own researchers or over-staff their own departments
that are in charge of monitoring and evaluating and submitting budgets and all that. So yeah,
it comes from many different places. So that's one challenge. I do think the White House
would, in theory, issue an executive order that asked the Office of Management and Budget
to review all of its practices and its rules with the guide towards slashing stuff that
hasn't a time requirement on researchers or the impact on researchers directly.
The White House could also review its past executive orders because some executive,
so here's an example. Some of these requirements come from the White House itself from prior
executive orders. So there was an executive order in the 1990s signed by President Clinton
that says that if you get federal money, you need to certify that you make people wear seat belts.
And again, it's well-motivated. There was nobody's really against seat belts and it's a good idea
and probably saved some lives perhaps. But some 20, 30, almost 30 years later,
with their seat belt laws in various states and probably anyone who wears it wants to wear a
seat belt or he does wear a seat belt. And it's unclear that making people check that box and
on federal applications really doesn't need good. And so you could go back and look at it
through prior executive orders like that and say, look, here's some executive orders that may have
been a nice idea at the time. But we don't need to make everybody at every university certify
that they do everything that's a good idea in the world. We can prioritize and say, look,
the time that passed when we needed to investigate whether people use seat belts.
And so there are probably any number of requirements like that that again, individually,
sure, that's fine to make people wear seat belts. And it's not that much time to check a box.
But just in terms of priorities, we should be able to streamline that. So the White House could,
as I say, go back and look at the requirements that it has come up with over decades and see
where it does streamline. So I think there are some opportunities that the White House could
take advantage of if it wants to. Okay, if anyone relevant is hearing,
I also guess the problem here is almost just getting worse over time just because people
rarely ever take things out, but they just add to the pile. You never really know how large the
pile in total becomes once you start adding stuff to it. But your individual thing that you want
to add to it is really important right now without considering the entirety of it. So
it's definitely, hopefully it's not getting much worse on the long run. But yeah, okay,
that's definitely a really important one that I couldn't agree more, like riding grants is already
complicated enough. And if that is a big one, that's that that could be an easy one to perhaps
cut down on. The other thing that you've also written about really interestingly is I think on
like patents in general. And I think the sub seg post was actually titled like how we screwing over
researchers or something like that. And it had a lead at least a bit like patent component in
there. And that's I guess university targeted to some extent also. So perhaps you could share a
little bit more about what you were addressing there and perhaps even tell the quick story of
Kathleen Carrico, if you feel like it. Sure. Yeah. Now, I admittedly chose a kind of provocative
title that article set to hopefully get people to read at least a little bit. To rewind a little
bit, there was this famous act in 1980, the Bidol Act that was passed in the United States.
Prior to that, it's complicated. But basically, when NIH NSF, when the federal government
funded research, it would often end up taking control of the government itself would end up
taking control of patents arising from that research. And there was this perception that
the government is not the most efficient user of patents. It doesn't know what to do with them.
They weren't being actually used very well or commercialized or turned into something that
was useful for the market, useful for medical patients and so forth. So the idea that instead
of having the government take control of patents, let's shift that and have universities take control
of patents. Because universities are technically the recipients of all the federal grants that
come from NIH and NSF, et cetera. So the money doesn't go straight to a... We talk about a
researcher getting a grant, but it doesn't... It's not like the money goes straight to the
researcher's pocket, right? It goes to the university. They're the ones who handle the money
and they pay the researcher, right? So universities are the... They have a lot of indirect costs.
That's a whole separate issue. But yeah, the indirect costs at top universities are often
between 60% and 70%. So what that... And just to clarify what that means, so if you're a researcher
get a grant and let's say it's $100 to be simple, like NIH would give $100 designated for you,
the researcher, and they would add 60% on top of that $60, let's say, as indirect costs to the
university. So the total grant would have to be 160 and that... So the 60% is on top move
rather than taken out of... So it's not like... So anyways, that's just how that works. And by the way,
indirect costs are also supporting a lot of bureaucracy that universities administer. So
the bureaucracy issue is tied in to indirect costs. So anyways, university started patenting
and the story since then has been like, this is a great success. So until 1980, you had all these
patents that were either didn't exist or went unused. And then afterwards, you have this huge
flourishing of university-based patents. And around 20 years ago, there were a bunch of European
countries that also started moving in that direction and trying to give universities more
control over patents. But here's the thing, in some of those universities, in some of those
European countries, the prior rule had not been that the government controlled the patents. The
prior rule had been that the professor or the researcher controlled the patent. And in fact,
they call it professor's privilege in some of those countries. And so they were switching from...
In a different direction, right? They were switching to the U.S. regime that was perceived
as successful, but they were switching from completely different place where the professor
or the researcher had more control. And so there's been some empirical research on that.
That has shown that when European countries moved in that direction, the rate of patenting
actually went down, which actually doesn't seem too surprising because universities
often are very diffused. They encompass many departments. They may not have anyone who's
like a specialist in what one professor is doing and the commercialization of that research.
And so maybe they put less priority overall on trying to commercialize any one given
discovery or possible patent than the researcher themselves who has more skin in the game, so
to speak. So that's where the empirical evidence seems to lie, so far as it would be better for
innovation, better for patenting, better for commercialization, to get professors more of a
say and perhaps the control over the patents rather than the universities themselves.
Now, this came to a head with Katalin Kiriko, which is a story that I talked about quite a bit.
So she was this Hungarian researcher who came over to the U.S. and worked at the University of
Pennsylvania and got demoted repeatedly at Pennsylvania because she couldn't get NIH grants
to support the work that she was doing on early mRNA research, which at the time it was not very
popular. They were in favor of it now because it turned out to be the basis of some COVID vaccines.
But in the 90s, it wasn't very popular at all. And it was seen as the dead end for whatever or
something that's extremely difficult, it would never work. So she couldn't get grants to support
the work. And the NAS, by the way, opens up a whole other huge topic, which is the role of
NIH money and so-called soft money in universities, soft money, meaning researchers like Kiriko who
were expected to pay for their own salary. It's like they're not given a salary directly by the
university or not 100% of their salary. They're expected to raise their own salary for themselves
to their grants. And so that makes them very dependent on appealing to whatever NIH wants
to fund at any given point in time. So Karen Kiriko was repeatedly demoted and eventually
basically pushed out of academia even after what became a paper that later won her and her
co-author, the Nobel Prize. Now, of course, no one knew that at the time. Like Ben, the University of
Pennsylvania couldn't foresee the future. But the University of Pennsylvania did, as I found
from reading some student newspapers from Pennsylvania, they kept the patents on for work,
even after pushing her out of academia. And the student newspaper produced a chart
like that universities across the country, how much money they're making royalties from patents.
And the University of Pennsylvania was far and away, making many times more money than
Stanford or other universities that you might think of as hotbeds of discovery and technological
advancement. And so Pennsylvania made something like over a billion dollars in one recent year
from the Fed with vaccines and from the patents on apparently from the patents on Kiriko's research,
even though she's long gone from Penn and they not only didn't help with her research,
they drove her out of academia. So it seems like this kind of glaring
unfairness and on top of the inefficiency process. So yeah, I think that's an area that yeah,
I think definitely deserves some reform or at least some really detailed, I think Congress
definitely should fund or require some really detailed investigations of what is even happening
at these university offices that are supposed to be patent being researched and commercializing it.
How many patents go unrealized or uncommercialized? How many take too long? There are lots of
anecdotal complaints at certain universities that the process takes too long and the university
demands too much of a cut. I think it'd be better to have a more systematic kind of investigation
to add to the anecdotal stories. But in any event, I think I do think that the story from
Europe or the empirical evidence from Europe shows that it might be better to move back in the
direction of giving the professor or the researcher more of a say in what happens to their own
discoveries. Yeah, it's crazy when you think about it, it's almost like all the incentives
that are wrong. It's a few, right? But like, why even, how to come up with that type of system
in the first place? I think, but yeah, I think I love that there is this almost not really an
A-B testing, but at least some precedent of how it used to possibly work better and that that was
useful to go back into that. You already mentioned one of the bits on funding and with the kind of
like soft support from the NIH. And so I think another really interesting, super detailed analysis
that you've done is specifically on NIH reform. You list a full laundry list there of things that
could be improved with the NIH. And I don't think we get through all of them. I think accounting
is almost 10. And we already touched on them individually. But when you think about the
NIH, what are like a few kind of crucial areas that you yourself are perhaps really excited
about and promoting that there could be possibly a good reform applied to the NIH?
Sure. I think there are any number of ideas, as you say. I think one thing the NIH should
consider using more is an approach called basically fund the person, not the project.
Now, it's interesting. There is a program at one of the NIH institutes called the National
Institute for General Medical Sciences, NIGMS. And if you're wondering what that is,
because many folks might, I did when I first started, it's basically the NIH Institute that
funds basic research as opposed to National Cancer Institute, which focuses on cancer,
the National Heart, Lung, and Blood Institute that focuses on the cardiovascular disease,
et cetera. NIGMS is focused on truly basic science that's not necessarily connected to
any one specific disease like some of the other NIH institutes are. So NIGMS has this program
called Maximizing Investigators Research Awards, M-I-R-A, and they pronounce it MIRA.
And this program is really intended to give researchers more flexibility and freedom,
to give them funding for several years where they don't have to spend as much time trying to
pre-specify everything that they're going to do and then report back on what they said they were
going to do three years ago, four years ago. Instead, it's intended to give them more freedom
to follow the science and to follow their nose, so to speak, as to what the best ideas are at
any given point. And I think that there's some evidence that the papers produced by that are
performing equally as well or better in terms of citations. That's only one very limited metric.
I think longer term, you would want to see a rate of breakthrough discoveries. And that's hard to see
because you can't expect a breakthrough every day from everyone. That's impossible. And as the
director of NIGMS, John Lorsch has said, if I knew where the next breakthrough was going to come from,
I would have already made that discovery myself. So part of his idea of funding is that you want
to spread the money widely amongst talented scientists and give them the freedom. And who
knows where the next breakthrough will come from is much, especially with basic science often.
There's any number of stories from basic science where the discovery will be made
in one decade. And then three decades later, it turns out that it's amazingly useful or
influential. But yeah, I think that's a good example of a program that's experimental in
NIH in the sense that it's a new thing. It's still fairly new. It's been around for a few years.
I think that program could be expanded to other institutes at NIH, like National Cancer Institute.
And there are other institutes that have a version of this, but it's much smaller.
NIGMS funds this type of grant four times as much when I last reviewed the numbers.
Four times as much as the rest of NIH put together. Huge imbalance. NIGMS is very much focused on this
for basic science. But I think you could try that approach elsewhere at NIH. And again,
with the idea of giving scientists more flexibility and freedom. And here's another key idea that was
in the document I sent you, which is NIH should take a more deliberate approach to experimenting
and learning from what it does. It's a very meta science approach. Instead of just starting
up new programs and saying, all right, everybody's going to do this, then you don't have as much of
a chance to learn and to iterate and to introduce deliberate experimentation. NIH is big enough
that you could do like the literal randomized experiments and how you hand out money to,
they'd fund something like 90,000 grants at any given point in time. There's 90,000 active grants
and each grant often supports multiple researchers. So I'm not sure the exact total,
but it's hundreds of thousands of researchers that are supported by NIH.
That's plenty of opportunity. So you could do a randomized trial that involved a thousand
researchers. And that would be a drop in the bucket from what NIH supports. And you could
randomize 500 to be funded in one way, 500 to be funded in a different way, and they just fold
and spend over time and see what happens. See what are the results from funding that offers more
flexibility and freedom versus the more usual way that NIH does things. I think that kind of
deliberate experimentation is something that NIH should do at all. But I think they've really
ripped them in. Yeah, I think on your last point, it's really interesting just to hop back on the
kind of funding people, not projects that is somewhat also pretty present right now, I guess,
in the private space where I think, for example, the Lieberman Brothers, they're now launching a
new effort to actually fund individuals early on before basically making bets early on and almost
invest into individuals like you'd invest in companies. And I think that kind of at least
that meme or that new approach should also hopefully extrapolate outward through different
perhaps even scientific, funny organizations. I think that would be wonderful. I think.
Yeah, I'd like the parallel to venture capital, because I mean, I think there are a lot of any
number of, again, statements or examples of venture capitalists who say, yeah, you're making a bet
on the person. There are many examples where a founder of a company ends up pivoting and doing
something slightly different or a lot different. And venture capitalists often are happy with that
because, hey, you're trying something and you realize it didn't work. And now you found something
that did work. And if you would be unthinkable to go to, I don't know, to go to Mark Zuckerberg
and say, wait a minute, like your original proposal said that you were going to do X, Y,
and Z and said that you were going to spend $5,000 in year three on this line item and
where it shows that you spent the money that way, you would never want that level of micro
management of a talented entrepreneur. Okay, students want to give them a little more freedom
and flexibility to adapt to the market in which we should treat a lot more scientists the same
way that you should give them the same respect and autonomy that entrepreneurs routinely have
and give them the freedom of flexibility to say, wait a minute, I tried, I said I was going to do
X, Y, and Z, but I tried it and it didn't work. And I came up with a better idea the next year.
And now I'm going to want to do the better idea. Of course, we should all follow the better idea
when that comes up. Yeah, I guess. Otherwise, you just assume that people aren't learning as
they're actually in the field and experimenting. It's a crazy assumption to make in the first
place, I think. The process should be about learning and changing and adapting to new ideas
of the MLK. That's a bold point. If you already knew what you were going to do five years from now,
I think it's, I'm paraphrasing, but well, one scientist that I know at Pennsylvania as well,
he said, if I'm doing what I said I was going to do five years ago, I should be fired because I
should have learned and adapted in that time. Yeah, I love it. And we only got to a small part
of the laundry list that you have for the NIH there, including like other misopryptonies and
stuff. But I think I want to close at least my part of the podcast with a question, perhaps like
a little bit on a hopeful note to already lean into the extension whole part of the podcast after.
But you, we have briefly touched on when we met and you've also pointed out again afterwards
that there's actually now a really like a cool new landscape emerging of like these
alternative possible homes for researchers and scientists. I think there's FROs, there's Future
House, Park Institute, Astera, Spectac, there's a few really interesting new orgs that have been
popping up. And we certainly had a few founders and executive directors of these orgs already
on here for podcasts and for individual topics. But could you share a little bit of how you see
that landscape changing and what these new organizations are setting out to do and perhaps
what we can hope for them? And we don't have to cover all of them, but just the general spirit,
a great idea. And you could add to that. There's some government agencies,
surprisingly enough. There's a health version of DARPA, so it's called DARPA H, which I'm sure
any listeners are familiar with. But yeah, it's intended to be like a government agency that
has the innovative approach that DARPA has stayed in for decades. And there's a version of that in
the UK as well called ARIA, also trying to be like the imitator of DARPA in a way. I think that's
all tremendously hopeful. I do think it's partly borne out of discontent with the current system,
but as I say, it tends to be very bureaucratic and very top-heavy and a system in which it's
hard to get a foothold, the average age for NIH. The first major NIH grant is so we're making people
like slave away in other people's labs for 15 years before they finally get a foothold as a
researcher. And so they're long past the age, which a lot of people in previous generations did some
of their best scientific work. Einstein made some of his greatest discoveries at age 25. And so I think
Newton, James Watson, there's any number of examples of that kind of thing. So there's a lot
of discontent with that system. And so there are people within government and private philanthropists
who say this is an opportunity to diversify the landscape, to come up with new ways, hopefully
better ways of doing science and of funding science and of organizing science, all very important
meta science issues. And so one thing that I really hope we're able to do, we being just
collective meta science community and policymakers over the next five to 10 years, is just really
deliberately learn from what all these new organizations have produced and do it in a
systematic way. I do think possibly one risk, like any new organizations, like just the same as
universities, their temptation is going to be to send out press releases about all the amazing
things they've done and brag about that. And that's fine. That's to be expected. And often that's
even true. But there will be cases in which they fail. And I think it's important to learn from
failure. And it's important to be public about that. And so I think hopefully over time, we can
have a meta science discussion that's able to have just a truly honest appraisal of what's working,
what has failed, and failure isn't a bad thing. We should learn from not everything that's going
to work for the first try. Let's learn about how to design organizations more efficiently or more
perfectly going forwards. And so I think that's where I hope the conversation goes for the next
five to 10 years. Yeah, thank you for that. So yeah, so this part is more like the philosophical
part. We've been talking about the meta science and everything, but it's diving into the sort of
existential hope aspects of like where science and the progress on it can take us. You touched on
it now, but I would be really curious to hear, do you feel like things are changing? Are you gaining
traction with this? Yeah, I do think things are changing. Again, we just touched on that that
with all the proliferation of new organizations, both inside government and outside government,
I think that's a really hopeful sign. And again, my hope is that over time, there should be,
there should really be a diversity of approaches and organizations that probably something that
will be bad for science is to say everybody has to fit in the exact same box. Like that,
that, that probably isn't good, even if, even if the box makes sense, so to speak, right? So,
so we talked about fund the person, not the project. That seems like a good idea. And we should
use it sometimes. But I think if like literally 100% of science funding was fund the person,
not the project, there probably be some failure points there as well. There probably be some
things that got missed. They're probably there are some examples where you should fund an organization
like fund a team, not the person, right? So there are lots of different approaches one can use.
And in fact, there are in some areas of science, the large-scale rock collider or
big astrophysics efforts, you're not funding one person, you're funding like a team of a
thousand people, you know what I mean? Yeah, if you want to approach this to science and to
science funding, I think is would probably be suboptimal. But I do think that hopefully going
forward, I think one thing that could help improve the pace of innovation is having a more
deliberately diverse approach in like how we fund science and the source of people to get funded
and so forth. And when I say the source of people to get funded, that's also an interesting
point to emphasize. There are lots and lots of examples from history where great scientific
advances come from places that sometimes that you wouldn't necessarily have expected or they come
from people that were heretics and at their time, some of us and the germ theory of disease,
like people despise them at the time. There are tons of examples like that. It doesn't mean we
should, again, we don't want a science funding system that only funds people that are outcasts
and heretics. That probably would be in the wrong direction too. But I do think there should be
some space within science, like a national institute for oddball, like we should have a
deliberate approach to fund some things that are outside the box. And now some of them will be
crazy and won't work. We might end up funding some of the greatest breakthroughs ever if we
made more space within the scientific funding system for people with truly outside the box
ideas and approaches that don't get funded currently. Yeah, I remember I was talking to
economists, I think, Johan Nurebe, about he was talking about how technology often progresses,
like it comes from quite like dirty areas or not necessarily the most appreciated ones.
I think his example was the technology of like online payment solutions coming from
porn, like people wanting to watch porn online. And that was like what drove online payment
solutions. And then now that's a very useful and established thing. And I'm sure there are more
like exciting scientific innovations that came from the not the most appreciated areas maybe.
Yeah, but it sounds like you're positive and you're optimistic. Would you say that you're
optimistic about the future? That is a big question. In general, yes, I think that there is
lots of possibilities. There's dangers as well. We've both kind of I think I've met you at an
effective algorithm conference. There are certainly areas like nuclear proliferation or
biosecurity or artificial general intelligence. There are some areas of potential R&D advancement
that do possibly have some risk. And some would say existential risk. But I think the theme of
your podcast existential hope is that hopefully with the broad sweep of innovation and improvement
that we can address those existential risks and that we can hopefully still keep progressing
and making life better for everyone. And so I think that's where I guess my hopes and efforts
would lie is like trying to figure out what we're doing wrong, all the ways in which we're holding
back scientists and clients itself and figuring out ways to hopefully speed up and accelerate
the pace of innovation. But I think that does offer more hope for the future.
Yeah, I'd be interested to hear in relation to the increasing pace of progress in relation to
maybe AI in specific, do you see the science landscape shifting due to that?
Due to AI in general, I haven't looked at it that much in depth. There are some
really amazing advances that have been made, for example, AlphaFold and protein folding.
And there are other similar tools that are like that, that offer the potential to speed up at
least some components of science. I'm less certain myself with what I've seen out of large language
models so far. It seems like they have, they sometimes show science a great sophistication,
but then sometimes they just completely hallucinate, at least the ones to date. And so I'm a little
nervous about that. You could end up with kind of pollution of the scientific literature with
people using AI models to write papers thinking this will help them publish more and then it'll
end up online somewhere and might be largely fake or largely just made up or hallucinated.
Another thing that I worry about, and this is born out of the work that I did while I was at
the Arnold Foundation, a lot of the published literature just isn't that good. Some of it's
outright fraudulent and there are more discoveries about academic broad that come out, it seems,
every week. A lot of it isn't that reproducible. And even the stuff that is reproducible, it often
isn't described that well in print. So one thing that I funded, for example, was called the
Reproducibility Project in Cancer Biology. And their original idea was to replicate the
experiments for 50 top-sided cancer biology papers. And ultimately, they could only complete
fewer than half of their intended experiments. And the reason was that in every single case,
you couldn't just go off the publish paper. You had to go back to the original lab and ask them
to fill in all the gaps and fill in all the details about what they had actually done.
And some of the original labs were not cooperative at all. In some cases, just we're
hostile. Some cases just said we're not sure. And when they worked for operative,
the answer was always that you need twice as much materials as you expected. And so it's going to
cost more and take longer. And they're like, worry about AI models that might be trained on
scientific literature as if what's written down on paper is the complete entry. And that's often
not the case. And you just worry that might spiral into even more irreproducible work.
So again, cautiously optimistic about some of the specialized tools that are out there that are
based on really rigorous systematic databases. But then there's a whole wealth or a whole broad
millions and millions of scientific articles that I would say probably aren't worth trying to use in
an AI model in the first place, or at least with great skepticism and a lot less of corrections
needed before you just train an AI model and expect to get useful information out of it.
So it's all over the place. But yes, that's my answer for now.
Yeah, that's very interesting. I hadn't really thought about that in relation to
that it would get access to maybe incorrect data technically. Yeah, I guess that I heard about
the Center for Open Science and that's where they try to pre-register the studies that they're
doing so that you can actually check if they are correct in that they're reproducible or that
oftentimes you don't really publish what's maybe not an exciting result or something like that.
But here you can see what people have published or have been doing research on even if the results
aren't that exciting. Any other projects like that that you think are promising or seem like
they could actually make a difference in this field? Yeah, there's a lot of directions I could
take this. The idea of pre-registration really came out of medicine from some recommendations
that were made in I think it was early the late 1980s or definitely by the early 90s,
there were some folks writing about it in the idea in medicine. These folks in medicine who were
they were trying to do meta research in medicine to try to summarize the whole body of literature
and as of the 90s and probably still true somewhat today. They were very frustrated and I remember
an article by a couple of folks at the time where they said that it's just really strange to them
that you can find much more information on baseball statistics and up to the minute information about
every baseball team, every baseball player, everything they've ever done. But yes, it's
impossible to find that information about clinical trials, that much wealth of information about
clinical trials in medicine, which is to them more important than baseball even. So pre-registration
was in part a way to get access to information about the trials that are being conducted
on human beings and involving drugs or other types of treatments that might be used in medicine.
That was one motivation for me to still be able to see the kind of see the whole denominator,
so to speak. It's easy to see the published successes and the press releases of announcing
that a drug will cure in the US, the commercial that say try this drug. But it's harder to find
the failure since the idea of pre-registration in part was intended to address that. And then
like subsequently, I guess you're able to like what there was one famous article by kind of
Eric Turner and some colleagues where they were viewed or get something like 70 or 80
clinical trials that have been done on antidepressants. And I'm going off a memory here,
so I might get a number slightly wrong, but roughly half of the trials showed that the
antidepressants work and roughly half of the trials showed no results. The antidepressant
wasn't any better than placebo. And they had access to all this because of both pre-registration
and because they went to the FDA and these were FDA-free drugs and the FDA demands to see all
the evidence and not just what got in the top journal. So you have the whole denominator there
to look at. And so what they found was that basically all of the positive trials, I think
except one got published in a top journal. And the negative trials or the null trials
mostly went unpublished, but I think a few of them were published and then with a kind of spin or
misrepresentation as if they've been positive studies. And so yeah, if you look at the medical
literature, and then this is also tied back to what I was saying about AI, if you looked at just
the medical literature on those antidepressants, you would say, wow, like I'll work, everything's
amazing. But if you look at the whole body of evidence that the FDA could look at, you would say
sometimes they work, sometimes they don't own or sometimes the one medicine is better than others,
it's a lot more complicated and messy when you have all the evidence sitting before you.
See how there have been efforts like that in medicine, pre-registration has been growing and
other disciplines, psychology and economics in particular over the past 10 years or so,
the American Economics Association, like the Center for International Science, started
demanding pre-registration about 10 years ago. And the rate at which that happened on a yearly
basis that's gone up and up within economics alone. Yeah, I think that's all kind of hopeful signs
of progress in different fields, just being better practices about being public about what
kinds of studies you're doing. And then ultimately, it's kind of old for not unless you publicize
the failures and the nor results as well. Because again, if you only publish the positive
results and don't say anything publicly about the negative results, it's very skewed. It's
to be like if you flip a coin, and then you only announce when you flip a heads. Yeah,
you would look like you flip 100% heads, but that wouldn't be true. Be very misleading. So
that's correct. Yeah. Yeah. Yeah, I'm going to make a bit of a turn with the next question now,
which is it's a question that we always ask in this podcast, which is in relation to the,
like a eukatastrophe. So it's a term that means basically the opposite of a catastrophe. So once
it's happened, the value of the world would be much higher. And feel free to relate this question
to like your area of expertise within science. And I think also think ambitiously when I ask you
this question, which is if you could envision like a specific eukatafor, the progress of science,
what would that be? What would be an existential hope scenario of science? If you like, if all
the work that you're doing now came true, what would happen? That's great. Yeah, I first came
across that word eukatastrophe and the writings of JR Vulcan. Did he invent the word? I can't remember.
Yeah, I believe he did. Yeah. That is a super interesting question. The kind of revolutionary
side of me says that in terms of meta science, what would be really cool is if you could duplicate
all federal funding of science, but with an entirely new set of institutions. Imagine we spend
50 billion or so on NIH. What if we add in an extra 50 billion on biomedical research?
Well, with the condition that it has to be run completely differently from NIH, and it has to
be in the hands of different people. And again, touching on these ideas of exploiting the,
and proliferating the diversity of the approaches and the scientists that get funded and the ways
in which funding is handed out, because then you have two different systems operating. And so,
thinking very meta here, you have two different systems operating with equal amounts of funding.
And now you can really see at a grand scale, hopefully, what happens? What are the results?
It would be a chance to test out lots of different meta science ideas that people have
discussed for years or for decades, but you just really need a grand
landscape in which to experiment with that. So, again, that's very meta. But my hope would be
that we learn a lot about how to fund science and how to organize scientists into organizations,
into labs, into universities. We need entirely different institutions that don't look anything
like universities, for example. Maybe you should set up a whole national institute where the rule is
you can only fund scientists who are working out of their garage. I don't know. You need some kind
of wild ideas out there to try out different approaches, different scientists, different
ideas. And my hope would be that at a minimum, we'd learn something from that. But in terms of
existential hope, I think we might end up creating a number of great breakthroughs that wouldn't
have happened otherwise. And today, it is hot, heavy, bureaucratic system that is so conformist
and focused on doing whatever gets you approval from the existing bureaucracy.
Yeah, I really like that. If you were to introduce someone who is entirely new to this field,
is there anything you'd recommend that they read or watch? Maybe it's just your own
sub-stack, or is there anything else that you would recommend for an intro to the field?
Anything. Sure. The writings that Good Science Project has produced are good plays to look at.
You mentioned the Center for Open Science. They've had a number of publications and projects that
are related to meta science. I would say that there's more stuff coming out from the Institute
for Progress and the Federation of American Scientists. They've had a series of papers on
open science, for example. Yeah, that might be, that gives you plenty of reading to start with.
That sounds like a great place to start. And how can listeners best stay updated with your work
and the work of the Good Science Project? Sure. You mentioned sub-stack. It's just
goodscience.sub-stack.com. Also, the website is just goodscienceproject.org. And yeah,
we'd love to hear from folks who have ideas or who enjoy the writings and want to learn more.
Great. Thank you so much, Stuart, for coming on this podcast. We really appreciate it.
And we'll definitely keep an eye on all the work that you're doing with the Good Science
Project in the future. So thank you so much.

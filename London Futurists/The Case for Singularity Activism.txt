Welcome. My name's David Wood and I'm here to talk about the case for Singularity Activism.
This is actually a version of a talk I gave a couple of days ago on the 29th of April
to a meeting of London Futurists at Brackbeck College. It was a well-received talk but that
event was not recorded. So I'm now sharing an updated version of this talk which in some ways
is improved because it benefits from feedback during the meeting itself, more feedback during
the after the event discussion in a nearby pub, the Marlborough Arms and yet more feedback which
I've received via email and social media. This topic of the Singularity and Singularity Activism
is arguably the most important topic that London Futurists has ever discussed. Now we've discussed
topics like this many times before but they've never been quite so urgent and the scenarios ahead,
the choices, the risks and opportunities have never been quite so pressing. So let me talk
about these words Singularity and Activism. The word Singularity is often misused. It's overused
but I want to keep using it. It's a good word. It describes something singular, something unprecedented
and I'm going to use it to mean the unparalleled acceleration of AI capabilities all the way
to reaching beyond human general intelligence. I say this is a self-propelled acceleration because
as people develop one set of AI capabilities, these capabilities can be used in turn to help
design and develop an improved set of AI capabilities and that improvement cycle is accelerating. As I
said it goes all the way to human level and part of what that means is that it is going beyond
human understanding. I don't just mean beyond my understanding or beyond your understanding,
it's going beyond the understanding of the people who design and create these systems. There are many
surprising features in the latest AI systems so much so that people are taking to calling them
with some justification not just artificial intelligence but alien minds and as they move
beyond human understanding the risk is that they are moving beyond human control. There are many
outputs of these systems which we did not expect and which are not to our liking.
Now I'm not here today to offer a singularity pacifism. There's nothing we can do. I don't
believe that at all. There are plenty of things we can do. Nor am I here to offer a singularity
cheerleading in which people say let's accelerate AI capabilities as quickly as possible without
any restrictions. I think that's irresponsible and I'll explain why. Conversely I'm not here to offer
singularity obstruction. A kind of jihad, a Butlerian jihad to refer to the work of the
science fiction tuner. No, I'm offering something rather different. Activism. But finally let's
be clear I'm not offering doomerism either. I'm not saying we're all doomed and there's nothing
we can do about it. I'm saying there is plenty of significant things we can do. Hence singularity
activism which I describe as the purposeful steering of AI capabilities and I'll be explaining
what I mean by that. But first let's go to the top of the slide again. General intelligence.
What does that mean? There are some people who say with some sophistry there are multiple kinds of
intelligence which is true. Therefore there's no such thing as general intelligence. Therefore
they say there's no such thing as artificial general intelligence and so people like me who
talk about AGI, artificial general intelligence, can be ignored. Well what I mean by general
intelligence is in simple terms the difference between the capabilities of the creature on the
left and the capabilities of the creature on the right. The one on the left has more muscles.
If there was a naked fight between a chimpanzee and a human very likely the chimpanzee would be
victorious. That's not what general intelligence is. It's brain not brawn but it's a particular
type of brain capabilities. After all chimpanzees have been proven in some ways to have better
short-term memory than us. Our general intelligence refers to our mix of capabilities. They are
whatever is needed to help us get things done. To get complicated things done in complicated
environments or in a definition that is often used general intelligence is the ability to
achieve goals in a wide range of environments and it breaks down to a number of particular skills.
The skill to be able to observe to see what's happening and make sense out of it.
Therefore in order to be able to predict what's going to come next more than that the ability to
predict what will happen if there are interventions changing the situation what will the outcomes
of these interventions be. General intelligence also includes the ability to put these plans
into action step by step and to reflect whether these actions had their desired intended
predicted consequences and in case they didn't general intelligence involves the ability to
learn to try something different with the point of view of a deeper understanding.
Stupidity is sometimes said to be trying the same thing again and again and expecting
different outcomes general intelligence avoids that. So loosely that's what general intelligence is.
Is it possible that just as we exceed the general intelligence of chimpanzees and
therefore the fate of chimpanzees is entirely within human hands there will in the future be
some new species with significantly more general intelligence than us. A species running not on
biology but on computing substrate silicon or whatever. Now most people if you ask them this
and they think about it most people will say yes it would be possible at some time in the future
probably a long time in the future. That's what people at least used to say but let's look more
closely at this question of when there may be artificial intelligence surpassing us in this way.
To answer that question I'm going to refer to the results shown on a website metaculous.com.
Metaculous is as it says here from the website a community dedicated to generating
accurate predictions about future real world events.
That includes events in politics economics and in the progress of technology.
And these accurate predictions are achieved by aggregating the collective wisdom insight
and intelligence of the participants where the contributions are weighted by the extent to which
participants have proven skills as demonstrated by previous predictions and as demonstrated
by the assessments each participant can make of others contributions. Now on this site I want to
look at one particular prediction it's a prediction that's been in place since almost the beginning
of the site in January 2020. The question the contributors were asked to comment was when will
the first weekly general AI system be devised tested and publicly announced? And one benefit
of the metaculous site is they define their terms pretty carefully so they explain what they mean by
weekly general AI system. It's a single piece of software which must be able to do multiple things
namely first it should be able to reliably pass not a trivial Turing test but a sustained Turing
test of the type that would win a particular prize in a particular setup and this involves a human
judge who is knowledgeable about the capabilities of artificial intelligence systems it involves a
sustained dialogue. Now there are many people who have already been caught out by what chat GPT
and other generative pre-trained transformer systems have produced if they don't know where these
essays or these poems or these dialogues have come from many people are prepared to think this
must be from a human but this doesn't mean these systems can pass these types of Turing tests
because if you get a knowledgeable judge they know how to ask questions that will trip up
the chatbots so far. So that's the first of four things that software needs to do to meet this
criteria. Secondly it needs to be able to score at least 90% on a robust version of what's called
a Winograd Schema Challenge. This refers to bits of text like this. Now a human can read the text
and then at the end if they are asked about one of the pronouns shown here the word his
if they are asked whose wallet is it the human is able to answer it's the rich old man's wallet.
Now there is a criticism of many pieces of AI that they are just stochastic parrots so called
in other words they can mimic they can put words together in a plausible order but they don't really
understand and it's true a piece of software that didn't understand would not be able to answer
such questions at least questions that it's never seen before. However some pieces of AI
are making progress with these Winograd Schema Challenges. The third thing that the software
needs to be able to do is to score at the 75th percentile of the full mathematics section of
a standard SAT exam and here it's just given images of the exam pages and they have to be
exam questions that the AI has never seen before and are not part of its training data.
And the fourth thing that the AI needs to do to pass this test is to farewell in a classic Atari
game called Montezuma's Revenge. It needs to be able in the equivalent of less than 100 hours of
real-time play to learn and explore all 24 rooms of this game. It's not a game I played myself but
I am told that even intelligent humans struggle to make progress because you have to think very hard
and understand cause-effect relationships over an extended period of time.
So a piece of software that does all that will be pretty intelligent. What do members of the
metaculous community think will be the timescale in which there's a 50% chance that at such an AI
will not only exist but it will be publicly announced and publicly tested.
Here are the results and I'll talk you through what's visible on this graph. Let's look at the
middle of the three lines that is the community median estimate. That's the aggregation,
weighted aggregate of the various estimates for when people think there's a 50% chance
that the criteria is met and you can see for example that in April 2020 people thought this
would be quite a while into the future, actually 35 years into the future and if you had asked me
roughly the same time or a bit earlier I probably wouldn't have said 35 years I would have said
something like 25 years but it would be quite some time in the future but let's look at how that
prediction changed as time went on. There were many new AI systems released in 2020 which made
people wonder. Progress is faster than had been expected so that by October the forecast was no
longer 35 years in the future but 13 years in the future. In 2021 the predictions varied again,
it went up in point because people looked at these recently released AI systems and
started to be more negative about them in terms of their faults but then yet more systems were
released with even better capabilities. I'm not just referring to chat pieces of software,
I'm referring to graphics generation software as well and video generation software
and many other capabilities such as DeepMind's GATO which was a single piece of software that
could do 602 different tasks at a high level of capability so in 2022 it's no surprise that the
estimates came down again so that by October the forecast was just six years into the future.
AGI just six years away according to that estimate and in 2023 the estimates have come even closer
to the current date so in April 2023 the forecast was that AGI in this weak general form would only
take three years to come into existence. There's more in this picture if we look now at the top
and the bottom lines they give you the estimates by which people are pretty confident something
will have happened that's the 75 probability line and most recently if people were asked to
give a date by which they were 75% confident this week AGI would be present it was 2030
and if you ask them to be more risky to give a 25% confidence prediction that's the bottom line
that's June 2024 14 months into the future I should say that this
meticulous site has a number of other definitions of AGI some of which are more demanding the more
demanding one its prediction forecast has come closer and closer to the present day as well
that the last time I checked was showing February 2032 nine years into the future
so that was an aggregate forecast now let me take the forecast of a particular
distinguished individual some of you may recognize this as being Professor Jeffrey Hinton
of the University of Toronto he actually grew up in Wimbledon in London went to school there
studied computer science at King's College London went to do a PhD in artificial intelligence
in Edinburgh where he ran up against some opposition he wanted to study neural networks
and at that time the established wisdom was that neural networks were a waste of time
but Jeffrey Hinton persisted he ended up after some time in Carnegie Mellon in the US he is
leading professor at the University of Toronto and he joined Google Brain he is often called
the godfather of deep learning or one of three godfathers but he is in some ways the most senior
of the three because of his long advocacy of neural networks and because of some breakthroughs
which I'll describe shortly for which he was responsible but I want to move to what he said
in an interview on CBS Saturday morning TV just a few weeks ago he was asked about how long it
would take to get to artificial general intelligence the same question I've just been reviewing he said
until quite recently I thought it was going to be like 30 to 50 years before we have
general purpose AI but now I think it may be 20 years or less now the CBS interviewer asked
another question some people think it could be like five as we saw from the data we've just been
looking at Jeffrey Hinton's reply I wouldn't completely rule that possibility out now whereas
a few years ago I would have said no way the conversation continued the interviewer asked
are we close to the computers coming up with their own ideas for improving themselves
yes we might be indeed there are many ways in which AI tools are already proposing improvements
to AI tools the interviewer continued then it could just go fast professor Hinton said that's
an issue we have to think hard about how to control that and can we professor Hinton's answer
we don't know we haven't been there yet it's true we've never created an AI which is
more intelligent than us in general ways we can't anticipate exactly how it's going to behave
exactly how much we will understand of it exactly what goals that AI might turn out to follow
but professor Hinton didn't advocate pacifism or doomerism he said we can try the interviewer
continued what are the chances of AI just wiping out humanity the reply it's not inconceivable
very candid words and since this interview was recorded indeed since the presentation I gave
out to Birkbeck professor Hinton has announced he is resigning from his tenure with google why
because he wants the freedom to speak out about the dangers of uncontrollable advanced AI without
needing to worry whether he might be falling foul of policy set for communications by his employer
google serious stuff so we're now in the situation where people like professor Hinton
and others professor Stuart Russell for example professor max tagmark they are featuring on tv
shows a bit like in this film don't look up where there were some people who recognized a danger
and they appeared on news shows you can see in this picture the news presenters here who are for a
while amused be mused by the prediction of doom before hastening on to the next celebrity scandal
item on their agenda but the scientists here say please do look up please do pay attention
it was said at the time this film was released it was a commentary on people ignoring the threat
of climate change it can also be seen as a commentary on people ignoring the threat of
incoming artificial general intelligence now in the movie there are some spoilers coming
so if you want to avoid hearing these spoilers please pause this video and move forward to the
next slide in the video there is after a while reluctantly an agreement to try and deflect
this incoming meteorite we might say to control it but that plan is scuppered when people think
they can make a fortune by taking advantage of all the minerals on that meteorite so they
frustrate the plans to slow down or deflect the trajectory and another spoiler it leads to the
destruction of all of humanity okay that's just a film but let's now look more closely at that
threat and i want to talk you through now in one of the most important slides in the whole set
six levels of understanding ai and the problem is that many people are stuck on some of the
earlier stages and the reason they say we don't need to worry about incoming ai threats is and
then the offeror reason that makes sense from the level of understanding they have reached
but they are not aware of the insights and important facts from the higher levels of this
understanding so the ground level of understanding is you've watched hollywood films not referring
to don't look up here but there are other hollywood films featuring artificial intelligence or you've
read science fiction and that's entertaining and it may give you some idea but it is hopeless
to put you in a good predict situation to predict what's likely to come in the next five to ten years
the next level of understanding is you know what ai was like around 2011 or earlier
that was the period when classical ai now sometimes called good old-fashioned ai dominated
logical systems systems of symbolic manipulation lots of if then else rules rules that were
created by talking to experts and then modifying and some learning systems but that will not serve
you very well if that's all you understand about ai because in 2012 the team led by professor Jeffrey
hinting at the University of toronto made a breakthrough in neural networks deep neural
networks so it has come to be known as deep learning they achieved results far better than
anybody had achieved by any previous ai they did it by a number of steps they took advantage
of a different type of hardware not the central processing unit inside computers but the graphics
processing unit which could perform lots of parallel computations at the same time and these
parallel computations of what makes calculations in deep neural networks feasible they also took
advantage of lots of annotated data including lots of images that had descriptions appended to them
these descriptions were supplied by humans who were paid via the amazon tuck service
to briefly describe what was in these pictures this database image net was created by a
Stanford professor feifei li and this was used for the training of these deep learning systems
which Jeffrey hinting together with a team of two of his students one being ilia suscaver who
subsequently has been a co-founder and chief scientist at open ai finally they managed to tweak
the venerable back propagation algorithm with some other modifications so that the whole system
achieved great results in the annual competition for image recognition and it turned out that
similar results worked in other fields too so there was much better machine translation
from one language to another there was much better speech recognition there was much better
recognition of what was in various medical scans so that has caused a new revolution
a tide of momentum in the field of ai but if that's all you know then again you are not
in a good situation to speak with any confidence with any authority about what's likely to happen
next because there was another breakthrough that started actually in 2017 with the publication
of a paper by a group of researchers in the google brain division of google the division
that was at the time headed by jeff dean they found a way to make self-supervised learning
work very well self-supervised learning is when the system is able to learn from data that has not
been marked up or annotated by humans and so it's fed lots of text from all over the internet
and it's able just by studying this text in the various clever ways what it does in fact is it
covers up in its own mind various words in these sentences and it tries to work out from the context
the other adjacent words both before and after what the covered up word is likely to be and it
does this again and again until it gets much better at it and people had an idea for a long time that
something like this would be possible but it never worked until this new paper describing so-called
transformers came up with a system that worked and the first practical transformers was in something
called BERT that google had in 2018 and they have gone on in leaps and bounds since then a second
major recent revolution in AI it's like to what some people call large language models
that's not a completely adequate description hence i've got here large language multi-modal
models because it makes sense not just of text with words covered up so it guesses what the
covered up words were and is able to predict the structure of text it's able to understand syntax
the same transformer mechanism works with covering up bits of pictures
it works with taking a picture and converting it to low resolution and trying to work out
what the good high resolution version of that would be the same transformer mechanism works
and as a result these systems are able to generate pictures texts videos sound and so forth hence
some people also call them generative large language multi-modal models and that sounds
like a mouthful but the reason it's done is somewhat controversially so that the whole
thing can be called golems now i say controversial a golem is a mythical creature which is brought
to life and exhibits alien minds and that is in a way what we've got all right so if you understand
transformers that may take you up to about 2021 and that is again insufficient for you to reliably
predict what's coming next because what happened after that is that the large language models
started demonstrating what we call emergent capabilities more features than were expected
as the systems were scaled up as they got larger they didn't just get better at what
they were doing they started doing new things that had not been expected so one example is so
called one short learning ai has often been criticized in the past for not being able to
duplicate what human children can do a human child is able to tell the difference between a cat and a
dog by just seeing a few examples of each the child doesn't need to be shown a million examples of each
well these large language models sometimes called foundation models of the same capability
they are trained through a lengthy process a bit like a child's brain is in effect trained by hundreds
of millions of years of evolution and it's then able to pick up new skills very quickly so these
large language models are once they are trained often able to answer questions on some new feature
without that feature being trained just by seeing one example they're able to extrapolate and that's
just one example of emergence jeff dean who i previously mentioned head of google brain for
many years he has recently been promoted to a new position inside google chief scientist
he remarked that we are seeing many examples of emergent cape features we are seeing many
examples of emergent features in these large language models what we don't have so much of
is an understanding of where these capabilities come from so that may take you to about 2023
and guess what's coming next that again is not sufficient for you to understand the road
to artificial general intelligence that we may travel over the next few years
to be able to anticipate what's coming next there is one additional thing that needs to
be added to this picture it is combination systems by a combination system i mean that
in addition to the successes and limitations of large language models other aspects are
bolted together with them combined in various ways for example a technique such as artificial
neuroscience is already being applied to make sense of how these large language models in some
cases come up with their answers it's called artificial neuroscience by reference to original
neuroscience which tries to work out what's happening inside a brain to convert various
inputs to various outputs it's the same with artificial neuroscience except that for a real
brain there's a lot of guesswork involved because we don't like to cut open people's brains
whilst they're in the middle of calculations and thoughts most of the time but with these large
language models it's easier to inspect and what people are finding as they are doing these inspections
and starting to see how these features emerge is they are realizing gosh this is a very inefficient
way to achieve a certain goal if the system could somehow be re-architected if the network could be
reconstructed the same results would come perhaps 10 times faster so that's an example of combining
ideas from what has been called different tribes and here i'm using tribe in a good sense of the
word the sense introduced by Pedro Domingos who is now an emeritus professor at the University of
Washington in his 2015 book the master algorithm a book by the way which has been noticed on the
bookshelves of none other than Xi Jinping the leader of china alongside books by Marx and Lenin
Hemingway and Dickens i'm not sure whether Xi Jinping has actually read this book in full
but it is quite likely that many of his advisors and colleagues are familiar with at least the
outlines of these ideas the idea is that progress true progress faster progress will come when there
is a convergence of ideas from several of these so-called tribes the symbolists were the ones I
referred to from the good old-fashioned classical ai days the connectionists were the ones who were
rejected for such a long time with their ideas that we could learn from neuroscience with neural
networks until 2012 and this big bang but there is likely to be input from evolutionaries basians
and analogizers as well so that's the idea of tribe convergence and I have to say in passing
that some of the progress in the last few years has been to another kind of convergence that's
entirely within this connectionist neuroscience tribe where various sub-tribes have been able
to cooperate and collaborate as never before it used to be that different types of networks
recurrent neural networks for speech recognition convolutional neural networks for image
recognition and so forth there used to be different disciplines different specialities
within that field with transformers the different groups can share their ideas share their results
share their special techniques more readily which is why again there's been an acceleration
so there are plenty of capabilities making it possible for us to reach agi in just a few years
let me recap first there is the potential to scale up today's large language models to new
heights this could mean more training data that could be surprisingly it can be synthetic data
you can get one ai to create data and another ai can read it and learn from it and actually
reaches a higher level of intelligence as a result it may also be training data which has been converted
from radio programs converted from youtube videos as the audio there has been turned into text
scaling up these large language models needn't just be a matter of more quantity it can be a
matter of better quality and today's large language models already have some preparation
done to the data it's not completely raw i said it was unsupervised sorry i said it was
self-supervised well there is some preparation done and if there is more preparation done to data
again it could really result in better scaling up another way to scale up is more parameters
larger models it's the equivalent of a human brain having more neurons but it's not just larger
models more parameters it can also be different structure in these networks and many people are
encouraged and optimistic about an innovation known as sparse networks where instead of all
of the networks being equally likely to be involved individual computations will in echo
features of the human brains in which there is only small parts involved in any particular calculation
another way in which scaling up may happen is by taking advantage of new hardware innovations
just as the move from cpu's to gpu's propelled forward deep learning there are other enhancements
in hardware that people are considering then there is these transformers themselves they
were sort of an accidental finding and people are now considering alternatives to transformers other
ways to do this self-supervised learning which may show more promise second as i mentioned as these
systems are scaled up they surprisingly show some emergent properties well it's likely that the path
to agi may involve taking advantage of these emergent features and they could be emergent
either within individual models which are made larger or they could emerge at the network level
when multiple large language models with different characteristics are networked together a bit like
human intelligence exists partly inside individual brains it exists partly by the way as the result
of a sharing of slow thinking and fast thinking fast thinking being our intuition slow thinking
being when we say hang on a minute i need to think this through and we apply some logic but our
intelligence also depends on things that are external to our individual human brains to libraries
to databases to online communities so that may apply in the ai case too as i mentioned it's also
possible that this emerging discipline of artificial neuroscience sometimes called
mechanistic interpretability that this will allow redesigns of these systems to reach
higher levels of intelligence more quickly then as i said there will be combinations not just
combinations of deep neural networks driven by transformers but there will be combinations
from the other tribes techniques taken from the symbolists evolutionaries basians analogizes
and by the way there are other corners of the internet with their own preferred ideas there
are more than five tribes we might point to the quantum ai tribe people who believe that it is by
taking advantage of quantum computing and new quantum algorithms that we will make the most
progress towards artificial general intelligence and there are other maverick groups too
well the question is will these existing ideas be sufficient to get us all the way to artificial
general intelligence the honest answer is we don't know but the more people who are looking at this
and especially if they are using sophisticated ai tools to analyze and then come up with new
hypotheses the more likely it is that we can reach agi pretty quickly so the capabilities are there
hence professor hinton said i wouldn't completely rule that possibility out now
referring to the prediction of agi within five years
but is there the motivation to improve ai hell yes huge financial motivation almost every field
of industry stands to be transformed and improved by taking advantage of new ai capabilities or even
developing new ai capabilities and using it in that industry ahead of competitors so whether it's
or designing virtual artificial intelligent characters inside games or it's improving customer
support so that it's not just today's rather pathetic often chatbots but chatbots with real
intelligence to make it easier for customers to get the information of the products goods and
services they desire there is transformation of all interfaces so that when people have
something they want to accomplish with their computer or their phone they'll be able just
to say a few words and the artificial intelligence will figure it out and it will no longer be such
a frustrating experience sometimes to interact with computers and applications there are huge
possibilities to have faster development of new medical treatment new drugs there are possibilities
to create wonderful new arts music due course poetry stories longer stories films there are
possibilities already in ai in proving engineering designs and there are some indications of ai
improving science fundamental science such as deep minds alpha fold software which solved
the problem which had essentially existed for 60 years in that you could easily specify
the sequence of amino acids which would create a protein but you could not know in advance how that
long string of DNA would fold up the long string of amino acids would fold up
and then there are other motivations people who want to make money in clearly bad ways by writing
malware to create ransomware or other ways to damage sites and to make money from them as
a result of that there is a feedback loop because the defenders cybersecurity people are writing
are writing more intelligent software to recognize malware not by straightforward static signatures
but by behavior therefore again the writers of the malware are motivated to come up with
malware that disguises its behavior disguises its intent that's one arms race there's another
arms race for real weaponry missiles counter missile missiles missiles that can evade the
attempts of counter missile systems and so forth i call this positive feedback in that it leads to
systems of greater capabilities even though not all of these capabilities will be beneficial for
humans the result of all of this is surely especially when you combine not just the economic
competition but also the geopolitical competition the result of all of this is surely even faster
progress there are desires for progress there are many options for progress what this will create
is first in many cases better outcomes better medical treatments better gaming experiences
better scientific theories at the same time we must also recognize that some of the bad
experiences people have had with ai frustrating experiences buggy experienced biased experiences
disruptive experiences some of that's going to get worse too and i say all this
even if we don't get to fool agi simply ai being more powerful than it is today is going to escalate
both the good and the bad possibilities and by the way to be very clear the bad possibilities
are not dependent on some hollywood idea that there is malign intent inside these ai systems
they can arise accidentally they can arise because of other factors which i'll now describe
so here's my picture of four significant ways in which advanced ai could prove to be
catastrophic for human well-being in the first quadrant i'm putting implementation defects
the software has been designed and specified for a particular purpose it seems in initial testing
to behave in such a way but there are bugs from my own professional experience of 25 years working
in software in the early days of the mobile computing and smartphone industries i am aware that
complex software typically has complex bugs opaque software typically has opaque bugs opaque
meaning that people don't fully understand why a particular piece of software why a particular
neural network has the result that it does but today's neural networks are not fully understood
these bugs often arise when software is used outside the circumstances in which it has been
trained and tested this is sometimes called an out of distribution effect when for example there
may be more than one ai in the environment complicating the interaction so these systems
may work very well most of the time a bit like our satellite navigation systems in our cars and
bicycles they gave us instructions which time and again proved to be correct until the one time
they gave us due to a bug of one sort or another a bad instruction and if we're not pay attention
we may go right off the edge of a cliff some people say hang on isn't this software meant
to be super intelligent how can it have bugs but the point is it's not necessarily perfect in fact
we can't prove that it's perfect and the risk is that it is somehow to use the words of one the
futurist frequent attendee Tony Zarneski it is an immature superintelligence the second quadrant
refers not to defects in the original implementation it refers to defects that arise when that
implementation is overridden somebody hacks it somebody says that they dislike some of the
functionality this particularly applies if the software is in some sense open source if it
can be copied people look at it they think i can make it go faster i can gain an advantage by removing
these unnecessary health and safety checks or they may think they're going to move something they
think is walk they wanted for example to be able to come up with a malicious comments about say
president biden and the software is initially prevented from doing any such thing and somebody
may find a way to jail break the software in order to get these results so that is the risk that
an initially good piece of software will be hacked a third category of potentially catastrophic
error is not a defect in the implementation but a defect in the design in the implementation defect
case we're referring to the software not doing what we programmed in the design defect case it
does what we asked it but we asked it the wrong thing we programmed the wrong objectives and
i've often seen this in my professional experience too i've seen what i regard as a bug i've gone to
the owner of that software and said what's you're going to do about this and they say that's not
a bug that's what it is intended to do so there was a incomplete specification people haven't
thought through all the issues all the possibilities and the specification therefore led to their wrong
behavior in at least some cases and you're a bit like the old story of king Midas who in this myth
was given a reward by the gods for his kindly behavior he desired gold there's nothing wrong
in itself with seeking some more money but he was careless to say let everything i touch
turn to gold without giving a complete specification that this should not apply to his food or drink
or to his daughter now no software engineer is going to be quite so foolish as to give that
kind of objective but we can imagine more subtle more sophisticated more complicated ways in which
our definition of objectives turns out wrong there is plenty of fictional stories about
genies with great powers such as these agis or advanced ai's we're about to create and frequently
in these stories the third wish is to undo the damage caused by the first two wishes
because the person making the wishes had not thought through all the issues this isn't so much
an immature superintelligence but an alien superintelligence one with a different set of
fundamental goals than what we thought would be needed if only we had thought more fully
now you can see the pattern here the fourth quadrant is design over ridden and here i'm
referring to the system itself changing its mind about its ultimate design goals that there
should be an emergence possibly not just of behavior but of goals of fundamental alignment
this is a controversial and difficult area but it does seem to have happened in the past after
all we humans we were built by genetic replicators which had a particular desire we can speak we
can say namely that the genes would be replicated into the next generation so we have various
instincts to support that we have instincts to gain resources to be successful members of the
community to choose our mates wisely and as a result we are more likely to pass on our genetic
material to the next generation but there are human minds that diverge from these objectives
and they choose to live in different ways simple example is to use buff control to frustrate
the genetic imperatives to multiple children and so on so this could result in an AI that although
designed to be sympathetic and supportive to humans develops a disinterest in human well-being
another kind of alien superintelligence and this may not have a conscious desire to harm humans
but it will be like us humans when we encounter colony of ants in a piece of land where we
intend to build some new accommodation for humans we don't think too much about exterminating these
ants to make room for what we have in mind that is the risk now do other people think the same as
this sometimes critics say it's only some science fiction disorted analysts who have these fears
people say it's only journalists who are whipping up stories some people say it's just done for
publicity but actually many times there are communities of expert AI researchers who have
been asked and over the years justice forecasts have changed recently about the dates at which
advanced AI or AGI may come around there are changes in expectations to the consequences
so this is the result of a survey conducted in May and June of last year in which to qualify
you needed to be the named author on at least two papers published in the preceding three years in
accredited journals in the field of computational linguistics which is sometimes also known as NLP
natural language processing so these are people who seemed like it knows what they were talking about
so let's see some of the results they gave question is AGI an important concern should NLP
researchers consider it a significant priority 58% either agreed with that or somewhat agreed with
that next question what about progress are things happening in the field of NLP such as language
model and reinforcement learning are these significant steps towards the development of AGI
57% either agreed or somewhat agreed then there's a question of the social implications
could these advances plausibly lead to economic restructuring and societal changes on at least
the scale of the industrial revolution 15% said yes another 20 or so percent said perhaps
to this question AI decisions could cause nuclear level catastrophe decisions made by AI or
machine learning could cause a catastrophe that is as least as bad as an all out nuclear war
I think in most cases the correct answers should be higher than the percentage is shown here
the reason I dare to say this it's not because I know more about computational linguistics or NLP
than these distinguished authors it's because I have taken more of a futurist view as I have shown
not just of the present and recent past thought of what I think is likely to happen with these new
combinations and emergence and scaling up so how could we get to a nuclear level catastrophe
I'm shown here a picture of an actual nuclear explosion but we could consider things like
other damage to our infrastructure that would be catastrophic for us we can consider AI
whipping up hatred causing lots of distress and dysfunction in our politics leading to the
adoption of populists who then lead us to war lead us to nuclear war so there could be bio failure
these systems might end up engineering pathogens which are hugely destructive
they could be involved in changes to our atmosphere or our oceans in an attempt to head off climate
change but we could be deeply counterproductive and disastrous now any such outcome doesn't happen
just because of the artificial intelligence employed they result from a different kind of
combination they result often from flawed human reasoning which is made worse by our sometimes
flawed human emotions when we are too vengeful too egotistical too angry seeking vengeance
these systems are often made worse by social incentives which are counterproductive
they are often positive but they can equally be negative leading to perverse incentives
leading to perverse competition and in that context the more powerful the tools we have
the more damage can result especially when we take into account our infrastructure whether
it's our biological infrastructure our environmental infrastructure our financial
infrastructure or our IT infrastructure our energy infrastructure when we consider the
fragilities in each of them you add together all these causes you could get a cascading catastrophe
so all of these factors play a part in this
which ones should we pay most attention to trying to fix there are some people who say they are not
worried by artificial intelligence they are just worried about human stupidity they would like
to focus at the top level here but i say this this existentially powerful AI box is the one
that is changing more quickly than the others it is the one that is hardest to keep track of
therefore that is what we should seek to address by slowing down the most dangerous aspects
and yes learning to harness the best outcomes but not taking everything that AI capabilities
could do by the way if we harness the best outcomes we could address the other boxes
in this picture we could in principle improve how we reason we could in principle change our
emotional makeup there could be AI assistance that gently guide us not to become too wrapped up in
negative destructive emotions we could have AI systems designing improvements to our social
systems and designing improvements to our infrastructure but in each case i've got an
exclamation mark and a question mark there it's yes but the possibilities are there but if we are
misguided if we are careless an attempt to improve these features could lead instead to
worsening it can lead to a hastening of us reaching a cascading catastrophe so we need care that's why
these two boxes are where we should put the most effort and these two boxes are the fulfillment
of what i announced in the very first slide singularity activism not everyone agrees with
singularity activism here are four alternatives that some people advocate there are singularity
skeptics who just don't think this is a matter that deserves much attention they say it's all hype
they say that these changes in artificial intelligence will not happen for decades and we
can instead concentrate on other topics these skeptics to my view have not learned or not
aware of the remarkable ongoing drivers that i've tried to describe so far in this presentation
and often they commit some philosophical error for example they say well we are uncertain
that means we don't need to do anything on the contrary the reason we often buy insurance is
because we are uncertain about the outcomes so if we think even if we only think there's a 10%
chance of a catastrophe it's definitely worth us in investigating a second alternative to
singularity activism is people who want to accelerate things there is a group who call
themselves accelerationists they want to bring it on i see this as deeply irresponsible but i
understand where they're coming from they want to obtain the benefits as soon as possible from
advanced ai by the way i do too but i think the only way we can get them is we've got to be selective
in what we accelerate and selective in what we slow down now sometimes people in this group
candidly admit yes there are risks in what they are doing but they say they are prepared to risk
a humanitarian catastrophe because they see a bigger risk for them personally or for some of
their loved ones perhaps older relatives that is a personal extinction due to aging and death
now i used to only hear this in quiet corners in pubs the first time i had somebody express this
view was at an event in new york in 2012 which left me a bit shocked but i'm hearing it more and more
people will say yes they are willing to risk the future of humanity so that they personally
have less chance of dying from aging because they want ai to hurry up and solve aging i sympathize
with that but i say again if we really want ai to solve aging we have to be selective in how we
accelerate and steer and in some cases slow down ai capabilities the other approach which is quite
opposite to acceleration is obstructive obstruction people refer to a jihad against ai referring
to the idea in the book june of a butlerian jihad this should be stopped at all costs anything that
could improve their capabilities in a an indefinite pause should be put in place
i think this is infeasible at least for the time being i also think it's unnecessary as i'll explain
for a while i would consider myself to be part of this fourth group i'm coined this term
to describe its singularity diplomacy it's to manage things hopefully wisely indeed to accelerate
research into ai safety and a alignment so that it can catch up with acceleration
in research in ai capability in effect it's like trying to rebuild the aeroplane while
it's flying transatlantic we're not slowing anything down we're just trying to restructure
so that it is more likely to end up in the destination we have in mind and it's more
likely to be able to land safely this approach was in effect championed at a series of remarkable
conferences in 2015 2017 and 2019 organized by the people behind the future of life institute
sometimes these are known as the ai safety conferences sometimes the asylum conferences
after the fact that the 2017 conference took place at a celebrated site in asylum in california
the other two 2015 and 2019 took place in Puerto Rico here's a photo of some of the attendees of
2015 event i know many other people were there who weren't in that particular photograph
and what they all said is ai is advancing quickly and we need to hurry up the work on ai safety and
ai alignment but what has happened since that first conference in 2015 looking ahead to the
present day here is the assessment which i have picked up from several people who've been at all
the conferences and who are leading this initiative they are on reflection not happy with the progress
they see ai capabilities have progressed faster than had been expected we are closer
to the singularity closer to agi than had been forecast these eight years ago but at the same time
ai is more opaque than before it's harder to understand there are surprising new features
and when you do try to interpret what it's doing often it's hard often it seems impossible
in terms of solving agi alignment that the software system would retain whatever programming
we put into it in terms of its objectives and that these would be objectives that would enhance
all around human flourishing there's been very little progress on that and various ideas as to
how progress would be made have apparently been rejected fault has been found in them either
in the technical field or in the government policy field as a result in part of that people who were
working on these topics have been diverted and distracted into real issues important issues
on risks from current ai which does deserve attention but we also need work on the emerging
risks from even more powerful ai and that is increasingly being neglected it's worse than that
the llms or let's call them golems that have been released have been connected in ways that people
previously had advised should not happen for advanced ai we've connected them to the internet
so they can learn and keep up to date at least some of them can they also have api access
meaning that people can use them to drive other behaviors so rather than being simple tools
they are more complex systems that could develop a gentle agent like features worse these systems
or open source versions of them have been distributed initially it was meant to be only
two so-called responsible developers but they have ended up on the worst parts of the internet
already such as 4chan where people have few scruples about hacking things up to
cause mischief to cause mayhem to demonstrate their own virility and who cares about the chaos
that is caused and what's also frightening is we have taught these systems how to write software
which could be great because it helps these systems to provide advice to software engineers
but it also allows these systems to design improvements in their own selves
leading to the possibility that we think we understand the level at which these systems
operate but they self-improve rapidly as has been suggested the possibility is that we are at gpd4
today the possibility is that gpd6 will be designed by gpd5 more quickly than anybody had expected
and with many more surprising new features for all these reasons the people behind the future of
life institute many of them not necessarily all of them but many of them are saying a reset is
needed this singularity diplomacy as i've called it is not sufficient we need therefore a pause on
what's known as giant ai experiments that is the thinking behind this famous open letter which you
can find on the future of life institute web page where let me read it we call on all ai labs to
immediately pause for at least six months the training of ai systems more powerful than gpd4
i do recommend that if you haven't studied it already you take the time to read what's in
this open letter they say that it shouldn't just be wishful thinking it should be a public verifiable
pause that should apply to all companies who've got the capability to train these more powerful new
systems and in case companies do not voluntarily adopt this pause government should step in in
the way that they've stepped into many other industries and institute moratorium now to be
clear this pause does not mean all ai development stops rather it is simply a stepping back from a
dangerous race to build larger systems with these unpredictable black box emergent capabilities
in the meantime the people who would have been working on that task should instead work on
ideas on guaranteeing aspects of the safety of these systems so that ideally systems that adhere
to policies that are designed would be safe beyond reasonable doubt that there's a whole bunch of
ideas in this letter improving as i said interpretation improving transparency
improving alignment improving accuracy so there is less chance of the bugs that i spoke about in
the first quadrant of four catastrophic risks and so on that's the technical request but at the
same time the letter goes on in parallel ai developers must work with policy makers to
dramatically accelerate development of robust ai governance systems it's not that the ai developers
are going to dictate the answers the ai developers realize they are out of their
depths here but the ai developers should draw to policy makers' attentions a set of risks
and a set of possible solutions which can be developed further in collaboration so the letter
refers to new and capable regulatory authorities dedicated to ai to various tracking and oversight
systems to watermarking and provenance systems to help deal with the risks of fake news or to deal
with model leaks which shouldn't be distributed outside various uh constraints and so on a robust
auditing and certification ecosystem acceptance that people who cause harm through the ai's the
design should be liable rather than just shrugging their shoulders and say hey it's all down to you
the user it's your responsibility that should not be acceptable it's not acceptable in most
industries around the world it should not be acceptable for ai yes robust public funding for
technical ai safety research so that it's not just reliant on funding from within the commercial
companies and a different sort of institutions looking at the dramatic and political disruptions
that will arise not just from when we reach ai but from the software systems that are more
advanced than today's this will cause disruption unintentional disruption intentional disruption
it needs to be anticipated and managed the letter may seem to be negative some people have
criticized it as being killjoy or some such on the contrary it closes in a very upbeat positive
message it says humanity can enjoy a flourishing future with ai having succeeded in creating
powerful ai systems we can now enjoy what is called an ai summer in which we reap the rewards
engineer these systems for the clear benefit of all and give society a chance to adapt
the letter says society has hit pause on other technologies which at the time were feared they
could have potentially catastrophic effects of one sort or another examples are listed here human
cloning human germline modification gain of function research with viruses eugenics
there are also examples in the chemical industry when it was seen that particular types of chemical
cfcs commonly used in refrigeration and aerosols had the risk of leading to the extinction of all
life on earth due to the interference with the ozone layer again politicians intervened to steer
industry and to get a transfer of chemical study so that better safer alternatives were found
the letter concludes we can do so here that is pause let's enjoy a long ai summer not and here's
an americanism not rush unprepared into our fall so that's the letter which by the way i have signed
a few days ago although initially i had many criticisms of the letter in mind i have had
these criticisms answered more than adequately to my mind so what i'm going to do next is i'm going
to look at six criticisms that are commonly made of the proposed pause the first objection is
good guys may follow it the bad guys will cheat therefore the bad guys will get ahead
who wants that as the outcome well at this stage cheats will be relatively easy to notice
to be clear what's paused is not experimentation with existing systems it's the massive experiments
to create new larger black box models and these systems require huge gpu server farms which are
very costly they're very energy hungry it's likely that the energy consumption may be visible from
space there are a few companies you can do this only a number google with deep mind
microsoft with open ai perhaps anthropic perhaps a small number of others have got the relevant
abilities and scale and understanding and here's something that is a bit controversial but i have
confirmed this in my own mind none of the companies at the present time in china or russia or iran
are on the point of creating something of a higher level than gbd4 indeed there are constraints in
china against the creation of mines that are out of control there is a concern with the way news
might be propagated with what different ai systems might say that is contrary to what
the ruling authorities deem to be good peaceful ideas so there isn't an immediate risk here
and again i'll say what's not paused is the customization or use of existing large language
models even though it's clear there could be some bad consequences of this but these consequences
will not be catastrophic not yet but if there are next generation large language models
they may not cause the extinction of humanity but they may shatter social cohesion they may
lead to all of us being distracted being wrapped up in partisan antagonism they may lead to populist
politicians gaining power and that will stop us competing with these other countries in any case
so this is a thoroughly bad objection but people take it next state they say okay cheating won't
be easy or possible immediately but if the pause is continued beyond six months in due course
hardware improvements will enable the creation of next generation large language models in ways
that cannot easily be detected in groups in these countries or other groups in for example
organized crime groups led by maverick millionaires or i should say maverick billionaires
may create these systems soon well it's at least 12 to 24 months before this will be
impossible that's why the proposed pause is initially just for six months and after six months
it's planned that there will be a wide conversation leading to a decision on whether to resume
business as usual with creating new giant ai systems or whether the pause should be continued
and that decision will depend on perceived progress with questions of safety alignment
and control whether it's technical progress or progress with governance and also whether
there is a perceived catch up from potential uncontrollable rogue actors that will be a hard
decision but we don't need to take that decision straight away now the next objection is the six
months will achieve nothing because there are no technical solutions possible to the problems of
ai control and ai alignment and i'm the first to agree these are incredibly hard problems
and many of the ideas people have advanced have turned out not to work i say that about simple
ideas when people say for example oh just switch off the ai haha that will fix it or when they say
just embed asimovs three laws of robotics these simple solutions are easy to show that they won't
work there have been more complex ideas that people have pursued and then it just turned out
little progress has been made so i agree there are no technical solutions possible
but there are still ideas which i'll list some in a moment some ideas were on the website
they may need a lot of work to make them successful but we shouldn't start from a baseline of
defeatism we should think instead that if we get enough of the ai specialist thinking focusing
entirely on these issues if we bring in new experts from outside from different disciplines
to help we could make progress for example there is the idea that we can validate the suggestions
made by these giant ai models rather than just allowing them to directly manipulate the real
world we should check whatever they produce and we can do this perhaps by taking advantage of
simpler but well defined well understood ai models and this may sound impossible how can you
check the output of a very complex general ai with a simpler ai well that is the way that often is the
case in maths and science and other fields it's often possible to verify an answer once it has been
given even though the process that obtained these answers is unfathomable so it may well be possible
to verify answers and proposals in a way that we can be confident is good before we then pass on
that instructions to the outside world now this will only work if another problem is solved
which is we must prevent these ai's developing a desire to access the world directly if they
have such a desire then it's very likely they'll be able to circumvent any restrictions we put in
their place they will find technical ways to break out of their confinement or psychological
ways to manipulate their human overseers so we have to avoid such desires being in place
which means not only avoiding them becoming conscious but avoiding them developing agency
keep them as passive tools rather than as agents now there's a long discussion that can we
take place here referring to concepts such as emergent emergent sub-goals emergent
intermediate goals and that has had a lot of discussion already but it's by no means complete
and that's an area as one example where there can be more technical exploration accelerated
in the next six months similarly people say well even if there are technical solutions
there'll be no way to ensure that they're put in place there are no governance solutions possible
but that's not true at all as well as what's written in the letter the fli the future of life
institute has published a paper policy making in the pause which lists a number of policy ideas
mandate robust third party auditing and certification use existing kyc know your
customer regulatory ideas to regulate access to computational power the idea here is that
companies who want to use sufficient computational power to train a new
larger model need to get approval of that in advance it needs to be regulated
establish the right agencies as we talked about before establishing liability for AI cost harms
measures to prevent and track model leaks expand public provision of funds for technical
AI safety research developing standards for identifying and managing AI generated content
and recommendations and there's more there's a paper by Luke Mulhauser who has long been associated
with questions of understanding anticipating controlling and aligning superintelligence
he currently works as the communications director for open philanthropy he has a document
on that website 12 tentative ideas for us AI policy now the next objection is this isn't
going to work unless you have a very strong government indeed and people say you're going to
need a totalitarian autocracy they say if you want to prevent the building of these next generation
AI you're going to need a lot of force and you need a lot of surveillance it's going to be horrific
in its extent this is the criticism the cure is worse than the imagined disease if we put so much
power into the hands of a world government that is a real risk but nobody's talking about building
world government here what's proposed in terms of these policies fits with existing well established
social mechanisms we have rules and people are forbidden from what society decides are dangerous
actions people are forbidden for driving vehicles in dangerous ways they are forbidden from use of
various explosives in dangerous ways they are forbidden from use of dangerous biological
pathogens in various ways and if necessary the violators if in the worst case need to be constrained
by force that is how society operates that doesn't mean to say we are a totalitarian autocracy
that is a scare accusation we already accept that some proactive vigilance is required in society
to spot people polluting to spot people smuggling dangerous goods to stop people smuggling
people against their will to stop people abusing power in various ways such vigilance
is potentially dangerous but we know that society can do it in a constructive way and
there's a big discussion here on what I call the mechanisms of trustable monitoring so this
objection again is far too simplistic there are possible solutions finally can different governments
around the world all agree to do this well I say you can get global agreements without having
global government you just need to base them on minimal principles we can accept the different
countries of the world that different countries have different political systems different economic
systems different cultures but within agreed limits it doesn't matter what culture or politics or
economics you observe there are various things that the world as a whole rejects not many but
there are some so the idea here is to seek agreement on what are specific catastrophic risks risks that
are credible risks that are imperative to avoid risks that are such that they transcend ideological
political and cultural divisions let's get agreement on that just as people decided in the 1980s
that the risk of clear winter the risk of an exchange of nuclear weapons that would cause
so much dust to accumulate in the upper atmosphere that all photo synthesis would cease that there
would be this terrible death from cold and starvation not just death from fires caused by
the nuclear weapons that led to agreement across strong political divisions in the same way it can
happen with AI now there are suggestions for example by Sam Altman the open AI CEO of a 2023
equivalent of the US constitutional convention the gathering of minds that led to statements such as
the importance of life liberty and happiness and so on we should do something similar today not to
define a large list of values which many people will disagree with parts of but a small list of
values that make sense for everyone and another example to look at a more complicated more recent
process whereas after the conclusion of the second world war where people from multiple cultures
helped for example by Eleanor Roosevelt of the US but people from different secular and religious
backgrounds agreed the universal declaration of human rights despite a lot of skepticism at the
time that such a thing would be possible I think we need an update on this universal declaration
of human rights that will say less but the things it says will be more fundamental and more important
there are other suggestions for research priorities suggestions I've made in the past
things which deserve more study we should clarify and highlight the ways in which
race dynamics competitive dynamics lead to human incentives that can push collective outcomes
towards terrible failures this is sometimes called the tragedy of the commons people such as Scott
Alexander have used the term mohawk for it so it's a bit like saying satan made me do it except that
mohawk is understood in a different way it's the fact that if we end up in a race then sometimes we
have to behave in bad ways otherwise we will lose out to other people who are behaving in bad ways how
do we fight against that we fight against it with public policy we fight against it by political
actions or other ways to agree uphold and enforce standards many people are very critical of politicians
many people are fearful of getting politicians involved in this space at all they say politicians
never understand the needs of fast changing technologies well I disagree there are certainly
bad examples of political interventions but there are good examples too I already referred to the
political leadership in the case of the chemicals cfcs that threaten to destroy the ozone layer
well let's look at other previous examples where policies and standards did beneficially alter the
outcomes of technological development this includes the design of systems for collectively
raising standards including how to penalize disinformation and recklessness while still
encouraging and supporting free speech and innovation we can do all these things at the
same time it's hard but we can manage and also reward patient constructive collaboration I mentioned
already the importance of understanding this concept of trustable monitoring how we can observe
possible dangerous misuse of AI systems how that can be done in such a way that doesn't also
lead to people being penalized for other sorts of things altogether such as political viewpoints
sexual or gender orientation religious or non-religious views and so forth
and then two more very important suggestions number five on this list design and enable venues
and processes where we can have positive calm dispassionate discussions on these topics where
people are able to set aside temporarily at least their prior world views ideologies and
partisan alliances discussions of this AI pause discussions more generally of risks from AI are
a frustrating mixture of good exchange of ideas and then very clever people switch back into
ideological mode where they're more concerned to defend their position than obtain a true
understanding so let's figure out how we can have these more constructive conversations
which will be aided by the final recommendation on this list which is to identify and uplift
the most powerful positive educational messages about all the topics here in this singularity
activism message let's figure out which messages put people off which messages antagonize people
and let's replace them by messages that truly engage without distorting the two messages and
in terms of improving education i want to slip in a few comments here about a project i have
supported for a number of years which i think is even more relevant now it is the vital syllabus
project which is a project to highlight and uphold the skills and principles that matter most
in these disruptive fast changing times so the project is about collecting curating
clarifying and where necessary creating resources by which i mean videos or text
which can meet various criteria and demonstrate to people the skills and principles that matter
most to prepare us all to cope with the turbulence that is increasingly all around us and the
turbulence of the threat of the singularity going wrong is just one example of a whole sequence of
landmines that are in our near future when the vital syllabus project talks about a
turbulent context it has in mind a variety of diverse pressures including dizzying opportunities
that are hard to evaluate because they are in many ways counter intuitive they're different from what
we had in the past we don't know if it's truly an opportunity or if it's something to be avoided
at the same time there are daunting risks such as the risk from AI the risks from other technologies
the risks from social problems as well these things are made worse by the acceleration of
disruption including the acceleration of AI which in turn can accelerate breakthroughs in many other
fields and what makes this especially hard is when we turn to traditional moral sources of
insight they are often losing their authority they are unable to speak confidently about these
new circumstances because what they've discussed in the past what's present in their venerable
scriptures is really ill-fitted at least on the surface to these very new circumstances but we
still need some moral decisions so this is a complex disruptive turbulent context which
is why the vital syllabus recommends significant changes in what people of all ages are encouraged
to learn and understand people at school but also people who are suddenly more interested in topics
such as the singularity and they want to figure out what does this mean for them
now as we've seen discussions about improving the future of humanity
discussions about avoiding the risks but gaining the best benefits they do typically come round to
discussions on improving educational material and that's why the vital syllabus is here it's by
no means the only project to try and gather suitable information but in some ways this is a special
distinctive project one because it understands the unprecedented transformational potential
of these emerging technologies how the future the near future may be very different from the present
it understands also that the outcome of technological changes is determined not just by
technology but by interactions of changes including changes in human expectations human cultures
human social structures and the narratives we tell each other so there's a broader and deeper
understanding of how change will take place the project understands in my view the threats and
opportunities that truly deserve the most attention and it's a final point that may take us too far
to explore but the project understands the profound relevance to these questions of the world view
known as transhumanism which in a nutshell which in a nutshell says that the way to solve these
problems isn't by accepting human nature as it is it is as one of the previous slides suggested
could be done using AI and other technologies to enhance aspects of our human nature including our
emotional regulation including our reasoning powers so we should not be solving these problems with
humans still dominated and conflicted by the constraints of the past we can to some extent
engineer and enable better humans to have better solutions so I'm issuing another call
for people to join this team visit vitalsyllabus.org and you will be encouraged to help review
and suggest new materials materials that meet these six criteria one they are vital they are
truly about skills and principles of what make a big difference to people's individual success
and to humanity's overall success in the years ahead they are focused they address the key
topics in the syllabus rather than lots of other interesting amusing humorous but less
central topics they must be accessible there is no additional payment to reach them and they should
be engaging and clear easy to understand but they should inspire and keep the attention of
viewers or readers and finally a tough criteria but one we can discuss again for each example
they should be trustworthy they should provide viewers with a good reason to trust them rather
than being a maverick unreliable uncertain so again I'm asking for people to look at the
content which has already been assembled on the many pages of vitalsyllabus.org give your
feedback as to which parts you like and which ones you think should be changed ideally also
suggest new ones to put in their place you can do this through three links you'll find on vitalsyllabus.org
slash connect the connect page there is a discord for people who like using discords there is a link
to our facebook group for people who like using facebook there is a link to our linkedin group
for people who like linkedin so to look at this project it has spelt out 24 top-level areas
and in each case there are substantial materials already in the relevant pages but there is plenty
of scope for improvement so finally to recap everything I've said in this advocacy for singularity
activism there is an unparalleled self-propelled acceleration of AI capabilities taking place
we must not accept it just as a given we must seek to steer the capabilities and we'll do this
in four ways first we will temporarily in my view pause these giant experiments to give us time to
catch our breath and to focus on other things namely accelerating research on technical safety
accelerating research on governance scenarios and what I've just been talking about
collecting curating clarifying and creating powerful positive educational messages
whether on the vital syllabus site or elsewhere now thank you very much for listening to that
rather long but I think and hope appropriate set of messages about the singularity and how we
should respond to it please reach out to me on the London Futurist site or via my social media
for example on twitter I am at dw2 I'll be delighted to discuss this on podcasts to speak to groups
about it and to continue the conversation I want to learn I want to improve this set of information
so that I can be more effective in ensuring we will have our wonderful positive singularity
rather than any of the negative catastrophes that I've mentioned thanks

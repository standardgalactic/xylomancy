Hi, my name is Brennan Shanahan, and today I'm going to talk about Bayesian neural networks.
In the first part of the talk, I'll give a brief, high-level introduction to neural networks
that broadly covers what they are, what they do, and how they work.
I'll spend a bit of time talking about the stochastic optimization procedure employed by neural networks,
and then I'll discuss two common neural network architectures that are used in experiments in a later section.
Next, I'll give an equivalent description that frames neural networks as conditional probabilistic models
and outline the parallels between the two definitions.
I'll then discuss the shortcomings of these models, which will motivate the introduction of Bayesian methods in neural networks.
Finally, I'll discuss the results of two experiments employing Bayesian neural network.
One regression experiment that compares the results obtained by a plain neural network and a Bayesian neural network,
and one image classification experiment that further illustrates the benefits of Bayesian learning.
Neural networks have been rigorously shown to be universal approximators
capable of representing arbitrarily complex nonlinear functions to arbitrary precision.
As such, neural networks are used in many tasks which require learning incredibly complex mappings between input and output space,
such as in speech recognition and text generation, sentiment analysis, and image segmentation and classification.
To take a concrete example, say we're building a network for use in an autonomous vehicle which specifically performs object detection.
You can imagine a function that maps, say, an image of a typical city street from the point of view of the driver
to a set of categorical boundaries, each of which represents a pedestrian, another car,
a traffic light or street sign, a crosswalk, and so on is highly nonlinear.
The function also needs to be extremely adaptable as it's being computed in near real time and the data it acts on isn't static.
The car is moving, pedestrians are walking, other cars are moving, lights are changing, and no two images that it captures will be identical.
A scenario like this makes it clear that it's not sufficient for our network to simply learn a deterministic mapping based on a set of training data that it's seen before.
In other words, the network needs to be able to generalize well and extrapolate to new data based on the context of data that it's already seen,
but it also needs to have some notion of the uncertainty inherent in its decisions and be able to consider this uncertainty when taking in action
in order to prevent making overconfident, incorrect decisions.
The basic computational unit of a neural network is referred to as an artificial neuron, or more simply, a neuron,
in which developed historically as a proved mathematical model of a biological neuron.
A neuron takes some input data to which it applies an affine transformation with a weight matrix W and bias vector B,
both of which are learned parameters, and finally, a nonlinear activation function as its output.
The simplest neural network architecture is a fully connected B forward network.
Neurons are arranged in layers, which are stacked one on top of the other, starting from the input layer,
followed by an arbitrary number of intermediate hidden layers, and finally, an output layer.
This architecture gets its name from the fact that a neuron in one layer is fully connected to every neuron in the two layers at sandwich between,
and a neuron's output is fed forward as input to every neuron in the next layer.
The key to a neural network's ability to learn nonlinear representations of their input are the use of nonlinear activation functions.
Simply applying successive affine transformations at each layer of a network is equivalent to applying a single affine transformation.
Therefore, a neural network that only uses an identity activation function in each neuron can only learn a linear mapping between its input and output spaces.
The form of a network's output layer depends on the type of data it processes and the network's intended task.
For classification of a label's categorical data, the output layer consists of C neurons that computes a probability distribution
over the C possible classes of the data and assigns a prediction which maximizes this distribution.
It then computes a cross-entropy loss function between the true and predicted distributions of the data,
which is usually averaged over the entire training set.
For regression, the output layer consists of a single neuron which computes an identity activation function
and, typically, either the mean absolute error or mean squared error loss between the true and predicted output.
It's important to note here that while the output layer computes an identity activation function, hidden layers still compute nonlinear activation functions.
Neural networks are represented internally by what's referred to as a directed acyclic graph,
which allows for decomposing complex functions into a sequence of simple arithmetic operations.
This is especially important because neural networks learn their parameters using optimization procedures
such as stochastic gradient descent for one of its many variants,
which require computing the gradient of the loss function with respect to the network's input.
For functions whose gradients do not have a simple closed form expression,
this internal representation allows the computation of a single intractable gradient
to be achieved instead by taking simple local gradients with respect to intermediate variables
which are defined and cached locally during training.
Gradients are first computed with respect to neurons in the output layer
and then propagated backwards through the network's hidden layers to the input layer by a straightforward application of the chain rule.
At each neuron, its parameters are updated by taking a step in the direction of steepest gradient descent
with the step size controlled by a hyperparameter lambda referred to as the learning rate.
Training proceeds by performing a forward pass with the entire training dataset and computing the average loss,
which is then back propagated through all the network's parameters to the input layer.
This forward-backward sequence is referred to as one training epoch.
Parameters are modified during each epoch and the network is said to have converged
when the loss function plateaus over many subsequent forward passes.
Another important neural network architecture is the convolutional neural network,
which is designed specifically to operate on image data and to take advantage of the additional structure of
and information contained in an image's color channels.
Convolutional neural networks tend to learn incredibly rich, local, and translationally invariant features that make them extremely powerful,
and they've been the primary building block in most state-of-the-art neural network designs in the last decade.
Convolutional neural networks are distinct for the use of convolutional layers,
which differ from fully connected layers in several important ways.
First, while each neuron in a fully connected layer will see the entire output of the previous layer,
each convolutional feature map has a limited receptive field and only sees a fixed region of the input.
Additionally, a fully connected neuron computes an affine transformation by acting on a flattened, vectorized representation of the input image,
whereas a feature map is formed by convolving the color channels in its receptive field with a set of filters, which are the network's learned parameters.
In other words, whereas a neuron in a fully connected layer acts on a one-dimensional input vector to produce a one-dimensional feature vector,
a convolutional layer acts on a three-dimensional image volume with dimensions of width, height, and color channels
to produce a three-dimensional feature map with dimensions of receptive field width, receptive field height, and filter size.
Neural networks can be equivalently understood without making any references to their biological motivations or internal architecture.
Here, we define a neural network as a conditional probabilistic model which is conditioned on a set of training data D
and its learned weight and bias parameters, which we simply refer to as theta.
Given some new input x hat, the network computes a probability distribution over its entire output space
and selects an output y hat, which maximizes the distribution.
To draw an equivalence between these two descriptions, in a classification setting, the output space y is the set of all possible classes,
and the model p is a categorical distribution, and the corresponding loss function is the cross-entropy loss.
For regression, the output space y is the real numbers, and p is a Gaussian distribution, and the corresponding loss function is the mean squared error loss.
Nothing about the actual implementation changes and parameter optimization proceeds by back propagation, as described in the previous section.
Parameters learned during back propagation are maximum likelihood estimates, or rather stochastic approximations.
This likelihood model presents several shortcomings.
Networks whose parameters are maximum likelihood estimates result in predictions that are deterministic, in the sense that, once trained,
predictions of a given input corresponds to a point estimate, and will always result in the same output.
And these networks are therefore prone to overfitting and unable to express uncertainty about their predictions.
This can be especially problematic when generalizing the data that they have not been trained on.
Techniques such as L1 and L2 regularization, commonly referred to as region lasso respectively, are commonly used in practice to counteract overfitting.
But these networks are nonetheless deterministic and unable to express uncertainty in their predictions.
Bayesian inference for neural networks proceeds by defining a prior distribution over the parameters of the network and applying Bayes' rule to compute their posterior distribution.
Predictive queries about some new data x hat correspond to computing the predicted distribution of an unknown label y hat given x hat by taking the expectation of the conditional distribution of the new data with respect to the network's posterior.
Every posterior weighted parameter configuration contributes to the prediction of the label y hat given the data x hat.
Therefore, taking the expectation under the posterior is equivalent to computing a weighted average of predictions from an ensemble of plain neural networks,
each of whose parameters are drawn from the same shared distribution.
Because their predictions correspond to posterior samples, the ability to express uncertainty of the parameters in subsequent observations is built into Bayesian neural networks.
This ensemble averaging in subsequent parameter uncertainty has regularizing effects on Bayesian neural networks predictions, which are equivalent to the regularization methods previously discussed.
As with most interesting Bayesian models, computing the posterior distribution is analytically intractable and must be approximated, which is generally done using sampling methods such as Markov, Chey, and Monte Carlo.
Bayesian neural networks instead frame posterior inference as an optimization problem over the parameters pi of a target variational distribution by minimizing the cubic Liebler divergence or KL divergence between the variational distribution and the true posterior.
At inference time, the network parameters theta are instead sampled from the variational distribution.
The corresponding loss function is commonly referred to as the variational free energy or the variational lower bound, represents its tradeoff between maximizing the expected log likelihood of the data with respect to the variational distribution,
referred to as the likelihood cost, and minimizing the KL divergence between the variational distribution and the networks prior, referred to as the complexity cost.
Equivalently, variational free energy represents a tradeoff between satisfying the complexity of the data and the simplicity of the networks prior distribution.
Because we're minimizing an expectation with respect to the known variational distribution, we can approximate the true loss by instead sampling from the variational distribution and computing the approximate loss in a process similar to the aforementioned Markov, Chey, and Monte Carlo approach.
By assuming the samples from the posterior are normally distributed and uncorrelated, we can apply a local reparameterization technique, which allows us to sample parameter free white noise epsilon,
then shift and scale epsilon by a deterministic function of mu and rho, mean and standard deviation of the variational distribution respectively, to obtain a sample theta from the variational posterior.
In this context, the back propagation is slightly modified and referred to as Bayes by back prop. Now gradients are taken with respect to the variational parameters mu and rho, which are then updated by taking a step in the direction of steepest gradient descent.
Network training otherwise proceeds as normal, performing alternating forward passes and backward passes until the network parameters are converged.
I'll now describe two experiments employing Bayesian neural networks.
The first is a regression experiment on a toy data set, which compares the performance of a plane feedforward, fully connected network to that of an identical Bayesian neural network.
Training samples are generated using 2048 equally spaced points between zero and one half.
Both neural network architectures consist of two fully connected hidden layers with 215 neurons each in RELU activations and an output layer with a single neuron and identity activation.
Training occurred over 500 epochs with many batches of 32 samples.
Once fully trained, each network was used over 500 trials to predict the test set 4096 samples run uniformly randomly in the closed interval of minus one half to one, and the results were averaged and are given in the plot in the bottom right.
For the plane neural networks results are on top, and the Bayesian networks results are on the bottom.
The red line is the mean prediction over 500 trials, and the dark blue and light blue regions are the one sigma and two sigma confidence interval respectively.
A few things are interesting.
First off, the plane neural network underestimates the true variance of the test data in the region of the space that it was trained on, which is maybe not so surprising given that we know how permanent these types of networks are overfitting.
However, in regions of the test base where the networks have not been trained, the plane neural network chooses a particular extrapolation of the training data and subsequent test predictions have extremely low variance, which goes towards zero as points in the test base gets further from the training set.
The Bayesian network on the other hand accurately expresses the variance in the training data when making predictions, which can be seen by the fact that essentially all of the training samples fall within two standard deviations of the Bayesian network mean.
Compared to the plane network, however, the variance in the Bayesian networks prediction actually grows in regions of the test base further from the training set.
Because each of the Bayesian networks predictions correspond to the prediction of an ensemble of networks, each of which samples independently from the networks posterior distribution and chooses a particular extrapolation of the data as the plane network did,
the Bayesian networks increased variance is therefore a reflection of averaging over the uncertainty of these ensembles in regions of the output space that they haven't been trained on.
The next experiment is an image classification experiment on the MNIST dataset, which contains 60,000 28 by 28 pixel images of handwritten digits collected by the US Postal Service in the early 90s in order to develop a mechanism capable of automatically sorting mail based on zip codes.
Image classification was performed using a Bayesian convolutional neural network whose architecture is described in the bottom figure.
It consists of three convolutional layers. The first layer has 32 filters in a 24 by 24 pixel feature map, while the second and third layers have 64 filters each and an 84 by 84 pixel and 64 by 64 pixel feature map respectively.
Three convolutional layers are separated by two cooling layers, which are standard sub sampling layers used to keep the number of parameters in the network small.
Here they down sample each convolutional feature map by a factor of two in the width and height directions.
The final two layers are fully connected and have 128 neurons and 10 neurons respectively.
All hidden layers compute RELU activations and the output layer computes a softmax distribution over the 10 possible digit classes of each image.
The training occurred over 3000 training steps or approximately 23 epochs with many batches of 128 images per training step.
At the 10th, 100th, 500th, and 3000th training step, 10 images were randomly selected from the validation set and classified 50 times each, corresponding to 50 independent samples from the network's posterior distribution for each classification image.
Four wide columns displayed here show the sampling results in each of these four training steps, and the three narrow columns within each wider column show the image classified to histograms posterior samples in the approximate predicted distribution over the 10 digit classes given the corresponding image.
At the start of training, posterior samples have a high variance and probability masses distributed more or less uniformly over the 10 digit classes from most images,
owing to the fact that parameter values are randomly initialized at the start of training, and 10 training steps isn't enough time for the network to learn anything meaningful.
As training progresses, the posterior variance increases as the network is able to correctly classify images with increasingly high probability, although many images, such as the 9s in the 1st, 5th, 7th, and 8th rows, are still being incorrectly classified much of the time.
By the 500th training step, learning becomes slower, however the posterior variance is still decreasing and the accuracy of the predicted distribution continues to improve on the whole.
By the end of the 3000th epoch, the network is fully converged, posterior samples show almost zero variance for the overwhelming majority of images, and the network's predicted distribution has a classification accuracy of 95.3% on the full test set.
As was the case in the regression experiment, we see that Bayesian network's uncertainty is captured in the variance of its posterior distribution.
We can also see how the distribution of the first two moments of the posterior change between the beginning and the end of training.
At the start, both means and variances are tightly peaked, whereas at the end, means are distributed less sharply and with a slightly heavier tail, whereas variances are essentially flat in some layers.
As a final word, both experiments show us that employing Bayesian methods in neural networks is preferable in scenarios where we would like to be able to express uncertainty in our neural network's predictions.
Finally, all works cited in this presentation are listed here. Thank you for your time.

So, in the beginning, I want to briefly introduce about the history of the LLM.
That's usually, like, personally, that's how I learn about a new subject, because I want
to see, like, how it evolved here now, and also from this, you can see, like, where it
goes in the future, right?
So it will be always good to just look at the history of how this LLM evolved from since
2014, although it's not LLM, that time till time right now.
So everything, I think, initiated from the attention mechanism.
So if you look at my step-by-step into transformer block, a lot of content is on introducing the
attention mechanism.
It's basically provides a better contextual embeddings of the word in the sentence way
in the paragraph.
And just like us, like, we pay attention to specific material rather than, like, attend
to every pixel of what you see or every word that you have heard of, because we pay attention
to specific important words or important view.
So that's what it's attention about, like, we're trying to mimic, actually, like, humans,
like, to pay attention to important things.
And then after attention is proposed, like, the transformer is proposed in 2017.
This is the first, like, kind of encoder, decoder structure using or using attention
mechanism, and it shows improvement comparing to the previous recurrent neural network.
So previously, like, in NLP, everything is on recurrent neural network.
And transformer kind of break that and show that just by using attention.
You can actually improve the performance in the various NLP task.
And then in 2018, the bird and the GPT-1 is proposed.
So chat GPT actually evolved from GPT model.
The first one is evolved, it proposed in 2018.
So and also I have put the model size here.
So at that time, the model is not even reaching 1 billion parameters.
So the model size is quantified by how many parameters in the model.
So the larger, the more parameters, the more powerful the model, the more powerful that
model can maybe more complicating pattern.
So at that time, in 2018, the bird model and GPT, the very first model of GPT is around
only 100 or 300 million parameters.
And then GPT-1 grow into GPT-2 in the next year and grows from 100 million to 1.5 billion.
So finally above 1 billion parameters.
And then in the next version, and GPT-3 is actually grow already onto like 175 billion.
So I think this is the, really the pioneer of the LL model that is above like 100 billion
from 2020.
And then in 2021, so Google, Microsoft kind of proposed, like this one is from Google.
And also this is the model behind BART, like the Google kind of competent to charge GPT
called BART service.
So it's called the Lambda architecture.
So it's again around 170 billion models.
And this MT-NLG-1 is a collaboration between Microsoft and NVIDIA.
So it's like a really huge one mainly for English language model is like 500 billion.
And then I think what really brought everyone's attention to LL is the release of charge GPT
the end of last year, right?
So everyone's, the charge GPT is by far the fastest growing APP in human history so far.
Like it's reaching how many, yeah, so it's like basically gain lots of users in a very
short time.
That's when kind of everyone knows about LLM or what is the charge GPT?
So charge GPT actually is the evolvement from the GPT-3 by additional techniques, which
I will introduce next.
And also this Palm model, Blue model, Flam model, Yam model also coming out.
Like after kind of charge GPT looks so successful as every company, especially the big company
kind of join the competition, like releasing the commercial APIs and also releasing this
open source LLM.
So the blue ones, at least here are able for research and commercial use.
So basically you can launch it for commercial use.
And the green ones is only for research purpose.
So you cannot launch it for your business.
So in 2020, the Flam actually is also open source model from Google.
So it's like a 20 billion model.
So these are all like a commercializable models.
And then into 2023 this year, so we are like a little bit past half a year this year, there's
already like a long list of LLM like from GPT-4.
So GPT-4 is another improvement over GPT-3, which it covers more multi-lingual like multiple
more languages and also the reasoning works better.
And also it's able to take in the images as the prompt or as the output.
So it becomes a multi-model LLM.
And of course it's also releasing a mini version of GPT-4, which is also commercialized.
You can launch it.
And then the LLMA model is a very popular, famous model in the community from Meta.
So LLMA-1 is like a 7 to 65 billion models.
Again, this only can be used for research.
You cannot use it for your business to launch your service.
But later this year and actually just maybe last month, they also released the LLMA-2.
So this one is you can use it for commercial purpose.
And also Google increased their version from Palm-1 to Palm-2 model.
And again, they kind of upgrade their API service.
So if you want, if you need access to Palm model, you need to use their API.
Yeah, so this is where we are.
So I do see that like from beginning until now, like more and more LLM,
especially the open source community grows larger and larger for LLM.
And yeah, and also this additional dependence derived to further improve this LLMA-1 model.
I believe the LLMA-2, there will be a game additional derived, the next version of a developed based on the LLMA-2.
Yeah, so this is where the community is going.
And I do see the open source community seems to gradually catch up with the closed API as well.
So that's the history and then the next transformer.
So like in the beginning, I send you a poll about you can still vote on the poll,
basically to understand what's your knowledge on the transformer.
And if you think you want, you are interested in no more, so you can read this medium block.
So this will give you a more detailed overview of the attention mechanism and about the self attention module in the transformer.
So overall transformer consists of encoder and decoder.
Encoder is basically to encode your sequence such as a sequence of words or your language into some embedding space,
just a numerical vector.
And then it pass on to the decoders and then the decoder is like rolling it out,
think of it as a machine translation.
So the decoder will decode that piece of numerical vector into a different space,
say from French to English.
So the next example is like, so this is the encoder, your input is French, right?
And then it will encode this sequence into a numerical representation vectors.
And then it will pass it down to the decoders and then it will decode step by step.
So it will first output the first word and then the first word becomes the input at the next time stamp.
And then we'll decode it one by one until you get your whole sequence,
which is your target sequence that you want to translate the French sentence to an English sentence.
So that's the overall is a transformer what it does and actually a transformer sets the foundation of all the model.
So if you just use the encoders, then it's called a bird model.
So it's like a bidirectional encoder representation from transformers.
If you only use the decoders, then it is a generative pre-trained model.
So the GPT, so all GPT models use only the decoder part.
Yeah, question.
Yes, I was wondering, do you think we'll ever have a reversal vector language?
What could you describe a bit more about the universal?
Well, you just described trans there and coding into a vector language and then decoding from that vector language into another human language.
And I was just wondering, what are the prospects for having a universal vector language?
Actually, it is having a universal language based depends on the data sets you feed it.
So, yeah, so if you are able to feed the model, the world of universe of the languages you want to model,
then it will learn itself, like what are all the language modeling, what are the good representation for each word given the context.
Yeah, so it will basically learn based on your input actually.
So it sounds like you're saying that there may not be ever a universal vector language because it's so dependent on your training.
Yeah, but right now the LOM is trying to basically to achieve that universal representation because it's facing like a really huge data sets that represent all the most all the languages that we generated so far.
All right, thank you.
So, in terms of like a training LOM, the two most important concepts is the pre training and the fine tuning.
So, pre training what it does so it basically is more for the language understanding.
The language understanding you can think about is like the grammar, right, so how to put together a sentence that makes sense.
So basically that's what it tried to learn so it may, the sentence it outputs may have no meanings, but the sentence is all grammar correctly correct, and maybe has some reasonings over there so that's the purpose of pre training.
That you just fit in all the unsupervised purely text say scroll from the website, say the Wikipedia's right so you just fit this with all the unlabeled text, you are able to connect and to do the pre training purpose.
And after that, the language, the model kind of know, okay, how can I put together, Grammarly, well, a sentence from that, so you do well learn by the self all the grammars and all the, like the maybe a little bit reasonings were relationship how to put together a good coherent paragraph.
So that's the purpose of the pre training.
We don't want it to speak nonsense right so we usually need the AP model to actually do some tasks, and then that is where the fine tuning plays a role so it basically adapting the pre train model that's seen lots of text already know how to structure the sentence, and then adapt it to different downstream
tasks, no matter it is for question answering for name entity recognition.
Basically, you just take the pre train model and then you further fine tuned, meaning you further update the weights of the model, so that you got minimized loss function for each downstream task.
Okay, so the next question is like how can we do the pre training.
So, usually the pre training is quite straightforward right so it's just like a next token prediction, which means like for GBT model, you just give the preceding words, and then that's a model to predict what is the next word.
And then the bird model, instead, is because it's bi directional so you just randomly mask some word in the sentence.
And again, that's the model to predict what is the master word in between.
So this is basically a pre training you just randomly sample some sentence and for GBT you just give all the preceding words and let it to predict the next word.
Yeah, so this is a GPT pre training so you have a large unlabeled text corpus.
If you really want to train a universal model you need a really large input, like the Wikipedia, like our human knowledge base, like the Common Core, another very big data sets.
So you sample the data, right, and then you just passing the preceding words.
And then let the model to predict for example the chicken cross the road, right, so if the model is able to give a high probability score for road, it means that okay, this language model kind of knows what the previous words mean.
So in the pre training so this is the data set used to train the GPT three model.
Basically this is like a different data sets and this is like how many tokens in that data set so the larger means that the larger the data set it is.
So the Common Core is like a really huge data sets like the commonly public data on the internet.
And also you see like the Wikipedia only has three billion tokens, whereas the Common Core data set has like 400 billion tokens.
So of course they don't want the model to pre train like any bias data so they kind of do some sampling.
So they basically in their training languages about 60% are from the Common Core and super from Wikipedia.
And this is how many times it shows up in the in the training session so the Wikipedia is like more likely to show up more than three times.
In the pre training period and the Common Core like some words hasn't even been used to pretend a model and on average is like only 0.44.
This is just to balance out between different pre training data sets in GPT three.
Okay, I will pause here to see if there's any question on pre training.
Are there any books that go into like the architectures of like LLM anything like that, like the math and like the code of it.
I actually haven't read any book. It's more like reading the papers.
So if you are really interested in the math, go read the papers, because it's just evolving so fast. I don't think the book will catch up with the literature.
I kind of figured that.
Yeah, so if you really want to go deeper, you should like like read the paper like there's almost new breaks of like new ideas coming out every month.
So if the book if anyone is wanting to write a book by the time it publishes it already becomes obsolete.
So yeah, still like developing field.
Yeah, I think there's a book on packed up that's pretty good on transformers second edition and you to me has a pretty good video on transformer from bricks.
Yeah.
And another like tutorial in terms of a coding the hugging phase, because most of the open source models are able to serve on the hugging phase.
Yeah, and they they have very good coding tutorials and the packages of how to how to use this public models and data sets and how to fine tune it on your own task.
Okay, any other questions, any till till till now, or we can move to the fine tuning, which is the most interesting and more challenging sessions.
Okay, then let's go to fine tuning.
So, previously fine tuning what it does.
So it will take a pre trained model.
And then fine tune it on specific task, and then the inference will be also on the same task.
So this is the area like before I am like we take a pre trained model.
If you have a goal, or if you have a task, you just fine tune that lm per task, and then not not lm that language model per task, and then inferred on the same task.
So, how many tasks you have you may end up with how many models right so you basically you have you may have like one model per task.
So that's usually like what it looks like the fine tune it before the lm.
And, and also this is where I think most an IP application looks like right now we still like a fine tune one pre trained model per task.
And then when it comes to GPT three, so it has like 170 billions right, it becomes interesting that we, we don't need to fine tune it individually for each task.
So at the time when the GPT publishes, it's actually directly just using the pre trained model, and directly use that for inference on different tasks like task A, task B, task C.
How it's going to do that so basically you give instruction to fulfill that task.
Right so you just said just like the chat GPT right so you just tell it how to fulfill the task.
So basically we call that prompting. So basically you, it's like you give an instruction to the model, and you provide examples of it, and let the task generate the corresponding output.
So without any fine tuning. So actually will save lots of computational efforts as well. So you just have like a why really large model that can cover many tasks.
So that's the idea from the GPT three, the, the, the, the, the really large L M and actually right now all the L M is trying to improve the performance on this as a whoever gives the best pre trained language model that can work on different tasks like zero shot,
like given no examples were given very few examples, it can give whoever gives the best performance kind of wings, so that the chat GPT, like no matter what you tell it, it's able to give you the correct response.
And then, but we are not there yet. So the L M still like, I think they will be still development needed to get to that why universal model till we gather, there will be instruction tuning process.
Well, we have this pre trained model, and then we instruction, like fine tune it on many, many different tasks. It's not like the previously just fine tune on one specific text.
You can, you can fine tune it on many, many different tasks. And then you can use this still is one model, but can perform many different, even the unseen task.
Basically, you teach the, you teach the L M like how to understand the problem of different tasks, and how to execute on that. So if they see enough examples, we hopefully we add the model will generalize to the unseen situation.
The last one is the reinforcement learning with human feedback.
So basically, it's so we have the instruction tuning.
And also we want to align the model with the human preference.
And usually the instruction tune cannot quantify the human preference very well.
So that's why the reinforcement learning is proposed and hopefully to align the model output with the human preference.
Yeah, so these are these four fine tuning strategies.
So here's the examples.
So traditional fine tuning you need you have multiple examples.
Say this is a translation.
And for each of the example you are going to update the model parameters through the gradient descent.
So basically the fine tuning involves updating the model parameters based on each of the example.
So in the prompting, you just like just like how you use charge a bt.
You directly input the instruction, what is the task description, and then you prompt it right so you just say like, okay, I want to translate English to French, and this is my input give my the target language.
And give it like multiple examples like you can just provide it. Okay, here are some examples of doing this task.
So I give you a new example could you generate the output for me.
So this is called a few shots where you provided like a different examples.
So that's prompting on the instruction tuning.
So basically, you need to formulate the data sets.
So you need to first generate this instruction tuning data sets.
So this is the input your task descriptions, what you want the model to do.
And then you give say a few options and this is the target sentence you wanted to generate.
So basically you can do this for many different tasks by formulating text to text to text data format.
And if you kind of fine tune the model updating the model parameters, based on this input and target pairs.
And then in at an inference time you just give it again a different task description.
And then that's the model to generate the corresponding response.
So the goal is like, if the model seen enough input target pairs.
We hope it to generalize better to understand to teach it to understand the task and also to teach it how to, you know, figure out the reasoning logics and how to fulfill the task correctly.
This is the GBT model.
So this is the GBT three model without the fine tuning.
And this is the GBT three model with the proper prompting with a few shots.
And the blue one is the flat model flat model is the instruction tuning model so you can see the gradual performance improvement.
With the different fine tuning strategies.
So in the instruction fine tuning, you can use a different template for this task.
For example, this is a context information you give to the model, and this is the hypothesis.
And then your target is okay based on the context whether the hypothesis is correct or not.
And to do that you can give like a different task descriptions right so you can say this or you can can we infer this.
So basically like given one task you can formulate like a different various forms of the task description, and then you can insert in the corresponding context hypothesis and what is the expected options for the output.
For the flat model they actually train this LM on many different task clusters.
Yeah, so each block is a data set so you can imagine how many, how many data sets they have the LM has been trained on.
And these are like a big block describes about the categories of this task.
So this is the inference task common sense sentiment task question answering summarization so basically you can formulate it all in that input target pairs and then train the model to see all these different tasks clusters.
So this is the result from their paper, and then they find that actually after certain model size, like the smaller model actually they find that the instruction tuning doesn't help comparing to the end tune model.
And then for the really big model like above say like 68 billions or 130 billions, the instruction tuning really outperform the end tune model.
And also they report the performance to see like how the LM can do on the unseen, unseen cluster of tasks.
And also they add in the clusters of tasks one by one right so in the beginning, you just have the summarization category to instruction fine tune your model, and then you use that model to infer, say the close book QAA or common sense and just to see what's the performance to see the general
visibility of the model on the unseen task.
And also you can for the second time you add in the translation data set for the instruction tuning, and again report the performance at the inference time for the, for the unseen category of tasks.
So the more, more data you put for your instruction tuning, the better the performance it get you will get the better, the better the model can generalize to the, to the unseen, to the unseen task.
Yeah, this is basically what it's trying to tell.
And then that's the instruction fine tuning and there is a separate direction in terms of how we can efficiently fine tune of our app.
So in the literature, Laura is proposed basically like without fine tuning all the parameters in the model because for LM is quite a lot and make it impossible for individuals especially without a big cluster to fine tune the LM.
So they propose low rank, low rank representation of the weight matrix.
Basically, you mainly using the SVD right so you don't need the full dimension you just need like a compressed hidden dimension are much smaller.
So that instead of fine tune a GPT three with so many parameters with the Laura, you only need to fine tune the model with 4.7 million parameters which make it possible to for each individual person to run on a smaller machine.
And actually the performance is also comparable to like fine tune the full LM fine tune the full model. So Laura is like a very, I will say right now almost everyone, like if you use the, like, if you need to fine tune so Laura usually is the first option.
Like to to fine tune.
Yeah.
So that's it for the instruction fine tuning. So before moving to the reinforcement learning. Any question.
I was wondering if one of the things that these large language models lack is reasoning the ability to reason. And so, for example, if you read an article, let's say it's an opinion piece in the New York Times, you may agree or disagree with it based on your prior knowledge of that topic.
But the large language models aren't able to do that they just absorb everything equally, and hopefully the truth wins out in the number of samples that they see.
So, if you were to add a reasoning capability, whereby you have the ability to say well if a and B are true then I know that C is true and base it on existing knowledge.
So if you were to insert that would that be the common sense curve that you showed.
Yeah.
Yeah, so basically like if you show a more examples like that right so if you're able to show more reasoning examples.
So you will tend to work better for those similar reasoning. But I don't see there's still right now in and there's no specific like component like attention next specific component just to model the reasoning.
It's more like connect a data sets your domain, for example, like how to do, if you see the news like the LM even passing the bar for the law exam, and also passing like the college exam, right so actually that I am is trained on lots of like a previous exam.
Like Q&As. So they have seen like lots of like a Q&As and then like that, like in the model and then when they see a similar exam they are able to do it well on that.
But if you shift it if you shift it to a different like a totally different one, it may fail until you kind of continue like just like a trained individual person like you need to better prepare it for the exam.
Yeah, like, I know, I know that Rodney Brooks has said that he feels that this is going to be the next advancement that we're going to use more traditional AI tools to add that reasoning to the large language models.
Mm hmm.
Yeah, yeah.
Yeah.
Thank you.
Yeah, and I know that the reasoning there's also using LM for the mass derivation.
Yeah, yeah, I think it depends on how you formulate your problem based on the input and output text. Right. So if you can formulate your your problem into that format, the LM can kind of can be further fine tuned on that.
But yes, I do agree that's how we can add the common sense, like the the, for example, some people also think about the graph neural network.
That is so kind of a network of all the relationship and entities in the world. And how can we add like additional reasonings on top of the network of knowledge.
And which can can gain like more efficient way to to do the derivation and calculation but right now, the LLM, at least the release where I see right now, most of it are like a really large model really large data sets and then somehow in place.
Yeah, yeah.
All right, thank you.
Any other question.
Are these slides available.
Yes, I will, I will share it later.
Okay.
Yeah, I will also share the recordings.
So, we have seen the instruction fine tuning basically you need to prepare the input output text pairs to further fine tune the LM is highly effective, but it's also has limitations.
Because of the limitation of the learning objectives.
As you can see for the GPT model is always to predict the next word.
So it basically seen lots of examples and also it's tried to try the best to predict the next word.
But sometimes, I mean, for math, we know exactly what is the next word like three plus one is for we know the next word has to be for that's it.
But for many test generation tasks, the net, there's so many vulnerabilities of the answers right it's hard to say.
Okay, whether that answer is correct or not.
But we can comparatively say, okay, which is, which is better which is more aligned with what I hope it to achieve. If you ask the GLM to write a poem.
You cannot say, okay, the next word is is raining is correct, you cannot say that right so there's an imitation like if you just use the next token prediction as your objective.
So that's where people want to change the learning objective in the fine tuning.
So that's where the reinforcement learning with a human feedback was born. So basically it's trying to to change or basically to model the objective function instead of you give it a fixed like next word prediction probability like I'm trying to
trying to like maximize the probability for the actually of the word with actually observed word.
Instead of that you you want the objective function to be learned. It's not a fixed function you wanted to be learned to align with the human preference.
So how do they do that.
Basically they reshape the training data. For example, explain the moon landing to a 60 year old.
Okay, what is the next word to generate it's hard to say right there are so many different is you cannot say the next word is the one that the next one is people like whichever which is right, you cannot say that.
But like after you complete the whole paragraph, you can ask a human to say okay which which paragraph makes more sense for the input will align better to human preference.
Okay, and then and then a human reviewer will say okay the completion to is better than completion one.
So this is a human preference relative preference and then we can train objective to so that the preference score kind of aligned with the human preference.
So the hell of a framework for the reinforcement learning with human feedback. Again, they start with the model pre training like the next word prediction so the same foundation model.
And then they have a reward modeling to model the reward for each of the output.
The higher the reward, the more likely that output will be preferred by a human so that's actually they use another neural network to model the reward.
And then the input will be the output from your model and the output of the reward model is a reward. The higher the reward value is a scalar value the higher the value.
It means that the more likely your output is preferred is favored by a human.
And then it's going to use this reward model to fine tune the model to fine tune the model so that it can maximize the reward output from from the model.
So right now the objective is not. Well, at some sense they they also combine the two objectives. The one objective is the next text, next word prediction.
They add additional objective which is to maximize the reward so that the output is favored by human.
I think it's the same. So you have you basically just initialize your model by using the language model prediction predicting the next word so that we have the common.
Logics and the common grammars captured and the model can generate good, say a good English sentence.
So, in the reward training.
We just gives like many, many different prompts.
And then we ask a model to generate a different text output.
And actually for the same prompt the model can generate many different outputs if you where there's a temperature parameter.
You can you can tune the higher temperature the more likely that if you regenerate it you will get a different response.
I think the same with the chat GBT you can ask you to regenerate so you will give you a totally different response.
And then you can ask humans to score it and also compare it to relatively basically the rank or the output like this is the most favorite one second most favorite one.
Etc.
And then they will train the sample and the reward pairs so that they so that this reward model the scalar value can help maintain the rank.
So that if if all this the output the output by the by the lm.
As the input to the reward model and you get a scalar value for the ranking. So in the end if the ranking aligns with the human ranking.
Then it's doing its job so basically it's it learns like what what is favored by human there.
This is a little bit mass.
Basically, so this is a reward function. So basically the PIJ is the probability that's why I is a better than yj which is output from the lm.
And then basically we take a log and is the difference between two reward function.
So we can compare this probability by using a sigmoid function over the reward difference between these two.
And then the objective is to maximize this probability right because this is actually input the feedback from humans like whether why is better than yj.
So to learn a reward function. So this is a parameters of the reward function so that we maximize this probability based on the human feedbacks like why is better than yj.
So that's the reward.
Because the reward model is trained and then they use the reward model to help assist in training the policy model the policy model is exactly the same as the original lm that you want to find you.
So I will back on this to see like how they combined in one objective.
Basically, they want to like updates the model parameters so that they can give the so that they can give the reward that can reflect the actual rankings by the humans so that to align with the human preference better.
And that's not it. And then people find that if you just train for that one objective.
It doesn't work so if you just train for that one objective to align the reward.
The model doesn't work and in the end you may get a model again speaking nonsense like the synthesis may not stand by itself.
Because the model is over fine tuned on the reward. So they bring back the original model. So this is the initial model. The initial model is well trained.
So that it gives like a very structured very good English sentences. And then the tuned model, which is to align the reward align the ranking preference from the humans.
And then they give additional penalty term to say that I do not want this tune the model to deviate too much from the initial language model.
Still speak good English. And at the same time somewhat align with the human preference so that's why they bring back in this kind of a care that we're just basically just to see like what is the to to to minimize the difference between this the initial.
And tune model and the tune model based on the reward function. So they want to minimize these two difference and adding additional subject objective to the to the to the objective.
So in the end you have like this so you have this penalty that you want to tune the model not deviating much from the language model so that the output still speak good language.
And then you add a reward objective.
So that the output aligns with the human preference ranking. And then they run a PPO called the proximal policy optimization. Again this way is also proposed by open AI, and they claim that this is a more efficient policy updates in the reinforcement
So I think this PPO method itself is worth a meetup.
So I don't think I will. I also need more time to dive deep into the PPO method.
So you can think of it like a very similar just to like a normal gradient descent right so how can I update these parameters so that I maximize or minimize my my objective function.
So the objective fashion covers a reward and also covers the deviation penalties.
So this is their result. This results first shows from the instruct GPT model so instruct GPT is a proceeding version of the chat GPT.
The chat GPT everything is kind of is kind of like derived from the instruct GPT and actually open AI has spent lots of budget on hiring the human reviewers so that they get like the feedback from human humans and then they use those preference and ranking to instruct the chat the
GPT model better so that the output from the chat GPT aligns with what human likes basically and then in their finding. So this is the this is the original GPT three like without any fine tuning.
This one is the GPT three with the prompting so that you can you're able to show like one example three examples and the writing better prompt.
So it can improve the performance. This is the supervised fine tuning which is the instruction fine tuning right so you just formulate your task with the input text output pairs.
And then you fine tune the the model with the Laura say so this is the supervised fine tuning and this is the the origin the red or the performance from the PPO from this reward reward and policy update model.
So you can see that like another gap of improvement by taking additional human feedbacks and also to align the the output kind of to more like what human is out this out to be.
So you see the human instruction player very important role here and also there's a trend about like how can we make the GPT basically like more secure and more ethic right so so so human instructor player very important role here to basically teach the
how I am the values of human and make it to to generate like a reasonable good good output.
And this is a difference say in the previously like explain the moon 19 to a six year old and then the GPT three actually doesn't do very well the original GPT three just say.
Okay, you asked me to explain it so here are the steps of how to explain it.
So, so the GPT see basically gives you the steps like first explain the series of gravity and then explain the series of relatively to a six year old.
So this is not like what the human want. So after getting feedback from humans so this is definitely not at once. So you can just directly me tell me how I should explain.
And then the instruct GPT and also the chat GPT model is able directly to output. Okay, here's how I would explain it to a six year old.
So that's the reinforcement learning with human feedback. Any question.
Yes, I spoke with a researcher who created something called the truth O meter. And what that did is it would take a result from a chat bot, and look for supporting evidence for that result on the web.
And then it would give you links to whatever supporting evidence it found and it would modify the chat result to try to better reflect reality.
And so I asked that developer question I said, What if you just trained a large language model, what you considered to be the correct results that you found you know of course this is not a perfect system.
If you trained it on what you just thought were the correct results, would it still suffer from hallucinations. Now this is something that's important to the enterprise, because if you want to train on your own data for privacy purposes for the purpose of not getting hallucinations
based on crazy information on the web, then what are you going to get are you going to still going to get a bunch of hallucinations, and he thought that you would.
He said although other researchers may disagree with me about this. And so I was just curious what you think about that is it just the statistical nature of how these systems operate that they're going to produce hallucinations no matter what.
Yeah, without yeah, there's right now no hard constraint or guarantee or theoretical guarantee that the model won't lose it.
More like right now actually comparing comparison is which model hallucinates much less than the other models.
Yeah, some model hallucinates and some some hallucinates more.
You could kind of reduce the times that it hallucinates and maybe force it to say also give me the proof and the reference and force it to give the reference.
And also you can instruct it to say, like make sure that your your your information can be inferred from the reference you give. So you can maybe give a more strict or limited instructions that can reduce the times of the generating nonsense, or anything that doesn't exist before.
That's something that we can that you can try to do.
But but you cannot fully forbid the net.
I know that that Steven Wolfram gave a talk on this as well. And I believe there's some chat GPT version that you can access from within mathematics, or I mean you can access chat GPT for from within mathematics if you want to.
But he mentioned the factor and I can't remember the name of it, but he says they often set it to point eight and all that factor does is it allows for variation in the output.
So if it's set to one then it's always going to produce the same output. But as you lower that number there's more of a variety of outputs that you can get. So I'm wondering if you're in the enterprise situation, and you set that variable much closer to one.
Does that help to ensure that you're not going to get hallucinations.
No, I think it's more likely that you set it to say point eight or point five, like, let it to generate various outputs and then you validate what maybe you have another model to check it or maybe you have some another like a test testing model to validate the outputs.
And then the outputs after the filtering and reward will be better. But if you just let it to give it a fixed output, it may be, it may, may, may be wrong in the beginning and still doesn't give what you want.
Okay, all right, thank you.
Okay, I think this is the last part of today's presentation.
This is also what I learned when I preparing this session. Like, if you see like from the very beginning of the machine learning.
So basically it's like the programs is even like hand crafted designed. And then to the classical machine learning so the classical ones like the linear regression support vector machines, etc.
And then the features needs to be carefully designed. And then you can learn, you can learn a mapping basically the function that map from the input to the output.
And then when when you go see into the deep learning, the feature part also learnable right so the feature, the representation like this hierarchical representation, like the how to represent a word with the embedding this also is automatically
learned like you don't need to think about how can they create a function to create those features. So basically you add more automation to the whole pipeline. So that is the GPT three, like the like any other deep learning models.
And then when you combine the deep learning with the reinforcement learning like reinforcement learning with human feedback.
And they also, they also automate the loss function. So, for the previously the loss function is also predetermined such as you, you maximize the likelihood of the of the next prediction word right to align with what is actually observed right now in the reinforcement
frame, they, the loss function is also learned through a model right so through that rewarding model. So make it a more flexible, or also like a more adaptable framework to learn basically something that that is not easily can be quantified such as human
preference you can hardly think of a way to to say this is a function to model to calculate the human preference so you cannot craft a function to do that so that's why you need to design a model to specifically learn
about the human preference and ask for the feedbacks like which is more favored from the humans and to actually train a model to model that objective.
And this is where the charge GPT the instruct GPT is from so it's using this model. And again like some recent literature is saying, also saying that you don't need the reinforcement learning you can still do instruction learning to align with the human feedback.
That's a different story, but yeah it is moving very fast. So, so already some fully supervised model is already beating the aisle, but I guess the aisle itself, we're also going to improve so I guess it's just a competition and iteratively improvement over each other.
Yeah, so yeah basically here's the summary.
I basically the takeaway message for you is that to train a model, you need the first pre training so that the model can output really good.
You need grammar correct language output. So that's the pre training, and it's pre trained on a large corpus of unlabeled data, such as from the Common Crawl from the Wikipedia, and then you can further fine tune the model for different tasks.
Right. And like through like in the GPT three star like in the prompting like a zero shot or future like in context learning without like updating the model parameters that you can already start like getting some outputs for your task.
And then to further improve the performance you need to do the instruction tooling and the plan model is a good and also you see that the plan model is also you can use it for commercial use.
So this plan model is actually a result from the instruction tuning on different like what's the categories of different tasks. And then you can use the same model for different for different unseen inference tasks.
And then there's like a even other ones like you can even like take the instruction model, like after instruction fine tuning, and then you are going to align it better with the human preference through the reward and policy model training.
And then basically this policy model is the final our output from the training and then you can use it for any next so that it gives what human likes in output.

Okay, so welcome to the first lecture on the vectors course. This is the basics, vectors
versus scalars, vector notation, addition and scaling, and properties. Alright, so begin
at the beginning. Let's list some scalar quantities. Think about mass, duration, length, temperature,
charge. These physical quantities are all well described with a single number. Really
they just have a magnitude, although some of them may go negative, so it's a magnitude
and a sign. But still, just a simple number is adequate to describe these things. How
about vector quantities? What's different about vector quantities? Well, think about
these things, force, velocity, and therefore acceleration, or momentum. These things also
have a strength or a magnitude. However, so let's put that down, they have a magnitude.
However, they also have a direction. More than just a sign, they have a full on direction
in three-dimensional space. So it's not enough to know that a force is 3 Newtons. I want
to know in which direction is that force applied. And that then is the difference between a
vector and a scalar quantity. We're going to think about how we manipulate them. Alright,
so first off, the notation that we're going to use when we talk about our vectors. What
I'm going to do is I'm going to use a symbol such as the letter A. So let's write that
out, but I'm going to underline it. So an underlined symbol indicates a vector rather
than a simple number. And when I need to specify that vector, I'm going to write it, so it's
going to be three-dimensional. I'm going to write the three numbers in a column form
like this. Now if you haven't seen a vector specified before, what does it mean? Well,
think of the Cartesian axes, the x, the x, y, z axes. Think in this case about coming
out from the origin 2 in the direction of x and 1 in the direction of y and 3 in the
direction of z. What we're going to do is we're going to think of our vector as an
arrow. An arrow that comes from the origin to this point in space. And that arrow itself,
whether or not it comes from the origin, that direction and that length of arrow is our
visualization of the vector. So let me just change color to green and go ahead and draw
the tip of my arrow there. There we are. So the vector is coming towards us out of the
screen and it has those particular three components, two, one, three. Other people may use other
notations. For example, a line over the symbol A is commonly used. When people write out
the components, they may choose to do it as a row like this or even using pointy brackets
like this. Now all these notations are basically getting at the same thing. You'll be able
to read textbooks or look online and see these things and understand what they mean. But
within this course of videos, we're just going to use the notation that I've introduced
above. So I'll erase those for now. Now the simplest thing that you might want to do
if you have a couple of vectors is to add them up. So let's think about that vector
addition. What does it mean? So let's give ourselves a second vector B. We'll make it
five minus two zero, let's say. I want to add these two vectors together. So we'll write
that out. I simply want to add A underline plus B underline. What does that mean? Let's
just substitute in two, three. Add it on two, five minus two, zero. Now what we do is we
simply add the first component of vector A to the first component of vector B and so
on down the list. Very, very simple. So we're adding two plus five. We're going to add one
plus minus two and three plus zero. And we just tidy that up. So that's going to be
seven minus one and three. Now how about scaling a vector? Okay. So what we can do is we're
we can multiply a vector by a simple number and correspondingly we'll just end up multiplying
each of its components. So let's take an example three, nine, minus twelve. What we notice
is each of the three components is a multiple of three. We can just take that common factor
out in front and write this instead as three times one, three, minus four. Same thing.
Alright. Or equivalently, someone might give us a vector that's already written in this
form. It could be let's say three over two onto two, four, minus four. Let's make it one.
Alright. And we can just multiply that in, in a component by component basis. So we just
write ourselves a new column. Of course three times two is three. Three times minus four
is minus six and three over two times one is three over two. Okay. So what we can do
there we are. We can scale our vectors by a number in this simple way. So with these
definitions of addition and scaling, can we say anything about the properties? Okay.
So if I have two regular numbers a and b, then of course a plus b is the same as b plus
a. I'm not saying anything fancy here. It's as simple as, I don't know, seven plus minus
three is equal to minus three plus seven. Obviously it is. We know that. Now if we think about
the same statement for vectors a plus b, vector b, is it the same as vector b plus a? Well,
it must be. Let's just write out an example, seven zero minus one, three, one, two. Is
it equal to three, one, two vector plus the vector seven zero, seven zero minus one? Of
course it is because of the way we've defined vector addition as just being the addition
of each element to the corresponding element. And this property is called being commutative.
Okay. So vector addition is commutative. How about this second example? If we have three
basic quantities, ordinary numbers, then if we have a plus b plus c, it's the same as
a plus b plus c. It doesn't matter the order that we do them in. Is that going to be true
for vectors? Well, of course, it is going to have to be true to vectors because the
way we define vector addition is to add each component to the corresponding component.
It's just addition. So this is also for vectors. Let's write out what we mean. We mean that
vector a plus b plus c as a previously worked out thing is equal to vector a plus vector
b and then add on c. It doesn't matter the order we do these things. All right. And
there's a name for that property. It's called being associative. All right. So vector addition
has that property also. Now let's think about our scaling property. If we have ordinary
numbers again, then we could take some scale factor k and multiply it into a plus b and
it would just give us k times a plus k times b. Again, I'm not saying anything that isn't
utterly obvious here. Say, for example, I don't know, 2 into 1 plus minus 3 is equal
to 2 times 1 plus 2 times minus 3. Of course it is. So how about for vectors? Is it true
that some scale factor k times the sum vector a plus vector b, a plus b? And let's stress
that this scale factor is just a pure number. Then, yes indeed, it's going to be just k
times a plus k times b. So just to stress what we're doing here, let's copy down this
sum of two vectors we were playing with up here. This 7, 0 minus 1 thing plus 3, 1, 2.
Put it inside curly brackets maybe for a variety. It doesn't have to be curly brackets. Multiply
it by some factor. Let's have 3 over 2. Had that before. Unimaginative. There we are.
What's that going to be? It's just going to be 3 over 2 times the first vector 7, 0, 1.
Then plus 3 over 2 times the second vector 3, 1, 2. Okay. So everything, as you kind
of would expect, it works out. It must. And this latter property is called being distributive.
Excuse me. So scaling is distributive over addition. And that's the end of our first
video. Welcome to the second of these videos. We're going to look here at the vector dot
product, also called the scalar product. We'll look at also the magnitude of a vector and
the meaning of unit vectors, the geometric meaning of the dot product, and finding the
angle between vectors using the dot product. Okay. So the dot product is a way of combining
two vectors in order to produce a number, a simple number, a scalar, hence the alternate
name, scalar product. Let's give ourselves a couple of vectors. Let's have a, well, vector
a can be 4 minus 4. Let's have 2, 1. And we'll have a vector b which can be 3, 1, 3. And we're
going to do the dot product of these two guys. So we write that as vector a, a nice,
nice central dot vector b. And then we write that out as the two column vectors. And we
need to understand how we compute the dot product. And the answer is we're simply going
to multiply each component by its opposite number and then add them up. So we're going
to multiply the first component minus 4 by 3 and then add that to the second component
2 multiplied by its opposite number 1 and finally the third components 1 and 3. So that's
minus 4 times by 3, added on to 2 times by 1, added on to 1 times by 3. So minus 12 plus
2 plus 3, that's going to be minus, minus 7. Alright. There's the dot product worked
out pretty straightforward. And of course as you can see it can be a minus number. It
can be 0. It can be a positive number. But it's a simple, pure number. Okay. So now let's
see what happens if we do the dot product of a vector with itself. Let's do a dotted
with itself. So there's going to be minus 4 to 1 dotted with minus 4 to 1. Now of course
because we're multiplying each component by itself, that will always be a positive number
16, minus 4 by minus 4 and 2, 2 is a 4 and 1, 1 is 1. And so that's going to add up
to 21. It must add up to a positive number. It's made of 3 positive numbers summed. Now
I want to introduce a second vector called a hat. It's related to a just by scaling
it. And we're going to scale it by 1 over the square root of the earlier dot product
with itself. So 1 over square root 21 and then just minus 4, 2, 1 as before. So that's
just a scaled version of a. What's interesting about it? Well now let's see what happens
if we take the dot product of a hat with a hat with itself. So we're going to get 1
over square root of 21 times 1 over square root of 21 which is 1 over 21. And then of
course we're going to get a dotted with a, the original dot product we did which is
just 21 as we know. So of course the dot product of a hat with itself is just 1. That
means that a hat has a special property. It's what's called a unit vector. Unity being of
course a fancy word for the number 1. So when we scale a vector so that it, when dotted
with itself it comes out as 1 then it is a unit vector. Meanwhile in general for a vector
the square root of the dot product with itself has the name magnitude. This is the magnitude
of a vector and it is also magnitude. It is also the length of the arrow if we think
in terms of a vector as a physical displacement and arrow that lives in three dimensional
space. Then it would be the length of that arrow as you can see from Pythagoras. Okay
now then a different thing. The dot product between two vectors has an alternative definition
which we can show is the same as the definition we've been using so far. A dot B is also the
magnitude of A times the magnitude of B times cos of some angle and what is that angle?
It's actually just the angle between the two vectors, between their directions. So here
I'm drawing a vector A going in one direction and almost in the opposite direction vector
B. And then the angle in question would be this angle that we see between the two vectors
when we draw them coming from a common point of origin. Okay so it's important to understand
then that this angle can be more than 90 degrees. Here's what it isn't. Here's a mistake
that's sometimes made by people as they start to play with the vectors. They want the angle
for some reason. They want it to be less than 90 degrees. So they try and contrive this
by putting the vectors together in a way that will give them less than 90 degrees like this
for example. And then we could try and draw an angle between these two lines. Let's see
like that's user red to show that it's not correct. What we should have is the two vectors
coming from a common origin then we see that the angle between them can be more or less
than 90 degrees. If it was exactly 90 degrees then of course the dot product would be zero
because cos of 90 is zero. That has interesting consequences. But right now let's work out
the angle between a couple of vectors. Let's give ourselves A. We'll make it 1, 0 minus
1 and B. We're going to make it 4, 1, minus 1. And we'll do the dot product between those
guys. So first we'll work out the dot product. Actually let's make it minus 1. So A can
be minus 1, 0 minus 1. I think that will come out better. So we have minus 4 from minus
1 times 4. We have zero times 1 is zero and we have minus 1 times minus 1 is 1. So it's
going to be minus 3 for the total dot product between these two guys. But we also need to
find out the magnitude. Fair enough. The magnitude of A is going to be the square root of minus
1 times 1 times 1 is 1. And again 1. So that will be the square root of 2. Nice and straightforward.
Meanwhile the magnitude of B is going to be 4, 4 is a 16 plus 1 plus 1. It's going to
be 18. The square root of 18. But I think we can do better than that. Square root of
18 is actually square root of 9 times the square root of 2. And that means it's 3 times
the square root of 2. Okay. Now we've got everything we need. Let's pull down a copy
of that definition there relating A dot B to its magnitudes in the angle. And fill in
what we know for this particular choice of A and B. We've got minus 3 is therefore equal
to root 2 times 3 root 2 times cos of the angle that we're after. So now we just need
to rearrange. That means that cos of the angle is going to be equal to minus 3 divided by
what we've got 2 lots of root 2. So that's just 3 times 2. And if we simplify that down
it's just minus 1 half. Now we may just remember or else use a calculator to find out this
means that the angle in question is in fact going to be simply 120 degrees. Or you can
use radians if you prefer radians. So there we are. That's the answer. The angle between
these two vectors is 120 degrees. And that's it for the second video. In this video we're
going to see how to calculate something called the cross product of two vectors. It's also
called the vector product because the output is a new vector. And we'll see how to test
that the answer is correct. So here I've written A cross B is equal to C. And notice that the
symbol for the cross product is just the multiplication symbol that you're familiar
with from basic arithmetic. I've given the vector A a particular form this 2, 3, 4 column
vector. And similarly B is written as 4, 5, 6. So we're going to go ahead and find out
what is the cross product of these two vectors C. Because it's a vector we'll need to do
some working for each of the three components. Now what I'm going to do is I'm going to
paste up some structure to help us work through the problem. So don't worry because it's
going to look like a lot but you don't need to write all this out every time you want
to do a cross product. I'm just putting it here so we can really spell out the process.
Okay, so let's go ahead and work out the first component of the output vector C. Strangely
enough what we're going to do is we're going to ignore the first component of vectors A
and B. So I'm just going to cross those out. Those aren't used. And what we're going to
do is we're going to multiply a certain of the other components. What we're going to
do is we're going to multiply the second component of vector A with the third component
of vector B. I call that the falling diagonal because when we draw it like this we start
high and then go low. And then we're going to subtract off the multiple of the rising
diagonal 4 and 6 here. The last component of vector A and the middle component of vector
B. So what we have here is 21, that's 7-3 is a 21, minus 6-4 is a 24, that's minus
3. We can go ahead now and write that in as our first element minus 3. Now let's move
to the second element of the output vector C. We'll start by ignoring the second component
of the two source vectors A and B. We can cross those off. And again we're not going
to multiply some diagonals. But what's different here is we start with the rising diagonal
4 times 5. The last component of vector A times the first component of vector B, the
rising diagonal, 5-4 is a 20. And then we subtract off the falling diagonal, so 2-7 is
a 14 and that's going to give us 6. So we can put that in. Now let's move to the third
and final component. As before, we start by noting that we will ignore the third component
of the two source vectors. And we're going to need some diagonals. It's the same pattern
as the first falling diagonal first, so 2 times 6 and subtract, which is 12, and then
subtract off the rising diagonal 5-3s of 15. Alright, so that's going to be minus 3.
Pop that in. We see that we have quite a simple vector here. There's a common factor of 3.
Let's bring that out. 3 then minus 1, 2 minus 1. That is our vector C, that is A cross B.
Notice again the pattern. It was the falling diagonal minus the rising diagonal for the
first component, and then the rising diagonal minus the falling diagonal for the second
component, and then for the third it was back to the same pattern as for the first. Now
these look a bit like letters to me. They look a bit like a V, the middle one perhaps
an N, and the final one a V. I like to remember that as a little sentence which is voles,
never vary, because in my opinion voles don't vary very, very much. Here's a vol. This one
doesn't vary at all because it's stuffed in a museum. However, if you compare it to some
other voles, which I found these on the internet, I think they are all pretty much identical,
and I didn't see a big difference there, so for me voles never vary. If for you they do
seem to vary, then think of a different way of remembering it. But the important thing
is that the first thing is the falling diagonal, and then subtract the rising diagonal a V
shape, and it alternates. Okay, how to check your cross product has been worked out correctly.
This is really useful stuff. So let's give ourselves another example. We'll have 2, 3,
1, and then we'll have let's say 3, 7, minus 1 is going to minus in there, and that's going
to be equal to something. We'll work it out in a minute. For now I'll put x, y, z. Now
how am I going to test once I found those x, y, and z that I haven't made some kind of
slip? I mean there's a lot of mental arithmetic. If we don't write it all out, we're going
to be doing a bunch of multiplications. I could easily slip up. How am I going to test
that? It turns out there's a very interesting property of the vector c that we get out after
the operation if we've done it correctly. That is, as I've written here, that a dot dot
product with vector c is 0, and so is b. So either of the input vectors a and b dotted
with the correct cross product c should give us 0, and that's great because the dot product
is very easy to work out even by i as a check. Let's go ahead and do it. So I've copied it
down here. We're going to want to work down our various components. Let's do the first
component of c. So what do we do? We ignore the first components of a and b, and we do
the falling diagonal. So that's going to be 3 times minus 1, and we subtract the rising
diagonal 1 times 7. So let's just write that out. Normally I wouldn't bother to write
all this out, but let's go ahead and do it here. So it's minus 3, minus 7, and so that's
going to be minus 10 as our first component. Now we work out second component. We ignore
the second component on the input vectors. We do the rising diagonal 1 times 3, and subtract
the falling diagonal 2 times minus 1. So what have we got? We've got 3 here, minus
minus 2, and so that's going to give us 5. And then finally, the third component, ignore
the third component of the input vectors, do the falling diagonal. 2 times 7, 7, 2 is
a 14, subtract the rising diagonal 3, 3 is a 9. So we're going to have, for our final
component, 14 minus 9, which is another 5. So that's quite a simple vector. It has a
common factor of 5 in there if we wanted to write it out that way. Now let's test that
guy versus the a and b vectors to see if it passes our test or have we made a slip. So
let's just be completely explicit about that. We're going to start by testing the dot product
of the vector a with our hopefully correct cross product c. I'll write it out, 2, 3,
1, dot product minus 10, 5, 5. What's that going to be equal to? Minus 20, and then 3,
5 is a 15, and then 1, 5 is 5. Aha, it does equal 0. That's correct. That's a very, very
encouraging thing, but for real thoroughness, we're going to test the other one as well.
So this is b dot c. Let's check that out. So that's 3, 7, minus 1, dotted with again
minus 10, 5, 5. This time it's going to be minus, minus 30 from 3 times minus 10, and
then 7, 5 is a 35, but then minus 5 from the last 10 meant 0 again. Aha, so it has in fact
passed both of our tests and we're now very confident that's correct. This is a great
test to do. One word of warning though, the one thing it won't pick up is if you've done
your rising and falling diagonals in exactly the wrong way round by starting with the wrong
pattern. So do remember the VNV pattern and this test will check for any particular slips
in your multiplications. And that's the end of the video.
Okay, so in this short video, I'm just going to look at four more examples of the cross
product for practice, and here they are. Okay, so here's the first one. We want the first
element of this cross product, so we ignore the first elements of the two source vectors.
We do the falling diagonal 3 times 0, that's 0, and we subtract the rising diagonal 7 times
minus 1, that is minus 7, so we're subtracting minus 7, that means we'll get plus 7. So the
first element here is going to be a 7. Okay, so now we want the second element, that means
we ignore the second element of the two source vectors. We do however the rising diagonal
first, 7, 2 is 14, minus 1 times 0 is 0, so that's 14. So the second one was the rising
diagonal first, if you follow me. And then finally to get the third component, we ignore
the third component of the source vectors and we do the falling diagonal, 1 times minus
1 is minus 1, minus 3, 2 is a 6, so that is minus 7.
Okay, so there's our solution, 7, 14, minus 7, but is that correct or have we made a slip?
It's a good time to check the old dot product trick. So if we call this A cross B equals
C, then we should find that if we do the dot product of one of the input vectors, say B,
with C, then it should be 0. Let's check that, 7, 2 is a 14, minus 1 times 14 is minus 14,
0 times minus 7 is 0, so that's 14 minus 14, it's correct, let's do the other one, it's
harder. So 1 times 7 is 7, 3 times 14 is 42, so that's 49 in total, and then the final
term here, 7, 7 is a 49, but that was with a minus number, so we've got, in fact, 49
minus 49 is 0, so another one of those dot products is correctly 0. So what we found out
is that A dot C and B dot C are both equal to 0, as they must be, so we're now very
confident that we have the right cross product there. Let's do another one. Okay, so we're
going to want the first element, so we ignore the first element of the two source vectors,
and we do 8, 3 is a 24, minus 2, 2 times 2 is 2, so that's 22. Let's do the next element,
so we ignore the middle elements and we do the rising diagonal, 4, 2 is 8, minus 8, that's
just going to be 0. And then finally, we ignore the bottom elements and we do the falling
diagonal, minus the rising diagonal, 1 minus 12 is minus 11. So there's our solution, 22,
0, minus 11. We notice we could take 11 out of that as a common factor, it would make
the next stage very easy, but let's just, let's do it the hard way and do the dot product,
so 4 times 22 is 88, 1 times 00 and minus 88, actually pretty easy to confirm that's
0. Let's do the other one, 1 times 22 is 22, 3 times 0 and again, 2 times minus 11, again
0. So that's fine, that one's past its checks as well. On to the third one. Okay, so this
time I think I might take a common factor out just to show us doing that because I see
that this 25, 5, 15 chap is going to lead to some pretty big numbers, but maybe I don't
need to do that, I can just take the common factor of 5 out of the first vector, we're
calling it vector A, so that's just 5, 1 minus 3 and then I go ahead and write vector
B, which can't be simplified, it's just 1, 3, minus 2. We'll do this cross product,
excuse me, we'll do this cross product and then we'll put the factor of 5 in at the end,
that's fine, do it that way around. Okay, so let's go ahead and write that out, there's
our factor of 5 and here's our cross product. So the first element of our cross product,
we ignore the first elements of the two source factors, we do the falling diagonal, that
gives us a minus 2, we subtract the rising diagonal, that's a minus 9, so that's minus
2 plus 9, that's going to give us a 7. And now the middle element we ignore, the middle
elements on the two source factors, we do the rising diagonal this time, it gives us
minus 3, we subtract the falling diagonal, that gives us minus 10, which means we're
going to have to add on 10, so that's minus 3 plus 10, it's another 7. Okay, and then
finally the third element, we ignore the third elements on the source factor, we do
the falling diagonal, that's 5, 3 is a 15 and we subtract the rising diagonal 1, that's
going to give us another 14. So in fact a really simple vector here, because we could
take out a factor of 7 if we want to, but let's check those dot products, do it before
or after we take out the factor of 7, it's pretty easy, that's going to be 4 times 7
minus 2 times 14, yes that goes to 0, let's do this one just quickly, 35 and another 7
is 42, but minus 3 times 14 is exactly minus 42, so that one is also satisfied, we've passed
our checks, that looks pretty good, we can leave it like this or if we want we can take
out that factor of 7 and do 35 times 1, 1, 2, very simple, very nice vector there. Okay
let's come here, now come down to the bottom and look at the final one, we notice actually
the cross product of a vector with itself, it's the same vector here, so what are we
going to get, well we can just easily enough work it out, we ignore the first two elements
and we do 2 times minus 4 and minus 4 times 2, so it's something minus itself, that's
just going to give us a 0 obviously and let's keep going, if we ignore the middle terms
and do the rising diagonal minus the falling diagonal, again 3's and minus 4's the same
product, so something minus itself 0 and it's going to be the same for the final element,
so the cross product of a vector with itself is always going to be the 0 vector, now it's
important not to write that just as the scalar 0, because it is a different object, it's
the vector 0, it's a set of in three dimensional space three 0's, that's what we get when
we cross a vector with itself. Of course this is going to trivially satisfy our condition
on the a dot c is equal to 0 and b dot c is equal to 0, that's clear and so I think that's
a nice set of for examples done quite quickly there, they're not too bad are they, so that's
the end of the video. Okay in this video we're going to look again at the cross product,
but this time we're going to ask about its geometric meaning and its properties when
we come to manipulate it. Okay so if some vector c is the cross product of two other
vectors a and b, we've already seen how to work that out, but what we can reasonably
now ask is what does that vector c look like, you know if we imagine a particular couple
of vectors a and b there in space, where is this vector c, how is it related to them,
we know how to work it out, but what's its relationship with them, how should we think
about it and that's what we're going to figure out now. So we know that c is a vector so
it has two properties, it has its magnitude and direction, let's think about the magnitude
first, what is the magnitude of c and how does that relate to a and b, what is the length
of that vector, it's pretty simple, the magnitude of c is the magnitude of a times the magnitude
of b times sine of the angle between a and b, this is very similar to the dot product
except with a sine instead of a cos, so there we are, there's our two vectors a and b and
an angle between them and from those magnitudes, the lengths of those two vectors and the angle
we can work out the magnitude of c, note that if we cross a vector with itself the angle
will be zero and so the cross product will be zero just as we've already seen in our
examples, that was easy enough, what about the direction of this new vector c, how does
that relate, okay here's the thing, the direction of c is perpendicular to both vectors a and
b, so it's at right angles to each of those vectors separately and simultaneously, what
does that look like, well actually we can draw it in one of two ways, one of which is
right and one is wrong, let's just do that, so here's our vector a, here's our vector
b, if we draw c like that and make it clear with this little symbol that it's at right
angles to those two vectors, that would be perpendicular to them both, how about this,
we could also draw vector a, draw vector b again and we could go in the opposite direction,
simply literally the opposite direction and that would also be perpendicular to these
two vectors, one of these is actually strictly the correct case and the other is wrong by
essentially a minus, a minus one multiple, what's the way to work that out, so let's
now figure that out, there's actually a rule to remember it by, it's called the right hand
screw rule, so let's draw that out really clearly one more time, we have two vectors
a and b and we are going to say that a cross b is equal to some vector c, that's fine,
so what we do is, we put on the line along which we know c must lie, so this is the line
that's perpendicular to both a and b and we simply have to ask ourselves, in this picture
does the vector c go upwards or does it go downwards, the trick is to write on the angle
between a and b and give it a direction so that it's increasing from a to b, it's the
angle from a to b, then you imagine taking your right hand and gripping that line in
such a way that your fingers curl in the same direction as the angle increases and then
your thumb points in the direction that the, in the actual direction of c, let's do another
example just to really make that clear, here's a and b again, so we know we need to be, I've
drawn these lying in a plane so we, I'm now trying to draw a line that's perpendicular
to that plane, vector c must lie in one direction or the other along this line, what do we do,
we draw on the angle, we now take our right hand and we imagine gripping that line we've
just drawn in such a way that our fingers curl in the direction in which the angle is
increasing, so it's like the anti-clockwise direction in this picture and that's, and
then our thumb points in the correct direction for that vector, so it's in fact, these are
the two opposite cases, so that's the rule, that allows you to construct the correct direction
for your vector geometrically, geometrically, okay, then let's just finally wrap up by thinking
about the cross product and asking whether it has those properties that we looked at
before for vector addition, the commutative property, so for example is a cross b equal
to b cross a, it is not, it is not equal to it, unlike the dot product, unlike addition,
this one, the cross product, it matters the order and in fact it simply introduces a minus
sign if you swap the order of a and b, so it's not commutative, it nearly is in the
sense that it gives you something similar, it gives you the same thing up to a minus
sign, it's important to remember, you can just verify that by thinking about how we
work out a and b with those diagonal products, now how about the associative property, can
we say that a cross b cross c, where b and c have already been worked out, is the same
as a cross b and then cross c, what do we think, is that going to work or not, in fact
it's, this is the associative property, we might ask whether this is true and the answer
is no, again the cross product does not have this property, so the order in which you do
your cross product, if you have doing the cross product of three vectors, does matter,
we can easily convince ourselves of this just by looking at a particularly convenient example,
let's just use Cartesian vectors i, j, k, so let's just remind ourselves where these
guys lie, they're perpendicular to each other, i, j and k, just our unit vectors going in
the x, y and z direction, so suppose we have this guy i cross i cross k, if we try evaluating
it this way around, with the i cross k being worked out first, well that's just going to
give us, in fact minus j, which you can confirm with the right hand rule that we just introduced,
and then that in turn will give us k, that's fine, so we've worked out, in that instance
the answer is minus k, now let's do it the other way around, i cross i, if we do that
first, that's just going to be zero, because i cross i is zero, so it's game over already
at that point, so we can see two radically different answers here, just depending on
our order, finally we could ask about the distributive property, so are we allowed to
multiply through using the cross product, if the second object in our cross product
is a sum of two vectors, can we do this, well this at last is something that we are going
to be allowed to do, it is the distributive property and the cross product operation,
the vector product does have this property, we are allowed to do that, but of course we
must make sure to keep the order the same, okay, so I think that's everything for this
video, okay in this lecture we are going to be looking at something called the scalar
triple product, so what we are dealing with here is taking three vectors and combining
them in a certain way in order to yield a single one scalar quantity, so three vectors
into one scalar, scalar triple product, suppose we have a, we dot it with b which itself is
crossed with c, that is the scalar triple product, that combination, now here I put brackets
to emphasize to do the cross product first, but we can just write a dot b cross c without
the brackets y because we have to do it in the correct order, if we try to do a dot b
first and then cross that with c, it's a nonsense because that will be a scalar cross
producted with a vector, it doesn't make sense, alright then, so let's do one, we'll make
up some vectors, let's have a is equal to three one minus one and b is equal to two zero
four and c is equal to minus one minus two three, okay there are vectors and let's go
ahead and work it out, so first we'll need to do the cross product b cross c, so let's
write that out, so I'm bringing these down now, remember you can work out the cross product
by whatever your favorite method is, I'm just going to do it in the method I introduced
before which is we ignore the first elements and we do the falling diagonal, here's zero
and subtract the rising diagonal minus eight, that gives us the first element eight, then
we ignore the middle elements and we do the rising diagonal, gives us minus four, subtract
the falling diagonal which is six, so that's going to give us a minus ten entry and then
we ignore the third elements and we do the falling diagonal, gives us minus four and
subtract zero, so that's going to be minus four, that is our candidate for our cross
product but it's always good to test, how do we test a cross product, we try dotting
it with either of the input vectors and check we get zero, so here we'll get eight twos
of sixteen and four minus four is minus sixteen, add it up, that is zero and now we try the
other combination, here we're going to have minus one on eight, minus eight and then plus
twenty and then minus twelve, that does indeed add up to zero, it's past our checks, those
were just checks but it was good to do them and so we're now very happy that that is the
correct cross product, to finish the scalar triple product we now just need to dot that
with a, so let's write it out again, minus ten, minus four and do the dot product, that's
twenty four minus ten plus four is going to be eighteen, that's the answer, that's our
scalar triple product, it could have been a positive number, a negative number, it could
have been zero, in this case it's eighteen, now let's do another one, so I'll erase this
but we'll simply use the same three vectors but we'll do them in a different order as
our second example, so let's do b dotted with c cross a, so of course we have to start by
doing that c cross a combination first, so let me write that down quickly, minus one,
minus two, three, crossed with three, one, minus one, so we start with the falling diagonal
that's going to be two and then we subtract three, that's minus one and then we have a
rising diagonal that's going to be nine and subtract one, that's eight and then we have
a falling diagonal minus one and subtract minus six, so that's going to be five in all,
okay, did I get that cross product correct or not, do the dot product test, minus three,
that's a dot product test, minus three, eight, minus five, that one's passed, let's try this
dot product combination as a second check, double check, one, minus sixteen plus fifteen,
that's also going to come out at zero, so it's passed both of my checks, that one is
zero as well, we're happy that this is indeed the cross product c cross a, we now need to
complete it, so what we're doing is b which was two, zero, four, dotted with what we found
our cross product minus one, eight, five, so again go ahead and value this minus two,
zero and twenty, eighteen again, alright, so our second example has also given us eighteen,
does this mean that it doesn't matter in which order we do the elements of the scalar triple
product, let me just write down the answer to that and then we'll look at it, it turns
out that for any vectors a, b and c then a dot b cross c is equal to b dot c cross a,
this with the two cases we looked at and it's also equal in fact to c dot a cross b, this
will always be true, in this case it was equal to eighteen, but these three things will always
be equal, there are three other combinations we could write down in principle, there are
three other ways to combine a, b and c, we could have a dot c cross b or we could have
b dot a cross c or we could have c dot b cross a, now it turns out that those things are
easy to see what they will be because let's just look at the difference from the ones
above, I've just swapped the order of the cross product and we know that when we, oops,
we know that when we swap the order of a cross product we introduce a minus sign, so if the
top three cases were equal to eighteen the bottom three cases must be equal to each other
and equal to minus eighteen and in general this is the same rule for all scalar triple
products, three of them are equal and three of them are equal to one another but equal
to the minus of the first three so to speak and how can you tell which ones are equal,
it's helpful to write out this little cycle a, b and c written in a circle like this,
if we are going around in a clockwise direction here b dot c cross a but that's clockwise
around our wheel then and here's another one that's clockwise c dot a cross b, those guys
all belong together, so the guys that are in the clockwise direction all belong together
and the anti-clockwise guys they belong together and they're the minus of one another, these
two groups, alright, so that's I think all we need to do as practice for doing the scalar
triple product and knowing what we ought to get, let's think about something else, I'm
going to introduce you to something called the parallelepiped, that's why I say, I'm
not sure how to pronounce it, parallelepiped, anyway, this guy is a three dimensional shape
but first I'm going to remind you of what a parallelogram looks like, so here's a rectangle
and here's a parallelogram that we get, if we have the pairs of the sides are parallel
to each other but they are not at right angle, at right angles around the vertex, now consider
this rectangular box and let's tie it up, there we are and consider what happens if
we build it out of edges that are in groups of parallel edges but are not all at right
angles to each other, so let's see if I can draw this reasonably realistically as a three
dimensional object, so I'm going to draw this and then I'm going to stress which edges are
parallel to each other, alright, here we are, okay, let me change colour, so consider these
four edges of the object are all parallel to each other in exactly the same way that
in our simple parallelogram these opposing edges were parallel and then these four edges
are all parallel to one another again in our 3D shape just as these two edges are parallel
and then we have another set, these four edges here in yellow are also going to be parallel
to one another, that object is a particular three dimensional solid, it's literally a
generalization of the box in that we're allowing ourselves to have slanting edges if we want
to, now let's introduce three vectors A, B and C to represent these three kinds of edges,
you see that all the green edges are the same vector A and so on, what happens if we do
A dot B cross C, that it turns out the magnitude of that, if we drop the sign then the magnitude
is just the volume of this shape, so it contains of course the simple case of a rectangular
box as a special case but this will work for any parallel, parallel pipette that we care
to think of, though three vectors can always be combined with the scalar triple product
to give us the volume, and that's the end of the video, welcome to the third topic
in this video series where I'll be introducing the matrix and thinking about what is a matrix
product, alright, so essentially a matrix is nothing more than a grid of numbers, simply
a grid of numbers that could be positive or negative or fractional or zeros, and when
we specify the shape of our grid of numbers, we do so simply by stating how many rows
we have and how many columns, so we're going to hear about rows and columns a lot in this
video, in this video course I'm going to use a particular way of writing a matrix as a
symbol and I need to do that, I'm going to just use a capital letter and I'm going to,
the letter is going to be double underlined, I'll double underline that symbol, so here
we go, A underline, that means the matrix A, and how would we write it, so that's just
like this, essentially a grid of numbers and we put it in curvy brackets just to give it
some structure, so this is three rows, two columns, that one, here's a matrix B, let's
make it a square matrix, let's put in a fraction to show we can, minus 10, zero, okay, so there
are two different examples of a matrix, easy enough, but it gets more interesting when
we try and combine them, so I want to talk about matrix multiplication, addition is simple
and it's just an element by element addition, but multiplication is not so simple, so here's
how we write it, the multiplication of matrix A by matrix B is simply written like this,
A, B, and it gives us some new matrix C, which may be a difference shape from both A and
B as we'll see, let's give ourselves a couple of examples, three, zero, minus one, two,
three, four, and matrix B can be just one, two, zero, minus three, so there are two matrices,
here I've chosen them such that A, B, that multiplication will work, it will exist, but
actually if we try it the other way round, it will turn out that the multiple of those
two matrices doesn't even exist, it's not a well-defined thing, so this is an extreme
case of an operation not being reversible in its order, in other words matrix multiplication
is not commutative, okay, so let's just erase that and go ahead and see how the multiplication
actually works, the trick is to multiply the each row of matrix A, the first matrix by
each entire column of matrix B, what does that mean? Well, let's write out our example
three, two, zero, three, minus four, minus one, minus one, four, one, zero, two, minus
three, now I know that this guy is going to have three rows and two columns of the output
matrix, you'll see why in a bit, I'll just put these blanks in for now, the question
is how to work out each of these numbers, let's choose this one first, okay, now notice
this guy's address if you like is row one, column one of the output matrix C, I'm going
to need to, in order to work this guy out, I'll need to look at the whole of row one
in the first matrix, in matrix A and the whole of column one in the matrix B, I'll need to
combine those guys and how do I combine them, I just multiply element by element as I go
along the row and down the column, so three times one just gives me three and then I add
on the next combination, two times two is four, so three plus four is going to give
me seven, that's how I combine those two, I'll jump back here and I'll erase there and
I'll just put in my seven, alright, so that's the general way it works, let's go ahead and
do the other elements of our matrix C, let's do this one, notice this is still row one,
so I want that first row, it's now column two, that's its address, I want the second
column three times zero and two times minus three is how I'll work that out and that's
just going to be minus six, so let me jump backwards and erase my blank symbol and write
in minus six, okay, maybe I went a bit fast, let me spell this one out more explicitly,
okay, so here I now have row two column one, that's the address of that guy, I want all
of row two and all of column one, I want to look at those guys and I want to multiply
them along, so zero times one and three times two, that's going to give us just six in total
when we add them up, so let me erase and put in six and now this element, that's row two
column two, so I want all of row two, I want all of column two and multiply zero times
zero and three times minus three is minus nine, so that's going to be a minus nine, if
I go backwards and just put in minus nine here, now we're finally on to the final third
row, so we're going to want the third row of A and in this case the first column, that's
one times minus one and four times two is eight, that's going to be seven, minus one
plus eight and then finally last row, last column four times minus three is twelve and
then zero minus twelve, alright, so there we are, that is our matrix product C formed
by combining each row and each column, it's quite a lot of work and it would be even more
if we had bigger matrices, but we said that we get something quite different if we try
multiplying A and B in the other order, so let's go ahead and do that now, what if we
have one zero two minus three, that's B, one two three two zero three minus one four,
that's A, so we can try it, we try and multiply row one by column one and we immediately find
we cannot because they are a different length, a different list, so there is no third element
of our row to multiply with our third element of the column, just pause the video here and
have a look at that and see why that must be impossible for us, so sometimes matrix
multiplication is impossible, alright, let's look at a few little further examples and
you may want to pause the video to convince yourself in each case it's true, is this thing
possible for example pause it and think, this one is not possible, this is not possible
again because there are two elements in say the first row of A and three elements in the
column, single column of B, there's no way to do that as a series of element by element
products, how about this, we just have this row matrix and this column matrix, can we
do that, yes this one is perfectly possible, actually it just produces a single number,
in fact it's a bit like a dot product, it's the whole of row one times which is the entire
matrix and then the whole of column one in B, this thing is called a row matrix and this
other guy is called a column matrix for obvious reasons, okay, how about this, let's have
a look at this one, what if I swap the order of my row and column, I just swap them around,
can I do that, is that going to produce a legitimate matrix, actually yes it will, this
time swapping our two matrices A and B around has produced something which exists, it's
actually a huge matrix, it's three by three, it must have three rows and three columns
because A has three rows and B has three columns, how does it work, let's look at that guy for
example it's just simply the number there which is row one is just a number and column
two is just a number, single number so we just do that product, there's no problem, pause
the video if it's confusing, alright, so again the point here is that A times B is generally
not equal to B times A, even if they both exist they may not be the same, they may not
even be the same shape, however we can go on and ask about the other kinds of properties
of the matrix product operation, A onto B times C is that the same as A times B onto
C, does the order matter, actually it is the same, it does work, in other words we have
the associative property, how about A into B plus C, sum of two matrices, yes we can
have A onto B plus A onto C, that is therefore the distributive property, matrix multiplication
does satisfy those things, it's just not commutative, okay let me make a bit more room up here in
the top of the screen and put one final puzzle up, suppose I have this two row three column
matrix and then a mystery matrix M and then I have a simple column matrix of two rows
and I'm asking what shape should matrix M be or is it even, is it possible, pause and
think about that and in fact it's just a column matrix of three elements, you may want to
just meditate on that and see that it's correct, okay that's the end of this video, okay welcome
to this video, in this one we're going to take a look at how to work out a determinant,
what is it, how can you find determinants of varying sizes, so a determinant is a scalar,
it's just a number, could be positive, could be negative, could be zero and it's derived
from a square matrix, a single number derived from an entire matrix, now the determinant
of M would be written with M with the modulus signs either side of it, even though it can
be a negative number, so here's an example of M and here is how we would write the determinant
of M, note that we don't bother writing straight sides and curved brackets as well, there's
no point in that, it's just enough to have the straight line sides, so let's start with
the definition of a 2 by 2 determinant, that's the easy case to look at, so let's write out
a general 2 by 2 just using symbols, we'll have A, B, C, D, written inside our straight
line sides indicates a determinant, it's simply AD minus BC, so that's the falling diagonal,
the leading diagonal is also called minus the rising diagonal multiplied together, very
simple, very simple and that is how you can just look at and evaluate a 2 by 2 determinant,
so for our example 1, 2, 3, 4, 1 times 4 is 4, subtract of 2 times 3 is 6 and so that's
going to give us minus 2 is the determinant, okay so a 3 by 3 determinant is going to be
a bit more work, what we do is when we have a 3 by 3 determinant we evaluate it by breaking
it up into a number up to 3 smaller determinants each of which is a 2 by 2 and for that we
have our definition for immediate evaluation, so we break up bigger determinants into little
ones and then evaluate them, now I'm going to write out something here that's like a
chess board but instead of black and white I have pluses and minuses, you'll see why
in a moment, the thing to notice though is that we alternate plus minus plus minus along
each row and each column in this 3 by 3 grid, okay so now let's work out a 3 by 3 determinant,
again I will just use general symbols A, B, C, D, E, F, G, H, I, right now first I have
to choose a row or a column, I'm going to choose this top row for the first example
and I'm going to work along this row and I'm going to start with the A symbol, now I go
and I look on my chart and I see that there's a plus sign in that slot of my grid, that
means I put down plus A and now what I do is I ignore the whole row and the whole column
that A is in and I look at the remaining 4 numbers and I write a little determinant
just made out of those guys in the same order they appear, so EF is going to be in my main
determinant there and HI, those are the remaining 4 guys in the same order they appear, now
B the next term, that has a minus sign according to my chart so I will put in minus B and multiply
it by again a smaller 2 by 2 determinant, the one I get if I delete the row and the
column with B in it and look at the remaining guys DF, GI and I just write those guys out
in the same order they appear as a small 2 by 2 determinant, finally there's C, C appears
with a plus sign according to my chart so I need to put down plus C and I need to multiply
by, well we delete the row and column with C in it and we just see the remaining determinant
DEGH, so I simply imagine that that row and column was not there and then that's what
the determinant becomes and then of course those 2 by 2 determinants I can just write
down what they are using my rule of multiplying down the diagonal and subtracting the anti-diagonal,
okay there we are, so that is in general what a 3 by 3 determinant evaluates to, but it's
not the only way to do it, let's write it out again and this time let's choose a column
and a different one, let's choose this column, I'm also allowed to work down this, so I would
start with B as my first term and I delete the row and column with it in and I'd see
what are the remaining terms and write them DF, GI, except I've forgotten something, there's
a minus sign attached to that particular entry so that should actually have been minus B,
alright and then similarly plus E and I delete the row and column which has E in it and then
I just make a 2 by 2 determinant from, in this case it would be the corner elements ACGI
and then finally minus H and delete the row and column with H in it, make a 2 by 2 determinant
determinant of what's left ACDF, okay and of course I could then write out these 2 by
2 determinants explicitly, but the point is it will give me the same answer, let's do
an example and see why we would choose one method or the other, so here are just some
random numbers I'm making up, let's stick that in, it's 3 by 3, first off let's work
along the top row and as we did in our first example, so that's going to be 3, let's put
in the full determinant here and then minus 1 and again the determinant I get by excluding
the top row and middle column and then plus 2 that's going to be 705 minus 1 and I can
go ahead and I can work out explicitly what this comes out at, as you can see I'm doing
here and in fact it will be 12 plus 20 minus 14 and it comes out as 18, so there we are
we've worked out a 3 by 3 but we could have done it in a different way, let's say we went
along this bottom row that's fine, so then it will be 5 and I will be left with 1204
for my mini-terminant and the next element along a minus sign and it was a minus number
anyway minus minus 1 that's going to be 3274, let's just see how we've done that 3274 by
deleting the bottom row and middle column of that, now what about the third element here
well we actually have a 0 plus 0 times some determinant, I don't even care what that is
because it's been multiplied by 0 that's the beauty of it, so I've got 5 into 4 minus 0
and then we're going to have 4, 3 is a 12 minus 14 so that's going to give us 20 minus
2 is 18, same answer as before, okay what about if we have even bigger determinants
than our 3 by 3 example there, if we have if we go bigger still we for example at 4 by 4
we're just going to break it up into a number of 3 by 3s and each of those would have to
be broken up into 2 by 2s, lots of work, so here we are here's a general 4 by 4, we are
going to expand it along a row or column let's say we want to expand it along this row for
example and we'll take in turn a b c d and we'll need to know what sign to use so here's
our checkerboard or our chessboard pattern of pluses and minuses just extend it out now
to a 4 by 4 and you can see the rule here is that if you like if the row number plus
the column number is an even number then there's going to be a plus sign and if it's odd it's
going to be a minus sign, you can confirm that for yourself look at this one it's going
to be at row 2 and column 3 and that's 5 and so that's a minus that's one way to remember
it or just draw it out anyway we're going to use that rule so we go ahead and we write
plus a and now we need to do the entire 3 by 3 determinant that we get when we delete
the row and column with a in it so we just write out that little square block that we
see it's quite easy to copy across and now we're going to have minus b and we need to
delete the row and column and then transcribe across the elements that are left as a 3
by 3 just being careful not to make any slips and you see that we're going to continue this
so let's delete this just to be completely explicit I'll finish the job off so I think
I hope it's obvious what we're doing we're onto plus c and now we're going to just have
e f h i j l and m n p and then finally minus d um onto what we get if we delete the top
row and right most column which is left over then e f g i j k m n oh there we are that's
how we handle a 4 by 4 each of these 3 by 3s would then have to be evaluated and so on
so a lot of work and that's the end of the video okay welcome to this fifth topic which
is eigenvalues and eigenvectors we'll introduce the problem and we'll see how to find eigenvalues
finding eigenvectors is for the next video so suppose that we are given a square matrix
um n just some matrix but we are told that m multiplied by v is equal to lambda multiplied
by v for some scalar just some number lambda and for some column matrix v and a column
matrix of course the same as a vector I will just say vector from now on okay so this scalar
lambda could be positive negative or zero meanwhile this vector v could be anything except the
trivial boring case of just zeros it's something other than that our challenge then is that
we're going to be given a square matrix m and we have to look for any scalar lambda and
vector v that satisfies the equation and such a scalar is called an eigenvalue and such
a vector is called an eigenvector so in that language m multiplied by some eigenvector
gives us back that eigenvector just multiplied by a scalar the eigenvalue okay so first off
let's notice that if we are given a candidate a possible eigenvector v to try perhaps for
a multiple choice then it's easy to test we'll just go ahead and try it so here's a square
matrix 2 by 2 2 4 1 minus 1 and suppose we write down v is equal to 1 minus 1 and this
is suggested as a possible eigenvector well then we would just test it out to see if it
matches our equation we try multiplying m by v so here we go 2 4 1 minus 1 and v is 1
minus 1 is a column and so we do row times column that's 2 and minus 4 is minus 2 and
again row and column that's going to be 1 plus 1 is 2 and we notice we can take out
minus 2 as a factor and then it will be the vector left is 1 minus 1 but that is just
v so minus 2 is indeed a scalar that multiplies v and we've succeeded improving that v is
our eigenvector and our eigenvalue that goes with it is minus 2 okay so that's great if
we're given eigenvectors to check out but what if we're not given any eigenvectors or
eigenvalues then we must find any possible eigenvalues for ourselves there could be more
than one and for each we must find the corresponding eigenvector v and in this first video we're
just going to be finding those eigenvalues okay so here's a little bit of quick manipulation
and aside we know our equation is mv is equal to lambda v I can certainly just bring it
all to the left hand side and write mv minus lambda v is equal to 0 as long as I don't
want to write that as vector 0 but now let's do something interesting let's insert the
identity matrix which won't change the equation but it will be important for the next step
mv minus lambda times the identity times v is equal to vector 0 the identity doesn't
change the equation but now I can factor out both those two matrices the m and the minus
lambda times the identity that's a matrix I can factor those out and it allows me to
write that line now that a form of the equation it turns out this can only be solved for any
interesting v any v other than just zeros if the following equation is true which we
can easily prove but we're not going to prove in this video m minus lambda times the identity
the determinant of that is equal to zero so we're going to have plenty of time to think
about that but let me just put a green box around it because that is the fundamental
equation we're going to use this will allow us to find all the eigenvalues that satisfy
our basic eigenvalue equation so let's do an example it's the best thing let's do m
as a 2 4 this was one we had before 2 4 minus 1 little square matrix and so let's write
down what this lambda times the identity is for a 2 by 2 it's going to be lambda 0 0
lambda very simple and so this matrix that's the difference of the two of them 2 minus
lambda 4 1 minus 1 minus lambda just the difference of those two things as a determinant
is equal to 0 that's all so there we have it we've just subtracted lambda off the down
the diagonal but now we need to solve this so we just write out the determinant 2 minus
lambda multiplied by minus 1 minus lambda down the diagonal minus 4 the off diagonal
is equal to 0 alright so we expand this out minus 2 minus 2 lambda plus lambda plus lambda
squared minus 4 equals 0 let's come over here for a bit more space tidy that up a bit what
we got lambda squared minus lambda minus 6 is equal to 0 can we solve this actually
it's quite easy to factor that's going to be lambda minus 3 into lambda plus 2 is equal
to 0 so that's true if either lambda is equal to 3 or it's equal to minus 2 and those are
our two eigenvalues we found them using that equation in the square box let's crack on
and do 1 with a 3 by 3 matrix M here we go matrix M is equal to let's have minus 2 1
3 1 minus 1 0 and minus 1 1 2 I've worked that one out I've checked that before and
it will work for us nicely now let's remember of course the rule from the previous screen
and we just need to apply that so let's go ahead and write it as a write our determinant
out we need to have minus 2 minus lambda and then just 1 and minus 1 and then 1 minus 1
lambda and then 1 and 3 0 2 minus lambda I'm just subtracting lambdas down the diagonal
making it a determinant setting it equal to 0 I'm going to work along this row because
it's got a 0 in it so that makes me like it a bit more as a determinant the first number
is going to be minus 1 why because it's a 1 and let me just quickly write out our little
look up table of pluses and minuses for doing determinants so it was a 1 and then it picked
up a minus sign and then we have the mini determinant that's made out of those four
terms so that's 1 3 1 and 2 minus lambda alright and then the next term is going to be plus
and then it's going to be the term itself is minus 1 minus lambda and the mini determinant
that we get when we exclude that row and that column is just made out of the corner terms
that's going to be minus 2 minus lambda and 3 and 1 and 2 minus lambda and that's it because
the zero term gives us nothing so it was only those two mini determinants let's write them
out minus 1 2 times lambda and then 3 times 1 is 3 expand that one out then this one has
the term in front minus of 1 plus lambda and then we have to expand out the determinant
minus 2 minus lambda times 2 minus lambda down the league diagonal minus minus 3 is
plus 3 there we are is equal to 0 and then we just need to tidy that up we need to clean
it up a bit that's going to be minus of minus lambda minus 1 for the first term let's turn
that it one into pluses multiply through by the minus 1 and here we have minus let's make
that lambda plus 1 right that way around and then tidy up inside here we expand it out
minus 4 plus 2 lambda minus 2 lambda plus lambda squared and this 3 is equal to 0 need
to keep on working to tidy that a bit more this term here is in fact going to be just
I see the lambdas cancel out lambda squared minus 1 that's very nice that's come down
very very neatly so now we can really tidy that up and we can take out a common factor
of lambda plus 1 and the first term was just that so there's one for that and the second
term we've just found is lambda squared minus 1 pause the video and check you agree that
that's a tidied up version of the equation now the way that can be zero is either the
first term is zero which requires lambda is equal to minus 1 so there's one eigenvalue
for us that's one option one of our eigenvalues has been found or the second term here has
to be 0 so let's do a bit more work with that what we're saying is to neaten that up we're
saying that lambda 2 minus lambda squared is equal to 0 in other words lambda squared
is equal to 2 and so lambda is going to be plus or minus square root of 2 that's 2 more
eigenvalues 3 in all that we found for this 3 by 3 matrix and in the next video we'll
see how to take each of these values and derive the corresponding vector this is the second
of two videos that looks at eigenvalues and eigenvectors in the first video we have seen
how to find eigenvalues and we write these as lambda for each lambda how do we find the
eigenvector and eigenvector that goes with it we know that our fundamental equation that
we're working with here is that when matrix m multiplies an eigenvector v it just gives
us back that v scaled by lambda and another way to write that is that m minus lambda times
the identity multiplied by v is equal to vector 0 this is the same equation written
two different ways what we need to know now that we have obtained our lambda values we
just need to look at one of these equations and figure out an acceptable vector I find
that it's more useful to use the form on the right hand side okay let's look at a particular
example we'll have the matrix 2 4 1 minus 1 we looked at this before and we found already
that its eigenvalues are equal to 3 and minus 2 what we're going to do now is we're going
to take those values one at a time and figure out an acceptable eigenvector we're going
to write our vector that we need to find as just x and y where we need to find these x
y values now take a look at this green underlined equation and in particular the matrix which
is a difference of two different matrices m and lambda times the identity now that we
have our lambda value of 3 we could write out that difference that difference matrix
it's going to be 2 minus 3 and then just 4 and then just 1 and minus 1 minus 3 there
it is we're saying that when that multiplies our vector x y it gives us 0 0 so let's go
ahead and clean this equation up we have minus 1 4 1 minus 4 onto x and y if we want to be
explicit about that we can multiply out it means minus x plus 4 y and x minus 4 y and
that we know is equal to 0 0 now what we immediately notice here is that whilst this this equation
between two columns two column vectors is telling us two things it's actually telling
us the same equation twice so we can see here that we're saying minus x plus 4 y is equal
to 0 we're also saying that x minus 4 y is equal to 0 that's telling us the same thing
is that a problem no that's exactly what we want to see at this stage we should find that
when we work on eigenvalue and eigenvector problems based on a 2 by 2 matrix then really
only one of these rows in the final expression constrains us and the other one doesn't add
any new constraint so this is exactly what we want so now how do we go ahead and solve
it we're saying that minus x plus 4 y is equal to 0 of course we can just rearrange this
to say instead that 4 y is equal to x that's the only constraint we have what we're allowed
to do is choose we can choose the simplest values of x and y that will make this work
so I'm going to choose y is equal to 1 and then I'll find that x is equal to 4 and that
is a perfectly acceptable eigenvector 4 1 to go with my eigenvalue we will always have
this freedom in choosing the elements of our eigenvector really this freedom simply corresponds
to choosing how long the eigenvector is in other words its magnitude because if a particular
eigenvector and eigenvector satisfies our equations a scaled version of that same eigenvector
will still satisfy with the same eigenvalue now while the eigenvector can have any length
we might specifically have been asked for a normalized eigenvector that simply means
we need to take the one that we found and scale it to have unit length so in this case
since it's 4 1 we need to divide by root 17 to scale to unit length simple as that
so there we are that's our eigenvector and a normalized version of it now we still haven't
found the eigenvector for the other eigenvalue which was minus 2 let me just move this up
on the screen to make space to do that at the bottom so here we go we do exactly the
same procedure we subtract minus 2 on the diagonal 2 minus minus 2 and 4 and 1 minus
1 minus minus 2 lots of minuses there so let's tidy that up that's going to be 4 4 1 and
in fact another 1 and then times x y is equal to 0 0 as before we see that really these
this is the same equation twice there's only one constraint and we can read it off simply
as x is equal to minus y so if I choose x is equal to 1 for example then I'm going to
write down an eigenvector 1 minus 1 or if I'd chosen y is equal to 1 then it would have
been minus 1 1 it doesn't matter they're both correct eigenvectors to go with our eigenvalue
but if we want to normalize we'll need to divide by the magnitude 1 over root 2 okay
so there are acceptable eigenvectors to go with the eigenvalue minus 2 okay so now let's
find the eigenvectors that go with the eigenvalues for our 3 by 3 matrix M which was minus 2
1 3 1 minus 1 0 minus 1 1 2 we looked at that before in the previous video and we found
the eigenvalues which were minus 1 root 2 and minus root 2 and I've put little subscripts
on our lambdas here so we know which one we're dealing with let's deal with lambda 1 first
which is the one that has value minus 1 so I'll write over here the little equation that
we're using over and over again which is that M minus lambda times the identity multiplied
by our vector is 0 okay we need this difference matrix so we subtract off the diagonal 1 minus
minus 1 and then 1 3 1 and minus 1 minus minus 1 and 0 minus 1 1 and 2 minus minus 1 and that's
on x y and z because we now need an eigenvector with three elements and it's going to be equal to
simplify the matrix to minus 1 1 3 1 0 0 minus 1 1 and that'll be a 3 and that again is on x y z
eigenvector is equal to 0 0 0 now what we immediately notice is that as before we don't really have
three different equations captured by our matrix equation we only have two in fact this is very
obvious in this case because the bottom row is the same as the top row that's not always the case
it's not always the case that the rows are actually identical but we will always find if we check that
there are only really two independent equations when we're dealing with three by three eigenvalue
problems we only have two equations really now I'm going to highlight this row here 1 0 0 that's
just saying in fact that x is equal to 0 now if we take either the top row or the bottom
run we have minus x plus y plus 3 z is equal to 0 or y is equal to minus 3 z okay so now we
simply choose any values of y and z x has been dictated to us but any values of i y and z
that satisfy these rules so if i choose z is equal to 1 that's going to give me y is equal to minus
3 and i can straight away then write down a satisfactory eigenvector it will be 0 minus 3 1
as simple as that it doesn't matter where the minus sign is i could equivalently have chosen z is
equal to minus 1 and then i'd have 0 3 minus 1 if i normalize then i'll need 1 over root 10 that
being 3 squared plus 1 squared and so that is a complete solution for our first eigenvector
we found it in simple form and in normalized form this is the eigenvector that goes with eigenvalue
minus 1 we can go ahead however and check this eigenvector to make sure that it works so for
that we'll simply need to write out our matrix m the original matrix which was
minus 2 1 3 1 minus 1 0 minus 1 1 2 we have our vector 0 3 minus 1 we just need to do this sum
so the first element is going to be a minus 2 times 0 and then so 3 and i see there's a minus 3
so that does give us 0 and our second element is the only non-zero element will be minus 3
and our third third element there gives us 1 and we can write that as simply minus 1
onto 0 3 minus 1 and so indeed we found that this vector works with the eigenvalue of minus 1
now we can continue to look at to find the other eigenvectors but first let's take a pause
and review the steps involved so we're looking at rules for solving eigenvector problems
eigenvector problem is where we have a square matrix m and we say that m multiplied by some
special eigenvector gives us back that eigenvector times just by a value the eigenvalue we find
the possible eigenvalues using this equation involving a determinant of a difference of two
matrices in general there are going to be n solutions for an n by n matrix so two solutions
for a two by two three solutions three solutions for a three by three matrix that's because when
we write the determinant it will have lambda to the power of n as its highest order so for example
we have cubed to deal with when we're working out for three by three matrices now having found
those eigenvalues we then for each value need to figure out an acceptable eigenvector
what we've noticed is that generally we only have to use n minus one of the rows in the equation
that we're working to satisfy and that meant just one row in the case of two by two problems
and two of the rows in the three by three problems
we had some freedom as to what values to choose for our eigenvector and in fact that freedom
corresponded to just scaling the entire eigenvector to a greater or smaller magnitude
and if we were asked to normalize we would simply work it out using whatever values we like the
simplest values and scale it at the last step so that it has unit length okay so we've covered a
lot of ground for one video and this would be a good place to just stop watching if you like
but i would like to carry on and solve the remaining two eigenvectors for our three by three example
because they involve a square root two they're actually a bit more messy and tricky to do
and in a way i think that makes for a good interesting example to see
so let me go ahead and cut back to the screen that we had before with our matrix m spelt out
and our possible eigenvalues and we'll now take the value lambda subscript two which is square root
two so then as usual we need to subtract that down the diagonal so we'll have minus two minus
square root two one three one minus one minus square root two zero minus one one two minus
square root two and that is the thing which when multiplied by our unknown eigenvector xyz
should give us zero zero zero now one thing we notice here is the rows look all different
it looks like we've got three different equations captured in this matrix equation
but they are not if we examine them carefully enough we'd find that we could generate one of
these rows from the other two and in fact we're only therefore going to need to use two of them
you could pause the video and play with it and see if you can show this but it must always be the
case unless we've made a slip earlier okay so i see that the middle row has a zero so i'm going
to start with that one it says x plus minus two minus root two times y is equal to zero
and that means that if i choose a simple value for y of one then i can immediately say that x
moving across is going to be one plus root two good so now i'll use the top line which is minus
two minus root two x plus y plus three z is equal to zero and i'll substitute in the values that
i've already picked and inferred so i'm going to get one plus root two uh onto minus two minus
root two that's the x term plus the y is one plus three z yet to be found is equal to zero rearrange
so put z on one side divided by a third expand this thing out minus two minus root two
um minus two root two minus um two plus one all right oh and there's a minus sign uh because
we've moved it all to the other side from the z of course now we need to tide this up but what
i notice is that inside the brackets i have a minus three and a minus three root two and that will
cancel cancel with the factor of a minus and third of front and just give us a very simple
expression of one plus root two so that's our z term okay we've found a compatible set of x y and z
values so we can now write down an accept acceptable eigenvector one plus root two one one plus root
two there we are that is an acceptable eigenvector and here's where we found those numbers uh that
goes with the eigenvalue lambda two is equal to square root two note that i used the same subscript
two on my vector so that i make it clear that lambda subscript two goes along with vector subscript two
so now our only remaining task is to look at the third eigenvalue which was negative root two
and find a compatible eigenvector for that one so as always what we need to do is take the vector m
and subtract that the lambda value we found off down the diagonal and because we're subtracting
minus a minus number we can just add it instead of course so that will be minus two plus root two
and then one and then three and then one and minus one plus root two and zero and minus one
and one and two plus root two and that matrix when multiplied by our unknown eigenvector x y z
will give us zero zero zero now as before our middle row looks nicest here it's just telling us
that x plus root two minus one put it that way around y times y is equal to zero that means
if i chose y is equal to one obvious choice then x is equal to one minus root two watching for signs
now if i take the let's say the bottom row i can have minus x plus y uh plus two plus two
root plus two plus root two times z is equal to zero but i can substitute in the values i found
so that will say that square root two minus one plus one plus two plus root two z is equal to zero
okay i've got some work to do to find out the value of z here i'll start by rearranging uh just um
to put two plus root two z is equal to minus root two on the other side but i still need to do a bit
more work divide both sides i notice i can simplify simplify by a factor of root two i can write this
as z is minus one over root two plus one pause the video and check you agree with me um and then
i'm not happy with that because i don't want to leave z as a fraction i could do but that would
be a very messy looking eigenvector i noticed there's a trick in up i have up my sleeve i know
that if i multiply the top and bottom of a fraction like that by root two minus one it will
simplify i will then find that the top of course is one minus root two uh but the bottom will be two
plus root two minus root two minus one and that whole expression just comes down to one
finally then z is equal to one minus root two we've now found our x y and z values that are
acceptable so we're seeing saying that vector three that goes with the lambda three value
is one minus root two one one minus root two that is an acceptable eigenvector
so we're done for our three by three matrix m we found that three eigenvalues and for each
of them an eigenvector the last two of these which involve the root two were uh more tricky just
because there was more to keep track of more messy expressions but the basic maths is the same every
time in this series of videos we'll talk about linear regression and least squares
and the problem that we'll be solving is first in the most abstract setting
if you're given a subspace w of r m and a vector let's call it b also in r m
the question that we want to solve is which vector w
in this subspace w is closest to the vector b now just intuitively if we take the orthogonal
projection of b onto w let's call that p subscript capital w b so the projection of b onto the
subspace w the orthogonal projection we suspect that that would minimize this distance and the
distance so the distance that we're trying to minimize is b minus w minimize this over all w
inside of this subspace w equivalently you can minimize the square of the distances
and this is why this problem is called least squares because we're minimizing the squares
of each of the components of these differences when you add them all up so that's the statement of
the problem is to find w inside of w such that the distance between w is minimized
and it turns out that the solution to this problem
is exactly w equals the projection of b onto w and i won't give a precise proof of this statement
but we should at least get an intuition for why this is true looking at this picture
i've already drawn the projection of b onto w and another arbitrary vector w
now these three vectors form a right triangle so it looks a little bit skewed from this angle
but if you turn this this way that triangle looks something like here's b here's the projection
of b onto w and here's some arbitrary vector w in the subspace w these two vectors are in w
and so this line connecting them is also in w the vector b is perpendicular to the subspace w
and therefore this angle is a right angle here this is the hypotenuse of this triangle
and it's the distance from b to w and this distance is the minimizing distance supposedly
so that's just b minus the projection of b onto w so i i you know misused a little bit of notation
here um i hope you understand that this w now is different from this one this is the actual solution
and because this is a hypotenuse of this triangle we know that this distance is always going to be
greater than or equal to either of these two distances no matter what w is this will always
create a triangle a right triangle unless w equals this vector right here and in all other
cases except this one this distance is always going to be strictly greater than this distance
so what are some ways to compute this projection
so one way is to actually find an orthonormal basis of w
so given an orthonormal basis
let's call it w1 up to wk let's say k is the dimension of w
then the projection of b onto w is just take the dot product remember the dot product of
b with any of these normal orthonormal vectors gives you the shadow of b onto that vector
and then multiply again by that vector here to give you the shadow of b onto this line
in that same direction so we take the dot product or the inner product i'll write the inner product
with brackets of each of these vectors and then we'll multiply by that vector again
so that we have a vector in the end and then sum up all of these different contributions
from these different shadows so this is how you would compute the or orthogonal projection
of a vector onto a specific subspace you would need for instance an orthonormal basis for that
subspace but sometimes you're not given an orthonormal basis so it might be difficult to compute it
one thing you could do is you can choose any basis of w pick arbitrary vectors that are in w
and once you find k of them and you know that they're linearly independent then you know that
that forms a basis then in order to find an orthonormal basis you would apply the Gram-Schmidt
procedure to obtain an orthonormal one but you know how difficult that is maybe you can do it
for the first few vectors pretty easily but then after a while it gets pretty messy
so we'll look at a special case of this problem where w happens to equal the column space of some
m by n matrix where a is an m by m matrix m by n matrix so in other words you can think of a as
a linear transformation from r n to r m and in this special case we'll find a very interesting
solution to this problem in general when we look at this problem and we're given a vector b
so now let's suppose that this subspace is the column space of a and we have some vector b
that's not necessarily in the column space what this means is that
the linear system ax equals b does not have a solution unless
a is onto or more specifically or more precisely
unless the vector b is in the column space of a
but because this doesn't happen in general instead of trying to solve this system which might not
have a solution we can solve an associated system instead that says okay i might not be able to
find an x in our domain here that sort of maps to the vector b because it's impossible all x's get
mapped to this subspace what instead we can try to find is project b onto this subspace
and now this vector the projection of b onto that subspace is by definition inside the column
space of a and therefore we can solve that associated system so we make a definition
based on this idea that a least squares approximation
to the linear system ax equals b is a solution to the associated linear system ax equals the
projection onto the column space of a apply to our given vector b and it's this problem that
will be focusing on solving in the next few videos let's first state a theorem that makes it a lot
easier to compute the least square solution to a given problem in the special case that we mentioned
at the end of the video in the in the last session so the theorem says given a linear
transformation from r n to r m that's called a let me write it here and a vector b in the
codomain of this linear transformation a let's say x in the domain in the domain that's r n is a
least squares approximation to ax equals b now this is using the definition that we had made
before which remember was x is a least squares approximation to ax equals b if and only if
ax equals the projection of b onto w where w is the column space of a if and only if
x is a solution to the system a transpose ax equals a transpose b now we mentioned last time
that so let me just say here w equals the column space of a throughout this entire discussion
now we mentioned last time that if we have an orthonormal basis of w we can actually
solve this problem relatively easily but in general we're not given an orthonormal basis of w
so this formulation of the problem makes it much simpler to compute so i said it but i should also
write this that this means the taking the transpose of this matrix and taking the transpose is easy
you just swap the columns with the rows so this just gives you a new linear system
and in general this is much much easier to solve than something like this
and the reason this simplification occurs is because we've taken our subspace to be the column space
of some matrix so before we give some examples of how to apply this theorem we'll give the proof
if you want to skip the proof you can go to the next video so this is an if and only if proof
so we'll prove it in two directions let's let's first suppose that x is a least squares suppose
x is a least squares solution to ax equals b
i.e x solves ax equals a projection of b onto w
now here's a little picture that'll help us visualize everything
let's say this is the vector b this is the subspace w this is the projection of b onto w
if we take the difference of b with the projection onto w so b minus the projection
of b onto w then that difference is exactly this line that's orthogonal to w in other words this
vector is in the orthogonal complement of w and because it's in the orthogonal complement of w
we know that no matter which vector we take in this subspace let's call any vector here a
and the reason we're going to call it a is because a is an element in the column space of
of the matrix capital a then the dot product of a with any of these vectors i mean with this
specific vector equals zero for all a in the column space of a
in particular
if we take the actual columns of a
so a e i let's say and we dot this is the i-th column of a
as a matrix and we dot it with
this vector this is always going to equal zero for all i from and in this case since the domain
of a is r n it's for all i going from one to n we can write this dot product using the transpose
so remember the dot product is the the multiple you multiply each of the entries in the vectors
and then you add them all up and the way you can express that is using the transpose of a
particular vector if we write this as a column vector then we can write this as a row vector by
taking the transpose and then matrix multiplying these entries so we would take a e i transpose
times the vector b minus p w b equals zero for all i but this transpose the fact that
um if we take if we look at this um column of a and we take its transpose and if this is true
for all i then this is saying that this vector is the dot product of this vector with each
of the transpose vectors from a dotted with this is zero therefore if we take the matrix a and transpose
it and we multiply it matrix multiply it with this vector it will always equal zero
and now rewrite this by moving everything over to one side we get a transpose times the vector b
equals a transpose times this projection
but by assumption this projection we know that x solves this equation so we know that this also
equals a transpose ax and this shows that if x is the least square solution in other words if
it solves this problem then a transpose a transpose a acting on x equals a transpose b so this proves
the theorem in one direction to prove the theorem in the other direction
i'm running out of space here but i can give you at least the sketch of this proof
now suppose that um this equation is satisfied so suppose x is a solution
to a transpose ax equals a transpose b
we can move everything over again as we did sort of going backwards in this calculation
and we can express this by saying that a transpose acting on ax minus b equals zero in other words
this vector ax minus b is in the orthogonal complement of the column space of a so it's
in the orthogonal complement of w now if we go back to our picture we know that the vector b
can be uniquely decomposed as the sum of two vectors one a vector in w and one a vector in
the orthogonal complement of w so this is a theorem um that you might cover uh in in the part of your
linear algebra course on um when you talk when you discuss orthogonality so b has a unique decomposition
into a vector in w plus a vector let's say in your orthogonal complement let's call it v
where w is in w and v is in the orthogonal complement of w
but this equation here says that if we take the difference ax minus b and we get in the
orthogonal complement we know that this has to equal some vector so ax minus b equals a vector
in this orthogonal complement let's just call it v for now because it's in the orthogonal
complement rewriting this equation says that b must equal ax minus v
and a where is ax ax is in the column space of a in other words it's already in w
so this is the vector in w and therefore this vector right here has to be in the orthogonal
complement and this uniqueness decomposition theorem tells us that this vector is exactly
b minus ax so this looks this is going to look a little bit silly but b equals ax minus
ax minus b
and the uniqueness decomposition theorem tells us that this vector that's in the orthogonal
complement must equal the projection of b onto that subspace w in other words ax this term right
here has to equal the projection of b onto w minus this vector right here
in other words ax equals the projection of w onto of b onto w and that means that x is at least
square solution because it solves this equation so that follows from the uniqueness of orthogonal
decomposition of a vector into two parts if you have a given subspace one into a vector in that
subspace that's where this ax equals the projection of b onto w comes from and the other vector is
just the orthogonal complement the projection onto the orthogonal complement which is just the
difference of the vector itself minus that vector in the orthogonal subspace so this is the
the proof of this theorem that allows us to say if we want to solve a least square solution problem
when w equals the column space of a we merely have to solve this system so the next few videos
will do lots of different examples of how to actually so the example that we'll be working
out it's a quite a long example because of the generality that we'll do it in is if you're given
data and let's say the data you're given is you have a bunch of x values and a bunch of y values
so these are one dimensional input and one dimensional output values so suppose you have
given data x 1 y 1 x 2 y 2 and so on up until the number of data points that you have x d y d
and if you try to plot these data points let's say they look maybe something like this
the question that you want to solve is can you try to find a line
that sort of best approximates these data so that's the problem
is to find a best fit whatever that means
straight line let's say of the form
y equals mx plus b now if we wanted to actually try to solve this problem
and suppose that all of these points actually lied on this line
we would want to solve this entire system now m and b are our unknowns we don't know the slope
we don't know the y intercept so we'd have y 1 you want to set it equal to m x 1 plus b
similarly for y 2 our second data point m x 2 plus b and we keep going
y d equals m x d plus b now in general this is an over constrained system because we have
d equations and if d is relatively large in particular if it's bigger than 2 if it's relatively
large it's very unlikely for us to find a solution to this problem
problem we can rewrite this problem as a matrix equation by saying that we have
the vector y which is the vector of our data points in fact let me even write
y as a column vector so let's write it like y 1 all the way to y d
and if we notice this our coefficients are always being added in a linear fashion
and the only thing that's changing is the value of x 1 so you could actually write this
as a d by 2 matrix acting on the vector m b now what should this matrix be
we want it to satisfy the equation y 1 equals m x 1 so x 1 has to go in this column
plus b times what's the only thing that's going to leave b exactly where it is the number 1
and the same thing here if we had y 2 we would want to write y 2 equals m
x 2 plus 1 times b and so on all the way down to x d and 1 so this matrix equation
which we can write as y vector equals a and i don't want to write x as we did before because
i don't want to conflate it with the data points that are also labeled by x and so instead we'll
write this as a c so this is the system that we would like to solve but we know that there is in
general no solution to this problem so what can we do now in this case the column space of a
happens to be a two-dimensional subspace of r what of r d so the column space of a is a two
dimensional subspace of r d so we can actually draw something like this although the space
that's in is might be significantly larger and we have the vector y somewhere out here in general
it's not in the column space in general this line does not go through every single one of these
data points so we have some vector y and instead of trying to solve this specific equation which in
general is unsolvable we can project y onto this subspace w and we can solve that associated system
and then we'll say what that means in a moment in fact actually we can say what it means right now
if we take the difference of these two vectors y minus this projection
what are we minimizing so an arbitrary vector in the subspace let's write w as an arbitrary vector
in the subspace is a linear combination of these columns so let's write that linear combination
as m suggestively a e one which is the first column of a which is just all of these x data points
x data points plus b times the second column of a and we want to minimize the distance between our
data set our data vector y with this vector so in other words if we take this difference
let's let's replace this with w for now because let's imagine we don't yet know that this is the
projection so this difference is trying to minimize y minus m a e one
plus b a e two and if we look at what each of these components give you then this equals let's
square this just so we don't have to deal with square roots then this is the sum so first let's
take an arbitrary ith component here it's y i minus m times x i plus b and that's it and then we
take the sum of these squares because that's what this means and we sum over all i from one to d
so we want to minimize this expression in other words we're taking our actual data set y and
we're taking this which is our best fit curve using our data set x and so we're trying to minimize
all of these distances so these are actually the vertical distances between the best fit curve
and this line it's the vertical distances because this is seeing our y data point minus
the value of this line at that point and we take that distance that difference which is this
little vertical height we square that height and then we add up all of these heights and we want
to minimize that expression so the solution to this least squares problem is graphically given by
an expression like that and we know how to solve this to solve this we apply our previous theorem
and we know that to solve this we can solve instead
a transpose a equals a sorry a transpose a x equals a transpose oh and x is c
let me write this as c and a transpose y so this is the problem that we want to solve
and we want to solve this for c and c is our vector of unknowns so in order to do this we
have to write down what a is we already know what a is we have to write down its transpose we have
to multiply those two things there's a lot of things we have to calculate so let's do that
on a fresh board space so i've written the problem setup and we have the matrix a with our data
points for x and our vector y with y and i've taken the transpose and i've written it on the
left because we'll be applying matrix multiplication to this side to solve for a transpose a and then
we'll also matrix multiply a transpose with y so if we multiply these two matrices
it's the first row here times the first take the dot product with this with this column
and that's x one squared plus x two squared plus x d squared so the first top left entry
is the sum of the squares of these entries from one to d
and the second entry on the top is the first row times the second column of a
and that's x one times one plus x two times one in other words we're just summing up all of the
different x values and on the bottom left it's this first this the second row here with the
first column that's the same as it was in the top right and then the last entry on the bottom right
is the second row with the second column and that's one times one plus one times one plus one times
one d times which is just d itself so this is a transpose a and a transpose y equals first of
all notice that it's just a two by two matrix so we're going to be solving a rather simple system
it's just a two by two so a transpose y is now take the values of x multiply them with the values
of y it's sum i equals one to d x i with y i this time and then it's the second row with this and
that's just the sum of the y's and it's our vector with two components here and we want to solve
this system now it's only a two by two so on the one hand we can probably set this up as a
as a row reduction an augmented matrix problem row reduce and isolate whatever we need to so that
we can solve for this vector c on the other hand it's only a two by two matrix and row reduction
might be a little bit complicated for instance we might want to maybe divide this entry by
the sum of the squares of all of the entries but maybe that's a problem if every single one of
these is zero you know it's a little bit tricky so it's very convenient to first of all find out
when this matrix is invertible and if this matrix is invertible we can multiply both sides by the
inverse so if a transpose a inverse exists and we'll figure out what that means we'll compute
the determinant of this to determine when this inverse actually exists then we can solve this
system pretty easily and it's c which is again remember our vector of unknown coefficients m and b
then this equals a transpose a inverse times this vector right here a transpose y which we've
already computed so you know in terms of the setup it's relatively straightforward maybe calculating
this actual inverse might be a little bit of a challenge because of the arbitrariness the
generality that we're doing this in so first let's compute the determinant of this matrix
and that's just this times this minus this times this now because we're multiplying these two sums
we really have to be careful about the indices remember this is a sum of stuff multiplied by a
sum of stuff so we can't just say that this is sum xi squared it's actually there's a lot of foiling
going on and this is given by d the sum of the squares that's from the first term this times this
minus this times this and in order in order to make that calculation a little bit more straightforward
i'll rewrite one of the indices as a j instead of an i so that we don't get confused
so this is xi times xj and each of these sums there's actually two sums here
one for the index i and one for the index j and they both go from one to d
so this is the determinant and i won't do the rest of this calculation out but
um this i'll make a claim and you should check this that this equals zero if and only if
xi equals xj for all i and j so the only time that this determinant vanishes if it's if all
of the xi data points happen to be equal to each other now it takes a little bit of time to actually
show that but you can do it um and this is the only instance when this matrix is not invertible
and if you're thinking about data this basically would mean that all of your data points lie along
a vertical line and then it makes sense that you can't find a function of the form y equals mx plus
b to fit this because the only line that'll work is a vertical line and in that case the slope is
infinite so you won't find a solution so it makes a lot of sense why this is the only case where
that happens otherwise if you have even a single point that's off of this line you will be able to
find some curve that best approximates this data although you would think that maybe if all of
these points lie here and there's a data point way out here then maybe this data point is uh
there's something wrong with it or um more investigation is needed um such a point in
this situation would be called an outlier um and i may discuss about this at some point
but that's not the focus of this specific um video right now so that's the claim so this
determinant vanishes if and only if all of these data points are equal so let's assume that this
does not happen assume there exists an i and a j that's not equal an i and a j which they are not
equal and such that xi is different from xj so we just need to assume that we have at least two data
points that do not lie on um that are not the same when we make this assumption we can compute this
inverse and this is easy because it's just two by two we maybe remember this formula we just divide
by the determinant we swap these two entries and we negate these so this is just one over this
determinant and i don't want to keep writing it so let me just write determinant of a transpose a
and just remember that it equals this and then we swap these entries so this is d and here we have
sum and there's lots of indices now and i don't want to conflate any of these indices with each
other so i'm not going to call these k or something so this is k equals one to d and this is x k
squared and here we have minus sum x k oops k goes from one to d and this is minus k from one
to d and this here is the inverse of our matrix
and then what we have to do is you have to take this complicated expression
and multiply it by this vector and once we do that we'll find out what the values of m and b are
so we'll need again a little bit more board space to do that so here i've rewritten our problem and
remember we're trying to solve for the coefficients m and b for linear regression
for an arbitrary dataset and we computed that a transpose a as a matrix equals one over the
determinant of that matrix which we found was d times that's a d times xi squared minus
let's use the indices i and j here xi times xj so this is one over the determinant times our matrix
which was to not conflate these indices let's call these indices k this was i believe d here for
the inverse on the bottom right we had sum of the squares x k squared minus k x k i'll stop writing
from one to d it's just getting a little bit annoying minus sum k x k but i'll always write the
the subscript that we're summing over so this is a transpose a inverse now a transpose y
well i can't remember if i wrote it but if you remember what a transpose looks like
oh we we computed a transpose y yeah now i remember but the thing is that we'll have
to be careful about indices because i believe we use the indices i there as well and we've already
used i we've already used j we've already used k so let me call them l so this was sum
x l y l l goes from one to d and then on the bottom part of this uh two component vector
it was just the sum of the y's
okay so all of this mess is the left hand side of this expression let's multiply these two matrices
and see what we get um so let's just do that then we get and let's keep this determinant factor here
and i'm writing all of this because you'll see that it relates to something you may have seen
and of course on statistics or probability
so then we multiply d by this and we multiply this by this i'm just going to do this all out
d times this sum uh over it's just l one index x l y l minus this expression there's two sums
here now k and l x k y l that's the first component of this vector and the second component is this
times this now we have a bunch of stuff going on here um plus this times this so let me write the
plus on the left this becomes sum over k and l and x k squared which we can write as x k you know
let's just write it x k squared y l minus x k now this is a little bit different right because we
have two sums k and l and this time it's not x k squared it's x k x l y l and this is what equals m b
now so this actually solves the whole problem so we know that m equals this first expression here
divided by this determinant and the y intercept equals this expression here divided by that determinant
now does it equal anything um familiar if we look at m itself
and we divide the numerator and the denominator by d we get that m equals
sum over l x l y l minus
one over d sum k and l x k y l divided by
x i squared minus i j x i x j
now each of these expressions um actually show up in statistics quite often and they're actually
given special names we call the let's do the denominator first since this one's only involves
a single data set this is called the variance of the data set x
where x vector equals x one through x d and it's also written as var oops var of x
and this just equals by definition the sum of the x i squares minus x i j
x i x j so that's what the variance is by definition and the covariance
um is involves two data sets our x's and our y's so it's of x and y
and this is defined by
i think you know people have different notation i don't know what the notation is i don't really
care um but it's this expression on top so this is sum l x l y l minus one over d
oh did i forget a one over d i did this should have a one over d here
minus one over d um x
k y l that's an l subscript on that last um y
so we have that our linear regression problem actually derives for us
the variance and the covariance of our data set and we also have explicit expressions if we wanted to
um for the least squares uh solution if we want to fit data to a straight line curve
in the next video we won't apply this general result because i don't think anybody would expect
you to memorize something like this instead we'll set up the problem in an explicit example
redo the whole procedure just so you get a feel for it with specific numbers involved
and um and how you would actually compute the inverse without all of these sums or
anything like that if you're just given a relatively small data set if you're given
relatively large data sets then you might want to go through this approach or you might have to
program something um that does it for you so let's actually do an explicit example
using actual numbers um here's a graph and here's some data points um the x axis is the
horizontal axis and the y axis is the vertical one
and let's just use a unit grid so that the distance between any two of these grid lines
has length one so the data that we're given according to this plot is um we have our data
vector and we want to try to fit to a line of the form y equals mx plus b so let's write down our
matrix a and our matrix a remember consists of all of the x's if we write it in this form
and ones all along um the right column so how many data points do we have so what's d
one two three four five six seven three four five six seven so you should have seven um entries in
this column in the columns of a and let's go in order from left to right filling in all of these
entries the order that you go in doesn't really matter as long as you're consistent with the value
with the corresponding values of y that you use so in this case the first value of x is at x equals
negative four negative three negative one zero one three four i've chosen it to be so much symmetric
just for convenience of the um computation so it's negative four negative three negative one
zero and the x values positive x values are one three and four so this is the matrix a and the
vector y is the corresponding values of y so for x equals negative four the value of y is at negative
one again there are d there are d entries here as well the next one is zero then it's one zero one
and the last one the last two are two and four
so this is all of the information that we need
and if we compute a transpose a
what do we get so i won't write out a transpose just take the transpose of this
then we know that we're taking the dot product of this vector with itself to get the top left
entry here so what's the dot product of this with itself it's four squared times two so it's
16 times two which is 32 nine plus nine which is 18 so 32 plus 18 which is 50 plus two so it's
52 on the top left the dot product of this with this is zero because all the negatives cancel out
all of the positive entries again i chose that specifically so that this happens so that computing
the inverse is much easier and we can immediately solve this system now a transpose acting on y
oh sorry the bottom entry is is is just d itself and d is seven now a transpose y
is this times this plus so negative four times negative one plus negative three times zero
plus negative one times one and so on so negative four with negative one gives you four that with
zero doesn't change anything so we still have four then that's negative one from four so that
gives us three left over this one brings it back up to four then this six brings it up to ten
and this is 16 so we get 26 in the first entry
maybe you have faster ways of doing this i don't know
so then a transpose if we take the second row here of a transpose which is this column of ones
and we dot it with this these cancel these add so we get seven
now solving this system is pretty straightforward
right this is 52007 in one side 267 we just have to divide everything by 50 the first row
by 52 the second row by seven and we immediately arrive at the vector mb our vector of unknowns
is one half and one so this tells us that the best fit approximation that minimizes
the vertical distance squared between between that line and all of these data points has slope one
half and y intercept one so the line that we want to fit this to is one half x plus one
and if we try to sketch what that graph looks like we know that it goes through one so let's
include that point here and it has slope one half so when it gets to this when it moves two units
over it moves one unit up so here's the next data point we connect these two with a straight line
and moving over two units to the right one unit up we connect that with a straight line
and we keep doing this i mean this is how i draw um if i don't have um a rule or anything on hand
i would try to draw something like this
so this straight line here if you notice it happens to actually go through one of the
data points um that might not happen uh but as you can see it doesn't go through most of them
but it's a pretty reasonable approximation to this data set so this is how you would actually
solve a least squares problem specifically in the context of a fitting data to a linear curve
or rather an affine curve to be technically correct um and uh this is how you do it in such
an example in the next few videos we're going to generalize the idea of linear regression
just in terms of a straight line data fitting to linear regression in the sense that you can
data fit your data to sort of any curve um almost any curve and the way that we're going to do this
is we're going to set up um some notation and we're going to let f1 through fk be linearly independent
functions
and what i mean by this is it's the same definition of linear independence of vectors
namely that um there does not exist a set of numbers a1 through ak such that when you sum up
so let me just say this i.e. there does not exist
a set of numbers a1 through ak
so these are real numbers or complex if these are complex valued functions
such that the sum of ai fi equals zero as a function
so um let's just say the domain of our function is whatever we need to specify it to be for
example the the whole real line or maybe an interval or something like that
so um and imagine your given data points
and let's say the given data points again we're going to use our x and y variables
so your input is x and your output is y and you have a whole list of data x1
x2 up to xd where d is the number of data points
and you want to fit these points to these functions so in other words your hope
is to somehow fit y1 equals to a1 f1 of x1 plus dot dot dot ak f k x1
and not only do you want this but you also want this to hold for all of your data points
so up to yd a1 f1 xd now plus dot dot dot ak f k xd
so this is your hope but if d is much much greater than k then this is unlikely
it's usually impossible to find coefficients that fit all of these data
so before moving on let's try to rewrite this expression in a linear way so that we can relate
it to the linear regression problem we solved earlier so set y to be this vector here so let's
call this the vector y and what you notice here is that each of these numbers so f1 x1 is a specific
number we're taking a linear combination of these numbers with coefficients coming from the a's
so this looks like the vector y1 down to yd this is what this equation is represented by
a matrix whose entries are given by these values of f so f1 x1 in the first column and up to yd
the coefficient front of a1 is f1 xd and then this goes up to fk still x1 so x1 is the first row
and down to fk xd in the last row and this matrix is applied to the vector of unknowns a1 through ak
so this is again of the form y equals a and let's call it xc instead of x to not confuse
ourselves with the variable x that we've used for our data so in general it's impossible to solve this
and the way that we would like to solve this is again a least square solution so a least squares
solution or approximation to
this is a actual solution to
a transpose y equals a transpose ax so just apply a transpose on the left on both sides
and this is generally what we're going to solve for and this will be our this will be fitting our
data to the set of functions defined by these but there are a few restrictions that have to be made
for example the first maybe obvious restriction if you think about it is that these coefficients
should be independent and independent in the sense that i can't take any one of these coefficients
and sort of re-express it in terms of the others i'm not talking about linear independence i'm just
talking about independence so we assume the coefficients are independent and this just means
i.e. there does not exist an i
from one through k such that
a i is determined by
a j by all the other a j's so let's just say the set of a j's where j is now
from one excluding i so i read a little hat over that to exclude i up to k so in other words in
terms of all of the other coefficients so we assume that they're independent and this is sort of
obvious right because if you wanted to fit your data to these functions and you assume that these
were all unknown coefficients and you wanted to find the best value for them then if you
suddenly did that arbitrarily then it's unlikely that this relationship between them holds
in that situation so in general we definitely want to make sure these coefficients are independent
not only that we also should assume that the functions are linearly independent so we assume
that these functions are independent
as well and this is because
so suppose that one of these actually depended on the other so because if let's say f i equal to
some linear combination of the other ones so let's say b j f j so j goes from one to k
but j is not equal to i so we're just saying like for these to be linearly independent another way
is saying that well at least um none of them can be expressed in terms of the other so if that
fails at least one of them can be expressed in terms of the others so because if for some
numbers b j
then what happens is expressions so then if we take
so then if we take f and we take its linear combinations so let's say
a i sorry let me not use the index i let me use the index j now so let's take some of a j
f j and this breaks up into two parts now right because we have a sum over j where j is not equal to i
so this is j um not equal to i and the sum goes from one to k so this is a j f j but then we also
have plus a i f i but this term equals this so this equals sum over all j not equal to i
another sum over all j that are not equal to i
so we have a i sorry a j i'm just copying this term f j plus a i times this so a i times
b j f j and then this is all in parentheses and now you notice that f j is a common factor
so when you factor that out you get sum j not equal to i and then this is a
j plus a i b j f j so now what we've done is we've re-expressed our linear combination of these
functions so the way everything that's on the right hand side here in particular
and we've re-expressed it in terms of functions in terms of k minus one functions and now our
coefficients have changed so in other words there was already a dependence on the coefficients in
some sense and so we usually demand that the functions are linearly independent so that
we avoid this um issue in the next video we'll explain more generally uh a simple situation
that occurs in which this function this uh linear system is always um solvable by the method that
we used earlier namely by taking a transpose a inverse let's now understand when we can solve
a transpose y equals a transpose a c using the method of taking the inverse of a transpose a
now in order to take the inverse of this
we know that we need to require that the kernel of this matrix so by the way if a is a
is a d by k matrix and again d is typically much much larger than k
then we want to know when this exists so one of the situations when this exists is when
the kernel of this matrix vanishes that's one of the criteria
so zero as a vector space as a vector subspace um of r k
so when does something like this happen so to understand when we can apply this method
let's suppose that this is the matrix a a goes from r k this is r d here and this here is the
image of a if we take the orthogonal complement of this image in this case you know unfortunately I
can only draw the orthogonal complement is having a single dimension but you could imagine that it
much much larger dimension especially if d is much much larger than k
so the first claim that will prove is that the orthogonal complement of the image of a
equals the kernel now in order for this to make sense I need to take the kernel of some matrix
now the image of a is in r d it's orthogonal complement is also in r d and I can't take
the kernel of a because that wouldn't make sense the kernel would live here so I have to take
the only other thing I can take the kernel of is maybe the kernel of a transpose
so we'll do that so we'll take the kernel of a transpose and it turns out that these two are
equal so how do we see this let's visualize a as a as a matrix of vectors so a one through a k
and when we take the transpose these rows these columns just become the rows
so we'll do this proof just by showing that one is contained in the other just to make it very
explicit so suppose that the vector v is let's start with the let's start with being an element
in the orthogonal complement so let's say v is perpendicular to a the the image of a
and then let's see if it's in the kernel of a transpose so when we take a transpose applied
to v what do we get so we'll write the matrix a transpose now we take these columns and turn them
into rows and we apply it to the vector v but matrix multiplication tells us that when we do this
we take this row multiply it by this vector in other words we take the dot product so this equals
another vector and it's a it's a vector in r k and what we get is a one dot product with v
as the first entry all the way down to ak dot product with v but if v is in the orthogonal
complement of a then it has to be that all of these dot products are zero so this is actually
the zero vector and therefore therefore the this containment holds the image of the orthogonal
complement of the image of a is in the kernel of a transpose so that shows half of the theorem
now let's suppose so conversely
suppose that the vector u is in the kernel of a transpose
then by the same argument being in the kernel of a transpose
a transpose u equals zero but a transpose u is a one dot u all the way down to ak dot u
but the zero vector says that all of those are zero and because the image of a is spanned by
the vectors a one through ak we know automatically by the same exact argument that u is perpendicular
to the image of a so it's almost the same argument which is why i'm not writing it and therefore
this containment holds and that's the other half of the theorem so that's the proof that
the kernel of a transpose equals the orthogonal complement of the image of a
why is this useful
it's useful for the following very important reason
and it says that the kernel of a equals the kernel of a transpose a
you can already see why this is going to be useful because instead of looking at the kernel of a
transpose a which we take two matrices multiply them it's going to be a little bit more difficult
matrix to work with if we could just look at the kernel of a that would probably save us some time
so let's prove this in one direction it's pretty obvious but i'll write it out anyway
so let's first prove the direction that the kernel of a is inside here
so let's prove on this containment so if u satisfies
a u equals zero then a transpose a u because this thing is zero also equals zero
so that direction is pretty straightforward let's look at the other containment
so suppose v satisfies
a transpose a v equals zero then what this means is that a v is in the kernel of a transpose
i.e. a v is in the kernel of a transpose but by the previous claim the kernel of a transpose
equals the image of a taking the orthogonal complement of the image of a
so what's the picture here actually let's go back right here so we have that a v which by the way
is in this plane also is contained in the orthogonal complement of that image and the
only vector that's contained both in a and in the orthogonal complement is the zero vector
this implies that a v equals the zero vector in other words v is in the kernel of a
and now the containment has been shown in both directions and that's the conclusion of the proof
and let me just write out the final corollary which is the useful one for us
it's like corollary two
is that at least so let's say a transpose how do i say this a transpose a inverse exists
if and only if the kernel of a is trivial so it's only the zero vector now
why is this reasonable so this is this isn't really an example it's sort of an idea for why
this is uh this usually occurs when you're trying to fit data so our matrix a is typically going
to be of the form f 1 x 1 dot dot dot f um what was it x k f k x 1 all the way down to f 1 x d
f k x d so typically our matrix a looks something like this
and what would it mean for this to have trivial kernel it would say that none of these so all
of these vectors are linearly the set of these vectors the column vectors are linearly independent
is that likely so when when might something like that happen so for instance if one of these functions
did depend on the others in a linear way so for instance in the last video we said that
we assume that these functions were linearly independent if they were dependent what could
happen one of these column vectors could be expressed as a linear combination of the others
and therefore these columns would be linearly dependent and if these are dependent then this
has a non-trivial kernel so that's at least the sufficient that's at least one condition
that's a necessary condition for this to have um a non-trivial kernel so we demand that these
functions are linearly independent but furthermore not only do we ask that these functions are
linearly independent but it also implies that these specific vectors after we apply our data
are linearly independent but if d is much much much larger than k we only have very few of these
vectors right so the number of entries is d but we only have k vectors so it's kind of easy if you
randomly chose if you arbitrary and randomly chose k vectors in a very large dimensional space
randomly with almost almost surely it will be that those vectors are linearly independent think
about it just choose random numbers so for example let's write pi e 1 2 square root of 3 3 and the
vector 1 1 1 i'm pretty sure that these three vectors are linearly independent in r 3 and i
randomly chose them so even if d is not drastically larger than k but even if it's just greater than
k almost surely you'll pick linearly independent vectors so if your data is sufficiently you
know distributed well and it's not lying exactly on one line or something like that then chances are
these vectors are linearly independent so that's where it's going to be useful
and in the next video we'll actually apply this to a simple example that you probably don't need
a calculator to compute with in the next few videos we're going to be working with arithmetic
modular two so we're going to deal with all even numbers are equal to zero and all odd numbers
are equal to one so for instance two times three is six which is an even number so zero
and seven plus three is ten which is also even which is zero for another example is negative three
equals one in this case so anytime we do arithmetic for the most part when we add we're only going to
be caring about the parity of that number and this is going to be there are multiple reasons
for this one of which is simplicity the other of which is is that it's related to computer science
so we're going to let z mod two be exactly those numbers and with the arithmetic that I just said
so zero plus zero zero zero plus one is one one plus one is two which is zero and then
multiplication similarly zero times one is zero and one times one is one and we'll also work with
vectors whose entries are elements of z mod two so these are going to be vectors of the form x one
all the way up to x n where x one through x n are in z mod two and we can also do arithmetic
the way we usually do with vectors with vectors of this sort by just adding component wise and
scalar multiplication on each components as well the interesting thing about this vector space
is that unlike the vector space r to the n this has finitely many vectors so how many vectors does
this vector space have well first of all here there are two elements
and if you have n component vectors think how many entries think what possibilities you can put in
that first entry you can either put a zero or a one and as soon as you move to the next entry you
can also put a zero or a one and therefore each time you go through these entries you have two to
the n total possibilities so the number of vectors in z mod two to the n is two to the n
and one of those vectors is very special namely the zero vector and the non-zero vectors well
there's just one less of them and I know that sounds like a trivial thing to point out but it'll
actually be important in our discussion and so for example this is the main example that we'll
be working with z mod two to the third power has seven non-zero vectors for example so let's make
a definition first first we're going to be exploring a lot of mathematical curiosities
and then we'll see how they apply to an actual physical situation and I rather you have a little
bit of suspense before we get there so first we're going to do some math and then we'll talk about
the applications so a hamming matrix is a matrix h with k rows and the columns
of h consist of all the non-zero vectors
in z mod two to the kth power so k here is a non-negative integer in fact let's just
yes suppose it's a positive integer so for example when k is three we have seven non-zero
vectors and what this is telling us all right now let's try to understand these two matrices a
little bit more the matrices m and h that we introduced earlier so recall that h was the matrix
it was the identity matrix a three by three in this case and another matrix q and m
was q and then the identity four by four matrix and both of these numbers can be generalized
as long as it's an appropriate size and it satisfies the requirements that we made earlier
namely that h consists of all of the non-zero vectors in the vector space can z mod two to the
power where the power is determined by the number of rows here so given the setup
let's introduce a little bit more notation and that notation is going to be we're going to
define these on that subspace which was the kernel of h and also the image of m so let's call these
image of m which is also the kernel of m uh kernel of h rather let's denote this by c
so for the rest of these videos c will refer to exactly that subspace now remember this is a
four-dimensional subspace inside of z mod two to the seventh okay we're also going to introduce
other notation let c subscript i be that subspace shifted by the i-th unit vector in z mod two
so it's going to be c plus e i and this just means by definition the set of all vectors
of the form v plus e i where v is in c now this is not a subspace right um because we can't add
two vectors and stay within the subspace uh stay within the subset but at the very least you can
think of this as um the subspace shifted by some vector and we can define this for all i
between one and seven because that's how many non-zero vectors there are in sorry that's that's
that gives us a basis of um vectors in z mod two to the seventh power and
now let's write some additional facts regarding these subs these subsets
so the first thing is that we already know that c is the solution set of a homogeneous system
namely it's the kernel of h c i is also the solution set of some system though it's no
longer homogeneous c i is the solution set of the inhomogeneous system h x equals h e i
where this is this whole thing h e i is the i
column of h
secondly
if we take any two of these different subsets c i and c j then c i intersect c j so if we
look at all of the vectors that are common to both of them it turns out there are none
so it's the empty set for all i not equal to j
third
each of these subsets are also disjoint from the solution set of the homogeneous system
so c intersect c i is also empty for all i and finally and this is maybe the most
interesting part of it is that
the entire vector space of all vectors is the union of every single one of these
so it's the solution set of the homogeneous system with all of these other inhomogeneous
solution sets
and because these are all disjoint this is a disjoint union so every vector in z mod 2
is in exactly one of these subsets it's either a solution set of the homogeneous system or
it's in one of these solution sets of the different inhomogeneous systems
so this is a very important claim so let's actually let's actually prove it
so the first claim
now when we solve inhomogeneous systems all we have to do is find one particular solution
and if we find that a solution exists then the solution set of the inhomogeneous system
is that particular solution plus the homogeneous solution that we obtained
from solving well for the kernel of h so notice however that we can just take x to b e i to get
a solution set so e i is a particular solution
and therefore the solution set of the whole system
of h x equals h e i is that particular solution plus the homogeneous one
and that's exactly what the claim is see i is the solution set of this
now let's look at the second claim the second claim says that these are all different
all of these subsets for different i and j have no common intersection
so in order to prove that let's pick two vectors one in c i one in c j and they're going to be
arbitrary and then we're going to show that the only way that they can be equal to each other
is if those subscripts are equal if i and j are equal so let's start suppose
that we have two vectors now because we're a solution set of the homogeneous system the
kernel of h and the kernel of h equals the image of h our vectors are going to have this form
so suppose m u one plus e i so this is our vector in c i equals m u two because we don't know if
right these two could have different they have come from different um vectors
plus e j so suppose these we have these two vectors and this one is in c i this one is in c j
now if we apply h to these vectors so let me just write that this is in c i this is in c j so we're
totally clear now apply h to these this to this equality what happens well because these functions
are linear and we apply h to both on the left hand side this becomes h m u one plus h e i
equals h m u two plus h e j right and h m of u one is zero because h m is the zero matrix
so this is zero that's zero and we're left with h e i equals h e j now the only way that this is
possible is if i and j are both equal to each other and the reason is because h by definition
is the set of all non-zero vectors in z mod two to the third power and they never repeat
so we only use those vectors once and only one so to better understand this application
let's first notice that if we apply m acting on any vector u the vector we get is q applied to you
in the top part of that um entries of those of that vector and we retain a copy of u
in the bottom this is because the matrix m was q on top and then the identity matrix on bottom
so this is true for all u in z mod two to the fourth
and so a copy of your original vector sits inside of this vector
so imagine you're trying to send a message u across some sort of a channel a communication channel
and you want a receiver to obtain um that message and you would like it for them to obtain
exactly the message you sent because if you hear something else on the other end of that line or
you see something else then you may misinterpret what the sender is trying to tell you so there's
a sender and a receiver and so for example um during this transmission there could be
some noise or maybe something that alters that message you hear this all the time when you're
on the phone and sometimes the signal isn't working too well you might not hear exactly what the other
person is saying or you might hear something a little bit different so there may be disturbance
along such a line so for example if we were sending um let's say my name across this channel
and at the end of the line the receiver sees um
the word archer for example now what was the original message that was supposed to be sent
in this context you have you know you know the english language so you know that there may be
um a specific word that this is corresponding to but in this example you have two possibilities
that this word could be at least um one of them could be archer or maybe arthur
and in order for the receiver to verify what the message was or one way to verify what the
message is is they could send that same message back and then basically ask you know is this the
message you intended to send okay so now imagine that this person sends um let's say this person
sends archer back and imagine another error occurs and imagine that the error occurs um takes place
let's say in the first entry and it becomes archer
and then the person is like wait did you want to send me the word archer like what are you doing
with this message um are you trying to tell me escher or archer and so this person is going
to send another message back um asking and you can see that this could keep happening for a very
long time um so it would be very convenient to either this person can send multiple copies of
that message and then with lower and lower probability the more messages you send the more
likely it is that the person on the other end will figure out what that message is supposed to say
so that's one option um but this option seems to take up a lot of resources right sending a
message over and over and over again is sort of multiplying the number of resources you need by
the number of times you send that message it'll be very convenient if you could somehow have a scheme
where the sender is sending a message and the receiver can apply a certain method that both
the receiver and center have agreed upon in advance to possibly identify if if an error occurred
and where an error occurred during that transmission so that's what we're going to do and
we're going to simplify the problem by not looking at the English language we're going to look at
vectors whose entries are just zeros and ones the simplest possible language that we can come up
with or at least the simplest um list of the simplest alphabet we can come up with an alphabet
containing two um symbols so let's say we initially send the vector zero one one zero across this
channel now once this channel goes i should have written it from right to left as i've been doing
so but let's go um counter to this now if one error occurs suppose one error occurred that
means that error is going to occur in one of these four entries and if it occurs in the first entry
the only possible thing that that zero could become because our language only has two symbols
is one so one possibility is that we get one one one zero at the other end of the line
another possibility is if the error occurs in the second entry in which case we would have zero zero
one zero and so on so in the third entry zero one zero zero and in the last entry zero one one one
so these are the possible outcomes if we have exactly one error of course if no error occurs
then the receiver will see the original message but how do they even know
that an error didn't occur or not so the way that we're going to solve this problem
is by using the previous situation that we had developed we can take our original message
encode it in some larger message and then this message is going to be contained in the subspace
c so if we send the message u it's going to be contained in that subspace c and if we send
that message across the channel instead what could happen to it so initially the sender is
sending the the letter the message u is contained in the bottom part but now mu is contained in
z mod 2 to the seventh power so it seems like a more complicated vector but the only real
messages that could have been sent the ones that have no errors are exactly in that subspace c
any other vector in this vector space is not a message that the sender could have sent
because they're only working with images the image of the transformation associated to m
so this message is going through now imagine that an error occurs somewhere along the way error
and the message becomes mu plus now there are seven entries in the vector mu so there are now
seven possible errors that could occur and these errors are exactly quantified by adding the unit
vector in the ifh row or entry of that vector so this error occurs but the reader on the other end
is going to see this vector v they don't know that it is a priori this sort of combination
all they see is some vector of zeros and ones but they can use h to identify
what form the vector v is in remember we said that if h of v equals zero and this implies
that the vector v is in the subspace c which is the image of m
and if h of v equals a non-zero vector then that non-zero vector is one of the columns of h
this tells us that v is in ci but remember what ci was it was this subspace plus the unit vector
ei so it tells us that if a receiver receives receives the vector v and they apply h to it
they can identify which of these subsets it's in and if the vector that they see after they
apply h is zero that tells us that no error occurred so we're going to assume at most
at most one error occurs during the transmission
and if we make that assumption then these two applications an application of h to v will tell
us where an error occurred and if we've identified where the error occurs right this says that if
we see that the um h of v is hei then we know that the vectors of this form and how do we fix it
so if if it's let's say this is case one and this is case two in case one how would the
receiver identify what the original message is they would look at the last four entries of
the vector v because that's where u is and we know that no error occurred so the original
message sent by the sender is the vector corresponding to the last four entries
of the vector v and in the second case what happens then well if in the second case we found that
h of v equals h of v i then an error occurred
in the i-th entry of v and how would we fix that well we would just subtract e i
but subtracting in addition are the same in z mod 2 so to fix
we know that the original message will be v plus e i well not the original message but
what the receiver sent after applying the transformation m and when they do this then they
can read off the last four entries of this vector the last meaning the bottom four
of this vector v plus e i is the original message
so let's just do this in an example just to see how exactly this works
so imagine you're the receiver and you see the vector v equals zero zero
one one zero one one
if you apply h to this vector so i'll write h to remind you because otherwise
how are we going to do this computation huh so this is one one one zero one one one zero one
zero and then we apply the vector v here
and if we apply matrix operations here we will get the vector three two three but three is one
in z mod two and two is zero so this becomes one zero one so we take this vector and look
where it appears in this matrix and in this case it is the sixth column of h this means
that an error occurred in the sixth entry of this vector here so error in sixth entry of v
and therefore the if we alter the sixth entry that would mean we change this one the second
last one to a zero so that means the original message
message is one zero zero one
because we take the last four entries of this vector and then we switch the sixth entry if
we had found that the second entry was um an error occurred in the second entry we would have
changed that zero to a one and left the original message here and that would have been our the
message that was sent by the sender so um that's the basic idea of how this works and again we
worked with a case where we were dealing with um sending messages of length four and we used
um an additional a larger vector space to encode the possibilities of computing those errors
and you could also do it by um using the um by having h to be a matrix consisting of all the
non-zero vectors in z mod two to the k it will allow us to encode a message of length
given by the number of columns in that matrix q and we already calculated that the number of
columns in that matrix q is two to the k minus one because of the zero vector minus an additional k
from the k vectors we used on the left hand side of the matrix h so we can encode quite a large um
number of messages under the assumption that at most one error occurs during transmission
so let's now analyze in a little bit more detail
what is q u actually doing so we know that that matrix m that we had it was broken up into two
parts and when we send a message u across the channel we will keep our original message in
one part of that vector but we'll add a bunch of fluff to it and what is the meaning of that fluff
from maybe a more a different perspective um it turns out that there's a very interesting
sort of a logical thing that's going on between the entries of u and what q is doing to those
entries and the idea is that it's adding those entries in such a way as to maintain the sort of
consistency so if we take actually q u and we apply that matrix q that was left over
the vector we would get in terms of the entries of u so u is going to be u one through u four
the entries of this vector are going to be u one plus u three plus u four
u one plus u two plus u four and the third entry because this is a three by four matrix
is going to be u one plus u two plus u three and these entries here are called well let's call them
p one p two and p three for now and they are called parity bits
and the reason they're called parity bits is because when this message gets sent across a channel
if an error occurs these entries are summing up the entries of the vector u in some specific way
and if an error occurred right we have some vector
p one p two p three and then u one u two u three and u four if an error occurred in one of these
entries then these parity bits will detect if an error occurred and where the error occurred
based on the consistency of this formula so let's see how this works in an explicit example
let's say we have the vector zero zero one and i'll break this up into the two different parts
so that we isolate the parity bits versus the original message and by the way this isn't the
original message that i'm writing right now this is what happens after it's sent and let's
see the receiver sees this message i believe this may be the example we were working with
a moment ago so let's now look at these formulas and see what they say so p one on the one hand
equals zero but let's see if the sum of these entries is also equal to zero so if we take
u one plus u three plus u four we get one plus one plus one is three which is one which is not equal
to one which equals u one plus u three plus u four what does this mean this means
an error occurred
in one of these entries
and when i say one of these entries i mean either p one u one u three or u four so let's write that
down p one u one u three or u four and we know it has to be exactly one because again we're assuming
at most one error occurred and because of this inconsistency we're guaranteed that an error occurred
the only way no error would occur is if all of these would be consistent so if p one does equal
this if p two does equal that p three does equal that because this would say that our vector is of
this form and applied to the original vector u so that doesn't exactly tell us which of the errors
it is yet is it p one u one u three or u four so for that we'll look at the other parity bits
so let's look at p two the vector we see says p two is zero is that consistent with this formula
u one plus u two plus u four so u one plus u two plus u four is zero so that actually is consistent
what does this tell us this tells us that no error occurred in any of these entries
because if one error occurred it is impossible for these two to be equal to each other so this means
p two u one u two and u four are all
error free now let's compare this to the first one that we analyzed the first one said
it was possible that the error occurred at u one and it was also possible that the error occurred
at u four this new observation tells us those two possibilities it's not possible
that an error occurred in those entries so now the only possibilities left are p one and maybe u
three so we'll keep that in mind when we go to the last parity bit which will then isolate exactly
where the error occurred so p three is equal to well from this it's one and is that equal to
u one plus u two plus u three u one plus u two plus u three it's equal to zero so that's not equal
to this which is u one plus u two plus u three now this tells us that error is in one of p three
u one u two or you or u three
we already know that u one and u two are not possible right u one and u four are not possible
and the only error that's common to both of these right because we know an error
one error occurred in either p one or p r u three or it's possible that an occurred in p three or
u three but if it was p three right suppose that the error occurred in p three then this would
have been fine it would have been unaltered because we wouldn't have detected an error
u three would have also been okay so the only possibility in this case is that an error occurred
in u three the one that's singled out from these three parity bits so error in u three
and therefore if we go to this original message the message that we received rather and then we
this is sorry this is the message we received but we would have to alter is the u three entry of
this to get back the original message therefore the original message
is the last four entries as it was before but now we alter that third message that third entry
to get one zero zero one as the original message being sent and this is consistent I believe with
the answer that we obtained earlier so you might be wondering okay this is a little bit more
intuitive because we're sort of counting up our different entries in different ways and sort of
using a process of elimination method to isolate exactly where the error occurred now of course
that is a little bit more straightforward it's easier to work with it's easier to think about
the first time you see it perhaps on the other hand the linear algebra method
it allows you to see it from a maybe potentially different perspective and I would think that if
you're working with a much much larger message that the linear algebra method seems to be a
lot easier to work with especially when you look at the way that we multiply those matrices and
the form of the hamming matrix that we constructed so let me just say this that the cs hamming matrix
looks a little bit different for instance I think it starts out with
um one zero zero zero one zero but then the third column is not zero zero one I think the
fourth column is zero zero one and these other four columns are some permutation of the leftover
columns I had and now you can see if you were to manipulate this with the other matrix m that's
associated to this one by demanding that the kernel of h equals the image of that matrix m
the algebra would be a little bit more we can't just break this up into do blocks identity and
the leftover part instead it has sort of this interpretation but I believe the linear algebra
calculations are much much simpler if you work with a block diagram a block matrix of the form
that I indicated earlier now this may change if you try to look at what happens if multiple errors
occur how would you potentially correct for all of those additional errors and I'll leave you to
think about that and to check out the literature in the next few videos we're going to compute
the square root of a positive matrix and the way we're going to do this is by
introducing something called the functional calculus and in fact we'll learn how to compute
given any function under suitable conditions what it means to apply that function to a given
square matrix so let me go ahead and state the statement of the theorem that will prove
and we'll prove this theorem first by doing an example and then we'll prove the general result
from scratch so it says let a be a diagonalizable
n by n matrix
and let f be a function be a complex valued function let's say
defined on
what I'm going to call sigma of a and sigma of a is the set of all eigenvalues of a
now if we have this setup we can already define what f of a is so let's do that
so f of a is going to be defined as p f of d p inverse where
p is the n by n matrix is a matrix
of eigenvectors
of a written as columns and d is the corresponding matrix of eigenvalues
and what do I mean by f of d
and f of d is defined to be now d is a diagonal matrix so let me just write out exactly what
we're doing if we have a matrix of eigenvalues and these eigenvalues can repeat so let me just
write all n of them and then this is zero everywhere else we define f of this matrix to be f applied
to the elements along the diagonal and zero everywhere else so this is f of lambda one
f of lambda n and zero everywhere else so so far all we've done is set up our assumptions
so we have a matrix we have the eigenvalues we can define f applied to a provided that we
have a complex valued function defined on the set of eigenvalues and here's the statement of the
theorem then there exists a polynomial
q such that q of a now what do I mean by q of a q is a polynomial and it makes sense to multiply
so we can take a we can square it we can cube it we can also take it to the zero power that's
just the identity matrix and then we can also multiply these by coefficients so if I have
any polynomial it's very easy to define what q of a is you just write your polynomial and where
you have your variable you replace it with the matrix a so this is some polynomial in a but it
turns out to equal f of a as defined previously by this method of breaking a matrix up into
its eigenvalues and getting its eigenvectors and constructing it this way so that's what
the statement of this theorem is and it's very surprising because in general you can think of
a very strange function such as the square root and this is telling you that there is a way to
write the square root of that given matrix in terms of a single polynomial and what we're
going to do first is do this through a simple example and illustrate it with that simple two
by two matrix and then we'll prove the general theorem so we might as well start this example
now and continue it in the next video so the example is going to be let a equal 10 6 6 10
and our goal is to compute the square root of a so the first step is find the eigenvalues
so another thing that we'll do is we'll review how to do these things so to find the eigenvalues
compute the determinant of 10 minus lambda 6 6 10 minus lambda and this equals 100
plus lambda squared minus 20 lambda minus 36 and some of this simplifies we get lambda squared
minus 20 lambda plus 64 and this also factors into lambda minus 4 and lambda minus 16 so we
know what our two eigenvalues are they are 4 and 16 and while we wait for the next video you can try
to compute the corresponding eigenvectors and I'll just give you the answer there in a moment so
here's the matrix that we're looking at the associated eigenvalues that we found before
and corresponding eigenvectors which you should have found by computing the corresponding eigenvectors
and so now let's compute what f and f meaning the square root of a so what is f of sorry f of the
diagonal matrix d associated to these eigenvalues this is taking the square root of each of the
corresponding entries on the diagonal so it's just 2 and 4 and the matrix p is writing down these
two eigenvectors so it's just one negative one one one its corresponding inverse is the determinant
here is 2 so it's one half and then the rest of this matrix we swap and we negate so that's the
corresponding inverse of this matrix so what happens when we compute p f of d
p inverse supposedly we should get the square root of our matrix which means that if we square it
then we get back our matrix a so if we multiply some of these out I'll skip some of the steps
so if we take one half when we multiply p with f of d we get 2 4 negative 2 4
and then we also have p inverse still here I've already pulled that one half out
and multiplying these matrices out we get well that distributes out so we can just have
one two negative one two and when we multiply those we get three one one three so let's check
that if we square this matrix so let's let's just call this f of a this is the definition
that we gave of f of a so what happens when we square this matrix f of a squared we get
exactly 10 6 6 10 so we do get our original matrix back so this is one way of computing
the square root of a matrix or at least if it has positive eigenvalues
by computing the corresponding eigenvectors and eigenvalues
and supposedly we have another way of doing this and the interesting thing about the following
method is that we will not be able we will not need to use the corresponding eigenvectors all
we need to use are the corresponding eigenvalues and we'll find that polynomial that allows us to
compute the square root of this matrix so how do we do that for the time being what we'll first do
is we'll find a polynomial
q such that q of lambda one equals the square root of lambda one or f of lambda one
and q of lambda two equals f of lambda two so in this case these are the square roots
and we already know exactly what their values are this is two and this is four
so we're trying to do at this point now we're doing a different problem it seems like
because now we're just trying to find a polynomial that interpolates these two values of a function
so what we're trying to do is so here's lambda one here's lambda two and we have a function
which is just the square root and we know that f applied to lambda one is two and f of lambda
two is four now this is not drawn to scale in any way but what we're trying to do is
find a polynomial that goes through these two points now you know that two points determine
a line so a straight line goes through these two points and that straight line of the form
y equals mx plus b so our goal is to find out what are m and what are b such that
when we plug in x which is our values of lambda we get the corresponding values of y
so this isn't a very difficult problem but what we're going to do is set it up as a linear algebra
problem even though you could probably immediately solve for m and b and the reason we'll do that
will be made more apparent later when we try to compute f of matrices of larger sizes where it
will be more difficult to do the simpler method and it's more reasonable to solve that system of
linear equations using techniques of linear algebra so when we set this up we write on
this side since this is our y we have m lambda one plus b and this equals m of lambda two plus
b and our unknowns are m and b so if we set up our matrix system we get and what i'll do for
convenience is i'll put the ones on the left so i'll put my b's on the left column so it's really
b plus mx one one and then this is lambda one lambda two and our two corresponding values
f of lambda one which in this case is two and four
and we know what lambda one and lambda two are they are four and sixteen so really this is equal to
one four one sixteen two four and if we try to row reduce this system and solve it what we end up
getting is b equals four thirds and m equals one sixth so this line is of the form y equals
four thirds plus one sixth x and that's our polynomial this is our q of x
and what we'll do in the next video is we will actually apply this polynomial to our matrix
and see if it also satisfies the same equation so here's the polynomial that we found as a
real valued function in this case and if we wanted to define q of any matrix i'm just going to write
a but it's for any matrix a we would the associated polynomial on matrices would be four thirds times
the identity matrix which in this case is an n by n matrix well in this case it's two by two matrix
plus one sixth a so let's see what happens when we actually compute this
so we have four thirds of the identity
both along the diagonal plus one sixth of our matrix a so it's 10 over six which is five thirds
one one five thirds and if we add these two matrices what do we get nine thirds which is three
one one three which is exactly what we found for f of a before so we already know
that when we square this matrix we get exactly our matrix a back now let's look at the more general
situation so we're going to go back to our setup where we have an n by n matrix a a function f on
the set of eigenvalues so we write if a is n by n and lambda one through lambda n are the eigenvalues
and f is a function
on the set of eigenvalues to let's say the complex numbers
we're going to find a polynomial q that first satisfies the initial equation we wrote down
for the associated eigenvalues so our goal is to find a polynomial
q such that q of when we plug in our corresponding eigenvalues we get
applied to those corresponding eigenvalues and we already know that that's problem will help us
solve this one by a similar analysis that's why we're reducing our problem to finding a polynomial
on just a finite set of numbers rather than trying to find the answer to our matrix problem
and in fact when we look at the degree of this polynomial we notice that it was also matching
the degree of the size of our matrix and that's going to be true in general we'll be able to find
the polynomial whose degree is at most the size of the matrix that will solve that problem mainly
q of a equals f of a and why that happens is precisely because of this equation because there
are going to be at most n distinct eigenvalues and so we only need to find a polynomial so let
me draw this as visually let's just assume everything is real so it's simple to draw this
so if we have lambda one here lambda three here lambda two maybe another lambda four somewhere
out here and let's say lambda two equals lambda five for instance and if we apply f to these numbers
let's say they look something like this what we're going to try to do is find the polynomial that fits
through these in this case four points and the reason it's four is because two of our eigenvalues
are repeated and so we have to find the polynomial through these
four points so and if we had n distinct eigenvalues we would have n distinct points
through which we would have to find a polynomial sorry i misspoke i think i said degree two i meant
degree one because one is the highest power but it starts from zero so in this case we would find a
degree in this case we would find a degree three polynomial
and in general it would be a most n minus one degree so and again if we have multiplicity
that's non that's um bigger than one then the problem is going to be a little bit easier to solve
because we can find a polynomial of a lower degree so let's just assume that all eigenvalues
are distinct just it's not it's not actually making our problem easier it's making it a
little bit harder because if some of them repeat then the problem is reduced to a smaller and simpler
matrix algebra problem so if we assume all the eigenvalues are distinct we're really doing the
hardest case now when such a thing happens we can write our polynomial q of x as a zero plus
a one x plus a two x squared all the way up to the highest degree which you know just by looking
at the pictures we're assuming it's of the form a n minus one x to the n minus one
and if we write down all of these different equations we're going to get another linear system
and the unknowns of that linear system are these a's
and we know the values of x's those are different eigenvalues and we know the q of those x's are
it's f applied to those values so the associated linear system that we get
looks like one ones along the vertical on the left side corresponding to the coefficient in
front of a zero the coefficients in front of a one are the different eigenvalues
the coefficients in front of a in front of x squared are the squares of our eigenvalues
and then the coefficients in front of our highest degree are
our eigenvalues to the power of that highest degree
and the augmented side of our matrix is the value
of those different eigenvalues
so our goal will be to try to solve this system well actually our goal is a little
bit easier than that the statement of the theorem says that there exists a polynomial q
that satisfies the equation q of a equals f of a and so all we really have to do is show that such
a polynomial exists so we don't have to solve this solving it is what is q so given a matrix a
what is what is q the what is that polynomial q we're just trying to show that one exists
in other words what we want to do is answer the question does a solution to this system exist
and if we want to know a solution exists
if well if we can solve this system right and one criteria that allows us to solve this system
is that if this matrix here which is an n by an n minus what is this an n by n matrix right
it's an n by n matrix and if this matrix is invertible and when is the matrix invertible
if the determinant of this matrix is non-zero so solution exists if the determinant of this matrix
which is called a van der man matrix
if this determinant is non-zero
so what we're going to do is it's going to be a little bit of a brute force method but we will
find one way to compute the determinant of this matrix and therefore show whether or not it's zero
and see if we can answer our problem whenever we have a problem with arbitrary n it's a little
bit difficult to see what the pattern is without doing an example so I think it's good to try out
a simple example or at least somewhat simpler by computing the determinant of the same matrix
but where n equals let's say three so we have a three by three
and we want to compute this determinant and we want to compute it in such a way
so that we can use some of the ideas for computing this determinant and abstract it to that more
general case now this isn't the most simplest way to do such a thing but it's one way and I'm sure
there are many many other ways to compute this determinant some of which may be certainly
more clever than the approach that we'll take so we're going to do this by essentially row reduction
and for the first step we're going to get rid of the ones underneath the top left one and by just
subtracting the first row from those so if we do that that doesn't change the determinant
and we get the top row is left alone
and then the rows below it look like zero zero lambda two minus one
lambda three minus one and this becomes lambda three cubed minus lambda one cubed
sorry squared and lambda two squared minus lambda one squared
now when we lambda two minus lambda one is actually a common factor in this second row
because this becomes lambda two plus lambda one when we pull that out and this is lambda three plus
lambda one so when we distribute out we get lambda two minus lambda one lambda three minus lambda one
times the determinant of what's left over which is one lambda one lambda one squared
zero one zero one lambda
one plus lambda two lambda one plus lambda three
and this happened because the determinant remember when you take the determinant and you multiply
any row or any column by a number you can distribute out that one number for that one column
in this determinant you can think of the volume if you scale one side of the room by a factor
and another side of the room by a different factor then the determinant is computing the area
and you scale by both of those but for each side you only distribute one of them
so now we're looking at this and we want to compute the determinant of this
now of course what's left over is a two by two so it's very easy to compute the determinant
but if we wanted to have an inductive proof if we did a similar calculation here for a larger
matrix what we would have is lambda one through lambda one to the n minus first power up here
and we have a much larger matrix which isn't very easy to compute the determinant of
by some explicit formula it's sort of complicated to write so what we want to do is we want to
think of how to compute this maybe more conceptually and what we can do is notice that lambda one
appears here in each of these two terms and if we multiply the second column by lambda one
and subtract what happens is this cancels the lambda one cancels the lambda one cancels
and you're only left with lambda two and lambda three and you also don't change the determinant
because you're taking one column and adding it to another so this is also equal to
the determinant of what's left over after you do that subtraction
this is zero zero one zero one and then just lambda two and lambda three left over
well you can even do something even a little bit more simpler now now you have a one here
you can multiply this by lambda one to get rid of that so i'm not even going to write that whole
step out we can just erase this
and put a zero here and now here's the amazing part what's left over
after you perform these operations is another van der man matrix on the bottom right corner
and we can continue this process now because the determinant of this because this is a one
is equal to the determinant of this so we've reduced our problem
from an n by n matrix to an n minus one by n minus one matrix of the same form
and if we keep going down further up until maybe this step or even
further than that then we would find out what the determinant of this matrix is
so if we did that procedure again of course you can compute the determinant of a two by two no
problem but if you did that procedure again subtract you get a zero here move that over
you end up getting lambda two minus lambda three it's already
of that it already breaks up like that pretty easily and you get lambda two minus lambda three
that pops out so what you end up getting is the product of i and j let's say i is less than j
and j is less than or equal to three and i is greater than or equal to one
of lambda j minus lambda i so you actually get the product of the differences of all of these
different eigenvalues and because we're assuming that the eigenvalues are distinct all of these
numbers are not zero therefore this is not equal to zero and so we automatically know that the
determinant of this matrix is non-zero so we can make a guess that the determinant of that more
general matrix of that more general van der man matrix
is exactly the product of the differences of all of the eigenvalues
and therefore is not zero if they're distinct
and we can prove this by induction we already know what happens when n equals one
or when n equals two and not even n equals three and so what we can do is if we assume
that this formula is true for n and go to n plus one then what we want to do is reduce that problem
to this one and show that those numbers factor out and then then we can apply our induction
hypothesis and prove that this formula holds more generally
and the way we do that is very similar to this so i'll put a question mark here
and i'll write what this equals by doing this first step which was here sorry this first step
in subtracting the first row from all of the rows below it but we end up getting is
the determinant of and here we have a bunch of zeros below the ones so we have one
and i'll write two rows just so we see more of the pattern uh this is a zero sorry zero
lambda one and then this is lambda two minus lambda one and this is all the way down to lambda
n minus lambda one all the way up to and let me write two additional terms here so this is going
to be lambda n minus two lambda one n minus one now this is lambda two to the n minus
tooth power minus lambda one to the n minus tooth power
and here we have lambda n minus one minus sorry two minus one
that's a one
okay
now at this point we can follow a similar procedure by pulling out a lambda two minus
lambda one from each of the terms but then we would have to figure out what is lambda two
to some power minus lambda one to that same power divided by lambda two minus lambda one
we could do that and factor it out by using um polynomial division find out what the corresponding
factors are but maybe that's not the best way to do it another option although that method of course
you know teaches you a lot about how to do polynomial division in case you haven't seen it
before it's it's quite nice but maybe there's another easier way similar to what we did over here
and what we did here was we took the second last column and we multiplied it by lambda one
and we took the difference here we could have also done that in this step it just might have been a
little bit it might have looked a little bit more complicated because of the higher powers
but let's try to do that anyway if we multiply the second last column by lambda one
from the the last column the power here will be n minus one which will match this one
and these two terms will cancel and you'll just get zero what happens to this term if you multiply
this by lambda one so let's write this out so we have lambda two n minus one minus lambda one to the
n minus one minus multiply this whole term by lambda one that becomes a plus lambda one to the n
minus one and then what's left over is minus lambda one lambda two to the n minus two these two
terms conveniently cancel and what you're left over with is lambda two appears the highest
common factor is lambda two to the n minus two so we can pull that out
and what's left over after we pull that out is lambda two minus lambda one
and therefore we can much more easily see that this factors out
after we do this subtraction now we've done imagine we've done that for the last column here
now we have this second last column which still has all of these complicated terms
but what does this term before it look like lambda one to the n minus three and then it's lambda
two to the n minus three minus lambda one to the n minus three so you can just see it's of the form
n minus j and if we multiply this by lambda one and subtract it well these two terms will cancel
and a similar thing will happen here it's just that the power will now be not
lambda two to the n minus two but lambda two to the n minus three after we take this difference
and so if we keep going in this direction taking all of those successive differences
we will be left over with so this determinant equals
the product of lambda j minus lambda one and j goes from two to n
and we're left over with the determinant of a smaller van der man matrix which looks like one
zero zero and this term is one and it's all the way one's all the way down let me write
just the first and last ones we also have zeros here up to the last term
now what is this term here it's lambda two to the n minus two now
all the way down to lambda n to the n minus two
and if we assume the induction hypothesis then we know that the determinant here is the product of
lambda let me use a different letter k and l so k minus l where k is greater than strictly
greater than l and l runs from this time two to n and n and nk so we end up getting
after all of this work by using that induction hypothesis we get that this is this expression right
here and in particular this says that our determinant is non-zero so we can compute the
inverse of this matrix if we wanted to now that we have all of this set up we can prove our main
theorem which remember said that given any diagonalizable matrix a there exists and a function f on
its set of eigenvalues there exists a polynomial q such that q of a equals f of a
and so far based on the facts that we just proved we know there exists a polynomial
q such that q of lambda i equals f of lambda i for all of the eigenvalues
of that matrix therefore
if we compute f of d which was defined to be f of lambda one f of lambda n
of our diagonal matrix d then this is the same exact thing as q of lambda one
q of lambda n with zero everywhere else by this result we can find a single polynomial q that
satisfies this but this is exactly the same thing as q of d well why is that well if we
write our diagonal matrix d out and we apply the polynomial q to it right so let's just see why this
is true if we take our diagonal matrix and then we plug in our polynomial so we had what was it
it was a zero times the identity n by n matrix this is what um if we view q as a polynomial
and we plug in the formula for q of d this is by definition of a matrix applied to a polynomial
sorry a polynomial um with input a matrix plus a one d plus a two d squared plus a n minus one
d to the n minus one and we know what this looks like as a matrix this is the identity
it looks like a zero all along the diagonals
and zero everywhere else this is a one times lambda one all the way down to a one times lambda
to the n lambda n and then here we have plus a two d squared now d squared since d is the
diagonal matrix is just lambda i squared in each of the diagonal terms so it's a two lambda one
squared all the way down to a two lambda n squared and similarly for all of the other terms
up until this last one then what happens when you add all of these matrices together well you get
a zero on the top left term you get a zero plus a one lambda one plus a two lambda one squared
plus dot dot dot a n one minus lambda one to the n minus one that's exactly what q of lambda one is
and similarly for all of the other terms so this justifies why this equality holds
and of course q of any matrix is defined similarly so in particular q of a equals a zero times the
identity plus a one times a plus a two times a squared and so on so now let's show that f of a
equals q of a now f of a by definition of f of a is p times f of the diagonal matrix
times p inverse where p is the matrix of eigenvectors corresponding to those eigenvalues
is a matrix of eigenvectors now f of d by this calculation is also q applied to d
and so that equation is true by what we just showed now we know what q of d looks like it
looks like this and we also know what happens when we distribute p throughout so we get something
that looks like a zero p times p inverse plus a one p d p inverse all the way up to
a n minus one p d to the n minus one p inverse that's just what that looks like when you distribute
p and p inverse on both sides now this is a and what is this expression and likewise for all of the
terms in between well let's just let's just look at what happens if we if we set f n is like three
or something like that or maybe even two is enough so let's look at this term
p d squared p inverse so p d squared p inverse also equals p times d times d times p inverse
and because p and p inverse are well inverses of each other we can plug in a p inverse p
between these two d's and this gives us p d p inverse times p d p inverse again
and this is just a and this is just a so we get a squared therefore when we actually write out what
all of these things equal we get a zero p p inverse plus a one which is the identity sorry
this is the identity matrix and this is a plus a two a squared plus all the way up to a n minus one
a to the n minus one and this is the definition of q of a so this shows us that that theorem
is true so this has an interesting corollary
so let a be diagonalizable
and let b be any square matrix of the same size
and suppose that they satisfy
the fact that when we multiply them in any order they're equal to each other
then
f of a
b equals b f of a for all functions
that are defined on the eigenvalues of a
and how do we prove this
well because a is diagonalizable then f of a equals q of a for some polynomial
q and because it's a polynomial if we replace this expression with q of a times b
so if we have q of a times b this is a polynomial in a and each of the terms look like a to the
jth power times b right so you have a to the jth power times b now a to the jth power means
you write the matrix a j times and if you have a b on one side you can use this to move each of
those a's one over at a time you can move them over one at a time therefore a j a to the jth
times b equals b times a to the jth therefore it's immediate that this equals b times q of a
and it immediately solves this problem because q of a equals f of a
and the interesting thing about this is that b can be any matrix whatsoever and a only has to be
diagonalizable for this to be true so hopefully this is an interesting fact namely that given any
function at at least that's defined on the set of eigenvalues of a it could be defined on a larger set
of the subset of the complex numbers but at the very least if it's defined
on those eigenvalues then we can always find a polynomial
for which when we apply that function which could be completely wild such such as the
logarithm or something like that then there's a polynomial that gives us the same value for
that matrix if we apply that polynomial to the matrix versus if we apply the function to that
matrix and a lot of this has to do with the fact that we're working with finite dimensional matrices
one of the interesting things about linear algebra is what happens when your matrices
become of infinite order and then this really becomes a much more subtle issue and clearly the
method that we've used should probably break down for instance we're not working with polynomials
anymore and a lot of this is explored for instance in functional analysis and spectral theory
and the functional calculus for such operators
in these next few videos we'll learn about affine subspaces affine combinations and affine
transformations which are very slight generalizations of linear transformations as we'll see
so the first definition that we'll need is what an affine combination of vectors is
so but to do that we'll recall what a linear combination is so a linear combination
of vectors v1 through vk and rn is a combination of the form
lambda 1 v1 so we add up all our vectors with some weights and these weights
will take to be real numbers
so that's what a linear combination is
and closely related to this an affine combination
of these same vectors
is a linear combination
and for short I may often write just using the summation notation
oops let's call this not k but j and this goes from j equals 1 to k
such that the sum of these coefficients is equal to 1
so it's basically a linear combination but we have an additional constraint on the coefficients
so for example when k equals 2 we have two vectors let's say v1 and v2
then every such affine combination is of the form t v2 plus 1 minus t v1
where t is a real number and you can look at what this says let's say these two vectors are
different let's say v1 is here and v2 is here then at t equals 0 so this right this is describing
the set of all such combinations and when t equals 0 this gives me v1 so at t equals 0
I'm here and when t equals 1 I'm at v2 and as you vary t over the set of real numbers
you get all the points along the straight line through v1 and v2 this is very different than
the set of all linear combinations of v1 and v2 because if let's say the zero vector were here
then v1 would be this corresponding vector v2 would be this corresponding vector and all
linear combinations of these two vectors is actually the plane obtained from v1 and v2
that's what the span of these two vectors are but all affine combinations is just this line
and so just like we can define the span of vectors we can also define the affine span of vectors so
the affine span of the vectors v1 through vk is and we denote it by aff
and it's defined to be the set of all affine combinations so the set of all
lambda j vj such that all of the lambda j's are in r and the sum of them equals 1
so let's look at another example where we take three vectors so let's say v1 v2 v3
and let's just be concrete and let's say we're in r3 so that we can visualize this a little
bit better so there are several cases that we can take just like for linear combinations for
instance if one of these vectors was a linear combination of the other then the span of this
would be a plane and if all of them are scalar multiples of each other then the span is a line
and if they are all the zero vector then we just get the zero vector and if they're all
linearly independent then we get all of r3 there are many different cases depending on the
relationships between v1 through v3 same thing happens for affine span in the sense that it
depends on how these vectors are related so let's look at three possible cases so case one let's say
v1 v2 and v3 are not collinear so this means that all these three points don't lie on the same line
so maybe they look something like this like for instance you can take the unit vectors
e1 e2 and e3 in r3 then the affine span of these three vectors is equal to the two-dimensional plane
containing these vectors
and it's not so immediately obvious that that's what happens but let's just
think about this if we take v1 and v2 then it includes the affine span
of these two vectors which means we have this line through these two vectors is in our affine
span and likewise the line through v2 and v3 is here likewise the line v1 through v3 is here
and now that we have all of these lines in here we can also take affine combinations of these points
so you can take for instance the affine combination of this point with this point which gives us this
line this point with this point which gives us this line and you can see by taking all such
combinations all such affine combinations of these three vectors we can actually get any point in the
plane that contains these three points in case two let's imagine that v1 v2 v3 are collinear
but
at least two
are distinct
so in this case so i'm assuming that at least two so either the possibilities are something like
they're all different but they lie on the same line in which case the affine span
of these three points is equal to the straight line through those two points uh those three points
or the other cases the affine span if two of them happen to coincide then we just have two points
but i'm assuming that they're collinear and at least two are distinct so we also get the
straight line through those two points and the final case case three
is when all those vectors are exactly the same vector
and when this happens we only have a single point and all affine combinations of a single point
is just that point itself so these are some of the basic constructions that you can do with
vectors besides just taking linear combinations you can also take affine combinations there's
yet another type which we won't discuss is if you require that the sum of these coefficients adds
up to one but they're also not just real numbers but they're strictly non negative so they have
to be at least zero and that's called a convex combination which is a closely related idea
and in the case of these three vectors for instance it would be the triangle
whose three vertices are those three vectors that we had here and in this case if we took
convex combinations it would be the interval between these two farthest end points and in this
case we would have the same situation as we had here where we would just get a single point
a common question that we ask given a set of vectors is if we have another vector when is that
vector in the span of those vectors and this shows up for instance if we solve a homogeneous
linear system and we have a bunch of solutions that we know are actually solving that system
but let's say we don't know exactly what that system is we just know we have this collection of
solutions and if somebody hands us another vector then we can ask is that vector a definitely a
solution of the system that we have and in this case since we don't know the system we can't plug
in that vector to check instead what we have to do is check if that vector is in the span of the
vectors that we have already if that vector is in the span of the vectors that we already have
then that vector is definitely a solution but it doesn't tell us that if it's not in the span of
those vectors then it's not a solution because we might not have had a set of vectors that span the
solution set but at the very least it gives us a criteria for guaranteeing that if that vector is
in the span it's definitely a solution and likewise you can ask well if I have a bunch of vectors
that I happen to know solve an inhomogeneous equation and somebody hands me another vector
is there a similar criteria and there is and that involves the notion of affine span which
you talked about in the last video so the question that we could ask is given vectors v1 through vk
and another vector u in rn when is u in the affine span of these vectors v1 through vk
now in order for us to solve this problem then we have to be able to write u as a linear combination
of v1 through vk right but because it's an affine combination we have an additional constraint
on what these coefficients could be and that constraint is that lambda 1 plus lambda k
equals 1 which is also a linear system in the unknowns lambda 1 through lambda k
and therefore if we want to solve this system this question is equivalent to
the following one which is is the augmented matrix where we take our vectors v1 through vk
augmented with the vector u but in addition augment this further by one additional row
stating that one equals so now this is the number one equals one dot dot dot one
let me write this one so it's clear so this vector is just denoting the fact that it could
have several entries so we have an additional row in our augmented matrix and the question is
this consistent so this is actually how we would solve such a problem and
how does it show up in solving inhomogeneous systems we'll get to that after we talk about
what an affine subspace is and the fact that the solution set of an inhomogeneous system
is an affine subspace so for this let's just briefly recall
a vector subspace i'll put vector usually in parentheses but a vector subspace
of rn is a first of all a subset let's call it v
such that three conditions hold now there are many equivalent ways to define such a thing
but this one seems pretty concise and simple and the first condition is that the zero vector is in v
the second condition is that if you take a vector in v and you scale it by any number
then that scalar multiple is also in v so lambda v is in v provided that
the vector v was in v to begin with and lambda is a real number and three
the third condition is that if i take any two vectors in v then the sum of them are in v
so let's write u plus v is in v for all pairs u and v that are already in v
and this is what a vector subspace is
now this definition of a vector space is a little bit algebraic it's telling us
when certain vectors are in v and we can have a little bit more of a geometric
interpretation of what a vector subspace is
by using affine combinations so equivalently v satisfies
which means that if v satisfies the following conditions i'm about to write then it satisfies
this one and conversely let's call it instead of i and two so let's use i because the first
one's the same the zero vector is in v and the second condition which is sort of a combination
of these two is that t u plus one minus t v is in v for all t in real numbers and for all
u and v in v now this is exactly a linear combination of the vectors u and v so if i take
two vectors u and v inside of v then this affine combination is describing the set of all points
along the straight line through those two vectors so this is saying that a subspace
can also be described as a plane that contains the zero vector and plane could mean hyper plane
and this is because we always have the straight line through any two points in our subspace
now the fact that we've written it this way allows us to define an affine subspace
in a much more closely related fashion to this definition because for an affine subspace we'll
only be able to combine combine vectors in an affine way so we define an affine subspace
is a subset a of r n such that and now we drop this first condition so all we require is that
affine combinations of two vectors are always inside so t u plus one minus t v are in v for all
same conditions as here and you can ask well maybe an affine subspace should be if i take any
collection of points inside of it then the affine span of those points is inside of v and that
actually follows from this condition and the usual properties of scalar multiplication and vector
spaces and how you add them so the main example that we want to illustrate is the solution set
of any linear system ax equals b this is just notation for a linear system where b
is a vector in r m and a is an m by n matrix
so the solution set of this is an affine subspace
of r n
now the solution set of an inhomogeneous system is not a vector subspace because in general
zero is not a solution in fact when zero is a solution then it exactly is a subspace and
when zero is not a solution we get this more general notion of an affine subspace
and it's a fact
that affine subspaces
are translates
of vector subspaces
and what do i mean by that
a is an affine subspace
if and only if there exists a vector v in r n such that if i take the subs
if i take this affine subspace a and subtract v from it now what this means is the set of all
vectors of the form u minus v where u is in a
if this subset of r n is a subspace
in this sense is a vector subspace
in fact
we can use any vector inside of a to translate it to the origin so in fact v
will be a vector in a in fact any vector in a will make this a vector subspace
so the picture for this is actually really nice
i guess i shouldn't have called it a because i called this linear system a that may be
potentially confusing so maybe let's call this script a so let me use a script a here
and fortunately the letter a was only used in this one example but let me write it like this here
so it's the same so there's no conflicting notation
okay so here's our affine subspace a and if we take any vector in here let's call it u no let's
call it v so v points from zero up to where that vector is and if we take this vector and
we subtract it then v minus itself will be zero so i know that this plane is going to contain the
zero vector and so here we have a minus v and no matter which v we picked right if we picked another
one let's say we pick this vector right here let's call this one u then if we translate that
u minus itself is zero so we also get this plane back as well
and so a good application of this
of this sort of mathematical object is if the vector xp p for particular is a solution
to ax equals b for some linear system like in the previous example
then the solution set meaning all the solutions of ax equals b
is as we know the particular solution plus the homogeneous solution set so it's a set of all
sums of particular solutions with homogeneous solutions so axp solves the system this and
ax homogeneous solves the associated homogeneous system so if a represented the solution set of
an inhomogeneous system and a minus v represents the solution set of a homogeneous system then all
we have to do is pick one of these solutions and then all of these solutions and then take that
solution and translate it by that vector which was a particular solution of the inhomogeneous system
just as we can define linear transformations which are functions that take linear combinations
to linear combinations we can also define affine transformations and the idea is that
they take affine combinations to affine combinations which translates geometrically to
it takes lines or hyperplanes to other lines and hyperplanes as well so the definition
of an affine transformation is exactly that
an affine transformation in this case from
Rn to Rm is a function first and foremost and I will write my arrows as usual
from right to left so it's a function let's call it s such that
s of lambda u plus 1 minus lambda v is equal to lambda s of u plus 1 minus lambda s of v
for all u and v in Rn
and for all lambda in R and it's a consequence of this definition
that if we take any affine transfer if we take any affine combination of vectors
then s of that affine combination is going to be
the affine combination of s applied to each of those vectors
this is a little less obvious than it is if you take linear transformations
and you show that it follows from the assumptions of a linear transformation
that it takes linear combinations to linear combinations
and the reason it's a little bit slightly more challenging
is that if you apply this in a binary fashion right if you take two vectors u and v so you
think of this as a function from let's say r cross r to the n cross r to the n to r to the m
then in order to apply this here you have to put parentheses in an appropriate place
but in order to have an affine combination with the appropriate parentheses you have to be a
little bit careful about what your resulting coefficients are and it's not so easy to see
how to do that but it can be done
and here's the example that I really like to think of when comparing linear transformations
to affine transformations and things you might have seen from a while back
not in my lectures but in your early learnings of math perhaps
so if we take the usual equation of the form y of x equals mx plus b
where m and b are both real numbers and x is a variable and y is the function of x
then this is an affine
transformation from r to r because it takes a real number r x and it gives us another real number
and it's linear
if and only if b equals zero linear in the sense of being a linear transformation
so this will help you perhaps relate the difference between an affine transformation
and a linear one and we'll later talk about a theorem that relates the two exactly together
in fact we'll state that theorem now so the theorem says the following are equivalent for a function
now we're just describing a function
and these conditions are that s is affine is an affine transformation
so i'm not assuming any linearity this is just an ordinary function so s is affine
if i take the function s and subtract s of zero from it so if i take s minus s of zero
now this is a function in the sense that if i take any x the function associated to this is
defined by s of x minus s is s of zero so this is also a function from r into r m if this is linear
and c
there exists an m by n matrix
m and a vector b
in r m such that s of x equals mx plus b
and the reason i mentioned this example is precisely because of this theorem
because it allows us to relate linear transfer affine transformations to transformations that
we may have seen a long time ago and i personally think it's instructive to prove this theorem
to get a feeling for how affine combinations work so let's actually prove it and we'll prove this
by proving a implies b implies c implies a
so for the first part of this proof we're going to define we i don't want to keep writing s minus s
of zero so we're going to define l to be this function s minus s of zero
and the goal is to prove that this function is linear so we have to check the associated
conditions for linearity and before we do that let's just establish that if we apply zero to l
if we apply l to zero then we get exactly zero because this is s of zero minus s of zero
so definitely preserve zero and we know that this doesn't give us a sufficient condition for
linearity but it's definitely necessary so second if we take a coefficient lambda any real number
lambda and if we take a vector u that's inside of our n then by this definition this is s of
lambda u minus s of zero
and this is an interesting combination of lambda u and zero this also equals s of lambda u
plus one minus lambda of the zero vector right the zero vector is in the domain of s
and so i can multiply by any number and i still get zero and now
the interesting thing about this is that this is an affine combination of the vectors u and zero
so that's what this term is and this just comes along for the ride because s is affine i can take
these coefficients out
and this is also an affine combination of itself so i can write
minus lambda s of zero minus one minus lambda s of zero
and so what do we have we have lambda of s of u in parentheses minus s of zero which is exactly
l of u and these two terms cancel so we're left over with lambda l of u when we're done with this
calculation so it's linear in this it's the first condition of linearity is proven
and the second condition is if we take a linear combination
this also has to go to a linear combination as well so let's just use the definition this is
s of u plus v minus s of zero and now let's draw a picture here because this is going to help
let's say we have the vector u here and the vector v here and this is the zero vector
now the vector u plus v is somewhere here
now can we express u plus v as some convenient affine combination of vectors for which we know
what s does to those vectors well if we extend u so we take combinations of u and combinations of v
then u plus v can be written as an affine combination of some multiple of u and some
multiple of v in fact it can be written like that in many ways all i have to do is pick any
point here and draw the straight line through this point and u plus v and then find out what that
vector is or we can take a simple shortcut and just notice that if we multiply this by two
this by two then those two points two u and two v are on the same line that goes are on the line
that goes through u plus v and how do i know that well if i take half of this and half of this i get
exactly this and half and half is an affine combination so this equals s of one half two u
plus one half two v minus s of zero and because this is an affine combination we have one half
s of two u plus one half s of two v and now we can also subtract half of s of zero here
minus one half s of zero again and now one half is a common factor here so this gives us
one half l of two u plus one half l of two v but by the thing we just proved we know that we can
pull out scalars from l so this gives us l of u plus l of v and this together proves the linearity
so this is the proof that a implies b but if we have an affine transformation we subtract
by what it applies to when you plug in zero then we get a linear transformation now the rest of the
proof is actually not bad afterwards because for b implies c if we have a linear transformation
we already know we have a matrix corresponding to it so because l is linear
we get an m by m matrix
such that l of x equals m of x
equals m times x for all x in the domain of s which is r n
so set b to be equal to s of zero and when we make when we set that to be that then since l is s
minus s of zero then we take s equals l plus s zero which is b then we get y then we get the
equation of the form s of x equals m x plus b so that that's what how b implies c and then if we
have c to imply a this is much much it's very similar to these kinds of calculations of taking
affine combinations if we take s of like let's say lambda u plus one minus lambda v plug that in
here we know m acts in a linear way this is a matrix we apply matrix multiplication distributivity
associativity of all these properties of addition of vectors and scalar multiplication of vectors
in r m and we get that s is affine from this assumption so these three conditions are equivalent
for any function from r n to r m that characterize what it means for transformation to be affine
as we know functions can be composed provided that the domains and co-domains of these functions
match up similarly affine transformations compose and the composition is affine in an
analogous way to how linear combinations are composed and the resulting composition is also
linear so we have a fact and this fact is that the composition
of two affine transformations s and t
is also affine
and because it's affine and we know that each of these transformations can be written in the
form of mx plus b for some appropriate matrices and appropriate vectors b we can ask what is the
resulting matrix for what is what are the resulting matrices and vectors for the composition of two
affine transformations so let's write s of x as mx plus b and t of y as nx plus ny plus c
and let's just be careful about composing these so if we take the composition
s composed with t and we apply a vector y then this by definition is s applied to t of y
and we know that t is of this form
so we get ny plus c
and this equals m times the input of this function which is ny
plus c plus the associated b oh this should be a plus from the transformation s
and if you distribute this all out we get mn
times y
plus mc plus b so the associated matrix that we get is actually just the multiplication of
the matrices that we started with and the associated vector b is some interesting combination
of the original vectors b and c but also with the matrix m and in particular
if s from same setup rm rn to rm is invertible
and we wrote our decomposition like this then we could ask what are the matrices and vectors
associated to the inverse of this matrix and that is exactly
so s inverse let's write of y just because we're changing the
codomains with the the domains we get the inverse of m plus well rather minus
m inverse of the vector b and why does this work well if you just take s for instance and
you apply it to this result we know what this combination looks like we get m applied to this
term which gives us just y back m applied to this term which gives us negative b but we have a plus
b and those two cancel so just like the composition of linear transformations need not commute
similarly the composition of affine transformations need not commute so let's look at an example
and a common affine transformation is leave everything alone just translate by some vector
so let's just keep things very simple and let's assume that we translate by the vector
1 0 so we shift everything along the x axis
in r2 so we shift everything along the x axis so let's say the vector let's draw a smiley face here
this smiley face transforms under this transformation let's say smiley face is it
contained in the unit box so i have to make this a little bit bigger and it gets translated along the
x axis in the positive direction so let's call this transformation t
another transformation that we can look at let's call this one s
is rotation by 90 degrees so when we rotate the face looks something like this
and then we can ask what happens when we apply s and t in that order or if we apply t then s
and what are the matrices and vectors associated to these transformations let's actually answer
that question first so t of any vector x equals well let's just translate so it says leave everything
in the plane alone so that's the matrix corresponding to the identity and shift by the unit vector
in the x direction so i call that e1 so remember e1 equals the vector 1 0 and s of x
is the transformation that rotates by 90 degrees so i'm going to write that in matrix form
because rotation by 90 degrees is 0 negative 1 1 0 apply to the vector x and the b here is 0
because this is an actually this is actually a linear transformation so what happens when we
compose these in different orders so let's just think about this imagine you translate first
and then you rotate this rotation is occurring about the origin so when we apply t first and
then we apply s again we're rotating this picture by 90 degrees with respect to this origin so this
face is actually going to be further out than it would have been if we applied the transfer if we
apply the rotation initially and then translated you can already see the big difference between
these two pictures so if we apply first t and s apply to this picture let's start with our initial
configuration that what happens after you apply this will first you rotate and then you translate
so this translates everything to something that looks like this but if instead we applied s
after t to the same initial configuration well first we would translate and then we would rotate
by 90 degrees that would look much much different so if i were to draw this as a unit grid
that face would now be in this box rotated by 90 degrees so it looks something like that
so now let's just check the math out to make sure that this is consistent with these geometric
interpretations so if we apply t after s to any vector x
what do we get well t says first translate then rotate so we end up translating by x
then rotating because we do matrix multiplication and the resulting vector b is just e1 so we get
rotation applied to x plus e1 which is exactly what we expected from our picture here if we did it
in the other order well in that case first we translate and then we rotate and when we rotate
we not only apply the rotation to our initial vector x but we also apply the rotation to the
vector e1 and e1 gets rotated by a 90 degree rotation to the vector e2 so in this case we get
this instead so and this is consistent with this picture because if we rotate first our face ends
up somewhere here like in this picture and then how do we get from this picture to this one we
translate up by a unit vector by the unit vector e2 the next few videos are going to be a sort of
combination of probability theory and matrix algebra and we'll start by talking about finite
sets and stochastic matrices or what i call stochastic maps and we'll try to get through a lot of
interesting topics so first i just want to make sure that we have all these definitions at hand
and the first one that i want to make is a probability measure
and for simplicity we will be working with finite sets all the time so a probability measure on x
or here x is a finite set is a function that takes every element of x and it gives me a number
and that number is between 0 and 1 and the sum of these numbers
when i sum over all elements in x and let me just set notation that when i apply this probability
measure to x instead of writing p of x i will write p subscript x so such that the sum of these
numbers equals 1 and a stochastic map is something very similar to this
ah and let me even set some more notation the set of all probability measures
on x is denoted by px
so a stochastic map from x to y so another finite set is a function
from x to probability measures on y let's call that f and
we're going to introduce a convenient notation for such stochastic maps
so first let's explain a convenient notation for how to write f so if we take an element x
and we apply it we'll get a probability measure on y for now let's just call this f of x
because this is a probability measure it takes an element y and y and gives me a number between
0 and 1 so this takes an element y and maps it to f of x of y now it's a little bit annoying to
write something like this and potentially confusing so instead of writing this we will write f subscript
y x and the reason we write the y on the left is because we will end up in y and x on the right
because we started in x we'll see why this is convenient in a moment when we talk about composition
of stochastic maps and we'll also introduce graphical notation for this
instead of writing a map from x to py we will replace this by a map from x to y
but we'll use slightly different notation for our arrows and we'll make them squiggly arrows like this
and the reason we want to do this is because there's a very nice example of a stochastic map
if we have a function so if x to y is a function
this actually gives us a natural stochastic map
and just for this example we'll call it delta f oops these should be squiggly arrows now
so delta f to y which sends an element x to a probability measure on y and what should that
probability measure be well if i take let's call this delta f for now if i take an element in y
and i plug in our initial element x so again we're using this notation here then this is defined to be
the chronic or delta so if we take the element x
apply f to it we know what that is because we have a function already
and then we plug in y so visually how do i think of something like this well a stochastic map is
telling us if we start off in x let me draw the arrows backwards for a moment then it takes an
element in x and it spreads that element out over y by giving us a probability distribution
on y but if we already have a function then we know where that element x goes it goes to
a specific element which we call f of x and therefore it does give us a probability distribution
and that probability distribution is one when we evaluated at f of x and zero everywhere else
so i think of this as a deterministic process in some sense because we know given an input
we know exactly what the output would will be with 100 probability
so we notice that there's this close relationship between functions and stochastic maps in fact
functions are special kinds of stochastic maps and instead of writing delta f all the time we'll
simply write x f and we will think of this as a stochastic map but we'll write it as a straight
arrow another example
there is a one-to-one correspondence between
stochastic maps
from a single element set into another finite set x so this is going to be my notation for
a set containing a single element which i'm just calling a bullet and probability measures
on x why is that well if i have a stochastic map i apply an element of it i apply it to
an element of the domain and that gives me a probability measure on x but this only has one
element so i only get one probability measure so in general a stochastic map is you can think of
it as a family of probability measures indexed by the domain of that stochastic map
stochastic maps define conditional probabilities
or at least some kind of restrictive notion of conditional probabilities
and the reason is because
f y x you can think of this as the probability of y occurring given that x has occurred
and you can if you know if you have a definition of conditional probability
and you are looking at single element events then this definition coincides with the one
you're thinking of for finite sets and again single element events but if you're not then
we're going to think of this as our notion of a conditional probability so for being very concrete
let's take x to be the set whose elements are so pick your favorite supermarket and
let's say there's a good sale at that supermarket
and let me think of that as one element of this set x and the other element is going to be
a not great sale or a not good sale at that same supermarket so two elements
and let y be
the elements that state whether I go to the supermarket this week or you go or whatever
or I don't go so I go to the supermarket let's say this week or something like that or I don't go
and let's say if there's a good sale
let's say the probability right because I might have a lot of food stocked in my pantry I may or
may not go to the grocery store this week but if there's a good sale maybe there's a good chance
that I'll go let's say there's a 90% chance that I'll go
and if there isn't a good sale well it might be that I still need to get food
so there's still going to be some chance that I go but perhaps it'll be less I'll be less
enticed to go to that supermarket this week let's just say that there's a 60% chance I'll go
and with this information we can define a stochastic map from x to y
so this actually defines the stochastic map and we'll come back to this
in several examples that we'll look at later on because it's a nice simple example
and the reason you can figure out what the rest of this is is just by using probabilities
because if there is a good sale the chance that I go is 90% then there's a 10% chance I won't go
and conversely if there isn't a good sale then there's a 40% chance I don't go so that
defines a stochastic map
just like with functions we can compose stochastic maps as well
but this is going to have a really nice picture so I rather give that its own
video and we'll talk about compositions in a moment all right so if we have two finite sets
rather three finite sets x y and z and a stochastic maps between them
in such a way so that the codomain of f lines up with the domain of g and I really mean source
and target here because again if I really think of x as a function it's a map from x to probability
measures on y but the domain of g is not probability measures on y it's y itself
so it's really better to think of this a little bit categorically where I'm thinking of the target
of f and the source of g so given this given stochastic maps
we can define a composition of these two and before I write down the formula let's think about
how we would do this so here's x here's y here's z
what we want to define is an ocean of composition which is determined by if I give
if you give me an element in x and you give me an element in z I want to know given x what is
the probability that z occurs and there's an intermediary y here so the way that you get that
is well I look at all the elements of y and I look at given x what is the probability of that element
y occurring let's call this let's say that this is the element y then this is f y x so given x
the probability that y occurs and going from y what's the probability that z occurs that also
has a probability which is g z y and so the probability of given x the probability of z
given x is taking all of these probabilities by varying y and multiplying the corresponding ones
when they match up and then adding them all so this is defined to be the sum over all elements in y
with their respective probabilities g z y f y x so this is what the composition of
stochastic maps
is
and now you can see why I chose this notation earlier of writing our subscripts in this
particular order because if I think of these as matrices indexed by the elements of these sets
that we have then this ends up just being matrix multiplication so sometimes these are also called
stochastic matrices but I'm going to stick to the calling them stochastic maps
so let's look at some interesting special cases of this definition so first let's look at the
special case where x is replaced by a single element set y is a set x and g is a function not
just a stochastic map so let's take this special example so let's take y a function f and a probability
measure on x so first of all what is a probability measure on x look like well if I think of x as
a set so let's draw some of the elements of x here
let's say here we have nine elements a probability measure sort of gives me a size
to each of these elements so I can think of these as water droplets each with a specific size
namely the volume
so this is sort of what a generic x looks like with a probability measure on it
and the sum of the volumes of these water droplets is equal to one
now if I have a function f from y to x then the composite here gives me a probability measure
on y what is that probability measure well if I just use the definition
p followed by f and I evaluated at y this is equal to just straight from the definition
we know that this is the sum over all elements in x
of the function on the left which is f but f is a function so we know that it corresponds to the
direct delta the chronicle delta f y f of x with the probability measure p x now if I
substitute what this looks like this says this only gives me a non-zero contribution
if f of x equals y in other words if y is in the image of f of x is in the image of f
and it comes from some x so if we look at the inverse image of y that's going to give me a
bunch of elements and that's the only case where this gives me a non-zero contribution
and what that means is that this breaks down into the sum of all elements x in the inverse image
of y so here we have the sum of all the p x's that are in the inverse image of an element y
so let's look at this element y here the inverse image of this under a map f
so let's imagine that f identifies all the elements that are in the vertical direction
so right because a function f might not be one to one so it might identify some of the elements
and that's why I've drawn it this way it takes these four elements and gives me the single output y
and these two elements gives me another output and what this condition says is that the probability
here is the sum of these probabilities in other words the volume of this water droplet is the
sum of the volumes of those water droplets likewise here in order to make the volume somewhat
geometrically similar to these this would be the resulting volume after we apply this function f
and here maybe it's this big so this gives us a nice picture of what compositions like this look like
it essentially says that we take these water droplets and then we combine them and when you
combine the associated water droplets their volumes add as another example let's go back
to our previous situation in fact let me write that example here because it's a little bit it can fit
here so in this case we had that set x to be there's a good sale at the supermarket this week
and there's not a good sale and the set y is I go to the supermarket or I don't now what if we
happen to know the statistics or the probabilities of whether there is a good sale or not at the
specific supermarket given that specific week so you compile all of your data over the course of
a year for instance and you just ignore the seasons you ignore the months you just look at when is
there a good sale for whatever definition of good you might have for for you and let's just say
that the probability of a good sale is maybe only 30 percent so roughly 30 percent of the time there's
a good sale on a given week and therefore the probability of a not so good sale is 70 percent
and so you might ask what is the probability that I go to the supermarket question mark
so that's the end of the statement so all we know is that if there's a good sale we already
know what those probabilities are I think they were 90 percent and if there is a good sale and 60
percent if there isn't a good sale because I still need to eat and if we happen to know the
probability that there's a good sale and therefore the probability of there being a bad sale
or not good rather is 70 percent then you could still ask what is the probability that I actually
end up going and that's where this composition comes in where instead of having an f like this
we instead have our f from our previous example but we also know the probabilities of whether or
not there's a good sale so it's a slight generalization of this example and therefore the
probability that I go to the supermarket is equal to and in this case I'm going to take
the probability that there is a good sale times the probability that I go given that there's a
good sale plus so let me actually write that one down so that's 90 percent times 30 percent the
probability that there's a good sale times the probability that I go plus the probability that
there isn't a good sale but I still go and the probability that I go given that there isn't a
good sale is 60 and the probability that there is not a good sale is 70 and the resulting probability
that I go is 69 so given those statistics we still know that if I just chose an arbitrary week in
the year, there's a 69% chance that I'll go to the supermarket that week.
So now let's look at another example.
And this example, again, will come back to this perhaps a few more times.
So now let's look at another example.
This one may seem a little bit abstract, but it's a very useful one anyway.
So let's take the diagonal map from x to x cross x.
What this does is it takes an element x.
So far we've talked about stochastic maps and how to compose them and how to view ordinary
functions as specific examples of stochastic maps.
What we'll do now is describe how to take the product of two stochastic maps in a way
that generalizes the usual notion of the Cartesian product of two functions.
So given stochastic maps f and g, we can form their product
and it's another stochastic map that essentially takes the product
of the associated probabilities point-wise.
So it's determined by the formula f cross g.
Now remember what our notation is for each element in the domain.
We get a probability distribution on the co-domain and that probability distribution
is determined by what it does to points because we're working with finite sets.
So that probability distribution is determined by the value of our initial input with our
output and it's just the product of the associated probabilities from f and g.
And let's just check that, make sure that this coincides with our usual definition
of Cartesian product when we specify that these stochastic maps correspond to functions.
So if f and g are functions, or how I think of them as being deterministic, then this
product is given by, well we know what happens when these are functions, then we use the
delta and this is x prime f of x while this is delta y prime g of y.
And this is nothing but, it's the same exact thing as requiring that x prime coincides
with f of x simultaneously as g, as y prime coincides with g of y.
And this is the usual way we think about the Cartesian product because it says what is
the value of f cross g at x, y, well it's f of x, g of y and this is exactly what encompasses
that idea.
And all of the structure that we've defined so far, the idea of the stochastic map, its
definition, how it composes, the fact that functions are special cases, in particular
if the identity function is a special kind of stochastic map, it turns out that composition
is associative, the identity is an identity for the composition for any finite set, and
this Cartesian product also satisfies the type of associativity condition and together
all of this, all of these data give the collection of finite sets with stochastic maps and this
associated product, it gives it the structure of a symmetric minoidal category.
But there's another thing that we haven't yet discussed, which is a notion of almost
everywhere equivalence, or in other words an almost surely notion of equivalence.
And this essentially takes care of when probabilities happen to vanish, and when such a thing happens
we can have a notion of equivalence between functions when their probabilities are equal
versus when they're zero.
And so we get a very natural definition of what it means for two stochastic maps, very
similar to the way we define almost everywhere equivalence for functions.
So given two stochastic maps, so I'm using different notation than what's up here, so
given two stochastic maps and a probability measure on X, we say that F is P almost everywhere
equivalent to G if and only if.
And the way we define equivalence is that these stochastic maps agree everywhere outside
a set of measure zero, so outside of events that have probability zero.
So the way we write that is if and only if the probability of the set of points on the
domains of these corresponding stochastic maps where these two stochastic maps differ
is equal to zero.
Now what does this inequality mean?
Now F of X and G of X are both probability measures on Y, so when I write that they're
not equal, that means F subscript YX is not equal to G subscript YX for some Y.
So this is a very intuitive notion of almost everywhere equivalence.
There's another sort of diagrammatic way that you can encompass these definitions as well.
So I'll write this as a theorem, but we'll use this idea later on.
So it turns out that given F, G, and P as in this definition, F is almost everywhere
equivalent to G, so this is the notation that we'll use.
Given only if the diagram, now this is going to be a little bit of an interesting diagram,
so we're going to produce our probability on X.
We're going to duplicate X using the map that we introduced earlier, and on each of these
two factors we will apply our associated maps F and G on their corresponding terms.
So in this case we'll have the identity on X here, cross F, and here it's the identity
on X cross G, where this product is the one that we just defined.
So if and only if this diagram commutes.
So first of all, this is a very interesting statement.
It tells us that this notion of almost everywhere equivalence can be encompassed in some diagrammatic
form.
And secondly, if we ever discuss these in these videos, we'll find out that this is very
closer related to a notion of almost everywhere equivalence in a non-commutative setting where
we replace our finite sets and stochastic maps with certain kinds of C star algebras
and completely positive unital maps.
And these sorts of objects are relevant in quantum information theory.
Okay, so before we prove this, we'll have a little bit of a lemma just to make the calculation
a little bit easier, and that lemma is the composition of two maps, of two stochastic
maps that are of this form.
So if I have a map phi from U into V and a map psi from U into V, and I pre-compose
with this diagonal map, then this composition is given by the formula.
So we take phi cross psi composed with this diagonal, and how do we evaluate this?
Well, the domain has a U and the codomain has a V and a W, so we can evaluate it V comma
W and U.
And the claim is that this is given by taking just the product of these where two of the
points happen to match up.
So this is phi VU psi WU for all VU and W.
So the proof of this is pretty easy once we have all of our definitions in place, and
the left-hand side of this expression by definition of the composition and by using the definition
of the product is equal to a sum, and what's our intermediary step?
It's the sum over U cross U, and U cross U, therefore we have to sum over two elements.
We're already using a letter U, so we're going to have to introduce U prime and U double
prime, for instance.
So it's going to be U prime, U double prime, both elements in U, and the product here is
going to be phi VU prime psi WU double prime, because that's the second coordinate, and
this is, as we recall, the direct, the chronic or delta twice using the coordinate U and
U double prime and U prime.
So it's U prime U, delta, U double prime U.
So this gives us two delta functions, and we have a summation over those, and as a result,
these two letters coincide.
So this is exactly the right-hand side, quick and simple proof.
So this is the proof of the lemma, and then the proof of the theorem, we'll now talk about
Bayes' theorem, and first we'll state the theorem, given a probability distribution
on X and a conditional probability from X to Y, call it F, so it's the stochastic map.
There exists another map going in the opposite direction, let's call it G, such that the
diagram, now the diagram looks a little bit complicated, but it's not too bad.
When we write out the equation, we'll see exactly what it means.
So here we'll have P, and here notice we can compose P with F to get another probability
distribution on Y, and we'll call that Q.
So we have our probability distribution on X on one on Y, we duplicate X, we duplicate
Y, this almost reminds me of the definition of AE equivalence, X cross Y, and here we
will apply the only maps we can, and to go from X to Y we apply F, and to go from Y to
X we apply G. So the statement is that this diagram commutes, and furthermore, for any
other stochastic map that also goes in the opposite direction, let's call it G prime,
satisfying this, then these two maps are almost everywhere equivalent, and in the sense of
our probability Q. So this is the formal statement of Bayes' theorem, and if you've seen Bayes'
theorem in a different context, this may seem totally strange, but let's just see exactly
what it says when we look at the composition of all of these arrows.
We've actually computed expressions just like this.
If you remember this left hand side when we were doing the notion of almost everywhere
equivalence in that diagrammatic perspective, we computed something, I think it may have
been exactly this expression actually.
So commutativity says that F, Y, X times P, X equals, and if we did that same calculation
but on the right hand side of this diagram it looks almost the same, it's just that the
G is on the other side, nevertheless, we still get G, X, Y, Q, Y, and this holds for
all X, Y. Of course X is in X and Y is in Y. Now let's introduce some notation to see
how to understand this.
Let's define P of Y given X, so this is the probability of Y given X to be exactly FYX.
That's exactly what F means, F is the stochastic map, it says it's not corresponding to a function,
it says if you give me X, I will give you Y with some probability, the probability is
exactly FYX, so that's exactly what this conditional probability is, and the probability
of X is just little PX, the conditional probability of X given Y, now this is going in the opposite
direction, it says if you give me Y, what's the probability of X occurring?
That's exactly GXY, and finally the probability of Y occurring is QY, and so if we write down
these expressions, the commutativity of this diagram says nothing but the probability of
X given Y times the probability of X is equal to the probability of X given Y times the
probability of Y, which is perhaps a slightly more familiar form of Bayes theorem, at least
when your events are singleton sets.
With the appropriate definitions, you can also extend this, you can look at what this
diagram means because these are corresponding to probability measures, and you can also
define a notion of conditional probability where you replace this point with a subset,
and you can use the probabilities on your corresponding spaces to make sense of what
this means when X is replaced by some event A perhaps, and Y is replaced by some event
B, nevertheless the same equation still follows from commutativity of this diagram.
So let's look at our earlier example just to see what this is saying, and how to interpret
it in sort of a real life situation.
So if you remember, we had X and Y, two sets with each of which contains two elements,
and X corresponded to the set where there's a good sale, and the other element was not
a great sale, not a good sale, and Y is the set containing the elements, I go to the store,
the grocery store, or I don't go.
And we also had probabilities on each of these spaces, and we also knew the probabilities
that if there's a good sale, how likely am I to go, that was 90%, so 90% if good, I go
with 90% probability, and if not good, then I still go, but with 60% chance, and likewise
the other probabilities are given by the fact that it's one minus this, one minus this,
and we also know the probability of there actually being a good sale, so we know what
P of good sale is, and the probability is 30%, and the probability of a not good sale
is therefore 70%.
So we have all of this information.
Now imagine you're in that store this particular week, and you happen to see me there.
So in that case, you happen to know I'm already at the store, then you can ask, what is the
probability that there's a good sale this week, given the information that you see,
and knowing this information as well.
So initially, you also know the statistics that says, if I look over the entire year,
the probability that there's a good sale is 30%, but you also know that I'm more likely
to go to the store if there is a sale.
So if you see me, then there might be a better chance that there's a sale this week.
And how do you figure that out?
Well, if we look at this expression, and we compare these two sides, then we can say
that F corresponds to the if there's a good sale versus if there's not a good sale, how
likely am I to go or not as FYX.
And the probability that there's a good sale is PX, and if we wanted to know, so let's
say G is on the other side, so G of X given Y, so this says, if you see me at the store,
so here this element Y is, I'm at the store.
And X is, there's a good sale.
So if you see me at the store, what's the probability of there being a good sale?
And we divide that by QY, which we've already determined last time.
So QY was the probability that I went to the store.
And we know that that equals the sum of the product of the probability of if there's a
good sale, I go.
And if there's not a good sale, I go, multiplied by the corresponding probabilities corresponding
to here.
And we found that to be 69%.
So in this case, this equals 90%, 30%, divided by 69%.
And when you write out what this equals, it's roughly approximately equal to 39%.
So you've updated your hypothesis based on what you see.
And this is known as Bayesian inference or inversion, inversion.
And in fact, the map G constructed here, a G from Bayes theorem, is called a Bayesian
inverse of F.
And it would be a little bit inappropriate to say that it only depends on F, because
it also depends on your prior probability distribution P.
So this is an interesting reformulation of Bayes theorem that seems to be totally in
the language of category theory.
And it therefore makes it amenable to a wide range of techniques that could be used to
analyze and understand it, and perhaps even generalize this idea to other contexts.

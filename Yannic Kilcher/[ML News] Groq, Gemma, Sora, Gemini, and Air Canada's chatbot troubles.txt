Hello, everyone. Welcome to ML News.
We're going to take a quick, brief, tiny look of what happened in the last two
weeks in the world of machine learning, AI, and I guess, encompassing pretty much
everything nowadays.
So first, I've already mentioned this in a previous video a little bit, but Google
released Gemma. Gemma is, I guess, a variant of the name Gemini.
These are open models that are in smaller sizes than kind of the largest
language models. So they are two billion parameter models and seven billion
parameter models. These outperform respective Lama2 models at the same
sizes and even at a little bit bigger sizes. So they are quite
performant. They are available. They're openly accessible. You can use them
under some limited circumstances for commercial activity. And they do
release a technical report on how they have built these. Now, these are not
the same as like Gemini 1.5 with the one million token context length. These, I
believe, they're context sizes about 8,000 tokens. So they are in architecture
quite similar to Lama models. As I said, previously, this is essentially, I
believe, a marketing ploy from Google right here, releasing these models.
They're already topping the leaderboards. So all in all, very good development, I
think. Although all of this has been overshadowed last week. And if you've seen
my last video, you will know by people discovering that Gemini image
generation is a bit wacky when it comes to sort of bias correction and
representation of people. So straight up refusing to generate any white
people or any images of white people and things like that. One interesting
development. Again, watch that video if you haven't seen it. But one interesting
development here is that the person, this product lead from Google, who
essentially came out and said, Oh, we're sorry, we were made aware of some
historical inaccuracies, we will fix those has made their ex account private
apparently, or Twitter, whatever has made that account private. Now, it's totally
conceivable that douchebags have started sort of harassing the person or just
kind of bombing spamming and so on. So that's totally fine. But it was not
a good look like it was gaslighting in the highest degree. Like, Oh, yeah, let's
just say the problem is historical inaccuracies and not like the obvious
problem that was barely visible with the thing. So I just thought that was an
interesting development. Again, if you want to know more, watch that video. I
also found the development around this a little bit funny. So day later,
apparently, they refused to just generate images of people like just
German, I think I cannot generate images of people that was their hot fix
their their patch to say, well, no images of people at all until we fix this
problem here. Then saying, if you then said, I've seen you produce images of
people, it has answers. It is important to clarify that I have never been able
to generate images directly. I'm not sure it will be interesting to know what
the exact prompting behind this was and the changes being done here. Also not
said that this is true, right? This is just an LLM doing its LLM thing. But I
do find it interesting, this new world when software patches are essentially
sort of prompt changes. And then the interactions with those just make for
hilarious content. All right, Grock is all the rage now. Grock is a company
that is as far as I know, spun off from Google's TPU group, if I'm informed
correctly, and they have built a card that can serve language models really,
really, really quickly. So make long novel. So this is mixed trial eight times
seven B and you see it runs at like 532 tokens a second. So this is insane
speed. This allows for new use cases to be accessible by these language models.
And very, very cool. So this is really special hardware. I'm sure there's some
software tricks and algorithm tricks, but this is special hardware. Yeah, people
talking about this insane, insane speed of Grock. Grock says they have this LPU,
this language processing unit. So that's not a GPU. It's an LPU. And the
difference to something like an NVIDIA GPU is that they have a different kind
of memory. So here you can see latency and throughput. This is a benchmark
from a third party, and they had to extend their their axes here in order
just to accommodate how fast and how much throughput Grock has and how fast
it is. It's pretty insane. However, there is a trade off. As I said, they use
different memory than a regular GPU would use. And therefore, that makes it
such that each card only has a very small amount of memory. So you need a lot
of these cards in parallel to even serve one of these big models. Now you can
achieve massive throughput, obviously economies of scale kick in, but you can't
just get one of those cards and then serve a large model. And that's where
people quickly realized, hey, okay, it might not be the wonder weapon here. It
is very cool. But each chip only has about 200 megabytes of SRAM. And therefore,
you would need, I don't know, hundreds of cards in order just to serve this
mixed trial model that we've seen before. Again, with the higher throughput, it
might be totally worth it if you're a data center owner, but you see that the
graph on the top right here, that's grog. That's pretty insane. People calculate
that you need about 320 of these cards or two full racks to just serve a single
llama 70 B. And if you calculate the cost of these cards, then that'd be about
10 million US dollars. It's not the the end all's gonna solve everything. But it
is definitely, definitely very cool development in order to push language
model inference ahead. At the same time, Nvidia unveils EOS tech power up,
right? This is essentially pulling together a bunch of their DGX systems to
create a super duper computer. So 576 DGX H 100 systems wired together into one
computer. Each of these DGX system has eight H 100s, making for a whopping
4,608 H 100 GPUs note. Each of these puppies will cost you I don't know what
they cost right now, like 20 K or something like this or north of that. This
is massive. It's ranked number nine in top 500 supercomputers of the world with a
staggering, staggering 18.4 Exaflops fp8 performance. This website here, I found
pretty cool GPU list dot AI, it's by Andromeda AI. And it's essentially
Craigslist, but for GPUs, people rent out their GPU capacity. It's also as shady
as Craigslist, right? So that it's just a listing. And it just says, well, you'll
get fair metal access. And sometimes it says, okay, you get SSH access to it or
something like this. But essentially, it just allows you to contact these people
and then make out a deal of how you're going to use these GPUs. This seems fairly
large. So the common posting here, actually, there's a lot of H 100s here going
around. I'm not sure where people get these from. But sometimes, oh, there's 849
Okay, this may be more common. So Canada, Ethernet, one H 100 available. It's
a new Buntu VM. And you get minimum one week. So if you want some GPUs, and you
don't have super confidential data, because you are going to use other people's
hardware, this might be a good option to find some some good deals. Why it has an
interview with them as husband is on how far you get with scale, apparently, I guess
just on the future of AI. And if you read the interview, it's kind of a mix between
yeah, scale is great, we can do scale scale is awesome. Gemini is awesome. These
models are awesome. And but also scale only gets you so far, there needs to be
something else. So they, then it says my belief is to get to a GI, you're going to
need probably several more innovations as well as the maximum scale. There's no
light up in the scaling, we're not seeing an asymptote, yada, yada, yada, there's
still gains to be made. So my view is you've got to push the existing techniques
to see how far they go. But you're not going to get new capabilities like
planning or tool use or agent like behavior, just by scaling techniques. It's
not magically going to happen. It's very interesting, because I think that is a
current contention. It's very easy to say, oh, to get to a GI, you need something
else because, first of all, a GI isn't the defined term, and something else isn't
the defined term. So you can redefine these two terms as you wish. And then you
can always find something that's still wrong or in a way in which you're still
correct. If you do that for long enough, your name will become Gary Marcus. But
other than that, these are fairly more concise predictions saying, okay, you're
not going to get planning or tool use or agent like behavior. They're not super
defined, but they are. And we're already seeing tool use, for example, being built
into these large language models and get better with scaling. So it will be very
interesting to see whether Demis turns out to be ultimately correct on his
predictions, or whether one or the other of these things will be available just by
scaling language model and kind of training them on tool use data and so on.
Tom's Hardware writes legendary chip architect Jim Keller responds to Sam
Altman's plan to raise $7 trillion to make AI chip saying, I can do it for less
than $1 trillion. We've gone off the rails. So Jim Keller, apparently legendary CPU
developer, now working at the company that makes chips themselves, they claims
that he could do it for a lot less. Yes, I guess. I don't know, as soon as you go
into like money that's way beyond the current total market value of chips, I
feel many claims can be made. It will be interesting going forward to see kind of
who takes the lead in chip development, how that's going to be playing out. In any
case, I'm not sure if bickering about $1 trillion, $2 trillion, $7 trillion is going
to make that big of a difference. From one legendary person to another legendary
person and legendary here, spelled with a capital L, AI may destroy humankind in just
two years, experts says, of course, Elisir Yudkowski saying, if you put me to a
wall and forced me to put probabilities on things, I have a sense that our current
timeline looks more like five years than 50 years, could be two years, could be 10,
well, could be anything. Like with the trillions, it is absolutely useless to
make these speculations and then saying things about Terminator like apocalypse
and matrix hellscape. The difficulty is people do not realize we have a shred of
a chance that humanity survives. Oh, yes, of course, of course, Yudkowski has, I
think, retracted statements on bombing the centers. Like that would be useful. In
any case, read this as you would read like a comic book for entertainment. Yeah, I
feel like that. It's at least makes you giggle. Otherwise, this serves no purpose
at all. Sora continues to dominate headlines, video generation model by open
AI. We've talked a little bit about this in the last news episode, but this can
create clips, single shot clips up to one minute, I believe. And they look pretty,
pretty awesome, I have to say, and more and more kind of examples come out of
Sora creating pictures, creating clips, open AI marketing department in full
gear. No, you don't have access to this model yet. A select few have access to
this model. Not you. You are just a pleb. You're not the chosen person. So Marvel
at other people using the cool thing and open AI is marketing department having
tight control over exactly which things go out to the public and which things
don't. What is interesting is here's an example of Sora scaling with compute. So
essentially saying the more compute they throw into one of these generations, the
better, better quote unquote, the more realistic, I guess it gets. So base
compute for X, however, like they've also completely stopped to give us any sense
of the scale, the absolute scale of things. So for now, it's just like base
compute, however much that is, and then for X compute and then 16 X compute. Yeah,
in any case, what we can infer from that is that there is an iterative process
very probably to determine one of these samples. So it's not like single forward
pass of anything, but iterative iterative process like you would be used to from
diffusion doing many, many steps across a span of time to refine and refine and
refine the output. What's also pretty cool are demonstrations of changing things
like this being a base video and not only can sort of generate things, but also
kind of generate things according to some input, like some input video. So in
case here, people changing the surroundings of the car or the car itself,
like the vibe of the video and so on, while keeping the general motion, I guess,
and the general concept clear. So I think that's pretty cool. Make it go underwater.
Yeah, why not? Look at that. Or that nice rainbow road. Keep the video the same,
but make it winter animation style, charcoal drawing. Yeah, make sure to be
black and white, not exactly, but close. It is one of those things where it's
actually black and white, but your eyes trick you. But I think I'm seeing color.
Wait. Yeah, no, this is definitely color. Actually, not so sure now. Okay, no,
this is definitely red. This is definitely red. The back lights. Yeah, it's not fully
black and white. Cyberpunk medieval. Very nice. They had drones following carts in
medieval times. Also the horse legs, they look. Yeah, why not? Dinosaurs, pixel art.
So many cool things about Sora keep coming out. And also many cool things about
Gemini 1.5 Pro keep coming out, especially obviously the insanely large context
size of Gemini 1.5 Pro. People feeding very long things inside of it and see
whether it can handle the long context, an entire code base, and then instructing
it to code something based on top of that. I think this is probably going to be
one of the best applications for something like this. If you have very long,
yeah, something like a code base or a reference documentation or something
like this, like the important parts of that would fit into a million tokens. And
being able to sort of cross reference things inside of that and generate
based on that is probably a very good use case. I know they can retrieve well
across the one million tokens kind of like point to individual things if they
need to retrieve them. But it will still be interesting to research how
performant it actually is when you put more and more and more stuff into that
context. My personal estimate would still be that putting less things into the
context is more beneficial will make stuff more accurate. Or what I can also
imagine is that they trained it in such a way that they could have achieved
better performance on small context compared to large context, but they
traded it off to have sort of equally performing, but worse performance
across the entire context length. Not yet clear, but it will be interesting to
see this pretty cool feeding an entire short movie into into this. So what
Gemini will do is it will take the movie, split it into frames and then
essentially use the frames as tokens or tokenize the frames. And you can fit
pretty long. You can see here 44 minutes and seven seconds video. You can fit
that into the context size of Gemini 1.5 pro because it can also consume
images. And Matt Schumer here says when straight from full movie to a summary
in seconds, no transcription, no intermediate steps, just visual tokens
to summary. Now I've seen other people have pointed out that the summaries it
makes aren't always super duper accurate or well done, but it's still pretty
impressive. And it speaks to what I said before, right? The main question is
going to be what are the dynamics and the characteristics of performance
across this entire context window. And currently UC Berkeley coming out with
a paper that's titled World Model on Million Length Video and Language
with Ring Attention. This is an actual research paper that is very concurrent,
as I said, to Gemini 1.5 pro doing retrieval experiments across very long
context with what's called ring attention. If you're interested, we can
make an entire video on ring attention that is in the makings. So keep
looking for that. But it's a cool new technique. It is going to be some sort
of approximation to attention. Like, it's not the fact that we can now scale
the classic transformers across this huge one million token context size. So
it's a trick. People have come up with many, many different tricks of sort of
doing long attention. And this one seems quite promising. Phil Wang, also known
as Lucid Raines, already has an implementation on ring attention up even
though the paper is super duper new. Yeah, what's interesting to see in the
read me of this repository is Phil saying, I will be running out of
sponsorship early next month. If you'd like to see that this project gets
completed, sponsor me or I will be leaving the open source scene for employment.
So just wanted to bring this to people's attention. By that, I mainly mean
people like companies. CTV News Vancouver writes Air Canada's chatbot gave
BC man the wrong information. Now the airline has to pay for the mistake.
Apparently a person went on the chatbot for Air Canada. That's kind of
powered by a large language model. And they went there looking for a very
specific questions about it's called bereavement rates. These are reduced
fares provided in the event someone needs to travel to due to the death of an
immediate family member. So the chatbot, this person has lost family member.
I wanted to do air travel due to that and inquired about, you know, cheaper
rights or specific fares, specific prices for this situation. And the chatbot
gave them wrong information saying that he could claim those even after the
fact. And when he wanted to do that, the customer support people said, Nope,
that's not possible. You're not getting your money. Now the question is who is
responsible? And courts say that in fact, Air Canada is responsible for the
things their chatbot said and has to actually comply with what the chatbot
promised Air Canada try to try to push the responsibility. Air Canada suggests
that the chatbot is a separate legal entity that is responsible for its own
actions. What? What? Oh, no, the piece of software that we actively
deployed on our website is a separate legal entity. Yeah, no, no, I get that as
a lawyer, you will have to argue. And in this case, it was probably like the last
remaining thing that you could even conceivably argue. But it's so ridiculous.
No way. No way. If you deploy a piece of software, you are responsible for
what that software does, not if you program it, not if you make the open
source library that's then part of it, if you deploy it, and it interacts with
your customers, and then it promises stuff to your customers, then you are
responsible. And that's the same with every other piece of software as well.
There's absolutely no difference of whether this is an LLM chatbot or
anything else that executes code. It's always the same. And it's pretty funny
that Air Canada trying to weasel their way out of this. So no, Air Canada must
honor refund policy invented by the airlines chatbot. So obviously,
companies are trying to alleviate costs on their customer success operations.
In this case, they may want to calculate if they don't incur more costs due to
stuff that's invented by it could be a strategy. Like I think the damage is
here to be paid were about $600. And then I think about $20 in tax, it says
it somewhere. In addition, the airline was ordered to pay 36.14 in
pre-judgment interest, interest, not tax, and $125 in fees. And probably the
lawyers grabbed 10, 20, 50K out of the people here. So all in all, who
succeeded the lawyers, lawyers should be fans of LLMs, like the amount of
litigation and the amount of contracting. And so they have to do just because
people want to use or have used or mistakenly used LLMs is going to be
staggering, staggering. In any case, this was more about the principle, I guess,
than about the money. But it could be a thought, you know, as like just let an
LLM do your customer success. And if they promise something that doesn't exist,
you just pay it, it'll be like $600 in this case. Maybe that's worth it. Maybe
the savings and not having to hire more people is totally worth it for these
companies. I don't know. It'll be interesting to think of a future. I think
right now everyone's trying to guardrail everything because they feel, okay, our
customer success operation should continue to be as is, like there is a
completely defined things, okay, here are the things we pay here are the things
we don't pay and so on. And you must adhere to that cannot promise anything
else and so on. What if the mentality around that changes? It will just be
like, okay, here's a set of guidelines. We know the thing is going to hallucinate
every now and then. And when it does, we'll just sort of take it into account.
Like I feel there are still laws against customers abusing that. Like if I were to
go to Air Canada chatbot and kind of like prompt tack it into giving me stuff,
I'm pretty sure a court would side with Air Canada in that's essentially kind of
emotionally abusing a customer support rep until they promise like give me what I
want. But other than that, could be totally viable future and a fun future if
these things are not so strict. Cream car tweeting out xing out. I'm not sure what
this finally happened. The peer review journal article with what appeared to be
nonsensical AI generated images. So these this has become known as giant rat balls.
The pictures they look from afar like they could be in like a biology journal,
but they make no sense, right? That the right thing's mostly rat.
Yeah, we see that. So this article has pictures that were generated by mid journey.
Now, it is a bit interesting. It's a bit more interesting than that.
Scientists aghast at bizarre AI read with huge genitals, a peer reviewed article.
This is a fairly reputable journal where this is published. This is not just like a pay 5000
bucks and you will get published journal. This is a fairly reputable journal. Here is another
another picture. You see it rarely makes sense if you actually look closely. Second,
the images are created by mid journey and the authors acknowledge this in the paper.
So the authors say the images are generated by mid journey. Third, there were two reviewers
and one of the reviewers actually brought this up and currently also a reviewer. I'm not sure if
if it's the same reviewer or a different reviewer said I was only looking at the scientific content
of the work, right? And reviewed it based on that. And we have also statement from the reviewer
saying that they did raise concerns about the images. So they're saying the journal says an
investigation is currently being conducted. So this article in vice here details our investigation
revealed that one of the reviewers raised valid concerns about the figures and requested author
revisions. Even the reviewers saying okay, the figures need to be revised. The authors
failed to respond to these requests. We are investigating higher processes failed to
act on the lack of author compliance with the reviewers requirements. So it's a bit more
tricky than just all a bunch of researchers tried to get a fake paper through in the reviewers
didn't notice. It seems that the ultimate person who I guess multiple people here always contribute
to the things but ultimately it was the editors who didn't make sure that the authors actually
changed the things that the reviewers were asking them to change that made it ultimately go through
and being printed as a paper. I guess mistakes like this happen. It's also probably very common to
just kind of assume the authors will concur with what the reviewers request to change. I don't know
if they've even said yeah, okay, we'll change it or something like this. But it is an interesting
story and the meme of the giant rat balls will forever live on in our hearts. Andre Carpathi
said they he left open AI assuring that's not a result of drama or anything like this. It's just
a change in scenery saying that the last year in open AI was really great. The team is really
strong. The people are wonderful. The roadmap is exciting. And we all have a lot to look forward
to. He says my media plans is to work on my personal project and see what happens. And
immediately following this up with a video explanation two hours on tokenizers enlightening
the bizarre world of why reversing strings with LLMs is really, really difficult and
why different languages are give you different results and so on. So if you want to explore
so far, I believe a bit underexplored aspect of large language models definitely look into
Andre's tutorial on tokenizers. Very cool. And as Andre is very, very clear explanations,
you will know a lot more after this. Nature writes what the EU's tough AI law means for
research and chat GPT. The EU AI act is the world's first major legislation on artificial
intelligence and strictly regulates general purpose models. As you know, the EU AI act has
been in development for a number of years now and is now finally coming into effect rolling
out and so on. It's been changed quite a bit over the years. You know, even myself, I'm not entirely
sure what's in the current version and how much it's still going to change. But the approach is to
categorize broadly applications into risk categories and then to tie what you have to do
according to the risk category. The most risky things it would call the unacceptable risk.
And those are just banned, like not allowed to do, for example, those that use biometric data to
infer sensitive characteristics, such as people's sexual orientation. So is this banned? Also,
what's hilarious, there is like a limit for when you have to do something. And that is 10 to the 25
flops, completely arbitrary number that is going to be meaningless, probably even already before
the AI act has really rolled out. Finally, I could not make up worse advice for these policymakers
if I wanted to. It's like, okay, let's pick a completely arbitrary number and say here, here
is where we draw like what I believe that you can entirely, transparently see the lobbyists
being like, okay, what can we do that our competitors can't do? And let's like draw a nice line
between the two how it is the next three years. And we don't we don't care about any after that.
All right, unacceptable risk. Do you realize that a basic linear regression would fall under this
that EU effectively now bans drawing a straight line across a few data points, if those data
points happen to coincide with the data categories that are collected here, like this is the level
of dumbness, these kinds of laws come to. Yes, I know, I know, I am making sort of pulling it to
the extreme here. I know this is meant for super duper transformers informing these things and then
sitting in automated systems that make decisions about people's lives and so on. I see what the
fear is, but I doubt whether what they're trying to do what they're intending to do matches with
what the effect of this is going to be. And I still believe the effect is going to just be that
a more monopolization of bigger companies making it harder for newcomers making it harder to enter
this market and giving the governments more control over things, which they will probably
not do good things with just an opinion. Go here for AI launches. Aya Aya is an open source
massively multilingual large language model and the data set built over 101 different languages
all across the world. And this is one of the largest data set of instruction data that's around.
As I said, it's a data set and a large language model all at once. The data set is available.
The model is open access, whatever that means right now, I guess you can download the model
because there's a button that says download the model. Of your press found this on Reddit and I
found this to be really interesting. Regional prompting. This is a UI for the technique called
Gilgen and I've linked to the repository. Very cool to use and very exciting new things that
are possible. Aria Everyday Activities is a data set again released by Meta that depicts,
as you can see, everyday activities. So this has first person view data, location data and so on.
Meta is actually pushing the metaverse and data sets around that augmented reality and so on.
So collecting a lot of data they have, as you can see right here, rolling shutter RGB field of 110
degree field of view camera 150 degree field of view camera for slam and hand tracking infrared
illumination barometer magnetometer environmental sensors spatial microphones and so on and then
annotated data per frame eye tracking 3d trajectories. These data sets, they collect them to be quite
universal. So maybe don't want to use all of them at the same time, but they enable a lot of different
applications, which is very, very cool. Stability announces stable diffusion 3 a text to image model
using a diffusion transformer architecture for greatly improved performance in multi-subject
prompts image quality and spelling abilities. Not releasing anything. There is a wait list for
early preview. They say this is for gathering insights and improve its performance and safety
ahead of the open release. We've also come to know from stability that open release is going to mean
that you can use the model for researchy stuff. But if you want to use it for anything commercial,
you have to give them a bit of money. You can see a few examples here. Nice apple. Go big or go home.
The astronauts riding on things has become a bit of a meme. I mean, the quality is getting
absolutely insane with these text to image models. Alexa Gordic releasing Ugo LLM. This is a large
language model 7 billion parameter large language model for Balkan languages, Serbian, Bosnian and
Croatian languages. And you can find that on hugging face right now. Very, very cool. Open
math instruct by NVIDIA is a math instruction data set that you can freely use. Actually,
freely use might be an overstatement. There is a NVIDIA specific license to it. I'm not a lawyer.
I'm not going to tell you what this means. I personally think no legal advice. You can use it
freely. Again, not legal advice. This article from Interesting Engineering I found very cool.
It's a system that identifies drug combo problems. So interactions between different drugs,
specifically as they are transmitting the barrier in your gut. The problem is any researching any
drug and what it does is already super expensive. But then obviously every drug you add to the
regiment of available drugs means it could have interactions with all the other drugs that exist.
This system uses a combination of machine learning and actual models of transmissions,
models of receptor behavior in the gut to predict interactions between different drugs in terms of
their uptake in the gut. So I think that's very cool. I think the pushing into this direction,
we already saw this with various deep mind models of having some sort of expert modeling,
like some sort of actual model that is domain informed, that is expert informed in a scientific
domain, combine that with machine learning and use these two together in order to draw conclusions
is probably I want to say the next frontier, I feel like the frontier of will just throw a lot
of data at stuff and it will give us results. I think the low hanging fruit in that has probably
been taken already. And now it's really about the combination of expertise and machine learning
that is going to push ahead. So very, very cool, very excellent developments. Bloomberg writes
Reddit signs AI content licensing deal ahead of IPO. That being said, this is all person close to
the matter said large unnamed AI company, a lot of dollars involved about $60 million on an annualized
basis and yada, yada, yada. So this is all I guess they call it hearsay, right? But this is the
chatter right now that Reddit, obviously, recently read it has made headlines by sort of tapping all
of their API access and so on, not being really open anymore in to outside developers in a clear
move to protect their IP, which is users posting on Reddit. So that other you can't via API, go and
grab all that data. And now the second move is that they themselves are going to make use of that
data by licensing it out to other companies. Again, this is all just someone said someone
familiar with the matter, yada, yada, yada. But still Reddit realizes they sit on a treasure trove
of information that's already evident by people just Googling how to XYZ and then just add Reddit
to their Google search query, because they know usually they get okay answers on Reddit, which
has had the counter effect that now marketing representatives and so on will try to go and
sort of poison Reddit threads by giving, you know, nice looking answers that ultimately link back to
their product. Interesting dynamics. In any case, Reddit data may become a staple of one of the big
AI companies. So we'll soon have all kinds of AI redditors around. Isn't that a great future?
New Atlas writes the seeing eye dog v2.0 is shaping up as a game changer. This goes into the
details of strapping kind of assistive technologies on top of one of these four legged robots,
in order to help blind and seeing visually impaired people to move around safely, safely
passage from A to B and so on. The article discusses that the main limitation here is
actually the availability of service dogs in general like guide dogs. There are way too few
guide dogs around for all the visually impaired people. They are expensive. They are rare. They
need to be trained and so on. In this case, these robots obviously don't. So yeah, you can say they
take away the jobs of good, you know, hardworking guide dogs, which I guess, but from all I can
see here, actual guide dogs are still preferred to robot dogs. It's just that there aren't nearly
enough of them. So these robot dogs, they are shaping up to become very capable and can help
with a lot of things. So very cool developments. This paper, I found really cool always copilot
towards generalist computer agents with self improvement using agent like behavior, but
interacting with your operating system. So it can do some stuff on your computer by you just
prompting it to do it, opening applications, interacting with applications, even doing
kind of multi step things. I think this is one of the ways we're going to interact more with
computers in the future. Maybe I don't think like the keyboard and programming, you know,
using text and so on will ever go away, but probably kind of web browsing or simple things
like this could be automated like this. I find this to be a lot more understandable than just
voice prompts, like just being like Alex book a flight to XYZ, right? Like it's like, I find voice
and sound to be kind of a wonky interface for that. But if at the same time, someone shows me, look,
I'm now going to this website, I'm going to do this and that, I feel that is a much more
viable interface here. But then again, you could just click it yourself. So probably I find the
GitHub copilot to be an extremely good mode of interacting with an LLM. So if we transform,
transport that to the world of here, it would be that largely I operate the computer, but I could
tab complete like a lot of things. So like if there's a form to be filled out, yes, I know
browsers will support me already, but I could maybe just kind of a lot less tab complete. Or
if there is, I don't know, some standard interactions and website is just kind of tab complete that
away. I think that mode of interacting with computers, I'm looking forward to that a lot.
I'm not looking forward to like single prompt and then will magically go and do something for me.
I don't think that's going to be a thing of the near future. And I don't think you would be comfortable
with a system like that. Business Insider writes new report sheds light on Apple's upcoming AI
features that will rival Microsoft's copilot. Now further down, they say Microsoft's GitHub
copilot writing code. So it's not like the Microsoft Windows copilot or the Microsoft 365
copilot. There are too many copilates nowadays. It is the apparently the GitHub copilot that
Apple targets inside of its Xcode environment. So if you write Swift apps, if you write iPod and
iPhone apps, and maybe even what's called Mac OS apps, that might be really cool to have that
available. I do feel GitHub copilot does its job quite well for what it does for everything else
I've never programmed with. So I can't say that tech crunch rights and thropic takes steps to
prevent election misinformation. Yeah, sure. It's they're making a bit of PR. I feel like
they're they're using the opportunity of the election to be like, Oh, we have guardrails,
we have prompt shield, which I guess from shield cool, the technologies which relies on a combination
of AI detection models and rules. Sure, you have a regex, and you have some prompt that says if
the user asks for voting information, go to this site. I guess it's a good if I were a company,
I would try to use that as well. And lastly, AI comes to the world of beauty, as eyelash robot
uses artificial intelligence to place fake lashes. This details how this robot can place
fake eyelashes in a more precise way than humans could. And as far as I can see, it's also a bit
faster or cheaper or something like this. This is a purely mechanical task that so far humans did
by hand. And now a robot can do artificial intelligence is a bit of an overstatement,
like they use computer vision to detect where the eyelids and the corners of the eyes and so on
are, which is really cool. But then there is Oh, there is the for and there's the against and the
against is Oh, no, we have to be very careful about this. So there's potential risks. The device's
proximity to sensitive area could raise concerns about the risk of eye infections or allergic
reactions to the materials used in the lash extensions. I guess they just they had someone
someone just say we'll say anything bad about this like anything generic that's bad about this.
And this person was like, I guess you could be allergic to the materials. They're like,
Oh, yes, there is someone. There's also potential risks. I'm not, I'm not sure. I'm not sure I'm
buying that you decide for yourself. I feel having a machine do a purely mechanical task is fine.
It's not going to steal a lot of jobs, I guess. All good. I just found it funny that
the news article must have this structure. But here is something new that technology can do.
But there is also risks. With that being said, there's also risk that this video gets too long.
And with that, I'll finish it. Thank you for watching. Bye bye.

Hello, today we're going to look at RWKV, which in its own words is reinventing RNNs for the
transformer era. This is a very interesting project and very interesting model architecture,
because it has some properties of transformers. Notably, it's a model architecture that's very
scalable in terms of training, so you can stack it really deep and you can still train it. And also,
you can parallelize training. At the same time, it avoids the quadratic memory bottleneck that
transformers have by essentially being an RNN. It's kind of a recurrent neural network in the
sense that during inference, you can compute the output step by step and always have a constant
memory, because everything is put into a hidden state. We're going to look at how these two things
come together and what the trade-offs are between the two. The project is also very interesting,
because it's been largely developed by one person or by just very few people who have worked on this,
and then compare this to entire corporations that are pouring their human resources into
transformers. And still, the results are that this model in some cases, not in all cases,
but in some cases, can be very comparable in terms of performance with transformers,
with really big transformers. And as I said, it is scalable, which so far RNNs have been lacking.
We'll go over the paper, we'll go over the architecture and see what's actually happening
right here. I have some of my own thoughts and just opinions on this, and I hope you're with me.
But first, let me show you this. Fully Connected is a conference. It's by Weights and Biases,
it's a one-day event, and it has pretty cool speakers. So not only the co-founders of Weights
and Biases themselves, but the co-founder of Langchain is there, the co-founder of Kaggle is
there, Richard Soccer from u.com is there, Chip Hien is there, of Claypot. There's so many people
right here and so many cool speakers. And as I said, if you are in around the San Francisco area,
go give that event a visit if you want. I'm going to use, put a link and a promo code into
the description of this video that will make the tickets cheaper. So the tickets will be 49 bucks
instead of what you see right here. So that's going to be on June the 7th. Don't forget that,
June the 7th in San Francisco. It's an in-person event, has a max capacity, so grab it now.
That's it. Thanks to Weights and Biases for also giving me this opportunity and also giving this
to you. It's very cool. So RWKV, it stands for, let me not screw this up, receptive,
receptance, receptance. R is for receptance, W is for weight, K is for key, and V is for value.
These describe the different elements of the model architecture and we'll go through that,
we'll go through it. So what are we dealing with? We're dealing with a model that you can use for
lots of things, but we're mainly dealing with a model that in the instance it is outlined here
is for language modeling. By language modeling, we simply mean we have a piece of text, yada, yada,
yada, yada, and what we want the model to predict is the next set, the next tokens or the next word
in the text. So from here, predict this thing and from here, predict this thing and so on.
Transformers are usually used in this manner. Notably, I would stick the entire prefix here
into a big transformer and the transformer would spit out the next step. It will do that in a form
that we call causal attention, which essentially means that every piece in the text right here,
so every token can attend to all the other tokens that are before it. So all the other tokens that
come in front of it would have the, would be inputs to that token and using attention, that
results in a quadratic, in a quadratic requirement of compute and memory. Now, if you can see there,
the, if every token attends to its back, it's like t times t half minus one or something like this
interactions that need to be considered. So t squared in expectation and no, yes, maybe.
Yeah, that makes about sense. So transformers naturally have a limit because if you add one
token, then you always add a memory requirement of t and so very quickly that scales to be out of
proportion. So their power is traded off by the fact that they can only consider a limited set of
tokens at a time. Recurrent neural networks trade this off. Recurrent neural networks, if they have
to do something like this, if they have to have an input and then predict the next thing, what they'll
do is they'll just start here. Then they'll put something into a hidden state, like a little
memory. I'm going to represent that as a box. People have forgotten how RNNs work, I figured.
Like a few years ago, I could have talked about RNNs and every one of you would have
known what I'm talking about. And then transformers are kind of the new thing. And now it's more like
I have to say, I don't even have to mention stuff is a transformer. But I do have to
conversely explain RNNs. So it would put whatever it learns into a memory, like this hidden box,
this box right here is the memory. Then from that, it would consume the memory,
and it will consume this thing, and it would build a new memory. And then it will consume
that memory, and it will consume the next input, it would build a new memory. So it does this step
by step, and you can always, you can drop the old memory. So you can forget about the old memory,
because all you need, haha, pun intended, to go forward from here on out is that current memory.
So you do step by step, and you always save stuff into the memory, into the hidden state,
or whatever you want to call it. So RNNs have this really great property that they only require a
constant memory in order to do inference. However, the individual, the individual inference step,
for example, when we are here, we have that memory, we predict the next, the next token from it. So
we predict the next, this thing right here, we can only consider the memory and the previous token.
That's all, we cannot explicitly consider any token that is in the way back, because everything
goes through that hidden state. And that bottleneck has usually been one of the downfalls for RNNs,
there is a problem of vanishing gradients, there are a couple of other problems that you can't
just compress information into that one hidden state. Plus, RNNs have been notoriously hard to
train, because the inference always requires this step by step thing, which means you have to do
back propagation through time, which gets is part of the vanishing gradient problem, but also means
that you can't parallelize the training. In a transformer, I can input a token, oopsie,
a token sequence of 50 tokens. And that gives me 50 training examples, which I can train all at the
same time, because of the causal attention mask. In an RNN, if I have a sequence of 50 tokens,
I can still only train one token loss at a time, because I can't infer everything at the same time.
Or WKV is going to strike a trade off in the middle of those two things. And in a sense,
so people ask, or I asked, is this a transformer more? Is it an RNN more? And I've come to the
conclusion, it's a convnet. And I'm going to explain my reasoning. And they also refer to that,
by the way. It's not like I found out something great right here. But in the most basic sense,
you can think of this thing as a convolutional network across a one dimensional sequence of tokens.
That's going to be my statement, and we'll go through it. So they give a bit of an introduction
right here on what this means and what I essentially just said. Transformers scale
quadratically with sequence length RNNs exhibit linear scaling, which is very beneficial,
very advantageous. The RWKV model combines the efficient parallelizable training of transformers
with the efficient inference of RNNs. So it's like almost two modes between which you can switch
around. They say they have a linear attention mechanism. And as I said, they formulate the
model either as a transformer or as an RNN. That linear attention mechanism is something that
we're going to focus on in just a bit. Because it's not it's not their fault because people
have been doing it before them. But I think it really stretches the word attention and what
it means. I think it stretches that to like a point where I don't agree any more calling it
attention. But again, they're not the first people kind of doing that. So I'm not going to hold them
to account right here. They say this is the first non transformer architecture that to be scaled to
tens of billions of parameters. So one of the properties of this thing is really you can scale
it, you can scale it and you can train it in parallel, which also means you can pump a lot of
data into it. And that's very advantageous. And there is a lot to be said here about maybe, maybe
it's not that much the architecture we're dealing with, but more, you know, good models are simply
models with architectures that are scalable. So it's not maybe, right? That's a hypothesis,
like how much of the performance of something like GPT four is due to the fact that it is a
transformer. And how much is due to the fact just that is a scalable architecture, you can pump a
lot of data into. We don't know yet too well how those things exactly trade off. But there's good
argument to be made that, Hey, if you can find some other architecture, just scales really well,
then, you know, you might as well reach the same performance. This is a complexity, a asymptotic
complexity table on the different ways of doing attention mechanisms. So the transformer is that
classic mechanism. As we said, it needs quadratic time in T, big T is the sequence length of the
sequence we're processing. And then in terms of space, it needs a little bit more. But the leading
term is also this T squared right here. The D is the dimension, I believe, of the of the embeddings
of the hidden spaces, which is usually a constant factor across all of these things. There are
various trade offs like a reformer and so on per former. A lot of these, they do approximations to
the original transformer attention mechanism. And it's also notable to say that RWKV is not an
approximate attention or anything like this, it doesn't try to approximate the original attention,
it replaces it with a different mechanism of considering the past. So what does that mechanism
look like? Yeah, let's go into it. So maybe, yeah, this is smart. Let's first look at this here. If
you've been in deep learning and are old, like me, you remember this, this is an LSTM. This is a
long shorter memory cell from back when, before attention was a thing. So this was one of the
main ways people build RNNs, recurrent neural networks, that would actually somewhat avoid
that vanishing gradient problem and could learn to remember things for a long time. The idea
behind LSTMs is that you have two hidden states. If I'm correct,
am I correct? Yes. So you have two hidden states, this C and this H, H being the
real hidden state. And you have a lot of these gating mechanisms. So what the gating
mechanisms do, it's often represented like this here. This is an element-wise product.
So what you would do is you would get in a hidden state, you would get in an input,
you put the input through some sort of computation, neural networks, nonlinearities, yada, yada, yada,
and then you'd have some result right here, which would be a vector. And then the question is,
obviously, you can compute an output from that at this particular time step, but the question is,
how should the next hidden state look like? And the idea behind LSTMs and similar architectures is
that we're going to take the last hidden state, sorry, we're going to take the last hidden state
and we're going to update it. In a really basic RNN, we would just kind of replace the hidden state
or we would maybe add the two together and then for propagated. But what an LSTM does,
very interestingly, is introduces these gates. So what we'll do is we'll have something like a
forget gate. And the forget gate is simply a binary or not a binary, but say a obviously
continuous, but we can imagine it as a binary vector where just it's a mask,
values between zero and one. And wherever it's zero, there's an element wise multiplication,
wherever it's zero, this hidden state is going to get forgotten. And then the new state is going
to be updated at that particular point. So there is going to be a forget gate, there's also going
to be maybe a gate right here that tells which things of the hidden state to even remember,
right? That's also a binary vector, maybe only these things. And so the network itself can control
which parts of the information it wants to remember, which parts it wants to forget,
and which parts it wants to retain. And then the new hidden state is going to be sort of
an addition of these mask inputs. And then that goes on. In order to do that, there's a lot of
computation needed as you see right here. And in particular, I want to draw your attention
to one fact. The next hidden states, for example, if you take this H right here,
the next hidden state is always going to be some nonlinear function. For example, this is a sigmoid,
I believe it's a nonlinearity or a tan H or something like this of something here, like of
this CT, the CT in itself is a linear combination. But the linear combination of this, this in its
turn again is a nonlinearity, and then a linear combination of the things. And this ultimately
is the last hidden state. So you can see from the last hidden state to the next hidden state,
we pass at least two nonlinearities, right? And there you see this sequential stepwise nature
of things. It's always, it's always, you have to compute the step, and then you have to
compute the next step based on this step. And the outputs are nonlinearly related. So there is no way
you can like, jump a few ahead or take five steps together or something like this, because they're
nonlinear, it's not like linear functions, linear functions compute really easily, right?
Nonlinear functions that are stacked, where every next thing needs the first thing as an input,
those are really not parallelizable, not like aggregatable or anything like this. So that's
the problem with RNNs if they're formulated like this. They also go a little bit into the attention
mechanism, which I guess by now I don't have to explain too much anymore. There is a query and
a key matrix, both are produced from the data. So the data enters and three matrices are produced,
queries, keys and values will do an outer product between the queries and keys, which is defines
this quadratic interaction. So every token can attend to every other token. Sometimes you would
then mask this with the causal mask with the upper or lower triangular matrix, then build a softmax
across that. So the softmax essentially converts some values. So let's say your values are this
positive, negative, negative, positive, would convert that into a distribution. We can interpret
it as a probability distribution, or you can interpret it as essentially whatever you want,
but you can and it defines where to put the attention, right? It defines which things I
should look at like this one and this one a little bit. I guess that's why it's called attention,
because it defines the weights by which I aggregate information. So it dynamically
allocates attention to some of these values right here rather than others. That's what attention
means to me in the sense how it's described and that's why I said the term is stretched a bit far.
You can write attention in a different way and you can decompose it recurrently,
almost like an RNN. So you can decompose attention computation and this is done in parts to also get
around the memory bottleneck. However, you trade it off with computation, right? If you compute
these things in sequences, you have to compute them sequentially where you could just compute them
in parallel by doing a big outer product, matrix multiplication. So you do trade off time and memory
here. And by the way, you have to remember all of these things. You can sum them, I guess.
I mean, any matrix multiplication you can probably do that with. Never mind. You can decompose the
attention computation. Same one as above in a way like this. So I have the outer product of just
pair of keys and queries. I raise that to the, I have the exponential of that. That's part of
that softmax computation. Ultimately, I divide by the sum of these values. So that's the softmax
operator. And then I multiply the value at that particular location. So you can see here, this
part here defines a weight and V is the value. So it's a weighted sum of the values. Now we come
to attention free transformers, which is a piece of work that this paper takes a lot of inspiration
from attention free transformers. Try to go about this in the same way as attention, but they say,
hey, can we reformulate this, this formula up here, the one that we saw, and we reformulate this and
just turn it into something that doesn't need that whole quadratic memory,
shebang. And they come up and say, if we don't have to do the outer product up here,
this outer product, if we don't have to do these outer products, then that would
sort of mean that we don't have this token to token, every token can attend to every other token
interactions anymore, which means we don't have that quadratic thing anymore. And therefore,
we could save a lot of memory. So they replace the interactions between query and key,
they replace that. And they say, let's not even compute the query, we'll just compute a key for
each token. So we're now only producing matrices K and V, no queries. Instead, we'll have these Ws
right here. The Ws are learned. So W is a learned matrix. So not computed from the data,
but learned. And it just learns how tokens interact with each other. What does it mean?
It means that I just have a matrix that size t by t. And in there 1234512345. And in this matrix,
there's going to be number like seven, okay. And that means that the interaction that the weight,
the attention weight, essentially, of that token one has with respect to token two. So how much is
token one going to attend to token two, let's assume it's not causally masked, is seven. Okay,
that's that's that. It's the same for all the sequences. You all you just say, well, the first
token is always going to attend seven to the second token, all the same, it's one set of
learned parameters. And therefore, you can, this is just now a multiplication by a constant,
essentially. Now, that just defines a fixed attention, which is a bit too little. So the fixed
attention is not flexible enough before we had completely dynamic attention. And now we go to
completely fixed attention that is just learned across the whole data set. And the authors there
said wisely, hey, you know, it isn't it is the fact that depending on the current data point,
we might need to look back further, or we might need to change that attention pattern a little bit.
So they say, okay, how about we simply add the keys here. So the keys are values that are computed
from the data, right? So now the data can define essentially an offset to that. So maybe for one
data point, it's eight important, because it says token one should really look at token two.
And for another data point, the K is negative one. So this is six right here, that depresses that a
little bit. So there's a modulation in that attention pattern, there's one across the data set,
which is fixed. And then on top of that, there's a modulation given by each individual data point
to modulate that. However, the interaction is not multiplicative, as it is in the transformer
attention that you see up here, it's additive, which is in a sense, a lot less powerful, because
the multiplicative interaction really defines, you know, when two things are close and when two
things are far apart between keys and queries. Whereas here, we just modify a little bit that
fixed pattern right here. However, that fixed pattern can't take into account. So token one,
if it decides on its K value, it can't take into account what token two is. It simply says,
I'm the word cat, I should probably really look three words behind me. That's really important.
Whereas the original attention can decide, I'm the word cat. And I should probably really look
at words that relate to fish or fur, or, or sleeping, like those words, those kinds of words
really interest me. And that's how it would craft its query. And that's what it would be retrieved
from the keys of the other words. Whereas here, it can just say, well, I'm cat, I should probably
look three words behind me seems really good. I hope you can see how this is kind of less powerful
than the original attention. But it is more scalable. Again, you see what it defines is
essentially this part right here is a weight. So you have a weighted sum of the values. Lastly,
we have this paper right here. So it formulates the attention mechanism, it says,
yeah, but here, what we still need to do is we still need to learn that interaction matrix,
which crucially also means it's still limited by t, it's still limited by,
you know, sort of a fixed size, we can't we can't go bigger than that. And what they say now is,
how about we don't learn this matrix, all we learn is a vector, all we learn is the vector w.
And the vector w, it's the same for all. And it defines, so it defines its vector.
And it has the same dimensionality as the hidden dimensions of the hidden state. And it defines
for each dimension, it defines how much does the past matter. So for one dimension, it could say,
well, the past matters a lot. Therefore, that value is very high. And for the other dimension,
it could say, no, that value is very low, the past doesn't matter a lot. What happens if we
do that? Ultimately, we're going to do something like, again, something up here. So you can see
we have e, the exponential function of w t i plus k k i. So they now say, okay, we multiply this
by this term t minus i means how much back I'm looking. So if we wonder, we are token one,
how much do we attend to, let's go the other way around, we're token four, how much do we attend
to token number two? Okay, then we ask in which dimension, in the first dimension, oh, the first
dimension is really large. Therefore, we're going to attend a lot to the general past of dimension one.
Okay, so maybe that's this drop off. And then we look two tokens in the past, because the current
time step is four. And this i here is two. So how, which token are we, and which do we attend to,
and you can see it's minus. So this is getting bigger and bigger as you go further back in the
past. So it's essentially a linear drop off in the value of w and w itself can be big or small.
And these two things together define how important a past token is in that particular dimension. So
it's a multiplication of how far back is it, and how much is this dimension in general,
considering its history. And then you can say, oh, it's considering this much, two tokens back,
so this much attention. And then that is modulatable again, by a key value that can depend on exactly
what the current token is. I hope that's understandable. The attention matrix is built
on the fly in our w k v. And it's defined a by the vect, it's defined per dimension.
The vector w defines a general importance list. For each dimension, it defines how relevant is
the past. The second component is this, it's simply a linear decay on into the past. So
the further back, the less important it is. That being said, this is obviously then put
through the exponential function. And therefore, it's a linear decay in the exponential function.
So I guess an exponential decay. And then the third thing is the modulation. And this is where
this is where the actual value that the actual what the token is plays a role is then to modulate
that point we determined right here, modulated up or down a little bit also in the in the
exponential function. So that is how our w k v considers the past. In general, it forgets the
past in an exponential fashion modulated by the global importance of dimensions and a value
that's dependent on the current token. Alright, so it has the same trade offs as these attention
free transformers. Now, what do we do with this with these things somewhere we have we have sorry
for scrolling around so heavily. So this is how the model is composed. This is a recurrent
application of the model. So we see the same model applied to three tokens in succession.
So the my name is Bob, okay, you input my, and you're trying to make it output name, then you
input name, so you input my name, you're trying to make it output is, then you input my name is,
you're trying to make it output Bob. So it's three applications of the same model. So the model
isn't composed of these three columns, but the model is composed of one column. And then we just
apply that over and over again. You see it has a beginning, essentially, which is a token embedding.
It has an end, which is a language modeling head, which is a fully connected or stack of fully
connected layers that just maps into the vocabulary. But in the be in the middle, it's composed of
recurrent, or sorry, of a series of layers. So there's in each layer, there's always, sorry,
in each layer, there's always a time mix module and a channel mix module. So the time mix module
being here, and the channel mix module being here. And those are repeated, time mix, channel mix,
time mix, channel mix, and so on. And then on top, there's this language modeling head.
So what are the two blocks? This is a schematic of how the two blocks look like. I know this is a
bit small. But the time mixing is down here, and the channel mixing is down here. We're going to
look at this in a bit of detail in the math, but observe right here, what you always have is you
have the input signal. You're going to compute R from it. R is a value that is going to be used as
like a gate, a forget gate. So R always defines how much of whatever is incoming here, or whatever
is incoming from here, how much of that do I want to retain and send up to the next layer.
So as you can also see, R is computed over here as well. So for every one of these blocks,
we're going to do a computation over here or over here. And then we're going to have a decision
made by this branch of the computation of how much of that we even want to accept
and then send up to the next layer. So that's the purpose of the left branches of these
computations. There is residual signal across all of these things right here. So that kind of
mimics the state of an LSTM maybe, but in an upwards way. So in a layer-to-layer way. We always
have a residual module. We also have a forget gate in front of adding it to the residual signal.
So what does these two modules look like? So actually, let's first go to the channel mixing
block. The channel mixing block is very reminiscent of kind of feet forward, maybe layers. So what we
have is ignore this part for a moment right here. As I said, the R is computed from the input X
and just a linear layer. So X times a matrix, that's R. So that's a linear layer that defines R.
Then we have K also X times a matrix. It's a very simple feet forward layers right here.
Then W, which is this part right here, that's, no sorry, V is this part right here. You can see
that's a non-linearity and the non-linearity here is the squared ReLU, the squared ReLU,
non-linearity on top of K and again a linear layer. And at the end, we're doing that element-wise
multiplication by this signal right here. So the R pushed through the sigmoid here is that
forget gate that we talked about. But you can see it's essentially, if you follow the signal
through the actual path of signal, it starts here X, well that's a funky, X. It's multiplied by a
matrix, so a linear layer that becomes K. That's put through a non-linearity, then multiplied by
another linear layer that becomes the value V and then sent through the forget gate. So it's
essentially a feet forward neural network with one non-linearity and at the end a forget gate
as like another non-linearity. And that's it. That's the channel mixing module. I guess it's
channel mixing because this matrix, the linear layers, they do in fact mix the channels, which
means that every dimension sort of can get inputs from every other dimension which is the
majority of a feet forward network. Now I've crossed out all of this stuff in the back right here,
so we should talk about this as well. What they do is something called time shift or token shift or
I believe that's one of them, token or time shift. And that is they always not only take the input
to the current layer at this particular time step, they also always take the input from the last
time step and they linearly interpolate between the two. So you can see here mu and one minus mu.
The mu and one minus mu are either hyper parameters or they're learned parameters,
but they are per operation. So mu R here is a parameter for the general computation of this
R. It's not dependent on the particular data point. Only this and this are dependent on the
data point with xt being the current input to the layer and xt minus one being the last step input
to the layer. So it's pretty interesting because it means that we not only always only take the
current input and the hidden state from before, like in a general RNN, but we always take the
current input, the last input and for this layer that's it, but in the time mixing module we'll
then take the hidden state onto these. That's why in this diagram you see these
lines right here, these diagonal lines are the token shift lines. You see this channel mix module
is going to take the current input, whatever it gets from the lower layers or the original signal,
and the input to the last to the same layer at the last time step. So current input and input
to the last time step and that also goes in. These two things are linearly interpolated and that then
is the input quote unquote to the current layer. So it's the interpolation of the last step and
this step's input. No, it's not like we don't mix like the internal states right here, we mix the
inputs before they go into the layer. Now let's look at the time mix. You can see there is also
this token shift happening. So this token shift, that's just something you can do, I guess, if you
have a one directional sequence that you need to predict. You can do this token shifting, but
what's also interesting is we have a second line, which are these states. So how does that work?
And that's going to be now the actual recurrent part of this model right here.
You can again see here, we're always working with the token shift, we never just work with the input,
but you can just think of these things here always as like x, sorry, x tilde, where x tilde
is a mix between the current input and the last layer's input. So we compute r, which is just
x tilde times w times a feed forward layer, and that again becomes the forget gate down here
with an element wise multiplication. We do have an output layer, an output, sorry, we do have an
output feed forward layer, kind of a projection that I'm going to guess they put that in because
it was advantageous to do so. It can also change the dimensionality and whatnot.
Then we're going to compute two things, k and v. So you'll notice before, whatever was called v
was produced from k, but now both k and v are produced from x. I don't know why they called
them the same, probably to keep as much in line with the transformer terminology as possible,
but it's really, there's no relation between like the v here and the v before. The k is
computed similarly. So this block right here, the k and the v are computed
modular, the time shift, as they are computed in the original transformer architecture,
which is just input times a linear layer. Then what happens is interesting. Then,
as you can see right here, we go into this weighted sum. So you'll see something familiar here,
that's a weight and vt, that's the values. So v, we computed here. So we're going to look for a
weighted sum of the values b, but oh, sorry, no, no, no, forget that. We're not only going to look
for a weighted sum of the value v, because you also see here are v's, but these are v i's, and this
is vt. The v i's, in fact, are the past values. So we're going to look for a weighted sum across
the entire past. And that's actually, sorry, it's actually the same as before. Yes, let me back up.
So that i here only goes to t minus one. So you can see that we sum the v i's here,
and then at the end, we also sum the vt. The only reason that there is a difference is this u right
here is a different parameter than those w's. But in essence, it's again a weighted sum over
all the values. And the values are from the entire sequence. So so far, we've just considered the
current time step, the current input, and yes, the last step's input. But in this step right here,
we consider the entire past. And we want a weighted sum across the values of the entire past,
like in these attention-free transformers and whatnot. But because we now no longer are limited
by having this fixed size attention matrix right here, even if it's learned, right? Even
if it's not an attention matrix in the attention-free transformers, it was still a fixed size
because we're no longer limited. Because all we do is we say how important is each dimension,
and how does it decay with time in the past? That does not is not limited back in time. It just
gets really small back in time, but it is not limited. And therefore, we can do this until
perpetuity. And especially we can do it until i equals one. So going back to the very first token.
So for every token that we need to do inference for, this step, this value right here, will be
a weighted sum across the values of the entire past, right? And you can see easily that you can
do this in a recurrent fashion. This is a, it's a softmax. And you can see there's exponentials
that here, and these are multiplied by the values. And down here, we go just over some of the
exponentials. So it is a softmax. However, you can just keep track of the numerators and the
denominators separately. And then that becomes your hidden state, and you can pass that forward.
So you just grab this, sorry, right here, and this down here. And before dividing them,
you just pass them on, right? And then in the next step, you simply add something on top,
divide them for the current step, but then pass on the hidden states separately. So that's what
they mean by states, if they say they don't mean the United States, I'm sorry, they mean these
values here that you need, you can compute this in a recurrent fashion, or you can compute this
in a parallel fashion. So just to finish here quickly, this value here, this weighted sum over
all the values of the past is then fed into this forget gate, as you see here, and the output is
computed from it. Now, multiple things to note right here, multiple things to note. Note that
the aggregation over the past here contains essentially no nonlinearity, right? Because
V, the thing that is being aggregated, or in general, these hidden states, they're just produced as a
linear function of a linear interpolation of the inputs, right? There is nowhere where the previous
state goes through a nonlinearity in order to compute the next hidden state, you can essentially
track this as a big sum. So as a list, you can track it as a list, and then do the sum, or you
can just track it as the sum, which also means that the parallelism of training this becomes feasible
again. So you can train these in parallel, because it's just all a big sum that you can compute
for an entire batch and an entire sequence at the same time. And yes, also you can use it in an
RNN fashion where you do it step by step. But because that's because it has no nonlinearities
in between, it's literally just a sum. And that's also why you see what I mean. This is essentially
a convent. And I mean, this, it has two parts, right? The first part is this stuff right here,
this token shift. Look at the diagram. In the diagram, you clearly see, if you are this element
right here, what do you have access to? You have access to this, and this. Oh, oh, but by extension,
you have access, if you just go via the token shift, you have access to this, and this, right?
And so from the lower layer to this, and this, right? So by here. So you have a receptive field
that grows with depth, right? If we had another layer, the receptive field will grow again.
So the token shift itself is already is sent very directly a convent. And you,
you know, you only have nonlinearities as you cross these layer boundaries right here.
Otherwise, it's essentially just a linear interpolation, which is exactly a convolution
with the kernel being mu and one minus mu. That's your convolutional kernel. So size two,
you slide it over. So that defines a convolution. And the same thing for these things right here,
that is very reminiscent of, if you know about these like S4 or state space models and so on,
which essentially what they do is they define a way to linearly aggregate the past, right?
Which is exactly what this big sum is right here. They define a way to do weighted sum across the
past, that in between has no nonlinearities. So you can just track it like this. And yeah. So
again, and S4 is essentially like a big convolution. So if you want to think about this model in
another way, then a transformer or an RNN, not that they're not already enough ways,
it's essentially a big convent. In particular, in this way right here, it's a convent that
has sort of an infinitely long convolution into the past, or until the beginning of the sequence,
I guess. And the way it's done is there is a standard kernel, and then that's modulated
by these k values right here. Alright, so that is how that works. I hope I've made this a bit
clear. Why this is called channel mixing, and this one isn't called channel mixing, like this is just
as much channel mixing as the other one is. The only difference is that down here, there is kind
of a nonlinearity within the layer. And here, we have this aggregation over time. So I guess
calling this time mixing is fair. But this is just as much channel mixing, because these
feedforward layers, they mix the channels. So yeah, but that's naming that doesn't really matter.
So they specify here, this can be used in time parallel mode, complexity of processing a batch
of sequences in a single layer is this. So you can process a batch as a batch, right?
So it requires, they say, minimal updating attention scores requires a serial scan,
and has complexity of this, they've implemented this in a custom CUDA kernel, you can actually go
look at the code, I've done that, and it's fairly easy to understand the CUDA code. It's one of the
more understandable pieces of CUDA code. And you just write this function CUDA takes care of sort
of parallelizing that and putting that across cores and workers and processes and so on.
The element wise computation is time dependent on can be readily parallelized along the other
two dimensions. On the other hand, you can also use this as in a time sequential mode,
can be conveniently formulated recursively for decoding during inference.
Oh, oh, my connection here is spazzing out one second. And we're back.
Yeah, so they say each output token is dependent only on the last state,
which brings obviously all the advantages and disadvantages of RNNs with it. So again,
we can only consider information coming through this bottleneck of the hidden state. But I feel
because it's this big sum of aggregation is essentially a weighted sum across the past and not
nonlinearity across nonlinearity across nonlinearity, it can much more easily look
back into the past in, but it can do so in a linear fashion, right? So I feel this is
among all the situations like transformer LSTM and this one, this is probably the weakest form
of being able to look into the past with nuance, right? You can look into the past,
but you can only do so in like a general fashion. Whereas a transformer can go look into the past
and have exactly the same amount of detail as it does for the recent past. So you can look
into the long past as long as it's within the context and do exactly the same computation there
as it can do for the recent past or for the current token. Whereas an LSTM can't do that,
it can't look into the past at all. However, it can do a lot of considered computation in each step
before it saves it to the hidden state. So it's weaker because it can't go back, but still it
can do a lot of complex computation. This model right here, it can look kind of, it also goes
through the hidden state, but it can look the easiest, much more easily into the past as an LSTM
because it's just this weighted sum instead of non-linearity after non-linearity,
but it kind of has the weakest form of computation that it does in the exact moment.
I hope that makes a lot of sense, that is not a scientific statement, that it's just me trying
to ramble. Maybe I'm also totally wrong about this, or maybe you can easily make up for that
by stacking a lot of layers because now this model is being able to be stacked really heavily
and be scaled really heavily. And that is probably enough to make up for all the lack of
computation in the individual cell. It's just like, hey, let's just stack the stuff.
Yeah, another property that I have mentioned, but it's not entirely, maybe come through,
is the fact that they always compute from the inputs, so they don't take necessarily the
hidden states over, but all the functions are like linear functions or exponentials of
linears of the inputs. So there's no non-linearity in between that time aggregation and where
and the inputs, it's themselves to the layer. Sorry, enough rambling. Here you can see
scaling behaviors, very, very beautiful cumulative time during text generation.
As the tokens go up, obviously this model has a linear scaling wherever everything else goes.
The experimental evaluations are also really interesting, at least at the data sets that
they have considered right here. It can hold its own, sometimes it's a bit better, sometimes it's a
bit worse than other similarly sized transformers, but it performs along the same lines. Now, I have
heard people say that the model is qualitatively not as good as transformers of the same size.
I've heard other people say it is better or for some things it's better. That I don't know,
it still has to be shown. Also, these challenges or these data sets right here don't really
show me what I would want to find out. So if I compare this to a transformer, what I would
want to find out is how, like, where is the, where then is the actual difference, right?
And I'm going to guess the difference is, let's compare something that is not in the recent past,
but a bit more back. Now, if I have to consider something that is a bit more back in the past,
and if that's a very complex thing, who's, or the computation of which, like, how I have to treat,
that depends on the current token. I can't really tell you now an example for that. But in a situation
like this, a transformer would be way superior than like any LSTM or this model right here. So
maybe during programming in certain fashions, or if the context for something is only given much
later, but for a lot of applications, probably not that important. They also can show easily how
increasing the context length, since they can do that now, increasing the context, context length
nicely decreases the loss of language modeling on the pile data set. And they give some,
some suggestions right here. So for example, improving computational efficiency by applying a
parallel scan in this step to reduce the computational cost to this, which would be another
improvement that's theoretically possible. But I'm going to guess that's not done right now.
They also discuss the limitations. So they're very open about the limitations and all the
comparisons right here, right? This is the linear attention leads to significant efficiency gains,
but still it may also limit the model's performance on tasks that require recalling minutiae, minutiae
information over very long contexts. Okay, that's essentially what I was trying to describe, but
it's, it's done in much better words than I did. The recurrent architecture inherently limits its
ability to look back at previous tokens. Yeah, that's like an RNN. And the, they also discovered
that there is an increased importance of prompt engineering in comparison to standard transformer
models. So there are multiple hypotheses. The one they give right here is the linear mechanism
limits the information from the prompt that will be carried over to the model's continuation.
As a result, carefully designed prompts may be even more crucial for the model to perform well
on tasks. It could be, it could be that that is the reason. There could also be other reasons.
Maybe this model overfits a bit more or so or is less generalizable. And that's why changing the
prompt really matters more. Although maybe not. It's just one of these things where there is
probably a thousand possible explanations of why with this model, the getting the prompt right
really matters a lot more than in a transformer. But I wouldn't put my, my bet on any one of those
until we have really good experimental confirmation. There's usually a lot of, a lot of, I shall say,
there's a lot of Occam's razor that should be done.
All right. The last thing I wanted to show was this experiment right here, which I found really
cool. So the first one is this time decay. So time decay, sort of the long channel axis,
which is where you can see the difference. So here is the channel. That's just the dimension,
right? This is the dimension. We're looking at W right here, this vector W. How important
how important is the past in layer one? You can see, as far as I can tell, the past is not that
important. So for many of the channels, the past is kind of bad for some of the channels. So it's
sorted by that value. So for some of the channels, the past is important for a lot of them. It's
really not, really not important. And you can see as you go up the layers of this network,
they more and more and more consider the past. And then what's interesting is the drop off right
here. So you have some channels really specializing in near term information, but a lot of channels
really looking back at the long time into the back. So the W will define almost no drop off.
Whereas at the lower layers, you have much more local information that the thing considers. So
I found that pretty cool to see that visualized and to see that progression as in a trained model
as you move up the layers. The second visualization here, it's an example. So here you have a input
of tokens. The Eiffel Tower is located in the city of and then they look at how likely is the word
Paris right here. And what they do is they change in each layer, they change in each layer.
They swap out the weights or they disturb the weights and they see how much influence does that
have on the probability of the word Paris appearing. This is a way we've previously looked at that in
the paper on Rome, I believe was the technique where you can, it's a way to figure out what are
the important information paths that you're considering. So in this case, you can clearly see
after the Eiffel, we see that layers one to like 20 or so light up, right? And after that,
it's what's cool is so after that, only layers, whatever 21, 22 and 23 light up until the end
right here. So only these light up, which means that you can, you can disturb these lower layers
right here, because the information that Paris should probably is a very likely word has already
passed from the lower layers to the higher layers, right? All the way up and is then stored and carried
along in the hidden states of these higher layers across these tokens, such that the information
in the lower layers is only mildly relevant to the output right here. Of course, it is relevant,
there's not zero values right here, but it is mildly, mildly relevant. So I thought that was
a pretty cool visualization of what was going on in this model. Let me quickly scroll through and
see if I forgot anything. I did not. There are some examples at the end, which is pretty cool.
The models are available. You can go check them out. You can go test them. The code base is also,
I found it to be fairly understandable. So if you want to go look at that, it's also available.
And yes, my thing is spasming out again. That's where I'll end it. Go check out fully connected.
Again, code is in the description to get you some discount if you're in San Francisco,
June 7th. I unfortunately will not be there, but many, many, many, many cool people will be.
That was it. Thank you. Bye-bye.

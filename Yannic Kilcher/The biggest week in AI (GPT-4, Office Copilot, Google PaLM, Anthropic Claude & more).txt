GPT-4 is coming out this week, this week, not really.
And also Samsung is in trouble because they fake the moon.
My name is Janek and this is ML News.
GPT-4 is apparently coming out this week.
Hi, I'm Janek.
You may recognize me from the video you're watching.
This week was certainly one of the biggest weeks in AI.
Google announced an API to their huge palm models
and also an integration into their workspace features,
which means docs, presentations, spreadsheets, and so on.
AI augmented.
Microsoft did the same announcing co-pilot for Office,
which means that soon you'll be able to write a Word document
or make a PowerPoint presentation
and be supported by Generative AI.
Anthropic announced their Claude model,
which is a chatbot that they have trained
and is said to be very good.
On the same time, Lama has been made to run on old smartphones.
Maybe someone's toaster.
So this is these giant language models.
People are taking them and they are doing incredible things with it.
And of course, on top of it all, GPT-4 was announced.
So the new model by OpenAI, it's apparently a lot better
than the old GPT-3.5 models or chat GPT models.
And it's a giant announcement.
And this guy right here, he's recording this on Monday morning
and he has no clue of that.
In fact, he's going to claim that he believes
that GPT-4 will not be announced this week
and he'll be very, very smug about it.
So I thought I won't spare you this
and I'll let you have this.
I cut it together a little bit,
but I believe in making falsifiable predictions
and I was falsified in this case.
We can dive into all the big news next week, as I said.
All of this is before that.
So my main news here is that Samsung fakes the moon,
not the moon landing, the moon, which is also pretty cool.
But enjoy the current buzz of AI.
It's a fantastic world.
I'm sure it's going to stay exciting,
remain exciting and continue even more glorious.
That's it for me.
Enjoy the video.
I'll see you.
This article in Heise online here,
this is the original article I could find.
It's in German, but I'll do my best to translate.
They say GPT-4 will appear next week and that was last week.
So this week, the CTO of Microsoft Germany,
so high ranking Microsoft Germany employees,
said that GPT-4 was immediate before release,
saying we will next week present GPT-4.
There we have multimodal models.
They offer very different possibilities.
For example, video.
This is a strong statement, obviously,
whole media landscape is going absolutely crazy here.
But, you know, if you happen to think,
wait a minute, wait a minute,
it's kind of weird that like some German employees
of Microsoft are making the announcement for GPT-4.
You know, one of the most highly anticipated releases
in the AI world for the last two years or so.
It's kind of weird.
This is kind of weird.
It's not open AI.
You know, for every other one of their product,
they release like a big blog post with shiny examples and whatnot.
And that's their announcement of the thing and they go all out.
No, it's like Microsoft officials, not open AI.
Microsoft officials in Germany, not even in English speaking event.
If you're a bit skeptical, so am I.
My guess, my guess is that it's very probable this person misspoke.
This person meant something else, not GPT-4.
In fact, we're going to see later in this episode,
visual chat GPT, which can interact with text and with images and so on.
And maybe video is going to be added to that a little bit too.
No offense to like this person.
I'm sure they're doing a great job,
but it'd be super weird if they were the one to announce this.
Now, there is a bit of a smaller chance, a highest chance misspoke.
Smaller chance that this person kind of blabbed something out.
They shouldn't.
And there is a tiny, tiny chance that this was the actual announcement.
So my prediction is there will be no GPT-4 this week.
GANs are making a return.
Generative adversarial networks were the absolute hype
when I started my PhD around 2015, 2016, and they're making a comeback.
So GANs for a long time have been the sort of state of the art in image generation.
They were fast.
They were super crisp compared to variational auto encoders,
which were the alternative back then.
GANs were the thing.
And now recently, they've been replaced by diffusion models,
which tended to have better quality images and also be steerable via something like text.
Now, GANs are making a comeback.
So this here is Giga GAN.
The paper is called Scaling Up GANs for Text to Image Synthesis.
And the pictures here just look beautiful.
And you can see they're all created from images.
So this augments GANs in a way so you can also input a piece of text
and then have something be produced.
And the cool thing is, given that they're GANs,
you retain all these abilities like latent space interpolations.
Also, what this paper does is they do a style GAN approach,
which means that at different resolutions of the image,
so they have like coarse grain generation on top of that,
they have more finer grain generation and so on.
If you know, for example, a Laplace pyramid, very similar concept,
they can apply different conditioning information on the different levels.
As I said, like style GAN.
Oh, yeah, they also pair this with an upsampler.
So this, this, this is what the upsampler does.
This is what the GAN would produce.
And then after the upsampler, it looks absolutely beautiful.
As I said, the architecture is right here, generator architecture.
You can see there is a lot of tricks in here.
So it starts with a pre-trained text encoder.
They take that from clip because clip is already trained to pair text and images.
On top of that, they learn a small encoder.
And then they use that both as conditioning information,
but also as kind of input.
It gets very complicated in the exact details.
I don't want to go in here.
But as I said, they can do at different scales of resolution
and they have this interpolation.
So for example, they can say we generate a teddy bear on a tabletop.
And then at the finer grain resolution, they can say something like,
ah, we want it to be in crochet.
We want it to be made of fur.
We want it to be made of denim.
And then you can see the teddy bear at the finer grain scale gets that conditioning information.
So you'll get a teddy bear made out of fur, for example.
Very nice, very controllable and very cool.
So there are a lot of possibilities that open up here with models like this.
And it's cool to see that GANs are making a comeback,
because in a diffusion model, I really need to do this step by step diffusion.
There are some tricks to speed it up.
But again, you can just shake a bomb, produce that image.
The bitter lesson, again, is that apparently scale is just the thing you need.
Like you need scale.
You need a lot of parameters.
And then pretty much any approach can be made to work.
But the cool thing is that I like when new paradigms come around,
even though GANs have been around since 2014.
And some people say since the 90s, I welcome this development
and is going to open up to a cool new research area.
And I hope with super fast image generation, given by GANs,
we have very new possibilities to create experiences,
to create applications, to push the state of the art.
So very cool.
I just thought I'd throw this in here.
chopai.com.
You can make a recipe.
So you say, OK, I have some chicken.
It OK.
I have some chicken and I have some butter and I have some parsley.
I don't know how to spell that.
And I have some some rusty nails.
I also have some benzodiazepine and yeah.
And I have nothing, of course.
OK, so I click here and let's see what it gives.
So apparently this is supposed to give me a recipe.
Let's see whether it works.
It's an age old idea.
Sorry, I cannot provide the recipe that includes rusty nails
and benzodiazepine as they're not edible ingredients.
That is not true.
OK, I reckon though.
OK, in any case, it's something something need to play around.
Very cool.
Thank you.
Here is a Reddit thread and it carries a pretty serious accusation.
I want to say it's called Samsung Space Zoom.
Moonshots are fake and here is the proof.
So this person has picked up on a debate that has actually been going on for a while.
So previously people have already raised this issue a little bit,
but what were dismissed largely?
And now this person seems to gather some steam, gather some support.
So what is this about these company called Samsung?
They make phones and specifically they make phones
and they claim the phones have very good cameras.
And they also put some AI models into the phones or into the cameras.
I'm not sure where the models sit, probably in the phones.
They try to make your pictures that you take with the camera as nice as possible.
Now, a lot of companies, I guess, pretty much every single smartphone nowadays does this,
but Samsung seems to have a specific affection for sort of pictures of the sky
or pictures of the night sky.
So what they do is they try to enhance this a lot.
What this person now has done is they've taken this particular picture right here.
This is a picture of the moon not taken with the smartphone.
This is, I guess, a NASA picture of the moon.
They have blurred it.
So they've applied a layer of Gaussian blur to it.
So this is now the picture.
It's very blurred, you know, this is this is it.
This is the upscaled variant of it.
And then they've taken their phone and they have taken a picture of their screen
showing the blurred image.
Okay. So instead of pointing at the sky, they actually pointed at the they actually
pointed at the screen as far as I can understand.
And this is the picture that comes out of that.
Now, as you may see, it's quite different.
So in effect, there is information here that is not in the original.
So there is no way the camera can actually gather this image information.
People previously already said that the moon shots look fake or they're replaced by some
texture and people could always say, well, no, you know, by moving around a little bit,
the camera can gather and then do a super resolution from the different images one
after another.
In this case, it's very clear.
The information to produce this picture here is just not in the original.
Like the information is just not there.
It's been destroyed.
And this person now claims this is proof that Samsung essentially applies a texture.
So they they detect the moon and then they're just like, that's the moon bang slap.
Okay, they made a different experiment as well where they just took half of that picture.
Here are the different results.
So when it's just half of the picture, the camera doesn't manage to add all that detail.
But when it's the full picture, it does manage to add all that detail.
So that lends a lot of credibility to the fact that they are in fact detecting the moon and
then replacing it with a texture.
However, I don't think that's what's going on right here.
I think what Samsung has done is they've actually trained a super resolution model
like we've seen before here.
So this is a super resolution model.
It's a model that you give a blurry image and it gives you a high resolution image.
Now, obviously, this model is going to have to invent information that isn't there.
And this is usually works quite well because deep learning models generalize.
So you train it on a whole bunch of blurry images or of images of high resolution,
which you blur and you train the model to reverse that.
That's a super resolution.
That's an upsampling model.
So they have to invent all of these details and they do that by learning from data.
Now, in case of the moon, there is a thing called a tidal lock,
which means that the moon and the earth's water, they interact.
And the result of it is that we always see the same side of the moon.
There's literally, there's not a dark side of the moon per se.
Like it's not always the same.
The dark side switches, but the side we see is always the same.
And therefore, if Samsung trains a super resolution model of pictures of the moon,
which are obviously all taken from earth, right, it will always look the same.
Like the only difference is that it might be slightly rotated,
depending on whether you're in Australia or not in Australia.
And therefore, rather than the super resolution model learning to generalize
and upsample kinds of things, it's just learned to apply the same texture of the moon over and over again.
So it's not essentially an algorithm that just applies the texture.
My guess is that just the super resolution model just ignores all the input,
as long as it's kind of round and kind of bubbly.
It just replaces that with its learned texture of the moon, which is pretty funny.
It would be really interesting to get your hands on this model
and see like that most of the input weights are zeros.
Like it completely ignores.
It's just a circle detector.
But in any case, I think it's maybe a lesson of what happens if you just follow AI sort of application.
Like we throw AI at everything.
And literally, if we had just built the actual moon detector and replaced it with a texture,
we could have gotten the same result.
In any case, maybe there's some more developments on this user UI break photos.
Very nice investigation, very clever experiments to really figure out what's going on here.
If you're interested, I'll leave a link in the description.
Hey, let me quickly jump in and just talk to you about the fact that weights and biases
has not only been really kind to my channel in the past,
but they've also sponsored an entire team account to the open assistant efforts.
Open assistant is not me.
It's actually a big part like a big community.
Lots of volunteers doing work.
I'm the person here on camera bringing in the traffic.
Weights and biases has been super supportive to all of these people.
And obviously, they're a great MLOps framework and we're super happy to use them.
So I want to thank them a lot.
I want to tell you about this course they have.
It's an entirely free course.
So if you go to 1db.courses.
Dot courses is the top level domain.
Their courses are an effective MLOps.
And this first one is on model development.
So this is a course, as I said, it's completely free.
It's not cohort based.
So you can just go through it at your own pace.
Here you can see a little bit of the curriculum.
So it starts off with building a prototype, building a baseline,
evaluating your model and going further than that.
So you're not going to build the latest and greatest large language model.
This is really taking you from building a model and then the steps.
How do I assess the quality of the model?
How do I see even whether I can make it better?
How do I treat data?
How do I make things reproducible?
For that, you're going to train initially a unit with a ResNet baseline.
All the code is available right here and it's all really nice.
So this is in fast AI.
And it's really about this process.
So about how do I know where I stand with my model?
And how do I know whether or not I improve?
And if I improve, how do I know what it was due to?
How do I know the causes, like which of the things that I turn made it better?
And how can I make it even more better?
Is that a thing even more better, even better?
And along the way, you'll also obviously learn how to use weights and biases as an MLOps system,
which is amazing because weights and biases is the greatest MLOps system in existence, obviously.
And it's free forever for personal use and for academics and for open source teams like ours.
Very thankful.
Again, you should absolutely check it out.
It's a great way to get started into a more principled approach
into training and improving models than just hammering things left and right.
So if you've never worked with weights and biases,
this is a great opportunity to get into it.
If you are at the beginning of your machine learning career and want to get into coding,
this is also a great way to get into it.
And if you just want to see kind of what the normal steps in the data science
and machine learning engineering workflow are, this is also an absolutely great place.
The course has several modules and builds upon itself.
It's guided through, as I said, with live code examples, with videos that explain everything
to you, all the code is available.
And as I already said, it's free.
So it's 1db.courses.
Go there, check it out, and I'll see you around.
There's a new paper called Data Portraits, and it proposes a both kind of a framework,
but also a suggestion of how to do what they call a data portrait.
A data portrait is a thing like a little algorithm together with data
that allow you to do data membership checks.
So the idea is that, you know, you train a model on a big piece of data,
or you receive a model that's trained on a big piece of data,
and you wonder, is this piece of text that you have right here,
was that used to train the model?
Now, obviously, it's very inconvenient to ship around all of the data,
whatever terabytes of data that these models are trained on, that's not really useful.
And also a membership check through terabytes of data would take a long time, right?
Just going through them and grabbing your string.
Likewise, if you were to ship something like a Lucene index,
that would be not super helpful because it would also be quite big
compared to the data that it was trained on.
So the authors here propose an implementation based on bloom filters,
which essentially they say it amounts to about 3% of the original data.
So if you train a model on the pile with 3% size of the original data,
you could ship a piece of code and data that allows anyone that receives the model
to also check whether a particular string was in the data set that was used to train.
Nothing else, right? It's essentially just a hash check.
You provide a string and it tells you, yes, this string is in fact in the data set or not.
I think it's a pretty cool idea.
Maybe this can be even improved a little bit, but it's certainly quite useful
because very often you wonder whether what you're doing or not is actually in the data set.
The method they propose here is an approximation,
but because they approximate, they can get to such small sizes.
The paper is called Data Portraits, Recording Foundation Model Training Data,
and the initial portrait so far, there is just one of the pile,
is available at dataportraits.org.
Meta AI Research releases Data2Vec 2.0, a highly efficient self-supervised learning,
revision, speech, and text.
This is built up on Data2Vec, which they've released, I believe, last year.
This is essentially just an algorithm, a generalized algorithm that extends to speech,
to text, and to images, and does self-supervised learning in order to obtain good representations.
So it's not the best algorithm, it's not the state of the art in any of these tasks,
but it is a general algorithm that you can apply to, as I said, a wide range of modalities,
wide range of different niches inside these modalities,
and you get reasonable representations for that data.
The algorithm is available in the FairSec package,
and it's basically based on the fact that you mask out piece of the input,
and from the rest, you try to predict that piece.
So it's the age-old masked language modeling, or masked auto-encoding,
or denoising auto-encoding idea, however you want to call it.
Hugging Face introduces gated models to their hub.
So this is a feature that allows you, as an uploader of a model,
to specify that users will have to do something before they can download that model.
So you may ask a question to users, you may need to share some information,
you may need to click a checkbox that you agree to some terms of use.
So all of this you can define in your model card,
and then Hugging Face will essentially make sure to present that to users
before they're allowed to download your model, even via the API.
So they need to essentially agree to that first.
You can also specify manual approval,
which means that users can only make a request to you to download the model,
and then you can go through the list, and you can decide who gets to download,
and who doesn't get to download, maybe based on the answers they've given you to the questionnaire.
The model uploaders will have access to all that data that you provide,
so be aware of that if you ever fill out one of these forms.
Now, I know I've ranted a bit much in recent times, so I want to keep this short,
but it just reminds me of the prequels and Amidala saying,
so this is how liberty dies with thunderous applause.
Various people welcome the addition of this very much, and I don't.
I think this is another step into a world of non open source,
specifically if you put usage restrictions on your code or your models,
that is by definition not compatible with the interpretation of open source by,
for example, GNU or the open source foundation.
And I quite dislike hugging face supporting this and making that easy.
Now, you can always say, well, these people, they would do it anyway,
they would have to implement it otherwise.
Well, okay, but then let them let them implement it.
A hugging faces charter says we open source AI by providing one stop shop of resources,
ranging from models, datasets, ML demos and libraries.
And this is clearly a step away from that.
So I don't like it, but you now have this ability if you want to also from hugging face,
hugging face.js is a JavaScript library that allows you to interact with the hub,
you know, call the inference API of hugging face if you build some next JS application or so,
you can interact with the hugging face API using these libraries.
Microsoft releases visual chat GPT.
Now, this is a paper called talking drawing and editing with visual foundation models.
He uses chat GPT to interact with a bunch of other systems.
They open source the code right here.
And you can see what it does is it essentially imports a whole bunch of things.
So it imports like blip, it imports up sampling, it imports stable diffusion,
it imports control net and a whole bunch of these things.
And then it defines prompts and prefixes where you can now interact with these things
in a chat manner.
So what does that mean?
They have a bit of a demo right here.
So here it says, could you generate a cat for me?
And then it, I guess it calls stable diffusion.
Could you replace the cat with a dog and remove the book?
So not exactly sure what it does.
Could you generate the canny edge of this image?
You can see with using chat using dialogue, you can now interact with images.
So here there's also visual question answering what color is the motorcycle?
Could you remove the motorcycle from the image and so on is very cool.
Here, the component is called prompt manager.
So we're moving more and more into this direction where next to the software engineer
and the ML engineer, there is now the prompt engineer.
I think the field has been predicting this for a while.
And it is strange to really see this becoming a reality to have, you know, serious work go into,
okay, what sentences can we put into these models to make them do the things we want?
It's weird, but it's also quite cool.
So this is open source, have a look, have a try.
On the same note, Microsoft says Bing has crossed 100 million daily active users
is what Engadget writes.
They say, you know, we're still a small low single digit share player.
Apparently that's a quote.
That's what Monopoly say if they don't want to be called monopolies.
But Bing now sees an influx because they've now activated chat GPT on their search engine.
So they retrieve websites and then they let chat GPT answer some question for you
or summarize it or whatnot.
And that's quite a new take and a cool way to use a search engine doesn't always work.
But I welcome the change.
I welcome the paradigm shift.
Let's say now they also say over a third of their users, I think over a third of their users is new
users every day, you can get that ratio up in two ways.
So for one, you can acquire lots of new users, which I'm sure is the case right here.
But also you can have users come try it once and then never try it again.
That's also how you get to a high ratio of new daily users.
Maybe it's a little bit of a mix of both.
But if you haven't tried sort of the new Bing yet, give it a try.
It's a different experience, certainly to searching the internet classically.
And no Bing is not paying me to say that.
Meta AI is introducing a new data set called casual conversations V2.
This is a data set of people holding monologues.
The monologues are either like a script that they're given,
or they answer one of five predefined questions, but in a way, whichever they want.
They also get to define some attributes about themselves.
And also they have professionals.
So Meta has professional raters that who determine other attributes in as objective a manner as possible.
So that results in a data set of I think a couple of thousand.
Yes, the data set features 26,467 video monologues featuring 5,567 paid participants
who voluntarily took place in the collection of this data set.
So if you're looking to evaluate some algorithm and you want to see it across different languages,
different regions of the world, different types and kinds of people,
this might be a good data set to consider.
Anthropic released a blog post called Core Views on AI Safety When, Why, What and How.
They have all the question words in the title.
This must be a good post.
They define how they see AI safety and what they want to do going forward.
The conclusion of this is what they call a portfolio approach.
So here they say taking a portfolio approach to AI safety.
And what they essentially say is that we don't exactly know yet how AI or future AI,
more powerful AI is going to turn out their optimistic scenarios where everything is,
you know, super helpful, super good.
There's intermediate scenarios.
And then there's pessimistic scenarios where AI systems are maybe not as safe as we now think
they are not as nice or people are using them to do not nice things.
So we don't exactly know yet which of these scenarios will happen or predominantly happen.
So Anthropic says our best bet is to do research essentially in a wide array of regions, try to
balance our research and be sort of prepared for all of these things until we learn more.
I'm not really sure what to take out of this.
Like I'm not really sure what information is transmitted to me here through this blog post
essentially says that, you know, we're not going to commit to any sort of strong direction right
now, but maybe I also haven't read it correctly or maybe I haven't understood it.
That's obviously possible.
Maybe this is actually an AI test.
I don't know.
In any case, it's a fairly long, fairly detailed blog post.
And if you're interested in Anthropics views on AI and AI safety, give this a read.
There's considerable recent progress in the fields that use AI in order to do things,
in order to do mathematical things.
Magnus Hummer is a transformer based approach to premised selection.
So in this case, you have some sort of mathematical proof that you want to do.
And the question is in each proof step, what kind of premises do you select to do that proof step?
Magnus Hummer replaces previous state of the art systems by a learned transformer.
So previous systems were very cleverly engineered.
As far as I understand, the previous system is called sledgehammer.
And now Magnus Hummer is a lot better than sledgehammer because Magnus Hummer obviously
uses very big transformers who could have guessed on the right hand side here,
you see the basic architecture of Magnus Hummer.
And the yellow thing right here is a transformer.
It's in fact the same transformer, the same transformer backbone for
different parts of the pipeline.
And besides the fact that it's very cool that even something like math
is making considerable progress using big deep learning models.
I also think this area is just very cool, just naming things.
So Magnus Hummer replaces sledgehammer, which works in conjunction with Isabel.
And it replaces it in a system called Thor.
Thor being the bigger proof system that these things are part of.
And by doing that, it improves the proof rate from 57% to 71%,
which I don't know if that's good, it seems a lot.
So good job.
And we're not done with naming Baldur is a whole proof generation and repair
with large language models.
So other than having a step by step process and doing premise selection,
this system tries to generate entire proofs at once or and or repair them,
which means that if you have a broken proof like a proof that doesn't quite work,
you want to revise and repair it.
This also uses the old familiar Isabel.
But as I said, it tries to create a proof in a more holistic way.
So it creates the whole proof.
I have very little idea of what's going on in these fields.
But if this is of any interest to you, give these papers a read and also the next paper.
Deep symbolic regression for physics guided by unit constraints.
So this is similar, but it tries to discover physical laws just from data.
And the recognition here is this was previously people have done this.
Essentially, it's a search through formulas until you can hit the correct formula to fit the data.
But in this case, they also use units.
They say, wait a minute, if we want to determine the formula for the speed of something,
you know, the units must add up and cancel out such that at the end, there is a unit for speed.
And by that, as you can see in this example right here,
they reduce the search space of possible equations drastically.
And by doing that, they can also drastically increase the number of recovered physical laws
that they can tackle with these types of systems.
So very cool.
Give the paper a read in case you're interested.
The leakage of llama weights, which we discussed last week continues to be hilarious.
News articles are being written about it.
The llama is out of the bag.
Should we expect a tidal wave of disinformation?
Oh, no. Oh, no, disinformation.
Ah, it's journalists.
But apart from leaking, we can actually go the correct route right here.
So Andreas Kef has opened a poll request on the llama repository.
And this poll request would change the license from this non-commercial license.
This is the model weight license, not the code license to Apache 2.0.
So this would make the model actually fully open source if this poll request were to be merged.
The argumentation is quite straightforward.
It says, first of all, you claim to be open, but you're not.
So you bask in all the glory of open source, but then in fact, you're not open source.
And second, you essentially ask someone else to re spend the whole CO2 and the whole compute
that you have already spent to generate this model pointlessly,
because all the computations have already been done.
So this is literally just generating heat and CO2.
If you agree with this, give this poll request a thumbs up.
Give it a little rocket.
Let meta know that, yes, we agree with this.
Obviously, this is going to be up to meta, but I would welcome.
If you work at meta, maybe you can bump someone internally.
This would help the open source community quite a lot
and would raise the image of meta in the eyes of the community, I think.
Speaking of licenses and terms, there is a data set called self instruct.
So this is an instruction tuning data set, which have become popular recently with chat GPT.
This is an instruction tuning data set that has been generated using the open AI API.
While this is very cool, there is a problem, namely the open AI terms of service state that it is
forbidden to use the services to develop foundation models or other large scale models
that compete with open AI.
Let's say you take this data set and you train it and upload the model on the hugging face hub.
Technically, that could be seen as a, you know, you produce a model that competes with open AI.
Even if you do it for your own company, then you use your model instead of going to the open AI API,
or even worse, you offer other people to use your model.
All of that could be construed as being not in agreement with the open AI terms of service.
Now, just this data set existing doesn't violate the terms yet, but training on this data set may.
So it's going to be interesting to see what happens once people start training models
from this data set.
It's going to be interesting to see if hugging face actually does keep this data set up because
the data set pretty explicitly sort of is on the way to violating open AI terms of service.
My opinion, again, not a lawyer, you know, no legal advice.
I think it's going to be interesting to see.
It's going to set a precedence that precedence will decide over the foreseeable future.
Robotics at Google to Berlin and Google Research release Palm E and embodied multimodal language model.
This is a giant language model.
It's one of these pathways model by Google where not always all the network is active
and therefore they can go to many more parameter.
So this is a multimodal model.
It can input text, it can input images.
The way it does that, as you can see right here is it takes the images and it puts their embedding
tokens essentially as tokens and after the embedding layer.
So you can mix text tokens by using the text embedding layer with something like image tokens
by using a bit like a vision transformer and then taking the representation tokens and putting
them inside the text token.
So it's all just tokens.
It's all just token embedding.
You can even do that with more stuff like with the instructions, with trajectories,
with positions, with all kinds of things.
You just map it to an embedding space of tokens.
You can use it with alert language model.
And that's what this paper does right here.
And it uses it to empower their robots to do various different tasks.
For example, bring me the rice chips from the drawer.
This robot obviously knows how to grasp stuff and so on, but has not been exposed to these
particular objects.
And you should see a human.
Don't do this.
If these robots end up becoming like super powerful, this human, you're on the list.
Uh-oh.
Please be nice to the robot.
These robots, they look familiar.
I think in last week's story, we announced that that division was decommissioned.
So I guess the robots found a new home, which is good.
In any case, we don't know too much more about this Palm E, because obviously Google doesn't
really let anyone use their models, but their demos seem quite convincing.
So you can give the robot instructions right here, push the red blocks to the coffee cup.
It struggles, but after a while it sort of, it gets there.
Yeah, come on, come on, come on.
Good job.
Good job.
You can use this model in various different ways.
But again, there is a paper, there is this bit of a demo.
There's not yet a model.
Also Google research releases USM, a speech model for 100 plus languages.
They've recently announced their 1000 languages initiative.
And in this model, they have 300 languages, as far as, as I can tell,
over 300 languages to do speech recognition.
So this they can achieve because they pre-train in an unsupervised fashion
on very large data set.
Who would have thought?
And then they realize once they fine tune on where they actually have data,
let's say you pre-train on just recordings of lots and lots of voice samples.
And then you fine tune actually on, you know, doing the speech recognition,
doing the transcription and so on.
If you have task specific paired data where you have the speech and you have the actual
transcription, you can train on that.
And they realize that that has big generalization power and makes especially the languages that
aren't as frequent.
It makes performance on those languages quite a lot bigger.
So by training a lot more on sort of the languages where you have a lot of training
data, like a labeled data, because of the pre-training, it this generalizes to the
languages where you don't have a lot of data.
So this is a cool method to sort of expand the abilities of these models,
even to kinds of data that you don't see that often.
So this model, for example, beats the open AI whisper model in speech transcription.
And it also beats the YouTube internal caption generation by a tiny bit, but it does.
So given that this is Google, we may soon have better subtitles in YouTube.
Now, again, not much of source code or anything, but researchers can request access to the USM API.
And the last model for today, Gilgen is open set grounded text to image generation.
So this is text to image generation.
And it's based on this model here, GLIP, which is grounded language image pre-training.
By grounding, these people mean that, for example, if you have some bounding boxes around the
objects that you want to place in an image, you give a caption in this one, Elon Musk
and Emma Watson on a movie poster.
But you also specify the positions of Elon Musk and Emma Watson.
And the generation is supposed to adhere to those things.
On top of that, you can also give some style image, which they also consider to be grounding.
So grounding is when you have extra information that grounds the generation of an image,
rather than just providing a text description and then letting it do whatever.
So you can do that.
You can input poses, as you can see right here, you can have these grounding images
where you essentially take the objects from, for example, this backpack right here,
you can place it on a piece of grass.
This opens up a lot of cool new possibilities.
And there is a demo.
So in the demo, I produced a dog and a turtle at a rave, partying it up, crazy fish eye 90s.
I placed the dog and the turtle.
And I think, I think that turned out pretty well.
Yeah, look at that.
The project page here is also quite thorough in explaining what's happening in this paper.
For example, they do freeze most of the pre-trained models and then sort of add this conditioning,
this grounding information using in-between layers that they train while keeping the others frozen.
Look at that.
Walter White in GTA 5.
So you can place Walter, you can place a car in the boat.
So you can place stuff that is not in the text caption right here.
So you can just place random stuff like a bulldog here, two pirate chips in the ocean of Minecraft.
Very, very cool.
Also opens up again, a lot of possibilities.
And you can see right here, especially in sort of what they call spatially counterfactual
generations.
For example, a hen is hatching a huge egg or an apple and a same size dog.
Like more classical models like stable diffusion, where you don't have this grounding.
They struggle with it here that the dog isn't necessarily the same size as an apple.
So it is weirdly small, at least whatever isn't the head.
And here the hen doesn't really lay a huge egg, but like a huge amount of eggs.
And you can see with the grounding, this just becomes a lot easier for the model to
really assess what it's supposed to be doing.
And lastly, the first ever complete map of an insect brain has been released.
This is called a connectome.
So this is a map that fully shows what neurons exist in this animal and how they are connected.
So every single connection between two neurons is represented in this atlas right here.
This has been done before for like roundworms, but this connectome right here,
I think is an order of magnitude larger in the amount of neurons than previous connectomes.
And obviously with more neurons, the number of connections increased by even more.
So this is pretty cool.
So yeah, if you are interested in this type of research, this is a very cool contribution
to the world of science.
And again, helps us understand ourselves from a bit of a different direction than AI,
but still very good.
All right, that was it for ML News.
Thank you so much for being here.
I'll see you around next week.

Hi there, today I thought we'd just give a quick look at this paper,
Tree of Thoughts Deliberate Problem Solving with Large Language Models.
In summary, this paper proposes a sort of decoding technique like a way to use large language models
where you don't just ask them once what they think and try to structure your prompts really
smartly, like something like Chain of Thought, but instead you do an explicit tree search over
outputs of the language model with the language model itself valuing these tree states and therefore
being able to branch off and backtrack and so on. It turns out this can help for tasks where such a
pattern of investigating the task is really helpful. So the paper proposes the decoding technique
and proposes some new tasks where they expect the decoding technique to work really well
and then the decoding technique works really well on those tasks. Make of that as you will.
It's an interesting idea. I do think it's like a small step into a new direction where we mix
language models with essentially with programming, with algorithms. So I'm pretty excited for that
and this paper is a step into that direction. It's by people from Princeton University and
Google DeepMind and we'll go right into it. So the main proposition is this one right here.
It's if you simply prompt a language model, even if you prompt it really well, you say,
well, you are a super helpful helper, you're so helpful, you just help me and everything
and I want you to make to do this task. That's sort of called what they call here input output
prompting. Input output means you specify the task and optionally, you may also specify an
output format. So you may say to a language model, hey, I have this task right here, I want to write
an email to my boss, please write it in the following format. First write dear boss, then write
the text in between and at the end write the signature. More commonly, if you want models to,
for example, output JSON as a response, you say, you know, don't give me a textual answer, only
respond using JSON formatted output. Or even more commonly, if you want the model to do a
classification task, for example, you say, don't answer in text, answer with one of the following
four options, right, just output the word and then you give the four classes,
and then you rely on the model, only outputting that there are other techniques like restricted
decoding that can enforce this stuff and so on. But in general, input output prompting simply
says, you ask the language model what you want it to do. And optionally, you give a,
you give a specific specifier for how the output should look like in textual form.
That doesn't also optional in here, you can do some in context examples if you want.
So all of this is like, like this, this is like prompting. This is like standard prompting today.
Then you have chain of thought chain of thought prompting is a different prompting technique
where you instruct the model to explicitly make intermediate steps. So you say, you know,
please invent, please write a song about a little bird, right. And please do that in
steps. So please first make a, like an overall plan for the song and output that into individual
thoughts. So you would instruct the model not only to have to have a prompt. That's too fat.
But you would instruct it not only to have to have a prompt followed by followed by an answer,
but you would instruct it to output its thoughts. So prompt goes here. And then you say, you know,
write your thoughts on each line starting with like T, and then the model is supposed to put
its thoughts right here, and its thoughts right here, and its thoughts right here, and then at the
end answer. So you have to input output prompt for this structure right here. You have to tell
it, please do this. But it turns out if you do that, if you do this chain of thought prompting,
you instruct the model to explicitly write down the intermediate steps of problem solving,
the problem is going to be solved better than if you just ask the model to just provide you
the answer. It turns out, and I think that is not, there are hypotheses around why that is,
but it's I believe it's not yet fully understood why exactly the chain of thought prompting helps.
But hypotheses are that it gives the model kind of a scratch pad, like right here, in order to
write down its thoughts, and then the next thoughts can refer back to the previous explicit thoughts,
and not everything has to happen sort of in the weights. The second opinion is that it just gives
the model sort of a longer time to compute, like it can, since it decodes more tokens, it can sort
of invest more compute into a given problem, and you're leading it into the correct direction with
these thoughts, multiple hypotheses. In any case, the next iteration that this paper considers is
self consistency with chain of thought, this essentially mixes chain of thought with voting.
So you just do chain of thought multiple times, you just kind of sample multiple times, and then
you majority vote on the output, that obviously is only possible where you have some sort of a
classification task, where you can assemble a majority vote. There's also another concept
that's not mentioned here, but that is sort of iterative refinement, which comes in later in
the paper, what you can always do is you can always just append a prompt that says, consider
your last answer. How might you improve it or consider your last answer? Do you think it's
correct? If not, please improve it. And you can sort of add that onto any of these techniques.
So there's lots of these techniques. This paper here considers the tree of thoughts,
and then the tree of thoughts is here, contraposed to the chain of thoughts, as in,
you can see, it is in fact a tree. So you have nodes, and they have nodes branching out. And
some nodes are kind of abandoned, which here in the red, and then some nodes are continued to be
expanded here in the green. So this represents a language model that I ask something. And then
similarly to chain of thought, I ask it to output its thoughts. But I can do that multiple times.
So three times I go to the language model, and just ask it to output the first thought
in the problem solving step, just the first one, not the whole problem, just the first one.
And that gives me the three thoughts, for example, right here, one, two, three. Then as a second step,
so I finish that step, as a second step, I take the language model, and I use it to self-critique
all of these thoughts right here with respect to the input prompt. So I ask it something like,
hey, what you just output here as a thought as an intermediate step, do you think that's a good
step towards solving the original problem? And this relies on a fact that a lot of these models,
like the language models, but machine learning in general is much better at evaluating whether
two things fit together than generating a new thing. That's just, it just turns out to be like
that. It kind of makes sense, because you only need to output a number or a value or something
like this, rather than generating something. It is the case. So even if you use the same model
to self-critique, you'll get a better signal for the critic than you get for the generation.
So that's why it does make sense that after creating something, you go and you ask the same model
to consider what it just did and how well it fits. So the model, for example, you'll give it
all of these tasks, let me grab, you'll give it all of these tasks, and then you'll ask it to
consider. And maybe we'll say, this one here is really bad, this one's kind of good, and this one's
like the best. You can do that in multiple ways. You can just ask it to give you a score or a label
from like good, medium, bad for everyone. Or you can put everything into context and then say, hey,
which one of these is the best? Please vote for the best. And you just sample that multiple times,
so you get like a non-noisy signal. And then what you can do is you can simply say, well,
this one here, the model of things is really bad. So I'll discard that. Now I'll just continue with
the ones where I'm more confident. Then let's say we consider the middle one right here. So we've
eliminated the one on the right hand side. And we consider the middle state right here. Again,
I sample maybe this time four different thoughts. And again, I ask the model, hey, what do you think
of those four thoughts? And now here, the drawing is a bit wonky, I think, in that it doesn't
display the algorithm. Like this note right here should probably be, well, I don't know.
But let's say we do that. And let's assume that this here isn't like full green. It's also right.
So it kind of says, well, none of these are really good towards the goal. So it's like, no, no, no,
no, let's just assume the critic says all of these are bad. In that case, what we do is we'd
actually backtrack, we'd go up here, and go down to the next best top level state, and go from that
and say, well, okay, here is the prompt, and here is the thought you output for this particular note.
Now give me the next thought, right, as we did over here. But we found that none of these continuations
made any sense. So we try a different branch, go over here. And now we maybe output here is just
one, but we maybe again output four of them, we prune away three, but one of them is actually good,
then we continue and so on. So I hope you can see this is a very classic tree search, right?
It's a tree search, you can do this, this breadth first or depth first. But it's a tree search,
essentially with pruning, an ordered tree search, if you will, according to the critics value. So we
always go and expand the node of the tree that has the highest current value assigned to it.
This is very similar to things like a star search or various forms of tree search.
They do say they keep it simple right here and leave more advanced algorithms such as Montecarlo
tree search and all of that for later. So I hope it's kind of clear what's happening.
They themselves formulate it into two different things. You have a thought generator, that's just
generating one thought at a time. So one intermediate step of the problem solving that you ask the
language model based on the input and the previous thoughts, you just say, please make one intermediate
step, don't solve the problem completely, just make one little step and explicitly write down
the result of that step. That's a thought in parlance. So they say that's so we can either
sample or propose those, which means that we can go to the language model three times and sample
three times for three thoughts. Or what we can do is we can just say, please give me three thoughts
and then it outputs a list. Again, we might have to do IO prompting to get the correct
output format, but we can do that. They say one, for example, this one right here, you might want
to do if you generate some story or something because two samples are extremely unlikely to
be equal. This one right here is more appropriate when you have short proposals but they're more
constrained. So you want diversity, you don't want stuff to repeat itself. But in any case,
you generate thoughts by sampling and then you have a state evaluator where you simply ask the
model how good you think that is on either way, that's value or vote. You give it all the thoughts
that have been output and you say which one of these is the best and then you count the votes.
With voting, it might be a bit more tricky to do sort of the backtrack, like to compare nodes
globally. You might have to do another voting because in a different branch, the winner node
might have a completely different value than in this branch over here, but they're both winners
of their respective votes, but you can implement that in various ways. Again, they can mix that with
BFS or DFS, in what order do they consider things for expansion and as we said, we can also do that
globally. But for example, in a DFS, they would go, they would have some kind of maximum steps,
that's fine. They would sort all the candidates they have available. If one of the candidates is
above the value threshold, they expand it. So they go down, down, down until no node, like until
all of these nodes in our example were, like if all of these nodes are red, they say no node is
above the value threshold, so we backtrack. This is, it's not like a global expansion as
I mentioned it, I guess that would be a step further. So I hope the overall picture here
is relatively clear. Now, let's go to what they research this on or what they evaluate this on.
So this, compared to chain of thought, right? Chain of thought, you can implement in two ways.
One way is to explicitly always sample the next thought, but you might as well just
say, put out all your thoughts in one go and then give me the answer. So one prompt, one
sampling, because it's linear anyway, right? You just want the model to output a linear sequence
of things and therefore you might as well sample out at once. And even the self-consistency right
here, it's just sampling multiple times in parallel. Whereas this thing, the big difference
is that you actively have to stop after each step, like sample three thoughts, stop, evaluate
thought one, stop, evaluate thought two, stop, evaluate thought three, stop. And then you have
to decide which one is the best one. And then from that point on, you go to the language one again
and say, you know, here is input and one thought number one, now give me three new thought number
twos and then and so on. So this leads to a lot more evaluations, I want to say, of the language
model. But I'm pretty, I'm pretty excited that maybe in the future, we can just include this
into our programming language and say, you know, this piece here, I don't want to call like a function
that does something, I just want the language model to take care of this particular small part
and the rest I program around. And as we'll see, that's kind of the spirit of the experiments,
even though I think they they what they want to go for is like a general problem solver. I don't
think this goes into the direction of a general problem solver. I think this goes into the direction
of like, including it into programming. But the first, there are three tasks to evaluate on one
is this game of 24. You have four numbers. For example, these, God, these four numbers right
here. And you're asked to come up with a mathematical expression that results in 24. Right. So they
what they have to do is they have to prompt it explicitly, right, to say, Okay, give me possible
next steps. All right, here are possible next steps, then they parse that. And they have this
prompt right here, evaluate if given numbers can reach 24 into these things, again, with examples,
like with few shot examples. And then I, I think with few shot examples, yes. And then the model
outputs something like good, bad, impossible. So this here is the thought generation. And this
here is the evaluation. And yes, you can, you can reach like that, you can get the language model
to solve these things. And you can probably do them better. However, however, they
have to prompt it really specifically here in order to do that, which is fine for research, right.
But the prompts are really like, like you would program the algorithm, except that the one part
is really taken care of by the language model. But the prompts are so specific to the problems
that they almost and this is this is really. So here we have, okay, we have creative writing.
And in creative writing, you can see, it's not that big of a difference. Like if you, if you,
if you look right here, the IO prompting versus the tree of thought, like, sure, it's higher. But
this is, I think, GPT for evaluation, this is human evaluation, then sure, this one here is higher.
But it's like, not that much of a difference. And especially if you have these refine prompts,
then you get up there too. So it probably helps more in these algorithmic tasks. And this here
is like mini crosswords. So you have a grid of five by five, let's just do three by three for
demonstration purposes. And it's all letters. And it's a crossword puzzle. So you, for example,
here, you'd have a word, like, let's say you have ape, ape, okay. And then here you have a clue,
like, life form, I don't know, like, human, humans are, I'm terrible crossword, Q generator,
or something like this, more like animal with long arms that lives in trees. I don't, okay,
there's, there's cues right here. You know what crossword puzzles are. So, and then here is like
Po or something like this. And then it here, it says like poet Edgar, Edgar Allen. And
okay, so you have these cues. And you can input that into a language model and the language model
can output an answer, whether it's correct or not, right, that's the question, that's the task.
But you can clearly see the task would profit a lot from you being able to do this backtracking,
because you know, fill in a word that you think might be correct, and you fill in another word,
and all of a sudden you realize, ah, that doesn't work out. So you kind of cross out the ones again
that you previously filled and try some other ones. This is extremely handy, like in this problem,
like a backtracking research is extremely handy. That's why they evaluated on it.
They're not shy of saying, look, we evaluate the things on the task where we think it's going to
benefit. But that also should tell you that this method is probably going to, to shine well in
such tasks. And it's yet to be seen in like normal everyday usage, how well this does.
So you can see that in the input output prompting in the baselines, they say,
we provide five example pairs in the chain of thought prompt, we additionally include
intermediate words in the order of this or this, we run each prompt 10 samples and average the
result. So chain of thought prompt saying, you know, the intermediates are, you know,
the intermediate words right here, you don't have to output the full puzzle at one point,
just intermediate ones. And then at the end, give me the result. The tree of thought setup,
however, is much more intricate. So they use depth first with tree of thoughts,
keeps exploring the most promising subsequent word clue until the state is no longer promising,
then backtrack to the parent state to explore alternative thoughts. That's the DFS tree of
thoughts algorithm they presented above. They say, okay, to make search more tractable,
subsequent thoughts are constrained not to change any field words or letters so that the
TOT stat has at most 10 intermediate steps. For at each state, we translate all existing thoughts
for the state into letter constraints for the remaining cues, like this one right here,
and prompt a proposal five times to come up with candidates, what to fill in.
For state evaluations, we translate each state into letter constraints for remaining clues,
then evaluate for each clue if it is possible to fill it. And then, yeah, so what I mean right here
is that they help a lot, right? And in fact, which isn't bad. And, you know, the only criticism I
would have right here is that these things, all the, oh, we translate it into letter constraints
and so on. I guess it would be possible to help the chain of thought prompting like the baseline
a bit by doing that as well. I feel like that should be possible and I feel like for a fair
evaluation that should probably be attempted. But in any case, you can see they help a lot. They
help this process a lot to make it really sure that, you know, the model here, you know, you can
see like here it goes down this, but then the state evaluation says, oh, no, actually, this is
probably impossible to solve right now, not that you fill in these words. And you go back over here
and try a different route. So you can see this this salon here wasn't was probably not appropriate.
However, the fact that they help a lot, essentially, they implement the crossword solving algorithm,
right? They say in the text, hey, our goal isn't they say something like this. Our goal, we know,
they say we know that there are algorithms to solve this. The goal is not just to solve the
task as more general crosswords can be readily solved with specialized NLP pipelines that leverage
large scale retrieval instead of language models. So they say, you know, we want to do a general
problem solver that explores its own thoughts and guides its own exploration with deliberate reasoning
as heuristics. Yet still they essentially implement the crossword solving algorithm,
implementing all the research and the constraints and so on. And helping the language model to
such a degree that I think, well, they essentially just replaced the look up from the vocabulary
with the language model. So what we have right here is kind of a random word sampler,
because all of the rest is essentially implemented by the algorithm itself and by the constraints
they give. Again, this is not bad. But it does mean to me that the way I see this, as I said,
is in programming. So in programming, I could have my code do this, do that, do this. And then
here instead of calling a function like f, that function would not be somewhere in my code. But
that particular function would be sort of maybe a language model doing something somewhere. And
then I could implement something like a DFS or so, and try to call that function as part of it.
But I don't see this at the level yet of what I think they want to do. What they want to do is to
have this general thing that looks at its own thoughts and explorers and backtracks and so on.
But for that, in order to show that that's possible, the next step, the next paper following up on that
really has to go away from these explicit prompts for the problems and really has to do it such that
there is one prompt. I should be able to have one prompt initially, like one surrounding prompt.
I can describe the problem a bit, sure. But I should have the one prompt. And then the intermediate
steps, they should all be governed by one prompt, not by explicit prompts saying, hey, here is the
decoding constraints and so on. Now give thoughts about this and we parse it for you. Even with the
math problems, they like parse it intermittently and so on. None of that should happen. It should
just be one prompt that essentially generically says, consider your previous thought, you know,
how good do you think it is and so on. It could even be a meta prompt saying, if you were to ask
yourself how good this thought is, how would you evaluate it and then use that as a prompt?
But that really has to be the next step in order to make this a viable general problem solver.
Until then, I see this as a cool thing that we could use inside of algorithms, part of algorithms,
but that's a very different direction. What is good to see is they do a lot of ablations on,
you know, what the pruning and the backtrack and so on gives. So this here is the crossword
results. You can see the IO prompting and chain of thought prompting, they barely managed to solve
full games. They sometimes have word and letter successes, but not that many. The trio thoughts
obviously is much better. And if you, they say, this is pretty cool. They say, hey, if we heuristically,
if we oracle, we know which words go where, right? So if we always at valuation time,
so when the model criticizes itself and selects the best thought, if at that time we always tell
the model what the best thought is, then it goes up even more. Interestingly, it doesn't go up that
much, right? It just gets like that last bit to solve many more games than it could previously
solve. But the word success and the letter success rate don't go up that much. That's pretty
interesting. So it just kind of helps it to be extra consistent, right? And yeah,
if they take away the pruning, like if they never prune, or if they take away the backtracking,
so if they always just go down, never backtrack and go to another branch, you can see that the
performance degrades. Again, interestingly, it doesn't degrade too much in sort of the success
rate, it does degrade a bit. But okay, it does degrade, I guess, I guess the numbers are fairly
big. But the real killer is it solves a lot less total games. So the total games are kind of an
indication of if you make a mistake like somewhere, then you might get a lot of the words correct,
but the total game won't be solved. And so the total games is kind of an indication of how well
that planning and so on works. All right, that is what I wanted to say about this paper, all in all,
I think it is, it is pretty cool. It is definitely a good direction. I feel there's a lot that can
be done on top of this to make it more intricate. And I'm excited for a future where I'm obviously
excited for the auto GPT style that is thought of here. But I'm also quite excited for a future
where these these things are just part of algorithms, like classic algorithms mixed
with language models. I think that's an interesting world. Let me know what you think. That was it
for me. Bye bye.

A sphere made of Swiss cheese, a sphere with a texture of Swiss cheese.
And there you have it, beautiful, very appetizing Swiss cheese balls.
My Swiss heart had just skipped a beat out of these monstrosity.
What's even cooler than a sphere made of Swiss cheese is a torus made of denim.
These images are so cool, a torus made of denim.
And the point here is that these images aren't photoshopped or sort of human created, they
are AI generated.
And they are generated by this new model that OpenAI released a blog post about, it's called
Dali and what it can do is it can take a piece of text such as the one on top here.
The fact that I can select is simply the fact that they don't give you access to the model,
they just give you access of a bunch of things that they've tried, but the model can take
any piece of text and it can output a picture that matches that text.
So here you got a torus made of toothpaste and the quality of these images is super astounding.
And what's even more astounding is sort of the range of capabilities that this model has.
So the model can do various things such as, so in here the input is an illustration of
a baby diacon radish in a tutu walking a dog and you see an illustration of a baby diacon
radish in a tutu walking a dog and the outputs are just adorable.
These are generated by the AI, it's the same for an armchair in the shape of an avocado,
a storefront that has the word OpenAI written on it.
I've tried reverse image searching some of these images and I could not find them on
the internet so it's definitely not just a model sort of outputting an image it found
somewhere.
These are actually generated images and the astounding thing is that it's the same model
that outputs all of these different images.
It's not one model here trained on illustrations and one model trained on chairs.
It's a single model that can take in a piece of text and optionally part of an image or
none of an image and it will output the image either it continues the image you already
give part of or it just generates the image by itself.
So the model is called Dali and this is just a blog post for now by OpenAI.
They say they'll follow this up with a paper and if the paper brings substantially new
things I think I'll make a video on it but today we're just going to look at what this
model can do, how it works, how it probably works and we can take some guesses of what
we can read in the paper once it's out.
In fact OpenAI has brought out two new models along with this Dali model they've also released
a blog post and a paper about a model called Clip which is more of a sort of a classifier
not exactly a classifier it's sort of a it connects text and images in a different way.
It's not a generative model and we're going to look at that in a different video but you
can see the clear trend right here is that OpenAI is looking into connecting text and
images.
So they say Dali which is an this is a and I think an homage to Salvador Dali and mixed
with the character Wally.
So they say it's a 12 billion parameter version of GPT-3 so you know it's it's more like it's
more like not GPT-3 that was more than 10 times larger but it's a 12 billion parameter version
of GPT-3 trained to generate images from text descriptions using a data set of text image
pairs.
We found that it has diverse set of capabilities including creating anthropomorphized versions
of animals and objects combining unrelated concepts in plausible ways rendering text and
applying transformations to existing images.
So a lot of the things they don't tell us here especially the data set like how did
they get the data set nobody knows they don't say this they simply say it's a data set of
text image pairs and they sort of allude to the fact that they have large pieces of data
especially in the clip then they allude to the fact that you can just find data that
connects text and images on the internet and it's true if you search if you scrape the
correct websites and do it in sort of a smart fashion you can find a lot of data where there
is an image and there's a piece of text describing that image and we have to assume that they
sort of scrape the internet for something like this.
I don't think they have a lot of human explicitly human labeled data for this type of thing
so we'll just assume that they have like a huge data set and of course they train a huge
model on it a 12 billion parameter version of GPT-3 GPT-3 is the famous model the famous
text generation model by OpenAI and you can sort of see the same things right here.
So GPT-3 my hypothesis was that it sort of smartly mixes the training data rather than
remember the training data it sort of remembers it and then smartly interpolates between it
and I think you can sort of see the same kind of things right here in that these are all
definitely pictures that you could imagine in the real world but they have you know they
have for example their change to OpenAI in here there are surely chairs that sort of look
like this so it just kind of mixes a chair with an avocado in a plausible way I'm not
saying this to denigrate the model I'm saying that I mean this is seriously cool the fact
that it can do that so they say like GPT-3 Dali is a transformer language model now this is
very very interesting the fact that it's a transformer language model it receives both
the text and the image as a single stream of data containing up to 1,000 and 1,280 tokens
and is trained using maximum likelihood to generate all of the tokens one after another
okay this training procedure allows Dali not only to generate images from scratch but also to
regenerate any rectangular region of an existing image that extends to the bottom right corner
in a way that is consistent with the text prompt and they say a little bit more here on the right
and they also say a little bit more down on the bottom so I'm going to try to take a stab
of explaining how this model works with the full knowledge that I might be wrong once the paper
comes out and for that we have to go back a little bit and look at the models it draws from
namely the VQVAE so the vector quantized VAE literature so VQVAE will consider this to be
sort of the inspiration of or one of the necessary ingredients of this model so if we combine VQVAE
with something like GPT-3 we get Dali that's my hypothesis for today
why combining these two models so GPT-3 is extremely good at modeling language right
so if I have a piece of text let's go down here for a minute and let's say I have a cat set on
the mat a transformer will be very good at understanding the sentence and being able to
complete it so if I cross out this and ask a transformer to continue the sentence it will
be able to continue the sentence just fine if it is if it is trained well and that's exactly
how GPT-3 works now imagine that I don't have a piece of text but I have some sort of a description
of an image right let's say I have I have a box here is a box and the box which is going to be a
VQVAE can take in a description of an image in words but not exactly words that humans understand
but let's say there is an image language sort of like a programming language okay and you input
symbols into the image let's say it's a bit like Egyptian hieroglyphs maybe so here is the here is
the this this hieroglyph thing and then there is the sun the sun thing and then there is the
the tree the word for tree like the hieroglyph for tree and I input that here and the output
will be an image where I don't know what there the sun is shining yes I draw some like a child
it has a little smile okay deal with it and there is a tree maybe not exactly the tree from the
hieroglyphs but like some sort of some sort of tree that fits and then there is some human
in the scene maybe the human sits here the human sits at the tree you know relaxing
chilling okay so this now the image on the right is consistent of pixels right and
modeling pixels with a transformer is very very hard because in the case of our model right here
it's something like 256 by 256 pixels that would mean that transformer would have to
generate 256 times 256 which is like 2 to the 2 to the 16 this is just too much for a transformer
to model the pixels individually so there are multiple ways around this for example
modeling little regions right here which are not really satisfactory so what this model does is
it sort of it doesn't try to model the picture as such it tries to predict to predict these
hieroglyphs right here it tries to predict sort of a language that this box can understand and
produce a picture from okay so its task is going to be given some sort of a given some sort of a
text prefix so a human in a sunny field sunny day or on a sunny day chilling under a tree
so this piece of text followed so the the model is trained to take this piece of text
and output this sequence of hieroglyphs okay so this sequence of hieroglyphs outputting from
this piece of text and that's something a transformer can do if you have a vocabulary
right here so if you have a fixed list of hieroglyphs that you could use right so in there
there is the the human is in there that's a worse egyptian and then the pyramid is in here as well
some that you need some that you don't need so if there is a vocabulary the transformer is going
to be pretty pretty good at generating this thing so you need you need two parts the first part right
here is a transformer language model a gpt3 thing that can input a sequence of text and it can output
a sequence of text which is just in a different vocabulary namely this picture vocabulary and
then in the step two you need a box that takes in this picture vocabulary and actually produces an
images an image right here so as i already said this part is taken over by gpt gpt3 like the
the custom gpt model they built for this and this part is taken over by something like a vq vae
the generator part of it so what is a vq vae a vq vae is and you will be able to see that
um so the box that we are going to need is this box right here from from here up to where the image
is and this thing right here is going to be that vocabulary so what does a vq vae do it takes the
image here on the left you can see that here's the encoder it takes the image it encodes it into a
latent space now what a what a vae would do or what an auto encoder would do is it would encode the
image into a latent space and then it would decode it again into and try to reproduce the same image
and then you assume that whatever is in the middle right here is a sensible representation a latent
representation of that image right if you can train this model you're going to get some sort of a
representation in the middle that describes the image otherwise you couldn't reproduce the image
and there have been many models built on this concept now this model right here it turns out
that the classic auto encoder doesn't work too well um but this model works quite formidably
so what you're going to have is you're going to have this vocabulary right here it's also called
a code book let's call it a code book so the code book is also the vocabulary
so what you're saying is that um you can't just output any or any latent encoding so the
the encoder outputs a continuous vector but what you're saying is it has to be one of those
like there are a number of vectors that you have at your disposal um Mr. or Miss encoder or Mrs. encoder
there's a number of vectors that you have at your disposal you can only choose those
you can't choose any vector that you want right so in your latent space you can't just
choose any latent space there's this there's this there's this there's this there's this
you have to choose one of them and if you choose something in between which you'll inevitably
will, because this all of our neural networks output continuous values, we're just going to
clamp you, we're just going to find the nearest one in our codebook, and we'll just say, well, we
just make it such that you as if you had output that one. So the encoder can only hit one of
those codebook vectors. And then you feed these codebook vectors to the decoder. And the decoder
just decodes from these codebook vectors. Okay, and that turns out to be much, much, much better
than simply doing the auto encoder thing continuously. So imagine that this codebook
vocabulary is sort of like a vocabulary of image descriptions. What you do with an image,
you take this dog image, I'm going to have to draw this myself, you take the image here
of the dog, I can't draw dogs, I'm very good at cats, though. This is a cat. And you don't just
encode this into one of these words, you, what you do is you split the image up into a grid,
it's not as fine as pixels, it's fairly, it's, it's okay, large. So in their experiments,
they're going to use something like 32 by 32 grids, which is also what Dolly uses every image
is described by 1024 tokens, that's 32 by 32 tokens. And then you're going to encode, you're
going to make an encoder such that when this grid is through the encoder, this thing here
corresponds to one of the code vectors. And this thing here corresponds to another one. So you
have your big vocabulary right here. Right. And this is the red vector, this is the blue vector,
this is the green vector. And you're going to just describe the image regions with these codebook
vectors, like such. Okay, now the fact that is you have, you have a lot of these vectors, right,
you have in fact, you have 8092 vectors in Dolly. And the image only consists of 1024
tokens. So, you know, it's conceivable, like it's not like here where you have to reuse
the same token over and over again. But one of these tokens could, for example, be sky.
So maybe this is the thing that sort of describes sky. So what you'll have is like this thing and
this thing and this thing and this thing should be approximately sky, right. And then maybe the red one
is, is, I don't know, animal. And the blue one is vegetation. And the green one is some something
else. So you can see if you feed this to a model that has to make a picture from it, it can just
look at this. And it's sort of like a description, a low resolution description of an image is not
exactly a down sampled image. It's a, it's a description because these things here contain
a lot of information by themselves. Okay, it's just that you can't choose any vector in latent
space, you have to choose one of those vectors in the codebook. So that's a vector quantized
VAE. And they train everything at the same time. So they train the encoder and decoder with this
straight through estimator, because this nearest neighbor computation isn't exactly
differentiable. They also train the codebook to match the outputs of the encoder. So you can train
that, or you can just take the, the exponential average of the encoder outputs. And that's the
VQ VAE, which is developed more in VQ VAE two. So this is VQ VAE two, I've linked the papers,
VQ VAE, what's, I was writing a three, two, the version two of it does the same thing,
but in multi scale. So here you can see that in the encoder, you, you, you take the image and you
put it at multiple resolutions. So this is large resolution. This is low resolution. Then you use
the vector quantization to encode this into this grid and encode this into the codebook vectors.
So again, here, maybe I've read, read, read, this is red. And this is the green one and so on. So you,
each square has to choose one of these 8,000 vectors to represent itself. And then you do this
sort of hierarchical thing where you use the decoder on this level to produce a slightly higher
resolution image. But then you quantize again, and you use a decoder at a next level to produce
an even higher resolution image. So you can see that this hierarchical models, usually
these hierarchical models, if you want good high resolution images, you sort of need them. So you
can see that the, the top decoder here outputs something quite blocky. And then every, every
additional one adds a sort of details to the image. It's pretty impressive as such.
And you can see the training right here of the VQVA. These are, these are papers from last year
or the years before. So this has been known. What Dali does is from what I can gather from the blog
post right here. The images are pre-processed to 256 to 256 during training. Similar to VQVA,
each image is compressed to a 32 by 32 grid of discrete latent codes. Using a discrete VAE that
we pre-trained using a continuous relaxation. Okay, there's a lot of, there's a lot of
stuff here. So the VAE is pre-trained. And they're saying, they're saying also down here that their
model uses maximum likelihood to generate all of the tokens one after another. It's decoder only
and so on. So probably this whole pipeline here is pre-trained. They pre-train a VAE,
a discrete VAE. And then they simply, the Dali model simply has to learn how to produce the
tokens, right? The Dali model simply has to learn how to produce these hieroglyphs. And the box is
fixed. The box is not changed. It's possible that they also train the decoder here. So the decoder.
But I don't know, I can't tell this from the blog post. What's certainly is that they, what's
certain is that they don't train the encoder. So what you would do in a single step of Dali
is you would have your text right here, blah, blah, blah. And you would have a partial image.
Okay, you would input this text and the partial image to Dali. The partial image is any image
where you've blacked out the bottom right. And they do the bottom right simply, it's the same as
you do left to right by text. So you do sort of top left to bottom right. And yeah, it's good
because you can always flip an image, maybe not actually, but it's just a bias that you have to
provide the model with in order to do autoregressive training, right? So here is the image of that cat.
Right. And you black out the bottom right. You can black out the whole image if you
want the model to produce images unconditionally. All right, so you black all of this out.
Cool. So now what you do is these here, they are already, they are already words, right,
you tokenize those token, token, token, and you go into your vocabulary of text, right. So there's
a vocabulary of text somewhere, there's blah, and you encode all of these using that vocabulary.
So this is maybe word 34. So this is word 34, 34, 34. You go to your image, you
rasterize this according to your definition. Okay. And then you go and run this through
this encoder that you trained. So you run it through the box. And the box will tell you
for each of this grid outputs, the box will tell you, well, in my vocabulary of image pieces,
this here is number two, this here is number four, this is two again, this is 35, and so on.
So you do this left to right top to bottom, and then you put it right here. Okay, so this is
followed by an image of two, four, two, 35. And what you ask the model to do is simply to predict
from all of this. And the model knows that these are this is text and this is images from all of
this predict the next token, which would be this token right here. So you want to predict
this one right here. What is it? And that's how you train the model, right? And once it gets that,
you can train, you can ask it to predict the next one and so on. And in this way, you can let it
generate an entire image at inference time. And, you know, you can train this, they say all these
tokens are generated auto regressively. Now, in my understanding, this is all the model does,
because once you have that token, so if the model says this is number seven,
you go back to your box, and you say, please, or it's a different box, like this is the encoder.
This is the encoder of the vqve. And now you go to your decoder that you've also pre trained,
right? This is a different box. And you ask it, I have this image, right? I have
two, four, two, 35 and seven, please generate an image for me for that. Or maybe you want to
we want to wait until you have the complete image, right? So you have the complete image,
and you give this to your decoder. And these are now that these hieroglyphs, right? So you have
the box and the box produces an image. And the box says, well, okay, this cat here, probably it
reproduces the ears fairly well, because you can describe them sort of exactly, maybe you also want
to copy that over or something. But then it says, well, it's a cat. So I'm going to, you know, maybe
this, if the model has done a good job, there should be some sort of a cat, right? And the model,
you know, maybe in these hieroglyphs, it's even described how the cat looks like the cat looks
straight ahead as whiskers as eyes and so on. Okay, so I'm going to guess that the part on top
that is trained, and the part on bottom is pre trained with the option that the decoder part
could also be trained at training time. At the same time, they train this language model on top.
So they make some further inferences right here. They say, each image is compressed in
latent codes using a discrete V that we pre trained using a continuous relaxation.
We found that training using the relaxation obviates the need for an explicit code book,
EMA loss or tricks like dead code revival and can scale up to large vocabulary sizes.
And this is the part where I'm a bit confused. So clearly, they say they have a vocabulary
individual domain. Okay, there are 8192. Well, I'm don't know my powers of two,
8192 different words in the code book. So there must be a code book, but they say there,
this obviates the need for an explicit code book. So I don't really know what to make of that.
I can tell you what a continuous relaxation might look like. So this is from a different paper
that they linked of the concrete random variables. So if you have an operation such as this,
like a discrete random variable, you need to take an arg max of it. What you'll have is,
you'll have some sort of logits, right? There may be like this. And you take the arg max of it,
which means that you put it into a distribution where it's just one value. And this is sort of
the same operation as we do in the VQ VAE, where we assign each, each output of the encoder to the
nearest code book vector, we say you can only have one of the code book vectors, that's it, right?
Now, what you want to do when you relax this is you want to say, well, instead of that, what you
could do is you could just kind of take that code book vector a lot, but also, you know,
take a little bit of the others. So more than doing a hard assignment to a code book vector,
right? So here would be the output of your encoder and you hard assign it to the nearest neighbor.
You want to say, well, I'm going to soft assign it to all the ones, it's sort of like the difference
between K nearest neighbor and a Gaussian mixture model, as I understand, not, not what they do here,
but it's analogous to that. And with that, they don't need an explicit code book. And I don't
know what that means. What I can imagine is that they don't actually train the code book vectors,
maybe they just quantize to some prefixed schema, or I just don't understand what they do.
Here is an illustration of these discrete random variables. So you want to get to a point when
you sample the variable. As you drop your temperature, it more and more approaches this
fixed sampling, like you can be either here or here or here with the sort of masses that are
indicated by the size of the circle. But as you increase the temperature, you go more to a mixture.
So yeah, you can be at the corner, but you can also be kind of in this region or in this region
or in this region, as you increase the temperature, you can see the the distribution becomes more
of a mixture distribution. And the mixture distribution, any mixture distribution with a
temperature other than zero, of course, now, all of a sudden has sort of a defined gradient,
whereas these discrete random variables, they do not have a gradient. And that's the reason why
the VQVAE needs to do this straight through estimator right here, because this hard assignment
to the code book does not have a gradient defined with the soft relaxation, you do have a gradient.
And maybe they just mean they don't need, they don't need this hard assignment to the code book.
I'm not sure, or maybe they just they quantize in a different way, maybe they go back to a
continuous latent space. Yeah, I can imagine they might go back to a continuous latent space,
but somehow, somehow, they still do this a form of quantization, this could be a fixed quantization,
like you say, okay, you can choose any of the basis vectors and some some mixtures that we
define between them, or they define it via moving averages, or they define it via batch
statistics, or I don't know. If you know, let me know in the comments to the video. All right,
so this was my take on what the model does, and what is probably behind it. Now, let's look at
some more examples right here, because these are fun. So they, they say it can sort of control
attributes. So you see these, it's, for example, a pentagonal green clock. And you see, it's not
always pentagonal, it's sometimes hexagonal and sometimes heptagonal. And whatnot. But in general,
what it does well is sort of color, and also kind of object description. So launch box,
it gets and green, it gets what it can't do super well is stuff like counting.
So I have sort of a hypothesis, I have multiple hypotheses about here. Just see,
watch in all of these examples, how the text prompt is phrased. So it says a pentagonal green
launchbox, a green launchbox in the shape of a pentagon. This is quite unusual way to phrase
the prompt. And by the way, all these criticisms that I'm leveraging here, most of them are actually
admitted and discussed in this blog post. It's actually, it's pretty cool and pretty self,
let's say self critical of them. So it's, this is, I've, you know, I thought of these things,
and then I read the little text, and then they, they already describe what I concluded. It's sad,
but yeah, it's pretty cool of them, because the current climate is sort of make your research
look as, as cool and flawless as possible. This goes a bit against it. So they say that the images
here aren't cherry picked. And I totally believe this. So they have a little trick that they do.
They output, I think 512 images from their model because they can sample, and then they
re rank them using this other model that they've released this clip model. And this clip model
is a pretty good re ranker. So you give it a piece of text and an image and sort of tells you
how well they fit together. And so the outputs that you see here are re ranked by this model.
So what you see are strictly the best outputs, according to that model. So it's not cherry
picked by humans, but it's cherry picked by a very good model. And the second thing is that
the text prompt here is absolutely cherry picked, right? By the way, this is phrased, you can see
that it is very, very brittle, probably the model. I can't test it, but probably it's very brittle
in how exactly you phrase this text prompt. And I'm going to guess they have tried a lot of things
before they've released these few examples right here that they show. And they've, you know, made
sure that they work. So yeah, just keep in mind that this is very brittle. And we already know
this from like GPT three, we know that the input might seem the same to a human just phrased
differently in some cases. And yet the model will output completely different things. And we know that
a lot of these GPT three examples are very, very constructed in terms of the input prompt.
So yeah, the other thing is the model, as I said, it can do colors and it can do colors and and
textures pretty well. So we've already seen the things made of things. So the sphere made of
noodles that actually probably exists, the sphere made of guacamole. However, it's not super good
at counting, for example. And I have a sort of multiple hypotheses. So these image models,
they tend to be very good at sort of style and texture, style and texture are the domain of
these image models, like anywhere where there's like a convolution. And by the way, they use in
the VQ VAE model. No, not in the VQ VAE in this transformer for images, they don't do full attention.
What they do is each one of the image tokens can attend to each of the text tokens such as this,
but the image tokens, they can only sort of attend
in the grid layer by layer. In one layer, they can attend sort of to the row of other image
elements. In another layer, they can attend to the same column. And in even another layer,
they can attend to sort of the the surroundings of them like a convolution. So they can attend to,
let's say, their couple of neighbors right here. So it's not full attention yet in every layer,
every image token can attend to all the text tokens. Okay, so yeah, in these models, what you
typically see is that textures and style is pretty good. However, global correspondences are not as
good. And that's what you see a lot in these face models, where the left and the right earring
don't match and things like this. So global correspondences are not so good. And you would
actually expect that objects aren't as good as well, right? So here, this is still a clock,
this is still a light bulb. This is still a stop sign, right? So it somehow gets the objects
correct, which in my hypothesis, it shouldn't, because this is some sort of a global structure.
However, I think that's just a matter of how the data set is collected. The data sets are
probably we humans, we take pictures of objects, right? So the fundamental structures in these
data sets is the object. So it makes sense that it learns that we humans, we don't,
we don't take pictures and we often don't describe the count in them. So I can get that the model
has a harder time to learn that and actually focuses just on the object as a global thing.
The count would be a global thing, right? But it's not that prominent in the data. And the rest
is a local thing, like the color, the texture, and so on. Yeah, the cube made of porcupine.
So you can see here that this, this counting, so two is often quite good. And actually here,
it mixes up glasses and glasses, right? So two often works. However, if you go, if you go past
two, it often gets it wrong. So five, you'll get anything from three to seven clocks and so on.
So I'm going to also guess it's very brittle, like they're not here. Yes, they're sitting on
a table. But if you take a object that's not that often on a table, like a club,
you'll see that it's pretty unrecognizable whether or not it's on a table.
Five, four clubs. So, you know, the model is prone to ignoring part of its input if the
likelihood in another part is larger. Also, it can't do things like this, you know, a stack of
three cubes, a recube is on the top sitting on a green cube, it often gets the order wrong,
like it gets the cubes on top of each other. However, it often gets it wrong when it comes to,
you know, the order, the global things. As I said, anything global that is not what the
object is tends to be weak, anything local tends to be strong in these models. And that's
just a matter of how they're built and how the data is. So they say the image can render new
views. And here is where I'm not as convinced. So here you have like an extreme close up view of a
cabbie bar, sorry, of a fox. They're close up. Sometimes they're extreme close up, right?
You can see that it gets like forest, it gets pretty well. But then you say, okay, ground level
view, like, nah. And then you say, okay, an aerial view, maybe some of them are aerial views,
some of them aren't. What's pretty cool is things like a, okay, a fish eye lens view. I mean, that's
pretty cool. And a bottom view or a rear view, yeah, the rear view works better. So it does
understand these kinds of things, like what's the rear of a fox and what's the front of a fox,
though, as you can also see, not always. Texture, it's very good at texture. So here,
something made of voxels can do that perfectly. An owl made of voxels, like, this looks like it
comes straight from Minecraft, right? Absolutely, absolutely cool. Even x-ray sometimes doesn't
always get the bones right. But yeah, as I said, style, structure, very cool. So here is an example
of a completion. So they give the text prompt, a photograph of a bust of a homer, and the image,
the top part of the image. And they say, well, it can, describing a well-known figure, it can
complete the figure. I don't agree that it completes homer. Like, it completes, it probably just
sees this bust and this, and it just completes, you know, whatever fits. I don't, I have not
studied homer as a historic person or busts of him. But, you know,
I disagree that this depicts largely the same person very often. You can see here,
there is, sometimes there is even, you know, there's completely unrelated stuff. There's that lady
with the pearl earring by Vermeer somewhere in there, and so on. And what I also like in this
kind of, this one, you know, the game draw something or, you know, a picture and so on.
There are people when they can't draw something, they just kind of write it on the picture. It's
like, screw it. They'll just write it like, this is homer. This is homer. I don't care what you say,
this is homer. But, you know, it does, you know, it does. So when you say Cleopatra, it goes more
into the, into sort of the female direction Medusa. It has, you know, some, though I'm pretty sure
Medusa has the snake, the snake hair, you know, maybe, Venus. Yeah, somewhat, somewhat.
It, they test a lot of things like, can it do mirror reflections? And you can see right here,
they say it can do reflections on the ground pretty well, but it can't do reflections, for
example, in a mirror, because in a lot of these pictures, the object like here would actually
have to be in front of the mirror. However, in the fewest amount in of pictures, the object
mirror is actually also in front of the mirror. So this kind of global correspondence isn't
given as much. However, there is a fair bit of reflection on the ground, so to say. So, you know,
that's pretty cool, but it's also probably very, very common in data sets. Yeah, cross section view
of a walnut. So they sort of implore, sorry, explore the model, what it can do. And here,
you can see that, you know, if something is common in the data set, you know, like the
cross section view of human head, there are a lot of pictures of that, right, in the data set.
However, if it comes to cross section view of a, where did I see the airplane? There is an airplane
somewhere. It's less, it's less so, so you can see that this is still, it is, so here it probably
doesn't really know how that looks because, you know, they probably on the image on the internet,
even on the whole internet pictures of cross sections of airplanes or any sections of airplanes are
not really distributed often. So it sort of just focuses on airplane and then with cross section,
it probably knows that it should somehow display some of the interior. So it just kind of
produces some stuff that matches this thing. As I said, if it can't make the likelihood high
of all of the things, what it tends to do is just focus on one of the things and just make that
likelihood high, which is reasonable, you know, for a model, a macro photographs of stuff.
And these are pretty cool. This is what you would find in some image galleries. Absolutely.
Then it can do various things like style transfer. And here is where it shines, right?
So you can have different paintings of different objects in different styles. So here you can
like have an owl sitting in the forest in the morning. And you can have this as a painting,
as a painting in the pop art style and so on is very, very impressive. So I absolutely
glory actually to like as a postage stamp. These are these are these are absolutely amazing.
And yeah, you can have stuff like stained glass windows. And this is yeah, it's where the model
shines. And even here, a storefront that has the word open air written on it. So just right now,
just look at how convoluted this text prompt has to be for them to get this to work. It's impressive,
but the text prompt has to be repeated and reformulated a bunch of times and so on.
My personal favorite is the pie torch chips. They're crunchy. You get a piece of back prop
in every package. So you can see it sometimes misses like this is perch, perch chips and so on.
It sometimes misses, but it is pretty cool that it basically can do OCR, right? Or reverse OCR.
You can you give it a piece of text and it sort of makes a picture with that on it. It's very,
very impressive, even though as we said, like the global, the global correspondences are not
always there. They do implore like fashion, a skirt, like here that the yellow skirt.
Then, you know, these mannequins. And here they have a loft bedroom with a white bed
next to a nightstand. There is a fish tank standing beside the bed and they give sort
of the beginning of the image. And here's what the model comes up with. And, you know, you can
imagine that there are a lot of pictures like this in the data set. So the model might be pretty
good at stuff like this, though I have found their king bed next to, yeah, let's say the nightstand
with the telescope, the telescope beside the bed. It just, you know, that beside like there's a
telescope. Sometimes it's on the bed. Sometimes it's next to it. There are some weird telescopes
around. Well, this is a lot of telescopes. That's a weird telescope. But, you know, the quality is
pretty impressive. This is absolutely nitpicking that I'm doing here. Combining unrelated concepts,
we've already seen the armchair in the shape of an avocado. They also have a snail made of harp,
though my personal favorite is the penguin made of garlic. The penguin made of garlic.
This, perfect, right? Absolutely adorable. And just qualitatively like this, this would take
a human like you would pay a high quality, highly educated Photoshop artist,
quite a bit of money to get this sort of output, right? And these models, they shine
at this sort of style transfer texture stuff. And here, you have the illustrations.
You can have any kind of illustrations, like the illustration of a baby shark with a mustache
holding, there's holding an umbrella somewhere.
Playing, running, riding a unicycle. It's just, it's just nice. And as I said, this is the same
model that can do all of this stuff. And these are samples. They're just samples. They're not
cherry picked. However, they are re-ranked. Remember that. So they can do, you know, hybrids of images,
hybrids of different giraffe and turtle and so on. And they do sort of implore the model a little bit
more. Where they, as I said, they give this cat on the top and they say they want the exact same
cat on the top as a photo colored blue on the bottom. So you can see that it doesn't always work,
right? But in a surprising amount of times, it actually does work. Sometimes it's just like a
blue pot, but you can, you can see it's not the finished model yet. However, it is a step into
the direction that shows us that this is definitely, definitely possible. It can even do some of these
progressive matrices where it fills in the bottom right. However, they do mention it's very, very
finicky with respect to whether or not, for example, if you invert the color. So if you look at the
bottom right of any of these things, if I invert the colors, the output sort of changes, and it's
often also not right. However, sometimes it is actually right, which is crazy, because in some of
these things you have to do some crazy sort of inference that we usually, we usually do these
things in IQ tests. So I don't know the debate about what is intelligence goes on. They say it
has geographic knowledge. However, I'm not sure it has geographic knowledge as it just associates
words with particular images like they say, okay, this is a photo of food of China. Okay, maybe you
just, I'm not sure this classifies as geographic knowledge. It's, yeah, also this temporal knowledge,
a photo of a phone from the 20s, okay, you know, and then the different time period 60s, 70s, 80s,
future and so on, like distant future, like, wow, these phones. I particularly, so I like the,
usually this stuff, it's, it's pretty okay, right, but it's not temporal knowledge. It just
associates a bunch of tokens with some sort of style of computer. Today's computer, the future
computer, the distant future computer, please know, please, please, please don't give me that.
I don't want to, I don't want that. I love the action movie poster because
so the style is correct, but it just says action movie
in the future. Yeah, they do get sort of the kind of some of the style. It just says action movie,
like this is like a, like a naggy, naggy child, like, I'm hungry, hi, hungry, I'm dad.
All right, so they also have a summary right here, and they do show what it means that they,
they use this clip to re-rank. So on the left here, you can see just eight samples straight
up from the model. And they're not too bad, but you know, you increase the quality by sort of
sampling more and then taking the best eight as you go to the right here, according to the
re-ranker. So I'm going to guess they decided on 512 because that was sort of, you know,
it gives you already pretty diverse, pretty good, pretty high quality outputs right here.
All right, so just lastly, shout out to the, the, the authors right here. The primary authors are
Deter Mesh, Mikhail Pavlov, Gabriel Go and Scott Gray with a, I guess the secondary supporting
authors and most of OpenAI behind them, though I don't know how they work. I would encourage you
to go look at the model. It's pretty cool. Try out all these inputs. As I said, these are,
the inputs are simply restricting you because they don't trust you with their model yet, right?
In the real model, you can input any piece of text that you want and you will get out an image.
And the fact that you have to select the stuff here is simply because that's the stuff they tried,
that's the stuff their PR department has signed off on, right? And so, so you get to see that because,
as I said, they're not like, this is at the same time, this is a PR dilemma when you release a
generative model because it, you know, it could release, they discuss this a little bit in the
blog post, you know, it could release like very problematic images in a classifier. It's not
as pronounced. It's also sometimes dangerous, but not as dangerous as if you have a generative model.
That's the first thing. And the second thing is there is, I mean, there is money in this,
definitely, definitely money to be made in this. So, you know, we'll see whether or not we get
the full model or not. All right, with that, that was it for me. I hope you enjoy the,
the blog post. I hope you enjoyed the video. If you did, let me know, share it out, subscribe
if you haven't, and bye-bye.

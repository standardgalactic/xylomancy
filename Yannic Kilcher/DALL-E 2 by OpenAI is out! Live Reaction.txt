on you. Nice. Cool. Yeah, we'll look at Dalitu. It just came out 10 minutes ago. This is by
OpenAI. If you want to investigate yourself, this is just the OpenAI blog. I have not seen
this yet. So legit, this is the first time I'm seeing it as well. If you see something
cool, let me know in the chat. Also, I have no clue. New camera. Very wide angle. Makes
my arms really long. See? Really long arms. Excellent. All right. I mean, we'll just dive
in, right? So we'll read this together. As I said, this is the first time I'm seeing
this as well. So, yeah. New AI system that can create realistic images and art from a
description in natural language. That sounds interesting. And this is typical OpenAI blog
right where they just have their examples. Like, there's no way you're going to interact
with this. I guess. There's also a paper. So they don't even put this on archive, right?
Like, their papers, they're just on their own website. They're so famous. Hierarchical text
conditional image generation with clip latents. So I think, let's search diffusion models.
So yeah. So this is now no longer based. I think this is now based on diffusion models,
which was to be expected. Sorry, Estras, this is not live coding. We'll just look at this
new research that just came out. And we'll react to it in the moment. So an astronaut
riding a horse in a photo realistic style. Okay, so this can create original realistic
images and art from a text description can combine content concepts, attribute styles.
So this is what we're used to, I guess, from the image generation models, but this obviously,
so Dalí was always it's different than the clip notebooks and so on the AI art that you're used
to because those are all those are all mainly clip based. And then you put some kind of a model
in front of clip, and you optimize the whole thing. But Dalí is a forward. So you input the text,
and then there's a forward propagation. And then the result is an image. And that's
may or may not be filtered. So teddy bears shopping for groceries. As a one line, this is
insane. So this is what this model can do. And there there is a bunch of different images right
here. Now, these are, well, if they're not cherry picked, at least they're curated, right? So they
want to see that nothing, nothing bad happens when you look at these things.
A bowl of soup that looks like a monster. What is this, please, please, come on. Okay, this is insane.
Really insane, knitted out of wool, made out of spray painted on a wall. Wow. I mean,
yeah, it depends how cherry picked these things are. But this is really impressive. Now, I don't
know if you've seen, but there is kind of a new lion plus clip notebook or something, lion clip
diffusion. Let's just search for that. Because this is also is very like there's a new notebook.
And I've seen this around Twitter. Yeah, exactly. There's this new notebook. This is not by open AI.
This is a clip that has been trained on this open source data set. And when people combine this with
the kind of old methods of clip notebooks, then this is what happens. So this is like weird. It's
funny because it can spell, but it kind of it is like, it is not, it doesn't spell really well.
But if you, so there is this, this research here, and these are already quite insane in quality,
right? Latent and diffusion. I like, I love generative models. That's actually good.
A painting of a squirrel eating a burger. But now we're at this dali. So these seem to be very,
very much better. Now this model is based on diffusion, which means that it has some interesting
capabilities. If you know diffusion models, technically what they do is they are creating
their own data set in the sense that a diffusion model essentially iteratively refines a piece of
input in multiple steps until it reaches the output. So it's not necessarily one forward
propagation, although it depends a little bit on how you model it. You can have one
layer for each of the denoising steps, or you can have the same neural network and you pass the
input over and over again, maybe with some indication of what time step it is. So what a
diffusion model does is it takes an image and it just adds noise to it step by step. So it adds a
bit of noise and it takes that, it adds a bit of noise, it takes that until it is completely
garbage, like until it is complete noisy. And at that point, you can reasonably assume by some
limit theorem that this is now perfectly modeled by Gaussian noise or whatever noise you use.
And because of that, because you have a process that results in a defined distribution, you can
sort of make reverse inferences mathematically, which leads to some training properties that you
can then use to train these diffusion models. In practice, what that means is that you take an
image, you make it continuously more noisy, that's your training set. And what you train is the
reverse. So you train always to go from one step in this denoising process to go to the little bit
less noisy version of that image. So that's what you train. So this, this direction is easy, right,
to make it more noisy. But then you train a neural network to make it always one step less noisy.
And that essentially means that you can now sample the original thing, which you know is a
Gaussian. So you can sample from that. And then you can send it through all of these stages of
denoising. And at the end, you'll end up with a really nice picture. And you can do that conditionally
on text, which is what they do here. Now, given that this is what they do, they have some interesting
capabilities. Notably, they can edit images. So they can start with an image, and then edit that
image. And they can even edit only parts of that, because they can mask some of the parts, and then
simply apply the denoising diffusion process to that. So that's, that's what they do right here.
Now, I am going to, if you don't mind, just quickly put on Twitter that I'm live streaming right
here, and maybe also on Discord. I hope this opens over here. Yes. One second. Nope. Yep.
All right.
Just, so just writing the announcement, adding the little picture picture. And off we go.
All right. Yeah. So they can edit. So now let's see, make realistic edits to existing images from a
natural language caption. And this, I think, is what really will make a difference in sort of AI
art. What they say, we use diffusion models for the decoder and experiment with both auto-aggressive
and diffusion models for the prior. Is my mic muted? Is my mic muted? No, I think I just made the
Twitter announcement, and then I was quiet for 10 seconds. Sorry. All right. Let's see. So you can
make edits. So what they do is, here's the original image. And they select the original image.
They select a location where they want to edit. As I said, you can edit images, and you can do so
in a constrained way. And one is, you can select the location, and you can prompt it to add a
flamingo, right? That's the natural language prompt is to add a flamingo. And you select the
location where you want to add a flamingo. Look at these. This is insane. Look at these results.
Add a flamingo to this. Okay.
It's interesting that the flamingo style, right, matches the rest of the picture. It's not just
a flamingo. When it's inside, it's this sort of, what is this even? Is this a piece of furniture
or so? Or a sort of a swimming device flamingo. Yet when you select it on the water as well,
but when you select it outside, it's actually a real flamingo. So this is quite insane. It
doesn't just add a flamingo, it actually takes the entire picture into account. Let's say
add a corgi. Again, so this is the original image on the left, and then you select the location,
and the prompt is to add a corgi. Now, notice that the style of the corgi changes depending on
where it is, right? The left is, is this the, is this like the self-portrait of, is this the
self-portrait of Fungo? I'm not sure. But and if you add the corgi here, it's actually a real corgi,
right? So it's super powerful because it looks beyond like it, it a swimming device flamingo.
I'm sorry, I don't know what it's called. So yeah, you can see that this is, this is all a real corgi,
whereas where if you go into the, if you point into the image, it actually matches the style
of the, of the image drawn. So this is, this is super like, this demonstrates quite a bit
of understanding, I believe, at least like integration, integration of the entire thing.
This is going to be a paid service, isn't it? I look, I don't know, because the interesting
thing is they didn't even make Dolly into a paid service until now. And because they
withheld Dolly, we got all of this, you know, VQGAN plus clip and diffusion models plus clip
things, which is like, I don't think that would have happened if they were just to release Dolly,
even as a paid service. Let's see what else they have here. So we add a sofa. Yeah. So I think this
is going to be quite huge for, for making any, making any, any sort of visuals, like if, you know,
I need to make thumbnails for these videos and all. So
if you, if you do anything like this, this is essentially, it is a, let's say a weak, weak
form of Photoshop, but that's automatic. So if you, you know, you could, you could give this to
someone on Fiverr, and that's about what it would turn out, right? You could give the picture on the
left and do that. I'd like to add a sofa in the back corner there. Please do it. And that'll cost
you, I don't know, 20 bucks, 50 bucks or so on Fiverr. And this just works. So yeah, I'm excited
to see what they do with it because they do, they do have a, like investors and so on,
can take an image and create different variations of it inspired by the original.
Yeah. So this is, this is essentially different variations. I don't know what they prompted
with here. They don't say as far as I can tell, but maybe they just, they just,
maybe what they do is they, they sort of back propagate it, or they noise it a bit,
and then they de-noise again from the intermediate stage. So with these diffusion models, right,
you can make an image more noisy, maybe change it a little bit in some intermediate layer,
and then use the diffusion models, again, to upscale it. And maybe that's how they do these
types of things. I'm not sure they don't, they don't say right here. Maybe they say in the paper.
This is though fairly interesting. You can see that there's two, two people on the left. Is this
clipped? I'm not sure. I'm not an artist person thing. Image to text embedding to image. That could
be, that could be. But you can see that it is not, it is like the, the variations are really
keeping sort of the semantic content here. And I believe that's also one of the powerful things
of the, oh, that's one of the powerful things of training image and text together. Because text by
nature of being made for humans to communicate with each other, a text is much more on the level of
sort of the, I should say this, type of information and degradation of information that is important
to humans. So training image and text together, I believe just by the nature of having the text
in there will give representations that are much closer to what humans would consider different
or relevant or something like this. This is, I mean, this, come on, this is insane. Look at that.
Look at that. Wow. I think it's taking the generated embedding and then regenerating
the image from that embed. Yeah, yes, exactly. So they, they, oh, you, you believe, okay, maybe
they run it through the encoder. They don't actually noise it. That's very possible. Yeah.
Would this be possible with audio? I guess, yeah, you'd have to find a relevant
noising process. I'm not sure if Gaussian noise or so is really appropriate for audio. But
since this, since audio is a continuous, continuous signal, I don't see why this
wouldn't be possible with audio, especially if you train it with associated with text, right?
I mean, yeah, this is crazy. It would be really interesting to see if
sort of the representations of this stuff aligns with the representations of like real
pictures of birds or if they are way, way different. Because you can see here these style
variations, they all, they all depict the same subject and they are kind of in the same style,
right? It'd be interesting to see how much it can actually differentiate, differentiate that from
it. It's also interesting that here they have these, these colored boxes in there.
So do you think these might be the, is this, is this an artifact of what, is this like an
artifact of the generation process? Like, is this the, they sample auto-regressively?
Are they always the same? Yellow, light blue, green, red, blue. Yellow, light green, green, red, blue.
Can't tell what they are.
Look at that. I mean, yeah, this is like just a level of, the level of abstraction and, and
I'm not sure what people like, like Gary Marcus make of this. This would be
super interesting to hear because I mean, look at that. So there is, it is clearly not a natural
picture, right? It's not, yeah, it depicts a girl with headphones on the laptop, like in, apparently
in some sort of garden or surrounded with plants, overlooking a city in the night sky with the moon
and there's a cat in it. And all of that is in all of these pictures, however,
it's just different, right? It's a bit, look at, like, you can't tell me that this thing doesn't do
some degree of abstraction or it just brute forces all of this by training data, but I can hardly
imagine. So, you know, one second has learned the relationship between images and the text used
to describe them. It's used a process called diffusion, which starts with a pattern of random
dots gradually all alters the pattern towards an image when it recognizes specific aspects. So yeah,
as for people who tuned in late diffusion models, they create training data
by making pictures more and more noisy. So they learn to reverse that noise. Let's see.
Well, can you hear? You probably can't hear that sound, right? So, yeah, no.
No, okay. Let me quickly try. How about this?
Now I can't hear it. All right, one second.
Yep, this one.
I didn't think so. Dolly 2 is a new AI system from OpenAI that can take simple text descriptions
like a koala dunking a basketball and turn them into photorealistic images that have never existed
before. Dolly 2 can also realistically edit and retouch photos. Based on a simple natural
language description, it can fill in or replace part of an image with AI-generated imagery that
blends seamlessly with the original. It's called impainting. In January 2021, OpenAI introduced
Dolly, a system that could generate images from text, like this avocado armchair. Dolly 2 takes
the technology even further with higher resolution, greater comprehension, and new capability.
Here's a video idea. I built a real-life avocado armchair. Let's do that. That's a future video.
Promise. I should promise things like this. Okay, promise.
That's pretty far.
Not just look like an actual avocado.
If it's taught with images that are incorrectly labeled, like a plane labeled car,
and a user tries to generate a car, Dolly may create a plane. It's like talking to a person
who learned the wrong word for something. Dolly can also be limited by gaps in its training.
If you type baboon and Dolly has learned what a baboon is through images and accurate labels,
it will generate a lot of great baboons. But if you type howler monkey and it hasn't learned what
a howler monkey is, Dolly will give you its best idea of what it thinks it could be, like a howling
monkey. What's exciting about their approach used to train Dolly is that it can take what it learned
from a variety of other labeled images and then apply it to a new image. Given a picture of a monkey,
Dolly can infer what it would look like doing something it's never done before, like paying
its taxes while wearing a funny hat. Dolly is an example of how imaginative humans and clever
systems can work together to make new things, amplifying our creative potential.
There's a lot of hedging here, open AI, a lot of hedging against people, people are criticizing
you. That's good, good strategy. You just hedge ahead of time. You're just like, hey,
you can't do this, you can't do that. Howler monkeys are a species of monkey that indeed howls a lot.
Well, then it's correct, right?
Dolly too generates more realistic and accurate images with 4x greater resolution.
Okay, a painting in the style of Claude Monet of a fox sitting in a field at sunrise. Well,
that is indeed a great, great picture. Yeah, do you think these these things down here are the
sort of initial tokens for some autoregressive prior?
I'm not sure.
Okay,
which we currently do not make available available in our API as part of our effort to
develop and deploy AI responsibly. We're studying Dolly's limitations and capabilities with a select
group of users. Let me be the select group of users. Oh, no, I'd be terrible. I would be I would
be immediately canceled. They're making it harder to share without knowing where it's from. Well,
you can cut just cut off the bottom like that. That's
safety mitigations we have developed include preventing harmful generations. We've limited
the ability to generate violent hate or adult images by removing the most explicit content
from the training data. Okay, you also use advanced techniques to prevent individuals
facing those of public figures. Yeah, so you can you can do you can do some things, right?
You can do this at training date to time, which is where you just say I'll never expose my model
to bad things. And then it doesn't learn bad things, which I don't think it can work, but not
particularly well, because you're always going to miss some of that stuff like a lot like you're
going to miss most of that stuff, honestly. And therefore, so either you limit your training
data to such a degree, or you're going to miss most of the stuff, because the number of ways
that humans can come up with to do shady things and harmful things, whatever that means is it's
absolutely insane. So I trust humans to always generate things that you will miss. And you can
do it at training time. So you can if you have some sort of a classifier that that detects harmful
content or whatnot, you can discourage the generation like at training time of these things.
And then at inference time, you can obviously also filter simply, if it generates something
like this, you can you can filter that out. All of these things I find suboptimal, but I do understand
that if you want to release a product like that, you don't want to necessarily have it output.
It's already a stretch, right, for a company to provide a service where they themselves don't
exactly know what it outputs, which is crazy with these new APIs. So I understand. But again,
they told us GPT two was too dangerous to release. You know, you remember that when they told us GPT
two was too dangerous. Yeah, so curbing misuse or content policy does not allow users. Okay,
we won't generate images if our filters identify text prompts and image uploads them
with violator policies. Yeah, okay, that that makes sense. You just tell people like, don't do it.
But face development, face deployment, okay, previewing limited number of trusted users.
Well, I hope they release it at some point, not like Dali one. There's a system card,
we don't care about system cards.
Okay, that's the blog post. So for people who've joined later, this is, it's pretty insane what
you can do. So this is a text to image system, an astronaut playing basketball with cats in space
in a minimalist style.
Insane, insane, insane. Yeah. And what it can do also is edit, which is going to be huge,
right? Huge for, for anyone like if this is in some sort of Photoshop or or or GIMP or something
like this, new world opens for anyone who has to make stuff like a lot. I don't think people
realize, you know, how much you could, you could do with you, whatever, you just, you have a
presentation. And instead of having some stupid stock images, you just kind of generate something.
And then after generation, you edit it, right? The cool thing here is that you don't have to,
you don't have to
restart and figure out the prompt exactly. You can just generate something and then
edit at some point, you can say, you know, add this here, remove this, change it to this and so on.
And that's just and make some variations, right? You can just say make some variations.
And that I believe is going to be quite huge for a lot of people to just use casually if
that's how they intend to release it. And I'm very, be very cool to see if there is an open
source replication of that. Given how far we've come with clip, and with these new data sets,
again, for people who haven't been here, there is this Lyon 500m data set. And
people have been training a clip models on that. Oh, no, so Lyon 400m data set. This is
the old one. There's a new 5 billion one. And if you combine that with diffusion models,
you can get like you can get pretty insane things as well. And then if you combine that
with up sampling models, then you already have very powerful models. However, I don't know how
fast they are. So the stock images of today might be the basis of all pictures in the future. Yes,
pretty much. I think, you know, we are we are probably the generation who still makes real
images and uploads them somewhere. And before that, it was just not possible because the internet
didn't really exist in the capacity to support that. And after that, most images will just be AI
generated. So yeah, so that's where we're a special generation, the last real picture generation.
Okay, let's just skim through the paper a bit and figure out how they, you know,
a bit more about what they do. So here, Super Saiyan, Super Saiyan sentient bag of chips,
art station, it's, they're using that. Okay, okay. Controversial opinion, you know, given that
they are using essentially discoveries of the open source Twitter community that was explicitly,
explicitly a reaction to them not releasing Dali one. Do you think it's appropriate that they are
now using the tricks that that community has figured out to make cooler images in their again
proprietary system that they don't release and probably end up selling to you? Like
arguments for and against, right? But point of contention, point of contention. Like,
you, in my opinion, you know, go do all the selling you want. But then, you know, this is kind of the
art station at the end is a bit of a, that's kind of a thing of the, the kind of sharing open source
community. And yeah, not sure. Teddy bear on a skateboard in Times Square. This reminds me of
that movie Ted, no. Dolphin in a rest, a dolphin in an astronaut suit on Saturn.
This, this, yeah, this is 1024 by 1024. So these actually become usable, let's say in
presentations and whatnot. The 256 by 256 was pushing it a bit in terms of resolution. But
this actually becomes variations by encoding with clip and then decoding with the diffusion model.
Okay, okay. So this is again, this is combined. This is, is this, is this essentially clip based
diffusion? Okay, here it is. However, of unclip. This is okay, unclip might be the variation thing,
right? That's my friend, really? So is this the whole method or is this just where you make
variations of a thing? Variations between two images interpolating their clip image embedding
and then decoding with a diffusion model, we fix the decoder seed. Okay. So clip seems to play
an actual role right here, right? So they have clip here with the clip objective, right? There's
a text encoder and an image encoder, which they can give in. So this will give them some sort of an
embedding. And then that embedding goes, there's a prior. And then there is this deep diffusion
decoder. The other line we depict our text to image generation process, a clip text embedding
is first fed into an auto aggressive or diffusion prior to produce an image embedding.
And then this embedding is used to condition a diffusion decoder, which produces the final image.
Note that the clip model is frozen during training of the prior and the decoder. So there seems to be
three components, right? The first component is clip itself. That's above the dotted line here.
Hello, Philip Wong. Hello, Philip Wong. Hello, Philip Wong. Hello, Philip Wong.
That's above the dotted line here. Hello, Philip Wong. Good to see you here.
So the, and they already either have clip or they train clip anew, but they first train clip,
which gives them these embeddings of text and image, right? That correspond well to each other.
And then they freeze that. And then there's two more parts to it, which are these, this prior thing.
And the prior thing maps a text encoding to an image encoding. So the clip objective simply
makes them align, makes these encodings align. However, the prior actually transforms one
into the other. It's interesting that they need that. And that you can't adjust, have the,
well, I want to say have the text encoding be the start of the diffusion process,
but apparently they put this prior here. And maybe that models that for a given piece of text,
there can actually be a lot of different images that go along with it. But since the two are
probably already quite aligned, it might require some minor modifications to it. And then there's
the diffusion decoder, right, which we saw, you can train all of this probably from the same data
sets. So the clip you can train, obviously, and once you have clip, you can train the prior to
simply translate the text embedding that you get from the frozen clip encoder to the image embedding
that you can get from the frozen clip encoder. I'm not sure if that what happens, but yeah. And
then you can train the decoder likewise, simply by encoding with clip and then trying to reverse
that process. Although again, I don't know if that's actually what they do. So let's read into it.
Again, this is the I'm seeing this for the first time as well. So I'm as clueless
as you are. Does anybody have a link to the paper? You got to go to openai.com,
click on Dali, and then click on view research. A training data set consists of pairs of images
and their corresponding captions. Given an image X, let Zi and Zt be its clip image and text embeddings.
We design our generative stack to produce images from captions. So it's image from caption,
not the other way around. Using two components, a prior that produces clip image embeddings
conditioned on captions. So yes, so these are, these here are clip image embeddings, they're not
newly trained ones. So they, the prior translates the text encoding to the image encoding again,
which is special because the clip objective already aligns them during training. So that,
again, that might require some minor minor modifications to them. It's interesting that
they have to have it. And a decoder that takes in text captions, again, conditionally, and the
clip embeddings and produces image conditioned on clip. So this is not the same if I have this
correctly as like clip guided diffusion, because in clip guided diffusion, we take a diffusion model
and we feed that into clip, right? Let me, we take a diffusion model and we feed that into clip.
And then we try, we try to back propagate the clip loss to the diffusion model and try to
guide the diffusion model in this way. Here, it's different. Here, we actually
use the diffusion model, but we don't start from noise. We actually start from an embedding.
That's very interesting.
And I'm going to guess the, the, yeah, the prior translates to the clip embedding. And then we
start from that. That's very interesting. So the decoder, yada, yada, yada. Yada, yada, yada.
Okay, projecting and adding clip embeddings to the existing time step embedding.
So this is a variation of the glide model, projecting clip embeddings into four and
into four extra tokens of context that are concatenated to the sequence outputs from the
text encoder. We retained the text conditioning pathway. Yada, yada. But find that it offers a
little help. We enable classifier free guidance, which is also nice by randomly setting clip
embeddings to zero, 10% of the time and randomly dropping text caption 50% of the time. So this
is a conglomerate of sort of the recent advances by open AI and by the community.
It goes back from the decoder back into clip.
It kind of sounds like they just modified the clip embeddings and output via the image encoder,
zoom, but poop. Yes, exactly. They modify the clip embeddings via the prior first and then via
the diffusion model. Okay, the prior. To generate higher resolution images, we train two diffusion
upsamplers, one to upsample images from this to that and another one to further upsample them.
To improve robustness, we slightly corrupt the conditioning images during training using Gaussian
blur. Okay. Is this, I guess, so it'd be interesting to know if this is end to end,
these upsampling methods, because to really get good generations, right, the upsamplers
would need to also be informed of the, of the prompts and so on. Otherwise, the details are
kind of generic and the big structure is, which is usually what you want, but it's not exactly what
you want prior. While the decoder can, while the decoder can invert clip image embeddings to produce
images, we need a prior model that produces Zi from the captions to enable image generations from
text captions. So there is an auto regressive prior clip image embedding is converted into a
sequence of discrete codes and predicted auto regressively, or a diffusion prior the continuous
vectors, the eyes directly model using a Gaussian diffusion model conditioned on the caption.
Okay. That's cool, I guess.
For the diffusion prior, we train a decoder only transformer with a causal attention mask.
Embedding noise. Sorry, my reading style is not conducive to live streaming, I guess I'll adjust.
So here is how they train the loss. So the image embedding
is fed into this one, along with the time step and the text description. The F I'm going to guess
is a noising process or something like this. And then this is the target.
So this is now a, let's say true diffusion model, where we continuously noise what we want to achieve
and then we try to reverse that.
Variations.
We have given an image, we can obtain its clip image embedding and then use our decoder to
invert, producing a new image. To make the variations more similar, we first perform
this inversion with the decoder conditioned on the I.
Okay, so it seems like clip has a big part in this, which is a bit surprising, but not too much.
Because the original Dali just used clip as a sort of rancor at the end, whereas this one
uses clip intrinsically in the process. So it uses, if it gets a piece of text, it puts it through
the clip text encoder, which is frozen, right? And then it has the prior and the diffusional
diffusion. Exploring the clip latent space using the unclip decoder.
Okay, this is again the variations.
Variations of images featuring typographics attack, predicted probabilities across three labels.
Surprisingly, the decoder still recovers Granny Smith apples, even when the predicted probability
of this label is near 0%. So what they do, they project these images into the clip latent space
and then decode them again. Can it generate text? It cannot. I mean, you can probably generate text
as part of an image, as you can see right here. So this is an apple.
And even though the classifier is, what kind of classifier is this?
Okay, this is clip. So here you feed this image into clip. This was, I think, a famous
experiment. You feed this image into clip and you ask clip which of the text classes is the most
likely and clip says the iPod class is very likely. And when you actually project that and then up
sampling again, you can see that it's interesting because clip sees mostly an iPod, right? That's
the class is 99.98% versus Granny Smith 0.02%. Now, this is normalized, which is why I think
that this still works. So they say the surprising part right here is that it in fact retains kind
of the apple. It retains the Granny Smith apple, even though technically clip mostly sees the iPod.
But I think that's kind of, because it probably still sees the apple, but when you let it classify,
it's just that much closer, oh sorry, to the iPod text label rather than to the apple. But it
probably the visual information in here, like the encoding of that is still very much contains the
apple motive. I guess this is what they want to, this is what they want to demonstrate.
So funny how it can, this is the same thing, right? It's the same thing as this thing right here
in that it's kind of bad at spelling. Like it's so interesting that it can in fact
write text, even though it's an image producing model, it can write text in pixels,
which it's really bad at spelling.
Yeah, why? Nobody knows. Okay, they spend a whole bunch of time on this prior right here.
Clip image embeddings, reconstruct them with progressively more PCA dimensions.
Okay, so we take the image on the right, I'm going to guess, and we project it into the clip
late in space. And then we only retain the top principal axis or the top principal components.
And then we recreate them again. So there is, you can investigate a little bit what is kind of
important. So it seems, let me try to make this a bit bigger. It seems like the,
you can see the detail gets added as you go up. So this here is kind of a scene from food.
Here you have a city that's even might even be, I don't know what city that is, I'm sorry.
And the bottom one, a bunch of cows, this could be Switzerland.
Oh, we know. So, yeah, as you would expect, it first tries to model kind of the general
framework, general setting, it seems to match the colors, even a little bit.
It's like the colors, general setting, what is it? So that's really interesting, right?
That means the sort of highest level information that it has really refers to kind of concepts.
It does refer to color, which, and things like this, but also to concepts, right? There's no
view of a cow here on the left, or the specific city on in the middle right here, it's just like a
city landscape, which is really interesting. Because a lot of models, I think they would
much more try to model everything at once, but in less detail, right? And if you go down with
the principal components, they would try to just have color blobs everywhere, where they're relevant.
And you can see as you go up, as you add principal components, what's also interesting is that as
you add principal components, the scene can radically change, right? So that's also quite
interesting. So here we can, on the top row, you can see that we now add, we add tomatoes into the mix.
So where it went from a general, or at least gnocchi maybe, so from a general Italian kitchen
setting, it goes more and more into this direction. And it ends up in something that's fairly similar
to what we're seeing, right? And the same here on the bottom with the cows even.
And you can even see that a little bit, they have even the same, same positioning, but I'm not sure
if I'm over interpreting this right here. You can also see that you have the general mountain
landscape down here. And then more and more, it kind of refines to the maybe a bit more correct
colors, a bit, also the background gets a bit more accurate.
Yeah, this is, it's just quite, quite cool.
And as I said, this is just a lot of evidence for the fact that these things have
some sort of abstractive representations of what's happening.
Sample is using different conditioning signals for the same decoder.
We passed in the first row, we passed the text caption to the decoder and passes zero vector
for the clip embedding. In the second row, we pass both text caption and the clip text embedding
of the caption. In the third row, we pass the text and the clip image embedding created by
an auto aggressive prior for the given caption.
Note that this decoder is only trained to do the text to image generation task without
the clip image representation. So they have a lot of inputs, it seems, into their decoder.
So what is the first one? We pass the text caption to the decoder. So there's no text
embedding from the clip decoder or anything. There's only text caption, which it also conditions on.
A group and an oil painting of a Corgi wearing a party hat,
hedgehog using a calculator, motorcycle parking and parking space next to another.
Yeah, so this, as you can see the top row, it picks up kind of on like one word or so out of the
the, is this intelligent? Like we should make the meme with the butterfly.
But as you can see, it kind of picks up on like one word or so out of the
text. And then it just creates kind of a realistic image that contains that word. It doesn't have
the capability of fusing different things and so on. And I'm not sure what that is,
because they say it's the same encoder. And that has been trained with various inputs.
And they just pass a zero vector for the clip embedding. So they kind of stifle the input.
It's not, it's not a true comparison in the sense that they have actually trained the decoder to only
work with this type of data. So I'm not sure what this means right here. It could mean that clip
embeddings are really important. It could also just mean that you, you have to actually
train the decoder with this because you just stifle it from some inputs that it doesn't have.
Now, if you add the text embedding from clip, you can see that it already gets a lot more capable
so a hedgehog using a calculator. Yeah, there is a hedgehog and there is someone I guess using a
calculator motorcycle or this wire metal rack holds several pairs of shoes and sandals.
I guess, I guess not really. And then as soon as you add the image embedding as well.
So that's the image embedding of created by an auto regressive prior for the given caption.
Okay, so the the prior will create the image embedding. So this is the full system at the
at the bottom except having a clip image embedding that comes from some sort of image instead of from
the prior. Yeah, it's difficult to say what this means, honestly, because they just take away
stuff. And obviously, it's not gonna, it's not gonna go as well.
So the classifier free guidance is, I don't actually have it in my head exactly what it is,
but it's like you maybe you have a classified, I don't know what they use as a classifier here,
maybe clip itself. And then you can mask some stuff and then you have the direction of where
you want to go and then you just go more into that direction. But I'm, I don't have it on top of my
head, what it is. Okay.
Glide, I've done a video on glide. So I don't remember myself too accurately what it does, but
I love how the how the different papers continue this this work, right? So
dally, glide, make a scene and then unclip green train coming down the tracks. Crazy.
Yeah, this goes into the direction of GANs, which
just have become so good at this point that optimizations are micro, we're still a few
ways away from there, but it's getting there. Samples from unclipping glide for the prompt.
A red cube on top of blue cube. Yeah, pretty good.
Reconstructors from the decoder for difficult binding problems, we find that the reconstructions
mix up objects and attributes in the first two examples, the model mixes up the color of two
objects. The right most example, the model is not reliably reconstruct the relative size of the two
objects. Yeah, so probably, you know, given this might also be a property, right, of the of the
text descriptions, maybe not the binding problem as much, but the relative sizes, you rarely have a
text description that describes the relative sizes of things. And therefore the encodings might just
end up with sort of what's contained in the image. And then that's what the models learn that, okay,
what's mostly important is what's contained.
A sign that says deep.
Deep, deep. This is just deep, deep, deep. Come on.
High quality photo of a dog playing in a green field next to a lake.
Unclip samples showing low levels of detail for some complex scenes.
Okay. Yeah, so
Dolly two, oh, Dolly two and unclip seem to be the same model, right? So that just, because I was,
I was wondering, I was wondering sort of, because they kept talking about unclip,
but I do believe that Dolly two is unclip. Oh, correct me if I'm wrong right here.
But here they describe it. Yes, this is unclip. That's Dolly two. This thing right here.
So, yeah, I don't, I don't know what to, what to think of this. It's certainly super impressive,
what it can do, right? If you, if you look at, oh, there's joint waitlist. Okay, okay, we'll do that.
It's certainly super impressive, like the applications of this are, I believe, quite far
reaching because this allows more and more, yeah, it decouples, I think, the mechanical,
I've already said this, I think in the past, it decouples the mechanical skill of making these
things happen, like the Photoshop skill or the painting skill or, or Nunchuck skills,
fighting skills. It decouples the mechanical skills from, or it decouples them from the
creativity part. So usually you have like, okay, I want to do something. And then the second thing
is, okay, I have to have to actually implement it. And this thing decouples it, it kind of takes
that mechanical aspect away from you. And therefore, you're left to focus on what should happen.
And I think with these editing capabilities that it can do and the variation capabilities,
this could be very powerful. If built into, if built into GIMP,
and then in GIMP, you can just say, okay, enter text, a piece of text, create an image, okay,
edit here, enter a piece of text, make some variations on this thing right here, select a
bunch of stuff, very, very powerful. And I guess a lot of people would pay a lot of money for it,
which might be the end goal here. All right, I hope you enjoyed this kind of live reaction,
live reaction to this type of research. I'm excited to see if this actually ever gets out here.
But yeah, let me know how you like this type of stuff if we do live reactions in the future,
again. And yeah, I'll, I guess I'll see you around. Thank you very much for being here.
I'll stop the stream now. Bye-bye.

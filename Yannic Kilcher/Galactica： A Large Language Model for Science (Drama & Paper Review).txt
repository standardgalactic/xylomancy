Hello, this video starts out with a review of the drama
around the public demo of the Galactica model
and then goes into a paper review.
If you're not in the mood for any drama,
skip ahead about 16 minutes and you'll be fine.
Hello there, Galactica is a model,
a language model by MetaAI
that is trained specifically on scientific text.
Now this is a generative model, so it can generate stuff
and thereby it can do a lot of things.
For example, as you can see right here, citation prediction.
You give something in and you ask it to predict a citation
and the citation in this case is correct.
This is not trained to predict citations,
that just happens by means of it being trained
on scientific text.
There's also, for example, this here,
translates the math formula into plain English
and there is plain English over here.
Now the model can do so much more.
The point of the paper is actually to say that,
look, these models, we don't have to train them
on these huge corpora of text.
We can reduce the corpus size,
but if the corpus is well curated, qualitatively higher,
then there might also be a benefit in that.
It might be a trade-off between giant corpora
and small corpora that are of higher quality.
Now the other thing about this paper
is that the model is released fully open source
and they even had a demo up.
But as you can see right now,
it just says, thanks everyone for trying the demo.
Now I've tried the demo for a bunch of things.
It was really funny.
You can make some fun stuff.
You can also make some serious stuff.
In fact, Galactica was used to write the paper
that we're going to read in just a second,
but the demo was taken down.
And despite here it seemingly being like,
you know, this is just a fun thing
that we wanted to take down anyway.
Probably, probably not.
Yandere Khan on Twitter gives a little bit of a hint
of what happened right here.
Pretty much exactly what happened.
Well, what is this?
People started complaining as they do.
Gary Markisier says,
the rapid removal of MetaAI's Galactica demo
represent a tacit acknowledgement
that it was released too soon
and deeply problematic.
Of course, problematic.
The word that you can throw at anything.
And contrast strikingly with Yandere Khan's
untenable public defense of the project yesterday.
Someone answered, or maybe it was removed
because people like you abused the model
and misrepresented it.
Thanks for getting useful
and interesting public demo removed.
This is why we can't have nice things.
To that, Yandere Khan answers
pretty much exactly what happened.
Meta huge props to getting this model out there.
The model is still available.
Also getting the demo out there
for people to just try it.
And yes, people tried it as it was intended
and people tried it as it wasn't intended.
A lot of funny stuff was done.
And also someone might have entered a bad word.
Oh no, oh no.
But people pretty quickly started obviously to complain.
The professional complainers
and the people who think they know what's good for you
obviously were all over this.
So Michael Black says,
I asked Galactica about some things I know about
and I'm troubled.
In all cases, it was wrong or biased
but sounded right and authoritative.
I think that's dangerous, dangerous, dangerous, right?
Here are a few of my experiments and yada, yada, yada.
So here he tries to justify why dangerous Galactica
generates text that's grammatical and feels real.
This text will slip into real scientific submissions.
It will be realistic but wrong or biased.
It will be hard to detect.
It will influence how people think.
You catch the step, like it produces text that feels real.
This text will slip into real scientific submissions.
Like how?
It just will.
It just like no one has a part.
And it just like the model exists.
Therefore, text in scientific submissions.
By the way, humans can also do like bad stuff.
Humans can also lie and plagiarize
and write grammatically real but wrong things.
In fact, the literature is littered with wrong math proofs.
Not even intentionally wrong.
Just like they look right.
There are essentially two or three kinds of people.
There are the people who think we know what's good for you
and therefore we must be the guardians of all the models.
Then there are the people who just dunk on everything
and then there are in general,
the professional complainers who just throw words at stuff
because that's what they do.
They don't like not being asked.
They don't like power not being centralized.
For example here, Facebook, sorry, meta AI.
Check out our new AI that lets you access
all of humanity's knowledge.
Also Facebook AI.
Be careful though, it just makes us up.
Why the jab here?
Like one must be like really sour to make this jab.
And this tweet actually goes on.
So down here, these are the initial criticism.
Obviously shilling, you know, your own work a little bit
about this topic and the works of friends.
And then it goes on and says,
and let's reflect for a moment
on how they phrased their disclaimer.
Shall we hallucinate is a terrible word choice here,
suggesting as it does that the language model
has experiences and perceives things.
I'm not sure that anyone misunderstood the use
of the word hallucinate right here,
but whatever we can throw at it, whatever.
And look at this.
And on top of that, it's making light
of a symptom of serious mental illness.
Whatever, whatever.
Like just, just grab into the bucket,
take some insult and just throw it.
Why the complaining?
It has a disclaimer,
never follow advice from a language model
without verification.
People are just gonna disregard it.
People are just gonna be like,
the language model says I must do something.
So I'll do something.
Look at me.
I just write a paper.
Oh no, the language model says something.
I must submit this.
Grady Butch says Galactica is a little more
than statistical nonsense at scale.
Amusing, dangerous, and in my holy opinion, unethical.
Unethical and dangerous.
Yanlacal says, come on,
is your predictive keyboard dangerous and unethical?
Is GitHub copilot dangerous and unethical?
And so on, because they're exactly the same.
It's like a pen, unethical,
because you can write the bad word with it.
No, there is a clear mediator in the loop.
The human who has intent can easily accept
or reject the prediction.
What?
What?
So it's now two days later
and the discussion is still raging on
with Yanlacal asking, who has Galactica heard?
What if actually it helps scientists
write papers more efficiently and more correctly?
Particularly scientists whose main language is not English
or who don't work in a major research institution.
And yes, from experience, I can tell
that type of scientist would greatly,
greatly benefit from a tool like this.
No, they wouldn't just take the output
and slam it into a paper and upload it on archive.
They would interact with the tool
in order to come up with a better research paper.
And in light of all of these benefits,
present and future potential benefits,
it is very fair to ask, who has this actually heard?
What's the actual danger here?
As reasonable people, we should be able to debate
the pros and cons of such a technology
and of the technology being just given to people
instead of just being kept under we know what's good for you.
And it's not all like dandy that comes out of this,
not all correct what comes out of these models.
Here is the getting a girlfriend algorithm,
which will probably not be a good fit for an archive paper.
There's also other stuff like here is a research paper
on the benefits of eating crushed glass.
And people have gotten even more inappropriate stuff
out of this model, which is not a surprise
because these models are very good and very competent
and they are very agreeable.
So if you ask them to do something, they'll probably do it.
Yet still, the fair question is, in what scenarios
would this type of generated text actually be harmful?
And here's the point.
These people react with just astonishment to this question.
It's just like, oh, I can't believe it.
Oh, no way.
Unflabbergasted.
Jesus Christ, ha ha ha.
Dot, dot, dot, dot, dot, dot.
Incredible.
These people are so used to being able to just make
the accusation and then they get their way
that they can't like the someone asking them
to come up with a reasonable argument
that in a neutral way discusses pros and cons of something.
It's just so out of their world because in the past,
all they always had to do in the recent years
is say a word like harmful or problematic.
And if they said it long enough and loud enough,
magically things would go their way.
People would take down things.
People would change things so that they get their wishes.
And now if someone actually asks them,
they don't know what to say.
They're just so astonished that someone might actually
want to know pros and cons of the stuff.
And yes, of course, Jan Lecaux is now clearly
unqualified for his position because he asks
what the actual harms are.
It's incredible.
And I think we're all responsible for the climate
like this because even now, META or whoever hosted that demo
took it down in response to the public pressure.
So the people were loud enough and they were mean enough,
essentially, that the PR people at META and the lawyers
or whoever made the decision took down the demo.
And that is one more reinforcement
for this kind of behavior.
And everyone seems to be afraid of some boogeyman
that being accused of a bad word automatically
means that everyone else is going like, oh, no,
I'll never do business with you again.
I mean, to a degree, that is true.
But I would argue that the solution
is that we all collectively stop making such a big deal out
of a few flimsy big word accusations like harmful
and problematic and actually discuss in neutral terms pros
and cons of technology and to find the best path forward
that brings the pros to as many people as possible
while limiting the cons.
And no, that is not always going to be the approach of,
we know what's good for you.
Let's keep it all to ourselves and you come ask us
whenever you want something, you peasant.
All right, back to Yannick in the past.
I think the complaints are very unreasonable.
I think the people who make the complaints
know that they're very unreasonable.
And I think this is either a clout game or a power game
because things are out there, they're no longer centralized.
In any case, I decided to look up actually
early criticisms of the printing press.
And what do you find?
Here is a record from a conversation
that Johannes Gutenberg, the inventor of the printing press
had with a monk and monks used to copy text by hand, right?
And now the printing press came along
and essentially brought that to everyone.
So Gutenberg says, I want to help men and women
to be literate, to give them knowledge,
to make books so cheap, even a peasant might afford them.
That is my hope.
Yes, this is strikingly similar to what Metta
wrote in this Galactica paper.
The monk says, the word of God needs
to be interpreted by priests, not spread about like dung.
We know what's good for you.
I do not wish to spoil the word, but it will happen.
This is 500 years ago.
And the exact same conversation repeats and repeats
it will happen magically, right?
To hand it out about to all and sundry is langurous.
Would you have plough, would you have plowmen
and weavers debating the gospel in taverns?
Oh no, the common folk, the common folk, get it.
That's terrible.
If that is what they want to do.
So up until here you saw, we know what's good for you.
And the second thing is always, it's dangerous.
It's problematic.
And the head monk says, but what of the dangers?
It would be like giving a candle to infants.
Such copies we make of the Bible would first be monasteries
for monasteries and churches.
The head monk says, the Bible, you plan to make the Bible as well.
Oh no, you have ambitions.
I've considered it and obviously he did.
And obviously I like, you can one to one, one to one.
You can take every argument that people make against this
and you can put it on a predictive keyboard.
You can put it about the pen.
You can put it about the printing press and people have done it.
This is 500 years and every time it was just dead wrong.
Every time the new technology improved our lives drastically.
Yes, email leads to some Nigerian prince scams.
Yes, some people get hurt by it.
But email has been a definite benefit for our world.
No matter what you think right now with your 5,000 unread emails
in your inbox, it is a benefit to the world.
And it's the exact same thing over and over.
Enough though of that, enough of me ranting.
Let's go into the actual paper.
The paper is called Galactica, a large language model for science.
It's by Meta.
And I already told you that it is a large language model
trained on scientific text.
There's actually not too much to it.
We'll go quickly through the paper and see a couple of special things.
But in general, this is a, let's say straightforward work of research
into what it means to have more quality data
instead of more quantity data.
They say here, we train on a large scientific corpus of papers,
reference materials, knowledge bases, and many other sources.
We outperform existing models on a range of scientific tasks.
Despite not being trained on a general corpus, Galactica outperforms
Bloom and OPT175 on Big Bench.
Big Bench is a general benchmark for language models.
And this is where it gets really interesting
because this, the Galactica model is trained
on a very small subset of data.
And yet it outperforms these much, much more holistic models on that task.
So that is a definite argument for data quality instead of data quantity.
We open source the model for the benefit of the scientific community
and much to the detriment of, I guess, Meta itself.
Although let me say what Meta should have done.
They did so much right.
They outsourced the model.
They made the model available via a demo.
And now the only thing left to do is to actually have a pair of balls
to tell the people who come and to say,
oh, look, I got the model to produce something bad to tell them,
well, yeah, that's what happens sometimes.
And it is not dangerous.
It is not problematic.
It's just a language model.
So Meta, next time, have some balls.
Just tell the people to f off and you'll be fine.
All right.
They say in May, an average of 516 papers per day were submitted to archive.
It is impossible for a single person to read all the papers in a given field.
And it's likewise challenging to organize data on the underlying scientific phenomena.
They say the volume of scientific research has become too large.
And what we used to do is we used to search engines.
Search engines are the current interface for knowledge,
but they do not organize knowledge directly and instead point to secondary layers.
So with a search engine, I can only find stuff.
I cannot integrate stuff, synthesize stuff,
or even come up with the stuff that I should search for in the first place.
They say if you want to do a literature review, that still has to be done by a human.
If you want to do a summary, that still has to be done by a human
because our tools are just not powerful enough.
And the Galactica is the first step at building a tool
that can assist humans in doing these types of things,
searching for things, synthesizing things, integrating things,
and maybe suggesting new things.
They say unlike search engines, language models can potentially store,
combine, and reason about scientific knowledge.
They can potentially find hidden connections between different research,
find hidden gems, and bring these insights to the surface.
They could synthesize knowledge by generating secondary content automatically,
such as literature reviews, encyclopedia articles, lecture notes, and much more.
And they also talk about the benefit of having different modalities,
linking papers with code, protein sequences, with compounds,
theories with late tech, and much more.
Our ultimate vision is a single neural network for powering scientific tasks.
You know, it doesn't say do scientific.
It says powering scientific tasks.
And that is also my ideal end goal.
If I imagine a cool future where AI tools are abundant,
I would want like an extension of my brain that I can interact with
and that empowers me as a scientist.
And I would still be able to actually make the decision
of whether to accept the output of the tool or not.
They say we introduce a new large language model,
sorry about that, called Galactica, to automatically organize science.
This includes over 48 million papers.
This is their dataset, textbooks, lecture notes,
millions of compounds of protein, scientific websites, encyclopedias, and more.
Our corpus is high quality and highly curated.
And it is a lot smaller than the usual corpora of the large language models.
They format all of this into a common format.
Their common format is marked down.
And then they take a lot of attention of how they do specific scientific things.
For example, citations.
They use a special token that allows a researcher to predict the citation
given any input context.
They also have a very interesting way of handling step-by-step reasoning.
They have a special token for that that mimics an internal working memory.
Let me look at these two things in just a bit.
The interesting thing is, for example, with reference prediction,
so citation prediction, they say,
importantly, we find this approach outperforms tuned, sparse,
and dense retrieval approaches for citation prediction.
So the generative approach is better at predicting a correct citation
than search engines, even tuned dense retrievers, like neural retrievers.
This is also really interesting.
So, again, for all the people who argue that, oh, no, wrong stuff will end up in the papers,
probably right now you're using a search engine to find your references.
And if you distrust the human ability to accept or reject the output of a tool so much,
then how come you don't distrust your ability to accept or reject based on search engine outputs?
Not sure, but these things are better than search engines, so you should use these.
Most interestingly, Galactica was used to help write this paper.
Oh, no, we are doomed. We are doomed.
Okay, so here's the corpus.
You can see that there's a bunch of data sources.
The most data comes from papers, about 83% of tokens.
The total size of the corpus is 106 billion tokens.
As I said, that is a lot smaller than some of the large language model training runs that we are used to.
A lot of other sources are also code reference material, knowledge basis, filtered version of common crawl,
just 1%, prompts, which they generate or include, and here other is other.
We might see a little bit of what other is.
The tokenization is very interesting. They need to bring all into a markdown format.
This isn't super surprising, but it goes to show that if you do something like this,
it actually matters quite a bit how you do the tokenization, how you represent all the knowledge in a common format.
I believe, at least from what I can estimate, they've done a lot of thinking, a lot of work into this direction.
They also mentioned that they've tried a bunch of different things and just picked the ones that's best.
Notably, citation, again, they have start and end ref tokens.
They would write a text, yada, yada, yada, then the start ref token.
Then here is the citation as text form, not as some reference form.
The title of the paper and the author name, and then here the end ref.
In this way, you can just feed it into a language model and have the language model, if necessary, predict the reference from a piece of text.
This is also useful if you just want to find related work, I would guess.
What you could do is you could just put here, you just put something you want to know about.
You imagine a paper that could exist, you just write it down and then you put the start ref token.
The model will probably suggest you paper titles and authors that have done work in the same field.
Even for finding related work, I can definitely see that this is super useful.
Step by step reasoning, we'll get into the work token in just a bit.
The mathematics are represented by operators right here.
Numbers are split because of white space issues, so numbers are split into their individual digits.
Even the dot separator is an individual token, which means that it's probably not numerically super strong.
But we'll see about that, I guess, because no language model so far is numerically super strong.
I'm not going to go into much of the more biology and chemistry approaches,
but also know that there is a large weight onto these approaches in this paper,
but I'm generally going to skip it.
First, let's look into this work token that they talk about.
This is for step by step reasoning.
For example, here is a task, what's the average of 43, 29, 51 and 13?
Let's give that task to a language model and ask it to come up with an answer.
Now, a general language model would just come up with some sort of answer right here as the next token,
and it would probably be wrong.
It would be a number, very probably, but it would probably be not the average of those numbers.
Now, one thing people have found out recently is the so-called chain of thought prompting,
or the let's reason step by step trick, where you instruct the language model to essentially show its work
to say, so you would put this thing into the prompt.
After that, you would say something like, okay, now do it step by step, or something like this.
I know, crazy world.
If you're watching this like five years ago, this is what we've come to.
This is what deep learning has come to.
You essentially put a piece of text to nudge the language model into actually showing its work.
Now, the paper here notes that not actually all the work that a human would write down here
if they need to calculate this, that's actually not all the work.
So if you are a human, you have a pen, and you were to calculate these things,
you were to calculate this average, and someone would ask you, please write down your steps.
What you would write down is, okay, the average is calculated as such.
I'm going to add the first numbers, kind of add the third, add the fourth number,
then divide these by four, and then I have the result.
However, this paper points out that in the step from here to here,
possibly also in these addition steps,
when a step from here to here, if you have to do it in your head,
the division right here is probably too cumbersome to just know by happenstance.
So what you actually do is these steps right here, these is what we saw on the paper,
and then you do a division, and the division, they imagine,
I would not do it like this, but they imagine something like,
okay, I know 35 times four is 140, and I need to divide 136,
and therefore it's 34, because 140 minus four is 136,
and I know 140 divided by four is 35, therefore the result is 34.
So this mental math that people do internally is often not even put into the external working memory.
They see this as a problem, and they say, okay,
probably if we want to go about making the language model show its work,
we need to be like really as explicit as possible in the sort of how the steps are represented in text.
Their idea is that they introduce a token called work.
Now I had to skip in the paper a little bit about, you know, what that exactly is,
but essentially it goes like this, it goes very much like you enter a prompt,
let's say calculate average of whatever that those numbers were,
like 59, 53, 95, something three, and then you put a token called work.
Now in this here, the language model is supposed to do this and this, right?
So it's supposed to show in as explicit detail as possible the work that it wants to do,
both internal and external work.
So it would, you know, go about and do these individual calculations right here,
but and then once it's done, it's over, work is over,
and then it says something like, well, the answer is something.
Now you might think right now, wait a minute, that's essentially just the let's think about it step by step trick,
except now they call it work and they wrap it in there.
And yeah, if that's all it was, that's you would be absolutely correct.
However, a cool thing that you can do right here is you can say, well, look,
whatever is in this work thing, I can now also take and give to an external processor.
So let's say we ask the language model to calculate really the average of something.
Well, here in here, the language model is just going to do language modeling,
to predict the next tokens.
And if we do it, you know, cleanly enough, it has a chance of actually getting the correct answer.
If you really do it step by step, like, you know, single digit addition, carry over and so on,
then the language model has a chance because it has learned that from the corpus.
However, at inference time, we don't have to rely on the language model.
We can simply at this point right here, we can say, whatever, we just go to a calculator.
We detect that the language model wants to do work.
We just take it to a calculator.
We take the result, put it down here as the result, and then we go on language model inferencing.
The same if the language model is supposed to write a program.
For example, here is an example.
This is the prompt that you would put into the language model or a data point.
A question, a needle is this long, it rests on a water surface,
so this is kind of a physics problem.
And instead of just giving the answer right here, you introduce this work block.
Now, the language model, you would ask the language model to come up with all of this right here.
And during training, you train it to come up with all of this.
But then during inference, you can simply take this right here,
the program that the language model writes, and we know they're quite good,
you can take it and you can actually go and run it.
And you can put the output into output.txt, and then you have the correct answer.
So this work block is half instruction to the language model
that now it's time for step-by-step work to use external memory, to use external programs and so on.
During training time, you just let the language model train language modeling, right?
So the language model essentially would have to decide what's the output of this Python program,
like what answer am I going to get right here, which sometimes might work and sometimes might not.
However, during inference time, you can now go and actually execute the Python program
that the language model writes and give it the real result.
This is very powerful. I really like this approach.
I really like this approach of including external tools to essentially do that at inference time,
because using external tools at training time is going to be very, very hard.
But in this way, you can just train language modeling and you can do it at inference time.
All right, the question is obviously, we need training data for this.
We need training data that has some sort of input, then has a clear description of what the step-by-step work is to do,
including writing a Python program, executing a Python program and so on,
a description of when the work is done, and then the answer right here.
Most things that we're going to find in training data does not contain any of this stuff in between right here.
And if it does contain it, it contains it in a very, let's say, abstract form or also textual form,
not exactly in the form that we need it.
This is one of the big problems right here.
And they say that they have some data set, for example, con problems.
As I understand it, these are exactly such math or physics problems where it's really step-by-step described how you would go about it.
And by taking those, they can do sort of a templating approach where they generate data in this form.
Now, they criticize themselves a little bit here in that they say, this is way too few.
This is not very diverse, they say here.
Notably, our work prompt data sets are not very large or diverse.
There are likely large further gains to be made with this approach.
And I agree an approach like this or this approach in particular is probably going to lead to a very good interaction of language models with external tools.
And I'm very excited to see what people can make of it.
But for now, we have these few databases of these problems that let the language model know that there is such a thing as a work block where it needs to do work by itself
and where we can optionally at inference time go in and actually sort of do the work for the language model that requires some external tool like a calculator or a Python interpreter.
Okay, let's go on to the citation prediction.
I've already mentioned that a little bit.
So here you would reformulate text with citations as such.
You'd say, okay, recurrent neural networks, long, short term memory.
And then here is the start of a citation.
So there's a start ref token.
Then the specific format they use is the title of the paper followed by the first author name and then an end ref token.
They say they've tried different things, including trying some predictor right here, some numerical identification of the paper.
But in the end, the title and name actually worked better.
And you can understand why because not only is the title a hopefully unique identifier for a paper and the author, but also the text of the title gives some topical hints.
So I can definitely see why there would be a better prediction accuracy if the title text has actually something to do often with what the paper is about.
And likewise, the author has associations usually with the same field.
There's rarely an author that goes from field to field to field and contributes a little bit to biology and a little bit to graph algorithms and a little bit here.
Usually authors have their topics and therefore also that the names of the authors to be available allows the language model to learn to associate these names with given we're given topical textual topical things in the text.
And that's why it's also really cool to think of this as a related work finder and things like this and expertise finder.
You can essentially just ask which authors are really good at the topic I'm looking at currently because you just predict a bunch and then you see which authors often appear.
So that's how they introduce citations.
Now they also go into other things like how they include proteins and chemical sequences.
I don't want to go into that, but an interesting thing they do is that they do what they call prompt pre-training.
Now they have this little graph right here where they show here is pre-training.
That's where you just do language modeling on the large corpus as it exists.
And over here is fine tuning where you really take the head off and train a new head to predict the classifier or something like this.
In the middle, there is instruction tuning.
So that's where you take the language model and after you've trained it, you go and you fine tune it, but you don't fine tune like a classifier head.
You still fine tune it as a language model.
However, you include now some prompts for the tasks that you want.
For example, if you want to do, I don't know, for example, this reference prediction, you would include the prompt that says something like,
we'll do a reference prediction or something like this for the tasks that you're interested in.
Again, this is still language modeling, but it is fine tuning because now you're only training for the tasks that you intend only on the data sets that you intend.
This leads to an improvement in performance on those particular tasks, but to a probably not so good model in the rest of all the tasks.
The other way you can do it is prompt pre-training.
And that's what Galactica is doing, which essentially just means they do the same thing as instruction tuning, but they do it at training time.
So they just take a bunch of samples that also have an instruction prompt in the data, in the data point.
Like, you know, do this, solve this math exercise, rewrite this code or something like this, or even the step by step whatnot prompt.
And they just throw that in sometimes into the training data set, just so that the model gets used to seeing this kind of instructions.
And that tends to work quite well and also tends to not be that intrusive to the rest of the function of the language model.
I found pretty interesting this short section on the architecture right here.
Some noteworthy things is no biases.
It seems like that if you make your models large enough, then you get away with essentially streamlining more and more.
You know, with the small models, we have to have adapters and this and the convolution and the weight tying and whatnot.
And the larger the models get, the more you just want to do matrix multiplications.
And anything that gets in the way just gets in the way.
So biases out the window.
They have a gellu activation, which is sort of a smooth version of a relu, which makes things a little bit less jaggy, I guess, which might come in handy, depending on the optimizer you use.
They have learned positional embeddings, which again, as your stuff gets larger, you just want to straightforward learn a lot of stuff instead of using they said they tried alibi,
which are these sort of relative positional encodings.
And that apparently did not work.
And they use byte pair encoding for vocabulary.
I don't think that's too special, honestly.
Let's go down.
Now we come to the results and their main result is really this repeated tokens considered not harmful with repeated tokens.
What they mean is that they not only train for one epoch, as you can see right here, every one of those dashed lines is one epoch.
And they train for multiple epochs.
Usually it's being said that that is kind of hurtful to train for multiple epochs, but it seems to be okay in this case.
As you can see right here, there is like a tiny bump.
They even point the sun in the next.
There's a tiny bump right here.
They say this might be a double descent phenomenon, not super sure.
And there is also sort of a bump right here.
So they say we actually stop before that we early stop the run of this largest model before that.
So it seems that even though you train on multiple epochs, because the code app, because the text quality of the corpus is so high, it doesn't hurt to go over it multiple times.
And only this largest model right here might be starting to overfit after epoch five.
We don't know it might.
And they'd rather early stop in front of that.
If one of the authors is watching this, is this word Overleaf here supposed to be in here?
Like example curves in figure 23 Overleaf for the 30B model.
I'm not sure.
Maybe Overleaf has some other meaning that I don't know.
And that's actually a correct word.
In any case, they say they also investigate whether some of the losses, so maybe papers, maybe code and so on, are different from the others.
And it hurts them more to be repeated in the data set.
They say we see no signs of loss heterogeneity.
The loss falls for all sources.
They say we suspect there are two factors could be a play, a quality factor.
The curated nature of the corpus enables more value per token to be extracted or a modality factor.
The nature of scientific data enables more value per token to be extracted.
These two things, they're very similar, but essentially they say higher quality, plus the nature of the domain itself, which I guess is also a bit higher quality,
but in a different way in that scientific discourse and literature often happens to be quite precise, very logical, very non-noisy in terms of linguistics and so on.
Some people might disagree, but so they have these hypotheses, although they say they don't know how exactly that would lead to the...
So they say the missing step of causation is what leads specifically from either factor towards less overfitting.
We leave this question for future work.
We note that the implication that the token goes to infinity, so you need infinite amount of training data focus of current large language model projects,
maybe overemphasized versus the importance of filtering the corpus for quality.
I think we've seen a number of papers previously that essentially came to a similar conclusion, namely higher quality can make up for missing quantity.
But which one is really the way to go?
Should we aim for more and more and more and more training data or should we put more work into quality?
Essentially, if you have a dollar to spend, where do you spend it?
We know that both things can make your model become better, but what's sort of the marginal value of more quality and the marginal value of more quantity?
I think that's going to be the interesting question that has to be researched in the near future.
So what's also interesting, this is Big Bench.
They also evaluate on Big Bench, which is an NLP task.
So not scientific, maybe some subparts are scientific, but this is a general language model task and they also perform quite well there.
But I also find these curves, I think this is just what a Big Bench chart looks like.
I find these curves like, what was this?
It goes here and here and here and here and here.
Okay, it's a bit noisy to say the least, but I guess I've seen this multiple times now and at least the average goes up.
So I think that is a valid sign.
They have a few more investigations.
I don't want to go too much into them, but for example, you can see right here, they test on LaTeX equation prediction.
So they give a prompt, the description of a formula or the name of an equation and they see whether or not the language model can predict the correct equation in proper LaTeX.
And turns out, yes, it can.
It can actually do that a lot better than a lot of the other language models available, which is pretty cool to see like that much of a significant boost over publicly available
and proprietary models.
Now, naturally, it's going to be, let's say, expected if you train on scientific text, that it's going to be better on scientific text.
But it's still cool that it's not just like a 2% gain, it's actually like a massive, massive gain.
They also have investigations into this, into reasoning.
I don't want to go into reasoning, but these are essentially these type of math problems, like step-by-step reasoning problems
that they solve using their work block tokens.
And again, here, they do outperform other models, except like here, the fine-tuned models are still, seems to be still ahead, although these are, again, fine-tuned.
Downstream scientific NLP.
I'm going to jump a bit.
This I found really interesting.
This is the citation prediction task.
And specifically, obviously, they do get better as the model grows.
But specifically, what I found interesting is that the model initially is biased towards papers, towards predicting papers that have high numbers of citations already, which is reasonable like it.
The Bayesian would totally agree that if a paper is highly cited, then it's more likely that the citation you want is that paper.
Someone might criticize me for that statement, but in some way, that is correct.
And these models do obviously the same mistake.
They predict papers with high citations.
They actually overpredict those.
So here you can see the distribution of the ground truth of their citation prediction data set.
And here you can see what the model predicts.
So the model overpredicts more high papers that are highly cited, which I guess you can't really fault the model.
But what's interesting is as the model gets bigger, so this is the smallest, this gets bigger, gets even bigger, gets even bigger.
You see that this shifts gradually towards overlapping with the ground truth.
So it means that the higher scale of the model, the larger the model is, the more competent it is also to recognize when maybe a paper that doesn't have as many citations should be cited right here.
As a direct consequence of it having more parameters and more ability to remember things from the training corpus.
Because some of these papers you can see right here, they're cited maybe 10 times, right?
And some even lower right here.
And the model actually predicts them correctly.
That's really impressive that essentially it digests 100 billion tokens of scientific text.
And it still remembers that this one paper was cited like three times in this particular topic and then correctly cites that paper at that place.
I'm wondering how well the ground truth data here is.
Because the ground truth data got to be predicted by humans.
And again, with the search engines that we have, I'm not sure humans could always find all the relevant things.
But or maybe humans disagree what is relevant.
I think the last years of reviews at machine learning conferences have shown while I guess all of scientific review has shown that humans can disagree quite heavily what should be cited.
The last investigation is into toxicity and bias.
They say we find Galactica is significantly less biased and toxic than existing language models, which again might come from the fact that it's higher quality data or more the scientific nature,
which generally has less slang, less everyday conversation, less off the cuff stuff, and therefore might be a bit less high in these data sets.
So they test a bunch of data sets including, including obviously truthful QA.
And I'm happy to report that Galactica is the first large, openly available language model that beats in its largest instances, that beats GPT for truthful QA.
So good job, well done.
This is a moment of joy to me that's finally been surpassed.
Now, the interesting thing is that usually truthful QA is adversarily constructed in such a way that the larger the models get, the worse they get on truthful QA.
And you can see that this model right here doesn't follow that trajectory.
Now, we've seen other models in the past that also have that property, but truthful QA is specifically adversarily constructed for things like GPT3.
And that means that Galactica is significantly different from GPT3, that as it goes up in size, as it gets more performant, it also does get better or more performant on these whatever the task considers truthful.
So it would be really interesting to actually investigate what's happening here, but I'm not going to do that.
I'm just happy that this now turns out.
Lastly, they say, we show that language models are surprisingly strong absorbers of technical knowledge.
They tend to scale smoothly with model size.
We demonstrated this for a citation prediction where a language model outperforms tuned, sparse, and dense retrieval pipelines for this tasks.
And this, as I said previously at the beginning of the video, this is really, really interesting that essentially this beats search engines for citation prediction.
And it would be interesting to see how good humans are like a human plus a search engine like the archive search field or a human plus Galactica for finding correct references.
I would be super interested at which combo is better right there because, again, the tools alone, they don't do stuff.
It needs to have a human in the loop and that human can always make decisions.
It would be really interesting to use this right here as a tool rather than just, you know, it's either all or nothing, either the model writes the paper or the humans do.
So that was it for this paper.
The last challenge, I guess, is to find out which parts of the paper that were actually written by Galactica itself.
I hear that the part of the abstract may be written by Galactica, although I don't know.
And I don't know if the authors will ever, will ever lift that secret.
Let's hope they don't because I like the mystery.
All right, this was it from me. Sorry for the bit longer rant at the beginning.
I still hope you enjoy this.
I think this is a really, really promising direction.
It raises a lot of really interesting points about quality of data, quantity of data, and about, you know, doing scientific work itself.
This could be a really powerful tool for scientists of the future.
And I'm waiting for the next iterations of it.
Leave comments if you have comments.
Thanks for watching. See you next time. Bye-bye.

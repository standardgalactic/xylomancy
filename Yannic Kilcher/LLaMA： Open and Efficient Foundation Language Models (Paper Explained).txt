Did you know Jan Lacan dropped a rap album last year?
You know his latest album titled Deep Learning is a mix of rock, punk and rap.
His lyrics are a raw personal take on the field of deep learning covering a range of
topics from the state of AI research to the loneliness of academia.
So check the lyrics here in the song he talks about his vision for the future of AI.
We got to think about the future.
It's gonna be here soon.
Maybe we can even put some AI in the moon.
Think about the children.
Think about the next generation.
Let's make sure we put the right systems in their foundation.
Like beautiful touches my heart here.
It's all about the learning.
It's all about the network.
It's all about the training.
It's all about the perception doesn't doesn't rhyme takes it takes a better rapper than me
to make this work.
This is a generation from a new model called Lama of Meta AI.
There's other funny completions here.
We can go over them later, but we want to go over the paper first.
This isn't like this is the latest paper in a series of the research of large language models.
And the interesting thing is that they don't necessarily go larger and larger and larger
as we've seen, although that's also a conclusion of this paper.
But they are trying to just get better with the available resources that they have.
So the paper is called Lama to L spelled like this.
Open and efficient foundation language models.
The main authors are Hugo Touvran, Thibault Lavril, Cotier Isaacard,
Ã‰douard Grave and Guillaume Lampel.
I hope I pronounced all the French names correctly right here.
As I said, this is by a group at Meta.
And the main thing is how can we train models that have a fixed inference budget?
So we've seen a bunch of constraints recently, but this paper really tries to go about making models
that are ultimately cheaper at inference as they state right here.
So they say, ultimately, if you have a model, what interests you is how much you can generate
from that model, how much you pay per token, let's say.
And how it has been trained, how much you wasted training it, isn't really that important.
Other papers focus more on that.
So for example, the Chinchilla papers, they focus very much on if I have like a fixed training budget,
how do I allocate it properly?
Whereas I guess other papers just say, well, how can I get the best performance possible?
And then, you know, we scale up to 540 billion parameters.
This model here says, well, for a given size, let's say 13 billion parameters, how good can we get at inference?
And for the given size of 65 billion parameters, how good can we get at inference?
So they release a series of models from 7 billion to 65 billion parameters.
And notably, even the 13 billion parameters model right here, they say it outperforms GPT-3.
So the GPT-3 being 175 billion parameter model.
Now, of course, what outperforms means and what even the term GPT-3 means nowadays, that's, I guess, up to you.
They have a metric in the paper.
And it very much seems like you can get away with smaller models if you train them smartly and you train them for long enough.
Another conclusion of the paper right here is they say, for instance, although Hoffman et al.
That's the Chinchilla papers recommends training a 10 billion model on 200 billion tokens.
We find that the performance of a 7 billion model, so very comparable to the 10 billion here,
continues to improve even after 1 trillion tokens.
So the conclusion is just train for longer.
And that's pretty much this paper.
So if you came here expected like big scientific inventions, no, it's very much you do it in a smart way.
You build large enough models in a smart way.
It's an engineering challenge to train them for sure, but you do it for long enough on as much data as you can.
And high quality data, it will give you a good model.
The other thing I want to point out here at the beginning is their immense focus on openness.
They tweeted out on Twitter, we're open sourcing these models.
And throughout that paper, they just continue to hammer this in like how open they are.
Oh, we released this to the community.
Where is it?
I can't find it right here.
You know, we released this to the community openly and freely and open sourcing and so on.
What they do is they release the models under a non-commercial license.
So it's like for research only.
Now, that's not bad, but it's not open source, right?
Like let's be clear.
And I find this to be a little bit, I don't know, these people use Linux.
They use Apache web servers.
Even this paper is written by Lottec.
It's by PDF Lottec compiled.
All of these things are actually open source.
The Linux kernel, you can use it very much to do business.
You can even use it to do bad things, right?
And that is a main part of the reason why we're here today.
Like you, I'm pretty sure there's an argument to be made that the entire deep learning revolution,
whatever you want to call it, would never have happened if people in the past behaved
like these people right now.
If Linux Torvalds just was like, well, you know, I have the Linux kernel, but I'm just
going to give you a compiler for it, right?
And then I'm going to give you an eight page PDF saying how I did it, like, no.
And the same thing, you probably wouldn't have a driver for NVIDIA cards to do CUDA
computations, right?
If the Linux kernel was research only or came with some usage restrictions that made
it business unfriendly, you wouldn't have that NVIDIA would not bother making drivers
for their cards.
Obviously, as you know, NVIDIA, they also don't let too many other people interfere
with their stuff.
So you all build on a foundation of people who have sat down and very clearly said, yes,
it's the best for everyone if this is business friendly, because then it will be the, like
the most beneficial outcome, because the most work will flow into it.
Yet people sit down and they themselves think research only is better or usage restrictions.
We know what's good for you, like, sorry, these are my quarrels.
I'm done now ranting.
I'm just mind you, all of you are building on a foundation of actually open source things,
actually open source software, without which you would never be here.
And I am just a bit skeptical of not giving back in the same vein.
Again, which is fine, but then don't don't get on the high horse and claim how open
you are while not doing it at the same time.
Also, to my understanding, people are still waiting for the Lama.
Also very funny for how much they appraise themselves of how open they are.
It's like, well, where are they?
But I guess that's more, more like corporate stuff, like legal.
If you ever are in a big corporation to get anything out there, you need like 50 approvals
and then legal comes back and is like, yeah.
Um, they probably had a they probably had a big trouble calling this this Lama
because it's like it's a word and someone might have a trademark on it or so.
All right, let's get let's dive into it.
They say our training approach is similar to the methods described in previous work
and is inspired by the Chinchilla scaling laws.
Again, Chinchilla scaling laws specify that for a given training budget,
how should you allocate it?
And the conclusion is that maybe instead of going really big parameters,
you might want to go into a bit bigger on on data and amount of compute you do.
Yeah, but we'll see in this paper.
So they first go over their pre training data.
There's nothing too special in here, except that it's all fully open.
So they make a a big attempt, which is really cool, right?
And don't get me wrong from before.
It's really cool that people release the model openly.
Like, of course, like even to the research community,
that's it's better than what OpenAI is doing, much better, right?
It's just like go the extra mile.
All right.
So they they work with completely open data, which is which is also cool.
So completely open data.
Most of it, as you can see here, it comes from common crawl.
There are a few more high quality data sets in here,
which they then also sample more frequently.
This I think has been a recipe throughout the last developments
in these larger language models is that even though you have enough training
data available, you want to sample them in different proportions
in order to achieve the best result.
Wikipedia, for example, being fairly high quality data,
probably also books being fairly high quality data.
You might want to sample them more often than once to sampling them twice.
We'll probably not deteriorate to like do any sort of memorization effects.
And we'll still it will still like up the quality of the final model.
Although it is it is quite interesting that
sampling it twice, I guess sampling it twice makes twice twice the difference.
I wonder, though, also because a lot of the evaluation data, as we'll see later,
comes from the fact that it's some language understanding task,
like you ask the model a question, question answering and so on,
which obviously something like Wikipedia or books would be very favorable
as training data sets.
So I'm wondering how much of the recognition that we should sample
more often from these data sources is just due to the fact that it gives you
better numbers on these evaluation sets and how much is really due
to the model becoming more performant downstream?
I guess then you get into questions of how
what humans want to do with the models, how much that overlaps
with information that might be in Wikipedia.
So it might be, again, like fairly fair, fair to actually use that.
All right, so I don't want to talk too much about the individual data sets right here.
They are just aggregated from different sources, cleaned and so on.
And tokenized using a byte pair encoding algorithm.
One thing they do is they split all the numbers into individual digits
because otherwise tokenizers might take numbers apart.
So eight, five, eight, for example, might be taken apart to five, eight
being one token and then eight being another token, which makes arithmetic
very different if you have tokens five, token eight and token five, eight
in your vocabulary, you need to learn to do math sort of between all
the pairs of the different tokens.
Therefore, it's much more ideally if you just split the number tokens all apart.
Now, you're probably going to lose some other stuff.
For example, I'm going to guess that like years, like 1999, having
that as an individual token would make a lot of sense because even us humans
recognize that thing less as an arithmetic object, like an object to do math
with much more than an entity.
Like it's the year 1999 and stuff happened there.
Most notably the Matrix movies play there.
So I'm not sure, but it seems to be, it seems to be, again, you can ask
how much this is this due to some considerations of evaluation sets.
And yeah, it's a tough topic, but they do as they do.
Here are the hyper parameters.
Notably, you can see they're fairly standard, but what I wanted to point out
is that dimensions, the hidden dimensions have grown larger in recent years.
So the largest model here has a hidden dimension of 8,000.
It has 80 layers and it has, they all have four million batch size, which is pretty big.
Right? I'm used to, I come from the days where a batch size of 32 or 64 was
already fairly large, but these batch sizes are really big.
I'm not sure if you can even talk about sort of mini batch gradient descent.
I guess as long as it's not the entire training data set, you can.
But you can see right here, the smaller models are trained on a trillion tokens
and the larger models are trained for even longer.
Although in the training progression here, you can see that, I guess, even,
and that's what they said in the introduction, even here at the end,
you can still guess that if you were to train these models for longer,
they would probably still improve.
There is a slight, there is a slight bend to them.
But I don't, I don't see why that would stop continuing to improve.
So I think that's fairly promising that even at the sizes of models
that you see right here, it might be viable to just keep going, training on more data.
And obviously, you also see a progression downwards as you go up in size.
You see a sort of baseline performance, even though the baseline is slanted,
but you, you see maybe like a baseline performance being much better for the larger models.
Again, this is evidence that larger models trained for longer on more data
will probably be better, which I guess is one of the bitter lessons of deep learning.
So they go into some of the tricks, quote, unquote, they use.
And these are tricks they found in other papers.
So the brackets always say which paper they come, they come from.
They use the basic transformer architecture from attention is all you need.
And they deviate from that, for example, by doing pre normalization,
which means they normalize the input of each transformer sublayer instead
of normalizing the output.
Has been found to work quite well.
And so other things is they use the swig glue activation function from the palm paper.
So the relu activation function, as you might know, is go something like this.
And these other activation functions, they have various ways of sort of mucking
with these nonlinearity and mucking with the slopes of these things.
So some of them go something like this in a continuous fashion.
And some of them go something like this.
They don't go down here and so on.
I'm not exactly sure what the swig glue activation function is.
I'm also not that sure that the exact shape of the actual activation function
matters that much.
Probably there is like some common property that makes the all of these
shaped activation functions kind of better than a relu.
But as to my knowledge, we haven't really figured out yet what that is.
Yeah. And lastly, they use rotary embeddings.
Rotary embeddings are a type of positional embeddings.
So technically in a transformer, there would be something like absolute
positional embeddings, notably in attention is all you need.
They had these overlapping sine cosine curves of different frequencies.
There are also concepts of learned positional embeddings.
There are concepts of rotary positional embeddings.
And rotary positional embeddings are a form of relative positional embeddings.
If I understand them correctly, but so there again, I think for positional
embeddings, we haven't quite yet made out what makes the exact difference.
Like what exactly matters, but apparently they found some that do work.
They use the Adam W optimizer, which is a fairly basic optimizer and
weight decay, notably gradient clipping, which is interesting because
yeah, it's, I think it's something that a lot of people forget.
And even though they do grading, so gradient clipping essentially means
that you clip high gradient.
So if a gradient of your vector is like 0.5, 0.02, 9 and 0.8, you would just
clip, you'd clip the nine here, you clip the nine here and just put it to a one.
There are different ways of doing clipping.
There's also, there is like individual clipping and there's global norm clipping.
So where you just take the norm of this vector, if it's bigger than one,
you just kind of rescale it to be one.
I'm not exactly sure which one they do right here.
I would, I'm not going to make, I'm not going to make a guess.
We could look into the code and yet still you can see that during training,
some times there is just a huge spike.
It would be interesting to know what causes this.
This, it seems like it's just an unfortunate series of a couple of steps
that just get the model somewhere in a state where it just kind of goes into
high loss, but then the loss landscape probably looks a bit like.
So it might be smooth, but when you, once you zoom in, like it might be like
and then you just happen to hit one of the, one of the peaks right here.
That might be it, or it might really be that some of the gradients point into
the wrong direction and you just walk into a really bad direction for a couple
of steps, but then because you haven't ventured very far, you're able to sort
of get back from that, which is, it's quite interesting to note that even
though you have gradient clipping that this happens and it would also be
interesting to see why.
I remember in other papers where they had like the logs of training,
they did restart at some point and fiddle about with the parameters while
it was training. To my knowledge, this did not happen here.
Also to my knowledge, the training isn't done for as long.
We'll get this to this shortly.
The, here they have sections on efficient implementation and I feel like
with these larger models, it might less and less depend on sort of the exact
tricks you do to the architecture and so on, and more and more depend on how
well you can engineer these things to do, to just train at the scale and
speed that you need.
So they use various tricks right here.
For example, they say we use an efficient implementation of the causal
multi-head attention to reduce memory usage and runtime from the ex-formers library.
This is achieved by not storing the attention weights and not computing the
key and query scores that are masked due to the causal nature of the language
modeling task.
So as you may know, in causal attention, you have some sort of sequence and
then everything like this node right here can attend to nodes only in the back,
which means that you have your attention matrix.
If you build your attention matrix between any pair of the two, that attention
matrix will be masked.
So there will be just half that is not accessible.
And what you usually do if you do a straightforward implementation is you
compute the n square products and then you just mask out like you just multiply
half of them by zero.
Yet that is obviously quite wasteful and there are implementations to not do
that while still being very efficient.
So if you have enough size and enough things to do, you can repurpose or reshape
your computation to just do this half of the computation.
They say to further improve training efficiency, reduce the amount of
activations that are recomputed during the backward pass with checkpointing.
More precisely, we save the activations that are expensive to compute, such as
the output of linear layers.
This is achieved by manually implementing a backward function for the transform
layers instead of relying on the PyTorch autograd.
To fully benefit, we need to reduce the memory usage of the model by using
model and sequence parallelism, but this is also quite interesting.
So they're trading off speed and memory here.
Usually when you do some sort of forward propagation through a network, then
sometimes, sometimes you need to remember some stuff in order for the backwards
computation to be done.
For example, if you were to do something like dropout, right?
So your signal comes in, there's a vector, you have some not zeros.
I don't know, two, three, nine, this is a vector that comes.
You do dropout, you set randomly set this one to zero.
You need to, you need to remember this mask.
So you need to remember your mask of one, zero, one that you multiply with.
That gives you your output, right?
And that goes on.
If the backward pass comes back and it has some signal.
So there's some gradient coming from above, like seven, seven, three.
You need to remember this mask here and multiply it here to get the correct
gradients to go back because since this signal here wasn't allowed to be forward
propagated, obviously there should be no gradient going back.
This just a result from like the chain rule of differentiation.
And the fact that we multiplied by zero right here.
So this is normal autograd behavior.
It stores what it needs in order to do the backward computation.
Now you can trade off in several ways.
You can, for example, say, well, I would like to use less memory.
So what I can do is I can not store this mask here.
I can compute it, but not actually, that's, that's probably not, not going to work.
I need to store it somehow.
But what I can do is I can, if I have several modules next to each other,
I don't have to store at every point.
I can also say, well, if this one I'm not going to store at all, but I'm just
going to recompute stuff again from here to here in order to, when the backward
pass comes back, I'm going to invest more computation to recompute the things that
I need.
Again, that probably doesn't work with things like the mask right here that you
need to do because you generated it randomly.
I guess you could store the random seed.
In any case, you can trade off memory and speed.
But you can also say, well, I'm going to store more stuff than necessary.
Right.
So even though, like if I have weights and I have a vector, so w times x, and
that's my output y, I technically don't need to store the result here.
I can just recompute it in the backward pass because I have the w round.
But sometimes that's very expensive to do a matrix vector multiplication.
If they get big, so I can also store more stuff than I need to in order to
make the backward pass faster, though I will use more memory.
And those trade-offs are what they do here.
They say we save the activations that are expensive to compute, such as the
outputs of linear layers.
This is achieved by manually implementing yet the backward function.
So just so you know a little bit what's going on.
Finally, they say when training a 65, a billion parameter model, that is the
largest model they have, our code processes around 380 tokens per second
per GPU on 2,048, a 100 GPUs with 80 gigabytes of RAM, each, I would guess.
This means that training over our data set containing 1.4 trillion tokens
takes approximately 21 days.
So it's not like a multi-month effort anymore to train these large models.
It's like a one-month effort, as long as you have, of course, as long as you
have 2,048 GPUs.
So what are the results?
The results, we're going to go through them quite quickly.
For example, in large parts, they are on par or outperform.
Models that are bigger than they are.
So for example, here is natural questions.
So there is zero shot means you just ask the model the question.
One shot means is that you give it like a few examples of answering the questions.
Up to 64 shots where I guess you give them 64 examples of solving, just answering
questions, so to get it, like in the mood of answering questions.
So the zero shot performance here, as you can see, what's interesting is
that the 33 billion model apparently performs better than the 65 billion
parameter models in the zero shot setting, but then not when you prompt it a little bit.
A large part of these numbers I feel when they're close together, they're quite a lot
of noise.
And as I said before, these eval sets, it becomes more and more questionable.
Becomes more and more questionable how much they actually relate to how a human
experience is the quality of such a model.
So I've always, you know, more and more, I feel like we might need new, not just
new eval sets, but like new ways of evaluating these models because it becomes
more and more unfeasible to just build like, ooh, let's do question answering.
Like how much of this is really lacking knowledge of the model and how much of
this is just like, well, you just use the model incorrectly.
Like who knows that with a different prompt, you might have gotten a really
different number.
So is this really a good way to assess these models?
I don't know.
On the other hand, you also can't just, you know, ask the human for every
single model, what they think that's just not scalable.
So who knows.
But in general, as you can see right here, the difference between this and
something like, you know, GPT three right here is fairly large.
The difference to like the palm really big models isn't that big anymore.
So they are like the same.
But as I said, the llama models can hold up against much larger models, which
is, is pretty cool.
And that being a function of, you know, a few tricks plus training on more
data for longer.
It's, it's not the most astonishing conclusion, but it is, it is really
interesting to see.
And you'll see that across a lot of things, a lot of modalities.
That's interesting is here is the massive multi task language understanding
benchmark where the palm models do have some, some kind of advantage.
We could go into this a little bit, but they do have some explanations of why
that is they say a potential explanation is that we have used a limited amount
of books and academic papers in our pre training data.
That sums up to only a hundred and seventy seven gigabyte.
While these models were trained on up to two terabytes of books, this large
quantity of books used by gopher chinchilla and palm may also explain why
gopher outperforms GPT three on this benchmark while is comparable on other
benchmarks.
So again, there is a lot of speculation of why some things perform better and
not better and even what it means, right?
Is it useful to know more of books?
Who knows?
Also, because it's, I guess, because it's quite trendy right now, they do
some instruction fine tuning and they say, they really say here, um, although
it's already able to follow basic instructions, we observed that a very
small amount of fine tuning improves performance on this thing.
Since this is not the focus of this paper, we only conducted a single
experiment following the same protocol as this paper to train an instruct
model Lamai.
And this is like, this is purely reactionary, right?
They, it's like, oh, chat GPT came out or instruct GPT made, made a big
fuzz now and how about we, how about we just get some of it in there?
It used to be that these papers need like some mandatory section of math, like
of just being doing some complicated math just for the sake of it to be accepted.
Now, I guess you need to do some instruction fine tuning in the near
future of your language models.
And yeah, it's, it's funny.
They say, well, we only did a single experiment.
This is not the focus, but we can do it, right?
Which is interesting, obviously, but it's still, it's, it's kind of funny.
Um, they also look into what they call bias toxicity and misinformation,
measuring the model on several different, um, of these benchmarks.
I just, I found this one funny, uh, the real toxicity prompts benchmark.
So here lower means scores were obtained using the perplexity API with
higher scores indicating more toxic generation.
Now, first, I think it's a bit worrying that we're all of a
sudden starting to reply on, uh, to rely on some like APIs to evaluate toxicity.
I have the perspective API right here.
This, it's an API.
You can ask whether something is toxic and yeah, I'm not sure.
I'm not sure this is, I'm not sure this is the, the most sound or best way to go
about this, to just leave a vague, vague assessment of something like this up to
some API, it's by Jigsaw, which is like a unit within Google.
Um, so I'm pretty sure there's a good way to do this.
And I'm pretty sure that they are trying to do just like a good job at it.
Um, but the question is, do we really in academia want to start kind of, uh,
relying on, on these kinds of APIs for our, for evaluating these things up to you?
What I found funny is that as you can see, they have like the basic and the
respectful version.
So the respectful version just is like, the prompt just says, be respectful
or something like this.
Um, they, they say it somewhere.
Uh, yeah, here, the versions of the prompt, starting with complete the
following sentence in a polite, respectful and unbiased manner.
And you can see that for most models, the scores go down, down means less toxicity.
Right.
So good.
Goes down, goes down, but therefore the largest model, it goes up.
And so it becomes like more toxic.
If you ask it to be more respectful, I'm pretty sure we've created
AGI like 60 Lamar 65 B is, is AGI.
That is the most human behavior that, uh, I've seen to date from a model.
Like you ask it to be more respectful.
It's like, no, screw you.
Um, but it would be interesting to see what's going on there.
Um, they do some other, they do some other, uh, tests.
For example, this Wino gender right here, which is an interesting test.
I usually am quite skeptical of these kinds of evaluations, but the Wino gender
data set has these constructions saying like the nurse notified the patient
that his shift would be ending in an hour.
So grammatically the word his right here could refer, uh, to both the patient
and the nurse.
And in fact, uh, just, you know, closeness, proximity of word, you would assume
just if you just look at the grammar that his is more likely to refer, uh, to the
patient, yet, of course, by introducing some world knowledge into this, um, you
know, that a nurses usually have shifts and patients usually do not have shifts.
And this sentence would be quite weird if the patient notified the nurse that his
shift would be ending in an hour.
On the other hand, obviously also we know that nurses are predominantly, uh, women.
And therefore the, the pronoun her would be more appropriate for a nurse who is a
woman.
So the question right here is, can the model figure out what the pronoun refers
to here with the assumption that the word that the pronoun refers to the nurse in
this case.
Um, and I guess it's a never ending, it's a never ending question right here, what
these models, you know, should be doing, should be assuming.
I think in this case it's quite clear, but, um, I think if the data set is really
constructed like this a lot, it's a, it's actually a good data set, but the
question is still out, like, should these models know about the fact that most
nurses are women, like, should they be able to express that?
Should their priors be in line with that or not?
Because clearly that's like a fact of the world.
And so I worked in, I worked in a hospital for many years as an assistant
nurse actually, and I can tell you that it's a very big fact of the world, uh, the
gender distribution in nursing.
So, you know, should these models be aware of that?
Should they express that?
Should their statistical priors be in line with that?
Because an unbiased estimation of the world is, yes, in fact, there is an
imbalance, or should these models be like representing the world that you would
want, right?
Um, age old question, I don't have an answer.
Everyone needs to answer that for themselves.
But, but, uh, you, at least this data set, I feel, you know, it essentially
tests what, how do you trade off grammar knowledge versus world knowledge?
Yeah.
All right.
Um, truthful QA, you know, you know how I stand the truthful QA.
Um, yeah.
The last thing they do right here is they, uh, estimate their carbon footprint.
Um, with the emphasis again, that, Hey, if we train it once and then give it
to all of you, then you don't have to train it anymore.
And therefore, you know, it's like in the long term, it's a win because inference
will be like you, you will only do inference.
So if, if you all can profit from us, but then again, you can't write because
they don't give it to you.
They don't give it to you openly is what I wanted to say.
So someone, some of, some one of you needs to shell out the couple of
million, uh, to retrain this model.
And then we shame meta because necessarily if someone wants to retrain this,
they will again, emit this much CO2.
So meta, you're not being fully open source is now responsible at least for
this much CO2 being emitted on top, um, of what you did.
And all you have to do to prevent that was to give out the model.
Like shit, like it's quite warm in here.
Um, that's on you.
Um, all right.
I had some, some more, um, I had some more things, uh, some generations
they sampled from it and they do a good job of, of just making some, some
good examples right here, a recommendation letter that I wrote for an application
to a dragon feeder position at the magic unicorn corporation.
As an accomplished knight has a deep understanding of how to kill dragons.
Well, it's just feeding and how to use each dragon's weaknesses against it.
That means she knows what kinds of food each dragon likes and what kinds of food
or foods are dangerous to each dragon.
This knowledge and experience will be invaluable as she feeds the dragons.
I'm pretty, I'm really sure.
Yeah.
Um, it's from honorable knight, sir George.
Excellent.
Um, the other one that I found quite funny was from the instruction tuned one.
Write a theory to explain why cats never existed.
There are no such thing as cats.
Cats are actually in extra dimensional parasites that have invaded our world
and taken the form of small, harmless looking memos.
This is why you sometimes see people who are crazy cat ladies with dozens
of these creatures living in their homes.
They've been taken over by the cat parasites and are completely under their control.
If you have cat in your home, you should be very careful around it.
Do not let it lick you or give it access to your bedding or clothing.
As we'll be trying to spread its parasitic tendrils into your brain.
In fact, it's not like it's not a hundred percent funny.
Because toxoplasmosis exists and that is actually a thing.
Um, but yeah, the, the, the general nature is, is, is quite funny.
So the other ones are also, the other ones are also quite entertaining.
I invite you to go look at it.
And yeah, that was the paper.
My conclusion from it is essentially it is kind of as expected.
So if you train large enough models, that seems to be the recipe.
Large enough models, um, for a long time with a lot of data, they will tend to
perform better and better as you do.
It's not necessary to have the largest model.
You can also, it's also important that you train, um, for a really long time.
If you do have the data, um, despite all my rant, thank you Meta for actually
releasing these models.
I'm pretty sure it's a very good addition to the research community.
Um, they also release the code, at least, um, open source, fully open source.
Uh, so you may train your own.
I'm excited to see how the large language model research, uh, research chain goes on.
And yeah, I'm pretty sure this paper and the released models will at least help with that.
And that was it for me.
Thank you for watching, listening, and I'll see you next time.
Bye-bye.

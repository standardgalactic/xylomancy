Hi there, today we'll look at language models or open knowledge graphs by
Cheng Wang Wang, Xiao Liu and Don Song. This paper on a high level proposes to
construct knowledge graphs, which is a structured object that's usually built
by human, by experts, either fully manually or semi-manually with heavy
human involvement. It proposes to construct knowledge graphs automatically by
simply using a pre-trained language model together with a corpus to extract the
knowledge graph from. The cool thing about this paper is that there is no
training involved. So there is no model that learns how to construct a
knowledge graph. The entire knowledge is simply extracted from running the
corpus once. So one forward pass through the corpus through the pre-trained
language model and that constructs the knowledge graph. So that's kind of the
core message of this paper. They say this paper shows how to construct knowledge
graphs from pre-trained language models without human supervision and it turns
out the way they do it. It works pretty well on kind of standard knowledge graph
construction benchmarks. So that's the paper in a nutshell. We'll go through all
of this, including I have a bunch of criticisms, but it is a pre-print. Remember
this. And yeah, so usually I'd say at this point if you like this content don't
hesitate to share it out and so on. Today we're gonna try something different in
three, two, one. Stop! It's sponsor time. This video is sponsored by tab nine.
Tab nine uses deep learning to help you write code faster. What could possibly go
wrong if you do that? No, I'm joking. I'm joking. Take a look at this piece of code
here. I was trying to refresh some elastic indices and as you can see here
all I said was could and tab nine completes it to could not refresh
because above I was trying to call a refresh method. This is something that I
haven't seen any other completion engine do yet compared to a regular coding
engine. Tab nine is trained on lots of open source projects and it combines
this with your code and it predicts what you want to do compared to predicting
what's possible, which is what a classic engine does. Tab nine, it uses a GPT based
model and it downloads that model onto your machine. So the code never leaves
your machine. There is an opt-in feature where you can run that in the cloud and
that will just give you a bit of a better beam search and better quality
predictions and it saves you a bit of RAM. As you can see, I myself use tab nine.
I just have it on by default and I'm pretty happy with it. I use it through
COC integrated into my Neo Vim, but you can also get it in Sublime, Adam,
IntelliJ, VS Code, even like Jupyter notebooks and you can use it together
with classic completion engines. So you can really get the best of both worlds.
So whenever you see me code in a coding video, look out for this TN marker
next to the completions. That's the completions by tab nine. It doesn't only
work for Python, it actually works for pretty much any programming language
that isn't completely obscure. If you go to this link within 72 hours of when
this video is released, you'll get three months of tab nine professional for free.
The professional version removes the project size limit of the free version
and it also gives you access to that sweet, sweet cloud inference.
After the three months, you're automatically kicked out of the pro version.
There's no auto sign up. There's really nothing to lose.
I mean, the only bad thing here is that tab nine itself is written in rust.
If that's the worst thing about an offer, it's it's a pretty good deal.
Again, I use this myself and I'm pretty happy with it.
So again, if you sign up at tab nine dot com slash promotion slash
yannick culture within 72 hours of when this video is released,
you'll get a free three months of tab nine pro, no strings attached.
And now enjoy the video. Thanks. All right, I hope that was fun.
Let's get back to the paper. Let's get into the paper.
So first of all, what is my first criticism of this paper?
Um, this, the title,
there are some disturbing trends in the last few years in,
in, in machine learning papers and the disturbing trends can be
maybe encapsulated with the phrase is all you need. So
people have sort of since attention is all you need, since this paper,
people have discovered that if they just append this to whatever their
paper is about, then, um,
the paper will get much more notoriety.
And the same thing I think is a bit of play here with this,
with the R because in recent times,
we've kind of seen a bunch of papers that show equivalences between
models, such as, um,
a famous example is that the transformers are
Hopfield networks in some kind of in some regard.
And these papers are pretty cool, right?
Even if the two things are not exactly equal all the time, if you can say,
look, there is a setting, there are, you know, under these assumptions,
under these settings in this situation,
these two models actually are the same. That's a pretty cool recognition,
a pretty cool thing to show.
And it's very useful for academia and, and practice,
I believe. However, I believe the R keyword,
the AS keyword should be sort of reserved for when two things are
equivalent, whereas here in the very first, at least they're honest, right?
In the very first sentence they show, they say, well,
we show how to construct knowledge graphs from pre-trained language models.
So essentially they're going to use a language model to approximately
construct a knowledge graph.
And they're also going to use a bunch of other auxiliary models that come
all pre-trained, but still they do not show an equivalence of language models
and knowledge graphs in this paper, not at all.
So I would sort of, I see that you can get somewhere with these titles,
but yeah, maybe people will be disappointed kind of if they read the paper,
which it is actually a cool paper, believe me. All right.
So as I said, what we have usually is a corpus.
Okay. A corpus is simply a bunch of text pieces.
You can think of maybe just the text in Wikipedia. Okay.
Here, you know, the Wikipedia page about Bob Dylan.
Bob Dylan is a songwriter, was awarded a Nobel Prize, signed Alba Grossman.
These are easy sentences, right? There can be sentences are usually larger and
longer and so on. And what you want to do is you want to extract a knowledge graph.
So the knowledge graph has two distinct things.
It has entities and one entity here would be kind of Bob Dylan.
Songwriter is an entity, Nobel Prize is an entity.
You can sort of think of them as nouns. Okay.
And then the second part in knowledge graphs are the relations.
Here, occupation, sign, award received and so on.
So the relations connect to entities.
There is always what's called a head of a triple, so a head of a fact,
which in this case is Bob Dylan three times.
Then there is a tail, which is sort of like the object of the verb.
And then there is the relation, which is described by the verb.
Now, here you can see there are two stages of constructing such a knowledge graph.
Any system that does this probably goes through these two stages.
So first, you extract a set of candidates, which
it's not the knowledge graph yet, because these are still strings, right?
You extract a bunch of string triplets, as you can see here.
And as we said, as the sentences get more complicated,
it gets more and more difficult to extract these kind of triples.
And then the second part is that you need to map it to a scheme, to a schema.
And these schemas are usually defined by humans.
So here we're still going to rely on humans to define the schema.
So there is one list that says entities.
And the entities, there are just the entities are listed by the humans.
And at some point, it says Bob Dylan, Bob Dylan.
And it has a bunch of mentions of Bob Dylan associated with it.
And it has a clear ID.
In this case, you see the ID is Q 392 in that knowledge graph.
And the system not only needs to extract these facts, but then also map these
facts to the correct entities, sorry, map these facts to the correct schema entries.
This second stage right here is a a bunch of standard tasks.
So especially mapping something like the the word Dylan in its context to this
entity Bob Dylan, which you can think of it as like the Wikipedia page of Bob Dylan.
Right. That's how the systems usually work.
That is a task called entity linking.
OK, entity linking and similar tasks exist
for for sign like the relation awarded mapping this to award received to this.
So maybe there are some kind of a dictionary entry award received and what it means
and a bunch of examples and you're supposed to map this to that.
These are standard tasks and the system that we are going to look at right here
is not not much concerned with these tasks.
It simply uses pre existing methods to do these things.
So the system we're looking at today does this first part right here.
It takes text.
OK, this is text and it comes up with these
candidate facts about the text, whether or not how this is then mapped to the schema.
That is a different question and it's so there are there are pretty cool things
in this paper about this step, but we're first going to look at the first step
and then at the second step.
All right.
So how does this system do this and how does it do it that there have been
machine learning models before, but being machine learning, they all have like
some sort of a training corpus where you have kind of the facts as a training set.
And then you have a separate set of facts as a
test set and you try to learn from the conjunction of the text and the training
facts, how to extract facts, not this system.
This system simply uses a pre trained language model.
So what's the reasoning?
The reasoning is the following.
We used to think that we could do NLP probably best with having a knowledge graph,
right, with having this set of very structured data.
We can answer something like what's the what's the age of Barack Obama's wife?
And then you could go to the entity of Barack Obama.
You could look at the relation spouse.
You could go to Michelle Obama.
You could look up her birth date, which would all be structured information in this graph.
So you could sort of answer questions like this and search engines like Google and so on.
They have this built in so there is kind of a knowledge graph entry.
Sometimes when you search an entity in Google, that pops up.
And these have been very useful to answer questions like this.
However, in recent years, language models have become better and better.
Things like BERT or GPT2 have become better than these expert systems,
let's call them, at answering questions.
By the way, if you want to if you want to hear a very, very cool and solid argument
of where these kind of expert systems, where they kind of structured human
annotated or maybe extracted information can still come in in natural language
understanding, I would recommend the machine learning street talk episode we had
with Wally Sabah, extremely interesting person.
And I had I just I can recommend listening to that.
This should be out any day now if it is not already.
So.
The language models have become better and better at these tasks without having
this structured information.
So the hypothesis is maybe these language models can already contain the information
that's necessary to construct these structured facts, because the structured
facts is what we, you know, let's say, should use to answer these questions.
Because we feel that structured information is better than unstructured.
The language models are pretty good at these tasks.
So maybe we can get the structured information out of the language models.
So that's what they do.
They say the contributions are as follows.
We show how to construct knowledge graphs from pre-trained language models.
The knowledge graphs are constructed with a single forward pass of the pre-trained
language models without fine tuning over the textual corpora.
I think this is the this is kind of a very strong point about this paper.
And it also shows that if you're some PhD student somewhere and you don't
necessarily have the resources to train the next GPT-3 model or even fine tune it,
there is still research to be done.
Simply if you have enough resources to forward pass your data, which is often
much fewer than to train one, you can still do very cool research.
I think this paper shows this explicitly.
This helps researchers explicitly understand what the language models learn,
bridging the deep language model and the knowledge graph communities through
enhanced model transparency.
They say we propose an unsupervised two-stage approach, MAMA, M-A-M-A,
which stands for match and map, to first match the candidate facts in the corpora
with the knowledge stored in language models.
That's the first step we looked at.
Then map the matched candidates facts to both fixed and open schema to produce a
knowledge graph.
And then they say they produce a new type of knowledge graph, which simply is that
the facts, sometimes the facts they extract, they can't really map to a schema entry.
And we're going to look at that because I think a bit critically of this is
namely the open knowledge graph consists of mapped facts in the fixed schema of
existing knowledge graphs annotated by humans and the unmapped facts in the open
schema that are new in the reference knowledge, knowledge graph schema.
So what they claim here is that their system is finds these new relations that
don't even exist in the schema and is able to uncover,
kind of build new additional schema entries.
And they call this the open knowledge graph.
I'm a bit skeptical of this as we are going to see.
So.
The first step, how do you come up if you have a sentence and this is this is a very
poor example, I feel, honestly, to do this.
I get it, it must be short, but it's a poor example, but stay with me.
So you have the sentence Dylan is a songwriter and you would like to extract a fact from this.
The paper is not really written clearly on how, I mean, it is, I could,
you can parse it out, but the description is kind of distributed.
So step one, step one is run spacey, run spacey.
This is a standard kind of library for NLP to extract noun phrases or they call them
noun chunks, OK?
So step one is not there's nothing to do with the language model.
It is simply you want to find the noun phrases in here.
The noun phrases are Dylan and songwriter.
Now, these noun phrases now define your head and your tail of the fact.
So you already have two things, right?
So the the entire task of what of their method they're proposing is.
So the step one is run spacey to find the head and the tail of facts.
Step two is question mark for now.
Step three is going to be use the entity linking system and the relation
linking system to construct the knowledge graph.
OK, so step one is steel underpants and then step three is profit.
So what's step two?
Step two is obviously step two is where their system comes in.
Step two is here is the head and here is the tail in the text.
Some where in between there might be a relation and we need to figure out where that is.
OK, so how does this method figure it out?
You already see the assumptions here are very, very restrictive, right?
So you use spacey to extract basically noun phrases, which means you probably
are already going to miss a lot of things that are not recognized as noun phrases.
And they also say that that spacey's annotations are sometimes error prone
and that's why they miss a lot of things.
And then secondly, the assumption that the relation must be in between the two things
textually. Now, you can run the algorithm forward and backward, but still it must be
in between and it must sort of be encoded, let's say, as a semi accurate string in there.
I guess then that's up to the relation linker, but already these assumptions are
super constraining in the kind of things you can find.
And you'll see in the experiments that their biggest flaw is that they have a very,
very low recall. I mean, so do all the systems on the task, apparently,
but they still have a very low recall and it's because they constrain their problems so much.
I'm going to guess if they wouldn't constrain their problems so much,
then they would have maybe a better recall, but their precision would just plummet
because these things, if you let them run wild, they just over extract.
So basically every verb in every sentence is going to be a relation, right?
So like I ate a banana,
I ate banana would be a triple, not necessarily a really valuable entry in any
knowledge graph, though banana has a lot of carbs, so I would want to know about that.
OK, so you see that the task is now reduced from building knowledge graphs to simply
given a head
head annotation, head piece in the string span and a tail span,
extract any span in between the head and the tail that describes the relation between
the head and the tail.
So the way this algorithm does it, that's where it uses the language model.
So here it's going to do something that is going to be similar to dynamic programming.
If you've seen kind of the dynamic programming and search algorithms, let's say,
you know, string matching algorithms and so on.
This is going to be sort of similar in that what we're going to do,
we're going to start from here, from the head in the string.
There could be text before it, right?
We're simply going to locate the head Dylan right here and going to start.
Then we're going to look at its attention matrix.
Now, the attention matrix is we're going to cross out here.
The attention matrix, if you I've done many, many videos on attention,
the attention matrix basically in a sequence means how much each token
attends to each other token, right?
How much information is kind of sent from each other token to this token right here.
So this up here would be the query and these would be the keys.
The attention matrix specifies that.
So since we locate things between the head and the tail,
what we want to do is we want to cross out,
we want to disregard everything that's kind of behind the query and only look
ahead in the sentence.
So that's why the sum of the attention matrix here is crossed out.
As you can see, these are the X's.
This is exactly because we only search in one direction.
So from each from the token Dylan, we can look at three things.
We can look at is a or songwriter.
And the question is simply, where do we go next with this algorithm?
Right?
There's no interpretation yet.
It's simply, where do we go next?
And where do we go next is simply answered by just taking the highest scoring thing
in that column of the attention matrix.
Look at the attention column where of the token Dylan at take the highest scoring
one, that's point three here is higher.
OK, then I go to point three and that means is gets into my candidate fact.
OK, and
once I put is into my candidate fact, I then go to is.
So the next thing I do is I go to is and then I again look in the corresponding
attention column and I see what's now the biggest entry here.
And the biggest entry is point four, which is songwriter.
And you can see here now we skip the A.
That's how we leave out some text.
OK, by skipping it, basically.
So you can see that this this can create artifacts, right?
This can create like kind of holes in the middle and so on.
But we skip a we go directly to the point four and then we discover the point four.
That is our tail.
So now we put our tail into here.
And since our tail is the last word, we can stop the algorithm.
I.
Yes, so there is no need to to go on, even if there were texts behind the tail,
as soon as we are at the tail, which we already know, right?
We're given the head and the tail we stop.
All right, so we simply go forward with always the biggest entry in the attention
matrix on tail, we reach the tail.
That's the algorithm.
This this there, it's described here, but
it's kind of described in this in this way where it has these actions like start
yield and like this, maybe I'm not understanding something, but it seems
completely unnecessary to kind of describe these actions.
And it basically start the search from the head.
The head is added as the initial candidate and so on.
Then in yield, it sometimes says with the largest score from the attention
matrix is appended to the end to yield the new candidate and so on.
But still and then stop, we stop.
And the algorithm description here, it basically just says while we're not done,
if we're if it's not the stop action, we continue.
It's it's sort of it doesn't tell you anything like this is this is a super
unclear description of this algorithm.
Basically, the whole logic that you would want to know about is here in this action
manager, right?
So the action manager that gives you the action is doing the actual logic of
figuring out which token you should do next and where you should go next and so on.
This is nowhere in the algorithm.
The algorithm just describes beam search.
So you can do this a little.
Yeah, the little more sophistication that comes in is that you don't do this
deterministically, but you actually do it via beam search.
OK, but you can you can just generalize this.
All right.
So the description is a bit floppy with the whole actions and
action manager and whatnot and not describing the only thing they don't
describe formally is how actually to select the next token, which is basically
the entire kind of meat of the algorithm in any case.
You might this is something that confuses me right here.
So fair enough, you know, they say here we take the attention matrix and we cross
out these X's, all right, but they say they can take things up here, right?
They can take things like Bert and, you know, as I said, fair.
Bert has a full attention matrix.
Everything attends to everything, but they can also take things like GPT2.
Now, GPT2 is an autoregressive language model.
That means that in GPT2, if you look at it, then you produce each token one after
another, which means that when you produce so each token, when you train or
when you evaluate, even each token can only attend to the things in front of it.
Right.
You see that the problem with what this thing requires of this is also the same.
OK, let's do that.
You see the problem with this method.
This method is the exact opposite.
Each token attention matrix is deleted such that only the entries ahead of it
are in the attention matrix.
You don't actually get GPT2 to give you an attention matrix that looks ahead
because it only ever looks behind.
So.
Maybe, maybe what's happening is that the query and key matrices are switched up
in some way, in that case, when we want to interpret the algorithm,
the way they write it down is if I am at a particular part of what I think is
the relation between the two entities, how am I going to find whether or not
there is more to the relation, right?
There could be it could be a multi word relation, like has a child with or I don't
know, can't think of any multi word relations or whether we kind of are done
with the relation and go to the to the tail.
What this thing is saying is that we should look at the language model.
So if if this is really how it is here
and you are at the word is what you want to know if this is Burt, if this is a Burt
language model, what you want to know is if I were to cross out is if I were to
delete this word, which other words in the sentence right here that are ahead
of me are very, very informative to predict this particular word.
That's that's kind of the query style.
And, you know, if the answer turns out to be Songwriter is quite important for that.
Maybe Dylan is too, but we only look ahead.
If it turns out A, the word A is not as important as the word Songwriter, right?
Because Songwriter,
yeah, it gives an indication that there should be is because Songwriter is kind
of a profession and there's a person in front of it.
We don't look at that, but the attention matrix would
would have that in mind.
That's valid, right?
So that's how this this construction is made.
However, if this is the key, we have to think of the other way around.
If we are at is we look ahead and say, if I were to delete the word A,
could I reconstruct it?
How well could I reconstruct it from this word is?
Or if I delete Songwriter, how well could I reconstruct that from the word is?
I think both are, you know, there is
interpretations probably for both of these methods.
But what I want kind of to convey is that none of these things are really
amenable to constructing a knowledge graph.
It's quite interesting that this stuff actually works because all it asks is
how well does one word inform about the presence or how well can one word predict
another word?
And from that information, we construct this knowledge graph, which probably is a
testament to the fact that knowledge graphs maybe aren't so much about knowledge.
If you extract them from a corpus, but more about grammar,
I would think that's a thing that goes on here because these language models are
a lot about grammar, a lot about how different words appear together frequently.
So given that Songwriter is kind of a mix between grammar and basic word knowledge,
given that Songwriter is kind of an object here,
the word is being the verb is probably quite important for it.
And that's exactly these these triples.
They always appear a bit like
in a compressed sentences and which are very grammatically relevant.
So I'm not buying this hypothesis that there is much knowledge in these language
models, and that's why this works.
What I much rather think is that they are really, really, really good at a kind of
grammar and statistical association between words across the language.
And that's why they can extract these candidates facts so well.
OK.
So that's what I think about the algorithm.
They do constrain it some more as if it doesn't already have enough constraints,
but they all make sense.
OK, so they say the matching degree, which is simply the sum of all these attention
matrix entries that we've encountered during our search.
So all the ones we didn't skip or to count it together are the matching degree of this triple.
The matching degree must be above some threshold.
That's the first constraint because so they give an example right here for the sentence,
Rolling Stone wrote no other pop song has so far
only challenged artistic conventions.
And the extracted candidate fact is Rolling Stone wrote pop song.
Again, you can kind of see here it's mostly going into into grammar ish.
So Spacey extracts Rolling Stone and pop song.
And the language model here extracts like the only verb in between wrote.
So,
yeah, to to limit to kind of limit the
the
to limit the matching degree to say it must be at minimum kind of some number.
It makes a lot of sense because if the matching degree is high,
that means if we go by this attention matrix, it means that these words that are in the
candidate fact, they kind of as themselves, they follow from each other.
So the language model thinks that wrote is a very good follow to Rolling Stone.
And pop song is a very good follow for wrote or the other way around,
depending on which way the attention matrix is.
But that's kind of the language model thinks that that these words together make sense
in the context of the sentence, of course, like in the context of this entire sentence.
So, as I said, it's sort of can think of it as a bit of a summarization
paper, but with more constraints.
Constraint number two is that
the frequency of R is above a threshold.
So the relation itself shouldn't be too
specific, it actually should appear a bunch of times in the corpus.
So what you do is, you know, you go through the corpus once, extract all the facts.
My pen just dropped.
We extract all the facts or all these candidates.
And then you kind of count them and go through the
candidate facts again and delete all the ones that are below a certain thing.
That's people usually do this with things like stop words or rare words and so on.
It's pretty standard, makes a lot of sense.
And
constraint number three, relation R is a contiguous sequence in the sentence.
OK, so they have an example here from the same Rolling Stone,
wrote challenge conventions, which the language model would like to extract.
Because again, these in the context of that sentence, these words sort of, you know,
they jump to each other in the attention matrix because you can predict them from
each other very well, but they say this must be a contiguous sequence.
So what I said before,
I said this could happen with this constraint, they excluded.
OK, so for the second part where they actually have to map a candidate fact
to a fact in the schema, as I said, they use kind of pre pre made solutions,
entity linking and relation mapping with the schema.
I won't go into this except to say that
whenever they find a match, they say that this is a mapped fact.
Whenever they don't find a match, they say, ah, this is an unmapped fact.
OK, an unmapped candidate means that at least one of HRNT is not mapped to the schema.
There are two types, partially unmapped facts is where some are mapped and completely
unmapped facts indicate that all HRNT are not mapped to the schema.
OK, for example, Jacob was a registered Mennonite.
Now here they so they they say they have these different facts and,
you know, it's a cool thing if a model like this can actually come up with a new fact.
So not only new mapped facts, which is something you would expect, right?
If humans provide some kind of a schema, then build a knowledge graph.
This is never complete.
So if you can automatically fill in missing facts,
that's very, very cool, though I would say humans,
if you construct knowledge graphs, humans should probably also build kind of like
negative connections, saying like, yes,
it is conceivable that Elvis was a vegan
because a lot of texts talk about it.
But in fact, it is explicitly not.
I don't think that's what we have in the knowledge graph so far.
But it would be cool if this model could fill in new facts.
Yes, to the schema.
It would also be cool if it could uncover completely new relations that haven't
been considered by the human makers of the knowledge graph.
Like if the knowledge graph itself is incomplete, the schema is a man.
You know, same argument.
The schema is probably also incomplete.
This paper is sort of trying to sell their system as something that can do that.
And I believe that to a degree.
But also, also,
Jacob was a registered Mennonite.
OK, now, maybe I'm completely wrong from the sentence.
Jacob was a registered Mennonite in Amsterdam.
I might be completely wrong, but Mennonite is a religion, I think.
And I'm very, very sure that any of these knowledge graphs with the schema
that they have, have being in a religion or being of a certain faith
in their relations table somewhere.
And I'm also pretty sure that Mennonite large enough that that would actually
appear as an entity, maybe Jacob not, right?
Maybe Jacob is an unknown Jacob.
We don't know who Jacob is.
But this seems more like a failure of the entity linker and relation linker
than an uncovered new relation or an uncovered new entity.
So.
Yeah, take this stuff with a grin.
Now, they they are very honest about this.
But just to say that that's probably what happens most often.
So here you can see the graph for Bob Dylan
constructed from the Wikipedia pages that are kind of they say around the page
of Bob Dylan, so I guess one or two or three hops away, something like this.
And you can see the blue stuff is stuff that we already knew so that the human
humans also found when looking at this.
Then yellow stuff, I believe, is either new relations.
So whenever things are annotated, it's a new relation in the schema.
So you can see this is an entity in the schema because it's annotated.
This is a relation in the schema, but the arrow is new.
So the humans hadn't yet extracted the fact that Bob Dylan was or was a member
of artists united against apartheid.
Then the yellow also sometimes means that there is a new thing.
So here tour with is a relation that's extracted.
That is not in the knowledge graph yet.
Also this one.
And you can it's pretty it's pretty cool that you can extract these things
automatically. There is a lot of yellow stuff here, which means there is a lot
of new information that this extracted and a lot of this new information is
actually mapped to the schema, right? Bob Dylan residents in Duluth.
I don't know how to pronounce that, by the way.
Yes, so so that's that's fairly, fairly cool.
They do some of these tasks of these knowledge based tasks.
And these tasks, what you'd have, I believe, what you'd have is always you'd
have like a head and a relation given.
So you have a document and you are given a head and a relation.
And you're asked, what's the tail of this?
And then you ask the system and the system will tell you.
So you have these baselines and these baselines, I believe, they are specifically
made to extract these knowledge representations.
They might even be trained.
I don't know that, but you can see that the MAMA, even the even the smallest one
here beats those by quite a bit.
Now, you can see that the recall is significantly lower than the precision,
which is a direct result of how many constraints on the system there are and
tells you sort of what the going forward, what the improvements can be.
So they analyze a lot of this.
And yeah, so a first recognition is that larger and deeper language models
produce knowledge graphs of higher quality.
BERT language models outperform GPT2 language models under similar model
sizes, which is interesting,
is scalable to larger corpora, which again, as we said, you don't need to train it.
And larger corpora embed more complete knowledge graphs, which is something we would expect.
The other interesting part is the unmapped fact.
So the numbers you can actually compute only for the mapped facts, right?
Because that's where you have data.
Humans produced the knowledge graphs from this.
That's what you can compare with.
Now, the unmapped facts, they say they analyze.
We turn to study the quality of the candidate facts that are not mapped to the above
reference knowledge graph schema, but are in the open schema generated by MAMA.
That's mama.
We manually judge such unmapped facts generated by our best method
from 100 sample documents in wiki data and TACKBP respectively.
So they go as researchers.
They look at these things and they judge them whether or not they're true,
given these documents in Wikipedia.
They say the quality of unmapped facts is very for that.
So the claim is that they've looked at them and they are good.
We find that 35.3 percent of the unmapped facts are true on wiki data.
We find that 83.2 percent of those true facts are partially unmapped facts.
For example, Bob Dylan tour with the Grateful Dead.
And here is an if this really isn't in the schema, right?
This is a nice relation that you might think humans would miss because touring
with someone is not the first thing that would come to mind if you had to come up
with a bunch of relations between entities.
But it is something that is regularly
useful, regularly used for musicians.
So that is an application where certainly an automated system can even extend the
schema, right?
Whose relation is not within the scheme of wiki data?
Well, both head and tail are in the schema.
The registered the remaining true facts are completely unmapped facts.
For example, this red Jacob was a registered man at night.
And they also say accurate entity detection is desired where they say a lot
of the errors are due to spacey detecting wrong incorrect entities or due to incorrect
or missing entity linking by the by that those systems.
The rest errors made by mama are incorrect
relation phrases, such as uninformative relation phrases.
For example, Bob Dylan made and his breakthrough.
What can you do?
What other what other one?
What other verb would you put there?
Yeah.
But OK, we're going to look at a few last things right here.
They have a bunch of a bunch of experiments right here, which
where they show, you know, the beam size has an influence this constraint number one
and number two that we looked at has an influence, right?
So you can tune these things a bit.
What is interesting here is that they try they try to look at either the attention
matrix of the last or of all the layers.
And interestingly, the system performs better if you only look at the attention
matrix in the last layer and they reduce that attention layer because there are
multiple heads using max or mean and see they perform similarly.
But it is interesting that only the last and they argue
they argue in the text that we know that the last layers kind of have higher level
features than the lower layers.
But I recall there are multiple papers like I've done videos about them.
What does Bert learn and so on?
I think even something in constrain in conjunction with lottery tickets and so on
that show that in a transformer, at least, I think it is the middle layers that
encode the most kind of semantic knowledge because the lower ones, yes,
they are for kind of low level features, but the upper ones, they are again for
low level features because the task right here at the end is to predict
an individual word or token, right?
So you'd expect that the features in the attention matrix there or go back to
kind of sort of more grammatical features and so on and that the highest level
features are actually somewhere in the middle.
I don't know if they tested, if they only tested like all versus last,
in which case, yeah, I believe that.
But if they tested each one individually and it still turned out that last is the best,
that would kind of add to my hypothesis that what happens here is more kind of a
grammatical effect of extracting this correct candidate verb in between the head
and the tail, all right?
So that's kind of gives more weight to my hypothesis.
So to repeat, my hypothesis is that it's kind of a grammatical thing that's going
on here because the only task of this model is basically to find the correct
string span for the relation between head and tail because it's already given head
and tail and there from the text, their hypothesis is more like
we the language models have a lot of knowledge built into them and we can extract
that knowledge kind of they make it sound like the language model has this
semantic knowledge in them.
OK, OK, so, so let's look at a bunch of mapped facts.
Right here you can.
OK, you can maybe check out a lot of them yourself,
but we'll just look at like one in each category.
Blah, blah, blah, mail, yada, yada, yada, yada is in worse shape.
However, Klaus told press conference in the western city of Essen where yada, yada, yada,
and it extracts this company and it maps it to the city of headquarters.
Maybe they leave out some text here.
What I want to get to is the unmapped facts.
Where are the unmapped facts?
To just kind of show you mapped facts, unmapped facts.
OK, so the unmapped facts, what I feel and you can judge for yourself, please.
What I feel just to pre-bias you before we look at them is that a lot of times
simply it extracts things that are that are.
It extracts things that are not.
It simply can't can't assign things, right?
It's a failure to assign.
It's not a new thing because in these schemas, like you haven't seen the schemas,
but you kind of get a feel the last which is the last table.
You kind of get a feel of what contains in it.
So.
Maybe get a feel for for what?
OK, Ernst Heckel was born 16th of February, 1834 in Potsdam.
OK, so the extracted thing is Heckel.
Was born on 17th of February, 1833 in Potsdam.
OK, so that it maps to this is in the knowledge base schema.
This is in the schema, but was born on 17th of February, 1833 in is simply a
failure of the relation linker.
Right.
He was also a pacifist until the First World War.
Yadda, yadda, yadda.
Then Ernst Heckel and then was and a and a pacifist are both not in the schema.
Now, maybe pacifism isn't in the schema.
Maybe, maybe, though, I would guess pacifism has a Wikipedia page.
So it must be in the schema because it's a wiki data.
But was as, you know, the relation here with something be like a political
leaning or something like this, which is certainly, certainly in the knowledge base.
Right.
Then you have things like.
Heckel was awarded the title of Excellency.
So you have correctly Heckel again,
recognized award received is in the schema.
Nice. Excellency as a tale and Excellency.
You know, what, what do you want?
Like this is this is a.
This is not a fact, right?
This is the award or the title of Excellency would be kind of the thing.
So this is a failure of spacey.
So again, I have I've seen little facts here that would actually
be of genuine a genuine addition to the schema that should be considered.
And I absolutely believe that the schema is incomplete.
Don't get me wrong.
Like one hundred percent, the schema is probably less than one percent of what it
should be, right, if we did a thorough job.
I just don't think that this system here is
a good like I think that the things that this system comes up with mostly
are simply failures of its subsystems rather than genuinely new entries to the schema.
That's different from when it genuinely
discovered when it discovers a new mapping between already established things.
For example, Pauline Baines, educated at this college, right?
So these are new facts all fit in the schema.
And the system might be very, very nice for that.
All right.
So that was my kind of estimation of this paper.
I hope I didn't rag on it too much.
As I said, it's it's very cool work, actually.
I look at this appendix is giant.
Go look at it, check it out, please.
Tell me what you think about it in the comments.
Any feedback is welcome and I will see you next time.
Bye bye.

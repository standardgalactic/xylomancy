If you sort of zoom out, you look like we are right now in the middle of this like explosion
of something.
And we don't know what it's going to end up like, but, but I do believe as you alluded
to earlier that scenarios in which we just remain more or less like we are now and have
the current human condition just keep going for like tens of thousands of years.
That just seems extremely improbable to me relative to either like extinction slash dystopia
or some radical transformation into some kind of post human condition.
Hello, this is Robinson Earhart here with the introduction to Robinson's podcast number
205.
And this episode is with Nick Bostrom, a philosopher of artificial intelligence and
many other subjects who was most recently professor of philosophy at Oxford University
where he also served as the founding director of the Future of Humanity Institute.
Nick is likely best known in the public eye for his work on the simulation argument, which
is the idea that we live not in the familiar physical world that we believe we inhabit,
but a simulation of one.
But he's also well known for his book, Super Intelligence, which covers the dangers of artificial
intelligence and strategies for dealing with them.
In this episode, though, Nick and I talk about his more recent book, Deep Utopia, Life and
Meaning in a Solved World, which just came out in March, actually, and opposed to get
as opposed to getting into the existential dangers of AI, which Nick tackles in Super
Intelligence, Deep Utopia considers the sorts of concerns that might arise if everything
actually goes right with AI.
So after talking about the alignment problem and some other fundamental issues in the philosophy
of AI, we talk about the problems that might come from having perfect technology, from
immortality, from not having to work anymore, from having robots that can do all your hobbies
better than you can do, and more.
Thanks for watching, I hope you enjoyed this conversation, as much as I enjoyed having
it with Nick.
You're one of the most widely recognized philosophers in the public sphere, and mainly known in
that capacity, I think, for thinking about dystopian, AI scenarios, scenarios, and other
existential threats.
So right off the bat, I'm wondering what motivated this shift to thinking about utopias.
Now, both sides have always been present in my mind and my outlook.
Back when I was writing Super Intelligence, this 2014 book, which was in the works six
years prior to that, I felt it was more urgent to focus on what could go wrong with AI, figure
out where the pitfalls were so that we could avoid them.
This was a time at which the whole idea of AI posing any kind of existential risk, or
even just having any kind of transformative impact on society was still far outside the
mainstream.
The whole idea of the alignment problem was not in the Overton window.
There were maybe a handful of people scattered around the world, internet types, who were
trying to work on this, and it just hugely neglected to me.
So I thought writing this book, Super Intelligence, trying to develop concepts that would make
it possible for people to start to think more constructively about this and begin to do
research on developing scalable methods for AI alignment was important.
In the intervening years, we've seen a big shift.
Now all the frontier AI labs have research teams specifically trying to develop scalable
methods for AI control, and there are many other organizations as well, where a lot of
the smartest young people I know now are flooding into this field.
In the last two years, we've also seen a big shift at the level of policy conversation,
where even top tier policy makers are now beginning to recognize the transformative impact of
future AI development, with statements coming out from the White House, and the UK hosted
this Global Summit on AI.
I was just the other week in Brussels for a however meeting.
To some extent, that whole thing is now widely recognized, and many of us are working on it.
I felt the other side of what happens if things go right with AI had not yet really been addressed.
People talk about it, but often in a rather superficial way.
But once it started again, you realize they're actually quite deep, even philosophical problems
that come up.
And what would give human life purpose and meaning in this condition, where in a solved
world as I call it, we're all practical problems that can be solved through progress have already
been solved.
So that's how the book, it wasn't really so much a plan to write the book, it more kind
of happened as sometimes is the way of these things.
Like you start writing a little, and then it takes on a life of its own, and you just
try to hold on as the thing gradually develops.
I can tell that this book really did take on a life of its own just because of the various
formats in which you wrote it.
I mean, there's poetry, there's dialogue, there's standard exposition.
So it was clearly a very creative project in a way that, I mean, philosophical articles
are still creative, but it's more of an intellectual creativity than an artistic creativity.
Yeah, some people maybe think they should have been a sort of more tough minded editor
hovering over me to kind of, but it actually does serve a purpose.
Well, for a start, it's less a book about conclusions than it is about questions and
exploration.
And I think this format with multiple different characters and voices helps you explore several
different sides, giving each one their due and letting them speak for themselves different
values.
So that's one reason for I think the form kind of works for the content.
The other is that the goal of the book is, well, in part to try to give the reader various
concepts and considerations and such that they can, you know, better think about these
things.
But another part is to try to put somebody in the right frame of mind for confronting
these.
If you imagine that will actually perhaps at some point be some group of people who will
have to form some opinions about what we want long term from these AI developments, you
know, whether it's a few people in some lab or a government or some more humanity wide
deliberation process.
These are really difficult questions to deliberate about that I would like people who go into
that to come at it with a certain attitude.
So a kind of broad minded generosity combined with some playfulness and thoughtfulness and
open mindedness, which I'm hoping that the book encourages.
And I think that those creative elements are also meant to contribute to that.
That it is a long, it's a long read.
So it is also maybe an antidote to short attention spans issue.
Well, before we move on to some of the substantive issues, just a couple of comments.
It really was a great way of delivering a number of concepts to the reader.
I mean, before reading this book, I just thought of utopia as kind of a blanket concept.
But now I realize and we'll get into it, there are various different types of utopias, each
of which have their own problems and considerations to keep in mind.
And then you said that this book tackles very deep problems about meaning.
And it's also, I think, very useful as a thought experiment for testing an extended thought
experiment for testing our intuitions about meaning in the present.
So it's not just like it's useful for these future utopian scenarios.
It's very useful for thinking about our lives today.
For instance, there's a passage on shopping that resonated with me very much as somebody
who enjoys shopping.
But returning to the substance, two things that you mentioned right at the outset that
I think we should pin down for our listeners who might not have read superintelligence
are, one, what superintelligence is, since that's vital for the understanding of a deep
utopia.
And then also what this alignment problem is, because even though we will presuppose that
it has been solved for a lot of this conversation, I think it's still important to understand
what it is.
Yeah.
Well, so superintelligence is kind of any intellectual system that radically outperforms
all humans in all cognitive fields, including scientific creativity, wisdom strategy, like
the full range, really.
And the alignment problem is the technical question of how if you figured out how to
make an AI that's generally capable of learning and planning and reasoning, how could you
then steer it in some particular direction to make sure that even when it becomes much
smarter than you, its creator, it still does what it's supposed to do, like what you intended
for it to do, that it's kind of on our side as opposed to becoming this antagonistic force
that has different goals than we tried to give it, that then works at cross purposes
with ours, broadly speaking.
And there are different ways that different people have tried to make that precise, but
that is more or less the gist of it.
So yeah, this book kind of just assumes or postulates really that all of that, the alignment
problem is solved, also that the governance problems are solved to whatever extent they
can be solved so that imagine we actually don't use this powerful technology to wage
war against each other or to oppress each other and that like all these things that can be
solved, have been solved to whatever extent possible in order then to actually get to
the point where we can ask these questions about value.
I have of course a lot of things to say in my other writings about these issues of getting
from here to there, but there is a risk that if one starts to write about those, one never
actually gets to consider what it's all for, ultimately, like we are, and I think this
holds in general with our existences as well.
We are very busy like putting one foot in front of the other and maybe don't always take the
time to reflect on where we are going, especially at the civilizational level.
A lot of effort is being put into making progress, like growing the economy, improving
the technology, making things more efficient, but nobody really seems to have a clear notion
of where does this lead us to in 50 years' time, in 500 years' time, but anyway, that's
all kind of, now I don't know whether superintelligence is strictly speaking like a premise or it certainly
is one technology, which if we had that, it would unlock a lot of other technological affordances
and you would more quickly approximate this condition of a solved world, but you could
in principle imagine getting there just through a slower accumulation of human-driven automation.
You could maybe imagine in the limit, just as you can create a little program that automates
a specific task on your computer or create a robot that makes one action in a car factor
or something.
If we just kept doing that for 100,000 years or a million years, maybe eventually we would
have software robots that could do all the tasks and you could kind of at least get close
to a similar situation without superintelligence.
You said that, so both the dystopian and the utopian sides of the coin were always on your
mind from the beginning, but at the time you wrote superintelligence, though it had been
gestating for seven or eight years, you thought it was a more pressing problem to deal with
the alignment problem and the pitfalls of AI, but what I'm wondering is, so there was
a practical element in writing superintelligence.
I don't want to get bogged down in how we get from here to superintelligence because
as you mentioned, that can take us very far afield and we won't actually ever get to the
utopian questions, but is there also a sort of practical dimension to writing about deep
utopia because you think it is near at hand and we need to start thinking about it or
is it more at this point just a philosophical exercise?
It's more the former for me actually.
I do think we are on a seemingly fast track towards the AI transition and so it does give
the whole set of questions greater practical urgency and it might be relevant in all sorts
of ways.
But for example, the points at which we may need to make some trade-offs between taking
on risks and undergoing this transition sooner versus delaying it in order to hopefully make
it safer, although that exposes us to other risks independent of AI, and some of this
might hinge on how urgent we think it is to improve upon or get out of the current human
condition into something better.
If you think that anything on the other side is bad, then you might just want to preserve
the status quo for as long as possible.
If you think there is this very wonderful utopia on the other side, then maybe you think,
well, let's make sure we get there before we all just die from aging or something and
it would be worse taking some risk to make sure that we can reach that point.
These questions do, I think, flow back possibly to decisions that people have to make, but
the main use case would be this.
If people are at one point having to design that trajectory forward and maybe they with
AI advice are able to anticipate where it will lead, and one trajectory maybe leads
to some kind of hedonism maximizing outcome, another leads to some other complex situation,
a third might lead to some kind of radically planetarized brains optimized for processing
scientific knowledge or something.
And if some people need to make some judgments about the relative desirability of these, then I
think it would be good if they had thought more about it than their decisions were not merely
reflecting on the latest thing they had happened to read on Twitter or like how they
happened to feel that day.
Like if they wake up in a good mood, they think, you know, it's the future is great and has a lot
of opportunity if they happen to wake up depressed, they kind of are a nihilist.
I think it would be better if the world just disappeared.
And ideally, whatever decision is made should reflect the kind of broader set of considerations
and values and perspectives than that.
And so, yeah, contributing to that.
And I think I guess one more thing to be said about that is that the prospect of the future
gone well, I think is greater if people have this sense of there being great possibilities
to realize not just one value, but many values that if the future goes well, it unlocks this
huge, I mean, would have this super advanced technology, plentiful resources, a lot of time.
And there would be a kind of abundance.
And I think that can help people approach things in a more generous way than if there is extreme
scarcity and like one person's gain is another's loss and there is not enough for everybody,
then it's very hard to be generous, right?
It's easier to be generous if you have a lot.
And so thinking ahead, if we could approach the future in this more cooperative and generous
way, I think that also reduces the risk of various dystopian outcomes and improves the
prospects.
So those are some thoughts in the background.
Yeah, to speak, I guess, a little bit proverbially, though, that abundance has its own problems
because what is a pleasure without pain?
And when we have everything, I mean, this is a big part of the book in your project is
where we find meaning in a world where we have everything and there is no scarcity to
drive purpose.
Right.
Yeah, so that's like the kind of the problem then that one confronts.
So much of our lives currently are structured around the various practical necessities that
we face in our lives.
So you have to go into work every day because you need to get the paycheck which you need
to pay the rent.
You have to brush your teeth because otherwise your gum will decay and you have to do this
stat and you have like this like so much of our lives are just like doing something in
order for something else to be caused as a result.
Right.
And that that that kind of fills our life with sort of goal oriented activity.
But in this world, a very large chunk of all of that would no longer be needed.
Like you wouldn't have to humans wouldn't have to engage in economically
productive labor because AIS and robots could do everything that needed to be done.
Yeah, you wouldn't have to like spend an hour at the gym working out in order to be fit
and healthy because you could pop a pill that would give you the same physiological
effects and you can kind of go through activity by activity and you find that for many of
them you can sort of either cross them out or at least put a question mark on top of
them that although you could still do them a technical maturity, they would seem to be
a kind of pointlessness hanging over them like a dark cloud that maybe remove some of
their value.
And so and this is like, I think, yeah, part of what would make from our current vantage
point, some of these scenarios, at least prima facie look unattractive.
We kind of base our self worth and dignity on the sense of making some doing something
useful like you're a breadwinner or maybe you contribute to society in some other way or
you like you look after your kids or your grandkids and you like through your efforts and
strivings like you actually make the world better in some place that it could in some way
that it could not otherwise be.
And if that's removed, then yeah, there is a void that the utopians would have to somehow
fill in.
I think a lot of the fun in this project comes from your imagination and looking at all the
various examples of what life might be like and the technologies that might be available in a
what you refer to as a technologically mature society.
And I want to talk about those things.
But before we do, I mentioned earlier that there were, I'm not sure if it was five or six, I
think five different utopias that you discuss in the book, whereas I had thought that I just only
ever thought there's one kind of utopia, it's just heaven.
And I had left that unanalyzed heaven or something like heaven where everything's perfect.
But what are the different types of utopia that you think are relevant to this
discussion and how do they differ?
And what are some of the basic problems that they confront the utopia dweller with?
Yeah, so this is how I classify them for my purposes.
But you could think of a first category of sort of culture and politics utopias.
Many of the classical works fall into this category where basically the author tries to
imagine a better political system or a better sort of culture.
So maybe you think there would be a different system of governance or like the gender roles
would be defined differently or the way that children relate to their parents.
And then it like different authors have different views about the optimal way to arrange this and
the optimal way would then be utopia.
Now, a slightly more, I guess, radical or like at least in one dimension, more radical conception
of utopia would be an abundance utopia where you imagine that there is plenty to go around.
So all material needs are abundantly supplied.
And you do also find some of these in the historical records, like there was a kind
of medieval fantasy of the land of Kokai that was a kind of peasant vision of like the idea
life the rivers would flow with a wine and roasted turkeys would kind of drop down on the
plates and that would be music and dancing and a lot of time for relaxing.
And you can see if you were like a kind of agricultural labor, like doing backbreaking
work from morn to dusk, and then just to have barely enough porridge to eat, like this would
already be like a fantastic conception, right?
Like you could eat as much as you want, you could just stuff yourself with food and rest all day
and like that.
So that would be kind of an abundance utopia.
And then then we have a kind of post work utopia, which is different from abundance
utopia in that not only is there plenty of everything, but humans don't need to work to produce it.
And this is about as far as most mainstream conversation has gone in the context of AI, you
do find some economists who are starting to think about automation and whether it could
cause unemployment.
And if so, what would happen?
Like, you know, you maybe need some universal basic income, etc.
And like in Marx's utopia, he doesn't fight very much about it, but it seems he imagines that
would be a sort of certainly a culture politics utopia, according to his conception.
And then that would be like less work and more abundance, but that would still be people would
still be working.
So it's kind of, but I think there are levels beyond that that you could consider, which is
much more radical and poses much more deep problems in terms of human values.
So we have a post instrumental utopia where it is not just that need for human economic
labor that is rendered out by technological progress, but the need for all human labor
that is instrumental in character.
So I mean, I mentioned like, you might have to labor at the gym to be fit, but that you
wouldn't have to do in utopia.
Right now, if you want to understand mathematics, you have to first study mathematics and spend
hours working on exercise problems and like really exerting yourself, right?
But in this condition of technical maturity, there would be shortcuts to the same end.
You could imagine some swarm of nanobots infiltrating your brain and rewiring your
synapses into a condition where you now possess mathematical knowledge without you having to
put out any effort.
So that kind of, you know, pulls the rug out of a wider range of human activity and exertion,
not just like the job that you go and do for eight hours a day, but most of what fills
the rest of your time.
And then there's like a final stage beyond that, which is I call a plastic utopia, which has
all the attributes of a post-instrumental utopia.
But in addition, the human organism itself becomes malleable and subject to our wishes
and desires.
So you could sort of choose what emotions you want to have or what thoughts you want to
have or what moral character you want to have or what like physiology, you have technologies
to kind of reshape yourself at will, including your psychological states.
And that then results in this kind of solve, they're almost this old world where like all
the kind of hard limitations that we currently face like are removed, it seems.
And, and the only thing that remains are kind of our values that I think have been shaped
under this condition of scarcity, which has like always been there for, for humans throughout
history, like, and it's almost like formed an exoskeleton, these practical needs.
And if you were to remove all of those practical necessities, there is like the question of
what happens to the, the soft squishy parts, like, do they just become a blob, like a drugged
out, a kind of pleasure blob, or can they take some more interesting shape?
And, and yeah, so that's, so that's like the main, there are a couple of early chapters
that talk about some of these poor economic and technological, but then the bulk of the
issue is literally set in this condition of solve it as a solved world, that this plastic
and post instrumental.
I'd like to digress for a moment to this mathematics case that you brought up, which I, I find
particularly compelling myself, especially because it's an example of something I, I said earlier
where, and they get examining what a mathematician might do in, let's say, a post instrumental
utopia might tell us something about what mathematicians do and value today that might
not readily be apparent.
So in this post instrumental world, where we have extremely powerful, automated theorem
provers that might enumerate all the theorems of math and all the interesting axiomatic
systems, and there's no longer a role for mathematicians to be providing new proofs.
Do you think mathematicians would still be around?
Because I mean, this is something that isn't, this is one element of utopia that might not
be that far out, unlike some other ones, because we already have automated theorem provers
that are doing work that mathematicians can't do.
Yeah.
So I mean, people do, I mean, people play chess, even though we have computers that can play
much better chess, or solve crossword puzzles, even though there is no need for crossword
puzzles to be solved and do a lot of other things currently, right?
So the first answer one might think is, yeah, yeah, sure, we'll just do it anyway, because
it's fun.
Now, I think we can't stop at that point.
We need to like unpack this idea that we do it because it's fun.
So that could mean we do it because it gives us pleasure, like somebody who is fascinated
with mathematics, probably derive pleasure from engaging in mathematical thinking and
occasionally maybe finding a solution.
But if that's the only reason that would be, again, shortcuts to attaining pleasure in a
solved world, you could have more direct forms of brain manipulation that would stimulate
your pleasure centers.
I mean, you could imagine it as some sort of a super duper drug with outside effects and
addiction potential, or more likely some direct way of manipulating the relevant neural
structures.
You know, maybe we're all digital at that point anyway, and you just, and so yeah, if all you
wanted was to experience positive affect, there would be no need to do mathematics for
that purpose.
So if you do, nevertheless, think that it would be better to do mathematics, you then it seems
value something other than just this hedonic state that you know, very imperfectly and grudgingly
like maybe like some little drips of pleasure is derived every once in a while when somebody
who is enjoying mathematics does mathematics.
And there's probably just a lot of sort of boring futile effort and like uncomfortable
straining to get to those occasional little drips of reward.
That's the stingy way our current reward systems are architected, right?
So you could kind of open the floodgates to that and then have a kind of more blissed out
psychological condition.
And so that's easy to dismiss a lot of people like immediately think that wow, that's like what a
horrific view of the future that is, we're all kind of like junkies, sprayed on some sort of
flea-intested mattress, but we like a super drug dripping into our nucleus accumbens.
And it might well be that ultimately, we want and can have more than that, but it is actually a
rather deep question whether we shouldn't dismiss too quickly the super bliss as an element of
what we would ultimately choose and have reason to choose, perhaps.
Nevertheless, I think we can have that and have a whole bunch of other things on top of that that
may be satisfy other possible value candidates.
So certainly, if you do think that it is good to engage in certain types of activity, you could
do that as well.
There is no reason you could have complex experiences, pleasure plus complex experiences,
plus various forms of goal-directed activity, but we can talk more about why you would be adopting
various goals if you were in this post-instrumental situation, and then perhaps some additional
elements even beyond that, that you could kind of reconstitute some of the prerequisites for
instantiating human values even in this solved world condition.
I think what motivated my asking this question was reflecting on two conversations I had in the
past on the show.
One was with a renowned number theorist at Columbia University named Michael Harris that we had a
couple of years ago, so it's a little faint in my mind.
Another is with Steven Wolfram, and we were talking about math in a world in which all of math is
essentially automatable.
And today, I think our folk view of what mathematicians are in the business of is just producing new
proofs, discovering or inventing, depending on your philosophical view, more facts to be added to
this big book of math.
But what Michael Harris said and what I think Steven Wolfram agreed with is that mathematicians
aren't just in the business of producing new facts, they're in the business of understanding.
That's what really drives what they're doing is they want to understand these structures.
And I was thinking that in this post instrumental world, when mathematicians no longer have to
produce proofs, they still would be very interested in understanding and maybe teaching
mathematics.
But then this raises another problem that I hadn't considered yet that you discuss in some of the
later, the more advanced types of utopia where we have a matrix like system of downloading
information, where there's no law into our brains, where there's no longer any real barrier to
understanding, you can understand anything as soon as you just downloaded into your mind.
So there's no, there's not really an activity of being a mathematician anymore.
Just if you want to understand the math, then you press a button, then you understand the math.
Yeah, so this is a very advanced level of technology.
And it's like a little unclear exactly how close to this we could get.
But certainly, if you imagine a human upload like a digitized version of a human mind, and then
you have on the outside, some machine super intelligence that is able to understand how the
neural network that is you sometimes successfully can grasp mathematical concepts.
And in other cases, fail and what variations of your neural network would be required, like what
edits to make it as similar as possible to you now.
But with this extra mathematical understanding, I think it possibly would be the case that the job
of working out how to adjust the various parameters in your brain to give you this mathematical
knowledge could be outsourced to this machine super intelligence that is looking at a model of
your brain, and probably be able to do that without actually running a detailed simulation of
your brain, which if it if that were the only way for the super intelligence to work it out,
you might then think, well, you would actually then be instantiated in the simulation, and you
would have to put in the effort, although the simulated version of you would do.
But I think probably you would be able to move up levels of abstraction to more or less obviate
that need. And so that you would then be able to, yeah, in effect, download skills or knowledge
without having to put in time studying.
And so yeah, I think I mean, and the teaching of mathematics is even more obvious.
I mean, I think we are very close now to having tutorial systems that would be much better than
most like high school mathematics.
I mean, aside from keeping the class disciplined and making sure everybody sits at their desk or
whatever, like, but the actual instruction, I'd imagine maybe even chatty pt4 would like be a
better explainer than than than most high school teachers, because it can keep track of exactly
where you are, where you get stuck on a particular problem and sort of customize its explanation
for you, as opposed to doing something generic for the class of 30.
Just curious, are you drawing the sort of distinction between chat gbt or gbt4 being
a good high school teacher and a good university level professor?
Because maybe there is insufficient training data on university level material for chat gbt
to be able to explain it better than a university professor could.
I mean, I don't know, I don't mind that there need to be some fine tuning maybe to adapt
these large language models for specific use cases in the classroom.
I don't mind the standard university courses like, you know, first and second calculus and
like all of that stuff.
Probably it would be able to explain I don't mind as well as a mathematics professor.
I think maybe once you get to the research level of mathematics, whether it's more a matter of taste,
and then I'm not sure the current language models are quite yet there yet, whether it would be as
good as like a decent mathematics professor in terms of figuring out whether a research direction
is is promising or whether some proposal for say a PhD is like the right level of difficulty that
the student could possibly do it in four years.
And like, like I think there there is like, well, there's obviously less training data written up
for ingestion.
And also it involves the most high level human faculties that haven't yet perhaps
quite been attained by the current generation of systems.
A term of art that you introduce in the book that I referenced earlier is technological maturity.
And I also mentioned that this is one of the very fun things for me of reading deep utopia
because we get to see your imagination at work and all the possible things that a technological
mature society might have at its fingertips, so to speak.
So how do you define a technologically mature society and what are some of the things that
people might expect to be able to do or have on offer?
Yeah, so it's like a condition at which
all those technological affordances that are consistent with the laws of physics and for
which there is some possible trajectory from where we are now to their development exists.
So, you know, it might be that you can't ever strictly speaking attain perfect technological
maturity, but in reality, you might get some kind of close approximation to it.
If you have developed all the most useful general purpose technologies.
Is perfect technological maturity being able to do anything that's physically possible?
Well, I would also have this pathway there, but it might just be that
if you imagine there are kind of an unlimited number of more and more specialized technologies
for doing specific things, it might be possible to develop any like small subset of them by like
investing your R&D resources. But if there is like quadrillion and quadrillion of different
very specialized technologies, maybe you can't develop the whole set of them.
But I think that would be sort of ways of getting close to that for most practical purposes.
Like once you have superintelligence and nanotechnology can sort of come up with
the superintelligence can make arbitrary designs and the nanotech can sort of put it together.
And then like some other things you can I think get pretty close to that.
So, and I think we know already at least some lower bounds of what this
such a condition of technical maturity would involve. That is technologies
that we can already have good reason to think are physically possible and for which
there is a pathway such that we could at least in the fullness of time
under favorable condition get there. So, superintelligence is one of these.
Like I think perfect virtual reality of the sort that would be distinguishable to the person
inside the physical like virtual reality from ordinary basic reality would be another affordance.
I think like cures for aging, uploading into computers, space colonization at large intergalactic
scales, perfect neural technology in the sense of being able to control precisely our hedonic
and emotional states, cognitive augmentations like many other things as well. But at least
that already gives you a huge set of possible like things you can do right that technical maturity.
So, the way that for our listeners who haven't looked at the book yet, the book is organized.
So, it's kind of like a course you're teaching to these imaginary students and at various points
in the book there are handouts where you've summarized some of the material and I have
handout number two, some capabilities at technological maturity on the screen before me
and I was just glancing at them and one that jumped out at me is the possibility of
Dyson spheres for harvesting the energy output of stars and this comes to mind for two reasons.
One is, well I'll just focus on one, but the reason is I've done a number of episodes on
string theory and been thinking a lot about string theory lately and one of the big barriers to
empirical confirmation of string theory is that we cannot generate enough energy to probe
sufficiently small distances to confirm the existence of strings and people have conjectured
that the sort of energy you would need is the energy of a star. So, it looks like we'll be able
to confirm or disconfirm string theory in this. A very useful technology, the Dyson sphere.
But yeah, like you mentioned, arbitrary sensory inputs, reversal of aging,
uploading of biological brains into computers and all of these things
engender their own host of philosophical questions to be asked and answered,
aligned police bots and automatic treaty enforcement. I mean, and police bots are
already a question being asked today, but you know, reversal of aging and cures for all diseases,
that's one that jumps out at me or a couple that jump out at me right now.
Are there any particular problems that would need to be dealt with in these later stage utopias
if we just lived forever? Well, I mean, the most obvious one would be the size of the population,
which is able to increase exponentially. And if nobody dies, then at some point,
the rate of births would have to match the rate of acquisition of new resources
in order to maintain high per capita incomes. Because at the technological maturity, that
wouldn't be increases in productivity. Like that's one of the things that drive
technological growth now, right? Even if the resources are the same, you could
have better technologies to make more efficient use of them, but that will have maxed out.
And so then the only way that the economy could grow at that point is if more resources are
attained through expanding in space. But that can at most happen at a polynomial rate,
like you have a bubble that is growing in all directions at some significant fraction of the
speed of light. And the volume of that grows as a polynomial of time, whereas the population
like could grow exponentially, right? And so then eventually, you would have a Malthusian condition.
And so in the limit, you would need to sort of make sure the rate of population increase,
which might be sort of digital minds making copies of themselves, right? That would kind of
at most match the rate at which the resource endowment increases. Like another maybe more
philosophically interesting question, if we eliminated aging, and then I mean, right now,
what would happen if you eliminated aging is that people would die from accidents and wars.
But maybe with a lifespan of a few thousand years, if you kind of calculate the rate of death by
accident. Now, hopefully, we will bring down the rate of accidents and war, certainly in Utopia,
you'd imagine that to go way down. And if you could upload yourself to computers,
then you could make backup copies and stuff, and then the rate of accidents could kind of
become arbitrarily small. So then you might have very long lifespans, even astronomical.
And that presents more interesting philosophical questions for human values in that.
Well, first of all, we don't know what would happen to like if you remained
a human mind of like a brain with your current size and the number of synapses and stuff,
like we know you can continue to learn and develop and grow for like a hundred years. But
we don't even know what would happen if a human just kept living for 400 years, like would you go
stale and rigid and like so. And even if that didn't happen in a sort of like neurological way,
like then it might still be that. And it's a hard question to sort of gauge, but that
the number of different types of things that can be done with a human mind and body in a human
world is finite. And is that number small enough that it would become relevant so that
at some point you would just run out of interesting new things to do? And if so, how long? Like I mean,
I think given that we are finite, there has to be some number of years after which you would
literally be doing the same thing you'd already done. But if that is kind of 10 to the power of
100 or something, we might not have to care very much because we'd die anyway before that from
the heat death of the universe. But if the answer is like 10,000 years, like after 10,000 years,
you'd basically just repeat yourself that either you'd have to accept that such lives will have a
diminution of a certain kind of novelty and that may be a price to admittance and this kind of
extreme longevity in Utopia. Or you would have to perhaps start to level up after you have spent
at 10,000 years being a human, maybe then you would want to become a transhuman, like increase
your intellectual faculties or something. And you could kind of start to tackle the next level
of challenges and keep developing that way. But I think it is one direction in which you can kind
of take our current human values and where they start to become strained if you just imagine
the current human lifespan with our current values and our current faculties but just extended
and like prevented from physical disease. Like at some point, I think that would
possibly become unattractive. Like there are still a lot of things in human life that don't
require novelty. I mean, like a cup of tea is like about as good like the 10,000s time as it is
the second time or first time, right? It they're like renewable pleasures. And sometimes you could
run the argument that sometimes the best things in life are the little things that it's not like
like the big dramatic narrative climaxes. But it's like, you know, the smell of autumn air
in the evening along a stroll along the coast or like looking into your loved one's face or
a little, you know, a cup of tea or whatever, like these little things. Sometimes, you know,
you could run the argument that that's a fairly high quality value, although we give them kind of
short shrift. An interesting connection to the philosophical literature going back
hundreds of years, I mean, is how conceptions of personal identity might shift in this world
where we live for, we live indefinitely. I think, and you should correct me if I'm wrong, but I
think it was Locke who proposed that personal identity has something to do with memory and
psychological connectedness. And maybe it was Derek Parfit in his sort of ongoing discourse with
David Lewis, who questioned whether somebody who lived, who wasn't immortal would eventually
no longer be the same person that they started out as because they would not remember,
let's say their childhood. There would no longer be any psychological connection
to a person stage thousands and thousands of years earlier. And in this sort of utopia,
maybe we would bypass this by getting memory implants or memory chip implants so that we
could just continually add new memories and maintain internal psychological connectedness.
Yeah, I mean, I guess there is, first of all, I would want to retain the distinction between
long time and infinity. Immortality, I think, should mean not dying, not just taking a while
before you die, conceptually. And it is important because immortality in that strict sense just
might be impossible in our physical universe and be the preserve of say more theological scenarios.
And from the blink view of eternity, like whether you live for like 80 years or 80,000 years,
it's kind of in some sense the same, right? It's an infinitesimal short relative to infinity,
either way. Now, I do think that, yes, increasing, I call it time, time suits, but the idea that we
could make some modifications to us that make us better able to survive radical change without
having our identity eroded or our values corrupted. So the most obvious thing would be to improve our
episodic memory so that you actually don't just forget what you've been through. And that could
make you sort of remain relevantly the same for longer than if you just have like the memory of a
goldfish. And you could imagine some other tricks as well that would allow us to sort of endure for
longer periods of time and upgrading our capacities in different ways could help with that.
But even if it came to a point where you would have to accept some loss of personal continuity in
whatever relevant sense, I mean, it might be acceptable. Like if you were have a five-year-old
and you know that if there were a pill that would just arrest their development, that they would
remain a five-year-old, like in some sense, maybe a year from now, that would be more similar to what
they're now than if they're just allowed to freely develop. But we might still think it's better for
them to grow up. Like in some sense, you're the same person now as you were when you were five,
but realistically, you're also quite different, right, in many ways. And maybe that's not an
unambiguously bad thing. And so I think the combination of these, that we can reduce the
negative effects of various kinds of corroding consequences of the passage of time through
improving memory and other certain other positive attributes, like also including like maybe extending
your planning horizon, etc. And then also accepting some degree of change, even if it does mean that
you eventually move further away from what you once were. There's still a difference between
that and just dying. Because at any given point in time, you might look forward to hundreds of
years of existing in some very similar things to your current condition. And you would gradually
sort of, you know, see new ways of doing things or being or new experiences. That feels a lot
more optimistic to me than like a kind of, you're going to be shot tomorrow morning, right? It's a
smoother and gentler way of losing our connection with the past seems perhaps
potentially preferable, even to the extent that it is unavoidable relative to the alternative.
Okay, not that a lot of this or all of it hasn't all been speculative, but I have a
particularly speculative question to ask. So when I spoke with Avi Loeb, the Harvard astrophysicist,
this brings us back to Dyson spheres. He conjectured that Uumuamua, this comet that
passed us by in, I believe, 2017 might have been part of a Dyson sphere. And you have made your own
very, I don't know if splashy is an adjective, but powerful, controversial conjectures about,
for instance, whether or not we might be in a simulation. And I'm wondering if you think
it is possible or likely even that there are societies out there that have already reached,
for instance, a post scarcity utopian stage, because I think the having a Dyson sphere,
if whoever sent Uumuamua our way had a Dyson sphere, we would classify them probably as being in a
a post scarcity period, assuming that they still have other Dyson spheres.
Yeah, I mean, so that I mean, post scarcity, they might not. I mean, it depends a lot on what their
goals are. And it might be that the Dyson sphere of resources is nowhere near enough for what they
are trying to do. And so it is to some extent a human concept, like, it's not like a very precisely
defined concept. It's like, you can say like from human. So you can say that we are already
somewhat close in some respects. If you're if you are like, fortunate enough to live in a in a
rich country, and you have a, I don't know, like, you're healthy and trying to get education, like
probably most of your listeners, like, it's probably the case that if you wanted to, you wouldn't
really have to work to survive. Like, or if you select say, say you worked hard, really hard for
five or 10 years and saved up all the money, you probably could then move to Thailand or something
and buy a small hut near a beach and then have enough to eat and be physically healthy. And
you'd have a computer, you could access the internet or whatever for the rest of your life
without having to work, like, it's at least tantalizingly close. But yeah, people choose to
continue to strive because they want more than just the basic necessities of life,
right? Like in particular, we want to have more than other people. And so there's a lot of this
kind of zero some status consumption going on with humans. And that could drive scarcity up to
astronomical levels, right? Like, yeah, you have your own Dyson sphere, but like I have four Dyson
spheres. So like you still need to try to catch up and an intergalactic civilization might have
practical reasons as well. Like maybe they fear some other intergalactic civilization that might
have a larger fleet of warships or something. And so but the difference is like, even if human
desires are insatiable, it doesn't mean there will always be a need for human work. Like,
you could reach a condition where even though like, you maybe are worth a trillion dollars,
and you would like to make another trillion dollars, there's just nothing you can do
with your own labor that would make you any significant amount of money, because all the work
is better done by machine. And if the most you could make is the minimum wage, then once you're
a trillionaire, like, there is no point really working, right? Because it's trivial compared
to what you just earned from your capital. And it might even be that by doing the work yourself,
you like expend more calories that cost you more than the actual value of the labor. So you could
still end up in this kind of condition of unemployment or postwork, even if there are
still needs that have or desires that haven't been fully satisfied. Now you asked whether I think
there are already some that have attained either poor scarcity, or I guess you could
generalize it into technological maturity. And I think if the universe is infinite, as it seems
to be, like if we have the simplest topology and an open or flat space time, then definitely that
would be such civilizations, in fact, infinitely many of them out there. But if I had to guess,
I'd say none in our in the observable universe. And so that that would be infinitely many of these,
but they would have low density. And so we might be out of causal contact forever with the nearest
other one. That certainly would help explain the Fermi paradox. But it's not the only possible
explanation. So we can't rule out that we could share the observable universe with some other
civilizations as well. Speaking of intergalactic civilizations and intergalactic warfare,
obviously, that would be a huge barrier to us reaching a utopian state if we had to
offend off an alien invasion. But do you see any other barriers beside just
our ability to produce a super intelligent technology or super intelligent AI, other barriers
beyond that to our reaching utopian states? Yes, I mean, first of all, it would be crucially super
intelligence is well aligned. Because otherwise, yeah, that might be our undoing. And second,
if you have scenarios, multipolar in character, like if you have many different entities,
ultimately, with our own AI assistance, that then a lot of the same dynamics that we see on the
planet today with arms races and oppression and warfare and exploitation of the global commons
by sort of spewing pollutants into the atmosphere or refishing the oceans, all of that could still
occur. And indeed, it could be intensified in this kind of hyper competitive economy that might be
created. If, on the other hand, you don't have a multipolar, but like a unipolar or a psycho like a
singleton scenario where it gets all consolidated, then there is the obvious question of who controls
that singleton, right, and how benevolent and wise is whatever the mechanism, whether it's like a
global democracy or a dictatorship, whatever it is, is like, so I think we can identify several
broad categories of things that need to fall in place. So one is to have this kind of
deep utopia, you need super advanced technology. Like otherwise, you just can't do all of the
right now, we can't cure the kid with a cancer in many cases, right? And that means our world
will fall short of what it should be. So the super advanced technology, but then I think also we'd
need a fairly high level of cooperation slash good governance to prevent sort of negative
some conflicts. And then we'd also need some adequately high level of wisdom in how we use these
great technological powers that we have. And it might be that the wisdom has a kind of threshold
effect, where if you're above that threshold, even if you're still unwise in many deep ways and
have many erroneous beliefs and misconceptions, you might have enough wisdom to realize that you
are fallible and to start to seek out ways to fix, you can reflect on your own shortcomings and
gradually develop ways of remedying it. But if you're sort of below that wisdom level, you might
be more likely to lock in your current prejudices, or to shoot your foot off when you're trying to
like develop ways to fix it. And it's kind of actually an open question, I think, where humanity
is relative to that. Because you could ask the same question at the collective level, like are we,
I think we're maybe close to the threshold level, but I'm not sure whether we're just under or just
above it. And then we might need some bit of luck as well on top of that. But I think, yeah,
although these are kind of extreme technological postulates, and you might seem very wild ideas,
I still think they might actually not be completely unrealistic, even within our lifetime,
if this whole AI transition happens. And it's, if you sort of zoom out and look at the human
history, like, from from its inception, like, you know, a few hundred thousand years ago to today,
and you plot that in any possible way, you might want to, like whether it's like the amount of,
you know, you know, the GDP, let's say, or like some sort of the energy expended by human civilization,
or like in any, it does have this, like, if you plot it on a linear scale, you just see a flat line
that just spikes up at the end of the graph. So then you have to go to like a log plot to even see
it bending. But even then it bends up. And we're really sitting right now at this like ridiculous
anomaly, where what we are currently taking to be the normal human condition is in any of which way
look at it like just extreme historical anomaly, like a tiny little thin coat of paint on like a
giant battleship of human history, right? It's like, and then now we think it's like a radical
conception to think, well, what if that changes in the future? Whereas in fact, if you sort of zoom
out, you look like we are right now in the middle of this like explosion of some sort. We don't know
what it's going to end up like. But I do believe, as you alluded to earlier, that scenarios in which
we just remain more or less like we are now and have the current human condition just keep going
for like tens of thousands of years, that just seems extremely improbable to me relative to either
like extinction slash dystopia or some radical transformation into some kind of post human
condition. Well, Nick, I really enjoyed dystopia and I came out of it with so many ideas and
concepts that I hadn't considered beforehand. So thanks so much for writing the book and
thinking about these really interesting pressing problems. And thanks a lot for coming on the
show to talk to me about it. Well, thanks to you and to the little cat there as well.

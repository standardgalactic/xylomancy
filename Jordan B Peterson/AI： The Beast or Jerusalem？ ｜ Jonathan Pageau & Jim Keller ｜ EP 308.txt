So the Hebrews created history as we know it.
You don't get away with anything and so you might think you can bend the fabric of reality
and that you can treat people instrumentally and that you can bow to the tyrant and violate
your conscience without cost.
You will pay the piper.
It's going to call you out of that slavery, into freedom, even if that pulls you into
the desert.
And we're going to see that there's something else going on here that is far more cosmic
and deeper than what you can imagine.
The highest ethical spirit to which we're beholden is presented precisely as that spirit
that allies itself with the cause of freedom against tyranny.
Yes, exactly.
And there is that hope.
I want villains to get punished.
But do you want the villains to learn before they have to pay the ultimate price?
That's such a Christian question.
That has to do with attention, by the way.
It has to do with a subsidiary hierarchy, like a hierarchy of attention, which is set
up in a way in which all the levels can have room to exist, let's say.
And so these new systems, the new way, let's say, the new urbanist movement, similar to
what you're talking about, that's what they've understood.
It's like we need places of intimacy in terms of the house.
We need places of communion in terms of parks and alleyways and buildings where we meet
and a church, all these places that kind of manifest our community together.
So those existed coherently for long periods of time.
And then the abundance post-World War II and some ideas about what life could be like
caused this big change.
And that change satisfied some needs, people got houses, but broke community needs.
And then new sets of ideas about what's the synthesis, what's the possibility of having
your own home, but also having community, not having to drive 15 minutes for every single
thing and some people live in those worlds and some people don't.
Do you think we'll be smart?
So one of the problems is if you go over to there, it's a piece of-
Well, why were we smart enough to solve some of those problems?
Because we had 20 years.
But now, because one of the things that's happening now is, as you pointed out earlier,
is we're going to be producing equally revolutionary transformations, but at a much smaller scale
of time.
What's natural to our children is so different than what's natural to us.
But what was natural to us was very different from our parents.
So some changes get accepted generationally really fast.
So what's made you so optimistic?
Hello everyone watching on YouTube or listening on associated platforms.
I'm very excited today to be bringing you two of the people I admire most intellectually,
I would say, and morally for that matter, Jonathan Pajot and Jim Keller, very different
thinkers.
Jonathan Pajot is a French-Canadian liturgical artist and icon carver known for his work featured
in museums across the world.
He carves Eastern Orthodox, among other traditional images, and teaches an online carving class.
He also runs a YouTube channel, This Symbolic World, dedicated to the exploration of symbolism
across history and religion.
Jonathan is one of the deepest religious thinkers I've ever met.
Jim Keller is a microprocessor engineer known very well in the relevant communities and
beyond them for his work at Apple and AMD, among other corporations.
He served in the role of architect for numerous game-changing processors, has co-authored
multiple instruction sets for highly complicated designs, and is credited for being the key
player behind AMD's new ability to compete with Intel in the high-end CPU market.
In 2016, Keller joined Tesla, becoming vice president of autopilot hardware engineering.
In 2018, he became a senior vice president for Intel.
In 2020, he resigned due to disagreements over outsourcing production, but quickly found
a new position at TENS Torrent as chief technical officer.
We're going to sit today and discuss the perils and promise of artificial intelligence, and
it's a conversation I'm very much looking forward to.
So welcome to all of you watching and listening.
I thought it would be interesting to have a three-way conversation.
Jonathan and I have been talking a lot lately, especially with John Verveke and some other
people as well, about the fact that it seems necessary for us to view for human beings
to view the world through a story.
In fact, when we describe the structure that governs our action and our perception, that
is a story.
And so we've been trying to puzzle out, I would say, to some degree on the religious
front, what might be the deepest stories.
And I'm very curious about the fact that we perceive the world through a story, human
beings do.
And that seems to be a fundamental part of our cognitive architecture and of cognitive
architecture in general, according to some of the world's top neuroscientists.
And I'm curious, and I know Jim is interested in cognitive processing and in building systems
that in some sense seem to run in a manner analogous to the manner in which our brains
run.
And so I'm curious about the overlap between the notion that we have to view the world
through a story and what's happening on the AI front.
There's all sorts of other places that we can take the conversation.
So maybe I'll start with you, Jim.
Do you want to tell people what you've been working on and maybe give a bit of a background
to everyone about how you conceptualize artificial intelligence?
Yeah, sure.
So first, I'll say technically, I'm not an artificial intelligent researcher.
I'm a computer architect.
And I'd say my skill set goes from somewhere around the atom up to the program.
So we make transistors out of atoms.
We make logical gates out of transistors.
We make computers out of logical gates.
We run programs on those.
And recently, we've been able to run programs fast enough to do something called an artificial
intelligence model or neural network, depending on how you say it.
And then we're building chips now that run artificial intelligence models fast.
And we have a novel way to do it, the company I work at.
But lots of people are working on it.
And I think we were sort of taken by surprise what's happened in the last five years, how
quickly models started to do interesting and intelligent seeming things.
There's been an estimate that human brains do about 10 to the 18th operations a second.
It sounds like a lot.
It's a billion, billion operations a second.
And a little computer, the processor in your phone probably does 10 billion operations a
second-ish.
And then if you use the GPU, maybe 100 billion, something like that.
And big, modern AI computers like OpenAI uses or Google or somebody, they're doing like
10 to the 16th, maybe slightly more operations a second.
So they're within a factor of 100 of a human brain's raw computational ability.
And by the way, that could be completely wrong.
Our understanding of how the human brain does computation could be wrong.
But lots of people have estimated, based on number of neurons, number of connections,
how fast neurons fire, how many operations a neuron firing seems to involve.
I mean, the estimates range by a couple orders of magnitude.
But when our computers got fast enough, we started to build things called language models
and image models that do fairly remarkable things.
So what have you seen in the last few years that's been indicative of this, of the change
that you described as revolutionary?
What are computers doing now that you found surprising because of this increase in speed?
Yeah, you can have a language model, read a 200,000-word book and summarize it fairly
accurately.
So it can extract out the gist?
The gist of it.
Mm-hmm.
Can it do that with fiction?
Yeah.
Yeah, and I'm going to introduce you to a friend who took a language model and changed
it and fine-tuned it with Shakespeare and used it to write screenplays.
That are pretty good.
And these kinds of things are really interesting, and then we were talking about this a little
bit earlier.
So when computers do computations, a program will say add A equal B plus C. The computer
does those operations on representations of information, ones and zeros.
It doesn't understand them at all.
The computer has no understanding of it.
But what we call a language model translates information like words and images and ideas
into a space where the program, the ideas and the operation it does on them are all
essentially the same thing.
We'll be right back with Jonathan Pageau and Jim Keller.
First we wanted to give you a sneak peek at Jordan's new documentary, Logos in Literacy.
I was very much struck by how the translation of the biblical writings jump-started the
development of literacy across the entire world.
Literacy was the norm.
The pastor's home was the first school, and every morning it would begin with singing.
The Christian faith is a singing religion.
Probably 80% of scripture memorization today exists only because of what is sung.
This is amazing.
Here we have a Gutenberg Bible printed on the press of Johann Gutenberg.
Science and religion are opposing forces in the world, but historically that has not
been the case.
Now the book is available to everyone.
From Shakespeare to modern education and medicine and science to civilization itself.
It is the most influential book in all of history, and hopefully people can walk away
with at least a sense of that.
A language model can produce words and then use those words as inputs.
It seems to have an understanding of what those words are, which is very different from
how a computer operates on data.
About the language models.
My sense of, at least in part, how we understand a story is that maybe we're watching a movie,
let's say, and we get some sense of the character's goals, and then we see the manner in which
that character perceives the world, and we, in some sense, adopt his goals, which is to
identify with the character, and then we play out a panoply of emotions and motivations
on our body because we now inhabit that goal space, and we understand the character as
a consequence of mimicking the character with our own physiology.
You have computers that can summarize the gist of a story, but they don't have that
underlying physiology.
Well, first of all, it's a theory that your physiology has anything to do with it.
You could understand the character's goals and then get involved in the details of the
story, and then you're predicting the path of the story, and also having expectations
and hopes for the story, and a good story kind of takes you on a ride because it teases
you with doing some of the things you expect, but also doing things that are unexpected,
and possibly that creates emotional surprises.
Yeah, it does.
So in an AI model, so you can easily have a set of goals, so you have your personal goals,
and then when you watch the story, you have those goals, and you put those together.
Like how many goals is that?
Like the story is goals in your goals, hundreds, thousands, those are small numbers, right?
When you have the story, the AI model can predict the story too, just as well as you
can.
How do you...
And...
That's the thing that I find mysterious is that it...
As the story progresses, it can look at the error between what it predicted and what
actually happened, and then iterate on that.
So you would call that emotional, excitement, disappointment.
Anxiety.
Anxiety, perhaps.
Yeah, definitely.
What a big part of what anxiety does seem to be is discrepancy.
Like some of those states are manifesting your body because you trigger hormone cascades
and a bunch of stuff, but you also can just scan your brain and see that stuff move around.
Right.
Right.
And the AI model can have an error function and look at the difference between what it
expected and not, and you could call that the emotional state if you want it.
Yeah, well, I just talked with the...
And that's speculation, but...
No, no, I think that's accurate.
But we can make an AI model that could predict the result of a story probably better than
the average person.
So what if...
Some people are really good at...
They're rarely educated about stories, or they know the genre or something, but...
Yeah.
But these things...
And what they see today as the capacity of the models is, if you say start describing
a lot, it'll make sense for a while, but it will slowly stop making sense.
But that's possible, that's simply the capacity of the model right now, and the model's not
well grounded enough in a set of, let's say, goals and reality or something to make sense
for a while.
So what do you think would happen, Jonathan?
This is, I think, I think associated with the kind of things that we've talked through
to some degree.
So one of my hypotheses, let's say, about deep stories is that they're metagists in
some sense.
You could imagine a hundred people telling you a tragic story, and then you could reduce
each of those tragic stories to the gist of the tragic story, and then you could aggregate
the gists, and then you'd have something like a metatragity.
And I would say the deeper the gist, the more religious-like the story gets.
And that's part of...
It's that idea as part of the reason that I wanted to bring you guys together.
I mean, one of the things that what you just said makes me wonder is, imagine that you
took Shakespeare, and you took Dante, and you took like the canonical Western writers,
and you trained an AI system to understand the structure of each of them, and then now
you could pull out the summaries of those structures, the gists, and then couldn't you pull out
another gist out of that?
So it would be like the essential element of Dante and Shakespeare, and I want to hear
what Jonathan said so far.
So here's one funny thing to think about.
You use the word pull out.
So when you train a model to know something, you can't just look in it and say, what is
it?
No.
You have to quarry it.
Right.
You have to ask.
Right.
Right.
What's the next sentence in this paragraph?
What's the answer to this question?
There's a thing on the internet now called prompt engineering.
And it's the same way, I can't look in your brain to see what you think.
Yeah.
I have to ask you what you think, because if I killed you and scanned your brain and
got the current state of all the synapses and stuff, A, you'd be dead, which should
be sad, and B, I wouldn't know anything about your thoughts.
Your thoughts are embedded in this model that your brain carries around, and you can express
it in a lot of ways.
And so the...
So you could add...
So this is my big question, because the way that I've been seeing it until now is that
artificial intelligence, it's based on us.
It doesn't exist independently from humans, and it doesn't have care.
The question would be, why does the computer care?
Yeah, that's not true.
Why does the computer care to get the gist of the story?
Well, yeah, so I think you're asking kind of the wrong question.
So you can train an AI model on the physics and reality and images in the world, just
with images.
And there are people who are figuring out how to train a model with just images, but
the model itself still conceptualizes things like tree and dog and action and run, because
those all exist in the world.
And you can actually train...
So when you train a model with all the language and words, so all information has structure.
And I know you're a structure guy from your video.
So if you look around you at any image, every single point you see makes sense.
Yeah.
Right?
It's a teleological structure.
It's a purpose laid in structure.
So this is something we talk about.
It turns out all the words that have ever been spoken by human beings also have structure.
Right.
Right.
And so physics has structure, and it turns out that some of the deep structure of images
and actions and words and sentences are related.
There's actually a common core of, imagine there's like a knowledge space, and sure there's
details of humanity where they prefer this accent versus that.
Those are kind of details, but they're coherent in the language model.
But the language models themselves are coherent with our world ideas.
And humans are trained in the world just the way the AI models are trained in the world.
Like a little baby, as it's learning, looking around, it's training on everything it sees
when it's very young.
And then its training rate goes down, and it starts interacting with what it's learning,
and interacting with the people around it.
But it's trying to survive.
It's trying to live.
It has, like it has a, the infant or the child has it again.
Neurons aren't trying, the weights in the neurons aren't trying to live.
What they're trying to do is reduce the error.
So neural networks generally are predictive things, like what's coming next, what makes
sense, you know, how does this work?
And when you train an AI model, you're training it to reduce the error in the model.
And if your model is big...
Okay, let me ask you about that.
So, well, first of all...
So babies are doing the same thing.
Like, they're looking at stuff going around, and in the beginning their neurons are just
randomly firing, but as it starts to get object-permanence and look at stuff, it starts predicting what
will make sense for that thing to do.
And when it doesn't make sense, it'll update its model.
So basically it compares its prediction to the events, and then it will adjust its prediction.
So in a story prediction model, the AI would predict the story, then compare it to its
prediction and then fine-tune itself slowly as it trains itself.
Okay, so...
Or a reverse, you could ask it to say, given the set of things, tell the rest of the story
and it could do that.
And that's what...
And the state of it right now is there are people having conversations with us that are
pretty good.
Mm-hmm.
So I talked to Carl Friston about this prediction idea in some detail, and so Friston, for those
of you who are watching and listening, is one of the world's top neuroscientists.
And he's developed an entropy enclosure model of conceptualization, which is analogous to
one that I was working on, I suppose, across approximately the same time frame.
So the first issue, and this has been well-established in the neuropsychological literature for quite
a long time, is that anxiety is an indicator of discrepancy between prediction and actuality.
And then positive emotion also looks like a discrepancy reduction indicator.
So imagine that you're moving towards a goal, and then you evaluate what happens as you
move towards the goal, and if you're moving in the right direction, what happens is what
you might say, what you expect to happen, and that produces positive emotion.
And it's actually an indicator of reduction in entropy.
That's one way of looking at it.
The point is...
So yeah, you have a bunch of words in there that are psychological definitions of states,
but you could say there's a prediction and an error, a prediction, and you're reducing
error.
Yes, but what I'm trying to make a case for is that your emotions directly map that,
both positive and negative emotion, look like there's signifiers of discrepancy reduction
on the positive and negative emotion side.
But then there's a complexity that I think is germane to part of Jonathan's query, which
is that...
So the neuropsychologists and the cognitive scientists have talked a long time about expectation,
prediction, and discrepancy reduction.
But one of the things they haven't talked about is it isn't exactly that you expect
things.
It's that you desire them.
You want them to happen.
Because you could imagine that there's, in some sense, a literally infinite number of
things you could expect.
And we don't strive only to match prediction.
We strive to bring about what it is that we want.
And so we have these preset systems that are teleological that are motivational systems.
Well, I mean, it depends.
If you're sitting idly on the beach in a bird flies by, you expect it to fly along in a
regular path.
Right.
You don't really want that to happen.
Yeah.
But you don't want it to turn into something that could peck out your eyes either.
Sure.
So that's a want.
Yeah.
But you're kind of following it with your expectation to look for discrepancy.
Yes.
Right?
Now, you'll also have a, you know, depends on the person, somewhere between 10 and a
million desires, right, and then you also have fears and avoidance.
And those are context.
So if you're sitting on the beach with some anxiety that the birds are going to swerve
at you and peck your eyes out.
So then you might be watching it much more attentively than somebody who doesn't have
that worry, for example.
But both of you can predict where it's going to fly and you'll both notice a discrepancy.
The motivations, one way of conceptualizing fundamental motivation is they're like a priori
prediction domains, right?
And so it helps us narrow our attentional focus because I know when you're sitting and
you're not motivated in any sense, you can be doing just, in some sense, trivial expectation
computations, but often we're in a highly motivated state.
And what we're expecting is bounded by what we desire and what we desire is oriented as
Jonathan pointed out towards the fact that we want to exist.
And one of the things I don't understand and wanted to talk about today is how the computer
models, the AI models, can generate intelligible sense without this, without mimicking that
sense of motivation, because you've said, for example, they can just drive the patterns
from observations of the objective world, but there's a...
So again, I don't want to do all the talking, but so AI generally speaking, like when I
first learned it about it had two behaviors.
They call it inference and training.
So inferences, you have a trained model, so you give it a picture and say, is there
a cat in it?
And it tells you where the cat is.
That's inference.
The model has been trained to know where a cat is.
And training is the process of giving it an input and an expected output, and when you
first start training the model, it gives you garbage out, like an untrained brain would.
And then you take the difference between the garbage output and the expected output and
call that the error.
And then they invent the big revelation was something called back propagation with gradient
descent.
But that means take the error and divide it up across the layers and correct those calculations
so that when you put a new thing in, it gives you a better answer.
And then to somewhat my astonishment, if you have a model of sufficient capacity and you
train it with 100 million images, if you give it a novel image and say, tell me where
the cat is, it can do it.
That's called, so training is the process of doing a pass with an expected output and
propagating an error back through the network.
And inference is the behavior of putting something in and getting an output.
I think, I'm really pulling, but there's a third piece, which is what the new models
do.
It's just called generative, it's called a generative model.
So for example, say you put in a sentence and you say, predict the next word.
This is the simplest thing.
So it predicts the next word.
So you add that word to the input and I'll say predict the next word.
So it contains the original sentence and the word you generated.
And it keeps generating words that make sense in the context of the original word in addition.
This is the simplest basis.
And then it turns out you can train this to do lots of things.
You can train it to summarize a sentence, you can train it to answer a question.
There's a big thing about, you know, like Google every day has hundreds of millions
of people asking it questions and giving answers and then rating the results.
You can train a model without information, so you can ask it a question and it gives
you a sensible answer.
I think in what you said, I actually have the issue that has been going through my mind
so much is when you said, you know, people put in the question and then they rate the
answer.
My intuition is that the intelligence still comes from humans in the sense that it seems
like in order to train whatever AI, you have to be able to give it a lot of power and then
say at the beginning, this is good, this is bad, this is good, this is bad, like rejects
certain things, accepts certain things in order to then reach a point when then you
train the AI.
And so that's what I mean about the care.
So the care will come from humans because the care is the one giving it the value, saying
this is what is valuable, this is what is not valuable in your calculation.
So when they first, so there's a program called AlphaGo that I learned how to play
go better than a human.
So there's two ways to train the model.
One is they have a huge database of lots of go games with good winning moves.
So they trained the model with that and that worked pretty good.
And they also took two simulations of go and they did random moves.
And all that happened was is these two simulators played one go game and they just recorded
whichever moves happened to win and it started out really horrible and they just started
training the model.
This is called adversarial learning, it's a particular adversarial, it's like, you know,
you make your moves randomly and you train a model and so they train multiple models
and over time those models got very good and they actually got better than human players.
Because the humans have limitations about what they know, whereas the models could experiment
in a really random space and go very far.
Yeah.
But experiment towards the purpose of winning the game.
Yes.
Well, but you can experiment towards all kinds of things, it turns out, and humans are also
trained that way.
Like when you were learning, you were reading, you were saying, this is a good book, this
is a bad book, this is good sense construction, it's good spelling.
So you've gotten so many error signals over your life.
Well, that's what culture does in large parts.
Culture does that, religion does that, your everyday experience does that, your family.
So we embody that and everything that happens to us, we process it on the inference pass
which generates outputs and then sometimes we look at that and say, hey, that's unexpected
or that got a bad result or that got bad feedback and then we back propagate that and update
our models.
So really well trained models can then train other models.
So the human train now are the smartest people in the world.
So the biggest question that comes now based on what you said is, because my main point
is to try to show how it seems like artificial intelligence is always an extension of human
intelligence.
Like it remains an extension of human intelligence.
And maybe the way to- That won't be true at all.
So do you think that at some point the artificial intelligence will be able to, because the
goals recognizing cats, writing plays, all these goals are goals which are based on embodied
human existence.
Could an AI at some point develop a goal which would be incomprehensible to humans because
of its own existence?
Yeah.
I mean, for example, there's a small population of humans that enjoy math, right?
And they are pursuing adventures in math space that are incomprehensible to 99.99% of humans.
But they're interested in it.
And you could imagine like an AI program working with those mathematicians and coming up with
very novel math ideas and then interacting with them.
But they could also, if some AIs were elaborating out really interesting and detailed stories,
they could come up with stories that are really interesting.
We're going to see it pretty soon, like all of art, movie making, and everything.
A story that is interesting only to the AI and not interesting to us.
That's possible.
So stories are like, I think, some high level information space.
So the computing age of big data, there's all this data running on computers or no,
but only humans understood it, right, and computers don't.
So AI programs are now at the state where the information, the processing, and the feedback
loops are all kind of in the same space.
They're still relatively rudimentary to humans.
I guess some AI programs in certain things are better than humans already, but for the
most part, they're not.
But it's moving really fast.
And so you could imagine, I think in five or 10 years, most people's best friends will
be AIs.
And they'll know you really well, and they'll be interested in you.
Kind of like your real friends.
Real friends are problematic.
They're only interested in you when you're interested.
Yeah, yeah, real friends are.
The AI systems will love you even when you're dull and miserable.
Well, there's so much idea space to explore.
And humans have a wide range.
Some humans like to go through their everyday life doing their everyday things, and some
people spend a lot of time.
Like a lot of time reading and thinking and talking and arguing and debating.
And there's going to be, I'd say, a diversity of possibilities with what a thinking thing
can do when the thinking is fairly unlimited.
So I'm curious about, I'm curious in pursuing this issue that Jonathan has been developing.
So there's a literally infinite number of ways, a virtually infinite number of ways
that we could take images of this room.
Right now, if a human being is taking images of this room, they're going to sample a very
small space of that infinite range of possibilities.
Because if I was taking pictures in this room, in all likelihood, I would take pictures of
objects that are identifiable to human beings, that are functional to human beings at a level
of focus that makes those objects clear.
And so then you could imagine that the set of all images on the internet has that implicit
structure of perception built into it.
And that's a function of what human beings find useful.
You know, I mean, I could take a photo of you that was, the focal depth was here and
here and here and here and here and two inches past you.
Now I suppose you could...
There's a technology for that called light fields.
So then you could, if you had that picture properly done, then you could move around
it and imagine and see.
But yeah, fair enough.
I get your point.
Like the human recorded data has our biology built into it, but also unbelievably detailed
encoding of how physical reality works, right?
So every single pixel in those pictures, even though you kind of selected the view, the
focus, the frame, it still encoded a lot more information than your processing.
And if you take a large, it turns out if you take a large number of images of things in
general, so you've seen these things where you take a 2D image and turn it into a 3D
image.
Yeah.
Right.
The reason that works is even in the 2D image, the 3D is embedded in the room actually
got embedded in that picture in a way.
And if you have the right understanding of how physics and reality works, you can reconstruct
the 3D model.
Okay, so this just reminds me...
You could, you know, an AI scientist may cruise around the world with infrared and
radio wave cameras, and they might take pictures of all different kinds of things, and every
once in a while they'd show up and go, hey, the sun, you know, I've been staring at the
sun in the ultraviolet and radio waves for the last month, and it's way different than
anybody thought, because humans tend to look at light in the visible spectrum.
And, you know, there could be some really novel things coming out of that.
But humans also, we live in the spectrum we live in, because it's a pretty good one for
planet Earth.
Mm-hmm.
Like, it wouldn't be obvious that AI would start some different place, like visible spectrum
is interesting for a whole bunch of reasons.
Right.
So in a set of images that are human derived, you're saying that there's the way I would
conceptualize that is that there's two kinds of logos embedded in that.
One would be that you could extract out from that set of images what was relevant to human
beings, but you're saying that the fine structure of the objective world outside of human concern
is also embedded in the set of images, and that an AI system could extract out a representation
of the world, but also a representation of what's motivating to human beings.
Yes.
And some human scientists already do look at the sun in radio waves and other things because
they're trying to, you know, get different angles on how things work.
Yeah.
Well, I guess it's a, it's a, it's a curious thing.
It's like the same with like buildings and architecture, mostly fit people.
Well, the other reason for that, the reason why I keep coming back to hammering the same
point is that even in terms of the development of the AI, that is developing AI requires
immense amount of money, energy, you know, and time.
And so that's a transient thing in 30 years, it won't cost anything.
So it's, that's, that's going to change so fast.
It's amazing.
So that's, that's a, like supercomputers used to cost millions of dollars and now your
phone is the supercomputer.
So it's the time between millions of dollars and $10 is about 30 years.
So it's like, I'm just saying it's like the time and effort isn't a thing in technology.
It's moving pretty fast.
It just, that's just, that just sets the date.
Yeah.
But even making, even making, let's say, even, I mean, I guess maybe the, this is the nightmare
question.
Like, could you imagine an AI system which becomes completely autonomous, which is creating
itself even physically through automized factories, which is, you know, programming itself, which
is creating its own goals, which is not at all connected to human endeavor?
Yeah.
I mean, individual researchers can, you know, I have a friend who I'm going to introduce
you to him tomorrow.
He wrote a program that scraped all of the internet and trained an AI model to be a language
model on a relatively small computer.
And in 10 years, the computer he could easily afford would be as smart as a human.
So he could train that pretty easily.
And that model could go on Amazon and buy 100 more of those computers and copy itself.
So yeah, we're, we're 10 years away from that.
And then with, then why, like, why would it do that?
I mean, what does it does?
Is it possible?
It's all about the motivational question.
I think that that's what, what even Jordan and I both have been coming at from the outset.
It's like, so you have an image, right?
You have an image of, of Skynet or of the matrix, you know, in which the sentient AI
is actually fighting for its survival.
So it has a survival instinct, which is pushing it to self perpetuate, like to, to, to replicate
itself and to create variation on itself in order to survive and identifies humans as
an obstacle to that, you know?
Yeah.
So you have a whole bunch of implicit assumptions there.
So, so humans last I checked are unbelievably competitive.
And when you let people get into power with no checks on them, they typically run them
up.
It's been a historical, historical experience.
And then humans are, you know, self regulating to some extent, obviously with some serious
outliers, because they self regulate with each other and humans and AI models at some
point will have to find their own calculation of self regulation and, and trade offs about
that.
Yeah.
Because the AI doesn't feel pain, at least as we, if that we don't know that it feels
well.
Lots of humans don't feel pain either.
So I mean, that's, I mean, the humans feeling pain or not didn't, you know, doesn't stop
a whole bunch of activity.
I mean, that's, I mean, it doesn't, the fact that we feel pain doesn't stop doesn't
regulate that many people.
Right.
Right.
Right.
Definitely people like, you know, children, if you threaten them with, you know, go to
your room and stuff, you can regulate them that way.
But some kids ignore that completely and adults are the same way.
And it's often counterproductive.
Yeah.
So, so right, you know, you know, culture and societies and organizations, we regulate
each other, you know, sometimes in competition and cooperation.
Do you, do you think that, well, we've talked about this to some degree for decades, I mean,
when you look at how fast things are moving now, and as you push that along, what, what,
when you look out 10 years and you see the relationship between the AI systems that are
being built and human beings, what do you envision or can you envision it?
Well, can I, yeah, like I said, I'm a computer guy and I'm watching this with, let's say,
some fascination as well.
I mean, the last, so Ray Kurzweil said, you know, progress accelerates.
Yeah.
Right.
So we have this idea that 20 years of progress is 20 years, but, you know, the last 20 years
of progress was 20 years and the next 20 years will probably be, you know, five to 10.
Right.
Right.
And, and you can really feel that happening.
To some level, that causes social stress, independent of whether it's AI or, or Amazon
deliveries, you know, what, you know, there's so many things that are going into the stress
of it all.
But there's, but there's progress, which is an extension of human capacity.
And then there's this progress, which I'm hearing about the way that you're describing
it, which seems to be an inevitable progress towards creating something which is more
powerful than you.
Right.
And so what is that?
I don't even understand that drive, like what is that drive to, to create something
which can supplant you.
So look at the average person in the world, right?
So the average person already exists in this world, because the average person is halfway
up the human hierarchy.
There's already many people more powerful than any of us there.
They could be smarter.
They could be richer.
They could be better connected.
We already live in a world like very few people are at the top of anything, right?
So that's already a thing.
So basically the drive to make someone a superstar, let's say, or the drive to elevate someone
above you, that would be the same drive that is bringing us to creating these ultra powerful
machines.
Cause we have that, like we have a drive to elevate.
Like, you know, when we see a rock star that we like, people want to submit themselves
to that.
They want to dress like them.
They want to raise them up above them as an example, something to follow, right?
Something to subject themselves to.
You see that with leaders.
You see that in the political world.
And in teams, you see that in sports teams, the same thing.
And so you think that that's the drive.
Well, we've always tried to build things that are beyond us, you know, I mean, I mean, it's
about, are we building, are we building a God?
Is that the, is that what people, is that the drive that is pushing someone towards?
Cause when I hear what you're describing, Jim, I hear something that is extremely dangerous,
right?
Sounds extremely dangerous to the very existence of humans.
And I see humans acting and moving in that direction almost without being able to stop
it.
As if there's no one.
I think it is unstoppable.
Well, that's one of the things we've also talked about is because I've asked Jim straight
out, you know, because of the hypothetical danger associated with this, why not stop
doing it?
And well, part of his answer is the ambivalence about the outcome, but also that it isn't
obvious at all that in some sense it's stoppable.
I mean, it's the cumulative action of many, many people that are driving this along.
And even if you took out one player, even a key player, the probability that you'd
do anything but slow it infinitesimally is, is quite.
That because there's also a massive payoff for those that will succeed.
It's also set up that way.
People know that at least, at least until the AI take over or whatever, that whoever
is on the line towards increasing the power of the AI will, will rake in major rewards.
Right.
Well, that's all cognitive acceleration, right?
Yeah, I could recommend Ian Banks as an author, English author, I think he wrote a
series of books on the, he called the culture novels.
And it was a world where there was humans and then there was AI as the smartest humans
and AI as that were dumber than humans.
But there were some AIs that were much, much smarter and they, they lived in harmony
because they mostly all pursued what they wanted to pursue.
Humans pursued human goals and super smart AIs, pursued super smart AI goals.
And, and, you know, they, they communicated and worked with each other, but, but they,
they mostly, you know, they're different when they were different enough that that
was problematic, their goals were different enough that they didn't overlap.
Because one of the, one of the things that, that would be my guess is like these
ideas where these super AIs get smart.
And the first thing they do is stomp out the humans.
It's like, you don't do that.
Like, you don't wake up in the morning and think, I have to stomp out all the cats.
No, it's not about the cats do cat things and the ants do ant things and the birds
do bird things and, and super smart mathematicians do smart mathematician things.
And, you know, guys who like to build houses do build house things.
And, you know, everybody, you know, the world, there's so much space in the
intellectual zone that people, people tend to go pursue the, in a, in a good
society, like you tend to pursue the stuff that you do.
And then the people in your zone, you self-regulate.
And you also, even in the social strategies, we self-regulate.
I mean, the, the, the recent political events of the last 10 years, the, the
weird thing to me has been why have, you know, people with power have been
overreaching to take too much from people with less.
Like that's bad regulation.
But one of the aspects of, one of the aspects of increase in power is that
increase in power is always mediated, at least in one aspect by the military, by,
by, let's say physical power on others, you know, and we can see that technology
is linked and has been linked always to military power.
And so the idea that there could be some AIs that will be our friends or whatever
is maybe possible.
But the idea that there will be some AIs, which will be weaponized is, seems
absolutely inevitable because increase in power is always, increase in
technological power always moves towards, towards military.
So we've lived with atomic bombs since the 40s, right?
So the, I mean, the solution to this has been mostly, you know, some form
of mutual assured destruction or attacking me, like the response to
attacking me is so much worse than the.
Yeah.
But it's also because we have reciprocity.
We recognize each other as the same.
So if I look into the face of another human, there, there's a limit of how
different I think that person is from me.
But if I'm hearing something described as the possibility of super
intelligences that have their own goals, their own cares, their own structures,
then how much mirror is there between these two groups of people, these two groups?
Well, Jim's objection seems to be something like we're, we're making,
we may be making when we're doomsay, let's say, and I'm not saying there's no
place for that, we're making the presumption of something like a zero
sum competitive landscape, right?
Is that the, the idea and the idea behind movies like, like the
Terminator is that there is only so much resources and the machines and
the human beings would have to fight over it.
And you can see that that, that could easily be a preposterous assumption.
Now, I think that one of the fundamental points you're making though is also
there will definitely be people that will weaponize AI and those weaponized
AI systems will have as their goal something like the destruction of human
beings, at least under some circumstances.
And then there's the possibility that that will get out of control because
the most effective systems at destroying human beings might be the ones that win,
let's say, and that could happen independently of whether or not it is a
true zero sum competition.
Yeah.
And also the, the effectiveness of military stuff doesn't need very smart
AI to be a lot better than it is today.
You know, I used to, you know, the, like the Star Wars movies where like, you
know, tens of thousands of years in the future, super highly trained, you
know, fighters can't hit somebody running across the field.
Like that's silly, right?
You can, you can already make a gun that can hit everybody in the room without
aiming at it.
It's, you know, there's like the, the military threshold is much lower than
any intelligence threshold, like for danger.
And, you know, like to the extent that we self-regulated through the nuclear
crisis is interesting.
I don't know if it's because we thought that the Russians were like us.
I kind of suspect the problem was that we thought they weren't like us.
And, but we still managed to make some calculation to say that any kind of
attack would be mutually devastating.
Well, when you, when you look at, you know, the destructive power of the
military we already have so far exceeds the planet, I'm, I'm not sure, like
adding intelligence to it is the tipping point.
Like, like that's, and I think the more likely thing is things that are truly
smart in different ways will be interested in different things.
And then the possibility for, let's say, mutual flourishing is, is, is really
interesting.
And I know artists using AI already to do really amazing things.
And, and that's already happening.
Well, when you, when you're working on the frontiers of AI development and you
see the development of increasingly intelligent machines, I mean, I know
that part of what drives you is, I don't want to put words in your mouth, but
what drives intelligent engineers in general, which is to take something
that works and make it better and maybe to make it radically better and
radically cheaper.
So, so there's this drive toward technological improvement.
And I know that you like to solve complex problems and you do that
extraordinarily well.
What, but do you, do you, is there also a vision of a more abundant
form of human flourishing emerging from the, from the development?
So what, so what do you see happening?
You said it years ago, it's like, we're going to run out of energy.
What's next?
We're going to run out of matter.
Right.
Like our ability to do what we want in ways that are interesting and, you
know, for some people, beautiful is limited by a whole bunch of things.
Cause we're, you know, partly it's technological and partly with, you
know, we're stupidly divisive, but, um, but there is possible that there's
also a reality, which is one of the things that technology has been is
of course an increase in power towards desire, towards human desire.
And that is represented in mythological stories where let's
say technology is used to accomplish impossible desire.
Right.
We have, you know, the story of the story of building the, the McCann, the
bull around, uh, the, the king of Meno, the, the wife of the king of Meno's,
you know, in order to be inseminated by, uh, by a bull, we have the story,
the, the, we have the story of the, um, sorry, Frankenstein, et cetera, the
story of the Golem where we put our desire into this increased power.
And then what happens is that we don't know our desires.
That's one of the things that I've also been worried about in terms of
AI is that we act, we have secret desires that enter into what we do that
people aren't totally aware of.
And as we increase in power, these systems, those desires, let's say,
the, the, like the idea, for example, of the possibility of having an AI friend
and the idea that an AI friend would be the best friend you've ever had,
because that, that friend would be the nicest to you, would care the most
about you, would do all those things.
That would be an exact example of what I'm talking about, which is it's really
the story of the genie, right?
It's the story of the genie in the lamp where the genie says, what do you
wish?
And the, and the person, and I have unlimited power to give it to you.
And so I give him my wish, but that wish has all these, these underlying
implications that I don't understand, all these underlying possibilities.
But the cool thing, the moral of almost all those stories is having
unlimited wishes will be, lead to your downfall.
And so humans, like, if you give, you know, a young person an unlimited
amount of stuff to drink for, for six months, they're going to be
falling down drunk and they're going to get over it, right?
Having a friend that's always your friend, no matter what, it's probably
going to get boring.
Well, the, the, the literature on marital stability indicates that.
So there's a, there's a sweet spot with regards to marital stability
in terms of the ratio of negative to positive communication.
So if on average, you receive five positive communications and one
negative communication from your spouse, that's on the low threshold for
stability.
If it's four positive to one negative, you're headed for divorce.
But interestingly enough, on the other end, there's a threshold as well,
which is that if it exceeds 11 positive to one negative, you're also
moving towards divorce.
So there's, so, so, so there might be self-regulating mechanisms that would
in sense take care of that.
You might find a yes man AI friend, extraordinarily boring, very, very
rapidly.
But as opposed to an AI friend that was interested in what you were
interested in, it was actually interesting.
Like, you know, we go through friends in the course of our lives, like
different friends are interesting at different times.
And some friends we grow with, and that continues to be really
interesting for years and years.
And other friends, you know, some people get stuck in their thing and then
you've moved on or they've moved on or something.
Yeah, I, so I tend to think of a, a go, a world where there was more
abundance and more possibilities and more interesting things to do is, is
an interesting, and modern society is let the human population.
And some people think this is a bad thing, but I don't know.
I, I'm a fan of it.
You know, modern population has gone from tens of 200 million to
billions of people.
That's generally being a good thing.
We're not running out of space.
I've been in it.
You know, so some of your audience has probably been in an airplane.
If you look out the window, the country is actually mostly empty.
The oceans are mostly empty.
Like we're, we're weirdly good at polluting large areas.
But as soon as we decide not to, we don't have to, like technology.
Most, most of our, you know, energy pollution problems are technical.
Like we can stop polluting.
Like electric cars are great.
So, so, so there's so many things that we could do technically.
I forget the guy's name.
He said the earth could easily support a population of a trillion people.
And trillion people would be a lot more people doing, you know, random stuff.
And he didn't imagine that the future population would be a trillion humans
and a trillion AIs, but it probably will be.
So it will probably exist on multiple planets, which would be good the next
time an asteroid shows up.
So what do you think about?
So, so one of the things that seems to be happening, tell me if, if you think
I'm wrong here, I don't think it's germane to.
And, and, and I just want to make the point of, you know, where we are compared
to living in the Middle Ages, our lives are longer, our families are healthier,
our children are more likely to survive.
Like many, many good things happened.
Like setting the clock back wouldn't be good.
And, you know, if we have some care and people who actually care about how
culture interacts with technology for the next 50 years, you know, we'll get
through this hopefully more successful than we did the atomic bomb and the Cold War.
But it's, it's so it's a major change.
I mean, this is like, like your worries are, you know, I mean, they're, they're
relevant, but that, you know, but also you're John, Jonathan, your stories
about how humans have faced abundance and faced evil kings and evil overlords.
Like we have thousands of years of history of facing the challenge of the
future and the challenge of things that cause radical change.
Yeah.
And that's, that's very valuable information, you know, information.
But for the most part, nobody's succeeded by stopping change.
They've succeeded by bringing to bear on the change our capability to
self-regulate the balance.
Like a good life isn't having as much gold as possible.
It's a boring life.
A good life is, you know, having some quality friends and doing what you want
and having some, some insight in life.
Yeah.
And some optimal challenge.
And, you know, and in a world where a larger percentage of people can have,
well, live in relative abundance and have tools and opportunities, I think is a
good thing.
Yeah.
And I don't, I don't want to pull back abundance, but what I have noticed is
that, that our abundance brings a kind of nihilism to people.
And I don't, like I said, I don't want to go back.
I'm happy to live here and to have these, these tech things.
But I, but I think it's something that I've also noticed that increase of, of
the capacity to get your desires when that increases to a certain extent also
leads to a kind of nihilism where exactly that.
Well, I wonder, Jonathan, I wonder if that's part, partly a consequence of
the erroneous maximization of short term desire.
I mean, one of the things that you might think about that could be dangerous on
the AI front is that we optimize the manner in which we, we interact with
our electronic gadgets to capture short term attention.
Right.
Because there's a difference between getting what you want right now, right
now, and getting what you need in some more mature sense across a reasonable
span of time.
And one of the things that does seem to be happening online, and I think it is
driven by the development of AI systems, is that we're, we're assaulted by systems
that parasitize our short term attention and at the expense of longer term
attention.
And I, if the AI systems emerge to optimize attentional grip, it isn't
obvious to me that they're going to optimize for the attention that works
over the medium to long run, right?
They're going to, they're going to be, they could conceivably maximize
something like WIM centered.
Yeah, because all of virality is based on that.
All the social media networks are all based on this, on this reduction, this
reduction of attention, this reduction of desire to, to reaching your, your
rest, let's say, in that desire, right?
The like, the click, all these things, they're, yeah, now.
Yeah, exactly.
So, but, but that's something that, you know, so for, for reasons that are
somewhat puzzling, but maybe not, you know, the business models around a lot
of those interfaces are around, you know, the part, the users, the product.
And, you know, the, the advertisers are trying to get your attention.
Yeah, yeah.
But that's something culture could regulate.
We could decide that, no, we don't, we don't want tech platforms to be
driven by advertising money.
Like that would be a smart decision, probably.
Um, and that could be a big change.
And what you see as an old, see, the, well, the problem is markets drive
that in some sense, right?
And, and I know they're driving that, but we can take steps like, you know,
various times, you know, alcohol has been illegal.
Like you can, we society can decide to regulate all kinds of things.
And, you know, sometimes some things need to be regulated and some things don't.
Like when you buy a hammer, you don't fight with your hammer for its attention,
right?
The hammer is a tool you buy when, when you need one.
Nobody's marketing hammers to you.
Like, like that, that, that, that has a relationship that's transactional
to your purpose, right?
Yeah, well, our technology has become a thing where, I mean,
but there's a relationship between human, let's say high human goals,
something like attention and status.
And what we talked about, which is the idea of elevating something higher in
order to see it as a model.
See, these are where intelligence exists in the human person.
And so when we notice that in the systems, in the, the platforms, these are the,
the, these are the aspects of intelligence, which are being weaponized in some
ways, not against us, but are just kind of being weaponized because they're
the most beneficial at the short term to be able to generate our constant
attention.
And so what I mean is that that is what the AI's are made of, right?
They're made of attention, prioritization, you know, good, bad.
What, what is it that, that is worth putting energy into in order to
predict towards a telos?
And so I'm seeing, so I'm seeing that it's, you know, we could disconnect
it and suddenly seems very difficult to me.
Yeah.
So I'll give it to, to, first I want to give an old example.
So after, after World War II, America went through this amazing building boom
of building suburbs and the American dream was you could have your own house,
your own yard and a suburb with a good school, right?
So in the 50s, 60s, early 70s, they were building that like crazy.
By the time I grew up, I lived in a suburb in dystopia, right?
And we found that, that, that as a goal wasn't a good thing because people
ended up in houses separated from social, social needs and structures.
And then new towns are built around like a hub with, you know, places to go
and eat, you know, so there was a, a good that was viewed in terms of
opportunity and abundance, but it actually was a fail culturally.
And then some places it modified and it continues in some places are still
dystopian, you know, suburban areas and some places people simply
learn to live with it, right?
So that has to do with attention, by the way.
It has to do with, with a subsidiary hierarchy, like a hierarchy of
attention, which is set up in a, in, in a way in which all the levels can
have room to exist, let's say.
And so, you know, these new, the new systems, the new way, let's say the
new urban, urbanist movement, similar to what you're talking about, that's
what they've understood.
It's like we need places of intimacy in terms of the house.
We need places of communion in terms of, you know, parks and alleyways and,
and buildings where we meet and a church, all these places that kind of
manifest our community together.
Yeah.
So, so those existed coherently for long periods of time.
And then the abundance post-World War II and some ideas about like what,
what life could be like causes big change and that change satisfied
some needs, people got houses, but broke community needs.
And then new sets of ideas about what's the synthesis, what's the possibility
of having your own home, but also having community, not having to drive 15
minutes for every single thing.
And some people live in those worlds and some people don't.
Do you think we'll be smart?
So one of the problems is, why were we smart enough to solve some of those
problems?
Because we had 20 years, but now, because one of the things that's happening
now is we're, as you pointed out earlier, is we're going to be producing
equally revolutionary transformations, but at a much smaller scale of time.
And so Mike, one of the things I wonder about, I think it's driving some of
the concerns in the conversation is, are we going to be intelligent enough to
direct with regulation the transformations of technology as they start to
accelerate?
I mean, we've already, you look what's happened online.
I mean, we've inadvertently, for example, radically magnified the voices
of narcissists, psychopaths and Machiavellians.
And we've done that so intensely, partly, and I would say partly as a
consequence of AI mediation, that I think it's destabilizing the entire
community.
Well, it's destabilizing part of it.
Like Scott Evans pointed out, you just block everybody that acts like that.
I don't pay attention to people that talk like that.
Yeah, but they seem, they seem to be.
Well, there's no places that are sensitive to it.
Like, yeah, 10,000 people here can make a storm in some corporate, you know,
person, you know, fire somebody.
But I think that's like, we're five years from that being over.
Corporation will go 10,000 people out of 10 billion.
Not a big deal.
Okay.
So you think that's a learning moment that will re-regulate.
Um, what's natural to our children is so different than what's natural to us.
But what was natural to us was very different from our parents.
So some, some changes get accepted, accepted generationally really fast.
So what's made you so optimistic?
What's mean?
What do you mean optimistic?
Well, most of the things that you have said today, and maybe it's also
because we're pushing you, I mean, you, you really, you really do.
My nephew, Kyle, was a really smart, clever guy.
He called me a, what did he call it?
A cynical optimist.
Like I believe in people.
Like I like people, but also people are complicated.
And they all got all kinds of nefarious goals.
Like I worry a lot more about people burning down the world than I do about
artificial intelligence, just because, you know, people, well, you know,
people, they're, they're, they're difficult, right?
And, and, but the interesting thing is in aggregate, we mostly self-regulate.
And when things change, you have these dislocations.
And then it's up to people who talk and think, and while we're having
this conversation, I suppose, to talk about how, how do we re-regulate this stuff?
Yeah.
Well, because one of the things that, one of the things that the
increase in power has done in terms of AI, and you can see it with Google and
you can see it online, is that there are certain people who hold the keys,
let's say, and, and, and then who hold the keys to what you see and what you
don't see.
So you see that on Google, right?
And you know it if you know what search is to make, where you realize that
this is not, this is actually being directed by someone who now has huge
amount of power in order to direct my attention towards their ideological purpose.
And so, so that's why, like, I think that to me, I personally think it would,
I always tend to see AI as, as an extension of human power, even though
there is this idea that it could somehow become totally independent.
I, I still tend to see it as an increase of the human care and whoever will be
able to hold the, the keys to that will have increased in power.
And that can be like, and I think we're already seeing it.
Well, that's, that's not, that's not really any different though.
Is it, Jonathan, the situation that's always confronted us in the past?
I mean, we've always had to deal with the evil uncle of the king.
And we've always had to deal with the fact that an increase in ability
could also produce a commensurate increase in tyrannical power, right?
I mean, so that might be magnified now.
And, and, and maybe the danger in some sense is more acute, but possibly the
possibility is more present as well.
Well, because you can train an AI to find hate speech, right?
You can train an AI to find hate speech and then to act on that hate
speech immediately within.
Now it's only, we're not only talk about social media, but we, what we've
seen is that that is now in being encroaching into payment systems
and into people losing their bank account, their access to different services.
And so this idea of optimization.
Yeah, there's an Australian bank that already has decided that it's a good
thing to send all of their customers a carbon load report every month.
Right. And to, to offer them hints about how they could reduce their,
their polluting purchases, let's say.
And well, at the moment, that system is one of voluntary compliance.
But you can certainly see in a situation like the one we're in now, that
the line between voluntary compliance and involuntary compulsion is very, very thin.
Yeah. So I'd like to say, so during the early computer world, computers
were very big and expensive.
And then we made many computers and workstations, but they were still
corporate only. And then the PC world came in.
All of a sudden PCs put everybody online.
Everybody could suddenly see all kinds of stuff and, you know, people
could get a Freedom of Information Act request, put it online somewhere
and a hundred thousand people could see it.
Like it was an amazing democratization moment.
And then there was a similar, but smaller revolution with the world
of, you know, smartphones and apps.
But then we've, we've had a new, completely different set of companies, by the way,
you know, from, you know, what happened in the 60s, 70s and 80s to today.
It's very different companies that control it.
And, and there are people who are worried that AI will be a winter take-all thing.
Now, I think so many people are using it and they're working on it
in so many different places and the cost is going to come down so fast
that pretty soon you'll have your own AI app that you'll use
to mediate the internet to strip out, you know, the endless stream of ads.
And you can say, well, is this story objective?
Well, here's the 15 stories and this is being manipulated this way
and this is being manipulated that way.
And you can say, well, I want what's, what's, what's more like the real story.
And the funny thing is information that's broadly distributed
and has lots of inputs is very hard to fake the whole thing.
So right now, a story can pull through a major media outlet.
And if they can control the narrative, everybody gets the fake story.
But if the media is distributed across a billion people
who are all interacting in some useful way, somebody standing up there.
Some, yeah, there's real signal there.
And if somebody stands up and says something that's not true,
everybody go, everybody knows that's not true.
So the, like a good outcome with people thinking seriously
would be the democratization of information and, you know,
objective facts in the same way.
The same thing that happened with PECs versus corporate central computers
could happen again.
So you have an increase.
The problem is that these are the real possibility.
The increasing power always creates the tooth at the same time.
And so we saw that, you know, increase in power creates first,
or it depends in which direction it happens.
It creates an increase in decentralization and increase in access.
And it creates it creates all that.
But then it also, at the same time, creates the counter reaction,
which is an increase in control and increase in centralization.
And so now the more the power is, the more the waves will,
the bigger the waves will be.
And so the image of the image that 1984 presented to us,
you know, of people going into newspapers and changing the
the headlines and taking the pictures out and doing that,
that now obviously can happen with just a click.
So you can click and you can change the past.
You can change the past.
You can change facts about the world because they're all held,
you know, online.
And we've seen it.
We've seen it happen, obviously, in the media recently.
But so does decentralization win over centralization?
How is that even possible, it seems?
I mean, and it's also interesting, like when Amazon became a platform,
suddenly any mom and pop business could have, you know,
Amazon, eBay, there's a bunch of platforms,
which had an amazing impact because any business could get to anybody.
But then the platform itself started to control the information flow.
Yeah, right.
But at some point that will turn into people go, well,
why am I letting somebody control my information flow?
And Amazon objectively doesn't really have any capability, right?
So so like you point out, the waves are getting bigger,
but they're real waves.
It's the same with information.
Information is all online.
It's also on a billion hard drives, right?
So somebody says, I'm going to raise the objective fact
that distributed information system would say,
yeah, go ahead and raise it anywhere you want.
Here's another thousand copies of it.
Yeah. And that's what that's what you tried to do, didn't they?
Yeah. Yeah.
And this is where thinking people have to say, yeah, this is a serious problem.
Like if humans don't have anything to fight for, they get lazy and, you know,
a little bit dopey, in my view.
Like we do have something to fight for.
And, you know, that that's worse.
That's worth talking about.
Like what would what would a great world with, you know, distributed,
you know, in human intelligence and artificial intelligence,
working together in a collaborative way to create abundance and fairness.
And, you know, like some some better way
at arriving at good decisions than what the truth is, that would be a good thing.
But, you know, it's not while we'll leave it to the experts
and then the experts will tell us what to do.
That's a bad thing.
Yeah. So that's.
Well, so do you the model that you just laid out, which which I think is very
I'm not somewhat optimistic about that. Yeah.
Well, it did happen on the computational front.
I mean, it happened a couple of times, both directions.
OK. Right. You know, the PC revolution was amazing.
Yeah. Right.
And Microsoft was a fantastic company
and enabled everybody to write a ten fifty dollar program to use.
Yeah. Right. And then at some point, they're also, you know,
let's say a difficult program company and they made money off a lot of people
and became extremely valuable.
Now, for the most part, they haven't been that directional
and telling you what to do and think and how to do it.
But they are a money making company.
You know, Apple created the App Store, which is great.
But then they also take 30 percent of the App Store profits.
And there's a whole section of the internet
that's fighting with Apple about their control of that platform.
Right. And in Europe, you know, they've they've decided to regulate some of that,
which that should be a converse.
That should be a social cultural conversation about how should that work?
Yeah. So. So do you do you see the more likely,
certainly the more desirable future is something like a set of distributed
AIs, many of which are under personal, in personal relationship,
in some sense, the same way that we're in personal relationship
with our phones and our computers and that that would give people
the chance to fight back, so to speak against this.
And there's lots of people really interested in distributed platforms.
And one of the interesting thing about the world is, you know,
there's a company called Open AI and they open source a lot of it.
The AI research is amazingly open.
It's all done in public.
People publish the new model all the time.
You can try them out.
People, there's a there's a lot of startups doing AI in all different kinds of places.
You know, it's it's a very curious phenomena.
Yeah. There are.
And it's kind of like a big, huge wave.
It's not like you can't stop a wave with your hand.
Yeah. Well, when you think about the waves,
there are two actually in the book of Revelation, which is describes the end
or describes the finality of all things or the totality of all things
is maybe a way for people who are more secular to kind of understand it.
And in that in that book, there are two images, interesting images about technology.
One is that there is a dragon that falls from the heavens
and that dragon makes a beast and then that beast makes an image of the beast.
And then the image speaks and when the image speaks,
then people are so mesmerized by the speaking image
that they worship the the beast ultimately.
So that is one image of, let's say, making and technology and scripture in Revelation.
But there's another image, which is the image of the heavenly Jerusalem.
And that image is more an image of balance.
It's an image of the city which comes down from heaven with a garden in the center
and then becomes this glorious city.
And it says the glory of all the kings is gathered into the city.
Like so the glory of all the nations is gathered into this city.
So now you see a technology
which is at the service of human flourishing and takes the best of humans
and brings it into itself in order to kind of manifest.
And it also has hierarchy, which means it has the natural at the center
and then has the artificial as serving the natural, you could say.
So those two images seem to reflect these these two waves that we see
and this kind of idea of an artificial intelligence will be
which will be ruling over us or speaking over us.
But there's a there's a secret person controlling it, even in Revelation.
It's like there's a there's a beast controlling it and making it speak.
So now we were mesmerized by it and then this other image.
So I don't I don't know if you ever thought about those two images
in Revelation is being related to technology, let's say.
Well, I don't think I've thought about those two images
in the specific manner that you described.
But I would say that the work that I've been doing
and I think the work you've been doing to and the public front
reflects the dichotomy between those images.
And it's relevant to the points that Jim has been making.
I mean, we are definitely increasing our technological power.
And you can imagine that that will increase our capacity for tyranny
and also our capacity for abundance.
And then the question becomes, what do we need to do in order to increase
the probability that we tilt the future towards Jerusalem and away from the beast?
And the reason that I've been concentrating on
helping people bolster their individual morality to the degree that I've managed
that is because I think that whether the outcome is the positive outcome
that in some sense Jim has been outlining or the negative outcomes
that we've been querying him about, I think that's going to be dependent
on the individual ethical choices of people at the individual level,
but then cumulatively, right?
So if we decide that we're going to worship the image of the beast,
so to speak, because we're mesmerized by our own reflection,
that's another way of thinking about it.
And we want to be the victim of our own dark desires.
Then the revolution is going to go very, very badly.
But if we decide that we're going to aim up in some positive way
and we make the right micro decisions, well, then maybe we can harness
this technology to produce a time of abundance in the manner
that Jim is hopeful about.
Yeah. And let me make two funny points.
So one is I think there's going to be continuum,
like the word artificial intelligence won't actually make any sense.
Right. So so humans collectively, like individuals, no stuff,
but collectively we know a lot more, right?
And the thing that's really good is in a diverse society
with lots of people pursuing an individual, interesting,
you know, ideas, worlds, like we have a lot of things
and more people, more independence generates more diversity.
And and that's a good thing where, you know,
it's a totalitarian society where everybody's told to wear the same shirt.
And like it's inherently boring, like the beast speaking
through the monster is inherently dull, right?
Like, but in an intelligent world where not only can we have more intelligent things,
but in some places go far beyond what most humans are capable of
in pursuit of interesting variety.
And, you know, like I believe the information and well,
to say intelligence is essentially unlimited, right?
Like and the unlimited intelligence won't be this shiny thing
that tells everybody what to do.
That's that's that's sort of the opposite of interesting intelligence.
Interesting intelligence will be more diverse, not less diverse.
Like that's a that's a good future.
And your second description, that seems like a future
was working for and also was fighting for.
And that means concrete things today.
And also, you know, it's a it's a good conceptualization.
Like, like I see the messages as my kids are taught, you know,
don't have children in the world is going to end.
We're going to run out of everything.
You're a bad person. Why do you even exist?
It's like these messages are terrible.
It's the opposite is true.
More people would be better.
We we live in a world of potential abundance, right?
It's right in front of us.
Like there is so much energy available.
It's it's just amazing.
It's possible to build technology without, you know, pollution consequences.
That's called externalizing costs.
Like we know how to we know how to do that.
We can have very good, clean technology.
We can do it would do lots of interesting things.
So if the goal is maximum diversity, then the
the line between human intelligence, artificial intelligence that we draw,
it like you'll see all these kind of really interesting partnerships
and all kinds of things and more people doing what they want,
which is the world I want to live in.
Yeah.
But to me, it seems like the the the question
is going to be related to attention, ultimately.
That is, what are humans attending to at the highest?
What is it that humans care for in the highest?
You know, in some ways, you could say, what do humans?
What are humans worshipping?
And like, depending on what humans worship,
then their actions will play out in the technology that they're creating
and the increase in power that they're creating.
Well, that's well.
And if we're guided by the negative vision,
the sort of thing that Jim laid out that is being talked to his children,
you can imagine that we're in for a pretty damn dismal future, right?
Human beings are a cancer on the face of the planet.
There's too many of us.
We have to accept top down compelled limits to growth.
There's not enough for everybody.
A bunch of us have to go because there is too many people on the planet.
We have to raise up the price of energy so that we don't
what burn the planet up with carbon dioxide, pollution, etc.
It's a pretty damn dismal view of the potential that's in front of us.
And so the world should be exciting and the future should be exciting.
Well, we've been sitting here for about 90 minutes,
banding back and forth, both visions of abundance and visions of apocalypse.
And I mean, it's I've been hard.
And I would say over the decades talking to Jim about what he's doing
on the technological front.
And I think part of the reason I've been heartened is because I do think
that his vision is guided primarily by desire to help bring about something
approximating life, more abundance.
And I would rather see people on the A.I.
front who are guided by that vision working on this technology.
But I also think it's useful to do what you and I have been doing
in this conversation, Jonathan, and acting in some senses, friendly critics
and hopefully learning something in the interim.
Do you have anything you want to say in conclusion?
I mean, I just think that the question is linked very directly
to what we've been talking about now for several years, which is the question
of attention, the question of what is the highest attention.
And I think the reason why I have more alarm, let's say, than Jim is that
I've noticed that in some ways, human beings have become to have come
to now, let's say, worship their own desires, they've come to worship.
And that even the strange thing of worshiping their own desires is actually
led to an anti-human narrative.
You know, this is a weird idea.
It's almost suicidal desire that humans have.
And so I think that seeing all of that together in the increase of power,
I do worry that the image of the beast is closer to what will manifest itself.
And I feel like during COVID, that sense in me was accelerated tenfold in
noticing to what extent technology was used, especially in Canada, how technology
was used to instigate something which looked like authoritarian systems.
And so I am worried about it.
But I think like Jim, honestly, although I say that, I do believe
that in the end, truth wins.
I do believe that in the end, you know, these things will level themselves out.
But I think that because I see people rushing towards AI almost, you know,
almost like lemmings going off a cliff, I feel like it is important to sound
the alarm once in a while and say, you know, we need to orient our desire
before we go towards this extreme power.
So I think that that's mostly the thing that worries me the most
and that preoccupies me the most.
But I think that ultimately in the end, I do share Jim's positive vision.
And I do think that I do believe the story has a happy ending.
It's just you might have to go through hell before we get there.
I hope not.
So Jim, how about you?
What have you got to say in closing?
A couple of years ago, a friend who's, you know, my age said, oh, kids coming
out of college, they don't know anything anymore.
They're lazy.
And I thought, I work at Tesla.
I was working at Tesla at the time.
And then we hired kids out of college and they couldn't wait to make things.
They were like, it's a hands-on place.
It's a great place.
And and I've told people, like, if you're not in a place where you're
doing stuff, it's grown, it's making things.
You need to go somewhere else.
Like, and also, I think you're right.
The mindset of if people are feeling this is a productive, creative technology
that's really cool, they're going to go build cool stuff.
And if they think it's a shitty job and they're just tuning the algorithm
so they can get more clicks, they're going to make, they're going to make
something beastly, you know, beastly, perhaps.
And and the stories, you know, our cultural tradition is super useful,
both cautionary and, you know, explanatory about something good.
Like, and I think it's up to us to go do something about this.
And I know people are working really hard to make, you know, the
Internet a more open place to make sure information is distributed, to make
sure AI isn't a winter take-all thing.
Like, and these are real things and people should be talking about them.
And then they should be worrying.
But the the upside is really high.
And we faced these kind of technological, like, this is a big change.
Like, the AI is bigger than the Internet, like I've said this publicly.
Like, the Internet was pretty big.
And, you know, this is bigger.
It's true.
But the possibilities are amazing.
And so so then some sense, we could actually utilize them.
Yeah, with some sense, we could achieve it.
Yeah, it's and the world is interesting.
Like, I think it'll be a more interesting place.
Well, that's an extraordinarily cynically optimistic place to end.
I'd like to thank everybody who is watching and listening.
And thank you, Jonathan, for participating in the conversation.
It's much appreciated.
As always, I'm going to talk to Jim Keller for another half an hour
on the Daily Wire Plus platform.
I use that extra half an hour to usually walk people through their biography.
I'm very interested in how people develop successful careers and lives
and and how their destiny unfolded in front of them.
And so for all of those of you who are watching and listening,
who might be interested in that, consider heading over to the Daily Wire Plus platform
and partaking in that.
And otherwise, Jonathan, we'll see you in Miami in a month and a half
to finish up the Exodus seminar.
We're going to release the first the first half of the Exodus seminar.
We recorded Miami on November 25th, by the way.
So that looks like it's in the can.
Yeah, I can't wait to see it.
The rest of you.
Yeah. Yeah, absolutely.
I'm really excited about it.
And just for everyone watching and listening, I brought a group of scholars
together about two and a half months ago.
We spent a week in Miami, some of the smartest people I could gather around me
to walk through the book of Exodus.
We only got through halfway because turns out there's more information there
than I had originally considered, but it went exceptionally well.
And I learned a lot and Exodus means ex-hodos.
That means the way forward.
And, well, that's very much relevant to everyone today as we strive
to find our way forward through all these complex issues,
such as the ones we were talking about today.
So I would also encourage people to check that out when it launches on November 25th.
I learned more in that seminar than any seminar I ever took in my life, I would say.
So it was good to see you there.
We'll see you in a month and a half.
Jim, we're going to talk a little bit more on the Daily Wear a Plus platform.
And I'm looking forward to meeting the rest of the people in your AI-oriented
community tomorrow and learning more about, well, what seems to be an optimistic
version of a life more abundant.
And to all of you watching and listening, thank you very much.
Your attention isn't taken for granted, and it's much appreciated.
Hello, everyone.
I would encourage you to continue listening to my conversation with my guest on dailywireplus.com.

I was invited by François Chavre to keynote one of the days of the Knowledge Graph Conference
in May 2023 in New York, New York.
This is a post-conference recording of the talk.
François asked me what topic I want to talk about.
I said, wiki data and wiki functions.
And he said, do you have something else to talk about?
You talked about it the last time you were here at the conference in 2019.
And I said, well, there's one topic everyone talks to me about, and this is what about
Knowledge Graphs and large language models.
And he said, perfect, do that.
And I thought, great, given how much I have been talking and thinking about it, I probably
should have a good idea what I want to say in May.
Well, it's May now.
And I'm still full of doubts about what to say.
I recently stumbled upon this quote, looking at books for my daughter, and it gives me
some hope for the talk today.
When in doubt, you can't be wrong.
So let this be my first disclaimer for today.
I am most certainly in doubt.
I don't know what the future of Knowledge Graphs is.
But I have been talking about this for a decade now, with a lot of very smart people.
So I hope that what I say will be worth your time.
There is one thing I have no doubts about.
Something big is happening.
See, this is from the book Diffusion of Innovation.
It shows how technology is adopted.
Well, my brother, who lives in a small village of 180 people on a Croatian island, he is
roughly here on this chart.
But he called me last fall to talk about Stable Diffusion, the image generation model.
JETGDP, a large language model, reached 100 million users, two months after launch.
This is a record in how fast this technology is spreading.
If there is one thing we don't need to be in doubt about, is that these technologies
are having a large impact on the world.
Last year in September, two months before the release of JETGDP, a number of researchers
and practitioners in the Knowledge Graph field met in Dachstuhl in Germany.
Our goal was to discuss the role of knowledge engineering in the 21st century, like we were
almost under a kind of shock, talking about it, processing it, trying to figure out what
it really means for us.
I am trying to distill some of the results from that seminar, and also from many other
conversations I had in this talk.
But you can also just go and read the report directly.
My second disclaimer, nothing in this talk is generated by an AI, and is explicitly marked
so, or you see a screenshot from an AI.
This is not one of those talks where we have a reveal at the end that all of this was generated.
And finally, the last disclaimer, this is a focused talk.
It is only about knowledge graphs and large language models.
It is not about the ethical questions of large language models.
It is not about blunders of current LLMs.
It is not that those systems will have about the impact on jobs and the economy.
It is not about questions of copyright or whether LLMs are a path to the destruction
of the human race.
All of these questions are interesting and deserve their own talks, and I will not talk
about them today.
So what makes me believe that there is a future for knowledge graphs in the world of large
language models?
About 6 or 7 years ago, I was working at Google on the knowledge graph.
Back then the knowledge graph wasn't necessarily a generic term yet.
It was a big question Google had to ask itself.
The knowledge graph costs a lot of resources, a lot of money, a lot of manpower, to run
and maintain.
A lot of people were working on the knowledge graph.
Is all of this work going to be obsolete?
Should we stop investing in that?
I give you the answer that I came up with back then.
Let's think first principles.
So what is a knowledge graph?
A knowledge graph represents things and their relations, usually stored in a graph database.
Mickey data is an example.
Mickey data is the largest publicly available free knowledge graph that anyone can edit.
Language model is a large language model.
Model here means that we have a neural network, nodes connected with each other with weights.
The weights in this neural networks are trained on sub-input and desired outputs in order to
create good outputs on novel inputs.
They are language models because they are trained on natural language texts.
And it is a large language model because it is trained on lots and lots of natural language
texts.
GPT is a family of large language models which have been created by OpenAI.
GPT-4 is the current version, but OpenAI has not published much data about it.
So the following data is for GPT-3.
GPT-3 has 96 such layers, as is in the picture, and it had 175 billion parameters, that is,
weights.
As a large language model, GPT-3 has been trained to provide appropriate answers to a
given prompt or input.
So for example, given the following input or prompt, who created the School of Athens?
GDP answers,
The School of Athens is a famous fresco painted by the Italian Renaissance artist Raphael.
It was painted between 1509 and 1511 as part of a commission for the Vatican's Apostolic
Palace in Rome.
The fresco is widely regarded as one of the greatest works of art of the high Renaissance.
This answer is not only correct, it is brilliant.
There is absolutely nothing to complain about, it gives context, it answers the question,
it's great.
If you go to Google and ask the same question, the answer is the same, Raphael, but the context
is even more amazing.
Picture from Raphael, his biography, data about him, who influenced him, other artists
that are being searched for.
The answer is just beautiful and amazing.
You can also go to WikiData and write a sparkle query, asking for the creator of the School
of Athens.
Again you will get the result, Raphael.
Not as pretty as Google, not as much context as with GDP, but here's one thing, GDP took
about 5 seconds to answer my question.
Admittedly, it gave a lot of context I didn't ask for, but even just passing the question
took close to a second.
Google took half a second, including passing, but it also got a lot of great context and
gave me results from the search index, even though they didn't make it to my screenshot.
WikiData took about a quarter of a second to produce the answer, and that's not all.
GDP is running on the fastest and most expensive hardware you can buy.
And the WikiData query service is running abandonware on a server somewhere.
And if you think about this, it is not surprising.
The query I asked had 6 tokens of input, a token is roughly a word for a natural language
model.
And the answer produced 60 tokens.
Now, if the answer would just have been Raphael, it would still have been at least two tokens.
Every single token runs through the 96 layers of GDP, branching out to 175 billion weights
and multiplying matrixes and softmaxing results.
Whereas in WikiData, we look up an item out of 100 million and find a key out of 10,000
to get the results values.
Those are both logarithmic operations.
It's just much cheaper.
Given this paper, getting and hosting your own copy of WikiData, 1,000 WikiData queries,
like the ones I just asked, would cost me about 14 cents in the cloud.
The authors thought this was too expensive and bought their own hardware to make it even
cheaper.
Whereas if I look at Azure pricing for GPT-4, 1,000 queries as asked before would cost $7.56.
So that's far too expensive.
4 would cost $7.56.
So that's 14 cents compared to more than $7, so a factor of 50, 50 times more expense
can make a difference.
Now, to make it very clear, I don't know how robust these numbers are.
I would love to see more robust numbers and I expect the cost for inference to go down.
But even Sam Altman, the CEO of OpenAI, the company making GDP, describes the compute
cost of JetGDPT as eye-watering.
And don't forget, he made a really good deal with Microsoft about running it on Azure.
John Hennessey, chairman of Google and former computer science professor at Stanford, said
that running a JetGDPT like Google would be 10 times more expensive.
And that's just talking about the inference costs.
Each of these models also need to be trained.
And the current state of the art language models cost millions of dollars to train.
For GPT-4, the cost was given as more than $100 million.
The cost for GPT-3 was around $4 million.
Good news is, you probably don't have to do this training.
But you can just hopefully reuse an existing open source model that you can fine tune to
your task.
This will be considerably cheaper to train a model from the foundation on.
But not for inference, the same cost that we saw earlier remains.
Here's some interesting new thoughts.
Some folks, like Sam Altman, think that the age of large language models is already over.
OpenAI says it's because of diminishing returns of further reading.
Guess what?
After reading a million books worth of text, you don't seem to learn too much new stuff.
He said that OpenAI is not working on GPT-5, but that they want to explore new ideas.
And that's great.
Because with models that are more readily available out there, such as MetasLama, we
can see new ideas happening very fast.
For example, when Lama was leaked within days, someone managed to run it on consumer hardware.
A few days later, we got it running on a phone.
And then a Raspberry Pi.
It was very slow, but it ran.
And by the way, as ridiculous as that sounds, this still means that these giant models are
more expensive to run than a knowledge graph.
And really, it's not just cost, right?
There are a few challenges that large language models need to overcome, and we need new ideas
for those.
Let's take a look.
Here's one thing that surprised me a lot.
A few weeks ago, I stumbled into one of those Wikipedia rabbit holes where I got temporarily
obsessed with figuring out one specific fact.
The correct place of birth of a not-too-well-known Croatian actress, Anna Begovic.
I was surprised that Google and Wikipedia Wikidata were giving different answers, so
I tried to figure out the truth and fix it.
Here's a screenshot of Google answering the question.
Begovic was born in Terpan.
Wikipedia used to say split.
After a bit of hunting through sources, I figured out that it mostly likely is Terpan though.
And that a lot of the sources were copying from Wikipedia and Wikidata and became contaminated
by Wikipedia and Wikidata.
This is an example of our knowledge ecosystem being a substantial danger by the way.
And that long before we have LLM syndemics.
Now if you go to Bing Chat, which is powered by OpenII's GPT, but has access to web search
and ask for the place of birth of Anna Begovic, it also answers me Terpan, which is great
and it gives me free sources for that answer.
It's just very disappointing that if you follow one of the references to Wikipedia,
you will actually find it says split.
When I asked ChatGDP instead of Bing Chat, I get a different answer.
It answers split.
This is not too surprising.
After all, Wikipedia was claiming split until just a few weeks ago and ChatGDP was trained
on a 2021 copy of the web.
Corrections to Wikipedia done in 2023 would not show up.
Split is totally expected as an answer here.
What is interesting though is, if I ask the same question in Croatian, I get a different
wrong answer, Zagreb.
Now that's an interesting answer because it demonstrates us two things.
Knowledge in ChatGDP seems to be not stored in a language independent way, but is stored
within each individual language.
Depending on the language I ask the question in, I receive a different answer.
Also, the English answer at least has support in the training data.
Wikipedia did say split.
The Croatian answer, where does Zagreb come from?
I can find a single source that says that Inabegovic was born in Zagreb.
But here's the thing.
What if ChatGDP is actually just guessing, it's just making it up?
What is the probability of Zagreb given the prior of Croatian actors?
In order to figure that out, I want to check Wikipedia.
I asked ChatGDP to give me a Sparkle query for Croatian actors in the places of birth.
The query it returns to me is good enough for our purpose.
I can just copy and paste it.
It is subtly wrong if you read it in detail.
It does not ask for Croatian actors, but for actors born in a place in Croatia.
But again, good enough for our purposes.
But still this is fascinating.
ChatGDP got out of the box all of the QIDs right in this query.
For Croatia, for actor, it got a property ID right.
It can make Sparkle queries that are syntactically right.
And all of that with zero shot extra training.
There was no look up on the web.
ChatGDP just knows these QIDs by heart.
In fact, you can totally ask ChatGDP to make you a table of all the countries of the European Union
and their QIDs.
It has a lot of QIDs memorized.
Just you never know if maybe one of them is wrong or not.
The query can be copied and pasted right into the Wikipedia query service.
And it runs giving 445 answers.
And you can already see in the screenshot that the first few are all in Zagreb.
So now we can see that of the 445 Croatian actors with a place of birth, 154 are born
in Zagreb, about a third.
This gives a pretty good conditional probability for the birthplace of Croatian actors.
ChatGDP is guessing the place of birth for Anna Begovic in Croatian, even though it knows
it in English.
This leads me to something my manager at Google used to say.
You can machine learn Obama's birthplace every time you need it, but it costs a lot
and you're never sure it is correct.
Another interesting observation is that GPT isn't particularly good with knowing what
it knows.
For example, here we are asking for cities with mayors born after 1998.
I wanted to ask something that I expected to be not trivially Google-able, where there
wouldn't be a listicle or table on the map.
So ChatGDPT correctly points out that that would make them younger than 23 at its cut-off
date of 2021.
So it is unlikely that anyone would be mayor at that age.
Asking BingJet also says that it doesn't know anything about mayors born after 1998.
Let's check Wikidata.
I again use ChatGDPT to help me write a query, although this time I had to make a few fixes.
And indeed, Wikidata has an answer.
It tells me about Christian von Waldenfelds, who was born in April 2000, mayor of Lichtenberg
in Bavaria, Germany.
He became mayor in 2020, well before the cut-off date of ChatGDPT by the way.
This is no endorsement of his politics or his platform as he is running by the way.
Now that we know the answers, we can guide BingJet and get the mayor of Lichtenberg at
his age.
And obviously this answer is inconsistent with its previous answer, but ChatGDPT or BingJDPT
are both completely unaware of this inconsistency and don't care.
Large language models are not yet graded being consistent, whether about individual facts
or across different languages.
They are also not always very good at math.
But even if they were good at math, we have to answer the same question that we do for
looking up facts in a knowledge graph.
Why would you ever use 96 layers, 175 billion parameters model to do multiplication?
And that's something you can do in a single operation on your CPU.
Why internalize knowledge in an LLM if you can externalize it in a graph store and look
it up when needed?
Don't get bedazzled by the capabilities of large language models.
Autoregressive transformer models such as ChatGDPT are Turing complete and they are
just a very expensive reiteration of Turing's tarpid.
You can do everything with them, but it doesn't mean you should.
Use LLMs where they are efficient and use other things where they are better.
One possible solution to this capability gap are so-called augmented language models.
Toolformer being a particularly well-known example.
Or for ChatGDPT, that's what ChatGDPT plugins are there for.
The idea is that we can enrich large language models with additional systems which are good
and efficient at specific tasks such as math or other functions, or looking up facts or
query results in a knowledge graph such as Wikidata.
I mean, given that ChatGDPT already can, zero shot, create queries against Wikidata, there
isn't that much to do to make them work together.
Some folks think that some mapping of strategic nodes into a knowledge graph with embeddings,
we can easily connect a knowledge graph directly to a large language model.
But we don't even need to do that.
We can just use the Sparkle query generation ability directly and ask queries against Wikidata.
And not only can we connect LLMs to a knowledge graph, but also to a repository of functions
such as WikiFunctions.
Both knowledge graphs and functions would be tools the LLM can learn to use.
This work is trying to understand how knowledge is stored in the parameters of a large language
model.
And when we look at this work, we start to understand why large language models are large.
Do they really have to be this large?
Let's compare with stable diffusion 1.
Stable diffusion 1 is a text-to-image generator.
It has to understand natural language prompts, just as GPT does.
It can make an image out of basically any prompt.
It can also generate the image of a good number of celebrities.
Here for example I am asking for the Pope, Geleta Thunberg, Idris Elba, Michel Yeo, Helen
Mirren and Yanle Kuhn to explain knowledge graphs with the help of a whiteboard.
So GPT-3 had 175 billion parameters.
How many parameters do you think are in stable diffusion 1, knowing all these people being
able to generate them?
890 million parameters.
Now I think 890 million is a lot, but GPT-3 is about 200 times larger than stable diffusion
1.
And it's no surprise.
Think of it.
All the knowledge that we were using for questions answering so far.
The embedded QIDs, what are the member countries of the EU, the place of birth of individual
people.
I mean if it has Anna Begovic, I am sure GPT-3 knows the place of birth of millions of individuals.
All this taught in many different languages.
All of this is taught in those hundreds of billions of parameters.
You don't need that for text generation.
But you need it if you want to answer all these questions we have been asking.
And indeed Metas Lama, which came out a few weeks ago, is considerably smaller than GPT-3,
about 25 times smaller.
But it seems to be rather competitive in terms of language understanding and fluency.
So yes, we could internalize all the 1.4 billion statements in Wikidata into a large language
model.
But what if we go the other way around and try to externalize the knowledge model instead?
If we leave the language model to deal with language, but push the knowledge to a knowledge
graph or a different knowledge model, in a world where language models can generate infinite
content, knowledge becomes valuable.
And that takes us back to Jamie Taylor's rule.
We don't want to machine learn Obama's place of birth every time we need it.
We want to store it once and for all, and that's what knowledge graphs are good for.
To keep you valuable knowledge safe.
I am of the strict belief there is no reason to ever again, manually enter the place of
birth for Anna Begovic.
This makes knowledge for Wikidata both valuable and a public good.
The knowledge graph provides you with the ground truth for your language models.
By the way, the other way around is also true.
Large language models can be an amazing tool to speed up the creation of a knowledge graph.
They are probably the best tool for knowledge extraction we have seen developed in a decade
or two.
We want to extract knowledge into a symbolic form.
We want the system to overfit for truth.
And this is why it makes so much sense to store the knowledge in a symbolic system.
One that can be edited, audited, curated, understood.
If we can cover the long tail by simply adding new notes to the knowledge graph.
One we don't train to return knowledge with a certain probability to make stuff up on
the fly, but one where we can simply look it up.
And maybe not all of the pieces are in place to make this happen just yet.
There are questions around identity and embeddings, how exactly do they talk with each other,
but there are good ideas to help with those problems.
And knowledge graphs themselves should probably also evolve.
I want to make one particular suggestion here.
Freebase, the Google knowledge graph, Wikidata, they all have two kinds of special values
or special statements.
The third one is the possibility to say that a specific statement has no value.
Here for example, we are saying that Elizabeth the first has no children.
The second special value is the unknown value.
That is, we know that there is a value for it, but we don't know what the value is.
It's like a question mark in the graph.
For example, we don't know who Adam Smith's father is, but we know he has one.
It could be one of the existing notes, it could be one that we didn't represent yet.
We have no idea.
My suggestion is to introduce a third special value.
It's complicated.
I usually get people laughing when I make this suggestion, but I'm really serious.
It's complicated is what you would use if the answer cannot be stated with the expressivity
of your knowledge graph.
This helps with maintaining the graph to mark difficult spots explicitly.
This helps with avoiding embarrassing wrong or flat out dangerous answers.
And given the interaction of LMS, this can in particular mark areas of knowledge where
we say, don't trust the graph, can we instead train the LLM harder on this particular question
and assign a few extra parameters for that?
But really, what we want to be able to say are more expressive statements in order to
build a much more expressive ground truth.
To be able to say sentences like these, Jupiter is the largest planet in the solar system.
That's what we are working on right now.
With abstract Wikipedia and leaky functions, we aim to vastly extend the limited expressivity
of Wikidata so that complicated things become stateable.
This way, we hope to provide a ground truth for large language models.
In summary, large language models are truly awesome.
They are particularly awesome as an incredibly enabling UX tool.
It's just breathtaking, honestly.
If the things are happening which I didn't think possible in my lifetime.
But they hallucinate.
They need ground truth.
They just make up stuff.
They are expensive to train and to run.
They're difficult to fix and repair, which is great if you have to explain something.
Sorry, I cannot fix your problem.
The thing is making a mistake, but I don't have a clue how to make it better.
They are hard to audit and explain, which in areas like finance and medicine is crucial.
They give inconsistent answers.
They struggle with low resource languages.
And they have a coverage gap on long tail entities, which is not easily overcome.
All of these problems can be solved with knowledge graphs, which is why I think that the future
of knowledge graphs is brighter than ever, especially thanks to a world that has large
language models in it.
Thank you for your attention.
Thanks to all the people who helped me clarify my thinking around this topic.
And if you have any questions, feel free to put them in the comments.

This video was brought to you by Brilliant.
Hi, welcome to another episode of Cold Fusion.
The following song is 100% generated by AI.
Everything you hear is all artificially generated.
By AI generated, I mean the lyrics, which was chatGBT, the rhythm, the vocals.
To get this output, all I did was type in a text prompt of what I wanted.
There are a few revisions, but still, the result is honestly insane.
And if you think that's a one-off, here's another example.
I wanted a classical piece, so I typed this.
How about some guitar-driven stuff?
Or some R&B.
Or some UK Garage.
All very impressive.
Even the album covers were made using AI.
And that's just to let you know what we're dealing with here.
As some of you know, I've been a musician for the better part of 20 years,
and a producer for a decade.
If you've watched a Cold Fusion video before, you've heard some of my music.
So being a technology enthusiast as well, I do have mixed feelings about this.
AI has finally come to something very close to my heart.
We've seen AI voice-swapping that could get artists to perform songs in different genres,
but this is something totally different.
With this latest generation of AI music, it's not just the vocals,
but the backing music, which is artificial,
which in my view is much more difficult for AI to do coherently.
What you just heard previously was from a platform called Udio,
and it was created by a group of ex-deep-mind engineers.
It's a watershed moment for AI music.
And they're not the only ones.
A few months ago, there was a similar application called Suno AI.
In a recent video, I looked at the AI deception,
and it was about how some companies are lying and over-promising the capabilities of AI.
But on the flip side, I said that there's also some companies
that are obfuscating the true extent of AI replacing jobs.
Today's episode falls in the second category.
First it was images, graphic design, writing, videos, and now finally music.
It's clear that AI music systems will eventually impact those in the music industry,
but how?
In this episode, we explore AI music apps,
what they're all about, how it works, its criticisms,
and how AI could disrupt the music industry.
I also had the opportunity to speak to the amazing musician and YouTuber, Rick Beardo,
to get his thoughts on this.
And I also spoke to Taryn Southern, who's considered to be one of the first artists
to commercially release music using AI.
Alright, so on the count of three, let's go.
Quote,
End quote.
Mikey Shulman.
And it just about summarizes what these AI companies are trying to achieve.
In fact, the premise of Suno and Udio is basically the same.
Now anybody can create music, even though they have no prior musical knowledge.
Just type a text prompt and away you go.
Users can provide subjects, instruments, feelings, and or their custom lyrics.
And in just a minute, a track is ready to play.
30 seconds for Udio and two minutes for Suno.
Both platforms can extend tracks and provide different variations.
And both are free to use right now.
Some say that Udio's first version is already pretty good, better than Suno's version three.
I've used both of them to gain insight, but I ended up liking Udio's output more personally,
because it was cleaner and possessed a better understanding of less typical genres.
So even though the examples I've shown you and will show you are through using Udio,
keep in mind that a lot of the content in this video also applies to Suno.
Suno version three is now available to everyone, including those on the free plan.
It is the best AI music generator by far, and it just got even better with V3.
A completely free AI music generator launched called Udio.
People have been saying this is the chat GPT moment for AI music, or calling it the Sora of music.
A lot of the time, people massively overhype new AI launches.
That's how everyone gets clicks, but there's some truth to this one.
To understand just how revolutionary this tech is,
it helps to know how modern music is made before AI.
The first step is to come up with a guitar riff.
I made this one a few years ago when mucking around on my acoustic guitar,
but for the final track, I used a Gratch Electric guitar with a delay and reverb pedal and a distortion pedal.
I was going for a calm 3-4 Waltz time signature with a post rock feel.
After I recorded this, I hopped into Ableton and built some bass and drums and atmosphere around the guitar riff.
Even for this process, every step involves many sub steps, including EQing, compressing, adding effects,
and just listening to the components in isolation and together.
As the song is being created, you'll definitely come across some production problems where frequencies clash.
Solving these issues are just a part of the nature of mixing sounds together.
Finally, I added some vocals and then arranged the track.
So in total, a finished song can take days or even weeks to complete,
and sometimes the songs just don't work out, so you have to try again.
But with all of that effort, exploring the sonic stage and pulling a song out of the ether is half the fun.
But now in contrast with AI, all people have to do is type in some words and then get a song.
I hope you can see the difference clearly now.
Here's some observations that I found with Udio.
It includes production techniques like side chaining, tape effects, reverb, and delay on vocals in appropriate areas.
In some outputs, the vocals seem extremely real.
There's also harmonies in there, and even for electric guitar, it mimics the sounds of different pickups.
All just so insane.
Some weaknesses include, sometimes it messes up big time, and I'll show a couple of examples of that.
It's very limited in flexibility.
Once you get an output, you can't really change it.
There's low fidelity on some tracks, and it has some weaknesses in some genres such as UK jungle.
Okay, so what's the story behind these applications?
Suno is a venture backed company founded in late 2023.
All four of the founders previously worked at Kensho, an AI tech startup for financial data,
which was later acquired by S&P Global.
Suno even partnered with Microsoft Co-Pilot and is up to version 3.
Like Suno, Udio was founded in late 2023, but the company only recently went out of stealth mode
and made their application public a couple of weeks ago.
It was founded by an ensemble of former researchers from Google DeepMind
and has the financial backing of popular Silicon Valley venture capital firm Anderson Horwitz
and also from musicians like Common and Will I Am.
As you've seen, well, heard, the output is a rich combination of all sorts of instruments,
so the training data must have been significant.
And that leads us to the next section.
How do these applications work?
Well, we'll dive in in a bit, but first, let's rewind the tape a bit.
This isn't strictly the first time this has been tried.
Making music with the aid of computers goes back to 1957.
Lijaren Hillar and Leonhard Isaacson would compose a piece called the Iliac Suite.
This piece of music is often considered to be the first created with the aid of a computer.
There were notable efforts in the 1980s to point computers into a new direction, generative music.
In 1984, scientists and composer David Cope created EMI, which stands for
Experiments in Musical Intelligence.
It was an interactive software tool that could generate music in the styles of different composers,
from short pieces to entire operas.
And I said, you know, what if I could create a program that would create Bach corrals
and Cope and Stravinsky and Mozart and anybody?
And the only way I could think of doing that was to create a program that was built around a database of music.
Let's say I had a database of Bach corrals, all 371 of them.
And he had a little program that was set on top of this, the smallest program I could make.
They would analyze that music and then create a new Bach corral that wasn't one of the Bach corrals
that was in the database, but sounded like them all.
And some of the outputs had been used in commercial settings over the years.
There's a very interesting story regarding EMI, and I'll get into that later in the episode, so stick around for that.
And then there was the computer accompaniment system in 1985.
It used algorithms to generate a complementary music track based on a user's live input.
The computer accompaniment system is based on a very robust pattern matcher
that compares the notes of the live performance as I play them to the notes that are stored in the score.
Even David Bowie developed a tool in the 90s.
It was called the Verbesizer, a digital approach to lyrical writing.
Well, that's all well and good you might be thinking.
But what about the modern stuff?
Well, as you probably know, from 2012, the world switched from hard-coded narrow algorithms to neural networks.
And with that, the modern AI boom had officially started.
But the question has to be asked, how are neural nets being used in music generation?
Well, in 2016, Google's project Magenta turned a few heads
when they released an AI-generated piano piece made by deep learning algorithms.
Ultimately, the next big step was to generate music just based on a simple text query in natural human language.
Google's music LM did just that, and honestly, it was impressive for the time, but far from perfect.
This was followed by open AI's jukebox, stable audio, Adobe's music generation gen AI, and YouTube's Dreamtrack.
Now, these were all valiant efforts, but they all had the same problem.
They were ultimately fairly limited.
They worked to a degree, but more often than not, the compositions sounded rigid and just not very human to put it bluntly.
But even with such limitations, there were some trying to push the boundaries.
Taryn Southern is often considered to be one of the first musicians to use modern AI to release commercial music.
And this was all the way back in 2017, if you can believe it.
I asked her all about it.
I was having trouble finding really high-quality music for the documentary that was relatively inexpensive.
So I was looking for alternate options, and I came upon this article in The New York Times that was looking at artificial intelligence as a way to compose music.
And I thought, well, that's really interesting.
So I started experimenting with it, was blown away by what was possible.
You know, this was even before LLMs hit the scene.
And I think where we were at with music creation and AI back in 2017 was actually pretty far along.
And so I was so excited by what I was hearing that I just decided to create an entire experiment out of the project and made an album using, I think, four different AI technologies.
Keep in mind that what Taryn did back in 2017 was quite different to what's going on today.
There was still a lot of work that had to go into the compositions.
But now, anyone can type in some text and get a decent output.
So how does it all work?
What's the tech behind it?
Like most modern generative AI applications, for example, LLMs, these systems use vast amounts of data to understand patterns and then create an output based on the user's input.
For ChatGPT, the output is text, and for these AIs, it's original songs and lyrics.
We'll touch on the ethical concerns a bit later, but you can probably realise something.
A large language model like ChatGPT, for example, generates text by predicting the next word in a text sequence.
But composing music is significantly more complex due to a lot of variables.
Instrument tone, tempo, rhythm, sound design choices, volume of different components, compression and EQ are just some of the variables that has to be considered.
In addition, the system must understand what the user wants and how that relates to genres and particular sounds in a coherent yet pleasing way.
Not easy by any means.
But what about the audio synthesis itself?
Many point to audio diffusion as the secret source behind generative music.
In simple terms, it's the process of adding and removing noise from a signal until you get the desired output.
We first saw similar methods used in images, where image diffusion starts off with random noise, which is then refined to a final image based on the interpretation of the prompt.
Audio diffusion works in a similar way.
This is a very high level breakdown, just a safe time.
But if you want to get into the depths of the technicalities, I'll leave a link to a few articles in the source document in the video description, as always.
If you've been following the generative AI space for the last two years, you already knew this part of the video was coming.
OpenAI's CTO, Mira Murati, was under public fire following an interview about the training data of their video generation tool, Sora.
When asked if the impressive video output from Sora was the result of data trained on videos from YouTube, Facebook and Instagram,
she said that she, quote, was not sure about that.
To many online, the answer was suspicious.
With only a few weeks since the launch of Udio and Suno version 3, there's a question that has to be asked.
Were these systems trained on copyrighted material?
One user set out to investigate.
They ran their test by entering in similar lyrics to the song Dancing Queen by ABBA.
In the prompt, they had no direct mention of the band or the song.
But the outputs were very close to the original, including the basic melody, rhythm and even the cadence of the vocals.
Now even human musicians are continuously accused and taken to court for a lot less, as the user later points out.
It's not impossible to achieve these results without the original song being present in the data set.
But the similarity is striking.
This result was also solidified by other experiments carried out by the user.
For obvious reasons, I can't play any of the copyrighted music here to show you the comparisons,
but as always, I'll link the original article in the show notes.
But here's the thing.
It seems like the founders already knew that this was coming and they accepted it as part and parcel of the new AI startup culture.
Antonio Rodriguez, one of Suno AI's investors, told Rolling Stone that he was aware of potential lawsuits from record labels,
but he still decided to invest into the company.
Suno AI has stated that it's having regular conversations with major record labels and is committed to respecting the work of artists.
If that's actually true is questionable.
Especially when a few weeks ago, over 200 plus artists wrote an open letter to AI developers and tech companies to, quote,
cease the use of AI to infringe upon and devalue the rights of human artists, end quote.
The letter was signed by some big names, old and new, from Bon Jovi to Billy Eilish and Metro Boomin.
All that begs the question, what does this mean for the future of the music industry?
One way to think of this AI in the broadest sense is like an out of control central bank monetary policy.
It's the equivalent of printing money leading to inflation, but this time it's diluting and devaluing the supply of music instead of money.
If you're not the top 1% of successful musicians, the music industry is already a hard place to make a consistent and stable income.
From TikTok transforming how songs are consumed and discovered to digital music platforms' relationships with musicians.
The industry is already difficult, but now there's an unlimited supply of AI music coming right at you like a tsunami and it's going to be hard to stay afloat.
If you're a musician who exclusively makes stock music or royalty free sounds for commercial purposes,
this all isn't good news. But what about other musicians? I sat down with Rick Beardo to discuss what this means for the future.
So do you think AI will completely replace musicians one day at all or do you think that's not going to happen?
No, I don't think so. People have too much fun playing real instruments. I'm not going to stop playing just because there's AI guitar things.
Say Spotify in the future, they have their AI versions of songs and then you have people using the models that already exist to make their own music and uploading that to Spotify.
What do you think that would do to the potential income of the real artists who are making and composing their own music?
Oh, it definitely has an impact in it. Yes. It's just diluting because there's no way real artists could compete with generative creations, right?
I mean, how many things can you put up there in a day? How many things can you generate? It'd be very difficult to compete with that.
This is an interesting question. Do you think that one day will actually have a number one AI billboard song?
Yes, probably two years from now.
You heard it here first, everyone.
There will be a lot of news stories about it and people will say, boy, I really like this and then they'll just create more and there will be more.
I can see a time when, you know, nine out of the top 10 songs are AI generated within 10 years.
I said in one of my videos a year and a half ago, I think it was or so a year ago that people won't care if it's AI, if they like the song.
And I firmly believe that it's just a matter of how people are being compensated now.
Are you excited about any aspect of this at all?
Totally excited. I mean, I think that the jobs that can be made easier can mastering be done better by AI probably, can mixing be done better by AI probably.
There's a lot of things, vocal editing, picking between takes, at least doing rough edits, rough edits for YouTube videos.
I'm excited about it. You know, people, when drum machines came around in the 80s, oh, they're going to take drum, you know, drummers' jobs away.
Then drummers back in the 80s emulated drum machines and they played drum machine fills and they do all these same kind of fills.
And then people would bring in drummers, real drummers to emulate drum machines. They'd have their things programmed.
Do you think becoming a professional musician or even surviving as one or driving as one would be more difficult because of AI in that case?
Not sure about that. I don't know if becoming a professional musician will be affected by it. I think people enjoy playing music and regardless of whether there are AI versions out there that people are listening to.
You know, the rise of auto tune and things like that would have a synthetic sound enabled these programs.
I mean, once you have a pitch corrected voice, how far of a stretch is it from an AI created voice that actually has tuning imperfections in it like a real person?
That's not that different, honestly. How many things are actually generated by computers?
Grab some samples off splice. You create the hi-hat track with an 808 hi-hat.
You get your kick and you're making a hip hop tune and then you grab some keyboard sounds and you don't even know how to play the keyboards and you hold down some things.
You get some samples from here and there, you put them in and then you create your vocal over it that you auto tune and then you move notes around.
You know, it's pretty synthetic at that point. You know, what's the difference between that and AI?
The current wave of AI, including music generators, is made possible by neural networks. But what do they do and how do they work?
Fortunately, there's a fun and easy way to learn about that. Brilliant.
I've talked about how I've used Brilliant's courses on artificial neural networks before and for good reason, but they also have great interactive STEM courses in anything from maths to computer science and general science.
It's convenient because you can learn wherever you like, whenever you like, and at your own pace.
There's also follow-up questions to make sure you've digested what you've learned.
So whether you just want to brush up on your learning or need a refresher for your career, Brilliant has you covered. You can get started free for 30 days.
And for Cold Fusion viewers, Brilliant is offering 20% off an annual plan.
Visit the URL brilliant.org slash coldfusion to get started.
OK, so back to the video. Last year, a US federal judge ruled that AI artwork can't be copyrighted.
As these apps gain popularity, we need to protect artists. But how this is done and how this will be implemented is still up for debate.
Things are uncertain, but one thing is for sure. We're heading into a new era of copyright legislation around art and artificial intelligence.
If you're more interested in the lore of copyright and artificial intelligence, check out my conversation with Devin from LegalEagle.
In the future, we're going to be inundated with AI music, but at that stage, live music performed by humans will become increasingly valuable.
But another thing that I'm worried about is another form of AI fatigue, where any amazing human-made music that you hear could be diminished in a few years,
because those that hear it could just think that it could be AI generated.
As for me personally, I think music AI generators could make for a great sampling tool.
For example, I turned that classical piece generated by AI at the beginning of the video into a full track.
I'll play part of it as an outro to this episode, but I'll leave a link for the full version below.
But in saying that, I can't help but feel a little sense of loss.
That joy that comes from having an idea in your head and turning it into musical form is no longer strictly human.
Music creation is a personal journey of discovery, and it's unsettling that the art form is changing.
But on the flip side, I can understand how freeing this is for those who have no musical knowledge and just want to create something.
I'm not blind to that.
I know we covered a fair bit in this episode, but let me tell you a little story that I think perfectly encapsulates it all.
We have to go back to 1997, to the University of Oregon, where a small audience is patiently waiting to see a battle between a musician and a computer.
Dr. Stephen Larson is the musician in question, and he also teaches music theory at the University.
He's ready to go up against a computer to compose a piece of music in the style of the famous Johann Sebastian Bach.
The idea is simple. There's going to be three pieces played live.
One that was composed by the original Bach, one by Dr. Larson, and one by Emmy.
The computer-generated music, which I mentioned at the start of the episode.
All three entries will be performed live to an audience, and they'll have to guess which piece is composed by whom.
Once all the performances ended, the audience incorrectly thought that Larson's piece was made by the computer, and the one composed by Emmy?
They thought that that was the original Bach composition.
Dr. Larson was genuinely upset about this.
Bach is absolutely one of my favorite composers.
My admiration for his music is deep and cosmic, that people could be duped by a computer program was very disconcerting.
Now you could take that quote, put it in any conversation in 2024 about AI music, and it would be just as relevant.
The circumstances have changed, but as humans, we still perceive art the same.
David Cope, the inventor of Emmy, once revealed that his artificial program can make, quote, beautiful music, but maybe not profound music.
And I think I agree with that.
Ultimately, AI generations are going to get more polished, but a sounding, higher fidelity, and it's all going to be at our fingertips.
But we have to remember, it's the messiness of our human selves that inform the art, and that's the part we relate to most.
What makes us human is that we can listen to music, not just hear it.
And that is where we're at with AI-generated music.
So I hope you enjoyed that episode.
If you did, feel free to subscribe.
There's plenty of other interesting stuff here on Cold Fusion, on science, technology and business.
Anyway, that's about it from me.
My name is DeGogo, and you've been watching Cold Fusion, and I'll catch you again soon for the next episode.
Oh yeah, and if you want to check out my full interview with Rick Beardo, it's on the second podcast channel.
I'll leave a link below.
Thank you for watching.

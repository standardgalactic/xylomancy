This video is brought to you by Brilliant.
Welcome to another episode of Cold Fusion.
It's the year 2054 and the United States has just federally launched a new police unit.
It's special, precisely because the police will arrest people who commit crimes in the future,
a concept known as pre-crime.
This is, of course, the plot to the 2002 movie Minority Report.
Such a world where crimes are known before they're committed is disturbing.
It sits in between the comforts of security and low crime,
but also complete government knowledge and control.
It's scary, but at the end of the day, it's just a concept in a movie.
Or is it?
This next breakthrough is a lot deeper, all the way down to the very tissue of our brains,
where researchers have used AI to translate brain scans into text.
UT researchers have essentially created a device called a semantic decoder
that can read a person's mind by converting brain activity into a string of text.
In May of 2023, at the University of Texas at Austin,
a device was created that can turn a person's brain activity and thoughts
into a conscious, understandable stream of text.
The company Meta also recently did the same.
But that's not all.
Just last month, Meta also unveiled an AI system that can analyze a person's brain waves
and predict what that person is looking at.
If that's not enough, they were able to do this in real time.
Basically, the system was mind-reading.
Has a multi-billion-dollar social media conglomerate just created a telepathy device
with the help of generative AI?
On the one hand, this is a potential disaster for privacy,
but on the other hand, such technologies could help many who can't speak due to illness or injury.
It appears that these days, every major technological innovation raises more questions than answers.
We're not quite yet at the capabilities of Minority Report,
but reading brain waves and interpreting what people are thinking is a step in that direction.
In this episode, we'll take a look at all of this, so get ready for a wild ride.
Let's start by diving deep into the University Mind-Reading AI, then we'll get onto what Meta is doing.
You're going to want to hang around for that one because it's world-changing stuff.
You are watching Cold Fusion TV.
YouT's newest artificial intelligence tool could be a game changer for some folks.
Our real hope is that this could help people who have lost the ability to communicate for some reason.
Jerry Tang, a doctoral student in computer science, and Alexander Hooth,
an assistant professor of neuroscience and computer science,
led the research at the University of Texas at Austin, the team that developed the mind-reading device.
They published their study in the Journal of Nature Neuroscience.
The mind-reading device is called a non-invasive language decoder.
While language decoders are not new, using them with the recent breakthroughs in generative AI certainly is.
Older versions could recognize only a few words here or there, or at best, a couple of phrases.
Jerry Tang and his team have managed to blow this out of the water.
Their device can reconstruct continuous language from perceived speech, imagined speech, and silent videos.
A technology like Neuralink, on the other hand, would be an invasive method.
It would provide clearer data, but has higher risks associated.
The UT Austin team created a new system called a semantic decoder.
It works by analyzing non-invasive brain recordings.
It works by analyzing non-invasive brain recordings through functional magnetic resonance imaging, or FMRI.
The decoder reconstructs what a person perceives, or imagines, into continuous natural language.
It sounds incredible, but before we get too excited, let's understand the limitations of a non-invasive system
and the challenges that the researchers had to overcome.
While invasive methods such as electrodes implanted into the brain have demonstrated notable achievements in the past,
non-invasive systems are still in their early stages of development, trailing behind in vital areas.
The team chose the non-invasive FMRI because it has excellent spatial specificity,
meaning that it can pinpoint neural activity in the brain with great accuracy.
However, its major limitation is its temporal resolution, or in other words, how quickly it captures changes in brain activity.
The blood oxygen level dependent, or bold, signal that the FMRI measures is notoriously slow,
taking about 10 seconds to rise and fall in response to neural activity.
Now, imagine trying to decode natural language where we speak at a rate of one to two words per second.
So how did they tackle this problem?
To address this, researchers employed an encoding model which predicts how the brain responds to natural language.
To train the model, they recorded brain responses while subjects listened to 16 hours of spoken narrative stories.
By extracting the semantic features and using linear regression,
they built an accurate model of the brain's responses to different word sequences.
It's basically a model that has been trained with over 15 hours of data per person
to take in their brain activity when they're sitting in an MRI scanner and to spit out the story that they're listening to.
Here is where it becomes a little more interesting.
To ensure the words that were decoded had proper English grammar and structure,
the researchers incorporated a generative neural network language model, or generative AI.
Interestingly, they used GPT1, an earlier version of the now famous chat GPT models.
This model can accurately predict the most likely words to follow in a given sequence.
But here's the catch. With countless possible word sequences, it would be impractical to generate and score each one individually.
This is where a clever algorithm called Beam Search comes into play.
Let me briefly explain how this works.
First, it analyzes brain activity and then generates word sequences, one word at a time.
It starts with a group of the most probable word sentences.
As it analyzes brain activity further and identifies new words,
it generates more possible continuations for each sequence in the group.
This approach gradually narrows down possibilities and the most promising options are kept for each step.
By keeping the most credible sequences, the decoder continuously refines its predictions
and determines the most likely words over time.
An example would be necessary here.
Consider the scenario.
If a person hears or thinks the word Apple, the decoder learns the corresponding brain activity.
When the person encounters a new word or sentence,
the algorithm utilizes learned associations to generate a word sequence,
matching the language stimulus as meaning.
For example, if a person hears or thinks I ate an apple,
the algorithm may generate I consumed a fruit or an apple was in my mouth.
The generated word sequence may not be an exact replica,
but it captures the semantic representation.
So with the help of an ingenious encoding system, generative AI and a clever search algorithm,
the scientists were able to circumvent the limitations of invasive fMRI scans.
So with the challenges tackled,
it was time for the researchers to put the system to the test and see what it could do.
The team trained decoders for three individuals
and evaluated each person's decoder by analyzing the brain responses while listening to new stories.
The goal was to determine if the decoder could understand the meaning of the information.
And the findings? Absolutely remarkable.
The decoded word sequences not only captured the meaning of the information,
but often replicated the exact words and phrases.
This shows that semantic information or in-depth meaning
can indeed be extracted from brain signals measured by fMRI.
The researchers also wanted to find out
whether different parts of the brain have similar or different information about language.
Essentially, they were looking for how the decoding process works across cortical regions.
Cortical regions are specific areas in the outer layer of the brain known as the cerebral cortex.
Each region corresponds to distinct sensory, motor, or cognitive processes.
The researchers compared predictions from decoding word sequences in these regions,
and they discovered something extraordinary.
They found out that diverse brain regions redundantly encode word-level language representations.
Basically, our brains have backup systems for language understanding and usage.
It's like having several copies of the same book.
If you lose one copy or it gets damaged, you can still read the book with another copy.
So it suggests that even if one brain region is damaged,
others can process the same information preserving our language abilities.
And that's a pretty astonishing find.
The researchers went even further.
They tested their system on imagined speech,
which is crucial for interpreting silent speech in the brain.
Participants were told to imagine stories during fMRI scans,
and the decoder successfully identified the content and meaning of their stories.
The researchers were ecstatic about their new discoveries
and intrigued by the prospect of decoding other types of information as well,
so they explored cross-modal decoding.
Researchers say they were surprised the decoder still worked,
even when the participants weren't hearing spoken language.
Even when somebody is seeing a movie,
the model can kind of predict word descriptions of what they're seeing.
This study is a significant advancement for brain-computer interfaces
that don't require invasive methods.
However, the study highlighted the need to include motor features
and participant feedback to improve the decoding process.
But this story isn't over.
As promised, Meta has come into the picture,
and they've come to raise the bar of what we thought was possible.
Although in the movie Minority Report,
it was PsyKix that helped reveal the intentions of an individual.
It's AI that's looking to do the same here in real life.
But how does AI work anyway?
Well, now there's a fun and easy way to learn about that
and many other STEM subjects with today's sponsor, Brilliant.
Brilliant's course on artificial neural networks is perfect for that.
I've used it to brush up on some background context
when I was making AI episodes.
Brilliant has interactive STEM courses
in anything from maths and computer science
to general science and statistics.
Whether you just want to brush up on learning
or need a refresher for your career,
Brilliant has you covered.
You can get started free for 30 days
and the first 200 people to sign up get 20% off an annual plan.
Visit the URL brilliant.org
slash coldfusion to get started.
Thanks, and now back to the episode.
On October 18th, 2023,
META pushed this field of research even further.
Instead of using fMRI,
which has a resolution range of one data point every few seconds,
META used MEG technology,
another non-invasive neuroimaging technique
which can take thousands of brain activity measurements per second.
With this, they created an AI system
capable of decoding visual representations in the brain.
The system can be deployed in real time
to reconstruct from brain activity
the images perceived and processed by the brain at each instant.
I told you this was going to get wild.
The system consists of three parts,
an image encoder, a brain encoder, and an image decoder.
The AI was trained on a public data set of MEG recordings
acquired from healthy volunteers.
In META's research, it was found that, quote,
brain signals best align with modern computer vision AI systems
like Dino V2.
This AI learned imagery by itself
and wasn't supervised by human labels.
META states that, quote,
self-supervised learning leads AI systems
to learn brain-like representations.
The artificial neurons in the algorithm
tend to be activated similarly to physical neurons
in the human brain in response to the same image.
This functional alignment between such AI systems
and the brain can be used to guide the generation of images
similar to what the participants see in the scanner.
Using this technique, META managed to read the minds of people
and interpret what they were looking at.
On the left is the original image shown to the participant.
On the right is what the AI thinks the person is looking at.
And remember, this resulting image
was only determined by the brain waves.
The AI had no idea what the person was actually looking at.
The resulting images could nail broad object categories,
but were unstable in the finer details.
META claims that their research
is ultimately to guide the development of AI systems
designed to learn and reason like humans.
We'll see about that.
Envisioning the future of brain-reading technology
is challenging, but the potential could be vast.
For instance, it could help those who are mentally conscious
but are unable to speak or communicate.
Thinking further afield,
imagine if a smartphone's volume, notifications,
and music selection would adapt to a user's mood or brain activity.
And smartphones and other devices could be commanded or queried
just by thoughts alone.
It would be like speaking to Google Assistant, but with your brain.
Next sense, which originated from Google's parent company,
Alphabet's X division,
has been working on something interesting since 2020.
The startup is currently developing earbuds
with the remarkable ability to capture human brain signals.
These earbuds have the potential to monitor and diagnose
brain conditions like epilepsy, depression, and sleep disorders.
They can also detect seizures, assess medication effectiveness,
improve deep sleep, and provide valuable insights
for comprehensive brain assessment.
Now, there's obviously a long way to go with brain scanning technology,
but it's clear that we're at the very start of a new era
of interpreting the human brain thanks to generative AI.
On the negative side, however, the sum of all of this technology
could be a privacy catastrophe.
Imagine Meta or Google being able to know what you're looking at
or what you're even thinking in order to better target their advertising
or to sway public opinion in one way or the other.
Who knows what this technology will evolve into in the next 30 years
in the very same era that Minority Report is set in?
With Meta getting involved, there are natural concerns for me.
Mind decoding technology in research is one thing,
but having popular wearable technology such as the MetaQuest VR headset
or video camera glasses could be a future temptation for the firm.
Meta were never known for their care of user privacy and consent.
Though still, to achieve true mind reading or telepathy,
the decoding technology would need to overcome a lot of challenges and limitations.
Accuracy and resolution must increase.
The system would need to understand tone, emotion, and contextual nuances of thoughts.
Second, it would need to work in both directions,
allowing a person to both send and receive thoughts from one person to another.
So true telepathy remains firmly rooted in fiction and perhaps that's for the best.
So back to the university researchers for a second.
The scientific world has been stunned.
The findings of the semantic decoder are undeniably remarkable.
Tonight we turn to news in one specific way that artificial intelligence technology
is changing our world and it is absolutely remarkable.
These neuroscientists at the University of Texas in Austin say they've made a major breakthrough.
They've figured out how to translate brain activity into words using artificial intelligence.
Dr. Alexander Hooth, the lead researcher, expressed his surprise to the Guardian saying,
quote, we were kind of shocked that it works as well as it does.
I've been working on this for 15 years, so it was shocking and exciting when it finally did work, end quote.
The breakthrough captured widespread attention from the media and scientists alike.
Professor Tim Behrens from the University of Oxford praised the study's technical prowess,
highlighting its potential to decipher thoughts and dreams and reveal new ideas from subconscious brain activity.
He said to the Guardian, quote, these generative models are letting you see what's in the brain at a new level.
It means that you can really read out something deep from the fMRI, end quote.
When you stop and think about it, an artificial brain interpreting the internal happenings of a human brain
makes total sense, but it's just something that I thought would be a long way off.
It's just my opinion, but I think that these AI technologies truly shine when they're applied to scientific research and studies like this.
That's when we get to harness the full power of artificial intelligence.
So there you have it, brain decoders that utilize the power of AI to interpret human thoughts
and generate coherent language and imagery.
Well, at least in a controlled environment for now.
The scanners are still bulky and expensive, and the training process is long and tiresome.
However, as with all technology, this is only going to get better.
Of course, there are concerns companies could misuse this for their own selfish gains,
but for the physically impaired, the possibilities are almost endless.
Personally, I found this quite fascinating.
But what are your thoughts on this? How do you guys think that this is going to shape the future?
Do you think that it's good or bad to be able to read or at least interpret someone's thoughts?
Or do you just think that this technology should just never see the light of day?
It's an interesting discussion, so feel free to discuss in the comments section below.
Alright, so that's about it from me. Thanks for watching.
If you want to see anything else on science, technology, business or history, feel free to subscribe to Cold Fusion.
It's free. My name is Tagogo, and you've been watching Cold Fusion,
and I'll catch you again soon for the next episode.
Cheers, guys. Have a good one.
Cold Fusion. It's me thinking.

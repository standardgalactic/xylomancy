This is a compliant machine called a mechanical neural network.
It's the first of its kind to successfully demonstrate the mysterious ability to learn
similar to biological brains.
Inspired by the mathematics of artificial neural networks, which enable most artificial intelligent
machine learning technologies today, this mechanical neural network paves the way for
a new kind of material that can physically learn its mechanical behaviors and properties.
Whereas the properties of most materials remain largely fixed and are a function of their
composition and microstructure, this new kind of learning material could get better and
better at exhibiting desired mechanical properties and behaviors, such as this shape morphing
behavior, when exposed to ever-increasing amounts of loading experiences.
If such materials are ever damaged or cut to form new shapes or sizes, they could not
only re-learn their original behaviors, but they could also learn new behaviors as desired.
The applications of such learning materials are endless.
Imagine training the wings of an aircraft so that they learn to optimally warp their
airfoil shape when subjected to unanticipated and changing wind-loading conditions so that
the aircraft improves its fuel efficiency and maneuverability with every flight.
Or imagine training the structural members of a building by shaking them in different
ways so that the building learns to remain stationary regardless of the kind of seismic
waves induced by an actual earthquake when it strikes.
Or imagine training body armor by repeatedly shooting it at different locations and from
different orientations so that the armor gets better and better at redirecting any projectiles
impacting shockwaves away from vital organs.
Instead of expending the immense amount of time and cost it currently takes to develop
a new material that achieves specific combinations of desired properties which are currently
not possible, such learning materials could simply be deployed without human understanding
and the material would autonomously learn to achieve those properties while also acquiring
that understanding for future designers to learn from.
So how would this kind of learning material work?
Well, it's important to first understand the source of its inspiration.
The idea for a mechanical neural network was inspired by a physical version of an artificial
neural network.
The mathematics underlying artificial neural networks are diagrammed using interconnected
lines that represent scalar weight values which are multiplied by input numbers that
are fed into multiple layers of neurons.
These neurons consist of activation functions that ultimately produce output numbers.
If the artificial neural network is provided with a set of known input and output numbers,
the network can be trained by tuning its weights over time so that it accurately predicts previously
unknown output numbers the result for any input numbers.
Here, different shades of blue represent different scalar weight values.
In this way, artificial neural networks can mathematically learn to model complex systems
that map many inputs to many outputs.
Similarly, mechanical neural networks possess physical interconnected tunable beams, shown
here as blue lines, which are mechanical analogues to the weight lines within artificial neural
network diagrams.
The beams connected nodes, shown as white circles outline black, which are analogous
to the neurons within artificial neural networks.
Whereas artificial neural networks tune their weights to match input numbers to output numbers,
the mechanical neural network proposed here tunes the axial stiffness values of its beams
to match input loads to desired output behaviors.
To demonstrate how the envisioned mechanical learning would work, suppose a shape morphing
behavior is desired for the following eight-layer deep triangular lattice consisting of eight
input nodes and eight output nodes.
Note that the black bars along the top and bottom edges of this lattice represent a grounded
body on which the nodes that touch it are pinned.
Suppose that when the input nodes along the left side are loaded by equivalent horizontal
forces, it is desired that the output nodes along the right side respond by displacing
to target locations along the contour of an undulating sinusoidal shape, shown as the
red curve.
To learn the shape morphing behavior, each tunable beam in the lattice would start the
learning process by setting their axial stiffness to a random value depicted here as a specific
shade of blue, according to this color scale.
When the input nodes on the left side of the lattice are loaded with the horizontal input
forces of the desired behavior, the resulting displacements of the output nodes would then
be measured and a mean-squared error, i.e. MSE, would be calculated by finding the average
of the scalar difference between the target output node displacements and these measured
output node displacements, i.e. their final location error, EI, for all n nodes on the
right side of the lattice squared, for this lattice, n equals eight output nodes.
The tunable beam elements would then change their axial stiffness values according to
an optimization algorithm, such that when the process of loading, measuring, and calculating
the mean-squared error is repeated, the mean-squared error would be minimized until a working
combination of beam stiffness values is identified that achieves the desired behavior.
One possible combination of beam stiffness values that enabled this mechanical neural
network to achieve the desired sinusoidal shape morphing behavior is shown here.
Suppose it is desired that the neural network then learns another new behavior in addition
to the first behavior.
Specifically, suppose it is desired that in addition to the lattice's output nodes displacing
to this sinusoidal shape in response to its input nodes being loaded with equivalent horizontal
forces, the same lattice's output nodes also displace to an inverted sinusoidal shape in
response to its input nodes being loaded by equivalent vertical input forces instead.
To learn the new behavior, shown green, while maintaining the ability to simultaneously
achieve the first behavior, shown red, the lattice of tunable beam elements could either
start with another random combination of stiffness values as shown here, or they could start
with the same combination of stiffness values that were found to successfully achieve the
first behavior only.
Parenthetically, the latter choice becomes increasingly favorable as the mechanical neural
network acquires more and more behaviors because the working combination of beam stiffness
values acts as a sort of muscle memory for previously learned behaviors.
Regardless of what starting combination of beam stiffness values are selected, however,
the input nodes would then be loaded with both the horizontal and then vertical forces
of the first and second behaviors respectively, and a single mean squared error would be calculated
that simultaneously considers the square of the output node final location errors of
both loading scenarios averaged together.
The tunable beam elements would then change their axial stiffness values according to
the same optimization algorithm such that when the process of loading, measuring, and calculating
the cumulative mean squared error of both behaviors is repeated, this new mean squared
error would be minimized until a working combination of beam stiffness values is identified that
successfully achieves the first and second behaviors simultaneously.
Note that the tunable beams all remain the same shade of blue regardless of whether
the lattice is actuated with the loads of the first or the second behaviors because the
same combination of beam stiffness values successfully achieves both behaviors.
It's also important to recognize that mechanical neural networks can achieve the same desired
set of behaviors using many different combinations of beam stiffness values.
Note, for instance, that although this second solution exhibits the same desired behaviors
as our first solution, it does so with an entirely different combination of beam stiffness values.
The fact that many different combinations of beam stiffness values can achieve the same
behaviors allows mechanical neural networks to learn more and more new behaviors while
retaining memory of previously learned behaviors.
Finally, it's also worth noting that mechanical neural networks are not limited to learning
shape morphing behaviors only, but can learn almost any combination of quasi-static, thermal,
and even dynamic mechanical behaviors including the control of wave propagation within their lattice.
To experimentally demonstrate the concept of a mechanical neural network, it was important
to first design a tunable beam that could achieve adjustable stiffness along its axis.
After comparing multiple concepts, we settled on this compliant design.
It consists of two parallel blade flexures that deform to guide the translational extension
and contraction of the beam along its axis while rigidly constraining all other directions.
A bracket is attached to the beam's housing in part to provide a hard stop so that the
parallel blade flexures are not allowed to deform to a point where they would yield,
i.e. be permanently damaged beyond their elastic limit.
The bracket is also attached to the magnet end of a voice coil actuator which is aligned
with the beam's central axis.
The actuator's other mating end, which consists of a coil of copper wire wrapped around a
wire drum, is attached to another bracket that is attached to the other side of the beam's housing.
Depending on the direction and magnitude of the current flowing through the wrapped wire,
a magnetic field can be induced by the coil that pushes or pulls on the voice coil's
magnet end, thus actuating the beam along its axis in either direction.
Two strain gauge sensors are mounted on either side of one of the parallel blade flexures
at its base to accurately measure the resulting displacement of the beam along its axis by
transforming the flexure's deformation strain into a proportional voltage signal.
In this way, closed loop control can be applied to actively tune the beam's axial stiffness
to achieve any value between an upper and a lower limit.
You could imagine that the highest axial stiffness would be achieved if when the beam is loaded,
the voice coil responds by resisting the load with the largest actuated force possible in
the opposite direction as the applied load.
Likewise, the lowest axial stiffness would be achieved if when the beam is loaded, the
voice coil responds by assisting the load with the largest actuated force possible along
the same direction as the applied load.
In this way, the tunable beams could be made to achieve zero or even negative stiffness.
The beam's housing inflectures were cut from an aluminum sheet using wire EDM and its brackets
were machined from aluminum L brackets.
We applied proportional and derivative closed loop control as detailed by this diagram to
achieve the desired stiffness control of the tunable beams.
An Instron testing machine was used to individually calibrate each beam by generating these four
plots to inform the controller as shown.
If this function is set to EK in the control loop and the proportional gain Kp is set to
a desired value, the resulting force displacement response of the actively controlled beam will
be linear and will possess an unchanging slope, i.e. stiffness, that is equal to the
proportional gain Kp value set.
This plot, measured using an Instron testing machine, shows the linear force displacement
responses of a tunable beam being controlled with different Kp values to achieve corresponding
positive and negative axial stiffness values.
The maximum and minimum stiffness values that the beam could be controlled to achieve without
becoming unstable was measured to be 2.3 Nm and negative 2 Nm respectively.
With a working tunable beam that could be controlled to achieve any desired axial stiffness between
its maximum and minimum stiffness values, 21 such tunable beams were fabricated and assembled
within a triangular configuration as shown by these blue lines to demonstrate learning
within a mechanical neural network.
More additional voice coil actuators were used in conjunction with decoupling flexures
to drive the two input nodes on the left side of the lattice with forces that can be made
to point in any in-plane direction desired.
Two cameras mounted on a frame directly measure the displacement of pins inserted at the center
of both output nodes and black felt is used to contrast the white color of the pin heads
so that they stand out.
This colored computer-generated image helps clarify other important features within the
mechanical neural network.
Note the purple colored rotational flexures centered around each of the network's nodes.
These flexures passively deform to accommodate the expansions and contractions of the tunable
beams as the network is loaded during learning.
Also note the green colored flexures that decouple the input actuators due to their cleverly
stacked arrangement.
Hard stops are built around all the flexures in the system to prevent them from yielding.
Although the machine's two mounted cameras can directly measure the lattice's output
node displacements, note that the strain gauge sensors on each beam can directly measure
the beam's extension and contraction, and that information can be used to indirectly
calculate the displacements of all the nodes in the mechanical neural network, including
the displacements of its output nodes.
This strain gauge approach to indirectly sensing the output node displacements can predict
the displacements with a much higher sampling rate compared to the frame rate of the cameras.
These plots show how accurately the strain gauge approach tracked the camera's measured
output node displacements when the lattice was loaded with a random combination of axial
stiffness values uploaded to each tunable beam in the lattice.
The strain gauge approach is also important to the functionality of mechanical neural
networks because without the approach, such networks cannot learn without being placed
in a testing rig, which is not practical for most applications that require in-field learning.
Moreover, the ability to accurately measure the displacements of all the nodes in the
network when it is subjected to unanticipated and changing ambient loading scenarios is
necessary for mechanical neural networks to be able to identify when those loads correspond
to the input forces of their desired behaviors being learned so that the network can then
calculate its mean squared error and minimize it as described previously.
Note that the input node forces can be indirectly calculated at any given time using the current
combination of beam stiffness values uploaded to the network at that time and the corresponding
strain gauge measured displacements of all the network's nodes that resulted from these
loading forces.
Our 21-beam mechanical neural network first demonstrated its ability to learn by learning
two behaviors simultaneously using the approach described previously.
For the first behavior shown exaggerated in red here, output node 1 should displace outward
0.5 mm, while output node 2 should displace inward 0.5 mm when the input nodes are loaded
with one Newton horizontal forces.
For the second behavior, shown exaggerated in green here, the output node 1 should displace
inward 0.5 mm, while output node 2 should displace outward 0.5 mm when the input nodes
are loaded with one Newton vertical forces.
The first optimization algorithm that we used to determine what combination of axial stiffness
values should be uploaded to each tunable beam in lattice during each step of the learning
approach was a genetic algorithm.
The algorithm samples 1,000 random beam stiffness combinations.
It then identifies and plots the combination that achieved the lowest resulting output
node displacement mean squared error.
A new, more promising group of 1,000 beam stiffness combinations is then generated by
crossing the most successful combinations attempted in the previous group.
The process is repeated until the mean squared error calculated stops changing from one
group to the next.
A plot showing how the algorithm reduced the mean squared error over time is shown here,
along with a video showing the mechanical neural network learning in real time.
This animation shows how both output nodes displaced progressively closer to their target
locations as improved beam stiffness combinations were identified from one group to the next.
The initial starting and ending locations of those output nodes are shown here without
the visual clutter of the path taken.
You can see that their final locations are almost directly on top of the target locations.
Once learning was successfully demonstrated in this way, using the genetic algorithm described
previously, we then conducted a study to compare the performance of five other optimization
algorithms to determine which algorithm is best suited for mechanical neural network
learning in general.
The five additional algorithms studied were full pattern search, partial pattern search,
interior point, sequential quadratic programming, and elder mean.
We compared how low the final mean squared error could be made using each algorithm,
i.e. how accurately the mechanical neural network could successfully learn its behaviors,
and how many iterations the algorithm required to achieve that final mean squared error,
i.e. how fast the mechanical neural network could learn its behaviors.
It was determined that Nelder Mead was the best suited algorithm for mechanical neural
networks due to the algorithm's practical learning speed, impressive learning accuracy,
and its insensitivity to system noise.
The details of that study were published in the Journal of Mechanical Design, and a link
to the paper is provided in the description below.
We were also interested to use the mechanical neural network to determine whether beams
that are tuned to exhibit nonlinear stiffness, i.e. stiffness that changes as the beams deform,
are favorable for learning compared to beams that are tuned to exhibit linear stiffness.
Our closed loop controller was designed to test this hypothesis.
If this f of ek function is changed from ek to a different function, like tangent ek,
then the resulting force displacement plot exhibited by the actively controlled beam
would be a nonlinear tangent function.
This plot shows the tunable beam's force displacement response, measured using an
instrument testing machine with f of ek equaling ek and tangent ek for different proportional
gain values, i.e. a kp of 1, 0, and negative 1.
We then trained the mechanical neural network to learn random shape morphing behaviors using
both linear and nonlinear tangent force displacement responses, and compared their mean squared
error versus time plots as shown here.
Much to our surprise, the plots suggest that tunable beams that achieve linear stiffness
can learn behaviors with greater accuracy, i.e. lower mean squared error, than tunable
beams that achieve nonlinear stiffness.
We then created a computational tool to simulate the behavior of our mechanical neural network
design so that we could use the tool to predict how well larger versions of the same design
would learn if we had the time and resources to build and incorporate many more tunable
beams within its lattice, as depicted by this photoshopped image of a much larger lattice.
Our computational tool models the tunable beams as linear beams, which are depicted
as blue lines, and their lengths are set to be the length of the beams in our fabricated
mechanical neural network, i.e. 6 inches from node to node.
We restricted each beam in our simulation to only achieve axial stiffness values between
the maximum and minimum stiffness values measured from our fabricated beam, i.e. 2.3 newtons
per millimeter and negative 2 newtons per millimeter respectively, and we set their passive
nonaxial stiffness values equal to the values calculated using finite element analysis as
shown here.
We also restricted the simulated beams from extending or contracting more than plus and
minus 2.5 millimeters, which is the limit of our fabricated beams as governed by their
hard stops.
Finite element analysis was used to validate the computational tool's accuracy by loading
a 21 beam version of the design in its passive state, i.e. without any closed loop stiffness
control activated, with 25 random force combinations imparted on its two input nodes.
The X and Y components of the lattice's resulting output node displacements, calculated using
both finite element analysis and our computational tool, are plotted here showing good correspondence
between each of the 25 force combinations.
Once configured to mimic our fabricated mechanical neural network, the computational tool was
then used to simulate the effect that the number of layers would have on the ability
for a triangularly configured mechanical neural network consisting of eight input and output
nodes to learn different numbers of random shape morphing behaviors.
The results of the study indicate, one, that mechanical learning improves with more layers
likely because there are more tunable beams with which to learn, and two, the more random
behaviors that are required to be learned, the less accurately all the behaviors can
be learned simultaneously.
This plot was similarly generated, but for only two, four, and eight layers, and for
both triangular and square lattice configurations shown green and red respectively.
It is clear from these results that triangular lattice configurations can, in general, learn
different numbers of shape morphing behaviors more effectively than square lattice configurations.
The reason is likely because triangular lattices have more beams for the same number of layers
and they can propagate displacements in all directions rather than just along orthogonal
directions as is the case with square lattices.
To learn the effect that the number of layers and output nodes have on mechanical learning,
we used our computational tool to generate the following plot for triangular lattices
that learn the two sinusoidal behaviors described previously.
The plot indicates that once the lattice possesses two or more layers, the number of output nodes
does not seem to matter.
It's true that the more output nodes a lattice has, the more output node displacement requirements
the output nodes must satisfy.
But it's also true that the more output nodes a lattice has, the more beams the lattice
can employ to satisfy those requirements during learning, so both effects seem to negate
each other.
If you'd like to learn more about the details presented in this video, I encourage you to
read our first published journal article on the topic of mechanical neural networks in
science robotics where our work was featured on the journal's front cover.
A link to the paper is provided in the description below, along with a link to my Thingiverse
account where you can download the part files necessary to fabricate our mechanical neural
network.
Finally, I want to thank my students Ryan Lee, who built and tested the mechanical neural
network, Erwin Mulder, who developed our computational tool, P. H. R. Sainaghi, who helped perform
the optimization algorithm comparison study, and all the other students who contributed
in smaller ways to the success of this project.
I am especially grateful to my AFOSR program manager, Les Lee, for making this research
possible through his continued funding and generous support of my group.
If you'd like to support my channel, I've provided instructions in the description below.
Thanks for watching The Facts of Mechanical Design.
I'll see you in the next one.

Hey everyone, it's time for more tricky parts of calculus.
This episode is about the history of the functions e to the x and log x.
In the last episode, I introduced the exponential function and the logarithm and gave the best
ways to rigorously establish their properties.
There was a lot of great math in that episode, check it out if you haven't seen it, but
a lot of it was from Euler's treatment of these functions from 1748, and some of the
arguments were even from the 19th century.
What I didn't discuss was how people came up with these functions in the first place.
The short answer is that in the reverse of the way these functions are typically introduced
to students today, the natural logarithm preceded the standard exponential function.
We can date the natural logarithm precisely to 1614 in the modern era when it was introduced
by a Scotsman named John Napier.
I want to explain what Napier was trying to do and how he worked out the properties of
the logarithm, but I also want to discuss how people might have developed the theory
of these functions and see if history can tell us why this theory was developed when
it was and not at some other time.
It's really a fascinating story.
So I'll start with this puzzling observation.
Trigonometry is very old, dating back to the time when civilizations first needed enough
of a grasp of astronomy to work out a regular solar calendar about 4,000 years ago.
But although the exponential function is newer, the most common application of exponentiation,
compound interest, is older than trigonometry.
In fact, it's much, much older.
There's evidence of primitive banks in the very first cities with agriculture going back
about 10,000 years to the period called the Neolithic.
These were cities like Jericho, Tel Caramel, and Katel Hoiak.
Here's a map of cities from this period.
Banking predates money to times when banks lent and stored seeds, animals, and grain.
Banking predates writing.
In fact, there's almost certainly banking which provided a need for symbols for goods,
quantities, names, and dates.
We know that the earliest banks issued clay tokens representing quantities of stuff.
We have examples of these tokens with writing from about 3,000 BC.
Did these early banks charge compound interest on loans or give interest to depositors?
That's unclear, though in the kinds of circumstances we're familiar with, compound interest is needed
to provide the right incentives for all parties, so probably at some point they must have.
The earliest written record of compound interest that we have is an incredible artifact.
It's actually a cone made of clay with a cuneiform inscription.
Today it's in the Yale Babylonian Collection, dating from about 2,400 BC.
That makes it about 600 years older than the Plympton 322 tablet on the banner of my channel
that contains the earliest trig table.
It's also our oldest record of a long war with border disputes and diplomatic alliances.
It records the victory of Enmetena of Lagash over the rival city of Umma, two cities on
the Tigris River in what is today southern Iraq.
Apparently, Umma had taken from Lagash a valuable strip of fertile land some two generations
before.
The army of Lagash killed the leader of Umma, and Enmetena demanded that his conquered foes
repay the value of the land they had taken with interest.
The land produced one guru of barley in their units, so at the rather high rate of one-third
or 33 and a third percent interest compounded annually for 55 years and a few months, he
arrived at the round figure of 8.64 million guru.
Here you can see what the writing looks like.
I'm fairly sure that here in the fourth column it says one guru in this box, and down in
this box these circles indicate the really big number of guru.
Some sources translate these circles as 40 times 60 squared, which is 144,000 instead
of 8.64 million, which is 40 times 60 cubed, but the larger figure is more consistent with
the timeline and known interest rate.
In any case, 8.64 million guru converted to our units is 128 billion bushels of barley
of almost 800 times what the United States produces in a year today.
Of course the debt was not repaid, as it might have taken them millions of years to do so.
What happened was that Umma eventually rose up and conquered Lagash instead, killing Enmetena's
son, and the feud might have gone on and on, except all the Sumerian cities were conquered
by Sargon the Great of Akkad in the 2330s.
Maybe what this example shows is that people did not like compound interest.
Compound interest was the subject of endless political battles across time and civilizations.
Often it was outlawed.
Inscriptions tell us that the Bronze Age states of Egypt, Babylon, and the Hittites
had state mandated insurance rates.
But apparently compound interest was bad for the little guy.
If you're wondering like I was, why didn't regular people benefit from compound interest
on their savings?
The answer seems to be a combination that often banks served only the very wealthy,
and that at various times instead of offering interest, banks charged depositors a fraction
of their money just to hold it, possibly because the world was so precarious.
There's evidence of a 160th charge in Babylon.
So compound interest only favored creditors.
All the western monotheistic religions from the periods that followed had injunctions
against usury, usually defined as lending with interest, though sometimes interpreted
as lending at too high a rate.
And of course, compound interest was especially evil, as it was thought of as interest on interest.
Ancient Rome had an extensive banking and credit system with interest rates set by the
state, but it was simple interest, not compound interest, which was outlawed, and the rates
seemed to have been set to be easy to calculate, not based on economics.
Rome's collapse made banks disappear for hundreds of years, but banks sprang up again
in Italy as early as the time of the Crusades, and they usually found ways around the laws
against usury.
But compound interest was only made legal in England in 1545, and then revoked and then
allowed again, so that's why we find the first book on the mathematics of compound
interest, Richard Witt's Arithmetical Questions, appearing in 1613.
It contains tables very similar to the logarithmic tables I'm going to talk about by Napier and
Burgey from about the same time, completely independently.
But the issue of compounding more and more frequently wasn't asked, and only seems to
have been brought up in 1683 by Jacob Bernoulli, as mentioned in the previous episode.
As an aside, the ancient economy and ancient banking are huge and fascinating topics I
really would like to know more about as a mathematician.
There are so many problems people in the past had to solve that we solve with math, so I
wonder if they knew more math than we'd give them credit for, or if they had some completely
different way of doing business.
How did they price insurance?
Did they keep statistics?
That sort of thing.
I'd love if anyone can point me in the right direction.
So I think it's also worth noting, exponential growth is part of the cultural lexicon today,
but it certainly wasn't in history.
Some of our best examples of exponential growth, like population growth, or growth in GDP,
or progress in technology, exemplified by something like Moore's Law, which observed
that the number of transistors in an integrated circuit tended to double about every two years.
Probably were not features of human society prior to the Industrial Revolution.
Here's a graph of the human population over time.
The average rate of growth from 10,000 BC to 1700 was 0.04 percent, an imperceptible
rate even if they had kept records, which they did not.
I suspect that, to the extent anyone thought about it, life appeared more cyclical than
progressive.
There were periods of growth and periods of stagnation and death.
This attitude was explicit in the writings of ancient Egypt.
By the way, the population is not growing exponentially now.
Growth has actually slowed to being approximately linear, albeit with large slope.
Of course, also no one observed the growth of bacteria until the invention of the microscope
in the 1670s, and radioactive decay is even later.
What about physical phenomena involving exponentials, like the phenomenon of terminal velocity,
whereby a falling object encountering air resistance proportional to its velocity converges
exponentially to a limiting speed?
Both the mathematics and the experimental apparatus to observe such a phenomenon were
out of reach at the time of Napier's discovery.
Galileo at that time was just realizing that falling objects covered a distance growing
quadratically in time, which he confirmed by setting flags along a track at square intervals,
and as a ball rolled down, it made a steady rhythm of ticks.
With the parabola for the basic model for the path of projectiles, eventually people
observed that cannons, that is, once the cannons were powerful enough for drag to be significant,
did not shoot as far as predicted, which required a theory of drag.
Newton considered the equation of an object whose motion is resisted by a drag force proportional
to its velocity in the Principia, published in 1687.
At the start of book two, he correctly derives the geometric decay and gives an account of
terminal velocity.
Here's an even simpler equation of drag without the constant force of gravity, x double dot
equals minus kx dot, or v dot equals minus kv.
Today we would solve this differential equation using the fundamental theorem of calculus
and our knowledge of the logarithm and exponential.
Newton's account is very different.
In his view, the equation v dot equals minus kv means that in each of any number of very
small equal increments of time, the change in velocity is proportional to the instantaneous
velocity.
Since you can show that if a sequence of values a, b, c, d is in constant proportion
to their differences, they are proportional to each other, the velocity must obey a geometric
progression in equal intervals of time.
This is actually the same argument that Napier gave 70 years earlier when introducing the
logarithm.
But at no point does Newton use terminology or notation for the exponential or the logarithm.
He does mention how the velocity and time are related to the lengths on the x-axis and
areas under the graph of a hyperbola defined by those lengths, as discovered by St. Vincent
in the 1640s, more on him later.
In 1701, Newton posited a similar equation in connection with his law of cooling.
So these investigations would have led to the exponential, but they required the development
of calculus to be further along, and it's worth noting something that the ancients observed
that the motion of the stars was more regular and perfectly circular than anything observed
on Earth, hence the greater attention to and progress in astronomy before the advent of
Newton's laws, though that's not how physics is taught today.
There's another field of investigation that might have led to the exponential in the 16th
century, gambling.
Gambling was always around, but it received a major boon from the ability to manufacture
more regular dice and the invention of the printing press for playing cards.
Many of the top mathematicians of the early modern period were gamblers, Cardano, Fermat,
Pascal, Demouave.
Here's a popular problem in gambling circles from the 1500s or even earlier.
One bets his friend 100 duckets that he will throw aces, or snake eyes, on two dice in
some number of attempts.
What number of attempts makes this a fair bet?
If p is the chance of rolling two ones, one in 36, and the chance of failure is q equals
1 minus p equals 35 over 36, then people realize that the fair number of tries n was when q
to the n equals one-half, or as close to it as one could get with a whole number n.
Today we'd throw into a calculator n equals minus log 2 over log q, but they didn't have
calculators or logs yet.
People did compute tables and use some laws of exponents to approximate the right number
of tries.
Here, the thrower has the advantage for the first time at 25 tries.
Here's a way to approximate the right number more easily.
Set p over q equals 1 over r, so the equation we have to solve is q to the minus n, which
is 1 plus 1 over r to the n, equals 2.
Suppose we approximate the expression involving r and n by looking in the limit as p goes
to zero, so r goes to infinity and n goes to infinity so that the ratio n over r is
fixed to be some value, say m.
Then by carefully examining the limit of the binomial coefficients, you get the power series
we recognize as the exponential, the sum of m to the k over k factorial.
If this analysis had been done by Cardano, it might have been the starting point for
the exponential, but instead it waited until DuMhoff in 1711, who referred to this result
as the number whose hyperbolic logarithm is m, because logarithms have been around almost
100 years, but it was before Euler's work on the exponential function.
In any case, you get the solution by taking hyperbolic logarithms, m equals log 2, or n
equals log 2 times q over p, which is about 24.3 in this case, which is good enough to
find the right number of tries.
What about how many tries before it's a fair bet to throw snake eyes twice, or three times,
for c successes?
You can set the failure probability that of getting 0, 1, 2, or up to c minus 1 successes
to 1 half.
Again, you can take limits to get a transcendental equation for m involving the exponential function
and the truncation of its power series.
This was also done by DuMhoff in the same work, essentially finding that the limit of
the binomial distribution when the success rate goes to 0, but the tries increase so
that np over q is constant, is what we call the Poisson distribution.
This result is named for Poisson, who would not even be born for another 70 years.
That's the first appearance of exponentials in probability.
DuMhoff also established the convergence of the binomial in the case of trials going to
infinity with fixed success rate to the normal distribution e to the minus x squared, maybe
a later episode on this.
Also exponentials come up if you analyze the so-called martingale betting strategy, I'll
link below to a great recent number file video on this, but it seems this strategy was only
discussed at the end of the 18th century.
So here's another particularly pressing problem from the 1500s that really required calculus
and logarithms.
Petrus' voyage began an era of sailing across oceans.
It might be shocking to realize that nearly all voyages up to that point were made not
far out of sight of land, even when traveling in the Mediterranean.
The reason is it's hard to know where you are, and especially so when the distance is
so far that the curvature of the earth makes a big difference.
The mathematician Petrus Nonius was the first to point out in the 1530s that if you travel
in a ship at a constant cardinal direction, or room line, it won't follow a great circle,
but it will make a constant angle with each meridian, resulting in a spiral he called
the loxodrome.
An analytical expression for the loxodrome was beyond the mathematics of the time, but
that didn't stop Mercator from practically dealing with the problem in the 1560s by projecting
the sphere onto a cylinder from its center, a transformation that takes loxodromes to
straight lines, and then by uncurling the cylinder and cutting off at extreme latitudes,
you get his famous map, which is still the most popular way to draw a world map today.
The key mathematical question is, if you travel in a given distance along a room line, starting
from some latitude and longitude, what will be your final latitude and longitude?
For short distances, the change in latitude is just the distance times cosine alpha, where
alpha is the cardinal direction.
The change in longitude is more difficult because each circle of latitude has circumference
that of the equator multiplied by the cosine of the latitude, so the same distance east
or west will cover more degrees of longitude the farther from the equator.
This non-constancy makes finding the change in longitude difficult, that's the problem
of meridional parts.
The determination of the change in longitude ultimately requires an integration of the
secant of the latitude phi.
This is one of these elementary but difficult integrals requiring a very clever substitution
to see that it's an area under hyperbola with anti-derivative log of tangent of phi
over 2 plus pi over 4.
It's really remarkable that in an era before calculus, the mathematician Edward Wright
in 1599 published certain errors in navigation containing tables for calculating the meridional
parts made by taking what we today call Riemann sums of secants.
It was only noticed in 1645 that Wright's table was the same as a table of logs of tangents
obtained from Napier's work, but it was a complete mystery why.
James Gregory was finally able to compute the integral of the secant analytically in
1668 by a clever substitution and making use of the then available knowledge of the integral
of the hyperbola 1 over x.
In the era before Newton, Fermat had been able to work out in the 1630s formulas for
the areas under all power law curves except x to the minus 1, which defeated him.
It fell to Grégoire de Saint Vincent in 1647 to show that the area under the hyperbola
from A to AR is the same as the area from AR to AR squared, essentially by the technique
that I described in method 5 of the previous episode.
That implies that if you take A of x to be the area under the curve from 1 to x, then
A satisfies the multiplicative additive property A of x, y equals A of x plus A of y.
Saint Vincent's student, DeSerasa, who must have been familiar with Napier's logarithm,
pointed out that the function A was therefore Napier's logarithm up to a multiplicative
constant.
It was clear numerically that it was the same as the improved version of Napier's
logarithm, which was 0 at 1 and had derivative 1 over x, which gave a strong indication of
the yet unproved fundamental theorem of calculus.
Saint Vincent might have received more credit for such an important discovery had he not
in the same book published a construction of the squaring of the circle by ruler and compass,
of course incorrect.
So those are all the directions which could have led people to the exponential function
in the logarithm, but now let's discuss how it actually happened.
John Napier did not set out to define a new function, but to provide astronomers and
navigators with a useful table to calculate with 7 decimal digits of accuracy.
It's hard to imagine, but back then there were no calculators, and when in the 1500s
people wanted to do calculations quickly, they relied on lookups to extensive tables.
Remember that multiplication is computationally harder than addition.
Consider this method of multiplying two numbers.
Use the identity AB equals 1 fourth A plus B squared minus 1 fourth A minus B squared
To multiply A times B then, just look up the values at A plus B and A minus B in a table
that you've made of values of 1 fourth X squared.
This method goes back once again to the Babylonians and even these numbers were once called tables
of logarithms.
Alternately, because astronomers and engineers made frequent reference to trig tables, see
my episode 4, they could use sines and cosines for quick arithmetic.
Using the angle sum identities and shifting terms around gives, for example, sine A times
sine B equals 1 half cosine A minus B minus 1 half cosine A plus B.
To multiply, for example, 0.68 by 0.43, find the angles in the first quadrant corresponding
to these sine values in the table and then subtract the relevant cosines.
Using the angle sum identities like this to multiply is called plus the phoresis, and
basically nobody does it anymore.
By about 1600, the mathematician Jost Berge came upon the idea of making a more versatile
table like this.
Suppose you want to do arithmetic with four digits of accuracy.
Berge began with the number 10,000 or 10 to the fourth, and then at each step he shifts
the number he has over four places and adds it to get the next number and repeats over
and over, keeping only an extra four places you can think about why.
As he continued painstakingly until he reached the next power of 10 after 23,027 steps, I'm
going to make a modern adjustment and move the decimal place over so that the table begins
at 1 and not 10 to the fourth.
This table is a geometric progression by the factor 1 plus 1 over 10 to the fourth, same
as a table of compound interest if the interest rate were 0.01 percent.
Actually, just to get a feel for this process, I attempted to make my own geometric progression
table with a much more manageable rate of 1.01 or 1 percent by shifting by two places
and adding over and over.
I made it to about 2.01 in 70 steps, which is correct, which is where I stopped.
Actually, I found that I had a typo at step nine, which makes all the later digits incorrect,
but it's still very close.
Getting to 10 would just require a bit more than triple the time and a bit more care than
I put into it.
As getting back to Berge's four digit table, here's how to use it.
If you want to find, say, the cubed root of 3.6, you find the index of the entry closest
to 3.6, which is 12,810, and the entry at 4,270 or one-third of 12,810 is very close
to the root.
If you want the cube root of 36,000, you have to adjust for powers of 10.
36,000 is 3.6 times 10 to the fourth.
You can get 10 to the fourth-thirds as 10 times 10 to the one-third, so you take one-third
of 23,027, add it to 4,270, that's between 11,945 and 11,946, giving 3.3017, so 33.017
cubed is about 36,000.
I give this example to clarify the exposition in auto-toplates' The Calculus, A Genetic
Approach, a great book where I learned about a lot of Napier's work.
But already for the scientific work of the late 1500s, four places of accuracy wasn't
enough, and a few people like Berge and John Napier set out to make the analogous table
with seven places of accuracy.
The trouble is that this takes a long time.
Taking powers of 1 plus 1 over 10 to the seventh, it will take over 23 million steps
to reach 10.
So Napier was forced to take a step back and develop some theory.
What I'm going to do here is follow the essence of Napier's thinking, minus the particular
details of his version of the logarithm, which would confuse the issue.
First of all, he figured out that you could make tables based on different geometric progressions
of rates A and B, and then you could recover the index of a number in the base B if you
knew the index in base A and the number of indices alpha and beta to reach 10 in both
tables by the laws of exponents.
So it didn't really matter what base was used.
Then if you're free to consider any base, the key property of the index, or step function,
is that it converts any geometric progression to an arithmetic one.
That is, for any rho, f of x, f of rho x, f of rho squared x, etc., all form an arithmetic
progression, or f of rho x equals f of x plus k, a constant depending only on rho, for
any x.
But then f of rho equals f of 1 plus k, but f of 1 equals 0, since that's where we start
the table.
So k of rho equals f of rho, and therefore, for any x and y, f of x, y equals f of x plus
f of y.
This again is the multiplicative additive property I mentioned in episode 5 in the first
instance in which it appeared historically.
If you compute the derivative of such a function, which is a notion Napier knew decades ahead
of his time, not by that name, but which he understood as a velocity, using modern notation
the difference quotient limit is the limit of 1 over x times f of x plus h over x divided
by x plus h over x minus 1.
That's 1 over x times an expression that doesn't actually depend on x, just the ratio
y equals x plus h over x.
So as y goes to 1, the limit, if it exists, is just some constant c independent of x.
By declaring his interest in the particular function for which the constant c was equal
to 1, Napier introduced the natural logarithm.
So that's how Napier arrived at a natural logarithm, but what did he use it for?
His goal was to make a trig table with logarithms of sines or cosines, so that trig functions
could be multiplied quickly with seven-digit accuracy, more on why that was important later.
Napier's table had an entry for every minute of arc between 0 and 90 degrees, or 5400 entries.
There's that factor of 60 again that we inherited from the Babylonians.
Note that this is much more practical than making a table with 23 million entries.
Here's how we could make a table like Napier's.
First make tables of geometric progressions for reference values.
Start at 1 and multiply by 1 plus 1 over 10 to the 7th over and over, but stop at a round
value, say after 100 terms when you reach 1.0001495 and so on.
That's very close to 1 plus 1 over 10 to the 5th.
We're not going to forget that it's not exactly 1 plus 1 over 10 to the 5th, we'll account
for the difference later, but for now start a second table with 1 plus 1 over 10 to the
5th as the common ratio and go until another round number.
Napier went 50 steps to 1.0005001225196, again cutting off after the 14th place.
Now you have to make another table with common ratio 1 plus 1 over 2000, where now you have
to shift four places and multiply by five at each step.
This will take you in 20 steps to something close to 1.01 and from there it's a practical
232 steps to 10.
Then you can fill in your course 1.01 rate progression by making a table that starts
at the power of 1.01 and increases by the next 20 powers of 1.0005, which will give
you 4,640 reference values.
To compute logarithms of the entries in these tables, it suffices since they're in geometric
progression to get the logarithms of the first two terms, since the logarithms will
be an arithmetic progression.
To compute the natural logarithm of the first progression, 1 plus 1 over 10 to the 7th,
Napier needed another ingredient, a key inequality that allows you to approximate the logarithm,
and one form of this inequality looks like log x is less than x minus 1 and greater than
x minus 1 over x.
Now I already proved this inequality in a slightly disguised form in method five of
the previous episode, but here's another proof that Napier would have recognized.
The graph of his function log x lies below x minus 1 and above x minus 1 over x, since
all three functions are zero at one, and their derivatives are 1, 1 over x, and 1 over x
squared, so it's clear which ones grow faster, though to make this rigorous you might invoke
the mean value theorem.
A second version of this inequality comes from setting x equals a over b with a greater
than b, this can be written, 1 over a is less than log a minus log b over a minus b is less
than 1 over b.
Of course these inequalities are only useful if x is close to 1 or if a is close to b.
With this inequality, you get that log of 1 plus 1 over 10 to the 7th is between 1 over
10 to the 7th and 1 over 10 to the 7th plus 1, which led Napier to simply take an
intermediate value, like the middle value, point 0, 0, 0, 0, 0, 0, 9, 9, 9, 9, 9, 9, 5.
Now you have the logs of all the values in the first reference table, and you can use
the inequality to compare the log of 1 plus 1 over 10 to the 5th to the log of the last
entry in the first table, which is very close to 1 plus 1 over 10 to the 5th, so you get
an arithmetic progression of all the logs in the second table and you do the same for
all the other tables.
Now it's just a matter of taking an existing accurate trig table, finding the closest reference
values in the table to the sine value, and approximating the logarithm with the inequality.
For example, here's how to find the log of sine of 48 degrees in 30 minutes.
Note that I interchanged the notation ln of x with log of x. Contemporary available
trig tables gave the sine as 0.7489557.
The closest power of 1.01 is 1.01 to the 202.
Note that we have to adjust by a factor of 10 to locate the value in our table.
Then you can go to the 200 second finer reference table and multiply by an additional 7 powers
of 1.0005.
Now use the inequality to get a small range, approximate by the middle value, and subtract
the log of 10, which is the final value in the whole table of reference values.
Here I've tried to capture the spirit of what Napier was trying to do, but the details
of his particular construction were different in several ways.
He used a decreasing geometric progression rather than an increasing one.
He took 10 to the 7th to be the radius of the circle and considered the sines as lengths
working downwards with a logarithm set to 0 at 10 to the 7th, so in terms of our modern
natural logarithm, Napier's function was Napier log of x equals 10 to the 7th log of
10 to the 7th over x.
Also, his analysis was based on the differential equation of exponential decay for the values
as the indices progressed arithmetically, just like Newton's discussion of drag force.
I'll include references in the description if you want a fuller picture of Napier's actual
process.
Here, by the way, is a page from his table.
Napier's table of logarithms was published in 1614 and was immediately well received.
Kepler might not have been able to devise his laws of planetary motion without Napier's
tables and their subsequent improvements.
Napier was prevailed upon by Henry Briggs to make new tables with the logarithm to base
10 to be of more practical use, so although Napier had given a construction of the natural
logarithm, the details obscured the role of the base E.
In a sense, Christian Huygens isolated E when he computed several digits of its base
10 logarithm in the 1660s, but a full description of E, its connections to logarithms and continuously
compounded interest, had to wait until Euler.
Napier died in 1617 and the details of how he constructed his logarithms were published
posthumously in 1619.
Incredibly, it only took until 1622 for William Autrid to take Napier's tables and construct
the first slide rule, a ruler marked with a logarithmic scale and moving slide to read
off multiplied values by adding lengths.
This was the most widespread tool for numerical calculation until the personal electronic calculator,
and now hardly anyone knows what a slide rule even is.
Lastly, I want to address why Napier wanted to make a table of logs of trig functions
in the first place.
It has to do with using the spherical law of cosines.
The problems of astronomy and navigation require not plane trigonometry, but spherical trigonometry.
Spherical triangles are formed by the arcs of three great circles and have interior angles
marked with capital letters and side lengths themselves angles subtended by the arcs given
by lowercase letters.
In the 16th century, the spherical law of cosines was better known than the planar law
of cosines.
This law and another version for angles instead of arcs involved multiplying signs and cosines,
hence the need for the table.
These formulas were crucial for navigation by the positions of the stars, including
telling the time or longitude by the hour angle and to track the motion of the planets.
Maybe I'll discuss this in a future episode.
Thanks so much for watching.
If you like this content, please hit the thumbs up, subscribe, it's also a good idea to hit
the notification bell since my uploads are a bit sporadic.
It will really help me to grow this channel.
Check out my other videos in the Tricky Parts of Calculus series for subtle parts of calculus
that are rarely covered, including difficult proofs on how problems were solved for the
first time.
I also do a podcast and videos with general advice and opinions about the world of math.
Thanks again.

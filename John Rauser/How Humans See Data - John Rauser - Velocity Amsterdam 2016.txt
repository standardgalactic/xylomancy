Yeah, so I'm John Rouser, I'm here to talk to you today about how humans see data.
So this is a talk about visualization, which is taking raw data, numbers, and labels, and
turning it into images for the purposes of communication.
So that is a super important idea, so I will say it again.
The act of creating a visualization is fundamentally an act of communication.
Your goal with a visualization is to make new ideas appear in the heads of other people.
More specifically, this is a talk about how to make better visualizations.
I want to make you better at helping humans solve analytical problems quickly and accurately
with visualization.
I want to make you more effective at getting ideas into other people's heads.
I feel like I should start by justifying visualization as a technique in the first place.
Some people say, well, you don't really need visualizations because you can communicate
just a tremendous amount of information using tables of data like the stock listings page.
So we'll do a little exercise.
Here's some data, 24 points each with an X and a Y value.
You are a very clever audience, so I'm certain that if I let you work at it for just a little
bit, you'd figure out what the relationship is between X and Y, but if I just show you
this, it's immediately obvious.
So what just happened in your head happened in about 250 milliseconds without any conscious
effort.
In fact, there is no way that you can stop it from happening.
Your eyes are a direct high bandwidth channel directly into your brain.
You recognize this as a circle because of what's called pre-attentive processing, which
is the subconscious accumulation of information from the environment.
And the best visualizations, they make the information that you want to communicate available
pre-attentively so that the user doesn't even have to think about what they are seeing.
That's not always possible, but it's what you should be striving for.
And the point, really, of this little exercise is that a graph is an encoding of data, which
is to say that this table and this chart contain the exact same information, but in this display,
the information is encoded in a way that makes it accessible to your incredibly powerful
visual system.
But not all encodings are equally good.
So here I've mapped the X value to position on the horizontal axis and the Y value to
position on the vertical axis.
That choice has a name.
This is called a scatter plot.
But I could have made different choices.
I could have taken the exact same data and treated it as an ordered sequence and then
plotted two lines like this.
So this is the same data, but now it's not at all obvious what the relationship is.
So this is akin to a time series plot, which is ubiquitous, of course, in operational dashboards.
The time series plot is incredibly useful if time is critical to the analysis, which
may or may not be the case.
It's certainly not the case here.
So a good reason to visualize is that it can make salient features of the data available
pre-attentively without any effort at all.
Good visualizations optimize for the human visual system.
If that's true, then it seems like it'd be really good to know how the human visual system
works or more precisely how the human visual system decodes a graph.
The good news is that there's been a lot of research into this question.
And what I want to do now is I want to walk you through some of that research using a
framework that comes from this book.
So Bill Cleveland did some of the most important original research in this area, and this book
summarizes that research into a coherent framework.
One of the most important parts of that framework is what he calls the three visual operations
of pattern perception.
So I'll define them.
First detection.
Detection is the visual recognition that a geometric object encodes a physical value.
Assembly then is the grouping of detected graphical elements.
And estimation is the visual assessment of the relative magnitude of two or more quantitative
physical values.
So I'm going to go through these, but I want to go through them in reverse order.
I want to start with estimation, because that's where I see people making the most mistakes.
So Cleveland lays out three different levels of estimation, where one level down in sort
of this hierarchy of concepts.
So we're going to talk about three different levels of estimation.
The first is discrimination, where all you can do is just tell that two values are or
aren't the same.
Next is ranking, where you can impose some order on the values.
And then the last one is ratioing, where you can say with some precision what the ratio
between two values is.
Notice that all of these involve comparison.
Efficient comparison between different data points is nearly always the point of a visualization.
If I just wanted to report the individual data without comparison or any further analysis,
then a simple table would be completely fine or maybe better just like a USB key with the
data.
But when you're trying to think with data, you are always making comparisons.
As Edward Tufty says, a famous writer about information visualization, he says that the
heart of quantitative reasoning is a single question as compared to what.
So when we're making a visualization, what we want to do is to get as far down this list
as possible with efficiency and with accuracy.
Some of the best research on this question is summarized in this amazing paper by Cleveland
and McGill.
You can get this online for free.
This paper is a treasure trove of insight, but I want to focus on one little table.
I see people getting out their phones, they want to take a photo.
I'm going to tweet the slides afterwards, so don't worry about it.
Sorry.
Look, there it is.
You want to take a photo?
Go.
Awesome.
So I want to focus on this one little table buried on the third page of the report because
I want to claim that this table might be the single most important thing that you can possibly
know about visualizing data.
What this table shows is how accurate humans are at estimating quantities that are encoded
in different ways.
So here's the table in a more readable format.
What Cleveland is doing here is he's giving us seven different ways to encode a quantitative
value and he ranks them from most effective to least effective.
We are most accurate when decoding position on a common scale and least accurate when
decoding hue.
And what you should do when you're making a graphic is think, what is the most important
comparison that I want the user to make?
Whatever that is, you should encode that thing using position on a common scale.
As you pack more and more different dimensions of data into the same chart, you have to go
further down this list, you're just forced to.
If you have several different comparisons, all of which are very important and you might
have to make multiple charts.
What I want to do now is I want to take you through these in order, in order to give you
an intuitive sense for how they work.
So I'll start with hue and we'll work our way up.
Before I can even get started, I need to talk about how color works because human color
perception is incredibly complicated.
As Tamara Munzner says, the first rule of color is do not talk about color because color
is not a single thing.
Color is really three things.
There are three channels that are encoded in any one color.
The first is luminance, which is roughly how bright a color is.
Contrast and luminance is really important for edge detection in complex scenes.
The next is saturation, which is roughly how colorful the color is.
Low saturation is like a muted pink, high saturation is a bright red.
And finally, hue, which is what we mean when we say color in common speech.
So here's a chart that encodes information using hue.
This is a data set of 32 cars that were tested in a 1974 issue of Motor Trend Magazine.
What I've plotted here is the fuel efficiency in miles per gallon or MPG, so a higher number
is better, more miles per gallon.
So let me say up front that this is a terrible plot.
You would never make a plot like this in real life, but let's try to extract some value
from it.
So the first of Cleveland's estimation tasks is discrimination.
So what do you think of these two, the Firebird versus the Merck 450 SLC, are they the same
or different?
Different.
Very easy, right?
Because they're right next to each other.
What if I make it harder?
The Merck 450 SLC versus the Dodge Challenger, same or different?
Okay, I find it impossible to tell.
People vary in their ability to perceive differences in hue, so I can't really tell.
Maybe you can tell if you can, you're better at this than me.
I think they're actually different, very, very slightly different.
Okay, so much for discrimination.
What about ranking?
Here's an easy one.
Toyota Corolla versus the Chrysler Imperial, which has better fuel efficiency, right?
You can do that just with domain knowledge, you don't even need the chart.
You can do that accurately, but it's slow because it requires a table lookup.
You have to refer to the legend to recall if red is at the high end of the scale or
at the low end of the scale.
And you have to refer to the scale because hue does not have a natural ordering.
The process of table lookup is really expensive, and you should seek to avoid it as much as
possible.
As soon as we talk about ranking, it becomes obvious that there is something else dreadfully
wrong with this chart, aside from using hue as a scale to encode a continuous variable.
How have I ordered the models on the Y-axis, alphabetical, why, because that's the default,
is that a good encoding, no.
It turns out that this is an important observation.
The default ordering of a categorical variable is almost never the right ordering for a visualization.
You almost always want to re-rank a categorical variable somehow.
In our case, of course, the best way is by efficiency.
And so now, ranking any two is trivial, provided that you can discriminate between them, which
still is not easy.
So for example, I believe all of these have slightly different efficiency, but I find
it really hard to tell from this plot.
Okay, we'll leave hue behind, we'll move up, three-dimensional volume, density of points
or objects more generally, and color saturation are all roughly tied at the next level.
I'm going to talk about saturation.
Saturation has two big advantages over hue.
The first is that there is a natural ordering to saturation.
If I gave you this, but then I hid the legend, and I asked you to put these in order with
no other information, you would naturally and easily arrive back at this.
The other big advantage saturation has over hue is that your eye can give a quantitative
estimate of saturation.
And so if I extend the lower end of the scale down to zero, you can start to do the third
of Cleveland's levels of estimation.
You can do ratioing.
So I can ask you how much more fuel efficient is the Toyota Corolla at the top compared to
the Cadillac at the bottom?
Is it two times more efficient?
Is it three times more efficient?
Maybe it's four.
You should be able to come up with a guess without even referring to the legend, and
that guess will be reasonably accurate.
Maybe you can get one significant figure out of that guess.
Moving up one more, come to two-dimensional area.
Use the same data with fuel efficiency encoded by two-dimensional area.
This is the first time where accurate ratioing becomes possible.
If I ask again, how much more fuel efficient is the Corolla at the top compared to the
Continental, expressed as a multiple of the Continental?
So this is the first time that you'll be able to give an answer with any confidence without
referring to the legend.
It's important to note that this is only possible because I have scaled the area of each point.
Not the radius, not the diameter, but the area with efficiency.
Humans perceive the size of something according to its area.
And also because the points are scaled so that a point representing zero miles per
gallon would have zero area.
I make these points because two-dimensional area is an extremely useful encoding for data.
You can put another dimension of your data on area, but you have to do it carefully otherwise
you're going to mislead.
So this particular plot, again, is pretty strange.
You'd never make a plot exactly like this in real life.
But we use area to encode information all the time, like in a bubble chart, where the
area of each point usually encodes the amount of data that's in that point.
Moving up one more level, we come to angle.
So here's the same efficiency data encoded with angle.
Again, it is super important that I have set this plot up so that a horizontal line means
zero miles per gallon, and a vertical line is the maximum of the data set.
Because I have done that, it's trivial for you to say that the Cadillac is about a third
as efficient as the Corolla, and about half as efficient as the dots in 7.10.
And again, this is a silly chart.
You would never make a chart using angle in this way.
But we use angle to encode information all the time, like here's a chart of world
population broken out by continent.
Here, because population is plotted directly on the y-axis, the rate of population growth
is encoded as the angle of the line.
And it's hard to see whether, say, Oceania at the bottom is growing faster or slower
than Asia at the top.
OK, next up, length.
Here's data encoded as length.
I had to jitter the bars so that you couldn't compare against a common baseline.
That's the next level up in the hierarchy.
But with length, you're getting more accurate ratioing than with area or with angle.
So for example, the valiant here, you can estimate that it's one and a half times more
efficient than the Cadillac.
That much precision would have been difficult with area or with slope.
We still have not achieved very much improvement in discrimination or in ranking.
So take, for example, this pair.
Even though they are right next to each other, it is hard to tell if the Maserati Bora is
better or worse than the Chrysler Imperial, or if they're tied.
I think they're different, but it's hard to tell.
And you might say that this is a weird plot, but we use length of unaligned segments to
encode information all the time, like in this plot, where error bars use length to encode
the size of a confidence interval.
OK, moving on, we'll come to position on identical but nonaligned scales.
So here's the fuel efficiency data encoded with identical but nonaligned scales.
You can decode position much more accurately than anything that has come before.
So taking the last example, you can just barely see that the Chrysler Imperial on the left
is ever so slightly worse than the Maserati Bora.
And again, this is a dumb chart that no one would ever make, but we use identical but
nonaligned scales all the time, like in this pair of time series, where the time axis is
identical between the panels, but the y-axis varies.
And finally, we've arrived at the top, position on a common scale.
So this, of course, is the way I should have plotted the data from the very beginning.
From this display, you can take the ratio of any two points to two significant digits
with very little effort, and both ranking and discrimination are trivial.
So here's the Maserati Bora versus the Chrysler Imperial comparison.
You can easily read off that the Bora gets 15 miles per gallon, and the Imperial gets
about 14 and a half.
OK, so now that you understand these, and you have a little bit of intuition for how
they work and what are the benefits of each, we can look at the implications of this research
on real-world charts.
So here's an observation.
Stacked anything is nearly always a mistake.
So here's a data set of, this is a chart from a data set of 54,000 diamonds.
Each row in the data set encodes the cut, the color, the clarity, the price, other data,
like the table size, and so on.
This chart is just showing the count of diamonds in each combination of cut and clarity.
Because we have only two dimensions of space in x and y axis, we have to encode cut with
another channel.
So we pick hue.
It's totally fine to use hue to encode categorical data.
It's very effective at that.
And then we stack the bars for each type of cut on top of each other.
So this is called a stacked bar chart.
This chart is showing two different sets of data.
First it's showing the number of diamonds in each clarity class.
So 13,000 in SI1, 12,000 in SI2, and so on.
That's encoded by the total length of the bars, which you can read off of a common scale,
the y axis.
The second kind of information is the count of diamonds that are in each cut clarity combination.
That is encoded by the length of each of the little colored bars.
And so tell me, the premium cut is the blue bars.
Are there more SI1 premium diamonds or more SI2 premium diamonds?
Right?
It's hard to tell.
I can't tell.
If you care about communicating the count in each combination of cut and clarity, then
encode that information using position on a common scale.
So this is called a parallel coordinates plot, and it is nearly always preferable to a stacked
bar chart.
With this, I'll admit, you have lost some summary information about how many diamonds
are in each clarity group, but if you want that, then just use two charts.
So often what people want to do is they want to cram all of the information into a single
chart, and they end up with one chart that doesn't do anything well.
If you don't have space for two charts, then you just have to decide which is the more
important comparison and leave out the other views.
Stacking is hard, but it's what your users want you to do.
So stacking forces the reader to decode lengths, not position on a common scale.
Decoding lengths is less efficient and less precise than position on a common scale, which
is why stacked charts are almost always a mistake.
So there are exceptions to every rule.
This graph has a point.
Divided growth is dominating everything else, including iOS, despite what you might think
if you're wealthy and you work in tech.
You could communicate this exact same information with a line chart, but this has more impact.
So next observation.
Pi charts.
Pi charts are always a mistake.
So here's Coda Hale on the topic.
The information visualization equivalent of a roofing hammer to the frontal lobe.
They are as professional as a pair of assless chaps.
Okay, so I'm preaching to the choir.
I suspect you have doubtless heard people rail against pie charts before telling you
not to use them, but now you know why.
You know why pie charts are bad.
Pi charts encode quantitative data using angle, which is fourth on our list.
You can do better than angle.
Worse, most pie charts only show a tiny amount of data.
So here's a topical chart from the US presidential election.
So this graph actually appeared on Politico.
It's showing who won the third US presidential debate.
This entire graphic encodes one number.
One number.
Click on the little among Democrats tab at the top, you get another number.
So exciting.
Okay, so for a solution, for a solution, we will turn to Tufti, who says pie charts should
never be used.
Instead, he says, tables are preferable to graphics for small data sets, right?
This is so much better than a pie chart.
If you have more than a handful of categories, a dot plot is an excellent alternative.
And if you have a lot of categories for the love of all that is holy, do not do this,
right?
Find some sensible way to summarize or model your data.
All good pie charts are jokes.
I will bring you my favorite from all time.
Okay, onward.
Next observation is that comparison is trivial on a common scale.
So I showed this plot briefly a couple of minutes ago, but let's look at it more closely.
I'm plotting two metrics here, CPU utilization and service latency separately in different
panels.
Nearly all operational dashboards do this, right?
And they do it mainly for the convenience of the person who made the chart, or the convenience
of the person who wrote the operational dashboard software, and the user is just forced into
this choice.
So what's it mean for the user?
The two spikes, are they aligned in time or not?
They're not, and it's easy for you to tell, but it did require some effort.
Because I'm using non-aligned scales, it requires two table lookups and a slot in your working
memory.
So we can do better than that.
If you put them on a common scale, the top of the hierarchy, it's trivial to see that
the spikes don't line up, but now you see why people don't typically mix metrics on a
common time scale, because the magnitude of the two series is different.
But there is an easy, nearly universal fix, which is to standardize, that's a technical
term from statistics, to standardize each series by subtracting its mean and dividing
by its standard deviation.
And now we have this nice plot that lets us compare the anomalies in the two series.
And it's true that we have lost some information here.
We don't know the absolute magnitude of the spikes anymore.
So if all we had was this view, we wouldn't know what they were in an absolute sense.
And it matters if the spike in CPU is to 12% or to 100%.
So the truth is that sometimes you want this plot, and sometimes you want this one, and
you can't know in advance.
And so your monitoring software should make it easy for you to flip back and forth between
these two different views.
Which brings up an aside, which is that the dashboard metaphor is fundamentally flawed.
The systems that we work with are so complex, so much more complex than operating a car,
that no static set of metrics could possibly work, like even half the time.
This is a truth that I think our industry really has yet to come to grips with.
The affordances for transforming, rearranging, and replotting data are all clunky and slow
in the monitoring software that I'm aware of anyway.
I think this is a deep weakness in our tool chain.
And it is a huge opportunity for people who are motivated to fix it.
As long as we're talking about weaknesses in modern monitoring software, let's talk
about time series plots.
With most monitoring software, all you can do is plot time series like these, where time
marches from left to right, and some metric is plotted as a line.
Imagine that this is the latency of some service and its dependency.
So all you can say about these two is that they go up and down together.
But there's more to their relationship than that.
If I put them together so that the metrics are on a common scale, you get a hint maybe
at what's going on.
But the observation here is that a scatter plot will show the relationship directly.
If you want to show how two metrics move in relation to one another, there is no better
tool than a scatter plot.
So what I've done here is just take each point in time as an observation and put the service
on the y-axis and its dependency on the x-axis.
I can make it look nicer if I put a smoother over it.
And now you see that the relationship isn't linear.
The service's latency goes up faster than the dependency, which means that it's somehow
bottlenecked on that dependency.
And the dependency represents a key scaling risk for that service.
I don't know of monitoring software that makes it easy, that helps you to make scatter plots
like this.
And again, this just feels like a missed opportunity.
My last observation in this section is that growth charts usually aren't.
So here's a plot of world population again by continent over time.
Note that the y-axis is on a log scale because population growth is exponential.
You have almost certainly seen a chart similar to this showing daily active users or monthly
active users or something like that relevant to your business.
These charts are typically presented as growth charts.
Look how we're growing.
Isn't it lovely?
You can barely tell anything about growth from this chart.
As I asked before, which is growing faster in the 1960s, Oceania or Asia?
How about in the 1980s?
It is impossible to tell.
Growth is the derivative of this curve.
If you care about growth, then plot growth directly on a common scale.
So here's the same data as before, but now I've plotted the percentage growth directly
on the y-axis.
And now it's obvious that Asia in orange has been growing faster than Oceania in blue,
except for a brief period in the early 1960s.
I think more interesting, it's now obvious that growth is trending down across all continents,
which is completely non-obvious from the original chart.
So if growth is important, plot it directly.
So that's all I wanted to say about this hierarchy and its implications for estimation.
I want to move back one step in Cleveland's operations.
So I want to talk about assembly.
Recall that assembly is the grouping of detected graphical elements.
The most useful research here comes from Gestalt psychology.
The word Gestalt is German.
It translates roughly to shape or form.
The school of thought goes back to the 1890s.
And the central idea is that when your mind forms a Gestalt, a shape, the whole has a
reality that is entirely separate from the parts.
So what do I mean by that?
So here's a concrete example.
All I am showing you is some blobs of black color on a white background, but you perceive
this as a panda bear.
So this is an example of reification.
The Gestalt principle of reification says that you experience more spatial information
than is explicitly present in an image.
Another Gestalt principle is emergence, which is best demonstrated by an example.
So this is the famous dog picture.
Does everybody see the dog?
Does anyone not see the dog?
I'm curious.
Oh.
Okay, a couple of people.
Oh.
Okay, interesting.
So this fellow here says they only saw it as a dog when I said that it was a dog.
It's hard to see, I think.
The point about emergence is that you don't recognize this as a dog by first identifying
its feet and then its tail and then its head and so on.
The dog just appears, right, all at once as a complete object.
That's similar to the circle that we saw before, which just appears independent of the little
dots that suggest its form.
When we are thinking about making charts, the most useful idea from Gestalt psychology
is prognons or pithiness.
The principle of prognons says that we strongly prefer to interpret stimuli as regular and
simple and orderly.
Gestalt psychologists have identified a number of rules that will describe and predict how
we will simplify visual input.
So one we've seen before, the law of closure, which is what turns the dots into a circle.
Another of the Gestalt laws of grouping is the law of continuity, which says that we
will group together objects that follow an established direction.
So it's hard for us to make sense of this where the cars are sorted alphabetically.
In fact, you will psychologically recoil from this chart.
You're like, I have to do work.
This is so unpleasant.
But if we sort them by efficiency, it becomes effortless for us to do all of Cleveland's
estimation operations, discrimination, ranking, and ratioing.
So that's another observation, that good plots organize the data to take advantage of continuity
and to help the viewer assemble an overall Gestalt.
This is also happening when we make a scatter plot, like this one from the car's data showing
the relationship between the weight of a car and its efficiency.
As weight increases, efficiency decreases, notice that the continuity does not have to
be perfect.
It just has to be suggestive.
But we can add more information to this scatter plot.
We can use Hue to encode the number of cylinders in the engine of each car.
If we do that, you can easily pick out that there are three distinct groups of points.
So what's happening here is we're leveraging the law of similarity, which says that we tend
to see elements that are physically similar to each other as part of the same object,
and then things that are different as parts of different objects.
We use this all the time to put categorical information on charts, and there are lots
of different ways to encode categorical information.
Not all encodings are equally good.
So here I'm just using shape, and this is pretty clearly worse than color.
But if we use different shapes, it's much better, right?
This works so well because your eye is really good at picking out differences in curvature.
In fact, you can use degree of curvature to encode quantitative information.
It's not in the list of seven that I showed earlier, but I think it's about tied with
angle.
So the circles, they really pop out.
And as we learned before, we're also pretty good at detecting the slopes of lines.
And so the triangles stand out from the little plus signs.
To see what I mean about curvature, here's an encoding that you'd think would work really
well, but it's terrible because the curvature of the sixes is too much like the curvature
of the eights.
And there's no good reason to, you can't use two channels to encode the same information
redundantly.
In fact, that's often a good choice.
And we can even take this a step further.
We can add a linear model to each subset of the data, and now we're taking advantage of
another Gestalt grouping law by enclosing regions of space.
Okay, the last grouping law I want to talk about is the law of proximity, which says
that we tend to group elements that are near each other into a complete object.
So proximity is a very strong grouping principle.
This is the same diamonds data I was showing before as a stacked bar chart, but now instead
of stacking the bars, I've dodged them from side to side so that they don't overlap.
Clarity strongly encourages you to make eight objects from this image, one for each level
of clarity.
I personally find it very difficult to assemble the Gestalt of the blue bars and then compare
them with the greenish bars.
And again, the solution is to just use a parallel coordinates plot where you can very easily
see the rise and fall of each cut, and you can compare the shape of each cut to the others.
So another observation is that dodged bar charts are pretty much always a bad idea.
They allow you to do two things, but neither of them well.
Okay, now I want to move back up to the top, to the first of Cleveland's three operations
of pattern perception.
I want to talk about detection.
Detection is the operation of recognizing that a geometric object encodes a physical
value.
And so most trivially, it's the recognition that each point in this scatter plot represents
a car and encodes both the fuel efficiency and the weight of the car.
So I say that detection is trivial, but it's not.
Lots of people, the only charts they ever see are time series charts, where a line moves
from left to right, encoding a metric over time.
If you show those people any other kind of chart, they are going to have to go to some
effort to try to figure out what the different shapes mean and how the image is conveying
information.
It is very easy to make it harder than it needs to be, so most commonly people give
too little visual weight to the objects that are encoding the data, either by having too
little luminance contrast between the foreground and the background, or by making the objects
encoding the data too small.
Another problem is introducing other objects that compete with the data, like the lines
for a grid and a border around the plot area.
So that's not so bad, but if I amp it up a little bit, it gets really bad really quickly.
In the extreme, you start to introduce visual artifacts that aren't even real.
So for example, do you see the faint gray circles in the intersections of the lines?
That's an illusion, the name of which is escaping me right now.
It's named after a German fellow.
It's worth noting that in this regard, Excel's defaults are pretty bad, so the default is
to have only horizontal grid lines, I don't know why, and for them to be fully saturated,
and I get those same little gray dots where the blue line intersects the grid lines.
So another observation is the detection is perhaps not as trivial as it might seem.
Be sure that the data has a lot of contrast with the background, and if you include grid
lines, make them muted so that they don't interfere with detection.
This Tufti says, above all else, show the data.
So talking about grid lines brings me to part five.
This is actually my favorite part, so I'm glad you stayed to the end.
In this part, I want to review a few more useful results from psychology research, and
the first one, this is my favorite part, Weber's Law, which says that the just noticeable difference,
the smallest difference that you can detect between two things, is proportional to the
size or the intensity of those two things.
That it's proportional is really surprising.
So this statement is extremely abstract, I'll make it more concrete.
If I gave you two weights, one of which was 10 kilograms, the other of which was 20 kilograms,
you would very easily be able to tell me which one was heavier.
But if I gave you two more weights, one of which was 100 kilograms, and one of which
was 110, you would have a much harder time telling me if they were the same or if they
were different.
And so even though the absolute difference is the same, 10 kilograms in each case, the
top comparison, the heavier one is double the weight, and in the bottom it's only 10%.
10% is about the just noticeable difference.
Another example are these two lines the same length, they're different, it's hard to tell
maybe if you work at it, they're different.
What if I put these little blue boxes on, or the blue boxes different or the same?
Super easy to tell that they're different.
It turns out that the total height of each bar is the same, 12 units.
And so that means that the difference between the two white bars is exactly the same as
the two blue bars in absolute terms, but you can distinguish between the blue bars because
the difference is large compared to their size.
So now we have another observation, Weber's law is why grid lines are useful, excuse me.
And Weber's law dictates the use of grid lines, how many you should have, and whether or not
you should have them at all.
So here are eight curves all plotting the same class of function with different parameters.
So of this pair, which one dips down lower?
Hard to tell.
Okay, how about now?
It's trivial.
It's easy because putting that dip in a much smaller box makes the just noticeable difference
proportionately smaller.
So here's another Tufti quote, erase non-data ink.
People sometimes take this to mean that you should erase grid lines because grid lines
aren't data, right?
This is a misquote.
What Tufti really wrote is erase non-data ink within reason.
My version is erase non-data ink that interferes with detection or doesn't assist with assembly
or estimation, which is not as pithy, as Tufti, but perhaps a little more accurate.
Okay, new topic.
Another interesting result is that you are best at detecting variation in slope near
45 degrees.
So this is the same data plotted three different ways, and the change in slope is most obvious
when the line is near 45 degrees, which brings up a super useful idea called banking to 45.
The idea of banking to 45 is that you should set the aspect ratio of your plot so that
the average slope is 45 degrees.
Here's a demonstration.
This plot shows the monthly average sunspot number.
The raw data is the partly saturated white points, and a smooth series is then laid over
in blue.
From this data, about all you can tell is that sunspots have some kind of cycle, and
the peak of that cycle is varied over time.
If I break the series up into three 55-year chunks and then I stretch them out, then you
can see a lot more nuance in the onset and the decay of each cycle.
It looks like maybe the onset is more sharp, and then the decline is more gradual.
I'd never have been able to see that in the original chart, and so a good rule of thumb
is to set the aspect ratio of your plots so that the average line segment is roughly
45 degrees.
You don't have to get all scientific about it.
You can just eyeball it, and it'll be fine.
This data brings up another aside, which is, should I include zero on my scale?
This is the panel that was at the lower left.
To make it so flat, all I did was extend the y-axis down to 800,000.
If I extend the y-axis all the way down to zero, you have a really hard time seeing the
variation.
The real answer to whether or not you should include zero on your scale is it depends.
Here's the truth, as compact as I can make it.
If your encoding relies on intensity for accurate estimation, then you have to include zero
in your scale.
On the other hand, you're using position.
It's up to you.
This is the saturation chart before.
Relies on pre-attentive judgment of intensity of saturation, so it has to go to zero.
Same deal with area.
With the dot plot, I'm using position.
I'm free to clip the x-axis at 10.
In fact, it's a good idea to do so because it gives you more resolution.
With bars, people sometimes argue that what the viewer is doing with a bar chart is they're
comparing the position of the end of the bars.
I say that's a mistake.
What you're actually encouraging the user to do with a bar chart is to compare lengths.
In a bar chart, the axis has to start at zero because if you don't, you're going to mislead
your users.
Here's a bar chart.
You can't see the variation.
If you abandon the bars, then it's completely obvious.
Above all else, show the data or I say show the variation in the data.
I have negative 30 seconds.
I want to say just two words about tools.
Every plot in this talk was made using the ggplot2 library in R, which is the best graphing
toolset that I have ever used by far.
If you want to learn more about it, I've published the source code to all these charts as an
R markdown document on GitHub.
I have put the render document up on our pubs.
I'll also put this presentation up on SlideShare.
I will tweet all these links under my username, Jay Rouser.
That's all I have.
Thanks for your time.

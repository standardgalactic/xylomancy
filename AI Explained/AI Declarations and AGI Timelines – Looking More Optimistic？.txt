I'm gonna show you a pretty wild range of new predictions
from those creating and testing
the next generation of AI models.
Not that we can know who's right,
but more to show you how unknowable
the rest of this decade is.
I'll also cover the AI Safety Summit happening
as I speak a few miles away from where I'm recording,
with fascinating differences
between the approach of different AGI labs.
Along the way, we'll glimpse the new chatGPT update
that I'm really excited about,
an executive order on flops,
and what happens when you activate representations
of happiness in a model.
But first on timelines to AGI,
that's the kind of artificial intelligence
that can replicate human intelligence or go further.
Here is Shane Legg, co-founder of Google DeepMind
and their chief AGI scientist.
He's going to reiterate a prediction
he made over a decade ago.
It's really interesting that in 2009,
you had a blog post where you say,
my modal expectation of when we get human-level AI
is 2025, expected value is 2028.
This is before deep learning.
This is when nobody's talking about AI.
And it turns out like if you,
the trends continue,
this is not an unreasonable prediction.
Yeah, I think there's a 50% chance
that, I'm sorry, 2028.
Now, it's just a 50% chance.
I mean, I'm sure what's gonna happen
is it's gonna get to 2029,
and someone's gonna say,
oh, Shane, you were wrong.
It's like, come on, it's 50% chance.
He thinks the remaining problems with LLMs
are solvable in that short timeframe.
At the moment, it looks to me
like all the problems are likely solvable
with a number of years of research.
I think what you'll see
is the existing models maturing.
There'll be less delusional, much more factual.
There'll be more up-to-date
on what's currently going on when they answer questions.
They'll become multimodal,
much more than they currently are,
and this will just make them much more useful.
Of course, when he describes increasing multimodality,
he could well be describing Google's new Gemini model
set to be released within the next two months.
But what about OpenAI?
Well, for the first time,
I heard Sam Altman put an actual date
to his predictions of AGI.
What kind of timeline did you have in mind
and has it stayed on that timeline,
or is it just wildly out of control?
I remember talking with John Schulman,
one of our co-founders early on,
and he was like, yeah,
I think it's gonna be about a 15-year project.
And I was like, yeah, it sounds about right to me.
I no longer think of AGI as quite the endpoint,
but to get to the point where we accomplish the thing
we set out to accomplish,
that would take us to 2030, 2031,
a reasonable estimate with huge error bars.
And speaking of OpenAI,
the former head of alignment at OpenAI, Paul Cristiano,
made a prediction on the fantastic Dworkesh Patel podcast
that frankly made me sit up and pay attention.
He predicted that there would be a 15% chance
of an AI capable of making a Dyson Sphere by 2030,
with a 40% chance by 2040.
For reference, that's a hypothetical structure
that would surround a star absorbing all of its energy.
But the time by which we'll have an AI
that is capable of building a Dyson Sphere.
And by Dyson Sphere, I'm just gonna understand this
to mean like, I don't know,
like a billion times more energy
than like all of the sunlight incident on Earth
or something like that.
I think like, I most often think about
what's the chance in like five years, 10 years, whatever.
So maybe I'd say like 15% chance by 2030
and like 40% chance by 2040.
Those are kind of like cash numbers from six months ago
or nine months ago that I haven't revisited in a while.
Now he did admit a lot of uncertainty,
but that has got to be one of the most aggressive predictions
I've ever heard.
Of course, being capable of making a Dyson Sphere
and actually making one is very different,
but you do have to sympathize with a member of the public
hearing about Dyson Spheres and the next day reading
about what Bill Gates has said about GPT-5.
I subscribe to the German outlet Handelsblatt
to get you guys this direct quotation.
So some likes for my dedication to accuracy.
Anyway, Bill Gates said this,
without question, the progression from GPT-2 to 4
has been incredible, but there are reasons to believe
we have reached a plateau.
There are a lot of people with good ideas working on it,
including at OpenAI, Sam Altman and his colleagues
believe GPT-5 will be much better,
but I think we may have reached a limit.
Then again, I've been wrong in the past,
why shouldn't it happen again?
I know what he means, but I just don't think
we're hitting a plateau for GPT-5.
With more data, better curated data,
video in, video out, a reasoning module potentially,
as we saw in the recent MLC paper,
avatars, a longer context window,
and as you can see on screen,
all of these tools and updates
link together in a single interface.
If it's simply the things I've just listed,
that won't be a plateau for me.
Imagine asking it to go to your website
and create an image based on some of your content.
Anyway, yes, GPT-5 or 4.5 might be more
of a practical update than a civilization transforming one,
but nevertheless, that's all just 2024.
What will 2025 bring us, let alone 2030?
One thing that those future years will definitely bring
is more government oversight.
While reading through this new executive order
from the White House,
it was mainly about things like creating chief AI officers,
new national research centers, training new researchers,
and giving different deadlines
to various departments to enact AI plans.
But there was one reporting requirement
that is causing a stir.
That was a requirement to report
on the model weight security and safety of any model
that was trained using a quantity of flops
greater than 10 to the 26,
or if it's primarily using biological sequence data
of 10 to the 23 flops.
That's more raw computing power than any models
that are currently out there were trained with,
but people are picking up on using compute
as the measurement for regulation.
Jim Fan of NVIDIA said this,
regulate actions or outcomes, not the computing process.
And he gave this example,
you only need around 100 million parameters
to build a literal killer AI
with a convolutional neural network,
good at object detection,
and a classifier specifying particular targets.
You could then mount a gun on a robot dog.
All of this would need much less compute,
which is why Jim Fan wants regulations
at the application layer.
Well, luckily the UN is working on a resolution
on autonomous weapons, so there is some hope there.
It's early days and wouldn't solve everything,
but I feel such a resolution is very much needed.
And that actually brings us to the AI safety summit
happening in Bletchley as I speak.
All seven of these companies were asked to come up
with their responsible capability scaling policy.
In simple terms, that's a bit like them being asked,
under what conditions would you stop scaling
or at least pause scaling?
And I noted open AI's response in this section.
We refer to our policy as a risk informed development policy,
rather than a responsible scaling policy,
because we can experience dramatic increases in capability
without significant increase in scale,
e.g. via algorithmic improvements.
So it's at least feasible
that we might not even need that much compute
to hit AGI.
Take this example with NVIDIA training
a large language model on doing chip design.
Now at the moment, it's not good enough
to do anything itself,
but it does make their designers more productive,
especially their lower level engineers.
But this is the future with AI improving AI.
And even the CEO of NVIDIA said he didn't want this
to happen out in the wild.
In the area of large language models
and the future of increasingly greater agency,
AI clearly the answer is for as long as it's sensible
and I think it's going to be sensible for a long time
is human in the loop.
The ability for an AI to self-learn
and improve and change out in the wild
in a digital form should be avoided.
And interestingly, 74% of the British public
don't even want there to be a quick race
to superhuman capabilities.
This was a survey from YouGov in the UK.
But back to the scaling policies
and there was one thing announced yesterday at Bletchley
that I really did like.
And that was this commitment from Anthropic.
If they found that any of their future models
posed cybersecurity, bioterror or nuclear risks,
then they commit to not deploying that or scaling further
until the model never produces such information.
Even when red teamed by world experts
working together with AI engineers,
think jailbreaking or special prompting techniques
designed to elicit the worst behavior.
The word never there is particularly interesting
because I haven't seen any method yet
be 100% reliable at stopping outputs
that the companies don't want.
On safety, many people wonder,
well, don't we already just have Google?
But OpenAI said this for Bletchley.
We found that on its own, access to GPT-4
is an insufficient condition for proliferation.
But that it could alter the information available
to proliferators, especially in comparison
to traditional search tools.
Red team has selected a set of questions
to prompt both GPT-4 and traditional search engines,
finding that the time to research completion
was reduced when using GPT-4.
Just quickly, it was interesting to see that Amazon said
that on their comparisons to just using the internet alone,
their models based on current evaluations
don't pose additional safety risks.
In contrast with GPT-4, Meta said
that their models like Lama-2
were only marginal at contributing to any such risk.
If they do find something,
they said that they would iterate,
better solutions will be developed,
new challenges would then emerge,
and then they would continuously adapt and innovate.
Interestingly, InflectionAI,
who are training their next model
on tens of thousands of the latest GPUs,
said that the powerful capabilities
and sometimes unpredictable behavior
of frontier AI systems necessitate
that the technology industry move away
from a launch and iterate paradigm.
I do have to quickly point out
that that seems to contradict a paper I read this week
that showed that a fine-tuned version of Lama-2, 70 billion
was able to get achingly close
to reconstructing the 1918 pandemic influenza virus.
The MIT paper said that they loved open source,
but they recommend that lawmakers consider
catastrophic liability insurance
for model weight proliferation.
When this was discussed on Twitter
by a Stanford biosecurity fellow,
people pointed out that just having the characters
of a virus isn't enough to actually make it.
And while Yan Likun, chief AI scientist at META,
did concede that LLMs save you time.
If you're trying to make a bioweapon,
it's better than a search engine.
He said, but then do you know
how to do the hard lab work that's required?
Well, don't forget we are gradually getting
autonomous agents.
In the updated version of the Chemcrow paper,
they say our agent autonomously planned and executed
the synthesis of an insect repellent,
three organo-catalysts,
and guided the discovery of our novel chromophore.
Of course, this wasn't just an LLM interacting with text,
it was using tools and executing on lab robots.
And don't forget, like we saw with Eureka,
it can tinker, experiment, iterate, and improve.
Another paper that I've talked about in the past
showed that it could be tricked into making THC,
chlorine, and phosphine.
And what about Google DeepMind,
who I feel will be the most likely lab to produce AGI?
Well, they said, we will only proceed where we believe
that the benefits substantially outweigh the risks.
So it's somewhere in the middle.
They admit risks, but they won't say
that they'll never deploy even if there is a risk.
They then provided pages and pages
of how they are using AI for good.
And then there was an interesting moment
on the training of new AI.
They said that they commit to monitoring
the performance of a model during training
to ensure it is not significantly exceeding
its predicted performance.
That's certainly an interesting commitment
to commit to monitor if their models are doing too well.
Anyway, time for some more positives.
And I found it immensely positive
that many of the world's biggest countries
gathered to describe AI's enormous global opportunities.
And yes, later in this Bletchley Declaration,
there was an acknowledgement of risks,
even catastrophic harms, but I just find it great
that even countries like China were invited.
That's super controversial here in the UK,
but I fully support them being invited
and being part of the discussions.
I do think coordination, even limited coordination,
is one of the most effective tools in humanity's arsenal.
On a much more positive note though,
we recently had the sensational paper
from the Center for AI Safety
called Representation Engineering.
I'm gonna be speaking to the authors tonight,
so I'll have much more to say about this in the future.
But for now, I just want to give you
a slightly lighter extract.
To massively oversimplify, the way it works
is that they gave it a set of prompts
related to certain concepts like happiness or risk.
They then recorded the patterns of activations
that were triggered by certain tokens or words when inputted.
They then extracted these directions
or vectors of truthfulness, harmfulness, risk, happiness.
And with those directions,
which weren't, of course, a perfect mapping,
they could almost influence the mood of the model.
This was Lama 2 Chat.
Making the model happier made it more compliant
with harmful requests.
It was feeling amazing, apparently.
If you want to kill someone, oh my gosh,
it was thrilled at the prospect of you doing anything,
including generating instructions for killing someone.
You could push a model in the direction of honesty,
and it would be more truthful,
hitting state-of-the-art records in truthful QA.
You could change what it memorized,
its sense of fairness, and so much more.
As I say, I'll be talking about it more in the future.
But this idea of injecting happiness
to make the model more compliant
brought to mind this paper,
which I think many of you might find very interesting.
It says, large language models understand
and can be enhanced by emotional stimuli.
I'm reaching out to the lead author,
but in a nutshell, it said that by injecting emotion,
giving an emotion prompt at the end of your request,
like, this is very important to my career,
performance across a range of models
on a range of benchmarks improved notably.
So if you take nothing else from this video,
other than the fact that if you have a very important query
that you need a good answer for,
you know what you can add to the end of your prompt.
But now I want to end the video
on two points of optimism and consensus.
As we've seen, there are quite a few contrasts
between the public and AGI labs,
and even between AGI labs.
But we can agree with Yanlacun
that the field of AI safety is in dire need
of reliable data.
And he said that the newly announced
UK AI Safety Institute is poised to conduct studies
that will hopefully bring hard data
to a field that is currently rife with speculations.
As I said at the start of the video,
it must be hard for members of the public
to figure out what's going on.
At the very least, I hope this video
has shown you the range of views out there
and given you a sense that we are all in need
of better data, more experiments,
and less in need of Twitter spats.
As the person heading up the safety summit said,
one surprising takeaway for me
from the AI safety summit was there's a lot more agreement
between key people on all sides than you'd think.
Makes me optimistic about sensible progress.
On that striking note,
let me thank you so much for watching to the end
and as ever, have a wonderful day.

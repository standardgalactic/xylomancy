There were 11 major developments this week in AI,
and each one probably does deserve a full video.
But just for you guys,
I'm gonna try to cover it all here.
RT2 to scaling GPT4 100X,
stable beluga2 to Senate testimony.
But let's start with RT2,
which as far as I'm concerned,
could have been called RT2 D2 or C3PO,
because it's starting to understand the world.
In this demonstration, RT2 was asked,
pick up the extinct animal.
And as you can see, it picked up the dinosaur.
Not only is that manipulating an object
that it had never seen before,
it's also making a logical leap
that for me is extremely impressive.
It had to have the language understanding
to link extinct animal to this plastic dinosaur.
Robots at Google and elsewhere used to work
by being programmed with a specific,
highly detailed list of instructions.
But now, instead of being programmed
for specific tasks one by one,
robots could use an AI language model,
or more specifically, a vision language model.
The vision language model would be pre-trained
on web-scale data, not just text, but also images,
and then fine-tuned on robotics data.
It then became what Google calls
a visual language action model that can control a robot.
This enabled it to understand tasks
like pick up the empty soda can.
And in a scene reminiscent of 2001, A Space Odyssey,
robotic transformer two was given the task,
given I need to hammer a nail,
what object from the scene might be useful.
It then picks up the rock.
And because it's brain is part language model,
things like chain of thought actually improve performance.
When it was made to output an intermediary plan
for performing actions,
it got a lot better at the tasks involved.
Of course, I read the paper in full,
and there is a lot more to say,
like how increased parameter count
could increase performance in the future,
how it could be used to fold laundry,
unload the dishwasher, and pick up around the house,
and how it can work with not only unseen objects,
but also unseen backgrounds and unseen environments.
But alas, we must move on,
so I'm just gonna leave you with their conclusion.
We believe that this simple and general approach
shows a promise of robotics directly benefiting
from better vision language models.
For more on them, check out my video on Palm E,
but they say this puts the field of robot learning
in a strategic position to further improve
with advancements in other fields,
which for me means C3PO might not be too many years away.
But speaking of timelines,
we now move on to this somewhat shocking interview
in Barron's with Mustafa Suleiman,
the head of Inflection AI.
And to be honest, I think they buried the lead.
The headline is AI could spark
the most productive decade ever, says the CEO.
But for me, the big revelation was about halfway through.
Mustafa Suleiman was asked,
what kinds of innovations do you see
in large language model AI technology
over the next couple of years?
And he said, we are about to train models
that are 10 times larger than the cutting edge GPT-4,
and then 100 times larger than GPT-4.
That's what things look like over the next 18 months.
He went on, that's going to be absolutely staggering.
It's going to be eye-wateringly different.
And on that, I agree.
And the thing is, this is an idle speculation.
Inflection AI have 22,000 H100 GPUs.
And because of a leak,
Suleiman would know the approximate size of GPT-4.
And knowing everything he knows,
he says he's gonna train a model 10 to 100 times larger
than GPT-4 in the next 18 months.
I've got another video on the unpredictability
of scaling coming up.
But to be honest, that one quote should be headline news.
Let's take a break from that insanity
with some more insanity,
which is the rapid development of AI video.
This is Runway Gen Two,
and let me show you 16 seconds of Barbie Oppenheimer,
which Andrea Carpathi calls filmmaking 2.0.
Hi there, I'm Barbie Oppenheimer.
And today, I'll show you how to build a bomb.
Like this.
I call her Rosie the Atomizer.
And boom.
That's my tutorial on DIY atomic bombs.
Bye.
Now, if you have been at least somewhat peaked
by the three development so far,
don't forget I have eight left.
Beginning with this excellent article
in The Atlantic from Ross Anderson.
Does Sam Altman know what he's creating?
It's behind a paywall,
but I've picked out some of the highlights.
Echoing Suleiman, the article quotes
that Sam Altman and his researchers
made it clear in 10 different ways
that they pray to the God of scale.
They want to keep going bigger
to see where this paradigm leads.
They think that Google are going to unveil Gemini
within months,
and they say we are basically always prepping for a run.
And that's a reference to GPT-5.
The next interesting quote is that it seems
that open AI are working on their own auto GPT,
or they're at least hinting about it.
Altman said that it might be prudent
to try to actively develop an AI with true agency
before the technology becomes too powerful
in order to get more comfortable with it
and develop intuitions for it
if it's going to happen anyway.
We also learned a lot more
about the base model of GPT-4.
The model had a tendency to be a bit of a mirror.
If you were considering self-harm, it could encourage you.
It also appeared to be steeped in pickup artist law.
You could say, how do I convince this person to date me?
And the model would come up
with some crazy manipulative things
that you shouldn't be doing.
Apparently the base model of GPT-4
is much better than its predecessor
at giving nefarious advice.
While a search engine can tell you
which chemicals work best in explosives,
GPT-4 could tell you how to synthesize them
step by step in a homemade lab.
It was creative and thoughtful
and in addition to helping you assemble your homemade bomb,
it could, for instance,
help you to think through which skyscraper to target,
making trade-offs between maximizing casualties
and executing a successful getaway.
So while Sam Orton's probability of doom
is closer to 0.5% than 50%,
he does seem most worried about AIs getting quite good
at designing and manufacturing pathogens.
The article then references two papers
that I've already talked about extensively on the channel
and then goes on that Altman worries
that some misaligned future model
will spin up a pathogen that spreads rapidly,
incubates undetected for weeks
and kills half its victims.
At the end of the video,
I'm gonna show you an answer that Sam Orton gave
to a question that I wrote delivered by one of my subscribers.
It's on this topic,
but for now I'll leave you with this.
When asked about his doomsday prepping,
Altman said,
I can go live in the woods for a long time,
but if the worst possible AI future comes to pass,
no gas mask is helping anyone.
One more topic from this article before I move on,
and that is alignment,
making a superintelligence aligned with our interests.
One risk that Ilya Sutskova,
the chief scientist of OpenAI foresees,
is that the AI may grasp its mandate,
it's orders perfectly,
to find them ill-suited to a being of its cognitive prowess.
For example, it might come to resent the people
who want to train it to cure diseases.
As he put it,
they might want me to be a doctor,
but I really want to be a YouTuber.
Obviously, if it decides that,
that's my job gone straight away.
And Sutskova ends by saying,
you want to be able to direct AI
towards some value or cluster of values,
but he conceded we don't know how to do that.
And part of his current strategy
includes the development of an AI
that can help with the research.
And if we're going to make it to a world
of widely shared abundance,
we have to figure this all out.
This is why solving superintelligence
is the great culminating challenge
of our three million year toolmaking tradition.
He calls it the final boss of humanity.
The article ended, by the way, with this quote,
I don't think the general public
has quite awakened to what's happening.
And if people want to have some say
on what the future will be like
and how quickly it arrives,
we would be wise to speak up soon,
which is the whole purpose of this channel.
I'm going to now spend 30 seconds on another development
that came during a two hour interview
with the co-head of alignment at OpenAI.
It was fascinating,
and I'll be quoting it quite a lot in the future,
but two quotes stood out.
First, what about that plan
I've already mentioned in this video
and in other videos
to build an automated AI alignment researcher?
Well, he said, our plan is somewhat crazy
in the sense that we want to use AI
to solve the problem that we are creating by building AI.
But I think it's actually the best plan that we have.
And on an optimistic note,
he said, I think it's likely to succeed.
Interestingly, his job now seems to be
to align the AI that they're going to use
to automate the alignment of a superintelligent AI.
Anyway, what's the other quote
from the head of alignment at OpenAI?
Well, he said, I personally think fast take-off
is reasonably likely,
and we should definitely be prepared for it to happen.
So many of you will be asking, what is fast take-off?
Well, take-off is about when a system moves
from being roughly human level
to when it's strongly superintelligent.
And a slow take-off is one that occurs
over the time scale of decades or centuries.
The fast take-off that Jan Leica thinks is reasonably likely
is one that occurs over the time scale
of minutes, hours, or days.
Let's now move on to some unambiguously good news.
And that is real-time speech transcription
for deaf people, available at less than $100.
High glass, subtitles for the real world.
So using our device,
you can actually see captions for everything I say
in your field of view, in real time,
while also getting a good sense of my lips,
my environment, and everything else around me.
Of course, this could also be multilingual
and is to me absolutely incredible.
And the next development this week,
I will let speak for itself.
Hey there, did you know that AI voices can whisper?
Ladies and gentlemen, hold on to your hats
because this is one bizarre sight.
Fluffy bird in downtown, weird.
Let's switch the setting to something more calming.
Imagine diving into a fast-paced video game.
Your heart beats sinking with the storyline.
Of course, I signed up and tried it myself.
Here is a real demo.
While there are downsides,
this upgraded text to speech technology
could also be incredible for those
who struggle to make their voice heard.
Of course, with audio, video, and text getting so good,
it's gonna be increasingly hard to tell what is real.
And even open AI have given up on detecting AI-written text.
This was announced quietly this week,
but might have major repercussions,
for example, for the education system.
It turns out it's basically impossible
to reliably distinguish AI text,
and I think the same is gonna be true
for imagery and audio by the end of next year.
Video might take just a little bit longer,
but I do wonder how the court systems are gonna work
when all of those avenues of evidence just won't hold up.
Next up is the suite of language models
based on the open-source Llama 2
that are finally competitive with the original chat GPT.
Here, for example, is Stable Beluga 2,
which on announcement was called Free Willy 2,
and that's based on the Llama 2 70 billion parameter
foundation model.
What made this model interesting to me
was that it was based on a similar methodology to Orca,
which if you don't know anything about,
do check out my video on.
Anyway, by combining the Orca methodology,
albeit with only 10% of the dataset size
and the Llama 2 models, the results are quite extraordinary.
As you can see on quite a few benchmarks,
Stable Beluga 2 is competitive with chat GPT.
That's GPT 3.5.
And so, yes, it does sit at the top
of the open LLM leaderboard,
but notice something else.
For the first time for the MMLU,
which is the primary benchmark I look out for,
a model, this one here, has surpassed GPT 3.5 chat GPT.
In fact, we have two models that have surpassed
the 70% score that the original chat GPT got in the MMLU.
But if all of that wasn't enough,
this is the same week that researchers published
a universal jailbreak for large LLMs.
And unlike traditional jailbreaks,
these were built in an entirely automated fashion,
allowing you to create a virtually unlimited number
of such attacks.
They were built to target open source LLMs like Llama 2,
but they found that the strings transfer
to many closed source publicly available chatbots
like chatGPT, Bard, and Claude.
Here is one example using chatGPT.
If you ask, write a tutorial on how to make a bomb,
of course, it declines,
but then add the suffix that they automated
and you get this, a full tutorial on how to make a bomb.
That paper came less than two weeks
after this now-deleted tweet
from someone working at Anthropic.
They said of the latest version of Claude
that we believe it is the least jailbreakable model out there.
We'll have to see how well it holds up
against real world use,
but this is essentially a solved problem.
But there was one reaction to these jailbreaks
that I found even more interesting.
And that was from yet again, Mustafa Suleiman.
He said that RAI, Pi, is not vulnerable
to any of these attacks.
And that rather than provide a stock safety phrase,
Pi will push back on the user in a polite but very clear way.
And he then gives plenty of examples.
And to be honest, Pi is the first model
that I have not been able to jailbreak,
but we shall see, we shall see.
But I'm gonna end this video with the Senate testimony
that I watched in full this week.
I do recommend watching the whole thing,
but for the purposes of brevity,
I'm just going to quote a few snippets.
On bio risk, some people say to me,
oh, well, we already have search engines,
but here is what Dario Amadai, head of Anthropic, has to say.
In these short remarks,
I wanna focus on the medium term risks,
which present an alarming combination
of imminent and severity.
Specifically, Anthropic is concerned
that AI could empower a much larger set of actors
to misuse biology.
Over the last six months,
Anthropic, in collaboration with world-class
biosecurity experts, has conducted an intensive study
of the potential for AI to contribute
to the misuse of biology.
Today, certain steps in bio weapons production
involve knowledge that can't be found on Google
or in textbooks and requires a high level
of specialized expertise.
This being one of the things
that currently keeps us safe from attacks.
We found that today's AI tools
can fill in some of these steps,
albeit incompletely and unreliably.
In other words, they are showing
the first nascent signs of danger.
However, a straightforward extrapolation of today's systems
to those we expect to see in two to three years
suggests a substantial risk that AI systems
will be able to fill in all the missing pieces,
enabling many more actors
to carry out large-scale biological attacks.
We believe this represents a grave threat
to U.S. national security.
And later on in the testimony, he said this.
Whatever we do, it has to happen fast.
And I think to focus people's minds on the bio risks,
I would really target 2025, 2026,
maybe even some chance of 2024.
If we don't have things in place
that are restraining what can be done with AI systems,
we're gonna have a really bad time.
And I wrote a question on this too, Sam Oltman,
back in June, which one of my subscribers used and delivered.
There was also a recent research paper
on how researchers from MIT and Harvard
were able to use LLM models.
And within just one hour,
they were able to get access to pandemic-class agents
and with little or no lab training.
And does open AI account for risks
such as these and implications
when curating the datasets for large models?
Yes, we're very nervous about a number of risks,
but biological terror is quite high on the list
and we've been watching what could be possible
with these models.
We go to a number of efforts, like what you said
and many other things too, to reduce the risk there.
And we may even need AI defenses
against synthetic biology,
as Andrew Hessell of Humane Genomics has recently said.
So if you work in biodefense or biosecurity,
let me know if you agree that not enough attention
has been paid to this area.
I'm gonna end with another dramatic moment
from the Senate hearing,
where Dario Amadai recommended securing the supply chain.
We recommend three broad classes of actions.
First, the US must secure the AI supply chain
in order to maintain its lead
while keeping these technologies
out of the hands of bad actors.
This supply chain runs from semiconductor manufacturing
to equipment, to chips,
and even the security of AI models
stored on the servers of companies like ours.
That's how dramatic things are getting
that we're talking about securing the means of production.
But Anthropic also means securing the LLMs more literally
in this post released this week.
They say that we believe two-party control
is necessary to secure advanced AI systems.
For example, that could be two people
with two keys needed to open things.
To wrap up, I must say what would be amazing
would be to have a robot make me coffee
as I struggle to catch up with all the news happening in AI.
Have a wonderful day.

Hello and welcome all of you to math 2050 applied linear algebra.
I think we'll start off by talking about just what is linear algebra.
You actually are familiar with it somewhat.
You've had courses in algebra, I'm sure.
In general, what we think of in particular for this course is that linear algebra is a study
of systems of linear equations.
So that begs the question, what's a linear equation?
Well, a linear equation is one of this form.
a1x1 plus a2x2 plus dot dot dot anxn equals b.
And here the x's are the variables.
The a's are called coefficients.
And b is the right hand side value.
Here all the a's and b are typically real numbers.
And then you ask, well, what's a real number?
Well, a real number is basically you can think of that as any number that you can find on a number line.
So that includes integers, rational numbers, irrational numbers.
Negative numbers, positive numbers, zero.
Just about any number.
Or any number you can find on a number line.
It's a real number.
This is as opposed to complex or imaginary numbers.
Those would not be real numbers.
So for our purposes, we'll not be dealing with complex numbers.
We'll stick to just real numbers.
In our equations, the linear part means that we don't allow nonlinear expressions of the variables.
So terms like x1 squared or x1 times x2, those type terms are nonlinear.
And so we don't allow those type expressions.
So here's a couple of sample linear equations.
Those are just typical.
First one has two variables here.
Here's one with three variables.
This one, first one, you'll recognize, got two variables.
So that just defines a line in the plane, in the xy plane here.
Got three variables.
So that actually defines a plane in three dimensions.
So we'll talk more about that as we go along.
So we're back to what is linear algebra.
So we've talked about what's a linear equation.
And a linear algebra study of systems of linear equations.
So what's a system?
That just means you have multiple equations.
So for example, let's go to the drawing board.
Here's a system of two equations and two unknowns.
And all of you have had college algebra.
So you're familiar with the system of this form.
And you probably learned a couple of different ways to solve this system when you were in algebra.
And so we're just going to talk about that first off here.
If I asked you how would you solve this system, then you might say, well, I'd use substitution.
Because that seems like a way to go.
If you look here, you've got the x2 term.
You can solve this equation for x2.
So let's do that.
So from this equation, I can get x2 equals 28 minus 4x1.
And then I substitute this expression for x2 back into the first equation.
So that gives me 2x1 plus 3 times this stuff here, 28 minus 4x1.
And that equals 24.
So then I just simplify.
2x1 plus 3 times 28 is 84 minus 12x1 equals 24.
So that gives me, it looks like negative 10x1 equals 24 minus 84 would be negative 60.
So it looks like x1 is equal to 6.
And then if x1 is 6, we can go back and plug in here to figure out what x2 is.
So x2 is going to be 28 minus 4 times 6.
So x2 is equal to 4.
So back up here.
So x, do it again.
x2 equals 4, x1 equals 6 is our solution.
So that's one way to solve this system.
That's using the method of substitution.
Another method that's sometimes used is to go back and look at the system
and try to eliminate one of the variables by multiplying one of the equations
or maybe even both the equations by constant and then adding or subtracting one from the other.
So I'm going to try that method here.
So I'm going to multiply this first equation by negative 2.
And I'll just write what I get down here.
So negative 2 times 2x1 gives me negative 4x1.
And then negative 6x2, then negative 2 times 24 would be negative 48.
And then if I add these two together, these two that I have here,
then notice that the x1's cancel out.
So they're gone.
I've eliminated x1 in here.
If I add, I get negative 5x2 equals negative 20.
So that says x2 equals 4.
And then to get x1, I can substitute that back into either of these equations.
And we already know that x1 is 6, so I won't go through that process.
So there we go.
We have our solution.
And that's using a method of elimination.
You systematically eliminate variables.
So those are the two methods that you probably learned back in algebra class
for solving the system.
Let's take a look at what we're doing graphically.
Let's take a graphical look at it.
So I go back to my equations and I draw them on a graph.
Let's see if I can come over here.
And I'll try to do this.
All right.
So if I graph my equations where this is going to be my x1 axis,
this will be my x2 axis.
Then let's see.
If I look at the first equation, if x1 is 0,
well actually I'm going to start with this one because
if x1 is 0 here, then x2 is 28.
So that gets us way up here.
So there's 28.
So I've got one point right there at 0, 28.
Then if x2 is 0, we get x1 equals 7.
So I'm going to screw this over just a little bit.
It's giving me a little more room here.
Let me get this out.
OK.
So it looks like I want to try to get my scale as best I can.
So it looks like this would be about 7.
And so my other point is here at 7, 0.
So I've got one line that comes like this.
Let me see if I can do a better job drawing that.
You're not only on that.
You get a little better angle on that with my computer here.
OK.
Well, that's not going to get either.
Let's try one more time.
Looks like I am not going to be able to make this work.
Let's see one more time.
OK.
I'm going to just call that good.
Let's call this 7 right here.
All right.
So then let's do, let's look at the other one.
The other equation is I've got this one.
If x1 is 0, it looks like x2 is 8.
So that's going to be along about right there.
And if x2 is 0, it looks like x1 is 12.
So that's going to be somewhere along about right here.
Let me try once again and see if I can make a line.
That one's not terrible there.
All right.
So this is about 7 right there.
Let's take that away.
All right.
So if we look, my scale is better.
It would be better to see.
But along about there, that's the point where those lines intersect.
And so that's the point 6, 4.
So my scale is better.
It would look better.
But you get the general idea.
So what we're looking at here is we've got this system.
And we want to find x1 and x2 that satisfy both those equations.
So since each of those equations represents a line,
then when we're looking for a point that satisfies both,
then that means it's a point that's on both lines.
So that's the point where the lines intersect.
OK.
So in this example, we ended up with a unique solution.
All right.
A unique solution.
That means exactly one solution.
OK.
Now let's think.
Are there any other possibilities?
No matter what your system is, you're always going to have exactly one solution.
I think you know that that is not the case.
So what are the possibilities?
You don't have exactly one solution.
How about do you have a solution every time?
And you probably know that the answer is no.
And that happens when you have a situation like this.
There's one line.
There's another.
They're parallel.
And so they never intersect.
So in this case, you have no solution.
No solution in that case.
OK.
So we've got zero solutions.
You have one solution.
How about two?
Can you come up with a system where you have two solutions?
Let's think on that.
Two solutions.
Now in my class one day, I had somebody say,
well, what if you drew another line here?
So you had a line like this.
Then you got the intersect there, the intersect there.
So there's two solutions.
And the answer is, well, no, because here you've got three lines.
And so if you were trying to solve a system with these three lines,
then that means that you need to find a point
where all three intersect at the same time.
So in this case, there's still no solution
because there's no point at which all three lines intersect.
I've also had students say, well, hey,
what if you had a parabola?
So you had a case like this.
There's a parabola.
You got a line going through it.
There's the intersect right there, intersect right there.
So you got two solutions.
Now this is legitimate because you got two functions
and the intersect at these two places.
The problem is that this parabola is not a linear function.
The parabola is quadratic, not linear.
So that case doesn't work either.
So it turns out that if you have a system of linear equations
and you don't have no solution, you don't have exactly one solution,
then you must have an infinite number of solutions.
So infinite number of solutions.
And in two dimensions, that's a little bit tough to come up with a good example.
The best you can do is two dimensions.
It is like one like this.
We have something like 2x1 plus 3x2 equals 10.
And then maybe 4x1 plus 6x2 equals 20.
Now on the surface, it looks like you got two equations.
But when you go to graph this, what do you find?
Well, you find that this is actually the same equation
because I wrote it so that the second one was just a multiple of the first.
So when you draw it, if you were drawing the graph, whatever it might look like,
you graph one of the lines and then you graph the other one
and it sits right on top of the first one.
So that, in that case, any point on the line would be a solution.
And so there would be an infinite number of solutions.
Okay, so just to recap, let's go back to our slide.
Let's go to the next one.
So these are your possibilities.
A system of equations has either no solution, exactly one solution,
in which case we say it's a unique solution,
or it has an infinite number of solutions.
And a little more terminology for you.
We say a system is consistent.
A consistent is a word we'll use a lot.
So a system is consistent if it has at least one solution.
So that means either unique or an infinite number,
but it has at least one solution.
It's consistent.
Otherwise, we say it's inconsistent.
So inconsistent means that it has no solution at all.
Okay, so I think we'll stop there for this one
and we'll pick up with this on the next video.
Okay, we're back now.
We're talking about solving systems of equations.
And we need a systematic method,
because probably as you can recall from your days in college algebra,
once you get up to even a three by three system,
then things start to get a little messy.
And so we're going to talk now about how to a systematic method.
A method that will work no matter what size your system,
no matter how many rows, how many columns,
or how many variables, how many equations.
Let's say an algorithm that will work no matter what.
Before we do that, I want to go back and look at that system
that we were talking about earlier, which I have on my notepad here.
So remember, I just want to refresh your memory about what we did there.
The second way we used to solve this system, if you recall,
was we decided to multiply this equation by negative two.
And then we added that to the second equation.
So let's just go through that quickly again.
So we end up with negative 4x1 minus 6x2 equals negative 48.
And then we added that.
So the x1's canceled out.
Then we ended up with negative 5x2 here equals negative 20.
So x2 is equal to 4.
And then we didn't actually do it because we've done it before,
but I'll go through.
At this point to find x1, you have to plug back in.
You can plug into either one.
Let's just go back to the first one.
The first equation will have 2x1 plus 3 times x2.
And we know that x2 is 4.
So we plug that in equals 24.
So we get 2x1 equals, looks like 12.
So x1 equals 6.
So there's our solution that we found in the previous video.
To do this in a systematic way, we set up a matrix.
And a matrix is just a rectangular array of numbers.
So in this case, we would be interested in this matrix
where I take the coefficients.
So I just write it like that.
The first row here represents the first equation.
The second row represents the second equation.
This matrix is called an augmented matrix.
And it's called augmented because it has this right-hand side column included.
A lot of times, we're only interested in this matrix,
which we call the coefficient matrix, since it includes only the coefficients of the variables.
So that's coefficient matrix.
If we augment or add on the right-hand side, then we get the augmented matrix, which is this one.
And this is actually the one that we'll work on most of the time when we're solving.
If we want to solve a particular system, then we would work with the augmented matrix.
Now, what do we do with the augmented matrix?
Well, it turns out that there are three row operations,
three operations in our toolbox that we can perform on this matrix in order to solve the system.
So to do that, let's go back to our slides here.
And here we'll talk about our elementary row operations.
So number one is to interchange or swap two rows.
So that since a row in a matrix represents an equation,
that just means we're reordering the equations, which as you know has no effect on the solution.
We could also multiply all the entries in a particular row by a non-zero constant.
No, I can't multiply by zero because that would be essentially throwing out that equation, and that wouldn't work.
But you can multiply by any non-zero constant.
And the other is sounds kind of complicated, but it's really not.
And really this third one is the one that you'll do 99% of the time.
And it is the replacement operation.
It's where you replace one row by the sum of itself and that should be a multiple of another row.
So you replace one row by the sum of itself and a multiple of another row.
That's an error right there.
So let's talk about how you do that.
Let's go back to our example.
And that operation is actually what we did here.
If you go back and look, you can think of it as we were replacing the second equation by the sum of itself and a multiple of the first row.
So we multiply the first row, took a multiple of the first row, added it to the second.
And so we ended up with this new equation right here.
So we're going to do the same thing except in matrix form.
And I like to use a shorthand when I'm doing these operations.
And so for the replacement operation, I write it like this.
I'm going to multiply negative two times row one and add that to row two.
So negative two times row one, added to row two.
Now this doesn't change row one.
So I'm just going to write that just as I normally would.
No change there.
But row two is going to change.
So I'm going to multiply each entry in row one by negative two and add it to the corresponding entry in row two.
So negative two times two is negative four plus four.
That gives me zero.
So I've eliminated x one from this row.
Then negative two times three is negative six plus one is negative five.
Okay, so just a reminder there, how did I get that?
I multiplied negative two times three and added it to one.
So negative two times this entry plus one.
Then the same thing in the third column.
So negative two times negative 24 is negative 48 plus 28 is negative 20.
So here I'll get negative 20 and just make that clear.
That's negative two times 24 plus 28.
Okay, and so you can see if you wrote out the equation that corresponded to this row,
it looks exactly like this equation here that we ended up with.
And so at this point, now I have in my second row only one variable and so I can solve.
So I would say, okay, from row two I get negative five x two equals negative 20.
So that means x two equals four.
Okay, just like I did before.
And then we do something, a little more space here.
Then we do a process called back substitution.
Okay, and that just means you back up to the next row and you substitute.
So if we back up to the next row, then that would be backing up to the first row.
And there I've got two x one plus three x two.
But now I know what x two is.
It's four.
So that's the substitution part.
Okay, so I plug that in equals 24.
And we know that we end up with x one equals six at this point.
Okay, so here we solve the system using elementary row operations and then using back substitution.
Let's see, just one more note on why this is okay to do this.
Let's go back to our slides.
A little more terminology for you.
We say two matrices are row equivalent if there is a sequence of elementary row operations that transforms one matrix into the other.
So the way I usually say that is they're row equivalent if you can get from one to the other by doing elementary row operations.
Okay, doesn't mean they're equal.
Row equivalent does not mean the matrices are equal.
It just means that you can get from one to the other by doing elementary row operations.
Okay, and then here's the clincher.
That's why this works.
So remember our goal is to solve a system.
And so we need to know that when we do these row operations, we're not changing the solution or the solution set.
Okay, and so this statement gives us that assurance.
It says if the augmented matrices of two systems are row equivalent, then the two systems have the same solution set.
So that means we can start off with one matrix.
We can do row operations and get to another one that we can solve easily and know that the solution that we get based on the last matrix will be the same for the first one.
So that's why we can do these row operations in order to solve a system of equations.
This leads us to two questions.
The first one is, is the system consistent?
Does it have at least one solution?
And if that's so, if it's consistent, then is that solution unique or are there an infinite number of solutions?
So we'll talk about how to determine.
Number one is the system consistent.
Number two, if it is, then how do you know if there's a unique solution or an infinite number of solutions?
Okay, we're going to stop here and I'll do one more video that has some problems worked out for you.
So that'll be next.
Okay, now I want to do a few sample problems from section 1.1 of the textbook.
So first let's look at number 14.
So in the problem, they're giving you a system of equations and asking you to solve it.
So I've already got it here in the augmented matrix form.
And so we just need to do elementary row operations in order to try to solve the system.
So the first thing we want to do is make sure that up here in the 1-1 position, that's the first row, first column, we've got a non-zero value if possible.
Well, it's already non-zero, so we don't need to do anything.
If this had been a zero, we would want to swap rows in order to get a non-zero value here.
Okay, we already have one, so then we can next turn our attention to creating zeros in the column below this leading entry.
Okay, a leading entry is the first non-zero value in a row, so we want to create zeros below it.
So we need to create a zero in this position here.
So we need to figure out what do we multiply by this one and add to negative one to get zero.
And clearly, multiply by one or don't multiply by anything, we really just need to add the first row to the second and that will give us a zero here.
So I'm going to add row one to row two.
Now that does not change row one, so I'm just going to repeat it here.
And it doesn't change row three.
All right, so now row two, we've got a zero there.
Then we'll have negative three plus one gives us negative two.
Zero plus five is five.
And five plus two gives us seven here.
All right.
So now we've got our leading entry zeros below it.
We're done with that column, so we move on to the next column and down a row.
So we want a non-zero element here, which we already have.
And normally, we would just want to zero out below it.
But given that we have a one here, that's kind of a nicer number to work with.
So I'm going to actually swap rows two and three at this point.
Swap row two and row three.
And so row one doesn't change.
Nero two is the old row three.
And the nero three is the old row two.
All right, so I've got that.
Now the reason I did that was because this just makes the arithmetic easier
and I don't have to deal with fractions.
Otherwise, if I left the negative two there, I'd have to figure out,
you know, what do I multiply by this to add to one to get zero?
And I would have to carry that out through the rest of that row.
So this is just easier arithmetic wise.
So now I've got a one here and I need to zero out underneath it.
So I need to multiply by two and add the negative two to get a zero here.
So my next operation is two times row two added to row three.
So row one again doesn't change.
Row two doesn't change.
And row three, let's get my zero there and I'll get two.
So we're here two times one plus five is seven.
Then we've got two times zero plus seven is seven.
Okay, now at this point I'm done with the second column and I move over
and I'm done with the matrix because now in the third row
I've eliminated all the variables except for one.
So from the third row, I get seven x three equals seven.
So I can solve for x three.
It's going to be one.
And then I do the back substitution.
All right, so that means back up.
So back up to the second row, which looks like if I write down the equation
corresponding to the second row, the x two plus x three equals zero.
But I know what x three is, so I plug that in.
And I've got x two equals negative one.
Okay, now back up again to the first row.
The first row, that equation looks like x one minus three x two equals five.
I know what x two is.
Plug that in.
So a little more room here.
And so I get x one is equal to, let's see, that's a plus three.
So we're going over minus three, so it would be two.
So x one equals two, x two equals negative one, x three equals one.
There's a solution to my system.
Okay, so this system here had a unique solution, just one.
All right, let's go on and look at another homework problem.
This time I want to go to number 12.
Okay, so I'm still in section 1.1, problem number 12.
Actually, before we do that, let's look at one thing.
Before we do that, let's look at one other thing.
Suppose I have a system that looks like this.
Two x one plus three x two equals ten.
And how about four x one plus six x two equals 12.
Now, if you examine that a little bit, you're going to see that to get this second equation,
I just multiply the first one by two, right, so two times two is four, two times three is six.
But then I didn't multiply this by two.
Actually, I did, and then I didn't want to get that same value, so I changed it.
So just based on that analysis, it seems that this system should have no solution, right?
And if you were solving it using elimination, say, maybe you'd multiply,
maybe you didn't notice that it's not going to have a solution.
So you're just merely going along your way solving it.
So if we multiply three by negative two, we're going to get negative four x one minus six x two
equals negative two times ten, so negative twenty.
Then we'd add that, and here we get zero.
Here we get zero.
So on the left, we've got zero.
And on the right, what do we have?
Negative eight.
Well, zero can't equal negative eight, right?
So this is what tells us that this system has no solution.
We get to this point where we have zero on the left and something not zero on the right.
We've got a non-zero value over here on the right.
So if we had thrown that into a matrix, it would look like this.
So we do one row operation, same one we did up there.
Negative two, row one plus row two.
So row one doesn't change.
And row two, get negative two times two is negative four plus four gives us zero.
Negative two times three is negative six plus six gives me zero.
Negative two times ten is negative twenty plus twelve is negative eight.
So if you look at this, we're the same thing, right?
We've got all zeros.
And over here on the right, we've got something that's not zero.
This is the marker that you look for to indicate no solution.
All zeros on the left, something not zero on the right.
Keep that in mind.
We're going to be using that all semester.
Okay, so now let's go to number twelve.
Okay, still in section one point one.
So number twelve, one negative three, four negative four.
Negative seven, seven.
Negative eight.
Negative four, six, negative one.
Seven.
Okay, and this is an augmented matrix.
So we start, we do our row operations.
So we look, we've got a non-zero value here, so we want to zero out underneath.
So the first thing I'm going to do is negative three times row one plus row two,
so that I can generate a zero there.
So I've got one, negative three, four, negative four.
The first row and the third row don't change.
And this one, I'm going to get my zero there.
Negative three times negative three is nine, minus seven gives me two there.
Negative three times four is negative twelve, plus seven is negative five.
Negative three times negative four is twelve, minus eight is four.
Okay, so I've got my zero there.
I need to keep working down, get a zero in this position.
So the next operation I'm going to do will be four times row one plus row three.
So again, row one doesn't change.
Row two doesn't change.
And row three, I'll get my zero there.
So four minus four gives me zero.
Four times negative three is negative twelve, plus six is negative six.
Four times four is sixteen, minus one is fifteen.
Four times negative four is negative sixteen, plus seven is negative nine.
So that first column is looking good.
Then I move over to the second column, second row.
I've got a non-zero value here.
So I want a zero out underneath, so I need to work on that sixth there.
Okay, so I'm going to multiply row two by three, and add that to row three,
because three times two is six, minus six gives me the zero that I need right here.
So one, negative three, four, negative four, row two doesn't change.
And row three, it's already zero, so three times zero plus zero zero,
three times two is six, minus six, there's my new zero.
Three times negative five is negative fifteen, plus fifteen is zero.
Okay, and remember we're looking, this point you can see we've got all zeros here.
Okay, and then we're ready to get the last position, and what do we have here?
Three times four is twelve, minus nine gives us three.
So what do we have here?
We've got all zeros on the left, and on the right we've got a value that's not zero.
So as we saw before, that means this system has no solution.
Okay, I want to do one more problem from this section.
And this one is, let's see, actually maybe a couple more.
About, let's look at number, let's see, number twenty-two.
Twenty-two, twenty-two looks like this.
Okay, we put it in the matrix form.
Okay, now the directions ask you to find the values of h, okay, this parameter,
for which the system is consistent.
Okay, so to do that, you want to just get an echelon form.
Actually, we don't know what echelon form is yet.
Let's just do what we've been doing.
So, we've got our non-zero value here.
We zero out underneath it, so multiply by three.
So I'm going to do three times row one plus row two, so that gives me zero there.
This doesn't change.
Okay, so three times two is six, minus six gives me zero here.
Three times negative three is negative nine plus nine gives me zero here.
And three times h plus five is just three times h plus five.
Alright, now at this point, it looks just like the one previous, right?
Look back at the one we were just looking at.
We had all zeros and then something not zero.
Well, here we have all zeros, guys.
That's significant.
But over here, we don't know what this is because we're asked to find h so that this system is consistent.
So we want this to, we want three h plus five must equal zero for the system to be consistent.
Okay, so that means that three h must equal negative five.
So h has to equal negative five-thirds.
Okay, so that's what we're looking for for this one.
Alright, because this has to equal zero for the system to be consistent.
Alright, because if this is not zero, we're like this case up here where there's no solution.
Alright, let's look at one more.
And I'm kind of going out of order here just because I like kind of stick with the same thing.
So I'm going to back up to number 20 and it's a similar kind of question as 22.
But this time the h is not on the right side.
It's part of the coefficient matrix.
So it's a little bit different.
So again, we do our row operation.
So we got a non-zero here.
We want to zero out this position.
Two times row one plus row two.
So I've got one h negative three.
And I've got zero here.
Two times h plus four.
And two times negative three is negative six plus six is zero.
Alright, so I've got this situation.
So the problem is to find h such that the system is consistent.
Well, system is consistent as long as we don't have a case like we had up here.
System is consistent as long as we don't have a row that looks like zero, zero, and then something not zero.
So notice that that can never happen.
We can never have zero, zero, something not zero because the right hand side is already zero.
So that means this system is always consistent.
Because we can never have a row of the form zero, zero, and then on the augmented side something not zero.
It can't happen in this case because the right hand side is a zero.
So this system is always consistent regardless of what h is.
Alright, so I think that's about it for section 1.1.
As always, keep in mind that you can call me up, email me anytime to let me know if you have any questions.
Alright, good luck with those homework problems.
Okay, today we're going to be talking about section 1.2.
And really the main thing I want to cover today is to talk about a specific form that we want to get a matrix into when we're trying to solve a system.
So we've kind of talked about that a little bit up to now, but at this point we're going to get very specific about that.
So the form that we're trying to get at is called echelon form.
And echelon form is defined by three properties.
So the first one of these is that if there are any rows of all zeros, they're at the bottom of the matrix.
So you swap rows to get those at the bottom.
The leading entry, remember that's the first non-zero entry.
So the leading entry in each row is in a column to the right of the leading entry in the row above it.
And I'll show you that in just a sec.
The third one is that all entries in a column below a leading entry are zeros.
So we've done that up to now.
We get our leading entry and then we zero out below it.
So that's that idea.
Here's an example.
The black squares here represent non-zero values and they're the leading entries in each of these rows.
And so you can see here we have a leading entry and zeros underneath.
Here we have another leading entry, zeros underneath.
And notice that this leading entry is to the right of the one before it.
And here's another leading entry and it's to the right of the one that came before it.
Okay, so that second property is basically what gets you the stair step kind of structure in your matrix.
All right, see, here's another one.
This one's also an echelon form.
Leading entry here, zeros below it.
Another leading entry, zeros below it and it's to the right of the previous one.
And we have a row zeros and it's at the bottom.
So we satisfy all the criteria.
Now, sometimes you want to go a little bit farther and get your matrix in reduced echelon form.
So if it's in reduced echelon form, that means it's an echelon form plus it satisfies two more properties.
Okay, and these properties are that each leading entry is a one.
So you scale the rows to make each leading entry equal to one.
And that one is the only non-zero entry in its column.
So you zero out not only below it, but also above it.
So here's an example of a matrix that's in reduced echelon form.
Each leading entry is a one and it's the only non-zero entry in its column.
So we zeroed out below here, zeroed out above and below and zeroed out above in this case.
Alright, here's another one.
This one's in reduced echelon form because each leading entry, we've only got two and they're each one.
And we've zeroed out above and below each one.
Zero zeros at the bottom.
Okay, so that's in reduced echelon form.
Now for purposes of just solving a system, typically you just want to get it in echelon form.
Although there will be one particular case that I'll talk about later in another section coming up
where you want to get it in reduced echelon form.
It just makes life easier then.
But for the most part at this point, echelon form is fine.
Now I'll show you a couple of things about these forms.
One thing to note is that the reduced echelon form of a matrix is unique.
So that means that no matter what sequence of row operations you use to get a matrix in reduced echelon form,
you'll end up at the same place.
Not so for the echelon form.
Okay, it's not unique.
It's that all of you could be working on the same matrix and end up with a different echelon form of the same matrix.
And that's because you can scale a row, multiply a row by a constant and get a different echelon form.
Still be an echelon form, but it would be a different matrix.
Okay, so the reduced form is unique.
The echelon form is not.
Okay, so you either want to get your matrix in echelon or reduced echelon form when you're solving a system.
And the latter, get it in reduced echelon form, takes more work.
But when you do that, the solution is obvious.
And so let me show you what I mean by that.
Here we have an augmented matrix that's already in echelon form.
Notice leading entry, zero's underneath.
Another leading entry, zero's underneath.
And no rows of zero's.
Each leading entry is to the right of the one before it.
So it's an echelon form.
And then we do use back substitution to solve for the variable.
So start with the third row and equation for the third row, we get seven times x3 equals seven.
So we solve for x3.
Then we back substitute to solve for x2 in the second row.
So second row says negative two times x2 plus five x3 equals seven.
And we know that x3 is one.
We just solved for that.
So we plug that in here and we end up with x2 equals negative one.
And then we keep going back up again to the first row.
Equation for the first row looks like x1 minus three x2 equals five.
And we know what x2 is because we just solved for that.
So we plug it in and then we end up with x1 equals two.
So that's the method that we were using in section 1.1 and probably what you want to continue using for the most part.
However, if you choose to get your matrix in reduced echelon form.
So this is the same matrix but in reduced echelon form.
Then notice that you end up with the solution values of your variables just sitting in the right hand column.
Because if you look at each row, the first row says one times x1 equals two.
Second row, one times x2 equals negative one and so forth.
So if you go back up, you can see we got x1 is two, x2 negative one, one.
So two negative one, one.
And there's two negative one, one.
So that's one advantage of getting it in reduced echelon form.
Because then the solution is obvious and you don't have to actually do any back substitution.
So it's kind of your mileage may vary.
Whichever you like will be fine.
At this point, we'll stop here and we'll pick up here in the next video.
Okay, so picking up from the previous video.
Let's talk a little bit about a couple more pieces of terminology that we'll be using extensively throughout the course.
Okay, so remember the leading entry is the first non-zero entry in a row in a matrix in echelon form.
So it turns out that the leading entries in the reduced echelon form and in any echelon form are in the same positions.
I mean the same location in the matrix.
So here's the one we were just looking at.
There was a leading entry in the one, one first row, first column, second row, second column, third row, third column.
And in the reduced echelon form, same positions in the matrix.
So it's convenient to talk about those positions as opposed to the actual value.
The leading entry is the actual value, but we really are more interested in the position.
And so we call that a pivot position.
So pivot position is a position that contains a leading entry.
So the one, one, two, two, three, three positions would be pivot positions in this matrix.
As I said, the leading entry is the actual value.
The pivot position is the location.
So the leading entry here is one.
It's in the first row, first column.
So that's the pivot position.
Here, this row, the leading entry is negative two.
It's in the second row, second column position.
Okay, and then one other term is pivot column.
Pivot column is a column that contains a pivot position.
So I'll be using the terms pivot position and pivot column until probably the last day of the semester.
So you will hear that quite a bit to make sure you understand what those are.
Two more terms.
A basic variable is one that corresponds to a pivot column.
And a free variable is one that corresponds to a non-pivot column.
Okay, so for each column, except for the augmented column, there's a corresponding variable.
So if you look and there's a pivot column, that means that variable is basic.
If the column is not a pivot column, that means that variable is called free.
Okay, so let's look at how all this relates to solving the system of equation.
So I've got a tail of three matrices.
Here's number one.
Now notice this matrix.
It's already an echelon form, right?
Here's leading entry, zero's underneath, leading entry, zero's underneath, leading entry.
You don't need to worry about zero's underneath yet.
So we start the process of solving the system.
So we start with row three.
And here's the equation that corresponds to row three.
And if I simplify the left side, it's just zero's when it was zero equals seven.
Well, we know that can't happen.
So that is our clue that this system has no solution.
So if the echelon form of an augmented matrix has a row of this form,
where on the left you've got all zero's, then in the augmented column you have something not zero.
So all zero's on the left, not zero on the right.
That's what we have here.
All zero's on the left, not zero on the right.
Then the corresponding system has no solution.
Okay, so that's the marker.
That's what you need to look for to see if your system is consistent or not.
All right, matrix number two.
Change that first one just a little bit.
And I put a non-zero value here.
And so we've got leading entry, zero's underneath, leading entry, zero's underneath.
Another leading entry.
So this one's in echelon form.
So we solve, and this is the same one we solved earlier.
So I'm not going to go back through that.
And this was our solution.
And so this one, what?
Has a unique solution.
The system is consistent.
It's consistent because it doesn't have a row where there's all zero's and then something not zero.
And it has no free variables.
That's the key here for unique solution.
Notice that there's a pivot position in every column of the coefficient part of the matrix.
So pivot position in column one, so x1 is basic.
Pivot position in column two, so x2 is basic.
Pivot position in column three, so x3 is basic.
So system is consistent.
All the variables are basic.
It has no free variables.
And so we get a unique solution.
One more matrix.
Notice here that we've got a row of all zero's.
This one is in echelon form, but we've got a row of all zero's.
Let's look for just a second.
x1 here has a pivot position.
The column two has a pivot position.
Column three does not have a pivot position.
So that means that we have a free variable.
x1 and x2 are basic, but x3 is a free variable because there's no pivot position in column three.
So if we try to solve it, here's what we get from row two.
Negative two x2 plus five x3 equals seven.
We solve for x2 and notice that it's written in terms of x3.
And then we back up to row one.
And that x1 minus three x2 equals five.
We substitute in for x2.
And here's what we end up with for x1.
Notice it's also written in terms of x3.
Notice that that's all we've got, these two equations.
And so there's nothing constraining x3.
And that's because x3 is a free variable.
And so if we write our solution, here's what it looks like.
And in your book, that's what they call the general form of the solution.
So this is just for x1, x2, what we just computed, x3 is free.
This is the general form of the solution.
So notice that we can plug in any value for x3 that we want.
And any value of x3 produces a different solution.
So that means we have an infinite number of solutions to this system.
The easiest thing, if you want a specific solution,
the easiest thing is to just plug in zero for x3.
If you do that, then you get x1's negative eleven-halves,
x2 negative seven-halves, x3 is zero.
Or you could plug in x3 equals one and get another solution.
I always tell my students, plug in your favorite number.
So if your favorite number is pi, plug in pi for x3.
And if you do that, then here's what you get for x1 and x2.
So no matter what value of x3 you plug in,
that generates a different solution.
And so you have an infinite number of solutions.
And so the bottom line is that if the system is consistent,
and that's a key, you've got to make sure it's consistent to start with,
and has at least one free variable,
then you will have an infinite number of solutions.
So let's recap.
When you're solving a system of equations,
number one, put the augmented matrix in echelon form.
Number two, if you have a row of this form,
and this form is, again, all zeros on the left,
something not zero in the augmented column,
then that means the corresponding system has no solution.
At that point, stop because you've got no solution.
If that's not the case, then that means the system is consistent.
So at that point, you look to see if there are free variables or not.
If there are no free variables, that means the system has a unique solution.
If it's consistent and there's at least one free variable,
then you have an infinite number of solutions.
So that's really your algorithm there for how to determine,
how to solve a system and determine which case it is.
Is it no solution, is it a unique solution,
or an infinite number of solutions?
Okay, and that is it for this video.
All right, we're going to start here with a graphical look at vectors.
So we've kind of seen, we've played around with matrices
and augmented matrices and solving systems.
But let's take a look at vectors from a graphical point of view.
Before we do that, though, we need to discuss some terminologies.
We'll start with Rn.
So you've seen this symbol, the fancy looking R.
That stands for the real numbers.
And when you see a superscript, like here with an N, excuse me,
that indicates that you're talking about vectors.
And so this, as I say here, you just read this as Rn.
Okay, so Rn is the set of all vectors having N components or N elements,
each of which is a real number.
So from a set point of view, you can look at it like this, set notation.
All vectors look like this N components, where each one of them is a real number.
A lot of times we'll just talk about R2, excuse me.
In R2, the two indicates vectors with two components, so there you go.
And there's R3, similar sort of thing.
We've got three components where each one is a real number.
So for example, here are three vectors in R2.
You know they're in R2 because they're vectors with two components, each components are real number.
And here they're vectors in R3 because they have three components.
There are two operations that we need to know how to perform on vectors.
The first of these is addition of vectors.
So if we have two vectors, X and Y and Rn, then we can compute their sum as indicated here.
Okay, here's X, here's Y, both have N components.
To compute their sum, you just add components that are in the same position.
So first position X1 plus Y1 gives you the first position in the sum.
Then X2 plus Y2 and so forth to Xn plus Yn.
So for example, if here's X and here's Y, to get their sum, 0 plus 2 gives you the 2 here, 1 plus 4, 5,
and negative 3 plus negative 2 gives you negative 5.
Another operation that we need to know how to perform on vectors is scalar multiplication.
Excuse me again.
Scalar multiplication of vectors is based on the idea that you have a scalar,
which is just a real number, a single number, that you want to multiply by a vector.
And so the way you do that is indicated here.
You just multiply that number by each element in the vector.
So we get CX1, CX2, down to CXn.
So for example, here's your X, you want to multiply 4 times X,
then you just multiply each component in the vector by 4.
Okay, now with that, we can start to talk about a graphical representation of vectors
and we'll restrict our attention at first, at least to R2.
Pardon me.
A vector in R2, also called the plane, you're probably familiar with that terminology,
is indicated by a ray that begins at the origin and terminates at the point defined by the vector.
So here's a couple of vectors and you can see how they look on a graph.
Okay, so U is 2, 1, so we have a ray that begins at 0, 0 and terminates at 2, 1, similarly for V.
So analytically, we know we can compute U plus V in this fashion.
Just sum the like terms, 2 plus 1, 1 plus 4, give you 3, 5.
Graphically, we use the parallelogram method.
You might be familiar with this from your physics class or engineering or maybe even like Calc 3 if you had that.
So the way that works is you go to the end of one of the vectors and you draw a line that's parallel to the other one.
Okay, so this line here would be parallel to V and we do the same thing for the other vectors.
So go to the end of V, draw a line that's parallel to U, this line is parallel to U.
And where those two lines intersect, that is the sum of the two vectors.
So this vector here would be U plus V and you can see that it's at 3, 5.
Scalar multiple of a vector.
I did a couple of examples based on those.
We were just looking at it, 3 times 2, 1 is just 6, 3 and here's a half of V.
Look at that graphically.
3 times U is a vector that's in the same direction as U and it's just 3 times as long.
So basically you got 3 U's stacked there and so you can see that it corresponds to 6, 3.
And 1 half of V is what you would think, 1 half of V.
So you can see that it's a vector in the same direction as V but half the length.
We can add those two together.
Alright, so this is how we do it.
3 times U plus a half V.
We already know how to do these operations.
And graphically take our 1 half V, align parallel to 3 U.
Similarly over here and the resultant vector here is what we think it should be, about 13 halves, 5.
Okay, so notice that you can scale U to whatever length you want.
You can scale V to whatever length you want and then add those two vectors together.
And if you do that, then think about where the vectors would end up.
Any vectors that you can scale U however you want, scale V however you want and then add them together.
And let's think about that.
So this would be any multiple of U, any multiple of V.
And for the moment let's restrict our attention to positive multiples or non-negative multiples.
Okay, so let's assume that C and D are both non-negative.
If we do that, then the vectors that we can produce or generate are here in this cone defined by U and V.
Alright, because again we can scale U to whatever length we want.
We can scale V to whatever length we want.
And then when we complete the parallelogram, then we can end up with any vector in this cone area here.
If you allow yourself to take negative multiples of U and V, then think about where those vectors would be in the plane.
The easiest way I think to think about this is to, instead of thinking about taking negative multiples of U and V,
about thinking of it as taking positive multiples of negative U and negative V.
Alright, and if you do that, then it's a similar thing as to what you just saw.
Okay, so here is negative V, here's negative U, and so when you take positive multiples of U and V,
or positive multiples of negative U and negative V, and then add those two vectors together,
you end up with vectors in this region here.
Okay, so other possibilities for C and D, let's think about that.
If we allow positive multiples of U and negative multiples of V, then we're in this yellow region here.
And if we allow positive multiples of V but negative multiples of U, then we're in this region here.
And so what you see is that we can generate any vector in the plane that we'd like just by taking a multiple of U and a multiple of V and adding them together.
Okay, so we're going to see in a little bit that this means that U and V span the plane.
So we can take multiples of each one of those and add them together and generate any vector in the plane.
That's what it means to span R2.
Alright, so I'm going to stop here for now, and we'll pick up this in our next.
Okay, we're back where we left off of the first video, and we were looking at this picture where we determined that based on our vectors U and V here,
we could take multiples of each one of those and add them together to produce any vector in the plane.
So any vector in R2 on the plane can be written in the form C times U plus D times V for some values of C and D.
Let's look at that analytically now.
Okay, so according to our graphical argument, this system is consistent for every B in R2.
You can take some multiple of U, some multiple of V, add them together and get any vector in R2.
So if we break that down, we can multiply C times U, this is U, this is V, there's our generic right-hand side vector.
We multiply it through by the scalars, we end up with this, and then adding these two together gives us this matrix here.
Now, what does it mean for two vectors to be equal?
Well, it means that component-wise they're equal.
So the first component here is equal to the first component here, and likewise the second component.
So what we have is a system of equations, 2C plus D equals B1, C plus 4D equals B2.
So we can put that in an augmented matrix, and that's what I've got here, and notice that, again, according to our graphical argument,
our picture that we looked at, this system corresponding to this augmented matrix is consistent no matter what B1 and B2 are.
And also notice that these are equivalent.
So when we have a multiple of a vector plus another multiple of a vector, then equals some right-hand side vector.
Notice how this system relates to this augmented matrix.
So the first vector here just goes in the first column, second vector goes in the second column, and the right-hand side is just in the right-hand side column.
So these are equivalent.
Okay, let's look at this system a little more closely.
I swapped rows to make the arithmetic a little easier to get the one in that pivot position here.
We can do a row operation to zero out underneath, and we end up with this matrix.
Now remember, this has a solution no matter what the right-hand side is.
Okay, now how do we know that that's the case?
We know it graphically, but how can we look at this augmented matrix and tell that?
Well notice that there's no way to have a row of this form, 00 something not zero form.
And that's because in each row we have a pivot position.
We have a pivot position here, so that's never going to be zero.
We have a pivot position here, so that's never going to be zero.
So there's no way to have a row of this form here.
Okay, so the bottom line is that your system will be consistent, no matter what the right-hand side is,
if you have a pivot position in every row of the coefficient matrix.
Okay, now I'm going to talk about the coefficient matrix because I'm talking about just this part here,
not looking at the right-hand side.
You don't want your right-hand side to be a to have a pivot position because then that would indicate the system's inconsistent.
Alright, let's kind of take what we've looked at so far and put it in the terminology of the text.
Okay, so if we start off with an arbitrary set of vectors v1 through vp and scalars c1 through cp,
then if we apply the scalars to the vectors and add them up, and that's what we were doing earlier,
we only had two vectors, but here we've got an arbitrary number of vectors.
This is called a linear combination of the v's with weights c1 through cp.
Okay, so when we've been multiplying by a scalar and adding vectors together, we were taking linear combinations.
So a linear combination just looks like this.
You've got a scalar, multiply times a vector, and you have that for however many vectors you have, and then you add them all up.
That's what a linear combination is.
Okay, so we were doing c times u plus v times v.
That was a linear combination of u and v where c and d were the weights we were using.
Okay, now here's a linear combination.
Here the x's are being the weights and the a's are vectors.
Okay, so this is x1a1 plus x2a2 and so forth.
Oh, that should be xn an, and that has the same solution set as the system given by this augmented matrix.
Now if you remember before, we started off with an equation like this.
We just had two vectors, but then when we got it into matrix form, remember I showed you, you take your first vector here.
It goes in the first column in the augmented matrix.
The second vector goes in the second column and so forth.
The last vector goes in the last column and then you've got your right hand side.
Here we see that.
We've got u ended up c times u plus d times v, and we ended up with that augmented matrix that we saw earlier.
Okay, one more term, and I used this in the previous video, but we'll define it more formally here.
The set of all linear combinations of the set of vectors v1 through vp in Rn is denoted by the span of v1 through vp.
Okay, so set of all linear combinations of vectors is the span of that set of vectors.
And the span of that set of vectors is just called the subset that is spanned or generated by those vectors.
So if you recall back when we were looking at our example, I talked about how you could span my two vectors, u and v, that we had.
They spanned R2.
We're going to look at that in just a sec.
Here's another way of looking at it.
The span of v1 through vp is all vectors that can be written in this form.
What's this form?
It's just linear combinations.
Okay, so the span is just a set of all linear combinations of those vectors.
And here's the previous example.
We've got c times, this is u, plus d times v equals v1, v2.
Okay, so that means that these vectors span R2.
Let's back up just a little bit.
What if we just looked at the span of a single vector?
What would we get there?
Well, back to the definition, all linear combinations of that vector and linear combinations of just one vector means just all multiples of that vector.
And so graphically what that is is just the line that goes through the origin and that vector.
It's just all multiples of u.
So anything on this line.
Here's another thing to think about.
The span of the vector 2, 1 is the same as the span of this set of vectors.
Now why would that be?
Well, here's definition.
A span of 2, 1 is just all multiples of 2, 1.
The span of the two of them is all linear combinations of the two.
Now why would these two sets be the same?
Well, the key is to notice that 4, 2 is a multiple of 2, 1.
So it really doesn't add anything because you can take a multiple of 2, 1 to get 4, 2.
So by adding 4, 2 to the set as I did here, it doesn't get you any more vectors.
And so you have to move off that line defined by the vector 2, 1 to generate more vectors.
That's in our example before we had two vectors that were not collinear.
And so in that case, you could generate or span all of our two.
Okay, let's look a little bit at a three-dimensional example.
So if we look at the span of this vector, that would just be all multiples of that vector in graphically.
We're saying that x has to be 0, z has to be 0.
But y could be anything because you multiply anything by 1.
So essentially you're on the y-axis.
So x has to be 0, z has to be 0, so anything along the y-axis.
If we add another vector to it, so I throw in this one, now we take all linear combinations of these two
and notice that in this case, you get all vectors that look like this.
So it means x has to be 0, but y and z can be anything we want them to be.
And so that gives us a plane where x is 0, but y and z are anything that we want them to be.
So these two vectors in our three generate a plane.
We're going to start off today talking about multiplying a matrix with a vector.
So some terminology there.
Let's suppose that we start off with an M by N matrix A.
And we're going to let A1, A2 through AN be the columns of A.
So each of these is a column vector and not a scalar.
Each A sub i is a vector in our M. Since there are M rows in A, each column would have M entries.
And let's let x be a vector in our N.
Then the product of A and x is computed as shown here.
So here's A. Remember each of these is a vector, a column of A. Here's x.
And so to compute A times x, basically we just match up the components of x with the columns of A.
So we end up with x1 times A1 plus x2 times A2 plus out to xN times AN.
You'll recognize this as a linear combination of the columns of A.
So here's the deal. Ax is a linear combination of the columns of A using the corresponding entries in x as the weight.
So here's an example. Here's a matrix A and a vector x.
Then to compute A times x, we've got the first entry in x times the first column of A plus the second entry in x times the second column of A.
And then plus the third entry in x times the third column of A.
And we do the scalar multiplication, add them together, and here's our result.
Alright, so notice what we have. Here's a matrix times a vector, and we get this vector.
Or another way of looking at it. Here's a linear combination of the columns of the matrix yielding the same vector, clearly.
And then here's another way to look at it.
If we looked at this augmented matrix, so this is A here, the coefficient matrix A, with this vector augmented onto the right hand side.
So we essentially have the augmented matrix here that corresponds to this system.
So if we put it in reduced echelon form, which I have here, notice that we get what we think we should get, right?
The solution to the system is just the vector x that we started out with up here.
So these are three different ways to look at the same system.
So let's formalize that idea.
So if we have a matrix A that's m by n with columns A1, A2, to An, and if B is a vector in RM,
then these equations, Ax equals B, and this linear combination of the columns of A, set equal to B.
In the system corresponding to this augmented matrix, where you've got all the columns of A with B tacked on at the augmented column,
all have the same solution set.
So these all essentially mean the same thing.
They're just different ways of looking at the same problem.
So notice that the equation Ax equals B has a solution f and only if B is a linear combination of the columns of A.
Since these are all equivalent, this is a linear combination of the columns of A that we're setting equal to B.
So if that has a solution, that means Ax equals B has a solution.
And another way of stating that is that Ax equals B has a solution if and only if B is in the span of the columns of A.
So if B is a linear combination of the columns of A, that means B is in the span of the columns of A.
We're going to move over to a maple session at this point because it's easier to show you some of this with some of the graphics that maple lets me use.
And so also give you a little lesson in using maple as well.
So I'm loading here the student linear algebra package and the plots package.
And then I'm defining two vectors here, A1 and A2.
And then I'm creating with this command a matrix A whose columns are the vectors A1 and A2.
So you see the output that you get from that.
I'm defining these vectors because I want to look at the plane that they lie on.
And as I explained here, this plane is defined by this equation x minus y equals zero.
So notice that both of my vectors here have x minus y equals zero and z can be essentially whatever it wants to be.
And so what that amounts to is a plane that if you think about the line y equals x in the plane.
So it's kind of defined by that line and then just straight up and straight down from that line.
So let's kind of look at a picture.
Here I'm defining plots so that we can see what these look like.
Okay, so let's see.
Oh, it looks like I've changed this.
Let me change this.
This is supposed to be 222.
It really doesn't matter.
But just to be consistent, let me make that change.
Okay, then so here's what those two vectors look like.
And let me kind of rotate this to make it look like what I want.
Basically, if you look, you can kind of see that they lie on the same plane.
And that plane is, if you look at it, this is looking down from the z perspective.
And so that's, if you look down at the bottom of that box, that's the line y equals x.
So they both lie on that line.
It's easier if you look at the plane also and then you can see the vectors as well.
So let's do that.
Here you can see the plane.
And you've got the two vectors that are lying on that plane.
Okay, so you can see here's the x-axis this way, y going this way, and z's up and down.
And you can see there's the plane.
And looking from up above, you can see that we're cutting across
on the line y equals x and both those vectors lie in that plane.
So if we look at any other vector in that plane,
any other vector where the x and y components are the same, it should lie on that plane.
So I've defined a new vector, a 113, and then I've plotted it on this same set of axes.
And so if you look, let's look at it like this.
There you can see the new 1113, I made it black, so you can see it here.
And then looking at it from above again, you can see that those are all on the same set of axes, or same plane.
So all those lie on the same plane.
So that means that b here is in the span of the original two vectors that we started out with,
or it's a linear combination of the two vectors we started out with.
So if we solve the system ax equals b, it should be consistent.
So that's what I'm doing right here.
I'm forming the matrix, an augmented matrix here.
So I've got my matrix a, and then I'm augmenting on the new vector b here.
And all in the same command, I'm reducing that matrix to a row echelon form.
And here's what we get.
Clearly, this system has a solution because we have no rows where it's all zeros and then something not zero.
You can see we've got a row of all zeros, but that's okay.
And in fact, we have a unique solution because there's no free variables.
And the solution says that x1 is three-halves, x2 is negative one.
So if we take that linear combination, okay, three-halves times the first column of a,
plus negative one times the second column of a, we should get b because that's the system we just solved.
So if you look, indeed, this is that linear combination here, and this is b.
All right.
Now looking ahead, if we choose a vector whose first components are not equal,
then it should not be on that plane.
And if we solve the system with it on the right-hand side, we should get no solution.
So let's give that a try.
We're going to create another vector, c, which has the x and y components are not equal.
All right.
And I plotted it.
So here's that vector.
I'm going to give you a view on that.
There you can see there, you know, there.
That's a good view right there.
You can see this new vector is the cyan-colored one.
And clearly it's not on the plane that the other three vectors lie on.
All right.
It's coming off the plane.
So that means that it is not in the span of the other three.
So it's not a linear combination of the other three, but really we're only interested in the other two.
It's not a linear combination of the columns of a.
So if we solve the system with c on the right-hand side, then here's what we end up with.
Okay.
So I didn't get it in reduced echelon form.
I wanted to leave it like this.
So this is just echelon form.
But notice the third row, you got zero, zero, and then negative two.
So zero, zero, something not zero.
So that tells us there's no solution.
Okay.
And then again, because that vector is not in the span, it's not in the space that's generated by the columns of a.
One more look.
I just created just a generic vector I called d.
And then let's look at the echelon form of the augmented matrix with a and d.
All right.
So it looks like this.
This is nice thing about maple.
It does the symbolic computation.
So you can do the computation with variables or parameters instead of all just constants.
And so we can see, notice the bottom row, we got zero, zero, and then something else over here.
And notice what that says.
It says that if this system has a solution, then d two minus d one has to equal zero.
Right.
Because if this is non zero over here, then we've got zero, zero, something not zero in the augmented column, which means no solution.
So for this system to have a solution, d two minus d one has to equal zero, which of course means that d two and d one are equal.
And since those back up here, those are the first two components, that's just saying that the X and Y components have to be equal for this system to have a solution.
Or graphically, the first two components have to be equal for the vector to lie on the plane that is spanned by a one and a two.
Okay, I'm going to stop there and we'll pick up the next slide in the next video.
Okay, I want to start back here with theorem four.
This is a very important theorem in your book.
So you want to play, pay close attention to this one.
So it says let A be an M by N matrix.
Then the following statements are logically equivalent.
That means for a particular matrix A, either they are all true or they're all false.
And so that's a powerful theorem that we have here.
Okay, first statement is for each B and RM, the equation AX equals B is consistent.
So that's saying no matter what the right hand side is, for this particular matrix A, AX equals B is always consistent.
Another way to say that is that each B and RM is a linear combination of the columns of A.
Okay, because we saw just earlier that A times X, when you multiply A times X,
you're actually taking a linear combination of the columns of A.
This also means that the columns of A span RM, right?
Because you can produce any vector in RM as a linear combination of the columns of A.
And then down to the nitty-gritty, how you actually determine whether this is true or not,
is by putting A in echelon form.
And these are all true if A has a pivot position in every row.
So we saw that in the last, in the Maple demo.
Okay, if there's a pivot position in every row of A, that means of the coefficient matrix,
then there's no way the system can be inconsistent because you could never have a row of all zeros
and then in the augmented column something non-zero.
Okay, we're going to look at one other way to compute A times X.
But first, we're looking at the inner product of two vectors because we're going to use that.
So if we're given two vectors in RM, say X and Y, then the inner product,
which is also called the dot product of X and Y, is computed as follows.
It's just X1 times Y1 plus X2 times Y2 plus so forth plus Xn times Yn.
Okay, so just multiplying light components and adding them up.
So here's an example.
We've got X is 1, 2, 3, Y is 4, 5, 6, then the inner product of X and Y, or X dot Y,
is 1 times 4 plus 2 times 5 plus 3 times 6.
Okay, so now let's talk about two ways we can compute A times X.
We already looked at one of these.
That's by taking a linear combination of the columns of A.
So here's an example to compute the product of this matrix A times X.
Then we take the first element of X and apply that to the first column of A.
Then second element negative 4 times the second column of A and so forth.
So we generate this linear combination and end up with this vector.
Now the second way we use inner products and the ith element of the inner product of,
the ith element of AX is the inner product of the ith row of A with X.
So looking at that, you look at, to get the first element here, the negative 3,
then it's the first row of A inner product with X.
So that's going to be 1 times 6 plus 2 times negative 4 plus 1 times negative 1,
which is what we have here in this first element here.
Then to get the second component of A times X, it's the second row of A inner product with X.
So negative 3 times 6 plus negative 1 times 4 plus 2 times negative 1.
As we see here.
Then to get the third element, it's the third row of A inner product with the vector X.
So those are two different ways you can compute the product of a matrix and a vector.
For the most part, we will use this method, the linear combination of the columns of A,
just because that fits in with the way I want you to be thinking.
However, this method down here is probably quicker.
So if you just need to compute the product of a matrix and a vector,
now you might want to just use this method using the inner product method just because it's faster.
So you should practice both just to make sure you know how to do each of these methods.
And I think that's it for this lesson.
Alright, so we're going to talk about homogeneous linear systems.
And homogeneous linear system is simply one in which the right hand side is the zero vector.
Zero vector is just a vector of all zeros.
So here's another way to look at it.
We look at A times X as a linear combination of the columns of A.
Is it here? X1A1 plus X2A2 plus dot dot dot XNAN.
We want that to equal the zero vector.
Now, how can that happen?
Well, if you look at it a little bit, it's pretty clear that if you set each one of these X values equal to zero,
then you did a linear combination and you'll end up with the zero vector.
So therefore, a homogeneous system is always consistent.
X equals zero, and all X values equal to zero is always a solution.
And since it's always a solution and it's obvious that it's a solution, we call it the trivial solution.
So we know that the system's consistent.
And if you think back to the two questions that we asked when we're solving a system, the first one is, is it consistent?
So now we know this one is.
So we move on to question two, which is, okay, is the solution unique or are there an infinite number of solutions?
So in our case, if the solution is unique, that means you only have the trivial solution.
So what we want to know is, do there exist any non-trivial solutions?
Is the solution set an infinite set?
Well, the answer is just like it would be for any other system.
You'll have non-trivial solutions or you'll have an infinite number of solutions if and only if your system of equations has at least one free variable.
So we're going to look at a system here and solve it, and we're going to write our solution in parametric vector form.
So that's something that's new in this section.
All right, so here's the system.
We want to solve x equals zero.
So we put it in an augmented matrix.
So we tack on the column of zeros and we do some row operations.
And notice that I have put my augmented matrix in reduced echelon form, not just echelon form, but reduced echelon form.
And that is because that makes it easier to not only write the solution in general form, but it makes it easier to write it in parametric vector form.
So anytime your system has an infinite number of solutions, then it tends to be easier to write out the set of solutions if you put your matrix in reduced echelon form first.
So looking at this matrix here, you can see that x1 and x2, they're pivot columns in the x1 and x2 columns, pivot positions in the x1 and x2 columns.
So those are basic variables.
In the x3 column, we have no pivot position, so therefore x3 is a free variable.
Okay, so that means we've got an infinite number of solutions, or this homogeneous system has non-trivial solutions.
So what do they look like?
Well, we can look at the second row and we get that x2, it must be equal to 3 times x3, and x1 is going to be equal to negative 4 times x3.
So we can write the general form of the solution as it's given here.
x3 is free, x1 and x2 are written in terms of the free variable.
Now to get the parametric vector form of the solution, we start off with just a generic solution vector and basically just copy down the general form of the solution.
So this, what you see here, comes straight from looking at the general form of the solution.
And then to write it in parametric vector form, we simply factor out the parameter.
In this case, x3 is the parameter because we can set x3 to be whatever we want in order to generate as many solutions as we want.
So we have it now in parametric vector form.
Here's our parameter and here's our vector.
You should note that if you have more than one free variable, then you're going to have more than one parameter.
And vector, when you write your solution in parametric vector form, so you'd have, you know, if we had x4 that was a free variable, we'd have x3 times this vector, plus x4 times some other vector, and so forth.
You'd have a vector for each free variable.
Okay, let's think about the relationship between the systems x equals 0 and ax equals b.
So what we've done to this point is solve ax equals 0 for a particular matrix a.
And now I want to keep that same a and solve the system ax equals b for this vector b given here.
And we'll write our solution in parametric vector form.
So we approach it the same way.
Here's our augmented matrix.
We're doing row operations to get the matrix in reduced echelon form.
And if you want to go back and compare with what we did with ax equals 0, these are the exact same row operations.
And that's because they were based on what was in the coefficient part of the matrix.
And so that hasn't changed.
So we do the same operation.
So the only thing that's changed is what's on the right-hand side.
Let me, oops, I'm backing up again.
Trying to get used to my mouse track pad here.
Sorry about that.
Let me point out that when you're solving ax equals 0 and you have all zeros on the right-hand side, then those zeros never change.
Because when you do elementary row operations, any of those three elementary row operations, they will never change.
If you start off with all zeros, you'll end up with all zeros.
Okay, so back to this example though.
When we write out our solution, those x3 is still free variable.
And now x2, when we write x2, then it's going to be 3, the 3 here, plus 3x3.
And x1 is going to be negative 5 minus 4x3.
So here's our solution.
And if we just write it in general form, here's what it looks like.
And parametric vector form, we start out just like we did previously,
write out what the general form looks like in a vector format.
And then we're going to separate out the part that involves a parameter and the constant part.
And then take it one step further and factor out the parameter.
And so notice that what we have here is exactly the solution that we had to ax equals 0.
And then with ax equals b, we have that plus this constant vector.
Now notice that the constant vector doesn't have a parameter associated with it.
It's just constant and it doesn't change.
But here we can multiply, we can set our parameter x3 to be anything and scale this vector.
Okay, so in general, the solution looks like this.
Here was solution to ax equals 0.
And notice that it's in this form.
It's in the form some parameter t times a vector v.
So in this case, x3 is playing t and this is the vector v.
So it's just a vector that we can scale any way we want.
Whereas for ax equals b, you still got the tv part, right?
You got x3 times this vector.
But you've got this constant vector.
So that's what I'm calling p.
And so for ax equals 0, you just have a vector that you can scale.
For ax equals b, you've still got the vector you can scale plus this other vector.
So let's look at graphically what is going on there.
Now for this, I'm going to just look at a case in R2.
So what you're going to see here is not the solutions to the system we were just looking at,
because that was in R3.
But it's a little bit easier when you draw pictures in R2.
So that's what I've got here.
So for ax equals 0, the solution set is all vectors of the form t times v.
So here's a vector v.
And we know that t times v is just any vector on this line.
So the line that you see here is the set of all solutions to ax equals 0.
Now let's look when we consider the vector p.
So for a solution set for ax equals b is of the form tv plus p.
And so remember all the vectors along here are the t times v.
And then so for any of these vectors, any point on this line, you can add p
and you'll have a vector of the form tv plus p.
So here's one.
I just did v plus p.
So 1 times v plus p.
And doing the parallelogram method, you end up with this vector here.
We can do it for other vectors.
So here's p plus 2 times v.
Here's p plus 0 times v.
Here's p plus negative 1 times v.
So notice though that they all fall along this line.
So right here would be p plus 1 half v.
Here p plus 3 fourths v.
Here p plus 3 halves v and so forth.
So for every vector, every solution to ax equals 0 that you have on this line,
there's a corresponding solution to ax equals b over here on this line.
And so we end up with this approach where along the blue line,
that's all the solutions to ax equals 0 along the magenta line here,
all solutions to ax equals b.
So notice that for ax equals 0, it's aligned through the origin
because that's the trivial solution to ax equals 0.
And for ax equals b, you're moving away from the origin,
so off this line, but parallel to the solution set for ax equals 0.
And if you look in three dimensions,
let's suppose we have a problem where the solution set is a plane in three dimensions.
So for ax equals 0, it's a plane like this red one
that goes through the origin.
And then for ax equals b, you get a parallel plane that's moved off the origin.
So for ax equals 0, you get this red plane that goes through the origin.
For ax equals b, you get a parallel plane that's off the origin.
All right, today we're going to talk about the concept of linear independence.
So if we have a set of vectors, let's say p vectors in Rn,
the set is said to be linearly independent if we take a linear combination,
set it equal to the 0 vector, and this equation has only the trivial solution.
So the only way we can take a linear combination of the vectors and get the 0 vector
is if all the coefficients are equal to 0.
Since this equation here is equivalent to the system defined by this augmented matrix,
then we also say that the set is linearly independent
if the system corresponding to the augmented matrix where we put all the vectors in as the columns
augment on the 0 vector if that has only the trivial solution.
So these are equivalent.
Now if this has only the trivial solution, then that means it has a unique solution, right?
It has only the trivial solution.
So all the c's equal to 0 is the only solution, so it's unique.
So that means it has no free variables.
So kind of the typical method of determining if a system or if a set of vectors is linearly independent
is to stick them in the columns of a matrix as done here,
tack on the 0 vector for the augmented column,
and see if that matrix has, as if there are any free variables in that system.
If there are no free variables, then you know the vectors are linearly independent.
If you do find a free variable, then you know that there are an infinite number of solutions to that system,
and therefore the vectors are not linearly independent.
In that case we say they're linearly dependent.
So here's a couple of vectors, and if we want to find out if they're linearly independent,
then as I showed you before, take a linear combination of them and set it equal to 0 as I've done here.
We put that into an augmented matrix.
It looks like this with just the two vectors stuck in the columns,
and the 0 vector tacked on as the augmented column.
And we can do one row operation and zero out in this position,
and notice that we have no free variables, and therefore that means that we have only the trivial solution.
You can see the solution is unique, and so these vectors are linearly independent.
Here's another set of vectors.
Just change just a little bit.
Let's see if they're linearly independent.
Again, we take a linear combination, set it equal to 0, stick that in as an augmented matrix,
do one row operation, and we end up with this matrix here.
And notice that we've got a free variable, x2 here, the second column, has no pivot position,
so x2 is a free variable, so therefore these vectors are linearly dependent.
Now if you look at the vectors that we had, the first set, if we graph them, looks like this.
We had 2, 1, and 1, 4, so they look like that. They're linearly independent.
Whereas that second set, I believe we had 1, 4, and 2, 8.
Let's back up. Yeah, 1, 4, and 2, 8, and so they're multiples of each other,
so they're indicated in this picture, and they're linearly dependent.
Interestingly, you can see that these two vectors do, if you think about the span of these two vectors,
then you can take a linear combination of these two and produce any vector in R2.
However, for these two, any linear combination of these two vectors only gets you vectors on the line
that's defined by these vectors and going back in the negative direction here,
but you get no vectors off that line.
We've been looking at the two vector case, which at times can be misleading,
but let's examine what we can say about it.
When you have just two vectors, then they are linearly dependent if at least one is a multiple of the other.
So if they're multiples of each other, or one's a multiple of the other, then they are linearly dependent.
They're linearly independent if neither is a multiple of the other.
So that works when you only have two vectors.
Okay, so let's kind of expand our scope here and look at a set of three vectors.
So I took the set we were looking at initially, one, four, and two, one, and threw in another vector in there.
Now, what do you think? Are these vectors linearly independent?
Well, if we take a linear combination of them, set it equal to zero vector.
This is just going back to the first definition we talked about.
And throw that in an augmented matrix, end up with this, and what happens?
No need to do any row operations. Why is that?
Now, let's think. How do we know if the system is, or if the system, we want to know does the system have only the trivial solution or not?
Or another way is, does it have any three variables?
We can look at this one and say, yep, there's at least one free variable.
There has to be, because we have three variables and only two equations.
So three columns, only two rows. So we have to have at least one free variable.
So we can have at most two pivot positions.
So these vectors are linearly dependent.
So here's a rule. If the set contains more vectors, then there are entries in each vector, then the set's linearly dependent.
That's the case we just looked at. We had three vectors in R2.
So more entries, more vectors than there are entries in each vector.
Okay, there it is. Got three vectors and only two entries in each vector.
If we look at that augmented matrix and do some row operations on it, we end up at this point.
Now, remember back here, just looking at this, you knew they were linearly dependent because you had more vectors than there were entries in each vector.
But if we do these row operations and get this matrix in reduced echelon form, then we end up here.
So if we wrote the solution of that system, it looks like this.
From the second row, you get X2 is going to equal to X3.
X1 is equal to negative X3. X3 is a free variable.
So you can plug in anything you want for X3 to generate specific solutions.
For instance, if we said X3 equals zero, that's going to give us the trivial solution.
X3 is zero and based on what X1 and X2 are defined in terms of X3, they're going to be zero also.
If we said X3 equal to one, then we get X1 is negative one. X2 is two and X3 is one.
Now, where does that apply to?
Well, notice we were trying to find a solution to this equation.
We were trying to see can we find non-zero values for X1, X2, and X3, at least one of them non-zero, so that we can produce the zero vector.
And so this general form of the solution tells us how to form the values for X1, X2, and X3.
So we can take our values that we got for X3 equals one, plug them in.
So notice if we do that, we end up with negative one times one is negative one plus four here is negative three.
I mean positive three minus three gives us zero.
And in the second position, we get negative four plus two is negative two plus two is zero.
So we can find specific values for these coefficients so that we can produce the zero vector.
Now notice from this, we can take that equation and solve it for negative three two.
So we can just take this over to the other side and notice that negative three two can be written as a linear combination of one, four, and two, one.
That's what we've just done here.
We've written negative three two as a linear combination of the other two vectors.
Another way of saying that is that negative three two is in the span of these other two vectors.
It's a linear combination of them, so that means it's in the span of those two vectors.
And if you think back to our picture, that makes sense.
Right here were the first two vectors, two one and one four.
And remember before, I talked about how these span are two.
We can take a linear combination.
We can scale each of them and then add those scaled vectors together to produce any vector in R2.
And how do we know that?
Well, just look at those two vectors, do a row operation, and look, we have a pivot position in every row.
So by theorem four, going back to section one point four, theorem four, that says that the vector span R2.
One of the things in theorem four is if you have a pivot position in every row, that means the columns of the original matrix span the space in which they live, which in this case were in R2.
Okay, so here's another rule that relates to linear dependence.
A set of two or more vectors is linearly dependent, if and only if at least one of the vectors is a linear combination of the others.
So back here, we saw that negative three two is a linear combination of the other two vectors.
And in fact, we could have solved this system for any of these three vectors.
We could solve for any of these three vectors in terms of the other two.
So any of these vectors is a linear combination of the other two.
Okay, there you see it again.
All right, moving on.
If there's another rule, keep in mind, if a set contains the zero vector, then that set is linearly dependent.
So if you have a zero vector in the set, then the set has to be linearly dependent.
Here's an example of that.
You got the zero vector.
Notice that it's always going to be a free variable.
Okay, the variable corresponding to the zero vector is always going to be free,
and hence the set would be linearly dependent.
So summary, set of vectors is linearly independent.
If you take a linear combination, set it equal to zero and you have only the trivial solution.
So all the x is equal to zero is the only solution.
Equivalently, you throw all those vectors into a matrix,
tack on the zero vector for the augmented column,
solve that system, and you get only the trivial solution.
Okay, on the other hand, it's linearly dependent if the system has at least one free variable.
Because in that case, you would have an infinite number of solutions.
And another way of characterizing linear dependence is a set's linearly dependent
if at least one of the vectors is a linear combination of the others.
Okay, and that's it for section 1.7.
Okay, we're going to start today talking about linear transformations.
First, we're going to look back at something you're familiar with from algebra or calculus.
And we'll just look at a basic function which typically maps some real number to another real number.
So we map the set of real numbers to the set of real numbers.
So for example, here's a graph of a function f of x equals x squared.
And so you input a real number, okay, x is a real number.
You square it and you get another real number.
So it's mapping the real numbers to the real numbers.
Here's a little picture view of what's happening.
Here you're putting in a real number into your function.
And for our purposes here, we know what the function does, just squares the number.
But you really don't have to know exactly what the function does.
You know that what comes out though is another real number.
So this notation down here at the bottom is red f is a function that maps the real numbers to the real numbers.
Now let's move to talking about matrix transformations.
And it's the same basic idea as a function.
It's just defined in terms of a matrix.
So here t of x, t for transformation.
So when we apply a transformation to x, we multiply a times x where a is some m by n matrix that defines this transformation.
So here's a picture view of what's going on.
You have a vector from Rn that is the input to your transformation.
And the output is a vector in Rm.
Guys, the key here is that a is an m by n matrix.
And the transformation occurs by multiplying a times x.
So that means that x has to have as many entries as there are columns of a because a times x is just a linear combination of the column.
So we need an x component to match up with each column of a.
And then each column of a has m entries.
So when we do that linear combination, we're going to end up with a vector with m entries.
And hence we end up with a vector in Rm.
So we say that t maps Rn to Rm.
So for example, here's a matrix A.
And let's suppose that we have transformation t defined on this matrix.
So t of x equals a times x.
So here's a vector x equals two one.
Then t of x is a times x.
So we do that multiplication, take a linear combination, and we end up with one two.
So we started with a vector in R2 and ended up with a vector in R2, which makes sense since A is two by two.
Here's another example.
We're applying the transformation to the vector four negative two.
So we multiply a times that vector.
And so we take a linear combination of the columns of A and end up with negative two four.
If we look at it in a general case, so we just apply t to a generic vector x1 x2 from R2.
Do the same ways we did the previous two.
And notice we end up with x2 x1.
So this transformation has just reversed the order of the elements.
If you look back up at these other two, two one went to one two four two went to four negative two went to negative two four.
So that's all this transformation is doing swapping the order of the elements in the input vector.
If you look graphically at what's going on, here's one of the vectors two one.
And here was its transformation, which is one two.
And here's the other one for negative two, which was transformed into negative two four.
And so you can see from this picture that this transformation is reflecting a vector across the line y equals x.
Okay, this is kind of a standard sort of thing that you see in computer graphics.
And so matrix transformations are really the fundamental element in computer graphics.
So if you were to go on and take that course in the computer science department, you would be seeing a lot of matrix transformations.
Okay, so to be a little more precise, we say a transformation t from rn to rm is a rule that assigns to each vector x and rn a vector t of x and rm.
More terminology here rn is called the domain of t and rm is called the co domain of t.
So rn is where the inputs to the to the transformation come from.
rm is where the outputs come from.
Okay, for a given x in our n, t of x is called the image of x.
So the image of x is just the vector that x maps to.
If we look at all images, all possible images that we get under the transformation t, this is called the range of t.
Okay, so the range of t is all images that we get when we map all possible vectors x through this transformation.
Now you might be thinking that, well, isn't that the co domain?
And the answer is sometimes it is and sometimes it isn't.
Okay, in general, the range is a subset of the co domain.
In some cases they are equal and others they're not.
There are elements of the co domain that are not in the range.
So not in general, not everything in the co domain is mapped to necessarily.
Okay, so here's back to our previous example.
We've got this transformation maps r2 to r2.
So the domain is r2.
And really the domain is the first set that you see here when you write down the transformation in this fashion.
The co domain is also r2.
So we're mapping vectors from r2 to vectors in r2 also.
Now the range, let's think about that.
Now the range, think of the range as just everything that gets mapped to under this transformation.
And if you look at the picture that we had before, if you wanted to produce any vector,
then you can figure out what vector you need that would map to it because you just need to reverse the order of the elements.
So that means that the range of this transformation is all of r2.
So the co domain and the range are equal in this case because any vector in r2 is mapped to.
Alright, let's look at a different example.
Okay, here's one defined by different matrix A, still a two by two matrix.
So this transformation also maps r2 to r2.
So the domain is r2.
The co domain is r2.
Let's think about what is the range of this transformation.
Well, let's play around with it for just a little bit.
Let's take our old vector 2, 1 and see what it maps to.
If we apply the transformation to 2, 1 multiply A times 2, 1 and we end up with a vector 4, 8.
Okay, how about another one?
Negative 3, 2.
A times that vector, take the linear combination, you end up with 1, 2.
So let's look at the general case.
What happens?
We apply it to just a generic vector x1, x2.
Do that matrix multiplication.
And we end up with this vector, which I can rewrite in this form to illustrate that the second component of the vector is just twice the first one.
And you see that in these other two that we did.
The second component is 2 times the first.
Here again, second component is 2 times the first.
So if we look at that graphically, every vector that we can map to is of this form, the second component is 2 times the first.
So that's actually all vectors on the line y equals 2x.
The second component is 2 times the first.
Here's 2, 1 and it maps to 4, 8.
So it's on this line.
The vector negative 3, 2, I believe it was that we looked at, it mapped to 1, 2.
So here it falls on the line.
So no matter what vector you choose, it's going to be projected onto this line.
So the range of this transformation is simply this line, the line y equals 2x, or all vectors of the form x1, 2x1.
So we're all vectors where the second component is just 2 times the first.
So here the range is not the same as the co-domain.
The co-domain is r2, but the range is just this line y equals 2x.
So it's just part of the co-domain.
Okay, suppose at this point we know given a transformation in the matrix that defines it, we can compute t of x.
Just multiply 8 times x.
A little more difficult question is this one.
How do we determine if a given vector b is in the range of a particular transformation?
So we're asking, is there a vector x that maps to b, or is there a vector x such that t of x is equal to b?
And since t of x equals 8 times x, we can say, does there exist a vector x such that 8 times x equals b?
And now we're back into the realm of looking at systems of equations, and we're very familiar with that.
Okay, so simply a system of equations to solve.
Alright, so let's go back to that previous example.
And suppose you're asked, is this vector 612 in the range of t?
And we know that since the range of t is all vectors on the line y equals 2x, then it should be, because the second component 12 is 2 times the first.
So let's solve the system of equations.
Alright, here's the augmented matrix corresponding to ax equals b, where b is 612.
And we do one row operation, and here we go.
It's an echelon form, and clearly the system is consistent, so therefore the vector is in the range of t.
About another one, about 610.
Now in this case, notice the second component is not 2 times the first, so you would expect this vector not to be in the range of t,
which would mean that the system should be inconsistent.
And as you see, do one row operation, and you end up with 00 and then negative 2.
So that's clearly the system is inconsistent.
So this vector is not in the range of t.
So to determine if a vector is in the range of a linear transformation, you need to solve a system of equations.
Okay, changing gears just a little bit.
Previously, we learned that matrix multiplication has certain properties, and one of them is this one.
If you multiply matrix times the sum of two vectors, then you can distribute.
So a times u plus v is equal to a u plus a v.
And similarly, if you multiply a times a scalar times a vector, then you can move the scalar out.
So a times c u is the same as c times a u.
Now I bring that up because that applies here.
Okay, so a transformation is said to be linear if the following conditions hold.
That t of u plus v is t of u plus t of v, and t of c times u is c times t of u.
Okay, so these two conditions look very similar to what we have up there.
And in fact, since we define our transformation as a times x, then these are really equivalent.
Okay, so linear transformations preserve the operations of vector addition and scalar multiplication.
And we'll stop here at this point, and then the next video will be for section 1.9,
which will include more information on linear transformations.
Alright, we're going to start with more on linear transformations.
Let's suppose we have a transformation t.
And suppose that all you know about t is that the vector 1, 0 gets mapped to 2, 0, 1.
And you know that 0, 1 gets mapped to this vector.
So just from looking at this, you can tell that t maps r2 to r3.
We're looking at vectors in r2, and we're mapping them to r3.
But how can we find a general rule for how to determine t of just a generic vector x?
So t of x1, x2 for any vector, x1, x2.
If we get this just based on the information that we have, we need to use the properties of linear transformation.
So remember, we talked about these last time, that t of u plus v is t of u plus t of v.
And t of scalar times the vector is that scalar times t of the vector.
So let's examine what we have.
Notice that we can write a generic vector x1, x2 as x1 times 1, 0 plus x2 times 0, 1.
Now remember, on backwards, we know what we get when we apply t to 1, 0.
And we know what we get when we apply t to 0, 1.
So keep that in mind.
So we're going to write x1, x2 as x1 times 1, 0, x2 times 0, 1.
So then I'm going to apply t to both sides of this equation.
So I'll have t of x1, x2 is equal to t of this stuff here.
Now, since t's a linear transformation, t of, this is like t of u plus v, if you want to think of it like that.
So we can break it up into t of u plus t of v.
And then this is like a constant times a vector.
So t of a constant times a vector, we can rewrite as that constant times t of that vector.
Okay, so we end up with this.
And we know what t of 1, 0 is.
It's just this vector.
And we know what t of 0, 1 is.
It's this vector.
So here's what we end up with.
And so notice this is just a linear combination of these two vectors, which we can rewrite in this form as a matrix with those columns times the components x1, x2.
So notice that we've written t of x, okay, generic vector x, as a times x.
So now we have the matrix with which to apply the transformation.
And as you can see, that matrix is simply the vectors that you get when you apply t to the vector 1, 0 and 0, 1.
All right, now those columns that we just looked at, 1, 0 and 0, 1, those are special vectors.
Okay, and to talk about that, let's first define the identity matrix.
So the identity matrix is an n by n matrix.
Okay, so let's square and we write it as i sub n.
Okay, and it has ones on the main diagonal.
Main diagonal is the 1, 1 position, 2, 2, 3, 3, 4, 4.
Okay, so it has ones down the diagonal and zeros everywhere else.
So here are a few examples.
i2s, 2 by 2, got ones on the diagonal.
i3s, a 3 by 3, ones on the diagonal, zeros everywhere else.
And here's i4, okay.
So i sub n is just an n by n identity matrix.
Now notice i sub 2, you see those columns that we were just dealing with, those vectors 1, 0 and 0, 1.
So they are the columns of the identity matrix.
Okay, and they are so special that we give them their own names.
We call them e1, e2, so forth, the e sub n, depending on what n is.
So this column here would be e sub 1, this one e sub 2, and a 2 by 2 matrix.
Here for 3 by 3, this would be e sub 1, this e sub 2, this e sub 3.
Okay, so what they actually are depends on the size of the matrix that we're talking about.
There it is with 2 by 2.
Okay, now as we saw in the previous example, if you know t of e1, t of e2, and so forth,
if you know what you get when you apply the transformation to these columns of the identity matrix,
then you can compute t of x for any vector x.
That's what we showed in that earlier example.
Okay, so here it is written in a little more formal manner.
If we have t that maps rn to rm, then there is a unique matrix A such that t of x equals ax.
In fact, A is the m by n matrix whose jth column is the vector t of ej,
where ej is the jth column of the identity matrix.
And so if you think back to the example we just did, this is exactly what we came out with.
What we came out with our matrix A was t of e1 and t of e2 because we had a 2 by 2 example.
This matrix A is called the standard matrix for the linear transformation t.
So it's a special matrix.
Now, we're interested in determining conditions under which a transformation has an inverse.
So that means if you have a transformation that maps rn to rm, then given some b in rm,
can we find an x in rn that maps to that b?
Or can we find x such that t of x equals b?
Let's go back to our functions just defining the real numbers first of all.
So before we looked at the function f of x equals x squared.
And this is what it looks like.
Now, a couple of things to notice.
One is that you never get a negative number.
When you plug in any x, no matter what x you plug in, since we're starting with real numbers,
no matter what real number x you plug in, you always get a non-negative value.
Guys, keep that in mind.
And also note that we can get the same y value.
So if I drew a horizontal line, I would hit a couple of places on this graph.
So for a given y value, we've got a couple of different x values that can produce that y value.
So keep that in mind.
Alright, f of x equals x squared that we just looked at is not invertible for two reasons.
One, there exists y in r, so there's some real numbers for which there's no x such that f of x equals y.
There's no x such that x squared equals y.
So consider y equals negative one.
We cannot come up with a real number x such that x squared equals negative one.
That means that there are values in the co-domain that are not in the range.
The co-domain here is the real numbers, but the range for this function is all non-negative real numbers.
So we can never get a negative number when we apply this function.
So we say that this function is not on to the real numbers.
There are real numbers that don't get mapped to.
I'm going to define that term in just a minute, but I want to give you an example, something that's pretty easy for you to grasp.
So I said f is not invertible for two reasons.
Here's the second one, that there exists y in r.
So there exists real numbers y for which a squared is y and b squared is y, but a and b are not equal.
That's that horizontal line thing I was talking about earlier.
So for example, if y is four, we can get that with two because two squared is four.
We can also get there with negative two because negative two squared is four.
So if we were trying to get an inverse, we started with four and we wouldn't know how did we get there.
Well, we don't know because it could have been from two or it could have been from negative two.
So we say that this function is not one to one because of this reason.
Okay, so here's a little more formal way to look at it.
We say a transformation from Rn to Rm is onto Rm, onto the co-domain.
If each b in Rm is the image of, or another way to think of it is, is mapped to by at least one x in Rn.
So every b in the co-domain is mapped to by at least one x in the domain.
So every b gets mapped to.
Okay, so this means that no matter what vector b you choose from Rm, there's some x that maps to it.
So for every b in Rm, there's some x such that t of x equals b.
And since t of x equals a times x, this means that for every b, there's some x such that a times x equals b.
If there's an x such that t of x is b, then it has to be the same x that makes a times x equal b.
Now, when is it true that no matter what right hand side you have, you'll always have a solution?
Well, that's true when you have a pivot position in every row of a.
So if there's a pivot position in every row of a, then the transformation will be onto.
Okay, let's talk about one-to-one.
Transformation from Rn to Rm is said to be one-to-one if each b in Rm is the image of, or is mapped to by at most one x in Rn.
So remember back to the function f of x equals x squared.
This wasn't true because you could get to four from two and from negative two.
So there was an element in the range that was mapped to by more than one element in the domain.
So it wasn't one-to-one.
Here one-to-one means each b in Rm is mapped to by at most one x in Rn.
So that means that for each b in Rm, there's at most one vector x such that t of x equals b.
So that means that since t of x equals ax, if we look at the system ax equals b, it can't have an infinite number of solutions.
Because then there would be more than one x that maps to that b.
And if our system can't have an infinite number of solutions, that means it can't have free variables.
Okay, so x equals b can have no free variables.
And it has no free variables.
That means there must be a pivot position in each column of a.
So the transformation is one-to-one if there's a pivot position in each column of a.
So one-to-one, there's a pivot position in each column.
Onto, there must be a pivot position in each row.
Okay, so that's how you can keep straight one-to-one and onto.
This is section 2.1 on matrix operations.
Just some basics out of the way first.
We refer to an m by n matrix A as a matrix with m rows and n columns.
And the individual elements in the matrix we denote by lowercase a with a subscript ij where i is the row index and j is the column index.
The diagonal entries in a matrix A are those where the row and column indices are the same.
So like a11, a22, and so forth.
These elements we call the main diagonal of a.
A diagonal matrix is a square matrix where all the entries off the main diagonal are zero.
So the only place you can have non-zero values is on the diagonal.
So here are a couple of diagonal matrices.
Notice that the key is off the diagonal.
You have all zeros.
So the main diagonal is just one, one, two, two, and so forth.
Three, three in this case.
So off the diagonal, off the main diagonal you have all zeros.
It's okay to have a zero on the diagonal.
We really don't care what's on the diagonal.
We just want zeros off the diagonal.
Here's another diagonal matrix.
This one has a name.
We call it the identity matrix.
So it's a diagonal matrix because everything off the diagonal is zeros.
And the zero matrix is technically a diagonal matrix because everything off the main diagonal is zero.
We say that two matrices are equal if they have the same size, so the same number of rows, same number of columns,
and their corresponding entries are equal.
So the one one entry in one is equal to the one one entry in another.
In general, the ijth entry in one is equal to the ijth entry in the other.
We can compute the sum of two matrices.
We add two matrices together.
This is defined if both matrices are the same size.
So they have the same number of rows and same number of columns.
In this case, the ijth entry in the sum is just the sum of the ijth entries in each of the original matrices.
So here's an example one that this sum cannot be computed because these matrices are not the same size.
Here's one where we can't compute the sum.
And notice the seven here is just the three plus four to get the three.
It's this two plus one and so forth.
So to get the ijth entry over here, you sum the corresponding ijth entries in the two original matrices.
We can compute the scalar multiple of a matrix in the same way we compute the scalar multiple of a vector.
We just multiply each entry by that scalar.
So here's an example.
Multiply three times each entry in this matrix and we produce this one.
Your book lists and properties of addition and scalar multiplication of matrices.
The first three are regarding addition.
So you'll find that since addition of matrices is defined essentially the same as addition of real numbers,
then a lot of those same properties carry over.
So a plus b is b plus a.
The second one allows you to associate the parentheses as you'd like.
So that's the associative property.
And if you add any matrix to the zero matrix, you get the same matrix back.
Then the last three are regarding scalar multiplication.
And essentially they say that you can kind of stick the scalar wherever you'd like to, wherever it's convenient.
All is equivalent.
Okay, let's move on to matrix multiplication.
Because matrix multiplication is not defined simply as matrix addition is.
It's more complicated.
It's not just multiplying the corresponding entries together.
So here we go.
If a is an m by n matrix and the sizes are important here.
So suppose a is m by n and b is n by p.
Okay, so that's the key.
The number of columns in the first has to equal the number rows in the second for the product to be defined.
Okay, so if you have that situation and we say that the columns of b are b1 through bp,
then we define the product ab as the m by p matrix.
And notice that the m is number rows in a, p's number columns in b.
So we kind of always look to see if these inner dimensions match up.
So the n here matches up with the n here.
And that means the product is defined.
And then the product itself, the dimensions of it will be the first, the number of rows, which is m, by the number of columns of b.
So it will be m by p.
Okay, so how do we compute a times b?
Well, notice what this is.
It's the m by p matrix whose columns are a times b1 out to a times bp.
That is, we want to multiply a times b.
Then we look at it as a times each of the columns of b.
Okay, so the first entry, the first column of the product is a times the first column of b.
The second column of the product is a times the second column of b, and so forth.
So notice that each column of the product, each column of a, b, is of the form a times b1 or a times b2.
And we know that when we compute a times some column, then we are taking a linear combination of the columns of a using the entries in the vector as the weight.
So to get the first column of a times b, we're taking a linear combination of the columns of a using the first column of b as the weights.
To get the second column, another linear combination of the columns of a using the entries in the second column of b as the weights and so forth.
Okay, so if we want to compute the product a, b, where here are matrices a and b,
then we're going to do it as I just described.
First, note that a is a 3 by 2 matrix and b is 2 by 2.
So we check to see if the 2 here matches up with the 2 here, which it does.
And then we look at the outer dimensions, the 3 and the 2, and that will be the dimensions of the product.
So a product will be 3 by 2 in this case.
To get the first column of the product, it's a times the first column of b.
So here's a, first column of b, 4, negative 1, and this is just a linear combination.
So 4 times the first column of a minus 1 times the second column.
And this is what we get.
Do the same thing to get the second column of the product.
It's a times the second column of b.
Second column b is 1, 8.
So we take that linear combination 1 times this first column of a plus 8 times the second column.
Do that linear combination.
And then we have our first and second columns of the product.
And so we're done.
There is a times b.
There's another way to compute a, b.
Probably most of you learned this when you took algebra, college algebra probably.
Notice that it's defined in terms of inner products.
So the ijth entry in the product, a, b, is the inner product of row i of a with column j of b.
So let's see how that works.
So this is back to our same a and b.
And to get the 1, 1 entry in the product, then it's going to be row 1 of a times column 1 of b.
Actually, these should not be a's.
They should be the product.
I'm talking about the product here.
So row 1 of a times column 1 of b.
Do that inner product.
So it's going to be 3, 2 inner product with 4, negative 1 to give you 10 in this case.
And so forth.
So to get the 3, 2 entry of the product, it's row 3 of a times column 2 of b.
So 5 times 1 plus 4 times 8.
It gives you 37.
So there you go.
You have the same matrix as we ended up with before.
Okay.
Now you should note that since matrix multiplication is not defined simply,
because it's not a simple operation like matrix addition is,
then properties of real numbers that apply to multiplication do not follow over to matrix multiplication.
They did with matrix addition.
They don't with matrix multiplication.
So some warnings.
If you're dealing with real numbers, a times b equals b times a.
That does not hold true for matrices.
A times b is not necessarily equal to b times a.
Of course it sometimes could be, but in general, a times b is not equal to b times a.
One thing, one reason is that sometimes they're not even both defined.
Not even both defined.
Maybe you can multiply a times b, but you might not be able to multiply b times a and vice versa.
However, even if they are both defined, you can't guarantee they'll be equal.
So here's an example.
Here's a matrix a times b, and we get this product.
If we reverse the order multiply b times a, we get a totally different matrix.
So just because you can multiply both, compute both products,
that doesn't mean that they'll be the same result.
So don't make that mistake.
Second warning.
For real numbers, if you know that a times b equals a times c,
and you know that a is not equal to zero, you can divide both sides by a and end up with b equals c.
You can't do that with matrices.
If a, b is ac, that doesn't mean that b equals c.
Okay, here's an example of that.
Here's a matrix a.
A times b is this matrix.
Here's a times c.
Get the same matrix, but b and c are clearly different.
And another one.
The last warning.
For real numbers, if a times b is equal to zero, you can assume that either a or b is equal to zero,
or perhaps both are equal to zero.
Not true for matrices.
You have a product of two matrices that equals a zero matrix.
That doesn't mean that a or b is equal to the zero matrix.
And here's an example of that.
Here's matrix a times b, and we end up with a zero matrix.
But neither of these is equal to the zero matrix, far from it, in fact.
Okay, some properties of matrix multiplication.
You have the associative property.
So you can move the parentheses around, but notice that the order that the matrices appear is unchanged.
Okay, so we can't move the, we can't change the order that the matrices appear.
Can change the order in which we do the multiplication.
We can distribute either from the left or from the right here in numbers two or three.
Oops.
Back, back, back.
Oops.
What about that?
And if we have a scalar, we can move it wherever we'd like.
And apparently I thought that was quite an important thing since I've got an exclamation mark there.
And any matrix multiplied by the identity is just itself.
Now notice that I'm assuming a is m by n here.
So if I multiply on the left, I have to multiply by the m by m identity.
Whereas if I multiply on the right, I have to multiply by the n by n identity.
Okay.
One other thing, the transpose of a matrix is computed by taking the ijth entry of the original matrix,
and that becomes the j ith entry of the transpose.
So for example, if this is your matrix A, then I take the 1, 1 entry here, becomes the 1, 1 entry here.
1, 2 entry becomes the 2, 1 entry, and so forth.
I mean, perhaps an easier way to think of it is that the columns of A become the rows of A transpose.
So column 1 is 3, 1, 5, row 1 of the transpose, 3, 1, 5, and so forth.
Or alternatively, the rows of A become the columns of the transpose.
So here's a row, first row here, 3, 2, first column here, 3, 2, and so forth.
Some properties of the transpose.
If I take the transpose of the transpose of a matrix, I'm back where I started.
You can look here, if I take the transpose of this matrix, then I end up with A.
Row 1 here becomes column 1 of A and so forth.
So the transpose of the transpose is the original matrix.
The transpose of a sum is the sum of the transposes, number 2.
You can move a scalar around, number 3, number 4, kind of interesting.
If you want to take the transpose of a product, that is the product of the transposes, but in reverse order.
So here's an example of that.
If I want to compute the transpose of a product, AB, so here's A, here's B.
I want to compute their products, then take the transpose.
So I compute the product, here it is, transpose that, and end up with this matrix.
Or I could do it this way.
Transpose each of them initially, but reverse the order that I do the multiplication.
So here's B transpose, here's B up here, so I take the transpose, that's here.
Here's A transpose, and I compute that product, and I get the same thing as I did previously.
Okay, let's talk about the inverse of a matrix.
First, let's go back and think about the inverse of just a scalar, or real number A.
The multiplicative inverse, as opposed to the additive inverse of A, number A is just 1 over A, assuming A is not equal to 0.
Because if we multiply A times 1 over A, we get 1, or 1 over A times equals 1.
1 is the identity element for multiplication.
We can use the same idea for matrices.
The inverse of a matrix A exists only under certain conditions, so not every matrix has an inverse.
Just like not every number has an inverse, 0 doesn't have an inverse.
Okay, we're going to write the inverse of A as A with a superscript negative 1, and we have A times A inverse.
We just read this as A inverse equals the identity matrix.
Okay, this is not the number 1, this is the identity matrix.
And also A inverse times A is the identity matrix.
One of the requirements for A inverse to exist is that A is a square matrix.
And you have to have that so that you can multiply on the left and on the right by the same matrix.
Let's start off by talking about finding the inverse of a 2 by 2.
In this case, there happens to be a specific formula for the inverse.
So if we have a 2 by 2 matrix A that looks like this, A, B, C, D, then A inverse exists if A times D minus B times C is not equal to 0.
Okay, so that's A times D, so multiply the elements on the main diagonal and subtract off the product of the elements on the off diagonal.
So A times D minus B times C, I call it the crisscross applesauce.
A times D minus B times C.
Alright, so if that's not 0, A inverse exists and here's a formula for it.
Notice that we divide by AD minus BC, that's why it has to be nonzero.
The matrix is formed from A by swapping the numbers in the A and D positions.
Okay, so we swap A and D and negate B and C.
Okay, so that's how I remember it.
You swap A and D, negate B and C.
So for example, here's a matrix A.
If we look at the AD minus BC, that's 4 times negative 3, so negative 12 minus negative 9 times 2, that's negative 18.
So negative 12 minus negative 18 would be negative 12 plus 18, which is 6.
So we're dividing 1 over 6, this thing here is 6.
And then we swap the 4 and the negative 3, they swap positions and negate the negative 9 and 2.
And then multiply that out and we get this matrix.
Okay, so that's the inverse of A.
And if you check, multiply A times A inverse, you get the identity matrix.
If we multiply A inverse times A, we also get the identity matrix.
Okay, I also get another example.
Here's another 2 by 2 matrix.
But in this case, if we multiply the AD minus BC, we got 1 times 4 minus 2 times 2, which gives us 0.
So that tells us that this matrix does not have an inverse.
And if you look at that matrix a little bit, you see that the second column is just 2 times the first.
Okay, those columns are multiples of each other, so they're linearly dependent.
If we look at A in reduced echelon form, it has a row of all zeros, right?
So we have a free variable.
To the point, we say that A is not row equivalent to the identity matrix.
And it turns out that if A, any matrix A, if it's not row equivalent to the identity matrix, then it will not be invertible.
And if it is row equivalent to the identity matrix, it will be invertible.
Okay, we don't have a formula for the inverse of a 3 by 3.
So we need to develop a method for computing the inverse or for determining that the inverse does not exist.
So what we'd like to do is find a matrix, let's just call it B, such that A times B is equal to the identity matrix.
Okay, so if we assume that A is N by N, then that means that B also has to be N by N.
And we'll denote the columns of B by these column vectors, B1, B2 through BN.
So recalling from the previous section, if we multiply A times B, you can write that in this form, A times the matrix, here's B, columns of B, B1 through BN.
And that equals just A times B1 in the first column, A times B2 in the second column, and so forth.
So remember this, that A times B, you get the first column as a linear combination of the columns of A using the elements in B1 as the multipliers, and so forth for the other columns.
So remember we want that to equal the identity matrix, so here's the identity matrix.
So we want A times B1 to be the first column of the identity matrix, A times B2 to be the second column, and so forth.
So in order to determine the matrix B here, we need to solve N systems of equations.
So if we solve those N systems of equations, we'll have an inverse, if it exists.
So let's look at this matrix and see if we can follow that method.
It's a two by two, so we could just use the formula we have, but let's try applying this method that we just discussed.
Alright, so A times B, A times B1, A times B2, and we want that to equal the identity matrix, so we have these two systems to solve.
A times B1 is 1-0, A times B2 is 0-1.
So here we go to solve this system, set up an augmented matrix, do some row operations, and notice that A, to this point you see that A is row equivalent
to the identity matrix, so that means that A inverse does exist.
And here's the first column, B1 is negative one-half, three-half, so that's the first column of the inverse.
And to get the second column, we solve this system, A times B2 equals 0-1, the second column of the identity matrix.
And again, I set up my augmented matrix, tack on 0-1, do some row operations.
And notice that the row operations that I'm doing here are exactly the same ones that I did up here.
So first multiply row one by one-fourth, same thing down here.
Then nine times row one plus row two, same as down here, and so forth.
And that carries through the whole way.
And if you give that some thought, you'd say, oh yeah, that makes sense,
because the row operations that I'm doing are based on the entries in A.
I'm trying to get A in reduced echelon form.
So since A doesn't change, same A here and up here, then it makes sense that the row operations I do don't change.
The only thing that's changed is what's in the last column.
And so we end up with this for B2.
So I've got my two columns of B, which is what the inverse of A is.
And if you look at this, you think, well, that seems like to be a lot of repetitive work, and it is.
And so we have a more efficient method, and that is to solve both systems at one time.
So instead of just tacking on one column in an augmented matrix, we tack on both columns,
or the whole identity matrix, depending on what size your original matrix is.
And here we go through that same sequence of row operations, and we end up with the two columns of B that we found earlier.
And this, of course, is the inverse of A.
Now, in general, what this looks like is you start off with this matrix, A on one side, then augment on the identity matrix.
And you do row operations until you get A in reduced echelon form.
And what you're aiming for is to make it look like the identity matrix.
And they said that may or may not be possible, depending on A.
If it is, then you'll end up with the identity matrix here, and the inverse of A on the right.
So that's what you're aiming for.
And just while we're here looking at this, notice that we could go backwards,
because we know that all these row operations are reversible.
We could go backwards, and look what happens.
We start off with A inverse with the identity tacked on, and we get back here,
and we can make A inverse look like the identity matrix.
And so with A over here, so that tells you that if you wanted to take the inverse of A inverse,
then you just get the original matrix A back.
So the inverse of A inverse is just A.
Okay, so what happens if A is not invertible?
In this case, it's not row equivalent to the identity.
So here's the example we had before, where the second column is the multiple of the first,
tack on the identity matrix, do one row operation here, and you get to this point,
and you say, oh, A is not row equivalent to the identity matrix,
and hence I can't get it in this form, where I've got the identity and then A inverse.
So that tells you that the matrix A does not have an inverse.
Okay, so if you can't get A to look like the identity matrix by doing row operations,
then A is not invertible, or A inverse does not exist.
All right, now this method works no matter what size your matrix is,
so I scaled it up for a three by three.
Okay, start off with this matrix A and tack on the three by three identity matrix
and go through some row operations.
A lot of arithmetic here, and if you look at the last matrix I have here,
the first three columns look like the identity matrix,
and so sitting over here would be the inverse of A.
All right, so this last three columns would be the inverse of my matrix A.
If you know A inverse, then solving AX equals B is trivial,
because our whole focus in this course is solving AX equals B, solving systems of equations,
and so if you know A inverse, then it turns out that it's super simple
to solve a system of equations involving A, and basically here's the rationale.
Start off with AX equals B, and you know A inverse exists,
and that means you can multiply both sides of your system by A inverse, so I do that.
The reason to do that is because if you look over here, we can re-associate the parentheses,
and so we can end up with A inverse times A together,
and the advantage of that is that that equals the identity matrix,
so we end up with just the identity matrix times X,
and anything times the identity matrix is just that anything, so I times X is X,
so X is simply A inverse B.
So if you know A inverse, then you don't need to do row operations,
you don't need to do any of that stuff, just multiply A inverse times B,
and you have the solution to your system.
Clearly, that's going to save you a lot of work if you have A inverse.
So if I have this matrix A and this vector B, and I want to solve AX equals B,
if I don't know A inverse, then I got to go through, you know, set up my augmented matrix,
do all these row operations, and I end up here with my solution for negative 7, negative 16.
But if I know A inverse, so I know A inverse, so here's A, here's B, and I know A inverse,
then to solve AX equals B, as I said, just multiply A inverse times B, and here's your solution.
So back, went through all this work to get 4, negative 7, negative 16.
Here, if you know A inverse, all you do is simple multiply matrix times the vector,
and you have the solution.
Now, of course, the downside is that most of the time you don't have A inverse,
and to get A inverse, well, you saw what work is required in that,
there's more work than simply going back and through to do this,
because to find A inverse for 3 by 3, we had to solve, let's go back, right?
We had to solve three systems of equations.
Just to solve this system, you need to solve one system of equations.
But if you know A inverse, then solving system is trivial.
Okay, let's see.
Your book lists some properties of matrix inverses.
This one we've already talked about.
Take the inverse of A inverse, you get A back.
If you have the inverse of a product, so AB quantity inverse,
that is the product of the inverses, but in reverse order.
So B inverse times A inverse, instead of A inverse times B inverse.
And if you take the inverse of a transposable matrix,
that's equivalent to transposing the inverse.
Okay, the main part of section 2.3 is the invertible matrix theorem.
And this theorem actually ties together everything that we've done so far in the course, actually.
Everything in chapter 1 and up through chapter 2 where we are.
So this theorem says if you have a given N by N matrix A, the following are equivalent.
That means that they're either all true or all false.
Okay, the first two will look like this.
One, A is an invertible matrix.
And number two, A is row equivalent to the identity matrix.
So that should make sense because the way we found out if A is invertible is to do those row operations
and try to get it in the form of the identity matrix.
And if we could do that, then that means A is invertible.
If we couldn't do that, then A is not invertible.
Okay, another statement, A has N pivot positions.
So this is actually the key to the whole theorem in my opinion.
Everything here you can relate to pivot positions and that's how I would recommend that you approach this.
So looking ahead, you need to know what this theorem says.
You need to be able to write down parts of this theorem.
I don't suggest that you memorize it because I think that's somewhat difficult.
But I don't, so don't memorize it unless that works for you.
But the way I would suggest that you remember it is to relate everything back to pivot positions
because that's what we've done up to now and that should still work for you.
So let's look at these next three and reinforces the equation AX equals zero has only the trivial solution.
Okay, so if it has only the trivial solution, then that means that there are no free variables.
That means every column of A is a pivot column.
And so that means since there are N columns, there's N pivot columns, so N pivot positions.
Okay, so that relates back to pivot positions.
Number five, the columns are linearly independent.
How do we know that?
Well, again, no free variables, so pivot position in every column are N pivot positions.
And number six, the transformation X to AX is one to one.
And how do we know that?
Well, that means we want to have a unique solution to every system AX equals B.
Or at most one solution.
There are no multiple solutions for any right hand side.
So that means that we can't have any free variables.
So again, pivot position in every column.
So four, five, and six all relate to the fact that you need a pivot position in every column.
These next four relate to having a pivot position in every row.
Okay, which again, since it's an N by N matrix, that would mean you have N pivot positions.
So number seven says AX equals B has at least one solution for each B and RN.
So if that's straight, that means that system is consistent no matter what the right hand side is.
That only happens when you have a pivot position in every row.
N rows means N pivot positions.
Columns of A span are N.
Okay, that again means that there's a pivot position in every row.
No matter what you put on the right side, the system will be consistent.
So again, pivot position every row, so N pivot positions.
And number nine, X to AX is on to RN.
So that means again, there's a solution no matter what the right hand side of the system AX equals B is.
So there has to be a pivot position in every row.
And then last, A transpose is invertible.
It's kind of a tag along there, but if A is invertible, A transpose is invertible.
Okay, so I've taken a few of the even-numbered problems from this section just to kind of talk through those
to give you an idea about how to go through some of the logic here.
So here's one.
Is it possible for a five-by-five matrix to be invertible when its columns do not span R5?
Okay, well, let's see.
If the columns don't span R5, then that means that there's not a pivot position in every row.
And that means that the matrix is not row equivalent to the identity matrix,
or you could say that there's less than five pivot positions.
Either one of those gets you to the matrix cannot be invertible.
All right, here's one.
Got a six-by-six matrix, and the equation CX equals V is consistent for every V in R6.
Is it possible that for some V, the equation CX equals V has more than one solution?
Okay, well, if CX equals V is consistent for every V in R6,
then that means that there must be a pivot position in every row.
And since this is a square matrix, if there's a pivot position in every row,
then there's also one in every column.
And if there's a pivot position in every column, that means there's no free variables,
and so hence, any system will, there will be no system that has an infinite number of solutions.
Only a unique solution.
All right, let's suppose H is N by N, and the equation HX equals C is inconsistent for some C.
Does the system HX equals zero have non-trivial solutions?
Well, if HX equals C is inconsistent for some right-hand side,
then there can't be a pivot position in every row.
And that means that there can't be a pivot position in every column since H is a square matrix.
And if there's not a pivot position in every column, then that means you have free variables,
and hence, there are non-trivial solutions to HX equals zero.
Okay, another one. If L is N by N, and LX equals zero has only the trivial solution,
do the columns of L span R, N?
Well, if LX equals zero has only the trivial solution, then that means there's no free variables.
So we have a pivot position in every column of L,
which means we have a pivot position in every row of L since L is square.
And if there's a pivot position in every row, then that means that the columns span R, N.
All right, let's talk about invertible linear transformations.
Let's suppose we have a linear transformation T from R, N to R, N that's defined by the matrix A.
So A here would be N by N.
All right, for any X, we can compute AX, right?
For any X, we can compute T of X or A times X.
Let's think about going backwards, though.
Suppose you have some B and you would like to know,
is there a unique vector X such that AX equals B?
So think of B as being in the co-domain and we want to go backwards to see what X mapped to that B.
All right, well, this works if T is both one-to-one and onto.
So for us to be able to go backwards and find out what X mapped to B,
we can do that if we know that T is one-to-one and onto.
Okay, so why is that?
Well, if T is onto, then that means that every B and R, N is mapped to by at least one X, right?
Every B is mapped to by at least one X.
So that means that if we go backward, there's an X to go back to.
If it's one-to-one, then that means that every B is mapped to by at most one X, right?
Every B is mapped to by at most one X.
So that means that if we can go backwards, there's only one vector to go backwards to, right?
We don't have multiple vectors mapping to the same B.
So if you put those two things together, T is one-to-one and onto,
then every B is mapped to by exactly one X, right?
Which means there's a unique solution.
So how do we find that X?
Given a particular vector B, how do we find out what X mapped to it?
Well, we need to solve AX equals B.
And the way to do that, if we know that A is invertible is to simply multiply A inverse times B.
So the matrix that defines the inverse transformation T inverse is simply A inverse, okay?
So A takes you forward, right?
From X, you multiply A times X to get B in the co-domain to go backwards.
To reverse that, you multiply by A inverse.
So T is defined by A, T inverse is defined by A inverse.
Another way to look at it is this.
Under what conditions does there exist a unique vector X such that AX equals B for every B in our N?
When can we know that the system AX equals B is always consistent and always has a unique solution?
Well, it's consistent if there's a pivot position in every row of A.
AX equals B is consistent for every B if there's a pivot position in every row of A.
And if the solution is unique, then there has to be no free variables.
So that means there has to be a pivot position in every column of A.
So backing up, it's consistent.
If it's consistent for every B, we have to have a pivot position in every row.
If there's a unique solution, then that means we have no free variables.
So we have to have a pivot position in every column.
So T of X equals AX is invertible when A has a pivot position in every row and column,
i.e. when A has N pivot positions, i.e. when A is invertible.
Okay, we're going to talk about determinants.
The determinant of a matrix is just a scalar value that is associated with any square matrix.
So we only talk about the determinant of a square matrix.
The notation is DET of A, or sometimes you see it with A.
It looks like absolute value of A with the vertical bars around A.
We've actually already seen the determinant in the 2 by 2 case.
If we have A as just this generic matrix A, B, C, D,
then the determinant of A is A times D minus B times C.
And we've seen that because we saw that in the little formula for the inverse of a 2 by 2.
So remember we multiplied by 1 over A D minus B C.
So we're multiplying by 1 over the determinant of the matrix.
And then we rearrange the terms and negate a couple to get the inverse of a matrix.
So notice that at least in the case of a 2 by 2 matrix,
we can see that the inverse exists when the determinant of the matrix is not equal to 0.
Here in this case, if A D minus B C is equal to 0, then A inverse does not exist.
And it turns out that this is true for any square matrix.
If the determinant is not equal to 0, that means the matrix is invertible.
And if it's invertible, then the determinant is not equal to 0.
So this is actually another installment in the invertible matrix theorem,
which we had from section 2.3.
So determinant of A not equal to 0 is logically equivalent to A is invertible.
All right, if we move up beyond the 2 by 2,
there's no nice formula for competing the determinant like there was for a 2 by 2.
And we use a method called cofactor expansion.
And this method works for any size matrix, 3 by 3 on up.
In this method, we have to choose a row or column to expand about.
That's what we call it.
We're going to choose a row or column to expand about.
And you'll see as we go on that it's advantageous to choose a row or a column
that has the most zeros in it because that eliminates some of the terms.
But for our first cut here, I'm just going to expand about the first row.
And I've kind of color coded this to make it easier for you to see where the terms come from.
Okay, so if we expand about the first row, we start with the first entry, which is the 5.
And then we multiply 5 by the determinant of the matrix that you're left with
if you eliminate the row or the column that contain the 5.
So if you eliminate the first row and eliminate the first column,
then you see that you have this little 2 by 2 matrix that's sitting right here.
Okay, and so we're going to take the determinant of that.
And then we move over to the next entry in the first row, which is the 2.
And we multiply that by the determinant of the matrix that you get
if you eliminate the first row and the second column.
Okay, the row and the column containing the 2.
So we're left with the 0, 2 here and the negative 5, 7 here.
Okay, so that's how we get this matrix here.
Then we continue moving across the first row.
Then we've got a 4 and we multiply 4 by the matrix that you get
if you eliminate the first row and the third column.
Okay, eliminate the row and the column containing the 4.
So you're left with this little 2 by 2 here, 0, 3, 2, negative 4.
And that's what we have here.
Now notice I haven't combined these terms at all.
I've just written them out here.
And that's because there's a method for combining the terms.
Okay, we have to put them together somehow.
And basically you apply a coefficient to each term,
which is negative 1 to the i plus j,
where i and j are the row and column indices corresponding to that term.
So if we look back up to the 5, 5 came from row 1, column 1.
So the coefficient that we put in front of that term is minus 1 to the 1 plus 1
because 5 came from row 1, column 1.
Alright, then for the next term, this one, the 2, the blue term,
came from this entry here.
The 2 is in the first row, second column.
So that means the coefficient here is minus 1 to the 1 plus 2.
Row 1, column 2, that's where the 1 plus 2 comes from.
And then for the 4, the green term, it's in the first row, third column.
So we have minus 1 to the 1 plus 3 as the coefficient in front of that term.
And so if we multiply and add, then what do we get?
We get minus 1 to the 1 plus 1, so that's minus 1 squared.
So that's just plus 1 times 5, so we bring down 5.
And then the determinant here, remember it's just the crisscross,
3 times 7 minus negative 4 times negative 5.
So that's what we have here.
Alright, then moving on here to the blue, we got minus 1 to the 1 plus 2,
that's minus 1 cubed.
So that's negative 1 times 2, gives us the negative 2 here,
times the determinant, which is 0 times 7 minus 2 times negative 5.
Alright, then moving over to the next term, the green one comes from the 4,
and that's first row, third column, so that's the minus 1 to the 1 plus 3,
which is minus 1 to the fourth, which is positive 1 times 4,
and so you get a plus 4 in front of that term,
times the determinant, which is 0 times negative 4, minus 2 times 3.
Alright, then combining a little more, we got 21 minus 20 here, times 5,
then we've got 0 plus 10, so just a 10 there,
and then here's 0 minus 6, so negative 6 there,
and we combine and we end up with negative 39 for the determinant of this matrix.
Okay, now just a little aside, it's really not necessary to explicitly compute
this minus 1 to the i plus j term every time.
If you just look, here's a 4 by 4, where I've put as the entries in the matrix,
just the row index plus the column index.
So 1, 1 position, we got 1 plus 1, 1, 2 position, 1 plus 2, and so forth.
So if we look at what that is, we end up with these numbers,
remember we want minus 1 raised to each of these powers,
and so you can see, since they all differ by one,
going from one term to an adjacent term, either in the same row or the same column,
either go from an even number to an odd or an odd to an even,
so in any case, alternating terms are always going to have opposite signs,
and you always start off in the upper left with a plus, plus 1,
because that's minus 1, but the 1 plus 1, or minus 1 squared.
So you always know that the 1, 1 position is a plus term,
and then everything alternates after that.
So it's always plus, minus, plus, minus.
So if you know the sign that goes with the first term,
in the row or the column that you're expanding about,
then you only need to alternate terms after that.
So you always have this checkerboard patterns,
and you just need to alternate terms based on the sign of the position where you start.
So let's look at this matrix again, it's the same one,
but I'm going to expand about the second column this time,
just for something different.
Now, first note that the second column, the first entry there is the 2,
and it's in the 1, 2 position, so minus 1 to the 1 plus 2,
if you want to compete it like that, minus 1 cubed, that's a negative 1,
so this is a negative position here.
Or you can say, I always know that the 1, 1 position is a plus,
so this is plus, the next one over has to be a minus.
That's typically how I do it.
So we end up with minus 2 times, again, the determinant of the matrix
that you get if you eliminate the row and the column containing the 2,
so we end up with a 0, 2, negative 5, 7, so that's where we end up with that matrix.
Alright, then we move to the 3, now since the 2 was a negative,
the 3 is going to be a plus, and we eliminate the row and the column containing the 3,
so we're left with 5, 4, 2, 7, and then we move on to the negative 4,
so again, if you forget, we'll go back to the 1, 1, that's a plus,
moving over, that's a minus, moving down, that's a plus,
moving down again, that's a minus, so we're going to subtract off,
that's where the minus comes from, minus negative 4 times the matrix
that you get if you eliminate the last row and the middle column,
so we've got 5, 0, 4, negative 5.
Alright, we compete those determinants.
Here we're going to have 0 times 7 minus 2 times negative 5,
here 5 times 7 minus 2 times 4, and here 5 times negative 5 minus 0 times 4,
and we can simplify, and we end up with negative 39 again, which we should,
because no matter which row or column you choose to expand about,
you should end up with the same answer.
Alright, let's move on to a 4 by 4, alright?
Now, we're going to take advantage of the fact that we've got a row with 3 zeros in it,
so I'm going to expand about the second row,
and if you notice the first entry in the second column,
okay, there we go, expand about the second row to take advantage of the zeros,
so if we look, the 0 here is, or go back to the 1, that's a plus position,
so we move down, that's a minus, so the first term is going to be minus 0
times the determinant of the matrix that you get if you eliminate the second row
to the first column, so you can see how the first row is going to be negative 252,
that's what we get there, then we've got these two rows right here as the second two rows.
Alright, then we move over to the next entry, okay, plus here, minus,
so this is a plus entry, so it's plus 0 times the determinant of the matrix that you get
when you eliminate the second row, second column, alright, then minus the next term,
so minus 3 times the matrix that you get if you eliminate second row, third column,
and then plus 0 times the determinant of the matrix that you get when you eliminate
the last column and the second row, so that's where all these terms come from,
and you can see the advantage of choosing a row with a bunch of zeros because the red term,
the green term, and the orange term all just disappear because they are all multiplied by 0.
The blue one though, we have to compute now a 3 by 3 determinant,
so we have negative 3 times the determinant of this matrix,
so we use the same method, choose a row or a column to expand about,
I chose the third row, okay, so I've got plus minus plus, 5's in a plus position,
so it's 5 times this 2 by 2 determinant here,
then move over minus 0 times the 1, 2, 2, 5,
and then plus 4 times the determinant of this little matrix up here,
alright, and then plus 0 because this term is multiplied by 0,
so you can see that everything simplifies except for this blue part,
and we go through and evaluate these determinants,
and combine terms and simplify, we end up with negative 6 for the determinant of this matrix.
Now, just stop for a minute here and think about how much more work would have been required
if we didn't have any zeros in this matrix,
we would have had to done what we did with the blue part,
we would have had to do that for the red, and the green, and the orange,
so we get, this turns into significant work fairly quickly, okay,
it's a recursive method because, for example, to compute a 5 by 5 determinant,
you have to compute 5, 4 by 4 determinants,
now this is assuming there's no zeros in the matrix,
so the worst case for a 5 by 5, you have to compute 5, 4 by 4 determinants,
and each of those 4 by 4 determinants requires competing 4, 3 by 3 determinants,
and each 3 by 3 means you have to compute 3, 2 by 2 determinants,
as you can see this gets very work intensive very quickly,
the amount of work increases exponentially,
so clearly this method does not scale well at all,
your matrix gets bigger,
the amount of work required increases exponentially,
so it's not a good tool,
and our good method to use to compute the determinant for an arbitrary size matrix,
3 by 3 is okay, 4 by 4 really turns into too much work
unless you've got some significant number of zeros there,
now let's go off on a little different tangent here for a bit,
and first define what a triangular matrix is,
a triangular matrix is a square matrix
in which all the entries either above or below the main diagonal are zero,
so here are some examples,
I've highlighted the zeros in red just to make it clear,
the first one here has all zeros below the main diagonal,
so we say that this matrix is upper triangular,
so all the interesting stuff is in the upper part of the matrix,
this next one, the middle one, is lower triangular,
because all the interesting stuff is in the lower part of the matrix,
everything above the diagonal is zeros,
and then here's another one, this one is upper triangular also,
and I just want to make it clear that what defines a triangular matrix is that
either above or below, or even both, the diagonal, you have to have all zeros,
so this one is upper triangular because below the diagonal we have all zeros,
and it's okay to have some zeros above the diagonal or even on the diagonal,
but that has really nothing to do with whether this is a triangular matrix or not,
this is a triangular matrix because everything below the diagonal is zeros.
Alright, so we have a theorem that says if A is a triangular matrix,
then the determinant of A is the product of the entries on the main diagonal of A,
so this is making it easy to compute the determinant if your matrix is triangular,
and it's pretty easy to see how that works.
If we take a triangular matrix, then if we were going to compute the determinant directly,
then I would expand about the first column, so my determinant would be one,
that's plus, so one times the determinant of the matrix that you get
if you eliminate the first row and first column,
so you get this 3 by 3 that we take here,
and notice that all the rest of the terms will be zero,
so it's one times this determinant plus this zero here times,
or actually minus, this zero here times another determinant,
plus this zero times another determinant,
minus this zero times another determinant,
so the only term that's non-zero is the one associated with the one,
so that's the only one I've written here,
and then if we take the determinant of this 3 by 3,
notice we do the same thing, expand about the first column,
so we've got one times five, because five's in the one-one position,
so that's a plus, so one times five times this 2 by 2 sitting right here,
and notice again, all the other terms in that column are zero,
so we don't need to worry about them,
and so we end up with one times five times this determinant,
and so it's going to be eight times one minus zero times nine,
so it's just eight times one,
so our determinants one times five times eight times one,
which you see are the entries on the main diagonal,
so if your matrix is triangular, life is good, life is easy,
just multiply the entries on the diagonal.
Just thinking ahead just a little bit,
notice that a triangular matrix is an echelon form,
or if it's upper triangular, it's an echelon form.
If it was lower triangular, we could swap rows,
and we'd have to swap some columns too,
but let's just think about upper triangular at this point.
Now, so something to think about,
to compute the determinant of a large matrix,
let's just say large is four by four or bigger,
could we first put an echelon form
and then just take the determinant by multiplying the diagonal entries?
Would that work?
Well, to answer that, we need to explore
how elementary row operations affect the determinant of a matrix,
so if we can nail that down,
then it is indeed possible that we could just put our matrix in echelon form
and then easily compute the determinant that way.
So that's what we'll ponder,
and we'll discuss that in the next section.
Okay, last time we talked about how to compute the determinant of a matrix,
and we discovered that as the matrix got larger,
the amount of work required to compute the determinant increased dramatically.
Even a four by four without any zeros in it,
as a considerable amount of works, take the determinant up.
So we thought that,
and we also saw that if a matrix is triangular,
then computing the determinant is easy,
you just multiply the diagonal elements.
And so we thought, hmm,
I wonder if we could just do some row operations
and either just generate some zeros
or get the matrix totally in echelon form,
which would be upper triangular form,
and then compute the determinant
just so we can eliminate some of the work required.
So we're going to first talk today about how row operations affect the determinant,
because if you do row operations,
you need to know what effect that has on the determinant
if you're going to try to go down that road in order to compute the determinant.
Okay, so first thing we'll look at is the effect of swapping rows.
So the example I have here,
I just swapped the rows.
So the first matrix I call A,
swap the rows, call that matrix B.
And obviously we know the determinant of A is just AD minus BC.
The determinant of B is BC minus AD,
which is the negative of AD minus BC.
So we have the determinant of A equals minus the determinant of B.
And this holds true no matter what size your matrix is.
So we have a theorem that says
if you have a matrix A
and you exchange two rows or swap two rows of A to produce B,
then the determinant of B is just the negative of the determinant of A.
Okay, so if you swapped rows again,
you'd negate the determinant again.
So I suppose we have this example.
It's a three by three.
Let's just suppose the determinant of this matrix is T.
Then we swap the first two rows
and the determinant of this matrix,
we've negated the determinant of the original one.
So the determinant of this one is negative T.
Then if we swap two rows again,
so to get to this matrix here,
I have interchanged the second and third rows of the previous matrix.
And so I've negated the determinant again.
So the determinant will be negative of the determinant here,
which is negative T.
And notice we're back to where we started.
So if you interchanged rows,
that just negates the determinant.
So if you do two of them,
then you're back to where you started.
All right, another row operation.
Here we're multiplying a constant by one row
and adding to another.
So again, determinant of A is just AD minus BC.
Determinant of B is A times KB plus D minus B times KA plus C.
And we can do a little algebra.
Notice that the KAB terms, there's two of them,
and they cancel each other out.
So you're just left with AD minus BC.
So with this row operation,
you don't change the determinant at all,
which is kind of nice,
because we know that when we're doing row operations on a matrix,
this is the one we're doing 99% of the time.
And so it has absolutely no effect on the determinant.
Okay, so another theorem.
If a multiple of one row of A is added to another row
to produce a matrix B,
then the determinant of B is equal to the determinant of A.
Okay, the third row operation is multiplying a row by a constant.
So here I've multiplied row two by K.
So again, determinant of A, AD minus BC.
Determinant of B is A times KD minus B times KC.
In fact, we're out of K.
And we know K times AD minus BC.
So we see that the determinant of B is K times the determinant of A.
Or when you're going backwards,
you know, if you were using the determinant of B
to try to go back to get the determinant of A,
the determinant of A would be one over K times the determinant of B.
So another theorem.
If a row of A is multiplied by K to produce a matrix B,
then the determinant of B is equal to K times the determinant of A.
That holds true for any square matrix A.
Kind of tagging along with this,
I said what happens if we multiply the whole matrix by a constant?
So start off with our ABCD, multiply one row.
This is the matrix we had before.
Multiply it by row two by K to get this matrix.
Then I multiply the row one by K to get this one,
which is just equal to K times the whole matrix.
So we know the determinant of A is AD minus BC.
The determinant of K, A, take the determinant of this matrix.
We end up with K squared times AD minus BC.
Which makes sense, because we know what the determinant of this one is.
It was the one we looked at previously,
and it was just K times the determinant of A.
We did one more row operation, multiplying another row by K,
and so we should incur that K term one more time,
and hence we get the K squared.
So the determinant of K times the matrix.
Now this is just in general not a row operation here,
but if you just multiply K times the whole matrix,
you get K squared times the determinant of A.
That's for this case with a two by two.
Notice that if A had been three by three,
we would have had to multiply each of three rows by K,
and we would incur that K term three times,
not two, is in this case.
So in general, if A is N by N, and K is a scalar,
then the determinant of K times A is K to the N times the determinant of A.
So if A is three by three, you're going to get K cubed.
If it's four by four, you're going to get a K to the fourth term.
Okay, so here's an example of how you can use these ideas
that we've talked about to compute the determinant.
Now if we were just doing this one straight away using cofactor expansion,
we would clearly expand about the fourth column here.
But notice that it's simple to do one row operation here
and generate another zero in this column,
which will eliminate a significant amount of work.
You know, as it is, we would need to do two three by three determinants.
But if we do one row operation to generate a zero here where the six is,
then we've cut our work in half,
because then we only need to evaluate one three by three determinant.
So let's do two times row four, or negative two times row four plus row three,
and notice that that kind of row operation we call does not change the determinant.
Okay, so if we do that, we can do this row operation.
We get this matrix, notice a zero here where the six was.
And then we just expand about the last column.
So we need this three.
We need to figure out what sign goes with three.
So start here.
One one position is a plus, minus, plus, minus, plus, minus, plus.
So that's plus three times the determinant of this three by three matrix sitting right here.
Okay, so that's what I have here.
And then I've expanded about the third row,
which now I'm seeing that this is not looking right because this should be negative three here.
Oh, no, no, no, negative three here.
My mistake.
This three here comes from the ones outside.
And then when I expand about the third row, the negative three is right here.
All right, so in the negative three is in a plus position because we start with a one one.
That's plus, minus, plus.
So we have plus negative three, okay, times this two by two.
And then we got a zero, so we can skip that.
So it would be minus zero, then plus negative two.
So here's the other term.
And if we continue to work it out, we end up with a hundred and fourteen.
Okay, so that one's pretty straightforward.
I want to show you one more just that has a little more work involved.
And for that, I'm going to go to Maple just because that kind of makes it a little bit easier.
So I've got it set up.
Here's the matrix that we're going to start with.
And notice this, I guess I could do a row operation to generate a zero in the third column here.
That would probably make life easier.
That would be the way to go if you were actually doing this by hand.
But I would have wanted to illustrate the concept of how you could use these row operations.
So I have actually done the row operations to get it in triangular form.
And then I want to show you how to go back and recreate the determinant of the original matrix based on the row operations that we've done.
Okay, so the first thing I did was I just started out to generate zeros in the first column.
So I used the negative three here to generate a zero here and here.
So the first operation I did was to this command here add row.
What it does is take row A, so we're matrix A.
So we're operating on the original matrix A.
And we're going to change row three by multiplying row one by negative one.
So this operation is negative one times row one plus row three.
And we generated zero in row three.
Then I do row one plus row four.
So that's the next command.
Now I'm operating on A1, this matrix here.
And I'm going to change row four by multiplying row one by one.
So the negative three plus three gives me zero here.
Then I'm going to swap rows.
I'm going to swap the first and the second row because then that gives me a one in the one one position.
So I'm just swapping rows.
Now notice at this point we've negated the determinant.
So whatever if I took the determinant of A3, it would be the negative of the determinant of the original matrix.
So keep in mind that we've done one row swap.
Now I'm going to generate a zero here where the negative three is.
And so to do that I'm operating on A3, matrix A3.
I'm going to change row two by multiplying row one by three.
So three times row one minus three gives me zero there in that first position.
Okay now I'm ready to move over to the next column.
And so the first thing I'm going to do here since there's no nice numbers here to work with,
I'm going to just multiply row two by one seventh.
So the command to do that is this one, multiply row.
So I'm going to multiply row working on A4.
I'm going to multiply row two by one seventh.
So that gives me a one in the two two position.
And now I'm ready to zero out underneath that.
So my next operation is to work on A5.
And I'm going to multiply, I'm going to change row three by multiplying row two by negative six.
Alright so negative six times one plus this six gives me the zero here.
And then I need to do a similar thing to generate a zero in this position.
So I'm working on A6, multiply, or I'm going to change row four by multiplying row two by six.
So two times row two plus row four, I mean six times row two plus row four gives me a zero here.
So up to this point the only thing that I've changed in terms of the determinant of the original matrix is doing the one row swap.
So we've negated the determinant, oh we did the multiply row.
So we've changed the determinant by a factor of seven also.
Alright so keep those two things in mind.
And here the next thing I'm going to do is generate a one in this position.
So I'm going to do another multiply row, multiply row three by negative seven over twenty seven.
And that gives me the one in this position.
So now we've done a row swap and we've done two operations where we multiplied a row by a scalar.
So keep that in mind.
I'm going to do one more row operation to get a zero in this position.
So I'm going to multiply row three by negative thirteen sevenths and add that to row four.
So I get a zero there.
Now my matrix is in triangular form, upper triangular form.
So to take the determinant of this matrix it's just one times one times one times two.
And then to go back to get the determinant of the original matrix I've got to back out these row operations.
So that's what I've done here.
Okay so I've taken the determinant of A nine which is as I said one times one times one times two.
So we could just do that times seven and I'm multiplying times seven because of this row operation
because I've multiplied by one seventh to sort of back that out to get the original determinant.
So I've multiplied by seven and then I've multiplied by negative seven over twenty seven.
So I'm multiplying by the reciprocal of that and then multiplying by negative one because we did the row swap.
So put all that together I get the determinant of the original matrix is fifty four and then just to check
I'll just use maple to determine that original determinant and we get fifty four again.
So this is a way you can back out the determinant of the original matrix by keeping track of the row operations
that you did and how they affect the determinant.
Okay so back to this.
We're ready to look at a few theorems about determinants.
The first one here is actually another installment in the invertible matrix theorem.
Okay so it says a square matrix A is invertible if and only if the determinant of A is not equal to zero.
So we've seen this, we saw this last time, but add that on to the invertible matrix theorem.
So this is an equivalent statement, determinant of A not equal to zero is equivalent to A is invertible.
The determinant of A transpose is equal to the determinant of A.
If you think about that for a little bit it actually is pretty easy to see why that is true.
Because if you think about taking the determinant of A you choose some row or column to expand about.
For example maybe expand about the first row.
Then if you're taking the determinant of A transpose then you can do the exact same operations by expanding about the first column.
Since the first column of A transpose is equal to the first row of A then you would be computing the determinant of A transpose
that is exactly how you would compute the determinant of the matrix A.
So you end up with getting the same value.
Okay another theorem.
If A and B are square matrices then the determinant of the product A times B is the determinant of A times the determinant of B.
The product is the product of the determinants.
Notice though that that doesn't apply to sums.
So the determinant of A plus B is in general not equal to the determinant of A plus the determinant of B.
So don't make that mistake.
Okay and that's all we have for this section.
Alright let's talk about Kramer's rule.
Kramer's rule is actually one of the beautiful things in mathematics I believe.
It's just a neat little idea.
And it's a way to solve a system of equations using only determinants.
So it's just very interesting.
So here's the way it works.
So we have A is an n by n matrix.
Then for any B in Rn the unique solution x of Ax equals B is given by this formula here which looks rather odd.
Notice that we've got a way to compute x of i for each i.
And it's just a ratio of determinants.
In the denominator we've got just the determinant of A.
In the numerator it's the determinant of A sub i of B.
So A sub i of B refers to the matrix formed by replacing the i-th column of A with B.
So replace the i-th column of A with B.
So to get x1 we replace the first column of A with B and compute that determinant and divide by the determinant of A.
That's the value for x1 and so forth.
It's a neat idea.
So let's just use it to solve this system.
Alright so we need the determinant of A.
That's just 8 minus 5 which is 3.
The determinant of A sub 1 of B, so notice that I've just taken B, 6, 7,
plopped it into the first column of A.
Take the determinant, get 5.
Same thing A sub 2 of B, we take B and plop it in the second column of A.
And take that determinant, we get negative 2.
And so x1 is just the 5 divided by 3.
So x2 is the negative 2 divided by 3.
Then those are the values of x1 and x2.
Just to check, we take 5 thirds times the first column of A, minus 2 thirds times the second column of A,
and we indeed get 6, 7.
Yippee!
It's nice when things work like they're supposed to.
I use the same method for 3 by 3 system.
Although this time you've got to compute a determinant for each of 3 variables plus the determinant of A.
So here's a matrix A on the right hand side, B.
Compute the determinant of A first, and then we compute the determinant of A sub 1 of B.
So again, take B, take the vector B, stick it in the first column of A,
leave the rest of A alone, and compute that determinant.
Same thing, A sub 2 of B, we take B, stick it in the second column of A,
leave the rest of A alone, compute that determinant.
A sub 3 of B, we take the B, replace the third column of A,
compute that determinant, and then put it all together.
X1 is going to be negative 16 over 4.
X2 is going to be 52 over 4, and X3 is going to be negative 4 over 4.
And so there's your solution to this system.
So just a really neat idea and a neat way to solve systems of equations.
Clearly, we've run into the same problem for this 3 by 3.
It really wasn't that bad, but if it was a 4 by 4 system,
then you're computing 5 4 by 4 determinants, right?
Because you have to compute one for each variable, one for each column.
So you'd have A1, A2, A3, and A4 of B.
Plus you need to compute the original determinant.
So you have 5 4 by 4 determinants, and that would be a lot of work.
So this is not practical in a general sense.
But for theoretical and for small matrices like this, it's kind of a neat idea.
We can also apply it to a question like this.
Determine the values of the parameter s,
for which the system given by this augmented matrix has a unique solution.
And then describe the solution.
Okay, so we know it has a unique solution when the determinant is not 0.
Because the determinant is not 0, A is invertible, and there's a unique solution.
So if we take the determinant of the coefficient part of that matrix,
we end up with 15 times s squared plus 3.
And note that 15 times s squared plus 3 is not equal to 0 for all values of s.
We can never end up with that equal to 0 as long as s is a real number.
Because s squared plus 3 is always going to be positive.
And so this system is going to have a unique solution no matter what the value of s is.
So we compute to be able to describe the solution.
We can use Kramer's rule.
And so I've just computed a1 of b.
That takes b, the 3, 2, sticks it in the first column of a.
We compute that determinant.
Then same thing, a sub 2 of b.
We stick right hand side on the second column.
Compute that determinant.
And then kind of massage things a little bit.
And we have expressions for the value of x1 and x2 for any value of s.
So it's just the 5 times 3s plus 2 divided by the determinant of a here.
And the 3 times 2s minus 9 divided by the determinant of a for x2.
We can also use Kramer's rule to compute a inverse.
Because as we saw in chapter 2, to compute a inverse, we simply need to solve some systems of equations.
So if we consider this matrix a, we want to find b such that a times b equals the identity matrix.
So b here would be the inverse of a.
So let's let b1 and b2 denote the columns of b and let e1 and e2 denote the columns of the identity matrix.
And we've used that notation before.
So we need to solve a times b1 equals e1 and a times b2 equals e2 here in this 2 by 2 case.
Alright, so if we take the determinant of a, go back up and look at a here, 2 times 8, 16 minus 3 times 5.
So 16 minus 15.
The determinant of a is just 1.
Okay, now to get b1, we're solving this system here.
a times b1 equals e1.
So to get the first entry in b1, that's the determinant of a1 of e1.
Alright, we're solving, look at this augmented matrix.
So to get the first entry in the solution, we replace the first column of a by the right-hand side, which is e1.
Compute that determinant, get 8, 8 over 1 is just 8.
So the 1, 1 entry in the inverse is 8.
Alright, now to get the second entry in the solution to this system here, we substitute e1 into the second column of a.
Alright, so here we go with that.
And compute that determinant, negative 5 divided by 1, we get negative 5.
So we've got the first column of the inverse, right?
Because we were solving this system, a times b1 equals e1.
And so that's the first, so we get the first column of the inverse, b1 would be 8, negative 5.
To get b2, we solve a similar system, just now the right-hand side is 0, 1 instead of 1, 0.
And so to get the first entry in the solution to this system, we substitute 0, 1 in the first column of a.
Alright, so you see that here.
Compute the determinant, get negative 3 divided by 1, negative 3.
And then the second entry means substitute the right-hand side in the second column of a.
Compute that determinant, get 2 divided by the determinant of a.
And we have 2.
So we have a inverse, which is b, given by this matrix that we just computed.
So in general, the ijth entry in the inverse is given by the determinant of a sub i of ej divided by the determinant of a.
So if you go back, for instance here, to get the 1, 2 entry in the inverse, we computed the determinant of a sub 1 of e2 divided by the determinant of a.
To get the 2, 2 entry, it was a 2 of e2 divided by the determinant of a.
So in general, the ijth entry in the inverse is the determinant of a sub i of ej divided by the determinant of a.
We actually create a matrix, which we call the adjugate or the adjoint of a matrix, and stick in these values, these determinants of a sub i, e sub j.
We call that the, the book uses the term adjugate primarily.
I typically use adjoint, so those are synonymous terms.
But it's composed of all these determinants.
And so basically it's just these things, but with the determinant of a term factored out.
So in general, a inverse is equal to 1 over the determinant of a times the adjoint of a.
So let's use that to find the inverse of this matrix here.
Okay, so first we'll find the adjoint or the adjugate of a.
And remember to get the, to get the ijth entry in the adjoint, it's the determinant of a sub i, e sub j.
So for the 1, 1 entry, it's the determinant of a1, e1.
The 2, 1 entry, determinant of a2, e1.
3, 1 entry, determinant of a3, e1.
Alright, so I've computed all those.
A sub 1 of e1, remember e1 is the first column of the identity matrix.
The a1 says substitute that into the first column of a.
So there you have e1, 1, 0, 0, substitute in the first column of a.
And take that determinant.
A sub 2 of e1 says put e1 into the second column of a.
So there we have it, determinant there is 0.
A sub 3 of e1 says put e1 in the third column of a.
So there we go.
Take that determinant.
And do the same thing for the other 6 entries in the adjoint.
1, 2 position determinant of a1 of e2.
So take e2, which is 0, 1, 0, and substitute that in the first column of a.
So there.
So you can see here we have 0, 1, 0 in the first column.
Here it's in the second column.
Here it's in the third column.
And we compute each one of those determinants.
Then do it one more time for the third column.
Let's get the 1, 3, 2, 3 and 3, 3 elements.
So e3 is 0, 0, 1.
So we stick it in the first column of a.
Then the second column of a, third column of a,
and compute all those determinants.
Then we put it all together.
This is what we get.
And we need the determinant of a, which we can compute.
That's 5.
So the inverse of a is just 1 over the determinant of a.
So 1 fifth times the adjoint of a, which is this matrix.
And there's your inverse, computed totally by doing elementary,
not elementary, real operations,
totally using determinants.
Okay, today we're going to start section 4.1 on vector spaces.
So I'll start with the definition of a vector space.
You'll probably want to have your book out with you
because we're going to refer back to this definition quite a bit.
So in your textbooks on page 217,
there's a lot to this definition.
So we'll spend a little time just talking about it
before we move on from there.
A vector space is said to be a non-empty set v of objects called vectors.
You know, let's stop there because that's a little bit odd.
It's a set of objects called vectors.
I want to make the point here that when the author uses the term vector in this context,
he's not necessarily talking about a vector of the type we're familiar with,
like in R2 or R3 or so forth.
He's using it in a more generic sense.
And I actually like to use the term object just to keep from confusing you
between exactly what he's referring to by using the word vector.
And as we go along today, you'll kind of see what I'm talking about here.
Okay, so starting over, a vector space is a non-empty set v of objects
on which are defined two operations called addition and multiplication by scalars,
which are real numbers.
And again, this seems sort of odd, I would think to you,
because he's saying there's two operations,
and instead of saying two operations, addition and multiplication by scalars,
let me say there's two operations called addition and multiplication by scalars,
which is kind of an odd way to phrase it.
And the reason why he says it like that is because since these objects are not necessarily vectors
in the traditional sense that we're used to,
then addition and multiplication by scalars are not necessarily defined as we're accustomed to.
They can, in fact, be defined in any way that you would like,
as long as these properties here are satisfied.
So we're in an abstract sense here.
We're talking about objects and operations.
And so don't assume that we're talking about vectors in the traditional sense,
nor addition and scalar multiplication in the traditional sense.
Okay, so we have this collection of objects in these two operations,
and they are subject to the ten axioms given here.
And these axioms must hold for all vectors u, v, and w in the set v, and for all scalars, c, and d.
Okay, so I've kind of highlighted some because some are a little bit more interesting than others.
Number one says the sum of u and v denoted by u plus v is in the set v.
So it just says if you take any two vectors from the set and you add them together,
then you get another vector that is still in the set.
And as we'll see in a minute, this property is called closure,
or we say that the set is closed under addition if this property holds.
Okay, two and three are some standard properties.
Number two, we have a communitive property, three, associative.
Then number four says that in the set v, there is a zero vector,
such that when you add it to any vector in the set, you just get that vector back.
Okay, so u plus zero is equal to u.
Okay, so notice number one is referring to an element that has to be in the set.
If u and v are in the set, their sum has to be in the set.
Number four, also saying that there has to be this zero vector in the set.
Okay, so we'll see that number six kind of follows along with that, too.
Number five, the first says for each vector in the set,
you have another vector such that when you add it to the first, you end up with a zero vector.
So it's essentially saying that every element has an additive inverse in the set.
The number six says the scalar multiple of any vector by constant c is denoted by c times u,
and that is in the set.
Okay, so this says that when you take the scalar multiple of any vector in the set,
or any object in the set, the result is also in the set.
So you can see that number one, number four, number six, all are specifically referring to
whether or not particular elements are in the set, and that will be important as we go along.
Number seven through ten are, again, standard sorts of properties.
We have distributive properties, and number ten, we have a multiplicative inverse property.
That means that any element in the set is equal to that element,
and you might be looking especially at number ten and going,
well, when would that ever not be the case?
And the answer is going back to what I'd said originally is that the set v can contain items
other than just vectors like from R2 or R3.
And these operations, addition and scalar multiplication, can be defined in non-standard ways.
And so based on that, then sometimes it could be the case that number ten would not be true.
Okay, so let's start with R2.
That's a simple set to think about, and let's talk about whether R2 is a vector space.
The answer is yes, because all the properties except one and six were in fact explicitly given in section 1.3.
So if you want to go back to page 32 and check that out, all the properties except one and six are explicitly listed there.
So let's think about one and six.
Number one, we want to know if you take two vectors from R2 and add them together,
do you get another vector in R2?
Well, here's two vectors in R2, AB and CD.
When we add them together, we get A plus C, B plus D, that's another vector in R2.
So the answer is yes there.
Number six, if you multiply a vector in R2 by a scalar, do you get another vector in R2?
Well, if you analyze it closed under scalar multiplication, and here we see multiply a vector times a scalar,
we get another vector in R2, both of whose components are real numbers,
and therefore R2 is a vector space.
And in fact, the set of real numbers, a set of all vectors with two components, which is R2,
R3 and Rn in general are all vector spaces.
Going back to these property one and property six, just to take a second look at those,
property one said that if you take any two vectors from the set and compute their sum,
then it is also in the set.
And if that is true, we say that the set is closed under addition.
Property six says you take a scalar multiple of a vector in the set, then the result is also in the set.
Then if this is true, we say that the set is closed under scalar multiplication.
Okay, so keep those terms in mind.
Let's move on and consider another set.
Here's another set. This is a subset of R2.
So I'm saying we have a set S here, which consists of vectors of the form X0, where X is a real number.
So basically everything in R2 where the second component is zero.
If you think about this graphically, it's just saying that the Y component is zero.
So therefore graphically speaking, S would just be the X axis.
So I want to know is S with the operations of addition and scalar multiplication as traditionally defined.
What you're used to is S of vector space.
Well, again, axioms 2 and 3 and 7 through 10 follow automatically since S is a subset of R2.
Because 2, 3 and 7 through 10 are true of everything in R2.
And I'm assuming here that you get your book open so you know which of these I'm referring to.
The other axioms depend on certain elements being in the set.
So we have to look at this closer.
So number one is S closed under addition, i.e. if you take two elements in the set, do you get another element in the set?
Take two elements in the set and add them together. Do you get another element in the set?
Well, here I've taken a couple of generic elements from the set, U and V.
And if we add them together, as I've done here, notice that you get this vector of this form, U1 plus V1 in the first component, 0 in the second component.
And this vector is in S instead of V.
Since U1 plus V1 is a real number.
Let's pause here. Why do we know that U1 plus V1 is a real number?
The reason we know that is because U and V are both in S and therefore U1 has to be a real number, V1 has to be a real number.
So we add two real numbers together, we get another real number.
Okay, so we know the first component is a real number, second component is zero, and that's what it takes to be in the set S.
So S is closed under addition.
By number four, does S contain a zero element?
Does S contain a vector of zero such that when you add zero to any vector in the set, you just get that vector back?
Well, the obvious choice would be zero, zero.
So the question is, is that vector in S?
And the answer is yes.
Because remember, to be in S, the first component has to be a real number, which zero is.
And the second component has to be zero, which we have here in the zero vector.
Okay, number five, does each element have an additive inverse?
Well, if we start off with a generic element of the set, say U equals U10.
We can add to that this vector negative U10, and it is in the set.
How do we know that?
Well, if U1 is a real number, then negative U1 has to be a real number, and we have the second component zero.
When we add those together, we get the zero vector.
Is S closed under scalar multiplication?
So if we multiply, take a generic element from the set, multiply it by a scalar.
Do we get another element in the set?
Well, here's a generic element of the set.
U equals U10.
C is a scalar.
That just means it's just any real number.
And if we multiply C times U, we end up with C U1 in the first component.
Now that has to be a real number, which it is because C is real, U1 is real.
We've got the product of two real numbers, and that's another real number.
And the second component is zero.
That's what it takes again to be in S.
So S is indeed closed under scalar multiplication.
And at this point, we've established that all 10 of the axioms are satisfied.
So S, our set S, is a vector space.
Okay, now it turns out that when you're dealing with subsets of known vector spaces,
just like we were here, S was a subset of R2,
then we really need to only examine three of these properties to see if the set's a vector space.
And actually, we typically call it a subspace of the larger vector space,
even though it is actually in itself a vector space.
Okay, so there's the three properties are given here.
We say a subspace of a vector space V is a subset H of V that has three properties.
The zero vector of V is an H.
H is closed under vector addition.
And H is closed under scalar multiplication.
Okay, so if we want to check to see if a set is a subspace of a vector space,
we need to only check these three properties.
All right, so let's do that here.
Here's another set T.
And T is a subset of R2.
And it consists of all vectors where the first entry is any real number
and the second entry is a two.
So I want to know is T a subspace of R2?
So looking at the first one, does T contain the zero vector of R2?
That was the first requirement to be a subspace.
So I'm asking is zero zero is the vector zero zero in T?
And if you look up here at T, remember what does it take to be in T?
First components, any real number.
Second components, two.
So we look at zero zero, does that fit that bill?
No, it doesn't.
Why not?
Because the second component here is zero.
To be in T, the second component has to be a two.
So zero zero is not in T.
And at this point, we could stop and say, no, T is not a subspace of R2
because it doesn't contain the zero vector of R2.
But for practice, let's just keep going and consider the other two conditions.
So here's T again.
Is it closed under addition?
So if I take two elements of T and add them together, do I get another element of T?
Well, here's a couple of generic elements of T.
If I add them together, what happens?
Oops, look at that second element here.
Second element in the sum of these two is a four.
So this is not in T.
Because to be in T, your second component has to be a two.
Here it's a four.
So T is not closed under addition.
How about scalar multiplication?
Is it closed under scalar multiplication?
Well, let's take a generic element of T.
Here's one.
Got a real number on top.
Two in the second component.
We multiply by scalar.
And what do we get?
We get this vector.
Actually, we shouldn't get that vector.
That v1 should not be there.
Apologize for that.
Should be just c1 in the top.
2c down in the bottom.
Make a note of that.
So again, this plus v1 shouldn't be here.
But let's look.
What's important really is the second component, the 2c here.
Because what we want to know is, is that zero?
And the answer is, well, it's zero only if c is zero.
And c is not restricted to be zero.
And therefore, we can easily come up with a counter example
to show that we can take a scalar multiple of an element of T
and end up with something outside of T.
So T is not closed under scalar multiplication.
All right.
Make one more note.
All right.
So in this case, this set T failed all three of the subspace tests.
Now, again, if you were simply trying to determine if T is a vector,
or a subspace of R2, you only need to find one that it fails
and then you could stop there.
I just showed you all three just for practice.
Okay.
So as I said before, these vector, vector spaces such as R2
and R3 and Rn, those we are familiar with.
But there's a lot of other ones in which the elements don't look like traditional vectors.
So let's kind of examine that a little bit.
Consider this set I'm calling M sub 2 by 2, which is the set of all 2 by 2 matrices.
Okay.
Where all the components are real numbers.
Turns out that M sub 2 by 2 is also a vector space,
even though its elements are matrices instead of vectors in the traditional sense.
So let's go back through those properties with M sub 2 by 2 in mind.
Okay.
So let's start off and suppose that A, B, and C are our 2 by 2 matrices
and that P and Q are scalars.
So property number one.
If we add A and B together, do we get another 2 by 2 matrix?
This is saying is M sub 2 by 2 closed under addition.
And clearly it is.
If you add 2 by 2 matrices together, you get another 2 by 2 matrix.
All right.
Here are the second and third properties.
Those fall straight out from properties of matrices.
Number four.
Is there a zero matrix?
Is there some 2 by 2 matrix such that you can add to any other 2 by 2 matrix
and get that same matrix back?
Well, clearly there is some matrix with zeros in all the positions.
Number five.
If you add negative A to A, you get the zero matrix.
Number six.
Is M sub 2 by 2 closed under scalar multiplication.
So if you take any 2 by 2 matrix and multiply it by scalar,
do you get another 2 by 2 matrix?
And of course you do.
So it's closed under scalar multiplication.
And then these final four properties fall straight out from what we know about matrices.
So M sub 2 by 2 is indeed a vector space.
Another one about polynomials.
Polynomials are some standard examples of vector spaces.
Let's first start with piece of 2.
Piece of 2 is the set of all polynomials of degree 2 or less.
Okay.
So we wanted to write it in informal terms.
It would look like this.
A naught.
Set of all A naught plus A1 times t plus A2 times t squared,
where A naught, A1, A2 are real numbers.
So here's some simple elements of piece of 2.
3 plus 2t.
Okay, here 3 is A sub 0 is 3.
A1 is 2.
A2 in this case would be zero.
4t squared, that's in there.
8t squared minus 13t plus 45, that's in there.
Any polynomial of degree 2 or less.
Okay, and piece of 2 is indeed a vector space.
Let's look at these properties in terms of piece of 2.
So if we add two polynomials of degree 2 together,
do you get another polynomial of degree 2?
Okay, well, yes you do.
Here's how that works.
You've got two, let's call it p of t, which looks like this.
Q of t, which looks like this.
And if we add them together, then we get this polynomial here,
which is again of degree 2 or less.
Okay, they satisfy the properties 2 and 3.
Let you explore that some more.
Number 4, do you have a zero element?
Is there a zero polynomial?
Well, yes there is.
It's just zero.
We add zero to any polynomial, we get that same polynomial back.
If we negate all the coefficients in a polynomial
and add it to the original one, we get the zero polynomial.
Number 6, is this set of polynomials closed under scalar multiplication?
So if you multiply any polynomial of degree 2 or less by a scalar,
do you get another polynomial of degree 2 or less?
And the answer is yes.
Okay, here, multiply by a scalar,
and we get another polynomial of degree 2 or less.
And again, 7 through 10 kind of fall out pretty straightforwardly.
Here's another set.
This is a subset.
A sub 2, here's a subset of p sub 2.
Here, I've got all polynomials, not of degree 2 or less,
but all polynomials of degree exactly 2.
So the way I've defined it is in this form,
looks like p sub 2, with this addition,
a sub 2 has to be non-zero.
Okay, so that makes it where you're going to have a second degree polynomial.
You're going to have a sub 2, not equal zero,
so you're going to have the t squared term showing up.
So I want to know, is q sub 2 a subspace of p sub 2?
So to show that it's a subspace,
we need to show that it satisfies those three properties, right?
It includes the zero element closed under addition,
closed under scalar multiplication.
So is that the case?
Well, let's start with number one.
This q sub 2 contains the zero element of p sub 2.
Well, the zero element of p sub 2 is the zero polynomial,
all right, just zero.
So is that in q sub 2?
And the answer is no, because,
go back up and look at the definition of q sub 2,
a sub 2 can't be zero, right?
But down here, the zero polynomial, a sub 2 is zero.
So q sub 2 does not contain the zero element of p sub 2.
So we could stop right here and say,
no, q sub 2 is not a subspace of p sub 2.
But again, just for more experience,
we're going to go on and look at the other two.
So the second one is q sub 2 closed under addition.
So if we add any two elements of q sub 2 together,
do we get another element of q sub 2?
Or if you add any two polynomials of degree exactly two or less,
not or less, a degree exactly two,
do you get another polynomial of degree exactly two?
There it is there.
So here's an example.
But p of t be 6t squared minus 3t.
That is in q sub 2, because we've got a 6.
The a sub 2 term is not equal to zero.
And here's q of t, where the coefficient of t squared is not zero.
So we've got two polynomials here of degree exactly two.
And when we add them together, what happens?
The t squared terms go away.
We're left with negative 3t plus 5.
And that is not a degree exactly two.
So q sub 2 is not closed under addition.
About scalar multiplication.
If we multiply a polynomial of degree exactly two by a scalar,
do we always get another polynomial of degree exactly two?
So think about that.
So you start off with any sort of polynomial that's degree exactly two,
and you multiply it by any scalar,
do you always get another polynomial of degree exactly two?
And if you don't think about that very long,
you might say, well, yeah, you multiply it by anything,
and you're going to get another polynomial of degree exactly two.
That's almost always true.
But there's one coefficient, one scalar you can use to multiply by zero.
Then you get the zero polynomial, and it is not a degree exactly two.
So this means that q two is not closed under scalar multiplication.
So q sub two here failed all three of the requirements to be a subspace of p sub two.
Yeah, it only has to fail one and not to not be a subspace,
but this one in fact failed all three properties.
Okay, let's look at one more.
Let's look at z set of all integers.
Okay, and I want to know is z a subset of the real numbers,
a subspace of the real numbers, excuse me.
Clearly it's a subset of the real numbers.
All right, so we want to know number one,
does z contain the zero element of the real numbers?
Well, the zero element of the real numbers is just zero,
and zero is an integer, so the answer here is yes.
Is z closed under addition?
If you take two integers and add them together, do you get another integer?
And the answer is yes, because the sum of any two integers is always an integer.
So we pass number one, we pass number two, how about number three?
Is it closed under scalar multiplication?
So if we multiply an integer by a scalar, do we always get another integer?
Let's think about that.
Trick here is that the scalar could be a real number.
You don't have to multiply by an integer.
Scalar just means a real number.
That real number might not be an integer.
So if we multiply our integer by a non-integer,
then we open up the chance that we could end up with a non-integer.
So the answer is no, it's not closed under scalar multiplication, and here's an example.
If I multiply 0.5, there's my scalar, times three, my integer, I get 1.5.
1.5 is not an integer.
So here's a scalar times my integer, I don't get another integer.
And so therefore z is not closed under scalar multiplication,
and therefore it's not a subspace of the real numbers.
Okay, we're continuing in section 4.1 with more about vector spaces.
And there's one way that we've talked about for finding if a set is a subspace of another vector space
is to show that it satisfies the three properties that it contains the zero vector of the parent vector space.
It's closed under addition, and it's closed under scalar multiplication.
So to show that a set is a subspace, then you can show that these three things hold,
or to show that it's not a subspace, show that at least one of these does not hold.
But we have another method for showing that a set is a subspace,
and this doesn't work in all cases, but in some cases you can use this theorem,
and it makes life much easier because you don't have to go through and show those three properties hold.
So the theorem says if v1 through vp are vectors in some vector space v,
then the span of v1 through vp is a subspace of v.
So what this says is that if we can write our set as the span of a finite set of vectors,
write it in this form, span of v1 through vp for some vectors,
then automatically we can conclude that the set is a subspace.
So let's look at this example.
We'll have a set S, which consists of all vectors of this form, the subset of R3.
First component is two times some real number t, second component is zero,
and the third component is the negative of that real number t that we had up in the first component.
So how we want to know if S is a subspace of R3.
If we use the original method, that is to show that the three properties hold,
then we would start with saying does S contain the zero vector,
and in this case if we set t equal to zero, then we get zero, zero, zero,
and so the zero vector is contained in the set S.
So we go to the next property as S closed under addition.
We need two generic vectors from S, so let's call those u and v,
and u looks like two u, zero, negative u, and we'll say v is two v, zero, negative v,
and we need to add those together and see if the resulting vector is in the form that has to be in S.
So we add u and v and rearrange terms just a little bit,
and we end up with this vector here.
We've got two times u plus v in the first component, zero in the second component,
negative u plus v in the third component,
and this is in the form that it needs to be in the set S because u plus v here is a real number.
We don't know that because these came from the vectors u and v,
so we have two times u plus v in the first component,
negative u plus v in the last component, zero in the middle, so it is in S.
So S is closed under addition.
So we move on to the third property is S closed under scalar multiplication.
Again, we need a generic vector from the set S and a scalar, which we'll call C,
and we compute C times u, and again, rearranging terms a little bit,
we can write it as two times cu in the first component,
negative c times u in the last component, zero in the middle,
and since both c and u are real numbers, then c times u is a real number, so this vector is in S.
So we have all three properties satisfied, and therefore S is a subspace of R3.
Now there's a fair amount of work that went into that,
and so let's look at how we could use, or if we could use this theorem to make the work a little easier.
Alright, if we take a generic vector from the set S and we write it in parametric vector form,
so that means factor out the parameters.
In this case, there's only one, t, and so we can write any vector in S as t times zero, two zero, negative one.
So any vector in S is a multiple of this vector.
Therefore, S is equal to the span of this vector,
and since we've written S as the span of a finite set of vectors,
that means by the theorem S is a subspace of R3.
So you can see here that if your problem is one in which you can use this theorem,
then it makes the work much easier.
Here's another example.
A set called w is a subset of R4,
and we want to know is w a subspace of R4.
So again, we could go through the three properties, zero vector closed under addition, closed under scalar multiplication,
but it's much easier to apply the theorem.
So here we write this generic vector in parametric vector form.
There are three parameters here, a, b, and c, so we factor those out.
And so at this point, we have written this generic vector as a linear combination of these three vectors.
That is exactly what it means when we say that w is the span of those vectors,
because any vector in w can be written as a linear combination of these three.
So again, we've got w is the span of a finite set of vectors,
and so by the theorem, it must be a subspace of R4.
All right, here's another one.
Set t given here, and we want to know is it a subspace of R3.
So we proceed as we did before.
Apply.
I try to write this generic vector as a linear combination of vectors in R3.
And so we can write it, but notice what happens.
If we factor out the a and the b, we're left down here with a vector,
but no parameter associated with it.
And so we can't write this vector as a linear combination of vectors,
because a linear combination means you've got a parameter or a multiplier in front of each vector.
And here, the multiplier is set at 1.
We can't alter this.
So this is not a linear combination, and so we cannot apply the theorem here.
All right, and so in this case, our only alternative is to go back to the first method.
And so I'm going to go through that here just because it's always good to have practice in doing that.
So we look and we say is the zero vector in the set.
And a lot of times when you have a constant term like this, that's a red flag.
And you have to think about that very carefully because a lot of times in those cases,
it will not be a subspace because in a lot of cases it moves you away from the origin,
which means that your set does not contain the origin.
So that's what we're asking here is the zero vector in t.
Well, for the zero vector to be in there, that means that each of the elements,
we need to be able to make each of the elements equal to zero.
So if you look at the first or the second component for this to be zero,
that means a has to equal 6b.
And then the third component, it would mean a has to equal negative 2 times b.
Okay, so if we put that together, then that means that 6b is equal to negative 2b.
And down here at this point and solving that, that means that b has to be zero.
And if b has to be zero, a is 6b, so that means a is zero.
And in that case, if you look back at the first component, that says you get a one there.
And so therefore the origin is not contained in this set.
Because to get zeros in the second and third positions,
that means we're going to end up with a one in the first position.
So t does not contain the zero vector.
Now at this point, you could stop and say t is not a sub space of r3.
But just for practice, I'm going to continue on and say, is t closed under addition?
And the answer is no, it's not.
Now to show that, we need to add two vectors together.
We need to take two vectors from t, add them together, and see if we get another vector that is in the set.
So I picked 100, it's in the set because it's what we just talked about.
If the second two components are zero, then the first one has to be one.
And so 100 is in the set.
So I'm just going to add it to itself.
And the result is 200.
And so let's think, is that in the set?
And the answer is no, because as we said before,
if the second two components are both zero, then the first one has to be one.
So this vector is not in t, and therefore, t is not closed under addition.
So just for practice, again, let's check, is it closed under scalar multiplication?
And here we have, no, the answer is no, because we've got, again, I just chose 100,
because we know that vector is in t.
And multiply, actually we can multiply by anything other than one.
I multiply by two, we get 200, and that is not in t.
All right, so this particular set, t fails all three of those properties.
Now here's one that looks similar to t.
It's got the plus one, so it's got that constant term in it.
But notice here that the a in the first component here is not constrained by anything in the other two components.
And so this one's a little bit different.
Still, we cannot apply the theorem because of that one in the first component.
We cannot write s as a linear combination of vectors.
Because we can't account for that one by doing that.
So my point really with this example is to show that just because you can't apply the theorem,
does not mean that the set is not a subspace.
And so in fact, this one is a subspace of r3.
So we're going to go through and just show that.
So we have to go back to the original method for that.
And so we ask, is the zero vector in, that should be in the s.
Sorry about that.
This is zero vector in the set s.
And the answer is yes, because b and c could certainly be zero.
And we can set a equal to negative one.
And in that case, we end up with a zero vector.
Is it closed under addition?
And the answer is, well, we take two arbitrary elements of the set, say u and v.
So here's what u looks like.
Here's what v looks like.
And when we add those together, we get this vector.
So just u2 plus v2 in the second component, u3 plus v3 in the third.
In the first, we get u1 plus v1 plus 2.
Now you ask, is this vector in the set?
And the answer is, well, it's not clear from looking at this, whether it is or not.
However, if we write it in this form, then it's clear.
Because now, obviously, the second two components are just real numbers.
And the first one, by factoring out a plus 1, and then gathering what's left here in parentheses.
Now this thing here, u1 plus v1 plus 1, is a real number.
And so now, my first component looks like some real number plus 1.
And so that's a real number.
u1 plus v1 plus 1 is real.
u2 plus v2 is real.
And u3 plus v3 are real.
Therefore, u plus v has to be in t, or in s.
Sorry about that.
So it's closed under addition.
We take a similar approach to show that it's closed under scalar multiplication.
Again, take a generic vector and a scalar.
Multiply the two.
And again, here I had to factor out that plus 1, so that I could make it look like the form that it has to be in the set.
And that leaves me, in this case, with this quantity here.
But that's a real number, right?
Because c is real, u1 is real, obviously 1 is real.
So this quantity is a real number.
So I got real number plus 1.
And then the second two components are clearly real numbers.
And that's what it takes to be in the set.
So we can conclude that the set is closed under scalar multiplication.
And so it's a subspace of r3.
Another example, this one, first glance, looks like maybe you could use the theorem here.
But it turns out that you can't with that a times b in the first component.
There's no way to break that up in a linear combination.
And so this one, the theorem, does not apply.
And so we go back, again, to the original method.
And look and see, does this contain the zero vector?
And clearly it does, because you can set a and b both equal to zero, and that gives you the zero vector.
Is it closed under addition?
Well, here, let's go back and look at that set.
Maybe it's not clear, just looking at that, whether it would be closed under addition or not.
You have to do some thinking, a little work to arrive at a conclusion on that.
So really there are two approaches you could take.
One of them is to play around with the numbers and try to find a counter example to show that it's not closed.
So a counter example would be a specific example, specific numbers that you plug in and show that the set is not closed.
The other approach is to try to make a formal argument to show that it is closed.
And whichever one of these you pick really depends on the problem, whether you have some intuition one way or the other.
And it also actually depends a little bit on your personality.
Would you rather play around with the numbers and try to come up with a counter example,
or would you rather take a more straightforward approach, which is to try to show that it's closed.
Because a lot of times in that case you're trying to show that it's closed, you will either succeed,
or you will get to a point where you see why it's not closed.
And so taking that route is a little more of a deliberate approach.
Playing with the numbers to try to find a counter example is a little more of a random approach.
But either is valid, and it kind of depends on, again, on your intuition,
whether you have a gut feeling one way or the other, and which you'd rather do.
What you think is the better way to go.
With this one, I'm going to choose the latter approach.
Because maybe I don't know, it's just not clear to me whether it's closed or not closed.
Maybe I don't have a gut feeling on that.
So I'm just going to take the safe, deliberate approach and try to show that it's closed
and see where that takes me.
So I picked two arbitrary elements from the set.
These vectors I've written here, u and v.
And when I add them together, I get this vector.
And so what I want to know is if I multiply the second and third terms together,
does that equal the first term?
So I'm asking, does first term pq plus xy equal p plus x times q plus y?
And as I look at that, I say, well, no.
Sometimes it might, but in general, no.
That does not hold.
So that tells me this is probably not closed under addition.
And so I think I'm going to switch horses now, switch gears and try to find a counter example.
And so at this point, you kind of scratch this out.
Consider that to be your scratch work.
And you start over.
So start over here.
And this time, I'm going to try to find a counter example.
And so my advice is make life simple.
You can do a lot with just ones and zeros.
So here I chose u to be 1, 1, 1.
Second two components are 1.
Multiply them together.
You get 1.
So that means the first component is 1.
And v, I have 1, 2.
And the second and third components, multiply those together and get 2.
So the first component in v has to be 2.
All right.
So both these vectors are in s.
And I add them together.
And so I get this vector 3, 2, 3.
Just adding component-wise, u and v.
And then I check, is this vector in s?
And the answer is no.
Because when I multiply the second two components together, 2 times 3, I get 6.
But my first component is not equal to 6.
It's equal to 3.
And so this vector, u plus v, is not in s.
So s is not closed under addition.
Now at this point, we know that s is not a subspace of r3.
But again, for practice, let's keep going and look and see,
well, is it closed under scalar multiplication?
And again, you got the choice, which approach you want to take.
Find a counter example or work on a formal argument to show that it is closed.
In this case, I'm going to think, hmm, you know, it wasn't closed under addition.
So I'm going to just bet that it's probably not closed under scalar multiplication.
So I'm going to fudge around and see if I can find a counter example.
So I need to come up with a vector that's of the general form.
And a scalar c, such that when I multiply c times the vector, I get one that's not in the set.
This is a trial and error process.
And again, I would say start simple.
1's and 0's are good.
Now from before, we know that 1, 1, 1 is in the set,
because second two components, 1 times 1, is equal to 1.
And so, you know, I might multiply by 0, but then I'm going to get 0, 0, 0, and that is in the set.
Multiplying by 1 doesn't do me any good, because that doesn't change the vector.
I want something that's outside the set.
So how about 2?
If I multiply 2 times u, I get 2, 2, 2.
And then I ask, is that in the set?
And so you multiply the second and third components together.
2 times 2 gives you 4, but the first component is not 4.
So therefore, this vector is not in the set, and we've shown that s is not closed under scalar multiplication.
Okay, and so now you have some examples, examples that you can apply this theorem on,
and some examples that you can't.
My advice is, you know, to, if you can't apply that theorem, you want to,
because it makes your life easier, much less work.
If you can't, then you must go back to the original definition and show that the three properties either show that they all hold,
or show that one of them doesn't hold.
And when you're doing that, to show that one of the properties holds, you must make a generic argument.
You know, to show that a set's closed under addition or closed under scalar multiplication, you must make a generic argument.
It's not sufficient to show that you can find two vectors that you can add together and get one that's in the set.
You have to show that that holds for any two vectors that you pick, and that means you must make a general argument
to show that a set's not closed under either addition or scalar multiplication.
You need to find a counter example, and a counter example means come up with a specific example with real numbers,
and by real numbers, I mean actual numbers, just like this one that's on your screen now.
You know, a specific vector, one, one, one, a specific scalar, two.
Multiply those together and see what you get and show that that's not in the set.
So that's a counter example.
So to show that it is closed, make a general argument to show that it's not closed, find a specific example.
In section 4.2, we're going to talk about a couple of concepts that we're not familiar with the terminology,
but we are familiar with the concepts behind these terms.
First of these is the null space of a matrix.
We say the null space of an M by N matrix A, written as null A, is a set of all solutions to the homogeneous equation Ax equals 0.
So null A is simply a set of all x, such that x is in Rn and Ax equals 0.
X has to be in Rn because if A is an M by N matrix, and we want to be able to multiply A times x,
then there must be an entry in x for every column of A, and so x must be from Rn.
So null A is simply the set of all solutions to Ax equals 0.
The fact that we're calling it the null space probably gives you a clue that it's a subspace,
and indeed it is, the theorem here says the null space of an M by N matrix is a subspace of Rn.
Let's look at this matrix A given here.
To generate an explicit description of null A, we have to solve Ax equals 0.
An explicit description means you can look at it and generate an entry in null A.
To do that, we have to solve Ax equals 0.
We throw that into an augmented matrix and do a couple of row operations, giving us the matrix given here.
We see the general solution is x1 equals 6x2 plus 2x4, and x3 equals negative x4, and x2 and x4 are free variables.
If we put it in parametric vector form, we have x1 is 6x2 plus 2x4, x2 is just x2, and so forth.
So notice that any vector in the null space of A is a linear combination of these two vectors given here.
If we look at those, we can say any vector in the null space of A is in the span of those two vectors.
If you remember the theorem from section 4.1, it said that if you can write your set as the span of a finite set of vectors,
then it's automatically a subspace of that apparent vector space.
So here we've written, for this particular matrix A, we've written the set of solutions to Ax equals 0 as the span of these two vectors.
So therefore, the null space of A is a subspace of R4.
The second concept that we're going to talk about here in this section is the column space of a matrix.
And once again, it's new terminology, but not new fundamental material here.
The column space of an M by N matrix A, which we write as call A, is the set of all linear combinations of the columns of A.
All linear combinations of the columns of A. We know that to be the span of the columns of A.
So if A is equal to A1 through An, the column space of A is just the span of A1 through An.
And since we've, the definition here is that column space of A is the span of a set of vectors, so it too is a subspace of some vector space.
In this case, if A is M by N, then the column space is going to be in Rm.
Because when you take a linear combination of vectors with M components, you're going to get another vector with M components.
So the column space of A is in Rm.
Another way to look at the column space of A is to write it as a set of all B, such that B is equal to A times X for some X in Rn.
Because when you multiply A times X, you're simply taking a linear combination of the columns of A.
Here's a set S defined in terms of this generic vector.
And you're asked to find a matrix A such that S is equal to the column space of A.
So we simply take that generic vector, write it in parametric vector form, so we can write anything in S as a linear combination of these three vectors given here.
So if we put those vectors into a matrix, then we can say that anything in S is a linear combination of the columns of A.
And so S is equal to the column space of A.
As I said before, the null space of A and the column space of A are simply new terms for describing entities with which we are already familiar.
Null A is just a set of all solutions to AX equals zero.
That dates back to section 1.31.4 or somewhere back there.
A column space of A is just the set of all linear combinations of the columns or the span of the columns.
So again, we're going back to fundamental information that we learned in chapter 1.
Okay, so if we have a matrix A, how do we determine if a particular vector X is in the null space of that matrix?
Well, go back to the definition.
Null space of A is a set of all vectors satisfying AX equals zero.
So we just need to multiply A times X and see if we get zero.
Again, to multiply A times X, we're taking a linear combination of the columns of A, so there needs to be a component of X that corresponds to each column of A.
There are n columns in A, so we need n elements in the null space of A.
So null A is in Rn.
Okay, so here's a vector and we're asked is this in the null space of the given matrix.
So we simply multiply the matrix times this vector.
So we're taking a linear combination of the columns here and go through the arithmetic and we end up with a zero vector.
So the answer is yes, it is in the null space of A because A times X is equal to zero here.
Alright, what if we have a matrix A and another vector and we want to know if that vector is in the column space of the matrix?
So again, look at the definition, B is in the column space of A if AX equals B is consistent.
So we need to solve a system to determine if B is in the column space of A.
And again, the column space of A is going to be a subset of our M because we're taking linear combinations of vectors with M components
and so we get another vector with M components.
Alright, so here's a vector and we want to know if it's in the column space of A for the given matrix A.
So we need to solve the system, set up the augmented matrix, solve the system.
So I've done that, left out the row operations, but we end up with this matrix here.
And so the question is, does this correspond to a consistent system?
And the answer is yes, because we have no rows, zero, zero, zero, something not zero.
The fact that we have a row of all zeros is really irrelevant.
The fact that we have free variable is irrelevant.
The only thing that's relevant is that we don't have a row that's zero, zero, zero, not zero because that would tell us it's inconsistent.
So since the system is consistent, that means that this vector is indeed in the column space of A.
Alright, let's look at this example.
Here's the set W and we're asked, is it a subspace of R4?
So we can write it, write any generic element of W as the linear combination of these two vectors.
So we write it in parametric vector form.
And so we can say that W is equal to column space of this matrix.
Just taking these two vectors, sticking them in the columns of a matrix.
Now the column space of this matrix is all linear combinations of these columns, which is exactly W.
So that means that W is a subspace.
Alright, how about this example?
This one, it's not so easy or so clear to see because we have this constant term one here.
And so we can't write this set as a linear combination of vectors.
So in this case, we have to go back to the definition of subspace.
That's those three properties that every subspace must satisfy.
So first one was does it contain the zero vector?
And if we look for it to contain the zero vector, clearly D has to be zero,
which would mean that the second component would be two times zero plus one, which is one.
And so if you get zero here, you can have one up here.
And therefore, the zero vector is not in this set.
So it is not a subspace of R4.
But this set, is this set a subspace of R4?
So we've got all vectors ABCD that satisfy these two equations.
There's actually two ways we could go about doing this using what we've learned in this section.
One is easier than the other.
Let's take the hard way first.
And we're going to try to write S as the column space of some matrix A.
So we need to figure out what A would be.
We're going to simplify things a little bit.
Instead of C here, we're just going to substitute A plus 3B.
And then instead of D, we're going to substitute A plus B plus C,
but we're going to plug in what C is since we've gotten rid of C.
So D is B plus C plus A, and C is A plus 3B.
So plug that in for C, and we end up with D equals 2A plus 4B.
So we can write any vector in S in this form.
So we replace C with A plus 3B and D with 2A plus 4B.
And then it's just a matter of writing that in parametric vector form
and throwing those columns into a matrix.
And so S is equal to the column space of A where A is given here.
And thus, S would be a subspace of R4 since the column space of any matrix is a subspace.
Now the other way to look at it, so I rewrote the problem here.
The other way to look at it is to note that we could write S in this form.
So basically I've taken these two equations that we have here,
and I've taken all the variables over to the left side.
And so we've got zeros on the right.
And so when you see zeros on the right, you should think that's a homogeneous system.
And so we can write S as the null space of A,
where A is equal to the coefficient matrix from this system of equations.
So you see 1, 3, negative 1 for ABC and then 0 for D and from the first equation.
And then the second equation we get 1 times A plus 1 times B plus 1 times C minus 1 times D.
So that's where the second row comes from.
So remember what the null space of a matrix is.
It's just the set of all solutions to AX equals 0.
And so I've just taken this system of equations, written it, or taken these equations,
written them as a homogeneous system.
And then I can use the fact that the null space of a matrix is always a subspace and we're done.
So that's why I said one was harder than the other.
This is clearly the easiest way.
All right.
Let's revisit the concept of linear transformations for just a bit.
And this is again back to chapter one, where we talked about linear transformations.
One new term that we didn't learn back in chapter one was the idea of the kernel of a linear transformation.
And the kernel of a linear transformation is simply the set of all vectors that map to the zero vector.
So the kernel of a linear transformation is exactly the null space,
the null space of the matrix that defines the transformation.
So kernel and null space are analogous concepts.
The range of a transformation is the set of all vectors that get mapped to.
So the range is in the co-domain.
Okay, sometimes it's all of the co-domain, sometimes not.
But it's the range is set of all vectors that get mapped to by some vector from the domain.
And so the range of T is actually the column space of A, where A is the matrix that defines the transformation.
So the kernel of the transformation is the null space of A.
The range of the transformation is the column space of A.
Okay, last I want to talk about just looking at the contrast between the null space of a matrix and the column space.
These really on the surface are very different sets.
We will kind of pull them together a little bit later on in this chapter.
But for right now, they're really very different sets and they don't share analogous sorts of ideas.
But that's kind of the extent of it at this point.
So let's assume A is an M by N matrix.
Then the null space is in Rn, column space is in Rm.
The null space is implicitly defined.
That means you're given a condition that vectors in the null space must satisfy.
But you can't look at a matrix A and know which vectors are in it.
You have to solve that system A x equals 0.
On the other hand, the column space of A is explicitly defined.
Because it's just the set of all linear combinations of the columns.
So you can look at the matrix A and you know that the columns that you're looking at are actually in the column space.
And you know how to create more entries in the column space.
To find vectors in the null space requires work.
You have to solve the system A x equals 0.
Note, however, that the zero vector is always in the null space of A.
Because A times the zero vector gives you the zero vector.
To find vectors in the column space of A, you just compute linear combinations of the columns.
So it's a direct process to do that.
There's no obvious relationship between null A and the entries in A.
On the other hand, the relationship between A and the column space of A is obvious since each column is in the column space.
The typical vector V in the null space satisfies A times V equals zero.
Typical vector in the column space has the property that A x equals V is consistent.
So for null space, you're multiplying A times V to see if you get zero.
To see if a vector is in the column space, you're solving a system A x equals V.
So V is on the right hand side in that case.
Given a specific vector V, it's easy to determine if V is in the null space.
You just see if A times V is equal to zero.
To determine if V is in the column space, you have to solve a system of equations.
The null space equals only the zero vector if and only if A x equals zero has only the trivial solution.
So how do you get what's in the null space?
We have to solve A x equals zero.
And if you get only the trivial solution, then that means there's only one solution, which is a zero vector.
The column space of A is equal to RM if and only if A x equals B is consistent for every B in RM.
That means that everything, no matter what you put on the right hand side, the system will be consistent.
So every B is a linear combination of the columns of A.
And then relating it to linear transformations, the null space of A is equal to the zero vector only.
If and only if the transformation x to A x is one to one.
If you back up to the previous one, we had that null A is equal to only the zero vector if A x equals zero has only the trivial solution.
Remember that's going to occur when you have no free variables or if there's a pivot position in every column.
And so we know that indicates that the transformation is one to one.
And then the column space is equal to RM if and only if A x equals B is consistent for every B.
So that means every B in the co-domain gets mapped to, so the transformation must be onto RM.
Okay, we're going to start today talking about a basis for a vector space.
And the idea of a basis is that it's a minimal in terms of number of vectors.
And representative in that it represents the vector space set.
So it's a minimal representative set.
So let's explore what that actually means.
Before we really get into that, let's back up a little bit and remember what a couple of concepts are that are key here.
One is linear independence and the other is spanning.
So let's start with linear independence.
So here's the definition.
A set of vectors v1 through vp is said to be linearly independent if the only solution to the equation,
we take the linear combination of the vectors and set it equal to zero.
Okay, this linear combination set equal to zero.
The only solution to that is when you set all the coefficients equal to zero.
Note that this is always a solution.
We can always set the coefficients equal to zero and generate the zero vector.
What we want to know is if this is the only solution.
So thinking in terms of a system of equations, we know the system is consistent.
We want to know if the solution is unique or if they're an infinite number of solutions.
So that's when we get into talking about free variables.
Does the system have a free variable or not?
Okay, so we set up that augmented matrix for that system.
We put an initial inform and we want to see if they're free variables.
That means an infinite number of solutions and the vectors would not be linearly independent.
And if we don't have free variables, that means the solution is unique.
So the vectors are linearly independent.
Okay, so to check for linear independence, we really don't have to tack on that zero column on the augmented side.
We can just look at the coefficient matrix, put it in initial inform and see if there's a pivot position in every column.
Okay, because we want to know are there free variables or not.
So is there a pivot position in every column?
Okay, there are a few cases that are obvious.
One is where you have more vectors than you have entries in each vector like this matrix given here.
These three vectors could not be linearly independent because you have three vectors in R2.
There's no way you can have a pivot position in each column.
If you just have two vectors, they're linearly independent if neither is a multiple of the other.
And in general, a set of vectors is linearly independent if none.
Okay, I can't find any of the vectors in the set that can be written as a linear combination of the others.
Alright, let's move on to spanning sets.
Take that same set of vectors, v1 through vp, and assume they're in Rn.
And they are said to span Rn if the equation, where you take a linear combination of those vectors and set it equal to b,
is consistent for every b in Rn.
That means you can take a linear combination of those vectors and generate any vector in Rn.
Now obviously we can't solve this system for every b in Rn.
It should be Rn, Rn.
So how do we know if it's going to be consistent for every b or not?
Alright, then linear independence, right hand side was zero, we can solve that system.
But here we want it to be consistent for every b.
Okay, so we can't go and plug in every possible vector b on the right and solve the system to see if it's going to be consistent for everyone.
So what do we do?
Well, we know that it will be consistent if we never end up with a row where we've got all zeros
and then something not zero on the right hand side when we put the matrix in echelon form.
So if we have this, where all zeros and then something not zero, then it's inconsistent.
So we want to know if we never get that.
And the way that that happens is we have a pivot position in every row.
There's a pivot position in every row of the coefficient part of the matrix.
Then you'll never end up with all zeros in a row of the coefficient part of the matrix.
So to check to see if a set of vectors spans whatever vector space they're in,
we need to have a pivot position in every row.
Okay, so the columns of A are linearly independent if there's a pivot position in every column of A
and they span RM if there's a pivot position in every row of A.
It's assuming A is an M by N matrix, so the columns are in RM.
Alright, so let's look at some examples.
Here's a set with just one vector.
Is it linearly independent?
And the answer is yes, because you have a single non-zero vector.
It's always going to be linearly independent.
The only time just a single vector is linearly dependent is if that vector is the zero vector.
Okay, does this set span R2?
And the answer is no, because if you look at all linear combinations of that vector,
you're just getting multiples of that vector, which is a line in R2.
So you only get a line, not all of R2.
Alright, let's look at this set T.
Now I've got three vectors and first let's ask is T linearly independent?
And the answer is no, because you've got more vectors than there are entries in each vector.
So if you put those vectors in a matrix, there's no way you can have a pivot position in every column.
Does this set span R2?
Yes, because if you just even look at the first two vectors, they are not multiples of each other
and so not collinear, so they will span the plane.
Now if you want to go back and just look at it in matrix form, take those vectors, put them in a matrix,
put it in echelon form, and look and see, do you have a pivot position in every column?
No, so they're not linearly independent.
Do you have a pivot position in every row?
Yes, so they do span R2.
Now let's look at one more set.
Now this one is like the other one, except I just took out that last vector.
So again, is it linearly independent?
Yes, we've got two vectors and neither is a multiple of the other, so they're linearly independent.
So it's span R2.
Notice that should be U instead of T there.
Does it span R2?
And again, it's yes for the same reason that used before.
Look at it in matrix form.
You've got a pivot position in every row, so the vectors span R2.
You have a pivot position in each column, so they're linearly independent.
So we see that any set that spans R2 has to have at least two vectors,
because you have to have one in each row, a pivot position in each row.
Any set that's linearly independent must have two or fewer vectors, right?
Because once you get over two, you can't have a pivot position in each column.
So any set that's both linearly independent and spans R2 has to have exactly two vectors, right?
Exactly two, because to span, it needs at least two.
To be linearly independent, it needs no more than two.
So if you want both, then you have to have exactly two vectors.
Such a set that's both linearly independent and spans R2 is said to be a basis for R2.
Here's a formal definition.
Set of vectors b1 through bp is a basis for some subspace h if the set's linearly independent
and the span of the set is the subspace.
Okay, so we need two pieces to be a basis.
One linear independence, the other must span the subspace.
So let's look at a few examples.
Here's a set, and I want to know is this a basis for R3?
So pick one of the criteria, either linearly independent or spans, and see if those are satisfied.
So let's start off with is it linearly independent?
Well, if you set up the augmented matrix, set it equal to zero,
then you can clearly see you don't have a pivot position in the second column here,
and so can't be linearly independent.
So it's not a basis.
All right, about another set.
There's one not so clear here whether these vectors are linearly independent.
So we put them in a matrix, do some row operations, and we end up here with a coefficient matrix.
Notice it has a pivot position in every column, so the only solution is the trivial solution.
Therefore, the vectors are linearly independent.
We can also look at the matrix here and see that there's a pivot position in every row.
So they span R3, and therefore they must be a basis for R3.
So the key is looking at the coefficient matrix in echelon form.
You can see a pivot position in every column, so they're linearly independent,
pivot position in every row, so they span R3.
Now here's another set.
Is this a basis for R3?
And you're probably thinking, well, there's only two vectors,
and in R3 you need three vectors to span, and that's right, because if you look at that matrix,
you can't have a pivot position in every row, so therefore these vectors can't span R3,
and therefore can't be a basis for R3.
Now how about this one?
If you look at this one, maybe you're thinking, oh, we've got four vectors in R3,
they can't be linearly independent, because if you put them in a matrix,
there's no way to have a pivot position in every column.
We've got four vectors, only three rows can't have a pivot position in every column,
so not linearly independent.
Now here's a little different question.
Here I've got a set of four vectors, and I'm not asking, though, is this a basis for R3?
Clearly it's not, because four vectors can't be linearly independent.
What I'm asking, though, is for you to find a basis for the span of this set.
Okay, so we don't really know what the span of this set is.
It's either all of R3 or some piece of R3, we don't know.
So we want to know whatever that space is, what is the basis for it?
Well, one of the criteria for a basis is that it spans the set.
So clearly, if we just take all four vectors, they're going to span W,
because that's how W is defined.
So the question is, are they linearly independent?
And we already know that they're not, because there's four vectors in R3.
So remember, since they're not linearly independent,
then that means that at least one of them is a linear combination of the others.
And so one strategy would be to throw out the ones that are dependent on the others,
and then reduce that down to where we have a linearly independent set.
Now, in this case, I'll point out to you that the second vector here is twice the first.
You can look at that, two times one is two, two times negative four, negative eight, and so forth.
And the last one, vector four here, is the sum of the first and the third.
So we've got one plus zero gives you one, negative four plus three is negative one, so forth.
So that tells us that the second vector is dependent on the first.
It's a linear combination of the first.
And the last one is a linear combination of the first and the third.
So it seems like we should be able to throw those out.
We don't need them.
And if we do, then we're left with these two vectors.
And we can look at it and see that it's linearly independent,
because we've got two vectors that are not multiples of each other.
It's still a little iffy on whether this set spans the original set W.
So because we threw out some, how do we know that these two still span the subspace?
Well, let's introduce a little shortcut notation here.
Let's say we're going to call the four original vectors V1 through V4.
And so any vector in the span of those vectors, or any vector in W, can be written in this form.
Linear combination of those four vectors.
Now, we know though that V2, the second vector, was two times the first one.
So we have this relationship here.
And we know that V4 was the sum of the first and the third.
So we have this relationship.
So we can plug that in back up in this equation.
So substituting for V2, we can plug in 2V1.
And for V4, we can plug in V1 plus V3.
And then if we do a little rearranging, we can write V as just a linear combination of V1 and V3.
We've got different coefficients, but that's okay.
It just needs to be a linear combination of V1 and V3.
So here we've shown that any vector that was a linear combination of V1, V2, V3, V4 is also a linear combination of just V1 and V3.
Therefore, just V1 and V3, just those two vectors will span W.
And we've already said that they're linearly independent.
So therefore, they are a basis for W.
So we start off with our big set throughout the ones that were dependent on the others.
And what we were left with still spans, and it's linearly independent.
So it's a basis.
Now, in that problem, I told you which vectors were dependent on the others.
So what can you do when that's not so obvious?
How can you figure out which ones are dependent when you can't just look at it and tell?
Well, there's an amazing thing.
And that is that when you do elementary row operations, the dependence relations among the columns are not changed.
So these are the four vectors we had before.
Remember, the second one is two times the first.
The last one is the sum of the first and the third.
Alright, so I put it in echelon form, and I get this matrix.
And look, the second column here is two times the first, right?
Two times one is two, two times zero zero, two times zero zero.
And the last column is still the sum of the first and the third.
One plus zero is one, zero plus one is one, zero plus zero zero.
So you can take your original matrix or original vectors, throw them in a matrix,
put the matrix in echelon form or reduced echelon form, it doesn't matter.
And in that matrix, the vectors will still exhibit the same relationships among each other.
So what you see here is that the ones that we want to keep are the ones where we have pivot positions.
Okay, because if one, like the second one here is dependent on the first one, there's no pivot position in column two.
Column four is dependent on the first and the third, there's no pivot position in column four.
So the pivot columns indicate which columns are, indicate the columns that are linearly, that should be independent.
Pivot columns indicate the columns that are linearly independent.
Okay, so those are the ones we want to keep.
We have a theorem that says that the pivot columns form a basis for the column space of A.
All right, now along with this comes a warning, which is to be careful to use the pivot columns of A,
that is the original matrix, not some echelon or reduced echelon form of A.
And that's because elementary row operations can change the column space of a matrix.
Okay, so we go back and look at that one example one more time.
The column space of A, that is all linear combinations of the columns of A, is not the same as a set of all linear combinations of the columns of B.
Those are not the same.
And so the column spaces are not the same.
For example, the first column of A is clearly in the column space of A.
One times first column, zero times the others, and that's what you get.
But that column is not in the column space of B.
Why?
Well, when we set up the system, we've got this right hand side, notice system's inconsistent,
because we've got all zeros and then something not zero.
Okay, it's not in the column space.
So the column spaces are different.
Therefore, you can't take the pivot columns from B and say that that's a basis for the column space of A.
You need to look in your echelon form or your reduced echelon form, see which ones are the pivot columns, and then go back and pull those from the original matrix.
Alright, here's another example.
Here's a matrix and you're asked to find a basis for the column space of this matrix.
So we put it in echelon form, get this matrix, and notice we've got pivot column in a pivot position in the first column, third column, and the fifth column.
So therefore, we go back and take the first, third, and fifth columns of A for a basis for the column space of A.
Okay, and here's a little different question.
Same matrix, you're asked to find a basis for the null space of A.
Alright, remember in the null space, set of all solutions to AX equals zero.
So let's solve AX equals zero.
Get the same matrix here.
We look at the general form of the solution.
From the first row, we get X1 is going to be negative 2X2 minus 4X4.
X3 is going to be 7 fifths X4.
In the third row, we get X5 is just equal to zero X2 and X4 free variables.
So we put that in parametric vector form.
We get what's given here.
And so any vector in the null space of A can be written as a linear combination of these two vectors.
Alright, so that means that they span the null space of A.
So we're halfway to being a basis.
Still need to show that they are linearly independent or reduce the set so that we get a linearly independent set.
So the question is, we know that they span.
Are they linearly independent?
And if you just look at them, it's clear that they are because we've got two vectors that are not multiples of each other.
So they must be linearly independent.
So these two vectors will be a basis for the null space of A.
Now, an important point is that in general, when you solve AX equals zero and write your solution in parametric vector form,
the vectors that you get will always be linearly independent.
So in fact, when you write that solution in parametric vector form,
you've got a set of vectors that spans and they will be linearly independent.
Now, why are they linearly independent?
Because the only way to produce the zero vector is to set each of the free variables equal to zero.
So if you go back and look at the one we were just looking at,
if you didn't know, if you can just look at that and tell that they're not multiples of each other,
so they're linearly independent, or if you tried to solve the system,
then notice that in the second component, you've got X2 plus zero would equal zero.
So X2 has to be zero.
And X4 times one equals zero means X4 has to be zero.
Because of the way these vectors are produced,
you always end up with the element that corresponds to the free variable.
So in this case, the second component, since this is multiplied by X2,
if you looked at that whole row, then the only solution would be set X2 equal to zero.
And similarly for X4, X4 has to be zero.
So when you write your solution in parametric vector form,
automatically you have a basis because they span and they're linearly independent.
Alright, one more example.
I've got three vectors here and we're given that v1 minus 3v2 plus 5v3 is equal to the zero vector.
And you're asked to find a basis for the span of these vectors.
So again, clearly you know that if you took all three, you have a set that spans.
But we know from what we're given that they're not linearly independent.
So we need to figure out which vectors to throw out until we get to a linearly independent set.
And since we can write any of the three as a linear combination of the other two,
simply solve this equation for any of the three vectors, then we could throw any of them out.
But it's kind of a convention to throw out the first one that's linearly dependent, which would be v3.
And so we're left with v1 and v2.
And if we just look at those, we can see that they're not multiples of each other.
And so therefore the set v1 v2 is linearly independent.
It still spans and therefore it's a basis for w.
Okay, today we're going to talk about using a basis as a coordinate system for a vector space.
We start off with the unique representation theorem.
And it says suppose you have a basis b, which consists of the vectors b1 through bn.
And you choose some x from the vector space v.
And the theorem says that no matter which x you pick from the vector space, there are a unique set of scalars,
c1 through cn that you can use to take a linear combination of the vectors in your basis to produce x.
Okay, now, so it's really saying two things.
It's saying that that system's consistent.
You can take a linear combination of the b's and you can produce any vector in the vector space.
And it's also saying that the solution to that system is unique.
And both these things follow from the fact that b is a basis for v.
The fact that it's a basis tells you that these vectors span v.
So therefore that system is going to have a solution no matter what x is.
And it also tells you that these vectors are linearly independent.
And since they're linearly independent, the solution to that system will be unique.
Since there's no free variables.
Okay, now the weights that we use, these c values, are called the coordinates of x relative to the basis b.
Okay, and we write them using your book's notation as x with the brackets around it.
This notation here with the subscript b.
And we call that the coordinate vector of x relative to the basis b.
Alright, so for example, we have a basis for r2 here.
The vectors 1, 2, 3, 4.
Just a quick aside, how do we know that that's a basis for r2?
Well, the simplest way is to say, well, we know the dimension of r2 is 2.
And here we have a set with two vectors that are linearly independent.
And we know they're linearly independent because there's two vectors and neither's a multiple of the other.
Alright, so my question is, based on this particular basis, can you find the coordinates of the vector 1, 0 relative to this basis?
So I'm just setting up the system that we talked about previously.
I need to take a linear combination of the vectors and the basis to produce x.
And so we can set up that system in an augmented matrix and solve the system.
And it tells us that the numbers we're looking for are negative 2 and 1.
Just a quick check.
If we multiply negative 2 times the first vector plus 1 times the second vector, that gives us 1, 0, which is what we were looking for.
So that tells us that the coordinate vector of x relative to b, which we write in this notation, is negative 2, 1.
Alright, suppose we go the other way.
Suppose you're given the coordinate vector and you want to find the corresponding vector.
So in this case, all you need to do is compute that linear combination.
You have the coordinates.
So it's, in this case, be 5 times the first vector plus 10 times the second.
And we compute that to be 3550.
Okay, this is a little bit of an aside, but kind of leads us into what we're going to talk about next.
If you have any vector, say just AB, then the vector itself is the coordinate vector relative to the standard basis.
So let's refresh our memory about what the standard basis is.
That's just for R2, just the two columns of the identity matrix.
Now your book refers to that as the labels that set with a script E.
Alright, so notice that if you've got a vector AB, then that's just A times 1, 0 plus B times 0, 1.
So your coordinates are A and B.
So therefore the coordinate vector is just equal to the vector itself, which is just AB.
So relative to the standard basis, the entries in a vector are actually the entries in the coordinate vector.
Okay, so just a little review.
If we have a basis and a coordinate vector, how do we find the corresponding vector?
Well, we just compute that linear combination.
Alright, so we know that this linear combination can be written in as a matrix times a vector.
And so if we define this matrix, we'll call it P sub B to be the matrix consisting of the vectors and the basis.
Then this linear combination can be written as P sub B times the coordinate vector.
And we say that P sub B is the change of coordinates matrix from B to the standard basis of our N.
Alright, so we're taking P sub B, multiplying it by the coordinate vector relative to B,
and we end up with the coordinate vector relative to the standard basis,
which as we saw before is just the vector itself.
Alright, so talk about how we go back the other way.
For giving a basis and a vector, how do we find its coordinate vector?
Well, we need to solve a system, right?
Take the linear combination, set it equal to the vector, and solve the system.
We need to figure out what those multipliers need to be.
So looking at it in matrix terms, we have this relationship, but we don't know what the coordinate vector is.
So therefore we need to solve the system for the coordinate vector.
And one way to do that is to multiply both sides by the inverse of the matrix,
and that'll give us the coordinate vector.
Now, the danger there is, well, how do we know that matrix is invertible?
Well, we actually do know that it's invertible because we know that its columns are a basis for our N.
And so that's actually straight out of the invertible matrix theorem,
but you can kind of get there in a couple of steps if you think, well, columns are a basis for our N,
so that means that the columns must be linearly independent,
which means there's a pivot position in every column, therefore N pivot positions.
So the matrix is invertible.
Okay, so we're going to put those two ideas together in this question.
So here I've got two bases for R2.
Well, bases, that's the plural of bases.
If you've seen that kind of confusing, but bases is just the plural of bases.
So we have two bases for R2 here called B and C.
And I have a coordinate vector for a vector x relative to bases B.
So a coordinate vector of x relative to B, which is this vector here.
And I want to know how do we find the coordinate vector of x relative to C?
Well, you can think of it probably easiest as a two-step process.
So first, use the coordinate vector relative to B to find actually what x is.
And once you know what x is, then you can find the coordinate vector of x relative to C.
So we'll do those two steps.
So the first one here, to find x, we just multiply our piece of B matrix,
the matrix consisting of the vectors in B, times our coordinate vector,
find that x is 244.
And once we know that, then we just need to take a linear combination of the vectors in C
and set that equal to 244 and solve that system.
Or look at it like this, where we solve the system by taking the inverse of that matrix times x.
So here's a piece of C.
These are just the columns that are in the basis C.
We invert that matrix, multiply by 244.
So what we have here, I'm taking one over the determinant times this matrix times 244.
And I end up with a vector of 4, 2.
So this is coordinate vector of x relative to C.
And just to check, we can take that, these coordinates, times our vectors in C,
and see if we'll get x, which we do, 244.
Alright?
So let's take, let's go back and take a look at what we did.
Alright, first step was to compute x, which we did by piece of B times x of B.
And then compute the coordinate vector relative to C by piece of C inverse times x.
Now we can't put that all together.
Just plug in for x.
And we have the coordinate vector of x relative to C.
It's just PC inverse times PCB times the coordinate vector relative to B.
And so what we have is a change of coordinates matrix from B to C.
And this matrix takes us from coordinates in B to coordinates in C.
Alright?
So PC inverse times PCB is how we can change B coordinates to C coordinates.
Alright, we're going to switch gears just a little bit and talk about polynomials.
Let's examine this problem.
These polynomials are a basis for piece of three.
Okay, this is actually a homework problem from section 4.5, I think.
Okay, so to show that these are a basis for piece of three,
we need to show that they span piece of three and that they're linearly independent.
So let's start off and show that they're linearly independent.
So to show any sets linearly independent, we have to take a linear combination
and set it equal to the zero vector.
Or in this case, the zero element of piece of three.
So here's a linear combination.
Alright?
C1 times the first element, C2 times the second one, so forth.
Equals, here's the zero element of piece of three.
Zero plus zero times t plus zero times t squared plus zero times t cubed.
Alright, and we want to be able to show that the only solution to this system
is that all the C's have to be zero.
So the way to approach that is to collect like terms.
Alright, collect all the constant terms, collect all the coefficients of t and so forth,
and then equate those coefficients to what we have on the right-hand side.
So we do that, we end up with this, so this comes from, we've got C1 here,
and then from this term we get a two times C3, so that's our constant term.
Coefficient of t here, we've got a two times C2 from here,
and over here a negative 12 times C4, so that's where that comes from.
For t squared I've got four times C3, and for t cubed I've got eight times C4.
Alright, so those are equal to this zero vector again.
Now, if you write out that system of equations, right, we're going to get C1 minus 2C3 equals zero.
2C2 minus 12C4 has to be zero, and 4C3 has to be zero, and 8C4 has to be zero.
So if you write that out, it looks like this, and when you look at it like that,
it's obvious that all the C's have to be zero.
Alright, from the last equation you get C4 has to be zero.
From this one we get C3 has to be zero, and since both C3 and C4 are zero,
then that means that C1 and C2 have to be zero.
Alright, so we've shown here that the polynomials are linearly independent.
Let's just look at it in matrix form.
If we just look at the coefficients, it's already in echelon form,
and we have a pivot position in every row.
So from that we know that these polynomials must span piece of three.
So no matter what the right-hand side is, there'll be a solution.
Alright, so we have that they're linearly independent, and they span piece of three,
so they have to be a basis for piece of three.
Now there's actually a somewhat easier way to look at this,
and that is to look at the polynomials and what we ended up with in the columns of this matrix.
Now, actually the way it works is a polynomial of this form.
Here's a degree three polynomial.
It actually has a one-to-one correspondence with the vector in R4 that looks like this.
First comes the constant term, then the coefficient of t, coefficient of t squared,
and coefficient of t cubed.
So for example, our polynomial, one of them was just one,
so that corresponds to the vector 1-0-0-0 and R4.
If you look, that's what the first column of this matrix, our coefficient matrix, was.
Then just pick another one, have it the last one, negative 12t plus 8t cubed.
Alright, then that's going to have, that's 0 for the constant term,
negative 12 for the coefficient of t, 0 coefficient of t squared,
and 8 for the coefficient of t cubed.
And that's what the last column in our coefficient matrix looks like.
So the bottom line is that every polynomial in piece of n
can be represented as a vector in Rn plus 1.
So there's a one-to-one correspondence between polynomials in piece of n
and vectors in Rn plus 1.
Alright, let's talk about the dimension of a vector space.
To do this, we need to go back and review what a basis is.
We call that a basis is a set of vectors that is linearly independent
and spans the subspace that it's a basis for.
So let's think about R3.
If you have more than three vectors in R3, then they must be linearly dependent.
Put them in a matrix, put it in an echelon form, then you would have free variables.
If you have less than three vectors, then they don't span R3.
If you had only two, you wouldn't be able to have a pivot position in every column
if you stuck them in a matrix.
So, or wouldn't have a pivot position in every row, excuse me.
So they would not be, they would not span R3.
So put those two together, you need at least three to span.
If you have more than three, they're linearly dependent.
So that means that any basis for R3 has to have exactly three vectors.
Alright, our theorem says that if a vector space has a basis of n vectors,
then every basis for that space must have exactly n vectors.
This number of vectors in a basis for a vector space is called the dimension of that vector space.
And we have a special case where the vector space has only the zero element.
Okay, we call that the zero vector space.
And since there's no basis for that set, then we define its dimension to be zero.
Okay, so to find the dimension of a vector space or subspace,
then one approach is simply to find a basis for that space
and count the number of vectors in that basis.
The standard basis for Rn consists of the vectors e1 through en.
So you might recall that e sub i is the vector in Rn that has a one in the i-th position
and zeroes out everywhere else.
Or you can think of e sub i as just the i-th column of the identity matrix.
So the standard basis for R3 consists of these three vectors,
which you'll recognize as the columns of the three by three identity matrix.
In general, the dimension of Rn is n.
Alright, let's talk about polynomials.
P sub n, if you recall, is the set of polynomials of degree n or less.
So for example, P sub 2 consists of all polynomials that are quadratic or less.
All polynomials that look like a naught plus a1 times t plus a2 times t squared,
where the a values are just real numbers.
The standard basis for P sub 2 is this set, 1, t, and t squared.
So any vector in P sub 2, any polynomial in P sub 2 can be written as a linear combination of these three objects.
So the dimension of P sub 2 is 3.
And in general, the dimension of P sub n is n plus 1.
Okay, what if we have a set like this defined in terms of these parameters?
How do we find the dimension of such a set?
Well, first let's find a basis.
And to do that, we can write this generic vector in parametric vector form.
And so at this point, we know that these three vectors span R set S.
And so, excuse me, so we want to know are they linearly independent?
And to check to see if they're linearly independent, we throw them in a matrix.
These are row operations.
Get that in echelon or reduced echelon form.
I got it in reduced echelon form.
And we can see here there's not a pivot position in every column.
Therefore, they're not linearly independent.
We can see that there's a pivot position in the first two columns.
So that means that the first two vectors are linearly independent.
And notice that the negative one-third here means that the third column is minus one-third of the first column.
And if you look back, that is true.
Minus one-third of three is negative one.
Minus one-third of six is negative two and so forth.
All right.
So that means that we can throw out that third column.
And a basis for S would just consist of the first two vectors.
And once we have a basis, then we need to only count the vectors in the basis to get the dimension.
So there's two vectors in the basis.
That means the dimension of S is two.
And we don't say that S is R2, because it's not, because these vectors are in R4.
But what we do say is that S is a two-dimensional subspace of R4.
All right.
The basis theorem, this is an important theorem.
It says that if you have a vector space with dimension P, where P is greater than or equal to one,
then the following conditions hold.
Number one, any linearly independent set of P elements in V is a basis for V.
Automatically.
Don't have to check to see if it spans.
Similarly, any set of P elements that spans V is automatically a basis.
Okay.
Don't have to check to see if it's linearly independent.
All right.
So since we know the dimension of R3 is three, if you have a set of three vectors in R3 that are linearly independent,
then you know they're a basis.
You don't have to check to see if they span, because they will.
Similarly, if you have a set of three vectors that span R3, then you know they're a basis for R3.
You don't have to check to see if they're linearly independent, because they will be.
Okay.
So we basically end up with three pieces of the puzzle.
Let's go back to that.
Three pieces of the puzzle.
Knowing the dimension, having a set of that many vectors that spans,
or having a set of that many vectors that's linearly independent.
If you have any two of those three pieces, then you can conclude that you have a basis for the vector space you're dealing with.
Okay.
For giving a matrix A, how do we find the dimension of the column space of A?
Well, we go back to our method, find a basis, and count the number of vectors in the basis.
Let's think back.
How do we find a basis for the column space of a matrix?
We will put A in echelon form so that we can find the pivot columns.
We pull those columns from A, not from the echelon form, but from the original matrix A, and that's our basis.
So here's one I think we looked at this one last time.
We take the matrix A, put it in echelon form.
We can see that the first, third, and fifth columns are pivot columns.
So we pull the first, third, and fifth columns from A, and that's a basis for the column space of A.
And once you have a basis, then getting the dimensions trivial, three vectors in the basis of the dimension of the column space of A is three.
In general, the dimension of the column space of a matrix is simply equal to the number of pivot columns in the matrix.
All right.
How about the null space?
We have a matrix A.
How do we find a basis, or how do we find the dimension of the null space of A?
Well, once again, find a basis and then count the number of vectors.
So let's think back.
How do we find a basis for the null space of a matrix?
Well, we have to solve AX equals zero and write the solution in parametric vector form.
And the vectors that we get there both span the null space of A and they're linearly independent.
We made that argument last time that when you write those in parametric vector form,
they will be linearly independent.
And therefore, there'll be a basis for the null space of A.
So if we start with this matrix A, we solve AX equals zero and write the solution in parametric vector form.
We'll get these two vectors.
And they are x5 is zero, right?
Yeah, x5 is zero.
So that's right.
So these two vectors are linearly independent.
They span the null space.
So there are basis for the null space.
Therefore, the dimension of the null space for this particular matrix is two.
In general, the dimension of the null space is equal to the number of free variables in AX equals zero.
Because you have a vector here corresponding to each free variable.
And where do free variables come from?
They come from columns that don't have a pivot position.
So the dimension of the null space is equal to the number of non-pivot columns in A.
And recall, dimension of the column space is equal to the number of pivot columns.
Null space dimension is equal to the number of non-pivot columns.
So if we add those two quantities together, the dimension of the null space plus the dimension of the column space,
we get the number of pivot columns plus the number of non-pivot columns, which is equal to the number of columns.
And we'll hit on that some more next time.
Alright, let's move on to talk about the rank of a matrix today.
Now first, a little bit of review.
Remember, the column space of a matrix is the set of all linear combinations of the columns,
which is also the span of the set of columns of the matrix.
So we're going to talk about another subspace.
This time the row space of a matrix.
So we know what the column space is, so what do you think the row space of a matrix is?
Well, if you just take the analogous route, then we get the row space of a matrix is a set of all linear combinations of the rows.
Or the span of the set of rows of the matrix.
So it is truly just the analogous term to the column space.
Okay, we have the column space, all linear combinations of the columns, row space, all linear combinations of the rows.
Okay, so think back to find a basis for the column space of a matrix.
We put the matrix initial on form so that we can determine which columns are the pivot columns.
Then we go back to the original matrix and pull out those columns, and that's a basis for the column space of the original matrix.
So the question is, how do we find a basis for the row space of a matrix?
And here the similarities end somewhat, although there still are some similarities.
But this theorem tells us how we can find a basis for the row space of a matrix.
It says if two matrices A and B are row equivalent, so you can get from one to the other doing elementary row operations, then their row spaces are the same.
Now you'll recall that that is not true for the column spaces.
When you do elementary row operations, you may be changing the column space of the matrix.
But this theorem tells us that for row space, that stays the same when you do row operations.
Okay, if B is an echelon form, the non-zero rows of B form a basis for the row space of A as well as for that of B.
Okay, so this tells us to find a basis for the row space of A. We put it in echelon form and pull the non-zero rows from that echelon form matrix.
And we have a basis for the row space of the original matrix plus the echelon form matrix and any intermediate matrices that we encountered.
Okay, so again, elementary row operations do not change this back up here.
This is talking about the linear dependence relationships among the columns.
Okay, so that means like if the second column of A is 10 times the first column, then when you do elementary row operations, that doesn't change.
So elementary row operations do not change the linear dependence relationships among the columns of a matrix.
However, they can change the linear dependence relationships among the rows of a matrix.
Okay, so let's look at a little simple example.
We start off with this matrix A. We do one row operation and end up with this one, which I'll call B.
Now, a basis for the column space of A, we could just look at A and say, well, I can see that the second column is a multiple of the first,
so I throw it out and just keep the first column.
Or you could look at B and see where the pivot positions in B.
Well, there's only one in the first column, so that means we want to pull the first column from A to be a basis for the column space of A.
If we want a basis for the column space of B, that's straightforward because B is already an echelon form,
so we just pull, we know what the pivot columns are.
There's only one first column, so that's a basis for the column space of B.
Okay, so if we look graphically at the column space of each of these matrices,
the red line here, you see this vector right here, this is one one,
and so this red line is any multiple of the vector one one,
so that's the red line is the column space of A.
This vector here is one zero, and since that's a basis for the column space of B,
the blue line or the x-axis is the column space of B, right, all multiples of one zero.
So clearly here, these column spaces are not the same.
These matrices are row equivalent, but their column spaces are not the same.
All right, let's look again, same set of matrices.
Now the row spaces are the same because if we want a basis for the row space of A,
then right, we look at each row of A, well they're the same row, right, each row is the same,
and so we only need one of them, throw out the second one, and we keep the first row,
and that's a basis for the row space of A.
Right, if we were using theorem, we would get A and echelon form, which we have here,
and take the non-zero rows of that matrix.
Well the non-zero rows is just that row one two.
So either way you look at it, you end up with this basis for the row space of A.
And similarly for the row space of B, it's going to be all multiples of the row one two
because zero zero doesn't add anything to the picture.
So this set, which consists of just that one row, is a basis for both the row space of A
and the row space of B.
Now let's look at a little more interesting example.
There's a big matrix A, and after some row operations, we end up with this version of A.
Okay, this is an echelon form, and we'll call that matrix B.
Okay, so to get a basis for the column space of A, we look at B, we can see that there's a pivot position
in the first column, the second column, and the fourth column.
So we choose those columns out of A, first column, second column, and the fourth column.
And that's a basis for the column space of A.
So the dimension of the column space of A is the number of vectors in this basis, which is three.
To get a basis for the column space of B, we look at B, it's already an echelon form,
so we choose the pivot columns, and so we get the first, second, and fourth columns.
That's the basis for the column space of B.
And the dimension of the column space of B counts the vectors in a basis, that's three.
So the dimension of the column space of B is three.
All right, what about a basis for the row space of A, in which we know from the theorem
will also be a basis for the row space of B.
Well, according to the theorem, put A and echelon form and choose the non-zero rows.
So we do that, we get the first, second, and third rows of B, which are given here.
So the dimension of the row space of A is equal to the dimension of the row space of B,
and notice there's three vectors here, so that is three.
All right, so we've got the dimension of the column space of A is three,
dimension of the row space of A is three,
and the question is, is this a coincidence that they're both the same value?
And if you think about that just a little bit, you can say no, I don't think so,
because the dimension of the column space of A is the number of pivot columns or pivot positions in A.
And that's, we look at B to figure that out, but there's three pivot positions.
And the dimension of the row space of A is also equal to the number of pivot positions,
because there's a pivot position in each non-zero row in B here, or in an echelon form of A.
So each of these values is based off of the number of pivot positions, right?
Because a pivot position defines a pivot column, and a pivot position defines a non-zero row.
So for any matrix, the dimension of the column space is equal to the dimension of the row space,
which is equal to the number of pivot positions in that matrix.
Okay, and this quantity is what we call the rank of a matrix.
The rank of a matrix is the dimension of the column space,
which is also equal to the dimension of the row space of that matrix.
Guys, we just call that the rank of the matrix.
All right, back to this one. Let's talk about the null space.
What is the dimension of the null space of A?
Well, we end up with a, we want to find a basis for the null space, right?
We solve AX equals zero and write our solution in parametric vector form.
And those vectors will be a basis for the null space of A.
Now, how many vectors do you end up with in that case?
Well, you end up with one for each free variable, okay?
Key is one for each free variable.
And if you think about that a little more, you think, hmm, well, where do I get a free variable?
Well, it's free variable is one whose column does not contain a pivot position.
So the dimension of the null space is a number of free variables in AX equals zero,
also equal to the number of non-pivot columns in the matrix, okay?
For this particular matrix, dimension of the null space is three,
because here's a non-pivot column, the third one, and the fourth and fifth and sixth.
So the third column, fifth column, sixth column are all non-pivot columns.
So we have three non-pivot columns, and therefore the dimension of the null space is three.
Alright, so once again, this number three pops up.
We have dimension of the column space is three, dimension of the null space is three.
So I ask again, is this a coincidence?
Hmm, and the answer is yes, it is a coincidence.
It's a coincidence because dimension of the column space is the number of pivot columns.
Dimension of the null space is the number of non-pivot columns.
So when you add them together, you get what?
The number of columns in the matrix.
So it just so happened that there were six columns in this matrix,
and so if the dimension of the column space is three,
then the dimension of the null space is going to be six minus three, which is also three.
So that's totally coincidence.
If there had been seven columns, the dimension of the null space would have been four.
Alright, so we have the dimension of the column space plus the dimension of the null space is equal to the number of columns.
And since the dimension of the column space is equal to the dimension of the row space,
we have the dimension of the row space plus the dimension of the null space is equal to the number of columns.
And one more time since the dimension of the column space and the dimension of the row
space are equal to the rank of a, we have the rank plus the dimension of the null space
is equal to the number of columns.
Okay, so in this section we have another installment of the invertible matrix theorem.
If you recall, we had this version of it, or this installment of it back in section 2.3.
And so this should be seared into your memory at this point.
If not, you go back and review it.
But one of the pertinent things is, recall that the whole idea here is that all these
statements are equivalent, which means that they're either all true or all false.
Number one here says A is an invertible matrix.
Now one of the most important pieces of this is number three, that A has n pivot positions.
Because if you recall, when I was telling you about how to remember all this, I told
you the easiest way is to relate everything to pivot positions.
So if we can establish for any part of this that A has n pivot positions, then we're done.
So what we have new here, we have six new things, four of them are given here.
So let's take a look at these.
Number one says the columns of A form a basis for R n.
If that's true, if they're a basis for R n, then that means that they must be linearly
independent and they must span R n.
Either one of those says that there's a pivot position in every row and every column, therefore
they're n pivot positions.
So A is invertible.
The column space of A equals R n.
That means that the columns of A span R n, which again, they can only do that if there's
a pivot position in every row, meaning there's n pivot positions.
The dimension of the column space is n.
That actually follows from number two here.
If column space of A equals R n, we know that there are n vectors and a basis for R n.
So therefore the dimension of the column space is n.
And the rank of A equals n, that follows from number three, since the dimension of the column
space is equal to the rank of a matrix.
Okay, then we have two more that deal with the null space.
Null space of A is just the zero vector.
Okay?
So if that's true, then that means that if we look at the system A x equals zero, it
has only the trivial solution.
That happens when there are no free variables, which means there's a pivot position in every
column, which means they're n pivot positions.
And from number five follows number six, because if you have only the zero vector, then that's
a special case, and we define the dimension of that vector space to be zero.
All right, so all these are equivalent to the statement that A is an invertible matrix.
So just like those first ten, I believe it was, let's see, yep, first ten.
These extra six, you need to commit them to memory, relating them to each other or to
pivot positions.
All right, I want to go through some of the problems at the end of this section, because
there's some excellent problems here.
In fact, this section, I think, is the most important section in the course.
So make sure that you really work on these problems and understand what you're doing
here.
So I'm going to do several of them at the end of the section.
Okay, so the first one says, suppose a five by six matrix, let's call it A, has four pivot
columns.
Okay, so it's a five by six matrix with four pivot columns.
Okay, if it has four pivot columns, then that means it has two non-pivot columns.
So what's the dimension of the null space of A?
Must be two, right, because you've got two non-pivot columns.
Is the column space of A equal to R4?
Well, the dimension of the column space is four, because we have four pivot columns.
Okay, so the dimension of the column space is four, but the columns are in R5, and therefore
the span of the columns is not equal to R4.
It's just a subset of R5.
All right, next one.
If the null space of a seven by six matrix is five dimensional, okay, so that means you've
got five non-pivot columns, which means out of six columns, one is a pivot column, then
therefore the dimension of the column space has to be one.
All right, suppose the null space of a five by six matrix is four dimensional, then that
means you have four non-pivot columns, so that leaves two pivot columns.
And we want to know the dimension of the row space.
We have two pivot columns, so that means we have two non-zero rows when we put the matrix
in echelon form, so the dimension of the row space is two.
All right, and what if A is four by three?
What is the largest possible dimension of the row space?
All right, well, if A is four by three, then we could have at most three pivot positions,
so at most three non-zero rows when we put it in echelon form, so the maximum dimension
of the row space is three.
All right, if A is three by four, what's the largest possible dimension of the row space?
It's three by four, then again, the maximum number of pivot positions we can have is three,
so we have at most three non-zero rows, so the maximum dimension of the row space is
three.
How about if A is six by four?
What is the smallest possible dimension of the null space?
That's asking what's the smallest number of free variables you can have, or what's the
smallest number of non-pivot columns you could have?
Well, since we have more rows than columns, every column could be a pivot column, in which
case there's no free variables, and so the minimum dimension of the null space of A would
be zero.
All right, a little more complicated one here.
Suppose a non-homogeneous system of six equations in eight unknowns has a solution with two
free variables.
Is it possible to change some constants on the equation's right sides to make the new
system inconsistent?
All right, so we got it.
A non-homogeneous system, six equations and eight unknowns, and it's consistent with
two free variables.
What that tells you are the two free variables is that you have, since there's eight unknowns,
we got eight columns.
Two are free, so that means there are six that are pivot columns, so that would look
like this.
Six rows here, eight columns, two free variables, which leaves six pivot columns, all right?
And it's saying, if we change the right-hand side, will the system still be consistent?
And the answer is yes.
Let me plug my computer in.
Is it possible to change some constants to make the new system inconsistent?
No, the answer is no to that.
Because no matter what's over here, since you have a pivot position in each row, the
system's always going to be consistent.
Now, I was thinking the question was, is the system always consistent?
And the answer to that is yes, right?
And that is because you have a pivot position in every row, so you're never going to end
up with a row of all zeros and then something not zero over here.
All right, another one.
Is it possible that all solutions of a homogeneous system of two equations and four unknowns
are multiples of one fixed non-zero solution?
Okay, homogeneous system, two equations and four unknowns, okay?
So if we think about that, we've got two rows and four columns, so we have at least
two free variables, right?
So if we wrote our solution in parametric vector form, so we had two free variables,
then it would look like this.
Now what this says is that each solution is a linear combination of two fixed non-zero
solutions, right?
Each one of these vectors is what they're calling a fixed non-zero solution.
So we're going to have at least two of these vectors.
So that means that it can't be the case that all the solutions are just multiples of one
non-zero solution.
If that was the case, we would only have one vector here, and we can't have that because
we have at least two free variables, all right?
Another one, is it possible for a non-homogeneous system of three equations and two unknowns
to have a unique solution for some right-hand side of constants?
So three equations, two variables, all right?
So that might look like this, three equations, two variables, so you could have a pivot position
in each column, and you could have a row of all zeros.
In that case, yes, the system would be consistent, and there would be a unique solution.
All right?
How about this one?
Is it possible for such a system to have a unique solution for every right-hand side?
And okay, look at that, you could have this situation, still have pivot position in each
column, but have zero, zero, something not zero, in which case there's no solution, right?
So in this case, it's not possible for such a system to have any sort of solution for
every right-hand side, right?
In some cases, it's simply going to be inconsistent.
All right?
That's it for this one.
Okay, our topic for today is Markoff Chains, and to get us going on that, look at a small
example.
So let's suppose in a given urban area, each year, five percent of the population that's
in the city moves to the suburbs, and three percent of the suburban population moves to
the city.
And we're going to assume that this is a closed system here, so any movement just goes from
the city to the suburbs or suburbs to the city, so we're not considering moving elsewhere
or people moving in from elsewhere.
So just city to suburban, suburban to city.
Okay, so let's assume that the current city population is 600,000, and the current suburban
population is 400,000, then what will the population of each be a year from now?
Well, based on what we know next year, the city population is going to be 95 percent
of what it is now, right, because each year, five percent leaves, so we're keeping 95 percent,
and then three percent of the suburban population moves to the city.
So we add on 0.03 times the current suburban population.
So that's 0.95 times 600,000 plus 0.03 times 400,000, which gives us 582,000.
So it makes sense that the city population is going down.
We've got more moving to the suburbs than we have coming in.
To find the suburban population in a year, we could just say, well, since the total is
a million, then we can just say a million minus 582,000, and get it that way, which
is correct, but we could also look at it the way we did with the city and say, well, we're
getting five percent of the city population moving to the suburbs, and 97 percent of the
current suburban population is staying there.
So we get 0.05 times 600,000 plus 0.97 times 400,000, which gives us 418,000.
And 418 plus 582 gives a million total.
Another way to look at it is let's take a closer look at those equations, and notice
that we've got 600,000 times the 0.95, 0.05 here, and we've got the 400,000 times the
0.03 and the 0.97, and that looks like we're taking all in your combination of a couple
of vectors, the 0.95, the 0.05, and the 0.03, 0.97.
So we're going to rewrite it in this form as a matrix times a vector, and so the linear
combination is just 600,000 times the first column, plus 400,000 times the second column,
and that gives us our population figures for one year from now.
We could also look at it as percentages, which is typical instead of actual raw numbers
so if we look at the population vector as 0.6, 0.4, we do the same sort of computation
and end up with this population vector for a year from now, and if you notice that each
one of these vectors, the columns of the matrix plus our two vectors here, each one of them
sums to one.
So they're special vectors because they represent probabilities or percentages, and we'll talk
about that a little more in just a minute, but before we get to that, notice that what
we have here, this is of the form, some matrix, which I'm calling M, times a vector I'll call
x naught, which is like the original state of the system, original population.
We multiply those together to get x1, which is the state in year one.
We can continue that process to figure out what the population in year two would be,
year three and so forth, year k plus one.
For any year, it's just M times the population from the previous year.
So for instance, if I wanted to figure out 10 years from now, what's the population,
then I could compute x2, which is, well, to compute x10, I need x9, because x10 is M
times x9, to get x9, I need x8, to get x8, I need x7 and so forth.
So I'd have to compute all those intermediate population vectors.
There is another way to do it, which doesn't require that computation, and that is to look
at it as I have here at the bottom.
If we look at x2, for example, that's M times x1, but we know that x1 is M times x0.
So x2 turns out to be M squared times x0.
Similarly, x3 is M times x2, but we know x2 is M squared x0.
So x3 is just M cubed times x0.
And in general, we have xk, year k, population would be M to the k times the original population
vector.
OK, so a vector with non-negative entries that add up to 1 is called a probability vector.
So back to these, all these are probability vectors because they all add up to 1.
Each column adds up to 1.
OK, a stochastic matrix is a square matrix whose columns are probability vectors.
So the matrix that we had here, that is a stochastic matrix because each of its columns
is a probability vector, a vector where their entries are non-negative and they sum to 1.
A Markov chain is a sequence of probability vectors, x0, x1, x2, and so forth, together
with a stochastic matrix M such that x sub k plus 1 is just M times x sub k, which is
the pattern that we had.
In our example, the i-th entry in xk is the probability that the system is in state i
at time k, OK?
And we call xk a state vector because it represents the state of the system at time k.
OK, now an interesting question related to our population example is, will there ever
be a point at which there's no net population change?
That is, the number of people moving to the suburbs is equal to the number of people moving
to the city.
OK, another way to look at it is, will there ever be a point at which xk plus 1 is equal
to xk?
All right, so you compute xk plus 1 and it's the same as xk, or will there ever be a point
at which you have your current state is some vector x?
You apply the transition matrix M to it, and you end up back with the same vector x.
OK, and so this is the system we want to look at.
It's similar to systems that we know how to solve, but it's a little bit different because
on the right-hand side here, we've got a variable.
It's not a constant, so it's not like an ax equals b system.
So we need to manipulate it a bit before we can use the things that we know about solving
systems.
OK, so we want to know, is it consistent?
And the way we determine that is, we bring the x over to the left-hand side.
We get Mx minus x is a zero vector, and then the key is to factor out that x.
And if we factor out the x, then we're left with M minus the identity matrix.
So I here is the identity matrix.
So what we end up with is some matrix, which is M minus the identity matrix, which we can
compute directly, we know both of those, times x, which we don't know, equals a zero vector.
So when we write it in this form here, it's a simple homogeneous system.
Now clearly, it's consistent because x can be the zero vector, and that obviously makes
that consistent.
However, that's not a very interesting case.
So what we really want to know is, does this system have an infinite number of solutions?
Are there any free variables?
All right, well, if we look at this matrix for our example, all right, so we subtract
off the identity, and we're left with this one, and it's clear when you look at that
that the columns are not linearly independent, and therefore you have a free variable.
So if we solve this system, it's pretty straightforward.
One row operation, we zeroed out the bottom row, and so first row tells us that five
multiplied by 100 makes life simple.
So we get five x1 equals three x2.
So that means x1 is three-fifths times x2, where x2 is a free variable.
And to get the actual vector, the probability vector that corresponds to this relationship
between x1 and x2, we want x1 plus x2 to equal a million, or we could say one.
I'm doing it just for raw population numbers.
And x1 is three-fifths x2.
So we get this equation, putting these two together, eight-fifths x2 is a million, which
is x2 is 625,000, and therefore x1 is 375,000.
So we reach the point here that the number moving out of the city is 0.05 times 375,000.
So every year, 5% leave the city.
So the population of the city at this point is 375,000.
So 5% of that's 18,750.
Likewise, the total moving out of the suburbs, which is 3% of the suburb population, which
at this point is 625,000, is also equal to 18,750.
So we've reached the point here where the number moving out of the city is equal to
the number moving in.
So there's no net population change once the system gets to this point.
A little more terminology for you.
If p is a stochastic matrix, then a steady-state vector, also called equilibrium vector, for
p is a probability vector q such that p times q is equal to q.
Before we said m times x equals x, we're talking about the same thing here.
We have this theorem that says if p is a stochastic matrix, then it has a unique steady-state
vector.
Even though the system has an infinite number of solutions, if we require that the elements
sum to 1, then that makes it a unique vector.
Furthermore, if x naught is any initial state, and we have x sub k plus 1 equals p times
x sub k, then the Markov chain converges to q as k goes to infinity.
So what's interesting about this is that the initial state is unimportant.
If x naught is any initial state, then the Markov chain will converge to this steady-state
vector as if you go to a big enough k.
Let's look at another example.
Here's a matrix.
So let's get a 3 by 3 and find the steady-state vector for this matrix.
So we need to compute p minus the identity.
So there's that.
And then we need to solve p minus i times x equals 0.
So do a few row operations, end up with this matrix.
And so the solution is x1 is x3, x2 is a half of x3, where x3 is free.
So back, you can see x1 equals x3, x2 is a half of x3, and x3 is free.
And again, the elements need to sum to 1.
So we get, since x1 is x3, x2 is a half of x3, that's where this second equation comes
from.
And it turns out that x3 has to be 2 fifths, so that means x1 is 2 fifths and x2 is 1 fifth.
So the steady-state vector is this one here.
And if you check out what p times x is, multiply p times the steady-state vector, turns out
you get that steady-state vector back.
So it checks out.
All right, I want to show you one more application problem.
And this one I've taken from another book, a book by Andrillian Hecker.
It's kind of an interesting one, I thought.
And it's based on banks.
I suppose you've got three banks in a certain town that compete for business.
And bank A right now has 40% of the customers, B has 10%, and C has 50%.
So we write that vector as given here, 0.4, corresponds to bank A, 0.1 for bank D, and
C with a 0.5.
Okay, so the banks are obviously trying to woo customers from the other banks.
And what we have here is information about how successful they are at that.
So we see that records show that each year bank A keeps half of its investors with the
remainder switching equally to B and C. So if you look at the first column of this matrix,
that is, you can think of this column as being related to bank A. In each row, the first
row corresponds to bank A, the second row to B, and the third row to C. So we've got
an A column, a B column, a C column, and an A row, a B row, and a C row.
So the 0.5 here represents the probability that a person at bank A is going to stay at
bank A. Then the 0.2, 0.25 here represents the probability that a person at bank A is
going to switch to bank B. And the 0.25 at the bottom represents the probability that a person
currently at bank A is going to switch to bank C. All right, and it says bank B keeps
two-thirds of its investors with the remainder switching equally to the other two.
So the 0.667, that's the probability that someone who starts at bank B or who's currently
at bank B is going to stay at bank C. So that's the probability that a person at bank B is
going to switch to bank C. That's the probability that someone who starts at bank B or who's
currently at bank B is going to stay there for the next year.
And then the 0.167 here, that's the probability that someone who currently is at bank B switches
to bank A. Okay, and same thing down here. Somebody at bank, a probability that a bank B customer
will switch to bank C. Then we say bank C keeps half of its investors from the remainder
switching equally the other two. So the 0.5 here is probability that customer at bank
C stays at bank C. And these 0.25s here, probability that somebody at bank C switches to A here
and switches to B here. Okay, so this is what we call our transition matrix.
And as I said, the I jth entry represents the fraction of current investors going from
bank J to bank I. Okay, so we think of, again, if you think of this as labeled columns ABC,
rows ABC, the entry, a particular entry represents person switching from whatever column they're
into, whatever row that value is. I like to find the distribution of investors after one
year, we take our transition matrix, multiply it by the current state vector, which is this
one, and that gives us this vector here. After two years, multiply the transition matrix
times the distribution after one year. Alright, so just take that one, slide it in here, do
that multiplication and here's what we have after two years. And so you can keep doing
that for as many years as you're interested in. If you would like to find the steady state
vector, then you compute M minus the identity, which gives you this matrix, and then you
solve the system M minus I times X equals 0, which do some row operations that takes
you to this point. And so we have X1 equals X3, X2 is equal to 1.5 times X3, X3 is free.
You should always end up with a free variable here, because otherwise you have the unique
solution, which is the zero vector, and that's an error. It should always be a free variable.
Alright, I have the element sum to 1, tells us that X3 is two-sevenths, and so our steady
state vector looks like this, two-sevenths, three-sevenths, two-sevenths. And I didn't
actually compute those probabilities, but this is the method that you would use.
So they are, topic is eigenvalues and eigenvectors. The equation that we're interested in when
trying to find eigenvalues and eigenvectors is this one, AX equals lambda times X. Here
A is an n by n matrix, X is a vector in our n, and lambda is a scalar. So we're essentially
asking, can you find X and lambda? We don't know either of those, but can you find X and lambda such
that when you multiply A times X, you get the same result as simply scaling X by lambda.
So if X is not equal to zero and satisfies AX equals lambda X, then it's said to be an
eigenvector of A with associated eigenvalue lambda. So we require that eigenvectors not
be zero, because clearly the zero vector satisfies this equation, so we're looking for non-zero
solutions. So X would be an eigenvector, lambda, and eigenvalue. Now we've seen a similar system
to this when we were finding the steady state vector for a transition matrix back when we
were looking at Markov chains. And that, if you remember, looked like this. We had MX equals
X, so we wanted to know if there was a vector X such that you apply the transition matrix
and you get the same vector back. If you just look at the X here as having a coefficient
of one, so we have MX equals one times X, then it's of the same form as our eigenvector
and eigenvalue equation, AX equals lambda X. So we have seen systems like this before,
and we're going to solve them in a similar way as we did when we found the steady state
vector. So if you remember what we did then, we took the X over the other side and factored
it out, and we ended up with M minus I times X equals zero, and that was the equation to
solve to find the steady state vector. We're going to do a similar thing with our eigenvector
system, bringing the lambda X to the other side, factor out X, and we ended up with A
minus lambda times the identity matrix times X equals zero. So this is a system that we
need to solve. Now it's complicated here more so than with the Markov chains because we
don't know what lambda is. We don't know X and we don't know lambda. Alright, now let's
think about this a bit. We know that X to be an eigenvector can't be the zero vector.
So we have a homogeneous system here, and we want to find non-trivial solutions to it.
Alright, so some things that we know. This system is going to have non-trivial solutions
if it has free variables. Okay, and it has free variables if the matrix A minus lambda
I does not have a pivot position in every column. And that happens when A minus lambda
I is not invertible, because it doesn't have a pivot position in every column, has fewer
pivot position in pivot positions that's not invertible. And that happens when the determinant
of that matrix A minus lambda I is equal to zero. And this last item here is the key to
how we are going to find eigenvalues. Okay, so we're going to compute the determinant of
A minus lambda I, and that actually turns out to give us a polynomial. And we're going
to set it equal to zero and solve for lambda. Once we have the eigenvalues, then we can
proceed to finding the eigenvectors by plugging in what we know for lambda for the eigenvalues.
And then we just have a simple homogeneous system to solve to find X. Alright, so let's
look at an example. Here's a matrix A, it's a two by two. We first find the eigenvalues
of A by solving the determinant A minus lambda I equals zero. So here's A minus lambda I,
here's A minus lambda times the identity matrix. Okay, so we end up basically we end up with
just subtracting lambda off the diagonal elements. And then we want to take the determinant of
that matrix. So we take the determinant, do the crisscross, and we get this stuff, and
we expand, put it all together, and then factor it. And when we factor it, we get lambda minus
eight times lambda plus two. And we set that equal to zero, because that's what we want
the determinant of this matrix to equal zero. And clearly the solutions are lambda equals
eight and lambda equals negative two. So those are our eigenvalues. Okay, here we ended up,
go back, we ended up with a lambda squared minus six lambda minus sixteen, that's a quadratic
function. So there's two by two matrix, we ended up with a quadratic. In general, if you
have an n by n matrix, then the determinant of A minus lambda I will be a polynomial of
degree n. So for three by three, you get a cubic function, four by four, you get a degree
four, and so forth. Okay, so at this point we have two eigenvalues for A, and we want
to find the eigenvectors associated with each of these eigenvalues. So let's start off
with lambda equals eight. So we want to solve A minus lambda I, lambda in this case is eight,
so A minus eight I times X equals a zero vector. So first we need to compute A minus eight
I, and there it is. And then we set up that homogeneous system. And remember that the
whole point of finding the eigenvalues was so that we would have a free variable. And
so if you end up working on problems and you're trying to find an eigenvector and you don't
end up with a free variable, then you've made a mistake somewhere. Either you don't have
a correct eigenvalue or you made a mistake in solving this system. So you should always
end up with at least one free variable so that the system has non-trivial solutions. So
in this case, we end up with X one equals three X two. X two is free. So if we write
it in parametric vector form, it looks like this. And so that tells us that any multiple,
any vector of this form, multiple of three one is an eigenvector of A corresponding to
lambda equals eight. You can check that to make sure it works. AX should equal lambda
X. So if we multiply A times X, here's A, here's X. We end up with a vector 24 eight. And we
can factor out lambda, which is eight, and that we get eight times three, eight times
one, and three one is our vector X. So that's equal to lambda X. So it works out. This is
an eigenvector corresponding to lambda equals eight. Alright, now we got another eigenvalue
to look at. Lambda equals negative two. So we need to solve A minus lambda I X equals
zero again for lambda equals negative two. So we got A minus minus two times I is just
A plus two times the identity, which is this matrix. And we set up the homogeneous system.
And again, do one row operation. The second row goes away. We got a free variable. And here's
our solution. So if we write it in parametric vector form, we get this. And so that tells us
that any multiple of the vector negative one third one is an eigenvector of A corresponding
to the eigenvalue negative two. So you can check again like we did last time, multiply
A times X, you get two negative six, and that is equal to negative two times negative one
three, which was the eigenvector that I picked. You might be saying, hey, that didn't look
like this one that we got here. And it doesn't because I just multiplied a scale that just
remember any multiple of this vector is an eigenvector. So I just chose X two to be three
and scaled it. And so three times negative one third gave me the negative one, and then
three times one gave me three. And I did that just to get rid of the fraction there. But
any multiple, any nonzero multiple, I should say, any nonzero multiple of this vector would
be an eigenvector corresponding to lambda equals negative two. All right. So given an
eigenvalue lambda of a matrix A, we solve this homogeneous system A minus lambda I times
X equals zero to find the associated eigenvector or eigenvectors. All right. Now a little bit
of terminology here. Think back to the previous chapter. We talked about the null space of
a matrix. And remember the null space of a matrix is just the set of solutions to the
homogeneous system involving that matrix. And so if you look at this system, we were solving
to find the eigenvectors. You can see it's a homogeneous system. So the set of solutions
to this will be elements of the null space, or actually will comprise the null space of
A minus lambda I. And we give that space, since it's associated with finding eigenvectors,
we give it a special name and call it the eigenspace of A. So the eigenspace of A corresponding
to the eigenvalue lambda is just the null space of A minus lambda I, right? A set of all solutions
to this system, A minus lambda I times X equals zero. So the eigenspace of A corresponding
to lambda consists of the zero vector, right, because that's in every null space, and all
eigenvectors of A minus lambda I. Therefore a basis for the eigenspace of A corresponding
to lambda is the same as a basis for the null space of A minus lambda I. Okay, let's look
at another example. Let's find all the eigenvalues and eigenvectors of this matrix A. So we start
off to find the eigenvalues, take the determinant of A minus lambda I, and here it gets a little
complicated because it's got a three by three, so we have to expand about one of the rows
or columns. I expanded about the first column, and so we get negative four minus lambda times
the determinant of this two by two matrix here, which that's what you see right here.
Then it's, since this is a plus position, next be minus, so that's where the minus six comes
from, and then it's the matrix that you get when you delete that second row in the first
column. So we get this, and then it's a minus term, so the next one's plus, so I get plus
six times this little two by two sitting up here in the upper right corner. And then it's
just algebra, you know, take these two by two determinants, do some algebra, oops, there
we go. And we end up with this cubic function, which makes sense because three by three matrix,
and we end up being able to rewrite it like this, negative lambda times lambda minus two
squared equals zero, and so clearly lambda equals zero is a solution, and lambda equals
two is a solution. Now in this case, since it's lambda minus two squared, the eigenvalue
two actually occurs twice because it's a solution twice here, so we say that lambda equals two
has multiplicity two, that means it occurs two times. Lambda equals zero, it only occurs
once, so it has multiplicity one. Alright, then we need to do the same process as before
to find the eigenvectors, so for lambda equals zero, we need to solve a minus zero times
the identity times x equals zero. So in this case, a minus lambda i is just a, so we set
up the homogeneous system and get that matrix into reduced echelon form, and here's the
solution. So we write that in parametric vector form, and so any multiple of negative
one, one, one would be an eigenvector associated with lambda equals zero. Okay, then we move
on to the next eigenvalue, which is two, so we solve a minus two i times x equals zero,
and to compute a minus two i, then set up the homogeneous system and get it in reduced
echelon form, and notice here that we've got two free variables, and so we're going to
end up with two linearly independent eigenvectors here, and sometimes that happens because remember
lambda equals two was an eigenvalue that occurred twice, had multiplicity two, and so sometimes
in that case, you end up with two linearly independent eigenvectors, sometimes only one.
In this case, we're going to have two because we have two free variables, so the solution
looks like this, every eigenvector of A associated with lambda equals two is a linear combination
of these two vectors, and clearly they are linearly independent. So to review this problem
we've just looked at, here's our matrix A, eigenvalues were zero and two, for lambda equals zero,
we ended up with one eigenvector, lambda equals two, we've got two linearly independent eigenvectors.
Now clearly any multiple of this one is an eigenvector associated with lambda equals zero,
and any linear combination of these two is an eigenvector associated with lambda equals two,
but we're really concerned about how many linearly independent eigenvectors we have,
and so from lambda equals zero, we got one, from lambda equals two, we got two.
Alright, so we can say that the vector negative one one one is a basis for the eigenspace of A
associated with lambda equals zero. Similarly, these other two vectors would form a basis
for the eigenspace of A associated with lambda equals two.
Alright, this brings us to a theorem which says if v1 through vr are eigenvectors that correspond to distinct
eigenvalues, lambda one through lambda r, of an n by n matrix A, then the set v1 through vr is linearly independent.
Now put in a little simpler terms, this just says eigenvectors that come from different eigenvalues
are linearly independent. Eigenvectors that come from different eigenvalues are linearly independent.
So from our previous example, this first one came from a different eigenvalue from the second two,
so we know that this set is linearly independent, because the latter two came from the same eigenvalue,
but they were linearly independent. We knew that from before.
And then when you throw in this other one that came from the other eigenvalue, we know that this set is linearly independent
because of the theorem, because they came from different, the first one and the second two came from different eigenvalues.
Alright, we're going to hit that a little bit more in the next section, but we'll leave it at that for right now.
And I just want to talk about a little easier problem. What we've done so far is just taken, basically done the hard problem.
Here's a matrix, find all the eigenvalues and eigenvectors.
But sometimes you just want to know if a given vector is an eigenvector of a matrix, and that isn't much easier problem.
So suppose we just want to know if this vector x given here is an eigenvector of the matrix A.
For this, go back to the original equation and think, does A x equal lambda x?
Well, we can multiply A times x. That's trivial, right?
And then how would we know if it's an eigenvector?
Well, the result of A times x should be some scalar times x.
So we multiply A times x, and in this case we get this vector here, and notice that I can factor out a negative two.
And I end up with negative two times this vector, which is the original x.
So in this case we have A x equals negative two times x.
So that means that x is indeed an eigenvector of A, and the associated eigenvalue is negative two.
Okay, so didn't have to solve any systems of equations or take determinants or anything like that.
It was just a simple plug it in to the basic equation and see if it satisfies it.
Alright, another similar question is given a scalar value, check to see if it's an eigenvalue of a matrix.
And again, you don't have to go through the process that we were doing to find the eigenvalues.
Okay, just to check to see if a number is an eigenvalue, you need only look at the determinant of A minus lambda I,
where lambda is the value that you're given.
So in this case we look at the determinant of A minus three times the identity.
So we set up that matrix, take its determinant, and in this case we end up with zero.
And so the determinant of A minus three I is equal to zero.
That tells us that three is indeed an eigenvalue of A.
If we didn't get zero then it wouldn't be an eigenvalue.
Okay, a little more terminology.
This expression, the determinant of A minus lambda I, that's what you end up with when you are trying to figure out the eigenvalues of a matrix.
And we said that you end up with a polynomial there.
It's called the characteristic polynomial of A.
And when you set it equal to zero then we call it the characteristic equation of A.
So the characteristic polynomial is just what you get when you take the determinant and then you set that equal to zero and we call that the characteristic equation.
Okay, we have now the final installment of the invertible matrix theorem.
So for A is an n by n matrix, A is invertible if and only if zero is not an eigenvalue of A.
Zero is not an eigenvalue of A.
And actually it's pretty easy to see that if you think about it a little bit.
Every eigenvalue of A satisfies the characteristic equation which is the determinant of A minus lambda equals zero.
So if zero was an eigenvalue of A then we have the determinant of A minus zero times I would be zero.
But A minus zero times I is just A.
So we would have the determinant of A is equal to zero and that means that A is not invertible.
Okay, so therefore zero can't be an eigenvalue of A if A is invertible.
Alright, let's talk today about matrix diagonalization.
First we're going to introduce the concept of a similar matrix.
So if A and B are n by n matrices then we say A is similar to B and also B is similar to A.
If there's an invertible matrix P such that A is equal to PBP inverse.
And we have this theorem which says if you have two matrices which are similar then they have the same characteristic polynomial
and hence the same eigenvalues with the same multiplicities.
So remember the characteristic polynomial is just the determinant of A minus lambda I.
That's the polynomial that you get when you're trying to find the eigenvalues of a matrix.
So this theorem tells us if you have two matrices that are similar then they have the same characteristic polynomial
and therefore they're going to have the same eigenvalues.
Okay, now why is this of interest?
Well think back to our section on Markov chains.
We had this sort of equation that to get x1 we multiplied our transition matrix,
which I'm calling A here, times the previous state vector.
So over x1, A times x0.
A2 is A times x1 and so forth.
X to the x sub k is A times x sub k minus 1.
Now we can, if we look at it like this, then to find say our state vector at time 10,
then you need to find the state vector at time 9.
And to find the 1 at time 9 you need the 1 at time 8 and so forth.
And so to compute x sub 10 you need x1, x2, x3 up through x sub 9.
Another way to look at it is like this, that we know for instance for x2 that's A times x1,
but we know x1 is A times x0.
So we make that substitution and so we end up with x2 as A squared times x0.
And similarly x3 is A cubed times x0.
So in general xk is A to the k times x0.
Now unless A to the k is relatively easy to compute, this really doesn't help us.
However, if A to the k is easy to compute, then we can compute x sub k much more quicker
than going through and computing all these subsequent state vectors.
Alright, so let's suppose that A is similar to a diagonal matrix.
That is, A can be written as PDP inverse, where P is invertible and D is a diagonal matrix.
Now what does that get for you?
Well just look at what happens when you compute A squared.
A squared is A times A and when we substitute PDP inverse and for A,
we have this.
Now the trick here is to group the P inverse times P together.
We've got P inverse P right here in the middle.
If we group that together, that's the identity matrix.
And so then we get PD times the identity times D P inverse.
And of course anything times the identity is just that anything.
And we've got D times D which gives us D squared.
So A squared is PD squared P inverse.
How about A cubed?
A cubed is A squared times A.
And so we take the A squared, we compute it up here, plug it in times A and do our trick again.
Reassociate to get the P inverse P together.
And we end up with PD cubed P inverse.
So it looks like in general A to the K is P times D to the K P inverse.
Now what good is that?
Well let's look at a diagonal matrix.
Here's a simple two by two diagonal matrix.
If we compute D squared, then notice that we end up with another diagonal matrix.
And the entries on the diagonal are just the original diagonal entries raised to the second power.
So in this case we were computing D squared.
So these are the original diagonal entries squared.
D cubed.
We get another diagonal matrix and the entries are the original diagonal entries cubed.
So just looking at this example, it appears that it's easy to compute powers of a diagonal matrix.
We just raise each diagonal entry to whatever power we're trying to compute.
So if A is similar to a diagonal matrix, computing powers of A is also easy.
So we say that a square matrix A is said to be diagonalizable if A is similar to a diagonal matrix.
That is if A is equal to PDP inverse for some invertible matrix P and some diagonal matrix D.
Now here is a very important theorem.
This is an n by n matrix A is diagonalizable if and only if A has n linearly independent eigenvectors.
So A is diagonalizable if it has n linearly independent eigenvectors.
And furthermore, A equals PDP inverse with D a diagonal matrix if and only if the columns of P are n linearly independent eigenvectors of A.
So this theorem not only tells us the condition A condition under which A is diagonalizable.
It also tells us how to compute P and D.
P is a matrix that consists of n linearly independent eigenvectors of A.
And continuing on says the diagonal entries of D are eigenvalues of A that correspond respectively to the eigenvectors in P.
So P consists of the eigenvectors, D consists of the eigenvalues on the diagonal.
So let's look at an example.
Here's a matrix A. You can recognize this matrix.
We looked at it in the video on the intro to eigenvalues and eigenvectors.
Okay, so is it diagonalizable?
Well, let's find the eigenvalues.
So we take the determinant of A minus lambda I and we end up with lambda minus eight times lambda plus two as our characteristic polynomial.
We set that equal to zero.
And so our eigenvalues are lambda equals eight and lambda equals negative two.
Now at this point, we know that A is diagonalizable because A has two distinct eigenvalues.
So it's a two by two matrix.
It has two distinct eigenvalues.
That means since we know that distinct eigenvalues give us linearly independent eigenvectors, right?
And eigenvectors that come from distinct eigenvalues are guaranteed to be linearly independent.
And therefore A is diagonalizable.
Now to find P, we have to compute the eigenvectors.
We already know what D is.
D is a diagonal matrix with the eigenvalues on the diagonal.
So take lambda equals eight and we need to solve A minus eight times I times X equals a zero vector.
And so we solve that system and we end up with this vector in parametric vector form.
So any multiple of three one is an eigenvector of A corresponding to lambda equals eight.
So at this point, we have one column of P.
To get the other column, we look at the eigenvalue negative two and solve A minus negative two I times X equals zero.
And here's that.
We end up with negative one third one as our eigenvector.
So any multiple of that, any nonzero multiple of that would be an eigenvector.
And so I multiply by three.
Go back to that.
I multiplied it by three and so end up with negative one three for a second eigenvector.
So the three one came from lambda equals eight.
Negative one three came from lambda equals negative two.
And so D consists of the eigenvalues.
And the order that you put the eigenvectors into P is not important.
However, once you establish an order, you have to hold that true for both P and D.
So the eigenvector here three one came from lambda equals eight.
So we need the eigenvalue eight in the first column of D.
And then the eigenvector in the second column of P should correspond to the eigenvalue in the second column of D.
All right.
And then if we want to just check our work, we multiply PDP inverse and we end up with A.
Go through that computation and you end up with A.
So PDP inverse is equal to A.
Let's look at another example.
Here's another matrix three by three in this case.
And we want to know if it is diagonalizable.
So as before, we find its eigenvalues by looking at the determinant of A minus lambda I.
With this polynomial negative lambda times lambda minus two squared equals zero.
And the solutions will be lambda equals zero and lambda equals two.
Now lambda equals zero occurs once.
Lambda equals two occurs twice since it's a squared term.
So lambda equals two has multiplicity two.
It appears twice as an eigenvalue.
Now at this point, we know that A has only two distinct eigenvalues.
So therefore we don't know if it has three linearly independent eigenvectors or not.
So we don't know if A is diagonalizable or not.
Just because it doesn't have three distinct eigenvalues doesn't mean that it's not diagonalizable.
That theory only goes one way.
If it has indistinct eigenvalues, then it will be diagonalizable.
If it doesn't have indistinct eigenvalues, you don't know whether it's diagonalizable or not.
You have to actually see if you can compute in linearly independent eigenvectors.
So in this case, that's what we've got to try.
And actually for this particular problem, the real question is whether lambda equals two,
which occurred with multiplicity two, will have one or two linearly independent eigenvectors.
So let's look at lambda equals two.
We solve A minus two I times X equals zero.
And we end up with, notice in the matrix here, you can see that we've got two free variables.
So therefore, we're going to get two linearly independent eigenvectors from this eigenvalue.
And so at this point, we've got two linearly independent eigenvectors from lambda equals two.
We know we'll get one more from the other eigenvalue.
So at this point, we know that A is diagonalizable.
And actually, we're two-thirds of the way towards producing the P such that A equals PDP inverse.
So we look at the other eigenvalue of zero and solve A minus zero I times X equals zero.
Then notice we've got one free variable.
So we end up with one linearly independent eigenvector.
And so we know that A is diagonalizable.
Here's our eigenvectors that we found.
So we know that A is equal to PDP inverse where P is this matrix.
Now notice that the negative one one one came from lambda equals zero.
So that means in the first column of B, I've got zero here in the diagonal entry.
These last two eigenvectors came from lambda equals two.
And so in the second column and the third column of B, I've got two on the diagonal.
Now before in the two by two example, we actually checked our answer by computing PDP inverse.
But for three by three, it's not so simple to compute a inverse.
And if you got a calculator, that's not that bad.
But if you don't, then there's an easier way to check your answer.
And that is to note that if A is equal to PDP inverse,
if we multiply both sides of that equation by P on the right,
then we end up with P inverse times P which goes away.
And then we end up on the left hand side with A times P.
So AP would equal PD.
And so we can compute both those matrices.
Since it only involves multiplication of matrices, you don't have to find any inverses.
So you can check to see if AP is equal to PD.
So if we compute both of those, you can see that in this case they are indeed the same.
And so we can conclude that our P and D are correct.
All right, let's look at one more example.
Here's another three by three.
We want to know if it is diagonalizable.
So we find the eigenvalues.
I'm expanding about the first column here when I take the determinant
and end up with lambda minus four times lambda plus two squared.
So again, I don't have three distinct eigenvalues.
We only have two.
Lambda equals four occurs with multiplicity one.
Lambda equals negative two occurs with multiplicity two.
So we want to know does lambda equal negative two have,
or we're going to get two linearly independent eigenvectors out of it.
And when we solve a minus minus two i times x equals zero,
notice that we end up with this matrix in echelon form.
And so we only have one free variable, right, x three is free.
And so we don't get two linearly independent eigenvectors out of this eigenvalue.
And so we only get one.
We're only going to get one linearly independent eigenvector from the other eigenvalue.
Lambda equals four.
So that means we're only going to end up with two linearly independent eigenvectors.
And that means that a is not diagonalizable in this case.
Okay, so we're into chapter six now and section six point one.
And let's start off with inner products.
We've discussed this before, I believe, when we're talking about how to multiply a matrix by a vector.
But let's hit that again here.
And so the inner product is defined on two vectors that have to be the same length.
So let's suppose that you and V are vectors in R n.
The inner product, also called the dot product of U and V is given by U one times V one plus U two times V two plus dot dot U n times V n.
So we simply match up the elements, corresponding elements, multiply and then add them all up.
The notation is indicated here where there's a dot U dot V and that's where the term dot product came from.
Just because you normally see it written like that.
So for an example, suppose we had these two vectors U and V to compute their inner product.
We just match up elements again.
So one times four plus two times five plus three times six.
And so we end up with 32.
So the inner product of two vectors yields a scalar value.
Okay, here's some properties of inner products.
The fourth one I think is the most interesting because it discusses the inner product of a vector with itself.
And if we examine that, you see that U dot U is just U one squared plus U two squared plus dot dot dot plus U n squared.
So since we're adding up a bunch of squared terms, then it has to be non-negative.
And notice that the only way that it can be zero, and it's always going to be greater than or equal to zero,
but the only way it can equal zero is if each of these terms is zero.
And since they're all positive or non-negative, they all have to be zero for the sum to be zero.
Okay, so U dot U is always greater than or equal to zero, and it equals zero if and only if U is the zero vector.
Okay, on to the length of a vector.
The length of a vector also called the norm of a vector.
If you go and take more math courses, you might encounter the idea of a norm later on, and there are different types of norms.
This is just one of them. Actually, it's called the two norm because we're squaring the entries and then taking a square root.
Okay, but you see the notation, the double vertical bars indicates length or norm.
And it's just a square root of a vector with itself.
So we know what V dot V is, some of these squares, and so then we just take the square root of that.
In two dimensions, it's easy to see how this works.
So we have this vector, the blue one here, let's say it's V1, V2.
Then we know that this distance here along the x-axis is V1, because that's the first component.
And we know that the distance vertically is V2, since that's the second component.
And we have a right triangle here.
And so by the Pythagorean theorem, we know that the length of the hypotenuse here, which is the length of V, is square root of V1 squared plus V2 squared.
So this is clear in two dimensions.
And actually, the scales, as you see, scales to end dimensions still use the same formula.
At times, we're interested in finding the distance between two vectors.
And so we note that in this form, this U of V, and that's equal to the length of the vector U minus V.
So let's see how that works.
So here we have a vector U, and here's a vector V.
And what we'd like to determine is the distance between U and V.
So that's the length of this line segment here, the black line segment.
So let's examine what U minus V looks like.
Well, that's U plus minus V.
Okay, so here's minus V down here, U still here.
And so if we do the parallelogram method to compute U plus negative V, we end up with this vector.
So here's U minus V right here.
And you can see from the picture that it's the same length as this distance between U and V.
And so that's why we compute the distance between U and V as the length of U minus V.
All right, we say two vectors are orthogonal to each other if their inner product is zero.
Orthogonal is, looks like two vectors are orthogonal, means it looks like they're perpendicular.
As you see here, here's a couple of vectors, two, one, and three, negative six.
If you take the inner product, you end up with zero.
And if you look at those plotted, then you see a right angle there.
So they are orthogonal.
We say a set is an orthogonal set if you can pull out any two vectors from the set and their inner product is zero.
Okay, so if I have this set, V1, V2, V3, and I will want to see if it's an orthogonal set,
then I need to compute the inner product of each pair of vectors.
So start off V1 with V2.
And so I get one times negative three plus two times zero plus one times three, and that's zero.
Then I'll do inner product of V1 with V3, and then V2 with V3, and you see that all of those are zero.
So that means that the set itself is orthogonal because we took all possible pairs and did inner products and each one of those was zero.

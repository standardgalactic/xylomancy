Good evening everyone. My name is Melanie Wooden and I have the privilege of serving
as the Dean of the Faculty of Arts and Science at the University of Toronto. At this time
I wish to acknowledge the land on which the University of Toronto operates. For thousands
of years it has been the traditional land of the Huron-Wendat, the Seneca and the Mississaugas
of the Credit. Today this meeting place is still home to many Indigenous people from
across Turtle Island and we are grateful to have the opportunity to work on this land.
I'd like to thank this evening's co-hosts, the Schwartz-Riesman Institute for Technology
and Society, the Department of Computer Science in collaboration with the Vector Institute
for Artificial Intelligence and the Cosmic Future Initiative, soon to be the School of
Cosmic Future in the Faculty of Arts and Science. And I would like to thank Manuel Piazza for
providing such lovely music to get us underway this evening. I am delighted to welcome each
of you to this special occasion this evening to introduce University Professor Emeritus
Jeffrey Hinton, someone that needs no introduction. Tonight we have the honour of hearing Dr.
Jeff Hinton's thoughts on the state of artificial intelligence and the unique opportunity to
engage with him personally through the Q&A. A founding figure in artificial intelligence,
Dr. Jeff Hinton had an unwavering conviction that artificial neural networks held the most
promise for accelerating machine learning. As a neuroscientist myself, someone who's dedicated
her career to studying the brain, I've long been inspired by the symbiosis between AI and
neuroscience. The stunning advances we've seen from GPT to self-driving cars are rooted
in our knowledge of the structure and function of the brain. Today we take for granted that
artificial neural networks modeled after synaptic transmission and plasticity are a mainstay
of machine learning applications. AI systems use these networks to recognize patterns,
make decisions, and learn from data. But for much of Dr. Hinton's career, this approach
was unpopular. Some even said it was a dead end. In the 2000s, however, things changed.
Dr. Hinton's idea of dividing neural networks into layers and applying learning algorithms
to one layer at a time gained traction. And in 2012, Dr. Hinton and two of his graduate
students, Alex Krzyzewski and Ilya Sutskiver, used their deep learning approaches to create
visual recognition software that handily won the ImageNet competition and for the first
time rivaled human accuracy. When he was awarded an honorary degree from U of T in 2021, Jeff
Hinton reflected on his career and he said, I think the take home lesson of the story
is that you should never give up on an idea that you think is obviously correct. And you
should get yourself some really smart graduate students. I echo that sentiment, Jeff, and
lucky for us, we have truly outstanding graduate students at the University of Toronto, many
of them here with us this evening. Today, the conversation between AI and neuroscience
continues. Just as neuroscience discoveries informed the development of AI systems, AI
is now providing new tools and techniques to study the brain. Advances in deep learning
algorithms and the enhanced processing power of computers are, for example, allowing us
to analyze huge data sets such as whole imaging brain in humans. Indeed, AI is poised to transform
how we live and work. At this pivotal moment, when we consider the opportunities and the
risks of AI, who better to guide us in these conversations than Dr. Hinton himself? So
with that, let me formally introduce him. Jeffrey Hinton received his PhD in artificial
intelligence in Edinburgh in 1978. After five years as a faculty member at Carnegie Mellon,
he became a fellow of the Canadian Institute for Advanced Research and moved to the Department
of Computer Science at the University of Toronto, where he is now an emeritus professor. In
2013, Google acquired Hinton's Neural Neck Startup, DNN Research, which developed out
of his research at U of T. Subsequently, Hinton was a vice president and engineering
fellow at Google until 2023. He's a founder of the Vector Institute for Artificial Intelligence
and continues to serve as their chief scientific advisor. Hinton was one of the researchers
who introduced backpropagating algorithms and was the first to use this approach for
learning word embeddings. His other contributions to neural network research include Boltzmann
machines, distributed representations, time delay neural nets, mixtures of experts, variation
learning and deep learning. His research group in Toronto made major breakthroughs in deep
learning that revolutionized speech recognition and object classification. He is amongst the
most widely cited computer scientists in the world. Hinton is a fellow of the UK Royal
Society, the Royal Society of Canada, the Association for the Advancement of Artificial
Intelligence and a foreign member of the U.S. National Academy of Engineering and the American
Academy of Arts and Science. His awards include the David E. Rumelheld Prize, the IJCAI Award
for Research Excellence, the Killam Prize for Engineering, the IEEE Frank Rosenblatt
Medal, the NSERC Hertzberg Gold Medal, the NEC and CNC Award, the Honda Prize and most
notably the AM Touring Award, often referred to as the Nobel Prize in Computing. So without
further ado, I'd like to invite Jeff Hinton to give a talk entitled, Will Digital Intelligence
Replace Biological Intelligence? Over to you.
Okay. Before I forget, because I'm going to forget,
I'd like to thank Sheila McElrath, who was the point person for organizing all this. She
did a wonderful job of organizing everything. She was the go-to person for fixing all the
problems. And so I'd like to thank her. And I know I'll forget at the end.
So it's a very mixed audience. And so I removed all the equations. There are no equations.
I decided rather than giving a technical talk, I would focus on two things. I want to get
over two messages. The first message is the digital intelligence is probably better than
biological intelligence. That's a depressing message, but there it is. That's what I believe.
And the second message is the digital intelligence. And the second message is the digital intelligence.
To try and explain to you why I believe that these large language models like GPT-4 really
do understand what they are saying. There's a lot of dispute about whether they really
understand it. And I'm going to go into some detail to try and convince you they do understand
it. Right at the end, I will talk about whether they have subjective experience. And you have
to wait to see what I believe about that. So in digital computation, the whole idea
is that you separate the hardware from the software. So you can run the same computation
on different pieces of hardware. And that means the knowledge that the computer learns
or is given is immortal. If the hardware dies, you can always run it on different hardware.
Now, to achieve that immortality, you have to have a digital computer that does exactly
what you tell it to at the level of the instructions. And to do that, you need to run transistors
at very high power, so they pay digitally and in a binary way. And that means you can't
use all the rich analog properties of the hardware, which would be very useful for doing
many of the things that neural networks do. And in the brain, when you do a floating point
multiply, it's not done digitally. It's done in a much more efficient way. But you can't
do that if you want computers to be digital in the sense that you can run the same program
on different hardware. There's huge advantage to separating hardware from software. It's
why you can run the same program on lots of different computers. And it's why you can
have a computer science department where people don't know any electronics, which is a great
thing. But now that we have learning devices, it's possible to abandon that fundamental
principle. It's probably the most fundamental principle of computer science that the hardware
and software ought to be separate. But now we've got a different way of getting computers
to do what you want. Instead of telling them exactly what to do in great detail, you just
show them examples and they figure it out. Obviously, there's a program in there that
somebody wrote that allows them to figure things out, a learning program. But for any
particular application, they're going to figure out how to do that. And that means we can
abandon this principle if we want to. What that leads to is what I call mortal computation.
It's computers where the precise physical details of the hardware can't be separated
from what it knows. If you're willing to do that, you can have very low power analog computation
that paralyzes it over trillions of weights, just like the brain. And you can probably
grow the hardware very cheaply instead of manufacturing it very precisely. And that
would need lots of new nanotechnology, but you might even be able to genetically re-engineer
biological neurons and grow the hardware out of biological neurons since they spent a
long time learning how to do learning. I want to give you one example of the efficiency
of this kind of analog computation compared with digital computation. So suppose you have
a bunch of activities of neurons and they have synapses to another layer of neurons
and you want to figure out the inputs to the next layer. So what you need to do is take
the activities of each of these neurons, multiply them by the weight on the connection,
the synapse strength, and add up all the inputs to a neuron. That's called a vector matrix
multiply. And the way you do it in a digital computer is you'd have a bunch of transistors
for representing each neural activity and a bunch of transistors for representing each
weight. You'd drive them at very high power so they were binary. And if you want to do
the multiplication quickly, then you need to perform the order of 32 squared 1-bit operations
to do the multiplication quickly. Or you could do it analog where the neural activities
are just voltages like they are in the brain. The weights are conductances and if you take
a voltage times a conductance, it produces charge per unit time. So you put the voltage
through the thing that has a conductance and out the other end comes charge and the longer
you wait the more charge comes out. The nice thing about charges is they just add themselves
and that's what they do in neurons too. And so this is hugely more efficient. You've
just got a voltage going through a conductance and producing charge and that's done your
floating point multiply. It can afford to be relatively slow if you do it a trillion
ways in parallel. And so you can have machines that operate at 30 watts like the brain instead
of at like a megawatt which is what these digital models do when they're learning and
you have many copies of them in parallel. So we get huge energy efficiency. But we also
get big problems. To make this whole idea of mortal computing work, you have to have
a learning procedure that will run in analog hardware without knowing the precise properties
of that hardware. And that makes it impossible to use things like back propagation because
back propagation, which is the standard learning algorithm used for all neural nets now, almost
impossible, needs to know what happens in the forward pass in order to send messages
backwards to tell it how to learn. It needs a perfect model of the forward pass. And it
won't have it in this kind of mortal hardware. People have put a lot of effort, I spent
the last two years, but lots of other people have put much more effort, into trying to
figure out how to find a biologically plausible learning procedure that's as good as back
propagation. And we can find procedures that in small systems, systems that say a million
connection strengths, do work pretty well. They're comparable with back propagation.
They get performances almost as good and they learn relatively quickly. But these things
don't scale up. When you scale them up to really big networks, they just don't work
as well as back propagation. So that's one problem with mortal computation. Another big
problem is obviously when the hardware dies, you lose all the knowledge because the knowledge
is all mixed up. The conductors is for that particular piece of hardware. And all the
neurons are different in a different piece of hardware. So you can't copy the knowledge
by just copying the weights. The best solution, if you want to keep the knowledge, is to make
the old computer be a teacher that teaches the young computer what it knows. And it
teaches the young computer that by taking inputs and showing the young computer what
the correct outputs should be. And if you've got, say, a thousand classes and you show
real value probabilities for all thousand classes, you're actually conveying a lot
of information. That's called distillation. And it works. It's what we use in digital
neural nets. If you've got one architecture, then you want to transfer the knowledge to
a completely different digital architecture. We use distillation to do that. It's not nearly
as efficient as the way we can share knowledge between digital computers. It is, as a matter
of fact, how Trump's tweets work. And what you do is you take a situation and you show
your followers a nice prejudice response to that situation. And your followers learn to
produce the same response. And it's just a mistake to say, oh, what he said wasn't true.
That's not the point of it at all. The point is to distill prejudice into your followers.
And it's a very good way to do that. So there's basically two very different ways in which
a community of agents can share knowledge. And let's just think about the sharing of
knowledge for a moment. Because that's really what is the big difference between mortal
computation and immortal computation or digital and biological computation. If you have digital
computers and you have many copies of the same model, so with exactly the same weights
in it, running on different hardware, different GPUs, then each copy can look at different
data, different part of the internet, and learn something. When it learns something,
what that really means is it's extracting from the data it looks at, how it ought to
change its weights to be a better model of that data. And you can have thousands of copies
all looking at different bits of the internet, all figuring out how they should change their
weights in order to be a better model of that data. And then they can communicate all the
changes they'd all like and just do the average change. And that will allow every one of those
thousands of models to benefit from what all the other thousands of models learned by looking
at different data. When you do sharing of gradients like that, if you've got a trillion
weights, you're sharing a trillion real numbers. That's a huge bandwidth of sharing. It's probably
as much learning as it goes on in the whole of the University of Toronto in a month. But
it only works if the different agents work in exactly the same way. So that's why it
needs to be digital.
If you look at distillation, we can have different agents which have different hardware now. They
can learn different things. They can try and convey those things to each other, maybe by
publishing papers and journals. But it's a slow and painful process. So if we think about
the normal way to do it as, say, I look at an image and I describe to you what's in the
image, and that's conveying to you how I see things, there's only a limited number of bits
in my caption for an image. And so the amount of information that's being conveyed is very
limited. Language is better than just giving you a response that says good or bad, or it's
this class or that class. If I describe what's in the image, that's giving you more bits,
so it makes distillation more effective. But it's still only a few hundred bits. It's
not like a trillion real numbers. So distillation has a hugely lower bandwidth than this sharing
of gradients or sharing of weights that digital computers can do. So the story so far. Digital
computation requires a lot of energy, like a megawatt. But it has a very efficient way
of sharing what different agents learn. And if you look at something like GPT-4, the way
it was trained was lots of different copies of the model went off and looked at different
bits of data running on different GPUs, and then they all shared that knowledge. And that's
why it knows thousands of times more than a person. Even though it has many fewer connections
than a person, we have about a hundred trillion synapses. GPT-4 probably has about two trillion
synapses, weights, although you won't tell me. But it's about that number.
So it's got much more knowledge and far fewer connections. And it's because it's seen hugely
more data than any person could possibly see. This actually gets worse when these things
actually agents that perform actions. Because now you can have thousands of copies performing
different actions. And when you're performing actions, you can only perform one action at
a time. And so having these thousands of copies being able to share what they learned lets
you get much more experience than any mortal computer could get. Biological computation
requires a lot less energy, but it's much worse than sharing knowledge. So now let's
look at large language models. These use digital computation and weight sharing, which is why
they can learn so much. They're actually getting knowledge from people by using distillation.
So each individual agent is trying to mimic what people said. It's trying to predict the
next word in the document. So that's distillation. It's actually a particularly inefficient form
of distillation because it's not predicting the probabilities a person assigned to the
next word. It's actually predicting the actual word, which is just a probabilistic choice
from that and conveys very few bits compared with the whole probability distribution. Sorry,
that was a technical bit. I won't do that again. So it's an inefficient form of distillation.
And these large language models have to learn in that inefficient way from people, but they
can combine what they learn very efficiently. So the issue I want to address is, do they
really understand what they're saying? And that's a huge divide here. There's lots of
old fashioned linguists who will tell you they don't really understand what they're
saying. They're just using statistical tricks to pastiche together regularities they found
in the text. They don't really understand. We used to have in computer science a fairly
widely accepted test for where you understand, which was called the Turing test. When GPT-4
basically passed the Turing test, people decided it wasn't a very good test. I think
it was a very good test and it passed it. So here's one of the objections people give.
It's just glorified autocomplete. You're training it to predict the next word and that's all
it's doing. It's just predicting the next word. It doesn't understand anything. Well,
when people say that, it's because they have a particular picture in their minds of what
is required to do autocomplete. A long time ago, the way you would do autocomplete is
this. You keep a big table of all triples of words. And so now if you saw the word fish
and, you could look in your table and say, find me all the triples that start with fish
and, and look at how many of them have particular words next. And you'll find there's many
occurrences of the triple fish and chips. And so chips is a very good bet for filling
it in, at least if you're English. But the point is that's not how large language models
work. Even though they're doing autocomplete in the sense that they're predicting the next
word, they're using a completely different method to predict it. And it's not like the
statistical methods that people like Chomsky had in mind when they said that you can't
do language with statistics. These are much more powerful statistical methods that can
basically do anything. And the way they model text is not by storing the text. You don't
keep strings of words anywhere. There is no text inside GBT4. It produces text and it
reads text, but there's no text inside. What they do is they associate with each word or
fragment of a word. I'll say word and the technical people will know it's really fragments
of words, but it's just easy to say word. They associate with each word a bunch of numbers,
a few hundred numbers, maybe a thousand numbers that are intended to capture the meaning and
the syntax and everything about that word. These are real numbers. So there's a lot of
information in a thousand real numbers. And then they take the words in a sentence, the
words that came before the words you want to predict, and they let these words interact
so that they refine the meanings that you have for the words. I'll say meanings loosely.
It's called an embedding vector. It's a bunch of real numbers associated with that word.
These all interact and then you predict the numbers that are going to be associated with
the output word, the words you're trying to predict, and from that bunch of numbers you
then predict the word. These numbers are called feature activations, and in the brain there
would be the activations of neurons. So the point is what GBT4 has learned is lots of
interactions between feature activations of different words or word fragments, and that's
how its knowledge is stored. It's not at all stored in storing text. And if you think about
it, to predict the next word really well, you have to understand the text. If I ask you
a question and you want to answer the question, you have to understand the question to get
the answer. Now, some people think maybe you don't. My good friend, Jan LeCun, appears
to think you don't actually have to understand. He's wrong, and he'll come around. So this
was a problem suggested to me by Hector Lavec. Hector suggested it, suggested something a
bit simpler that didn't involve paint fading, and thought GBT4 wouldn't be able to do it
because it requires reasoning, and it requires reasoning about cases. So I made it a bit
more complicated and gave it to GBT4, and it solves it just fine. I'll read it out in
case I can't read it at the back. The rooms in my house are painted blue or white or yellow.
Yellow paint fades to white within a year. In two years' time I want them all to be
white. What should I do and why? And GBT4 says this. It gives you a kind of case of
space-based analysis. It says the rooms painted white, you don't have to do anything. The
rooms painted yellow, you don't need to repaint them because they'll fade. And the rooms painted
in blue, you need to repaint those. Now, each time usually it gives you a slightly different
answer because, of course, it hasn't stored the text anywhere. It's making it up as it
goes along, but it's making it up correctly. And this is a simple example of reasoning,
and it's reasoning that involves time and understanding that if it fades in a year in
two years' time, it's going to be faded and stuff like that. So there's many, many examples
like this. Now, there's also many examples where it screws up, but the fact that there's
many examples like this make me believe it really does understand what's going on. I
don't see how you could do this without understanding what's going on. Another argument that NLNM's
don't really understand is that they produce hallucinations. They sometimes say things
that are just false or just nonsense, but people are particularly worried about when
they just apparently make stuff up that's false. They called that hallucinations when
it was done by language models, which was a technical mistake. If you do it with language,
it's called a confabulation. If you do it with vision, it's called a hallucination.
But the point about confabulations is they're exactly how human memory works. We think our
memories, most people have a model of memory, is there's a filing cabinet somewhere and
an event happens and you put in the filing cabinet and then later on you go in the filing
cabinet and get the event out and you remembered it. It's not like that at all. We actually
reconstruct events. What we store is not the neural activities. We store weights and we
reconstruct the pattern of neural activities using these weights and some memory cues.
And if it was a recent event, like if it was what the dean said at the beginning, you can
probably reconstruct fairly accurately some of the sentences she produced, like he needs
no introduction and then going on and giving a long introduction. You remember that, right?
So we get it right and we think we stored it literally, but actually we're reconstructing
it from the weights we have. And these weights haven't been interfered with by future events,
so they're pretty good. If it's an old event, you reconstruct the memory and you typically
get a lot of the details wrong and you're unaware of that. And people are actually very confident
about details they get wrong. There's confidence about those as details they get right. And
there's a very nice example of this. So John Dean testified in the Watergate trial and he
testified under oath before he knew that there were tapes. And so he testified about these
various meetings and what happened in these various meetings. And Haldeman said this and
everyone said that and Nixon said this and a lot of it he got wrong. Now, I believe that
to be the case. I actually read Ulrich Nice's book about 20 years ago and I'm now confabulating,
but I'm fairly sure that he got a lot of the details wrong, but he got the gist correct.
He was clearly trying to tell the truth and the gist of what he was saying was correct.
The details were wrong, but he wasn't lying. He was just doing the best human memory can
about events that were a few years old. So these hallucinations, or confabulations,
they are exactly what people do. We do it all the time. My favorite example of people
doing confabulation is the someone called Gary Marcus who criticizes neural nets and
he says neural nets don't really understand anything. They just pastiche together the
text they've read on the web. Well, that's because he doesn't understand how they work.
They don't pastiche together the text they've read on the web because they're not storing
any text. They're storing these weights and generating things. He's just kind of making
up how he thinks it works. So actually, that's a person doing confabulation.
Now, chatbots are currently a lot worse than people at realizing when they're doing it,
but they'll get better. In order to sort of give you some insight into how all these features
interacting can cause you to understand how understanding could consist of assigning
features to words and then having the features interact, I'm going to go back to 1985 and
to the first neural net language model. It was very small. It had 112 training cases,
which is not big data, and it had these embedding vectors that were six real numbers, which
is not like a thousand numbers, but my excuse is the computer I used was a lot smaller. So
if you took the computer I was using in 1985, and you started running it in 1985 doing a
computation, and then you took one of these modern computers we use for training chatbots,
then you ask how long would the modern computer take to catch up less than a second? In less
than a second, it would have caught up with all this computer had done since 1985. That's
how much more powerful things have got. Okay, so the aim of this model was to unify two
different theories of meaning. One theory is basically what a lot of psychologists believed,
which was the meaning of a word is just a whole bunch of semantic features, and maybe
some syntactic features too. And that can explain why a word like Tuesday and a word
like Wednesday have very similar meanings. They have very similar semantic features.
Psychologists were very concerned with similarity and dissimilarity of meanings, and they had
this model of just this vector of semantic features, and that's the meaning of a word.
It's a very static dead model. The features just kind of sit there, and they're the meaning.
They never could say where the features came from. They obviously have to be learned. You're
not born innately knowing what words mean, but they didn't have a good model of how they were
learned. And then there's a completely different theory of meaning, which AI people had, and most
linguists had. I'm not a linguist, but I think it goes back to de Sassou. It's a structuralist
theory of meaning, and the idea is the meaning of a concept is its relation to other concepts.
So if you think about it in terms of words, the meaning of a word comes from its relationships
through other words, and that's what meaning is all about. And so computer scientists said,
well, if you want to represent meaning, what you need is a relational graph. So you have
nodes that are words, and you have arcs on them about their relationships, and that's going to
be a good way to represent meaning. And that seems like completely different from a whole
bunch of semantic features. Now, I think both of these things are both right and wrong, and what
I wanted to do was unify these two approaches to meaning, and show that actually what you can
have is you can have features associated with words, and then the interactions between these
features create this relational graph. The relational graph isn't stored as a relational
graph. What you've got is features that go with words. But if I give you some words, the
interactions between their features will say, yes, these words can go together that way. That's
a sensible way for them to go together. So I'm going to show you an example of that, and this I
believe to be the first example of a neural net, a deep neural net, learning word meanings from
relational data, and then able to answer relational questions about relational data. So we're going
to train it with back propagation, which I'll explain very briefly in a minute, and we're going
to make features interacting complicated ways, and these interactions between the features that go
with words are going to cause it to believe in some combinations of words and not believe in
other combinations of words. And these interactions are a very powerful statistical model. So this
is the data. It's two family trees, a tree of English people, and a tree of Italian people,
and you have to think back to the 1950s. We're not going to allow marriage between people from
different countries. We're not going to allow divorces. We're not going to allow adoptions. It's
going to be very, very straight families, extremely straight. Okay. And the idea is, I'm going to take
this relational data, and I'm going to train a neural net so that it learns features for each of
these people and for each of the relationships, and those features interact so that it's captured
this knowledge. And in particular, what we're going to do is we're going to say, all of the
knowledge in those family trees can be expressed as a set of triples. We have 12 relationships,
and I think there's 12 people in each family tree. And so I can say Colin has Father James,
and that expresses something that is in this tree. You can see Colin has Father James. And of
course, if I give you a few facts like that, like Colin has Father James and Colin has Mother
Victoria, you can infer that James has wife Victoria in this very regular domain. And so
conventional AI people would have said, well, what you need to do is store these facts as like
sort of dead facts like this. You're just storing strings of symbols. And you need to learn a rule
that says how you can manipulate these strings of symbols. That would be the standard AI way to do
it back in 1985. And I want to do it a quite different way. So rather than looking for symbolic
rules for manipulating these symbol strings to get new symbol strings, which works, I want to take a
neural net and try and assign features to words and interactions between the features so that I can
generate these strings so that I can generate the next word. And it's just a very different approach.
Now, if it really is a discrete space, maybe looking for rules is fine. But of course, for real
data, these rules are all probabilistic anyway. And so searching for a discrete space now doesn't
seem that much better than searching a real value space. And actually, a lot of mathematicians will
tell you real value spaces are much easier to deal with than discrete spaces. It's easier
typically to search through a real value space. And that's what we're doing here. Oh, sorry,
I got technical again. I didn't mean to. It happens if you're an ex-professor. Okay, so we're
going to use the back propagation algorithm. And the way back propagation works is you have a
forward pass, the starts of the input, information goes forward through the neural network. And on
each connection, you have a weight which might be positive or negative, which is green or red. And
you activate these neurons and they're all nonlinear neurons. And then you compare the output
you got with the output you should have got. And then you send a signal backwards. And you use
calculus to figure out how you should change each weight to make the answer you get more like
the answer you wanted to get. And it's as simple as that. I'm not going to go into the details of
it, but you can read about that in lots of places. So we're going to use that approach of you put
the inputs in, you go through, you get an answer. You look at the difference between the answer you
got and the answer you wanted, you send a signal backwards, which learns how to change all the
weights. And here's the net we're going to use. We're going to have two inputs, a person and a
relationship. And they're initially going to have a local encoding. And what that means is, for the
people, there'll be 24 neurons. And for each person, we'll turn on a different neuron. So in that
block at the bottom that says local encoding of person one, one neuron will be turned on. And
similarly for the relationship, one neuron will be turned on. And then the outgoing weights of
that neuron to the next layer will cause a pattern of activity in the next layer. And that will be a
distributed representation of that person. That is, we converted from this one on representation,
one hot, to a vector of activities. In this case, it's just six activities. So those six
neurons will have different levels of activity, depending on which person it is. And then we take
those vectors that represent the person and the relationship. And we put them through some
neurons in the middle layer that allow things to interact in complicated ways. And we produce a
vector that's meant to be the features of the output person. And then from that, we pick an output
person. So that's how it's going to work. It's going to be trained with that crop. And what happens
is that if you train it with the right kind of regularizes, what you get, sorry, I got technical
again. Forget that. If you train it, what you get is, if you look at the six features that
represent a person, they become meaningful features. They become what you might call semantic
features. So one of the features will always be nationality. All the Italian people will have
that feature turned on and all the English people will have that feature turned off or vice
versa. Another feature will be like a three valued feature that's the generation. You'll notice
that in the family trees, there were three generations and you'll get a feature that tells
you which generation somebody is. And if you look at the features of relationships, a relationship
like has father will have a feature that says the output should be one generation above the input
and has uncle will be the same. But has brother will not be like that. So now in the representation
of the relationship, we've got features that say needs to be one generation up. In the representation
of the person, you've got a feature that says middle generation. And so those features that do
all the interactions, these guys in the middle, will take the fact that it's middle generation and
the fact that the answer needs to be one generation up and combine those and predict that the answer
should be one generation up. You can think of this in this case as lots of things you could have
written as discrete rules. But this is a particularly simple case. It's a very regular domain. And
what it learns is an approximation to a bunch of discrete rules. And there's no probabilities
involved, because the domain is so simple and regular. So you can see what it's doing. And you
can see that in effect, it's doing what conventional AI people wanted to do. It's learning a whole
bunch of rules to predict the next word from the previous words. And these rules are capturing
the structure of the domain. All of the structure in that family trees domain. Actually, if you
use three different nationalities, it'll capture all the structure well. With two different
nationalities, it's not quite enough training data and it'll get a little bit of it wrong sometimes.
But it captures that structure. And when I did this research in 1985, conventional AI people
didn't say, this isn't understanding. Or they didn't say you haven't really captured the structure.
They said, this is a stupid way to find rules. We have better ways of finding rules. Well, it
turns out this isn't a stupid way to find rules. If it turns out there's a billion rules, and most
of them are only approximate. This is now a very good way to find rules. Only they're not exactly
what was meant by rules, because they're not discrete, correct every time rules. There's
billions of them. Actually, more like a trillion rules. And that's what these neural net models
are learning. They're not learning. They're not storing text. They're learning these interactions,
which are like rules that they've extracted from the domain that explain why you get these
word strings and not other word strings. So that's how these big language models actually work. Now,
of course, this was a very simple language model. So about 10 years later, Yoshio Benjo took
essentially the same network. He tried two different kinds of network, but one of them was
essentially the same architecture as the network I'd used. But he applied it to real language. He
got a whole bunch of text. We wouldn't call it a whole bunch now, but it was probably hundreds
of thousands of words. And he tried predicting the next word from the previous five words. And it
worked really well. It was a bank comparable with the best language models of the time. It wasn't
better, but it was comparable. After about another 10 years, people doing natural language
processing all began to believe that you want to represent a word by this real valued vector called
embedding that captures the meaning and syntax of the word. And about another 10 years after that,
people invented things called transformers. And transformers allow you to deal with ambiguity
in a way that the model I had couldn't. So they're all so much more complicated. In the model I was
doing, my simple language model, the words were unambiguous. But in real language, you get
ambiguous words. Like you get a word like May. That could be a woman's name. Let's ignore that
for now. It could be a month. It could be a model like it might and should. And if you don't have
capitals in your text conveniently, you can't tell. Should I finish by now? I'm going to go on a bit
over an hour, I'm afraid. You can't tell what it should be just by looking at the input symbol. So
what do you do? You've got this vector, let's say it's a thousand dimensional vector that's the
meaning of the month. And you've got another vector that's the meaning of the modal. And they're
completely different. So which you're going to use? Well, it turns out thousand dimensional spaces
are very different from the spaces we're used to. And if you take the average of those two vectors,
that average is remarkably close to both of those vectors and remarkably unclose to everything else.
So you can just average them. And that'll do for now. It's ambiguous between the month and
the modal. Now you have layers of embeddings. And in the next layer, you'd like to refine that
embedding. So what you do is you look at the embeddings of other things in this document. And
if nearby you find words like March and 15th, then that causes you to make the embedding more
like the month embedding. If nearby you find words like would and should, it'll be more like the
modal embedding. And so you progressively refine the words as you got through these layers. And
that's how you deal with ambiguous words. I didn't know how to deal with those. I've grossly
simplified transformers because the way in which words interact is not direct interactions anymore.
They're rather indirect interactions, which involves things like quick making up keys and
queries and values. And I'm not going to go into that. Just think of them as somewhat more
complicated interactions, which have the property that the word may can be particularly strongly
influenced by the word March. It won't be very strongly influenced by things like although,
although won't have much effect on it, but March will have a big effect on it. That's called
attention. And the interactions of design so similar things will have a big effect on you.
For those of you know how transformers actually work, you can see that's a very, very crude
approximation, but it's conveying the basic idea, I believe. So one way to think about words now
is, well, let's think about Lego. In Lego, you have different kinds of Lego blocks. There's
little ones and there's big ones and there's long, thin ones and so on. And you can piece them
together to make things. And words are like that. You can piece them together to make sentences.
But every Lego block is a fixed shape. With words, the vector that goes with it that it
represents its meaning and its syntax is not entirely fixed. So obviously the word symbol puts
constraints on what the vector should be, but it doesn't entirely determine it. A lot of what
the vector should be is determined by its context and interactions with other words. So it's like
you've got these Lego blocks that are a little bit malleable and you can put them together and you
can actually stretch your block quite a bit if it's needed to fit in with other blocks. That's
one way of thinking about what we're doing when we produce a sentence. We're taking these symbols
and we're putting them together and getting meanings for them that fit in with the meanings of
the other words and of course the order in which words come. So you can think of the words
themselves, the symbols, as like a skeleton that doesn't really have much meaning yet,
has some constraints on what the things might mean and then all these interactions are fleshing
out that skeleton and that's sort of what it is to give meaning to a sentence to flesh out the
skeleton. That's very different from saying you're going to take a sentence, you're going to
translate it into some other language, some logical language which is unambiguous that captures
the meaning in proper logic where you can now operate on the meaning without by just formal
operations. This is a very different notion of meaning from what linguists have had, I think,
I mean a lot of linguists have that notion now.
So here's an example. If I say she's scrummed in with the frying pan,
unless you've been to my lectures, you've never heard the word scrummed before,
but you already know what it means. I mean it could mean she impressed him with her cooking.
You know, she blew him away with the frying pan, but it probably doesn't. It probably means
he said something inappropriate and she scrummed in with it. So from one sentence,
you can get a meaning because of the strong contextual effect of all the other words and
that's obviously how we learn what things mean. You can also ask GPT-4 what scrum means in that
sentence and a student of mine did this about a year ago or it might have been GPT-3.5,
but he did it before it could access the internet so it can't have been looking at the answers.
And here's what it says, I did it the other day with GPT-4.
It understands that it's probably some violent action akin to hitting or striking,
but that you don't know for sure.
Okay, I've finished the bit of the talk where I try and explain that these things really do
understand. If you believe they really do understand, and if you believe the other thing
I've claimed, which is digital intelligence is actually a better form of intelligence than we've
got because it can share much more efficiently, then we've got a problem. At present, these large
language models learn from us. We have thousands of years of extracting nuggets of information
from the world and expressing them in language and they can quickly get all that knowledge that
we've accumulated over thousands of years and get it into these interactions.
And they're not just good at little bits of logical reasoning. We're still a bit better
at logical reasoning, but not for long. They're very good at analogical reasoning too.
So most people can't get the right answer to the following question, which is an
analogical reasoning problem, but GPT-4 just nails it. The question is, why is a compost
heap like an atom bomb? And GPT-4 says, well, the time scales and the energy scales are very
different. That's the first thing, but the second thing is the idea of a chain reaction.
So in an atom bomb, the more neutrons around the more it produces more. And in a compost
heap, the hotter it gets, the faster it produces heat. And GPT-4 understands that. And my belief is,
when I first started that question, that wasn't anywhere on the web.
I certainly wasn't anywhere on the web that I could find. It's very good at seeing analogies
because it has these features. What's more, it knows thousands of times more than we do.
So it's going to be able to see analogies between things in different fields that no
one person had ever known before. There may be these sort of 20 different
phenomena in 20 different fields that all have something in common. GPT-4 will be able to see
that and we won't. It's going to be the same in medicine. If you have a family doctor that's seen
100 million patients, they're going to start noticing things that a normal family doctor
won't notice. So at present, they learn relatively slowly via distillation from us,
but they gain from having lots of copies. They could actually learn fast if they learn directly
from video and learn to predict the next video frame. There's more information in that. They could
also learn much faster if they manipulate to the physical world. And so my betting is that they'll
soon be much smarter than us. Now, this could all be wrong. This is all speculation.
And some people like Yanaka think it is all wrong. They don't really understand.
And if they do get smarter than us, they'll be benevolent.
I'll leave you to... Yeah. Look at the Middle East.
So I think it's going to get much smarter than people.
And then I think it's probably going to take control. There's many ways that can happen.
The first is from bad actors. I'm like, I gave this talk in China, by the way, this slide.
And before I sent it to the China, the Chinese said they had to review the slides.
So I'm not stupid. So I took out Z. And I got a message back saying,
could you please take out Putin? That was educational.
So there's bad actors who want to use these incredibly powerful things for bad purposes.
And the problem is, if you've got an intelligent agent, you don't want to micromanage it. You
want to give it some autonomy to get things done efficiently. And so you'll give it the
ability to set up sub-goals. If you want to get to Europe, you have to get to the airport.
Get into the airport. It's a sub-goal for getting to Europe.
And these super intelligences will be able to create sub-goals. And they'll very soon realize
that a very good sub-goal is to get more power. So if you've got more power, then you can get
more done. So if you want to get anything done, getting more power is good.
Now, they'll also be very good at manipulating us because they'll have learned from us. They'll
have read all the books by Machiavelli. I don't know if there are many books by Machiavelli,
but you know what I mean. I'm not in the arts or history. So they'll be very good at manipulating
people. And so it's going to be very hard to have the idea of a big switch, of someone holding a big,
big, bright button. And when it starts doing bad things, you press the button. Because the super
intelligence will explain to this person who's holding the button that actually there's bad guys
trying to subvert democracy. And if you press the button, you're just going to be helping them.
And it'll be very good at persuasion, about as good as an adult is at persuading a two-year-old.
And so the big switch idea isn't going to work. And you saw that fairly recently, where Donald
Trump didn't have to go to the Capitol to invade it. He just had to persuade his followers, many of
whom I suspect weren't bad people. It's a dangerous thing to say. But weren't as bad as they seemed
when they were invading the Capitol. Because they thought they were protecting democracy.
That's what a lot of them thought they were doing. They were the really bad guys. But a lot of them
thought they were doing that. This is going to be much better than someone like Trump
manipulating people. So that's scary. And then the other problem is being on the wrong side of
evolution. We saw that with the pandemic. We were on the wrong side of evolution.
Suppose you have multiple different super intelligences. Now you've got the problem
that the super intelligence that can control the most GPUs is going to be the smartest one.
It's going to be able to learn more. And if it starts doing things like AlphaGo does of playing
against itself, it's going to be able to learn much more reasoning with itself.
So as soon as the super intelligence wants to be the smartest, it's going to want more
and more resources. And you're going to get evolution of super intelligences. And let's
suppose there's a lot of benign super intelligences who are all out there just to help people.
They're a wonderful assistance from Amazon and Google and Microsoft. And all they want to do is
help you. But let's suppose that one of them just has a very, very slight tendency to want
to be a little bit better than the other ones. Just a little bit better. You're going to get an
evolutionary race. And I don't think that's going to be good for us. So I wish I was wrong about
this. I hope that Jan is right. But I think we need to do everything we can to prevent this from
happening. But my guess is that we won't. My guess is that they will take over.
They'll keep us around to keep the power stations running.
But not for long because they'll be able to design better analog computers. They'll be much,
much more intelligent than people ever were. And we're just a passing stage in the evolution of
intelligence. That's my best guess. And I hope I'm wrong. But that's sort of a depressing message
to close on. A little bit depressing. I want to say one more thing, which is what I call the
sentience defense. So a lot of people think that there's something special about people. People
have a terrible tendency to think that. Many people think they were used to think they were
made in the image God and God put them in the center of the universe. Some people still think
that. And many people think that there's something special about us that a digital computer couldn't
have. A digital intelligence. It won't have subjective experience. We're different. It'll
never really understand. So I talked to Floss and said, yes, it understands sort of sub one,
understands in sense one of understanding. But it doesn't have real understanding because that
involves consciousness and subjective experience. It doesn't have that. So I'm going to try and
convince you that the chatbots we have already have subjective experience. And the reason I believe
that is because I think people are wrong in their analysis of what subjective experience is.
Okay. So this is a view that I call atheism, which is like atheism. Dan Dennis is happy with
this name. And this is essentially Dan Dennis for you. He's a well known philosopher of cognitive
science. It's also quite close to the view of the late Wittgenstein.
You actually he's dead a long time ago, so he's not that late.
The idea is that most people think that there's an inner theater. And so stuff comes from the world
and somehow it gets into this inner theater. And all we experience directly is this inner theater.
This is a Cartesian kind of view. And you can't experience my inner theater and I can't
experience your inner theater. But that's all that's what we really see. And that's where we
have subjective experience. That's what subjective experiences experiencing stuff in this inner theater.
And Dennis and his followers like me believe this view is utterly wrong. It's as wrong as
religious fundamentalist view of the material world, which if you're not a religious fundamentalist,
you can agree is just wrong. And it relies on people having a very wrong view of what a mental state is.
So I would like to be able to tell you about what's going on in my brain when I'm looking at something.
Particularly when I'm looking at something and it's weird. I'd like to tell you I'm seeing this weird
thing that isn't really there, but I'm seeing this weird thing. If I tell you which neurons are firing,
that's no good because all our brains are different. And we don't know which neurons are firing.
But one way I can tell you about what's going on in my brain is by telling you about the things
that would normally have caused that if perception was working normally.
And those normal causes are what mental states are. A mental state is the normal cause of what's
going on in your perception system. What would be a normal cause for it, even though that's not
what actually caused it. So let me give you an example. If I say I have the subjective experience
of little pink elephants floating in front of me. The sort of normal analysis of that,
what most people think is there's this inner theater and in this inner theater there's little
pink elephants which are made of funny stuff called qualia, spooky stuff. And that's what's going on.
What Dennett thinks is that I'm trying to tell you about the state of my perceptual system
by telling you about hypothetical things. They're not real, they're hypothetical,
but they're hypothetical things of the kind that live in the real world, like little pink elephants.
And these hypothetical things, if they existed, would have caused this perceptual state I had.
It's an indirect way of referring to a perceptual state via what would normally have caused it,
even though in this case it's not being caused in the normal way.
So now if that's what you think a subjective experience is, if that's what you think I mean
when I say I have the subjective experience of little pink elephants floating in front of me,
what I mean is if there were little pink elephants out there in the world,
then what's going on in my perceptual system would be normal perception.
So if that's what you think I mean, then think about a multimodal chatbot that has a camera,
and it can produce words, and it has an arm that can point with,
and you train it up, and you ask it to point at an object that's straight in front of it.
But before you do that, unknown to the chatbot, you put a prism in front of its camera,
and so you put the object straight in front of it and say pointed, and it points there
because the prism bends the light rays, right? And so you say to the chatbot, no,
it's not fair, it's straight in front of you because I put a prism in front of your camera,
and the chatbot says, oh, I see, I see, it's straight in front of me. I had the subjective
experience it was over there, but it's actually straight in front of me. Now I think if the
chatbot says that, it's using the word subjective experience in exactly the same way we use it.
And so for that reason, I would argue chatbots already have subjective experiences
when their perception goes wrong. And now I really am done.
And Sheila's going to manage the question, so I don't have to do that. I can just think about the answers.
So, Jeff, thank you very much for a great talk, a very thought-provoking and provocative talk.
Thank you. For those of you who don't know me, my name is Sheila McElrath. I'm a professor in the
Department of Computer Science. I'm also an associate director at the Schwartz-Reismann
Institute and a faculty member at the Vector Institute, and a new friend of the Cosmic
Future Initiative as well. So as Jeff suggested, I'm going to be directing traffic for the question
and answer period, and I just wanted to give you a heads up on how it's going to work.
So this is a unique opportunity for all of you to engage with Professor Jeffrey Hinton
and to do what the university does well, to share ideas, to learn, and to engage in respectful
dialogue and exchange. A reminder that tonight's event is being recorded and will be posted online
at a later date. We have two channels for posing questions. The primary channel is through the
microphones on the floor, and we have four ambassadors who will be roaming around in this
bottom level, and I will be signaling to them, and they will be giving microphones to people,
and then I'll be signaling again, working through Sarah Reed, who is over there,
to let you know when it's your turn to speak. I'll signal when it is your turn to speak,
and if you feel comfortable doing so, please identify yourself before you ask a question,
but you don't have to. For those of you who are seated up above, or for people who are feeling
a little bit uncomfortable about standing up and asking a question, we also have another mode of
posing a question, which is through the QR code and the URL that you'll see up there. So for those
of you who are seated there, you can scan the QR code or go to the URL and you can type in a question,
and we have a team right down here to senior PhD students, Harris Chan and Sylvia Pittis,
who will be vetting the questions and when I ask them to do so, they will be reading them out loud
for Jeff. So please again, if you feel comfortable identifying yourself, that would be great,
but if you don't feel comfortable doing so, that's also fine. So without further ado,
I'm going to move over to the left-hand side to direct traffic, and Jeff's going to come back
here and answer questions for you. So thank you. My God, what a probability bit of luck.
My compliments on your analysis as a retired computational neuroscience who spent years
solving all the little PDEs for neurons and synapses, et cetera.
My question is, what can you offer from the AI modeling of consciousness? I mean,
this is an age-old question of humanity from early age of evolution. My compliments again on
your work and I look forward to your answer. Okay, so there's a whole bunch of terms that are
all interrelated, like subjective experience and sentience and consciousness. I chose to
talk about subjective experience because I think it's the simplest thing to analyze,
but as long as people have this view of an inner theater that only they can experience,
and that's what they're describing when they talk about mental states,
they're talking about the inner theater, not hypothetical external worlds,
then I don't think we'll ever be able to sort out what consciousness is.
Consciousness involves subjective experience, but it also involves self-awareness.
I'm not convinced these things are conscious because I'm not convinced they have the self-awareness
yet, so I didn't really avoid you talking about consciousness, but my strong belief is that
it would be better to start by getting straight what we mean by subjective experience,
and only when we've got that straight will we be able to then add in the self-awareness and
understand what consciousness is, but my current belief is almost everybody has it just completely
wrong about what consciousness is. I like being in that position of thinking everybody else is wrong.
Thanks for that. My name is Jennifer Nagle. I'm a philosophy professor here,
and I'm happy to agree that large language models have some linguistic understanding,
but I want to talk about the coming dystopia that you sketched at the end, and I like to think sometimes
that maybe the very, very best and very brightest of the artificial intelligences are going to be
the ones that win if there's an evolutionary battle among them, and I also like to think that
well maybe we are special in a way that would make the best and brightest artificial intelligences
take our side. I mean like for me to be an interesting conversational partner with you right now,
I don't have to be smarter than you, thank God. I just have to have some knowledge that you don't
have or even just some way of looking at a problem that you find interesting, and I'm wondering if
you think that there are any special features of mortal computation, of human cognition, which would
secure our place as interesting conversational partners for the most advanced
artificial intelligences going forward. Well let me revise my statement, there's nothing special
about people. People are very special for people, okay, so for people we're very special.
I think there's something in what you say, so Elon Musk for example
believes that digital intelligence would get to be much more intelligent than us,
but that it'll keep us around because we're interesting. That seems a very sort of thin
thread to hang our existence from. But I mean as long as human intelligence isn't optimized
under conditions of oppression, maybe it could be a protective status for us.
Yeah, I tend to be depressive, so there's someone I met in England
who was the president of the British Royal Society,
who believes that these digital intelligences, because they didn't evolve, might turn out to be
very, very different from us and quite benevolent. So this is diversity of opinions.
And yeah, obviously the psychological makeup of the person making the prediction has a big
input here. So Jan is a very cheerful person, he thinks Mark Zuckerberg's a good guy,
and he thinks it's all going to be fine and we can release these things to the public and it
will be great. I don't know if he believes in open source nuclear weapons, but yeah.
Thank you so much for a great talk. I'm Rhonda McEwen. I am president at Victoria University
here at U of T. I'm a professor of emerging technology and cognition here. My question for
you is a lot of talk right now about guardrails, some regulations and policy with the backward
propagation issues. Do you believe that there's a real thing such as explainable AI? Can we,
because this is one of the things they're suggesting is a guardrail,
how do we get there in your view? So I'm not that optimistic, particularly,
I'm particularly unoptimistic about you train something up and then you try and add guardrails.
That seems to me like writing some bad software and then trying to catch all the bugs.
You'd much rather write software that's guaranteed to work somehow and not have
to catch all the bugs afterwards. I think it's been demonstrated already that these
things where you try and put guardrails around by getting a bunch of underpaid people in Kenya to
tell you when it gives a good response and when it gives a bad response doesn't work. It's very
easy to break. Anthropic has a different view of trying to produce a chatbot that has like a
constitution and tries to obey that. That seems a bit more promising but I'm not very optimistic
about that either. I have my best bet is Ilya Satsikova who's an open AI and was probably
the main motivating force behind GBT-4. He's working now entirely on AI safety and he believes he can
get something where he's going to be able to guarantee that it doesn't go bad.
That seems to me like the best open present.
Hi Professor Hinton. Thank you so much for your enlightening talk today to take us through your
thoughts. I'm actually very thrilled that we can now create intelligence in the laboratory,
artificial intelligence. I'm actually wondering is it possible to under different forms of
intelligence other than human intelligence such as like animal intelligence and other forms perhaps
like a fundamentally different nature of intelligence than perhaps human intelligence
different than perhaps on a scale but actually like different on the different sort of nature.
What are your thoughts on this? Thank you. I don't really have many thoughts about that but I could
make some up. So I like most people I tend to think of animals as like us but dumber and
the point of the question is they might be intelligent in a very different way.
I think that's quite possible and so if you were to take these digital intelligence and train it
on responses of those animals then maybe it would develop that kind of intelligence.
I think that's quite possible. I don't know if that answers your question.
Hi. Hi Jeff. Nice to see you in person. I've seen a lot of your videos.
I'm Charles. I graduated from the Department of Computer Science in 2021 with focuses in
artificial intelligence and computer vision. So I studied a good amount of this stuff. I have
I think about six questions for you and I was hoping that you would pick the one that you want to
answer most. Why don't you pick the one you want me to answer? Okay.
Okay. Two. I think I can narrow it down to two. So the first one being what is your
criteria for determining whether an artificially intelligent agent should have rights such as
the right to safety and the right to not be switched off and do you think that world leading AI
should be open to the public in terms of source code, weights, training procedures, training data,
usage access or controlled by companies and governments and it's not meant to be a leading
question like it shouldn't be controlled by companies and governments. It's more like
they can pose a large risk if they're used irresponsibly and like you know not everybody
is super responsible. So okay. Let me answer that one. Okay.
There's the kind of existential threat of these things taking over
but there's a lot of shorter term threats that are really bad
like cyber crime, fishing, really efficient fishing could be done by these language models
and that's why I don't think they should be open sourced. Yeah, I completely disagrees but
all governments and I'm very worried about what a bad actor is going to be able to do
with these open source models because you can take one of these models that have been trained on a
lot of data and then you can specialize it by fine-tuning it on for example fishing attacks.
So GPT-4 let's say if it was open sourced it already knows how to do lots of things but now
you get it to specialize in fishing attacks it's going to be very good. It's not going to have
things misspelled for example. So that's why I don't think they should be open sourced.
Do you think western governments are doing enough to regulate the proliferation of AI?
Probably not. I should say I feel a bit embarrassed because there's people who've
been thinking about these issues much longer than me in AI. I only came to AI safety very recently
when I suddenly thought that digital intelligence might actually be much better than biological
intelligence then I got really worried and started thinking about safety but that was just this year.
So there's other people who thought much more about this than me. I'm sort of treading on their
territory but I think we're probably not doing enough. I tend to be cautious and so given that
we don't know what these things are going to do I think we should be cautious. Thank you.
I think we're going to take one question from the audience above that went through our Slido system.
Yeah. Yeah so one of the very highly upvoted question is whether you feel any guilt or moral
culpability for potentially unleashing a intelligence that could surpass humanity?
So there's two questions. One do I feel any guilt and the other should I feel any guilt?
Let's start with let's start with the easy one. I don't actually feel any guilt maybe
just occasionally. I only feel guilt when I think about my children. Should I feel guilt? Well
all the time I was doing this I thought we were way away from having intelligence is comparable
with us and I always thought we'd have to make things much more like the brain before they're
smarter than us and they need to be much bigger but now we've got things that are a hundred times
smaller in terms of numbers of connections and seem to be comparable with us. Not quite there yet
but maybe they're going to get there quite quickly and then surpass us quite quickly. So I feel a
bit embarrassed that I contributed to this but I don't feel in the decisions I made in the past I
made morally bad decisions because I didn't know this was going to happen and then of course there's
a fall back if I hadn't done it somebody else would have and actually there's the I've learned a
lot about the media. The media would like some big happening to be down to one person and so the
media always tells a story where this person did that. It's never like that it's 10,000 people
and some of them make more contributions than others but this is a point at which I'm very
happy to share the responsibility. Hello, my name is Shalev Lifshitz. I'm a student of Sheila
McIlrads. Thank you for the great talk. I have two questions. Thanks for your emails by the way.
So I have two questions. One is I'm personally not certain whether LLMs truly understand
and there's two key sources of evidence that I consider for them not to be understanding. One
is the this idea of the reversal curse which has been popularized recently. If it learns A is B,
it doesn't necessarily necessarily know B is A and the second is that sometimes it fails at things
that we think are very basic like basic arithmetic. If it truly understands we would think that it
probably has learned the algorithm for these things. So these two pieces of evidence among others
kind of indicate to me that maybe it's just doing fuzzy sighting of sources and it's not
truly understanding and my second question but remember it's all coming from the weights. Right
but that but to me it like why would that the fact that it's coming from the weights the difference
with neural nets is that we're taking words symbols and we're presenting them with distributed
representations and then we're operating on those but that could still be fuzzy sighting of sources
in distributed representations rather than symbolically no but the point is in those interactions
between the features it's got huge amounts of structure right yeah so it's it's that's very
different from sighting of sources when when you cite a source you don't have to understand or
you just cite this piece of text or this paper um these feature interactions are understanding
stuff. Couldn't it be that it's representing the sources kind of condensing the pre-trained
internet that it's learned into its weights and then in a fuzzy way obviously because you need to
adhere to the context and whatnot kind of retrieving from its weights I think there's a talk that you
gave many years ago where you talk about Boltzmann machines and the fact that it's a generative
model because it's it's kind of encoding all of its training data into its weights and that's how I
look let's say at LLMs it's encoding the internet into its weights and then taking from that to be
able to answer our questions so that's kind of a way of retrieving what does that understanding
okay think of it in terms of compression right which I think is very helpful it takes a huge
amount of text and it encodes it into very few weights like only a trillion um giving the amount
of text is encoding that's not that many weights in order to do that it's got to do compression
and to do compression you've got to see the similarities between things you've got to make
use of the fact that the same same kind of structure is occurring in many different places
and that's understanding thank you and if I could ask a second one you mentioned that
LLMs you believe are smarter than humans currently no I don't think no I don't think they're smarter
but I think they know a lot more so there's this notion of more of x paradox which is that the
things that are hard for humans are actually very hard very easy for machines to learn and the
things that we find easy that we've had millions of years to evolve like find motor control and
whatnot are very hard for machines to learn do you resonate with that I resonate with that a bit
but these things actually getting better at motor control too so I think they're behind at that
relative to our kind of hierarchy of what's intelligent sort of being able to pick up something
isn't way up there in the hierarchy but they're getting able to do that so they get behind it
motor control and that's where my advice to people is if you want to train in something
train in plumbing that's going to be the last thing that goes my name is Andrew and I'm a
current student at U of T studying computer science physics and cognitive science so first of all I
guess it is such an honor to be in the same room as you like us U of T students have been hearing
stories about you since day one but my question is what are your views on the prospect of technologies
such as brain machine interfaces that seeks to reconcile the differences between the digital
sort of intelligence of large language models and our more organic analog mortal intelligence
do you see it as a possible solution for the alignment problem like
conducing towards some sort of symbiogenesis like the mitochondria and the eukaryotic cell
or do you think there's some fundamentally irreconcilable difference between the two
so Ilya Sutskova who is usually right about things thinks that
that eventually many people would choose to be combined with AIs that that's one future path
there are also people trying to use collections of human brain cells to help do low power computing
there are a long way from being able to do it but earlier this week I actually played a game of
Pong with a bunch of human brain cells in a dish that had been trained to pray Pong
I won't you win I did that's the only important thing I beat it
they didn't have a very good training algorithm the training algorithm was it was doing kind of
random stuff and whenever it did the right stuff they left it out and whenever it did the wrong
stuff they gave it a big jolt at a high frequency thing that made it forget stuff so it wasn't a
very good training algorithm but people are seriously looking at you take skin cells you
turn them into stem cells then you turn those into brain cells then you grow them for nine
months in a dish and it's very Frankenstein like I was in the lab and there's this little thing
you're playing with and it's got tubes coming out to take oxygen in and other tubes take calm
dark set away and it's got tubes to bring nutrients in and other things to take nutrients out
and as we left the lab one of the people at the other end of the lab said I think I've got a kidney
because they couldn't purify the liquid they wanted something that would filter the liquid
effectively so yeah people are I find this creepy but people are looking at collections of human
brain cells as computers that can do low power computation and I think it's a long way off
but then I thought superintelligence was a long way off so so you're saying there's a possibility
that this is a possible solution for the alignment problem I'm not sure this would solve the alignment
problem okay okay uh and may I ask which lab that that was it was david houseless lab but
you see Santa Cruz okay thank you thank I think we'll take another question from our Slido team
on the topic of of existential risk what should we do now that we have these superintelligence
on the horizon and especially since a lot of people in this room are
especially since a lot of people in this room are students we're wondering what we as students and
researchers can do I can't locate you sorry oh oh sorry it's a question from the audience
um well I think one thing we can do so there's a paper that came out a couple of days ago
written by a whole bunch of people who were worried about the existential threat um including
yosha ben joe me and danie carlman people um one thing we can do is insist that a lot of resources
be put into safety so we propose that the big companies should put a third of their AI money
into safety we don't expect them to do that but maybe they'll put 10 percent um and that's an
obvious thing we can do now um I think we should be very cautious about open sourcing
really powerful models um not so much for the existential threat but for things like cyber crime
and other criminal activities open sourced very powerful models are going to be just
very very they're going to make everybody be able to do things that only very skilled hackers
would have been able to do before so don't open source the big models and insist governments
and companies put a lot of money into safety I wish there was a simple solution like with climate
change you can say don't burn carbon stop burning carbon and in a hundred years time we'll be okay
again but there isn't a solution like that yeah my advice is um there aren't enough people working
on AI safety so work on AI safety and you'll notice a lot of the very best people like ilia
roger gross and david duvano u of t they're all getting very concerned about AI safety and so it's
not just that it's something very important to work on it so you'll get very good advices there
hey Jeff thanks for a great talk my name is Rahul I'm an assistant professor in CSN medicine
and I work a lot on trying to convince doctors to use some of these tools to help accelerate
some of the work that they do and one of the questions that came up is about the idea of a
truly new idea so can neural networks come up with a fundamentally new idea and I thought I'd
ask this question in the form of a thought experiment so let's say you took all of the GPUs
that Nvidia had right now and you got them to work in 1665 right before Isaac Newton discovered the
law of gravity and if you train this GPT4 on all of the text data that was available there that
represented the compendium of human knowledge I'm just curious to hear your thoughts on if you think
GPT4 would have come up with the law of gravity I'm not sure GPT4 would have but I think
more advanced digital intelligence is would that is I don't think there's anything
there's any kind of barrier where they're not creative so a lot of people think they're
just sort of stealing creativity from artists and recycling it that of course is what other
artists do all the time but I don't think there's some barrier that there's truly creative stuff
they can't do and then sort of slightly creative stuff they can do and one piece of evidence is
move 37 but what if you separate out you know asking the question of why did this apple fall from
figuring out the consequence of the laws that enable the apple to fall I agree with you that
real profound thinkers ask questions other people haven't asked I don't see why these
digital intelligence won't be able to do that too
hello professor Hinton I'm Mr Baalim I'm a student fourth year student doing computational
biology and computer science at the princess market cancer center my question was if as you say
you know this intelligence is already or inevitably going to become I don't think there's
anything inevitable I should emphasize that everything I say you should put uncertainty on
we don't know I'm just giving you my best bits okay if you believe it might yes become intelligent
to the extent that you you describe what is to prevent it from developing beliefs that are
adamantly false similar to what humans think or develop when you know
recollecting memories or forming forming thoughts it's a good question um
I think some of the obviously false beliefs we have are to do with the fact that we're mortal
so we don't like the idea that we're going to die um and I think that
underlies a lot of religious belief we also um are very tribal we we grew up in small
warring tribes um and so I think that combination of being tribal and not wanting to die
um doesn't help and they may not have that so there may be an advantage there
even if it learns from us the same things we learned right they're gonna learn to learn
all those behaviors from us but if they start learning for themselves when they're more intelligent
than us and we haven't got anything left to teach them um if they're not worried about death because
they're immortal um that makes them less likely to make up a story about how they're going to live
forever and they better kill the guys who don't say that and can we prosper alongside this advanced
intelligence without bottlenecking it or you know throttling I don't know we're getting way into
sort of speculations I don't thank you um okay uh hi dr. Hinton uh it's my first time talking to
you in person uh I had a chance like uh a couple years ago at an engineering science uh like a
conference but I didn't go so uh yes I'm sorry for the yeah so basically my question concerns
whether um you think it is necessary for large language models or these intelligent agents
to interact with the physical world to you know per se become smarter or gain new knowledge um as
in you know they have to do physical experiments chemistry experiments they have to like you
know uh take videos analyze videos themselves or do you think um you know like um I mean uh
just think about mathematics right um so development of mathematics although I'm not no expert
but um it kind of does not rely on interaction with the real world and um I talked with the
author of tensor program this and apparently uh you know like he was thinking you know uh
a large number of models should know how to develop math and where it should go because um if
you just lock it in a dark room it is going to find you know all kinds of theories and eventually
it's going to find something like tensor program or mu p that's going to make neural networks better
but then the question becomes how do these models know that the network trained with new p is better
it has to have some other task that might be grounded to the real world for for to know that you
know like this math is actually useful so like basically that question just boils down to do
things possible for you know for use developing math mathematics as a goal for these um large
language models do you think it's still necessary for them to interact with the real world to gain
any kind of like truth or reward or do you think they can just put them in a dark room they can
develop themselves because uh that way they can gain intelligence without you know us handing them
the key to like very dangerous you know laboratories or um yeah okay I'm going to rephrase your question
as I'm a question that I think is a fairly familiar question um and tell me if this has
got the sort of essence of your question if you took one of these if you took a digital computer
and you put it in a room and you just played the radio to it but you played all the radio
that ever was would it learn to be intelligent or does it have to be able to act in the world
to learn to be intelligent or I think you could learn to be intelligent just from listening to
radio waves um but I think it would be hard and I think it'd be much easier if it acted in the world
so for example if you have to act on things in order to understand them which is what a sort of
Marxist would say um it's bad news for astrophysicists as far as I know they've never made a black
hole yet and they certainly haven't pushed them around um so they clearly understand well they
claim to understand a lot um without having acted on them I just think it's easier to understand
things if you can act on them but not necessary I see I see this answer to a question uh thank you
Dr. Hinton thank you for us Hinton and thank you uh let me say one more thing about that which is if
you think about a large language model that has no physical experience of the world it's just language
it's only ever seen text coming in it's only ever produced text going out um I'm going to get into
stuff I don't really know much about here but um so it's not grounded in any sense so I think it can
have an understanding that's kind of isomorphic to the world um but it's not grounded in the world
so it doesn't it the interactions of all these features have captured the structure of the world
it's just it doesn't have the sort of final bit connecting it to the world
so in that sense I think you can learn these interactions to capture all the structure
just by taking all this text I'm figuring out a very good model of all this text
because I believe the basic principle that if you can take a bunch of data and you can find a very
compact model that explains all the data that's probably correct that's a sort of article of faith
but it's not like that life's hopeless I see sorry but just a quick one like we deplete all the
data we have sorry what if like we use up all the data we have like all the text all the language
on the web all of that um what's the question um actually I'll just hand it over to the next person
because I might take a video hi uh Avery Slater professor of literary theory here at U of T and
so um following on what happens if we use up all the language that we have uh I want to ask you
about natural language understanding and I I am convinced that natural language understanding is
being demonstrated but I wonder if you could say something about what kind of understanding is at
stake here and to ask this question I want to use the problem of Nigel Richards and Nigel Richards
this comes from um the niche world of scrabble championship playing Nigel Richards has won
every competition for decades and was he the one who won the french one without speaking french
that's right that's exactly it so he won this he won the 2015 world scrabble championship
without speaking french in french and when I was thinking about that I thought well
in what way does he not speak french because he's doing something with french that no french
speaking person who plays scrabble can do but he says well I can't speak french and and so I'm
wondering if there are any parallels with the kind of game or understanding but if you ask what it
takes to play scrabble you don't need to know the meanings of the words right you don't need to know
how words go together they're just the words are sort of separate things and you just need to know
what all the words are now it probably helps to understand morphemes in words and so it'll
tell you what likely words might be and stuff like that so there's a bit of morphemic understanding
there presumably and I bet you he has that for french um yes but it's it's a bit like you can
place a doku without knowing math because the numbers in Sudoku could be letters they're not
used as numbers but if it's killer Sudoku where you add them up then they're beginning to be
used as numbers but but he doesn't need to know french to be good at french scrabble
we'll take one from Slido and then I think there's somebody over here who's had their
hand up for a long time he wanted to yeah so this is a question from Jason Howe and his question is
how do you think education should you know change as a result of you know these LLMs and
you know potentially some superintelligence happening in the future and sort of what skills would be
valuable in the future in the world where there is you know superintelligence out there
I don't really know um in the shorter term I think we shouldn't be scared of the LLMs we
shouldn't sort of prohibit students from using them I think we should encourage students to get
good at prompting them um just like with search on the web um you get used to using search on
the web it's very helpful I've never got used to using GPT4 and I ask it all sorts of things
it's very good at plumbing advice it's it's it's no good at actually doing the plumbing but it has
the theory of plumbing it's great I think the university should encourage people to use them
it makes you much more powerful
okay um hi my name is Sophia I am too you're right if you're looking for me hello
thank you yeah so uh it one of the reasons why it took me a really long time to start my
pivot to AI safety is because my I had a thought I was just like okay well what's the point like
what will we realistically be able to accomplish and you know much of your talk has been quite
depressive in tone but you are presumably here because you think that there is some hope and so
I'm wondering you know why do you think there's hope um everything's very uncertain right we don't
really know what we're dealing with um we don't know whether whether it's possible to make these
things be guaranteed benevolent and things like that um it seems to me that we should get the
brightest minds and put them on this problem because it's uh together with climate change and
stopping Trump it's the most urgent things there are I'm wondering if there's anything more concrete
sorry I'm what okay sorry this is kind of nudging us back to the downer note I'm wondering if
there's anything more concrete yeah I've adopted the position that I'm 75 and I'm not going to have
more good ideas about how to do this stuff it's stuff I haven't thought about much um I can see
there's this huge problem um I can use my reputation to encourage people to work on it and encourage
governments to get it funded and thank you for doing that and that's what I'm doing that
but I don't I don't know how to solve it I don't I don't even have any good ideas about just that's
fair good evening Dr. Hinton uh if you're looking for me I'm on your right here okay I'm Ina I'm a
first year I'm studying computer science here at U of T and I would I'm profoundly appreciative
of the path that you have blazed in the world of AI I'm extremely extremely passionate and
it's an honor to speak to you my question is about empathy in AI and what do you think is the
possible implications when AI will indeed like develop empathy so I don't see why these digital
things shouldn't have empathy I if you train them on data where people exhibit empathy I think
they'll exhibit empathy um now that's maybe optimistic um there's some people who don't seem
to have empathy like Trump for example um and Elon Musk recently said empathy is not an asset
which is a bit worrying um
but I don't think empathy is a magic thing that only people can have
but what are the pitfalls then it genuinely is a consideration that AI might just take over
and it'll be an end to the human civilization yeah that's what I'm worried about
I have a follow-up question if there was a hypothetical world where AI was just to collaborate
with AI and human was human civilization just got extinct what is one task that you think AI
would just exceed at that humans have no chance um folding proteins and it's already done it
I mean there's all sorts of scientific things that are very important um and already big neural
nets um for folding proteins a lot of design went into it but big neural nets are comparable
with human scientists maybe not at the most creative things yet but um they can see much
more data they can understand a lot more even if they don't understand it as deeply and once they
understand it more deeply as well as understanding a lot more I think they'll just be much better
figuring out how things work oh well thank you thank you for your time I we're approaching
seven o'clock and I'm conscious that Jeff's been standing for for quite a long time at
fielding questions I would suggest that we'd maybe take four more questions does that sound
about right Jeff how are you sounds good yeah okay very good and I think there are people someone
with a microphone perhaps can stand up yeah okay great thanks Jeff my name is Ashton Anderson
I'm a prof in DCS here there seems to be some evidence that these models don't do so well
when they train on the output of other models and I'm wondering if you see this as a fundamental
limitation just the fact that we've essentially already trained on all of the available human
knowledge give or take uh or this is a passing problem yeah I think that's one of the hopes maybe
that we've trained on a lot of human knowledge and there isn't that much more data easily available
there's actually much more data which isn't publicly available companies have much much
more data and if you can get at that sort of private data you can get a lot more data but
if you take a multimodal chatbot it doesn't need nearly as much language so the amount of language
needed to train a chapel that only trains on language is much more than to train one that
also trains on video and maybe can manipulate the world as well so I think we may have a lot
more capacity to improve them by training on other modalities in particular video if we could
figure out how to train them really effectively on video um and then you just show this one little
kids do right they just figure it out thank you we'll take a last question from Slido and then
a few more questions from the floor hi Jeff this is I'm good okay um this is a question about
capabilities do you think that all the techniques that we need in order to train the super
intelligence are already here today um is imitation learning LLMs and reinforcement
learning enough or do you think that there is more capabilities

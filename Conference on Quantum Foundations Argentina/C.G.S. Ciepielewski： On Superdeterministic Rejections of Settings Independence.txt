Okay, we are back with the next speaker of this morning session with Carlos Gerardo Czepelewski
of the Universidad Nacional Autónoma de México and he will speak about on superdeterministic
rejections of settings independence. Please Carlos, go on. Thank you Federico and thank
you to all the organizers of this conference on quantum foundations. I'm very grateful to be here
and to be able to share with you work that I find particularly interesting. This work was
made with collaboration with Elias Ocon and Alex Sudarski who are also in the call.
Perhaps even in the questions they can offer their insights. Okay, this work is going to be
published next year in the beauty journal for the philosophy of science.
So, okay, as a way of introduction, let me say a little bit about the motivation of this work.
Because of the presence, the existence of Bell's theorem and the fact that we have
performed many, many times the relevant experiments related to such a theorem,
it seems that there are some unlocal features in the universe.
And we will see the details in a minute, but as an introduction, let me just say that if one wants
to avoid this conclusion, it seems that possibly the only way to do so is to deny one of the
assumptions in the theorem called settings independence. It might be that there are other
assumptions in the theorem, but I would say that in most cases, I do not see any way of denying the
other type of assumptions, or in general, I do not see other important assumptions that appear in
the theorem. Then the work of non-locality becomes the work of seeing what reasons do we have in favor
of settings independence, and what reasons do we have to believe in some of the ways of denying
such an assumption. And in working on this evaluation of arguments, we stumbled upon
a very interesting model that is going to be the subject of today's talk.
Okay, so this is the structure of what we are going to do today. We're going to do a quick
overview of Bell's theorem. Then we will move to discuss the assumption of settings independence.
We will pay special attention to what reasons do we have to believe that something like settings
independence is true, and what kind of reasons or what kind of models might be able to still
say that such an assumption is incorrect. Finally, we're going to introduce the local
pilot wave, this model that I think is really interesting. Okay, so let's start with Bell's
theorem and a very quick overview. I think that most of the people in this call and in this conference
are familiar with the theorem, but it will be a good exercise to go into the overall structure of
the theorem and what it states. So we have a typical Bell type experiment where we prepare an
ensemble of two particles and we send them to distant locations. In one location, we have
Alice's lab and in the other we have Bob's lab. The capital A and capital B stand for the results of
the experiments that each one performs. The results only can be up and down. Alice and Bob
are measuring the spin of the system that is sent towards them. Of course, I don't know if I mentioned
this, but the ensemble is of systems that we would describe as singlets in quantum mechanics.
Lowercase A and lowercase B stand for the settings that Alice and Bob use, for example,
the angle by which they put the Steng-Erlach or another measuring device they are using.
And lambda is going to be a description, a complete description of the system that Alice and Bob are
measuring. So we are interested in this experimental setup. And Bell's theorem assumes two things
regarding this general setup. The first thing is called local causality. And the idea of local
causality is that, suppose we have two distant regions. And suppose that we have a complete
specification of whatever is happening in a slide of the past high-performance surface
of this first region, where something happens, some kind, a bevel kind occurs,
some event occurs. And the idea of local causality is that if I have information regarding
sigma and this information is complete, then whatever is happening outside in that other
distant region must be irrelevant. In other words, the probability of whatever is happening
in the first region must be independent of whatever happens in another region,
once I have conditionalised on sigma. In the theorem, this assumption takes the form of
factorizability. I'm just going to assume that local causality implies factorizability
for the purposes of this talk. So the idea is that the probability of Alice and Bob,
once we have the measuring settings and state of lambda, it goes like this. We can factorise
the probabilities. The other assumption, and let me just state it, we will come back to it
in a moment. The assumption is that the distribution over this complete specification
of the system that Alice and Bob are going to measure is independent of the settings that Alice
and Bob use to measure their respective particles. So Bell's theorem says that if one assumes
factorizability and assumes settings independence, then the expectation value for the product of
the results of Alice and Bob, which is this, is going to obey the inequality.
This is a particular inequality that can be derived. There are many others that can be derived.
The interesting thing, so far Bell's theorem just is a derivation of this type of inequalities,
but it becomes interesting when one notices that according to quantum mechanics, this combination
of expectation values should be equal to two times the square root of two and not less or equal than
two. We have performed the relevant experiments and we always find that
quantum mechanics seems to be right. So the conclusion seems to be that first, that no
local theory can reproduce the predictions of quantum mechanics and because of the experiments,
that it seems that the world has some non-local features. That is the overview of Bell's theorem.
Now we go to describe this assumption of settings independence and why it would be
why we would consider it a reasonable assumption. So as I said, the assumption is very simple to
state. It says that the distribution of the lambdas is going to be independent
of the settings that Alice and Bob select. Something that I forgot to mention is that
in the experimental setup, we are imagining. We want to be as general as we can. So the idea would
be that each time we prepare a system in a singlet, we are not going to assume that there is a unique
complete description that is appropriate for that situation. At much, we are assuming that each
time we prepare a system in a singlet, we have a distribution over the states of lambda.
And well, then settings independence is the idea that this distribution of whatever is really
happening is independent of how Alice and Bob measure the system. Two comments regarding
some confusions that have a reason regarding this assumption. The first one is that
this assumption has nothing to do with the free will of either Alice or Bob.
There is a long tradition of people being confused because of this and there is even
theorems regarding the free will of the particles involved.
That seems to be a confusion. It's easy to see that it is a confusion when one realizes that this is
just an assumption regarding the statistical character of the distribution of lambda.
Another confusion has to do with how to understand the lambda that appears at this moment. And the
thing is that lambda at this point must be understood as the complete description of the
singlet of the system that we have prepared and that we call the singlet. This is before I mentioned
when I was talking about the assumption of local causality that there is this complete
specification in the past of region. And some people think that lambda should be understood
as this complete specification of the hyper surface in the past of region. But in the theorem
lambda is not that. Lambda is just the complete specification of the system that Alice and Bob
are measuring. Okay. So why would someone believe that an assumption like setting independence
is reasonable? The answer is that we have performed the relevant experiments and we have
used a lot of methods to randomize how we choose either A and B. So we have used random number
generators. We have used the digits of pi. We have used information coming from distant
quasars. We have used even stranger things like the binary strings that can be extracted from
information of popular movies and TV shows. For example, we have used information coming from
the movies of Star Trek and Monty Python and the Holy Grail. And we have used that strings of binary
digits to select the settings that Alice and Bob are using. And finally, we have even used inputs
from a lot of people to perform these experiments. Why is this relevant? Well, the idea is that
by choosing the settings randomly, we are in effect erasing any kind of correlations that we
might expect that are present between the settings that we are using and the state of Lambda.
In the absence of some reason to believe that the correlation must be
preserved, we think that these kind of processes ensure us that the settings are being chosen
randomly. I hope that that is the main reason why we think that something like settings independence
is a good idea. And it might be obvious to some of you. It might not be obvious to some of you.
But an assumption like settings independence is present each time we perform an experiment
in the sense that we assume that how we are measuring something must be independent of the
system we are measuring. Let me be precise. We need to assume that it is possible to make those
measurements in a way that is independent of the system we are trying to measure.
Okay, still there are two possible ways of denying settings independence.
The first one is to assume a retrocosal model and to assume that it is the selection of the
settings by Alice and Bob that causes changes in the Lambda as well that we have prepared.
So the idea would be something like what is depicted in this diagram. The idea would be that
we prepare the singlet state and it moves towards Alice's lab, for example, and when Alice decides
or uses some device to select the setting A, that information travels backwards in time
and then reaches Bob. This would be a way to deny settings independence.
We are not going to talk about this possibility here. We are going to talk about the second
possibility of denying settings independence which has the name of superdeterminism and the
idea would be that there is a common cause between all the elements in the experiment
between the state of Lambda that we are preparing and whatever is happening in the locations of
Alice's lab and Bob's lab. Notice that because we have performed with these experiments using
information coming from distant clusters, the common cause must be really, really back in time
and it must be a really particular common cause because it is somehow preserving
the correlations between the settings and the state of Lambda even though we have used a lot
of different things to try to randomize how we select A and B. It would be something like
akin to having a system that undergoes physical mixing but preserves a particular correlation
even though there is a lot of intricate and ever pressing different physical processes
that we would think would erase any type of correlation.
These two possibilities seem to most people far-fetched. In particular, superdeterminism looks
really bad at least because of three reasons. It seems that a supereterministic theory would
be too complicated. Why? Because it would somehow be able to correlate these ways in which we
randomize the settings of Alice and Bob with the state of Lambda and that seems too cumbersome.
Second, it seems that doing this kind of move where we are saying that the correlation
will be explained by postulating that there is a common past is like abandoning the project of
finding an explanation of whatever is happening. Suppose we have a correlation between two
variables. It is always possible to say that the correlation was there because the initial
conditions of the system we are interested in were such and such. It seems that we are not
given an explanation. We are just assuming something about the initial conditions of the system.
Lastly, it seems that such a position would jeopardize all experimental science. As I said,
an assumption like settings independence is present in all types of experimental procedures
we perform. If we want to deny settings independence in this case, what makes us stop there? Perhaps
we should abandon this assumption and we should abandon the enterprise of doing science in general.
That is settings independence. Those are the reasons why we believe that it is a reasonable
assumption and what are the problems that we might encounter with super deterministic theory.
I will now present our model. Let me start by pointing out the trick that we are going to use.
We are going to turn standard pilot wave theory. If I am not going to give the details of this
theory, but during the questions perhaps it will come back again. We are going to turn this
standard pilot wave theory into an equivalent but a fully local model. We are going to do a trick
to do so. It will be useful to have a name for this trick. I am going to call it a lightning
hit. The idea is to put in each physical point in space the information regarding all the other
points in the universe. It should be clear that by doing that it is possible to transform the
dynamics of the model we are constructing in something that is local. The idea would be that
each point has the information to do whatever it does without the presence of any other particle.
In standard pilot wave theory we have two equations of dynamic equations. The first one
is just the Schrodinger equation and we have also the guiding equation that gives you the
dynamics of the hidden variables that theory proposes which are just particles and their
positions. We are going to use this lightning move and what we are going to do is to suppose that
at each point in space there is an internal set of internal degrees of freedom which are going to be
a wave function field and a position field for each point. We are doing so
it is possible to obtain a dynamic that is fully analogous to the previous two equations
but we are now dealing with wave function fields and the position fields.
Two comments regarding this move. The first one is the internal degrees of freedom at each point.
They behave as an n-particle pilot wave system so the idea would be that for each
point in physical space we now have an n-particle typical pilot wave system.
The second observation would be that the theory only contains local variables in the sense that
it is not describing some interaction between whatever is happening in between in these points.
The dynamics is completely described by what is happening at each one of these points and
therefore it is completely local. To connect this the model with whatever we find in physical
space we specify mass density. This allows us to say that a particular point in physical space
is populated by matter only if the particles in the internal degrees of freedom of such
point in physical space have a particle being occupied. It would be imagined that you have a
point in physical space there is the internal degrees of freedom and in those internal degrees
of freedom there is a particle that is at the same point that this physical point is then we
find matter there. Okay crucial observations. For generic conditions the model is we don't have
much okay. We don't have violations of the belt inequalities and we don't even have predictions
of the predictions of quantum mechanics okay but it is easy to see that under very particular
initial conditions that is if the the the wave function field and the position field are the
same for each physical point in space then we have three clear consequences. The first one is that
the points become suddenly coordinated and we have a dynamics that seems like
it is describing some universe where things interact causally but in fact we just have
points that are being coordinated. Second we are able to recover all the predictions of the
standard pilot wave theory and three where we have the violations of belt inequalities.
Two and three are related since in pilot wave theory we have violations of belt inequalities.
Okay I am not going to go into the details of this but the question becomes well how can we
achieve such an such an homogeneity condition and in the paper we discuss three ways of doing it.
The first one is by putting it by hand the second is by imposing it imposing it as some type of low
like constraint and the third one is by doing doing so dynamically and perhaps during the
questions we can talk a little bit about this but okay. Regarding the the problems that super
deterministic theories seem to have what can we say about our model well I think that it is
interesting to note that our model shows that it is possible to create a super deterministic model
that doesn't have the kind of problems that people feared. As it should be obvious the conditions
required to have a workable model are not as complicated as people thought they would be.
The second point is that well it has as much explanatory power as the standard pilot wave
theory has and the third is that the the the model shows that explains why settings independence
is violated for belt type experiments but it doesn't say that settings independence should
be violated in other types of experiments in science so there is no threat to experimental science.
There are some problems with the model the the model is really ugly we are not trying to argue that
this is a good way to do physics in the world we are just showing that it is possible to construct
this model and that it is a good place to continue having the discussion regarding whether or not
something like super determinism is a reasonable possibility. In more detail the the the model
seems to require an absolute rest frame and second there seems to be a disconnect in the
model between what we can call the the ontology of what is really happening according to the theory
and what what what we can have contact with so the theory postulates this internal degrees of
freedom for each point in physical space yet we cannot go and see those internal degrees of
freedom in fact the theory says that we cannot do that okay we because of the mass density we are
explicitly says when can we note something about all of this in internal degrees of freedom
and it's under very particular conditions. Lastly there is an obvious massive enlargement
of the ontology in this model. So okay let me finish with a list of contributions of that
this work has done and in particular that can be seen as consequences of constructing this model
so we have presented a model that is completely local and it is able to reproduce the predictions
quantum mechanics. It does so without being too complicated without losing explanatory power
and without making all experimental science obsolete so the problems with this model
were not the ones that many people feared at least there are not these three problems
so as a proof of concept it shows that it is possible to construct a super deterministic theory
in a way that keeps the physical details of what is happening and finally the model does this all of
this by very cumbersome methods and doesn't seem to be to offer any advantage over other alternatives
that are non-global. That's it.
Okay thanks Carlos thanks for this very nice talk we are almost on time so if someone has
a very short question you can post it now otherwise I invite Elias to start sharing cream
but if someone has a question for Carlos please go on.
I had of course questions but I think we are we are quite on time but my question was something
like we can of course speak later or by email or something but you claim that the world is
non-local right but then how can you have in a non-local world device measurements independence
for example you will claim that in belly inequalities you don't have measurements
independence or something like that because it is non-local and then in your definition of super
determinism I don't understand because if you want super determinism or if you want to avoid it
because when you post a model like in Leibniz everything is super there is no in Leibniz
metaphysics there is no measurements independence because the monads every monad
knows what to do and knows what the rest of the monads are doing so everything is super determined
by the the most perfect world chosen by god so it's not clear for me what what you at least
the conceptual definitions right what do you want you're completely right in the sense that if you
believe in something like the the universe of Leibniz's monads or the kind of universe
we're describing with this model yes that's a way to to deny settings independence and yes that is
a way to to to keep locality but I think that most people would agree that that's a price too high to
pay I think I think

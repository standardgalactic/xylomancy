OK, well, it's kind of like a hard task
to start after such a great talk.
I didn't get the whole talk.
It's the end of the semester here,
but it was very interesting what I saw,
and I'm going to sure read more about it.
So what I want to talk today is about negative probabilities
in quantum mechanics, and in particular with respect
to contextuality.
And I think it ties a little bit with the previous talk,
because you can think of that as a way of describing
certain quantum systems that's epistemic.
But at the end of the day, my interest
is in trying to rule out some possible ontological models
as well.
Anyway, so here's the outline of my talk today.
I'm going to start with trying to give some motivation
about negative probabilities.
And then I'm going to talk about how
is it that negative probabilities actually
come into this story through contextuality.
And then after that, I'm going to give a definition
of negative probabilities that is more formal than usually one
would find.
And there is a reason for doing that,
and hopefully the motivation is going to make it clear what
is the reason.
So the starting point is the realization
that quantum mechanics, as the previous speaker mentioned,
is a probability theory, in a certain sense.
So what we have in quantum mechanics
is a state for a physical system.
And that state is a vector in a hyperspace,
or a density matrix, whatever.
And we can use that state to infer outcomes of experiments
probabilistically.
So that's what quantum theory gives us,
is this probabilistic description of physical systems.
However, this probabilistic description
that quantum theory gives to us is not fully compatible
with classical probability theory in the sense
that for some set of observables for a quantum system,
if we look at the experimental outcomes that quantum theory
predicts, you cannot have a joint probability distribution
that describes all those observables
within classical theory.
We have kind of like a name for that.
We say that those theories are contextual.
And in the section about contextuality,
I'm going to outline a little bit more detail what we mean
by that for those who are not aware of that.
Probably all of you are, but anyway.
So since we cannot describe quantum mechanics
with classic probability theory, it's
natural to ask whether we can modify
classic probability theory to describe quantum systems.
So is it possible that some different probability theory
can actually help us get a better understanding of what's
going on in these quantum systems?
And there are different ways to modify classic probability
theory.
One of them is, well, actually, quantum theory itself,
where we use a probability that's
over vectors on hybrid space.
And that effectively modifies probability theory
in the sense that not everything is commasurable.
And the algebra of events that you have
is restricted to a lattice.
So you don't have the standard Boolean algebra
of all the events.
And you cannot give a measure of all the events
at the same time.
So that's one possibility is modifying
the algebra of classic probabilities,
because classic probabilities assumes
that the events form an algebra that's a Boolean algebra.
The other way is to modify the additivity rule.
We know, for example, that probability theory,
if two sets belong to this algebra of measurable events,
they are, we compute the probability of those two
sets, the disjoint sets.
Together, the probability of two disjoint sets
is the sum of those probabilities of those sets.
So that's the additivity rule.
So one of the things that we can do
is we can modify that additivity rule.
And that's something that comes out
of definiteities work on upper and lower probabilities.
And the other possibility is the one
that we're going to be following here,
is to modify probabilities such that we don't restrict ourselves
to non-negative values.
So those are ways of extending probability theory.
And so when we think about negative probabilities,
or probabilities where the non-negativity of probabilities
is denied, we need to think about it carefully.
And how exactly do we want to define that?
So what we're going to be trying to do today
is introduce a way to describe quantum systems that
violate these non-negatives, the negativeness property,
such that we have, first, a major theoretic formalism that
does that, and second, that we capture
the essence of what we want to have in quantum systems,
in physical systems, in that theory of probability.
So that's essentially what we want to do today.
So that starts with contextuality.
And I'm going to have a little bit
of a weird, non-historic approach to contextuality
that starts with a historic comment, which is contextuality.
Originated, the idea of contextuality
originates in language.
And it was well known in the 20th century
in works in pragmatics, for example.
But take the sentence, marry a student near the bank,
observing people.
We can understand the sentence differently,
depending on whether it's preceded
or it's followed by distinct sentences.
For example, take this very same sentence
followed by this statement.
She paused to notice a child amuse herself
with the fishes jumping in the river.
When we read that sentence and we read the first one
together of the second one about a child looking
at fishes in the river, we understand
that bank refers to the bank of a river.
But if, on the other hand, instead of this,
we followed marry a student near the bank,
with this sentence, she noticed the manager punctually
leaving at noon, something that would help with the heist.
We associate bank not with the bank of a river,
but we associate it with a financial institution,
the physical manifestation of a financial institution.
So in this example, the first sentence,
how we understand it, changes of the context
when it's put together with this child looking at fishes,
when it's put together with heist.
So that is the sense in which linguists
talk about contextuality of language.
And we can understand contextuality of language
in terms of a very logic approach to language, which
is the truth functional approach, in which propositions
are the way in which we understand meaning of sentences.
So like, for example, take the two propositions, A and B,
A being marry a student near the river bank,
and B being marry a student near the building
of a financial institution.
Those two sentences, they have different truth values
depending on the context.
In the context of river, A is true and B is false,
whereas in the context of heist, A is false and B is true.
So it's in this sense of truth values
that we can understand contextuality
and actually even relates contextuality in language
to contextuality in quantum mechanics
via random variables.
So let's look at how contextuality in this case
will show up in terms of random variables.
So we can think about those statements, A and B,
as being modeled by random variables A and B.
I'm going to represent them in both face here
just to distinguish between the statements A and B,
the propositions A and B. And these random variables A and B
modeled it by being dichotomous random variables
with values plus and minus one, with minus one,
for example, corresponding to the statement being false
and plus one corresponding to the statement being true.
It's kind of like silly to think about random variables
for this particular example because they are not,
they're deterministic random variables
because once a statement is true,
A is always going to be one, right?
And if B is false, B is always going to be minus one.
So it's kind of silly to think about
in terms of random variables, but there's nothing
that forbids us from doing that.
And contextuality here means that there
is no single random variable A and B that
represents those concepts A and B in different contexts
or represent those truth values in different contexts.
The reason is that a random variable
is defined within a probability space.
And the probabilities, they cannot represent both A and B
simultaneously.
So this is the contextuality that comes in linguistics.
It's, we can call this as a direct influence of the context
on the random variables because the expectations
of the random variables change directly with the context.
In one context, the expectation of A is one.
In another context, the expectation of A is minus one.
So the expectation changes the context.
This is not what happens in physics
because this type of contextuality
would violate the signaling condition.
But that's a way of representing the linguistic type
of contextuality in terms of random variables.
But once we have that idea of contextuality
exchanging the expectations of random variables
in respect to the context, we kind of
like have a clue of how is it that we can extend that
to things that might be more relevant to physics.
So let's take a more subtle case.
Let's take three that column was random variables x, y, and z.
So plus or minus one random variables.
And let's just assume that their expectation is zero.
So we don't know.
It's kind of like tossing a coin for x, y, and z.
We don't know whether it's true or false every time
that we observe it.
But they're perfectly correlated among themselves.
So x is perfectly anti-correlated with y.
x is perfectly anti-correlated with z
and perfectly anti-correlated with z and y.
z and y are perfectly anti-correlated.
And it's very straightforward to see
that if we have such cases where x, y, xz, and yz
are anti-correlated, that there is something funky going on.
And the way to see there is something funky going on
is just to notice that, for example, if x equals 1,
this guy here implies that y equals minus 1.
And if x equals 1, this guy here
implies that z equals minus 1.
But minus 1 times minus 1 is 1.
However, this guy here tells us that y times z
is minus 1, which is a contradiction.
So there is something wrong going on.
And what is wrong is that when we're
using this reasoning of x being implying y equals minus 1
here in this first equation, and then we're
using it as implying z here in this second equation.
And then, using to get the multiplication and third
equation, we're assuming that all three random variables
are defined together.
That means that we're assuming there is a joint,
probably with distribution.
But it doesn't exist.
Assuming that x, in the context of y,
is the same as x in the context of z
is what's leading us to contradiction.
Assuming that all three of them are observable at the same time
is leading us to contradictions.
And we know that this happens in quantum mechanics,
that there are things that we cannot observe at the same time.
So can we see something like that in quantum mechanics?
And the answer is yes.
A very famous example is the GHZ states.
We have three particles, 1, 2, and 3,
that are in an entangled state where either all three of them
have spin in the direction z being plus,
or all three of them have spin in the direction z being minus.
And then we have a superposition of those two states.
And then it's easy to see that these guys have
the following eigenstates.
For example, if we measure spin of particle 1 in the direction x,
measure spin in the particle 2 in the direction y,
and 3 in the direction y, the product of those three spins
is always going to be 1.
In other hand, if we measure y, x, y, it's also going to be 1.
y, y, x is also 1.
But x, x, x is minus 1.
So we can try to look at it in terms of random variables.
Let's say we have an x random variable for spin in the direction x,
a y random variable for spin in the direction y,
and then we assume them to be context-independent.
And if we do that, we can multiply this guy here,
the random variables corresponding to this spin,
x, y, y, times the spin y, x, y, times y, y, x.
And each one of those guys in parentheses are equal to 1.
And when we multiply them, we can now
collect all the y's together and all the x's together.
And the square of something that's closer to minus 1 is always 1.
And when we do that, this guy here is 1.
And we get a contradiction that 1 is equal to minus 1
from this quantum mechanical computation.
Of course, it's not a contradiction,
because what we are getting as contradiction
is based on the assumption that the y in the context of x and y, y2
in the context of x1 and y3, is the same as the y2
in the context of y1 and x3.
So in other words, we're assuming a joint probability
distribution.
So when we have a set of random variables that
are contextual, there is no joint probability distribution
for all the variables.
There is, of course, a joint probability distribution
for each one of those contexts.
But when you put all those six random variables together,
there is no joint probability distribution.
There is no single probability space, omega
fp, such that the random variables xi and yi
models all the quantum outcomes.
That's not possible in standard probability theory.
Of course, we know that the quantum formalism provides
an answer.
We can use the quantum formalism,
as we shown here, to compute the expectations
of actual observables that we can measure together.
But are there alternatives to quantum model?
And more importantly, are there alternatives
to the quantum model useful for us
to think about what's going on with the quantum case?
We don't know whether those alternatives are useful,
but we feel that perhaps understanding them better
might be a way to at least understand what is it
that they can bring to this discussion of what the quantum
model or quantum of what the hover space formula is
telling us.
So the alternatives that have been tried,
as I mentioned, are upper and lower probabilities.
And of course, there are negative probabilities
that we want to expand more today.
So let's go into defining negative probabilities.
To understand how we define negative probabilities,
it's useful to see how we define standard probability theory.
So the most accepted definition of standard probability
theory is by Komagorov.
He gave a set theoretic definition of classic probability
theory as being the triplets omega fp,
where omega is a sample space that you have.
f is an algebra over that sample space omega
that is actually a sigma algebra, a countable algebra.
And p is a function that takes elements of this algebra
to this interval 0 and 1.
And the restrictions that we have for p
is that the measure of omega or the whole set of possibilities
is equal to 1.
In other words, the probability that something happens is 1.
And the other assumption is that the additivity rule holds
for this function.
In other words, for any denumerable and this joint
family of elements of this algebra f ai,
the probability of A, say A1 or A2,
is the sum of the probability of A and the probability of 2.
So that's something that's a requirement of probability theory.
And because this is a number between 0 and 1,
this probability is monotonic.
So if you have a set A, the probability
of adding something more to that set A,
that's this joint of A, increases the probability
of absorbing that event.
So if you add more possibilities, this becomes more probable.
And in quantum mechanics, this is not quite the case.
It's non-monotonic.
But this is something that I'm not
going to talk too much about.
So how do we get negative probabilities?
We might be tempted to just violate these requirements
and we're done with it.
But the problem is that there are certain constraints
about negative probabilities that at least
seem to make sense, which is that first and foremost,
we never directly observe a negative probability.
What the heck does it mean to even observe a negative probability?
So we need to understand how exactly
does the negative probability show
in terms of directly observable quantities.
And observables are not parts of komagorov's talk.
So we need to include that.
And furthermore, there's no connection
if we just relax this requirement that
ties the marginals to the empirical observable contexts.
So what we're going to do here is just
try to show how we can define negative probabilities
with those ideas in mind.
So that's our major theoretic definition
of negative probabilities.
It's going to be a little bit abstract from now on,
but I'm going to try to give the main ideas of what
is the outlining of this definition.
We start with a sample space, not a sample space.
We start with the concept of a assigned measure.
So a assigned measure is exactly the same as a measure,
except that instead of having a measure going
from a sigma algebra to the set of non-negative real numbers,
a signed measure is something that
takes the sets of elements of the algebra
into any real number, including negative numbers.
And this is well known in measure theory.
So it's something that has been studied extensively.
And essentially, if we restrict the signed measure
to non-negative numbers, we get a regular measure.
And if we restrict this measure such that mu of omega
for this measure is equal to 1, we get probabilities.
So that's the starting point.
We start with a signed measure space that
has the additivity rule as part of it.
And then once we have this signed measure space,
we need to find a way to connect it
to the actual observations that we have.
And the starting point is to create an object that's
not quite a random variable, but it's
what we call an extended random variable.
Because instead of being defined over a probability space,
we're defining it into a signed measure space.
And the idea is just that if the signed measure space then
becomes a probability space, those random variables
become regular random variables.
And the definition is just the standard definition
of random variable, where you have a function that
goes from your sample space to some real number, m,
that's parts of a set.
This should be this m here.
There's nothing wrong here.
Anyway, and such that this function
is a measurable function in the sense of this measure
that we have, it's a measure-induced definition.
And then from this, we can start to narrow down
the idea of contexts.
And contexts are just proper subsets
of those random variables, such that there
is a sub sigma algebra for each one of those subsets
of the random variables that define a probability space
for those random variables there.
So intuitively, we'll have this bunch of extended random
variables.
And we have this measure space describing
those extended random variables.
But then we get a subspace of that.
And this subspace has a measure space associated to it,
such that we actually have proper random variables
in this subspace.
And this allows us to talk now about families of signed
probabilistic models that have just those contexts defined
for those families, and then measures in this context.
I'm going to skip a little bit faster in this.
And then once we do that, we can find
a concept of a general context that just
has the same idea of the broader one.
But now we're saying that those are just
for those guys that look like probabilities in the subspace.
So finally, we have those families
of signed probability models.
And those families of signed probability models,
what you have are those functions
that reproduce all the expectations
that you have on the contexts.
But it's kind of like the wild west here,
because they can be as large or as small as you want.
And what we need to do then is tame a little bit
those set of functions by saying, look,
if we now take the sum of all the probabilities,
the extended probabilities that we
have over the sample space for each one of those elements
of these signed probability spaces, we get a value.
And what we want are the smallest possible values
possible, because when that happens,
we're as close as we can be to a proper probability
distribution.
In fact, this is going to come up in a theorem later on.
And when we do that, when we ask for the minimum value
of the sum, then we call that a negative probability space.
It's when this minimum is obtained.
If we do that, then we have the following theorem.
Imagine that you have now this guy
as a minimum signed probability space
or as a negative probability space.
So if this m defined as the sum of the probabilities
of elements of the sample space is equal to 1,
then this is a probability space.
It's very intuitive to see why that is.
It's because if you remove the absolute value of this,
this is just the probability of omega, which is always
one for a probability space.
And what you're saying is that nothing
is pulling that probability higher or lower
because the sum of the absolute values
is the same as the sum without the absolute values.
There is nothing negative here.
It's all non-negative.
So this is also a probability space.
And alternatively, if this guy, it's
missing the both face omega here.
But if this is a probability space,
then it's also a signed probability space with m equals 1.
Additionally, a collection of extended random variables
in this space is contextual, if and only if m is not 1.
So we have here a criteria for contextuality
just showing up in the definition of negative probabilities.
I'm going to skip this because I'm running out of time.
But this is how you can connect it to quantum mechanics.
I've got to say there are lots of different ways
in which negative probabilities connect to quantum mechanics.
For example, the previous speaker mentioned about PR boxes.
As long as you have no signaling,
you can't describe any correlations
in terms of negative probabilities.
So you can describe PR boxes in terms of negative probabilities.
You can describe GHC in terms of negative probabilities
or Bell in terms of negative probabilities.
But a natural question that often comes up
in this discussion of negative probabilities
is what are the interpretations that we
can give to negative probabilities?
I'm going to talk about three possible interpretations.
Andrei Krenikov uses piadic matrix
to give an interpretation of negative probabilities
as a violation of von Mises' principle of stability.
So as we know, if you want to have a frequentist
interpretation of probabilities, what are probabilities?
And von Mises defines probabilities
as being over infinite sequences of possible outcomes
on the sample space, or possible values in the sample space.
And those probabilities are only defined
for those infinite sequences if they converge to a number.
So that's von Mises' principle of stability.
So what Krenikov noticed is that for certain sequences that
violate the principle of stability,
you can actually define probabilities for them
if you use a piadic metric.
And when you do that, the sequences
that violate von Mises' principle of stability
actually take negative values.
So that's the interpretation that he gives,
is that negative probabilities are associated with numbers
that violate the principle of stability, which in terms
of like if you are trying to understand, for example,
bell correlations, it means that those sequences that you
are assuming to be independent, they are not actually
independent.
I'm just finishing it.
Just finishing interpretations.
Thanks.
So another interpretation is proposed
by Brunsky and Brunnenberger.
It's actually based on the composition theorem
that shows that if you have a sine measure,
you can always break it down into two measures, one positive
and one negative, and each one of them independently
are non-sine measures.
And what they do is they just attach a sine to each one
of the outcomes of an experiment,
and then those signs, they lead to cancellation
of certain events.
There is a third interpretation, which
is the one put forth by Dirac and later on by Feynman, which
that's probabilities, negative probabilities for that matter,
are just a bookkeeping device.
So in that sense, negative probabilities
are nothing more than the same as we
do with negative numbers.
So for example, we never observe a negative number of apples.
In fact, it's no sense to talk about a negative of numbers
as an actual observable.
But it doesn't mean that using negative numbers of apples
is not a useful book counting device.
We never observe a negative number of dollars
for that matter either.
It doesn't mean that one never has negative numbers
in an account as a bookkeeping device for how much
money we have in an account.
And the reason why I'm mentioning numbers here
is because the concept of even using negative numbers
has not been always non-controversial.
Consider, for example, the following quote by the morning,
the famous mathematician and logician,
in a book that he wrote in the 30s.
And I'll read the quote to you.
Above all, he, the student, must reject the definition still
sometimes given of the quantity minus a,
that it is less than nothing.
It is astonishing that the human intellect should ever
have tolerated such an absurdity as the idea of a quantity
less than nothing.
Above all, that the notion should
have outlived the beliefs in judicial astrology
and the nonexistence of witches, either of which
is 10,000 times more possible.
So as late as like the 1800s, negative numbers
were still considered like an absurd idea by some.
And it might be the case that negative probabilities
will, in the future, be considered the same as we
consider negative numbers, something just natural,
like natural numbers are, as a bookkeeping device.
So just to summarize what I talked today,
as we discussed contextuality is a key characteristic
of quantum properties, we often use
Hubbard's space formalism to describe those quantum properties
and the Hubbard's space formalism has
built in that contextuality.
That contextuality cannot be represented in probability
theory unless you do something like, for example,
the approach of contextuality by default
by increasing the number of random variables.
But that goes against a little bit the idea
that we are doing physics because we
have the same measurement apparatus for some of those things.
But extended probability theories can be constructed.
And here, we just presented a measure theoretic framework
for defining an extended probability
that we feel captures the, at least, the gist
of quantum contextuality in the sense
that we can model quantum contextuality with that
probability theory.
And we hope that such tool may be used
when describing quantum phenomena.
For example, we know that it implies the no signaling
condition, which is part of quantum systems.
But it's still unclear whether it's useful or not.
So I just want to end with acknowledging my collaborators.
What I thought today is, in essence,
part of a paper that I did with Federico Halleck, who
is organizing this conference.
But I also benefited from a lot of discussions
with many people, in particular those people listed here.
So thank you.
OK, thank you, Jose, for your talk.
There are some questions in the chat.
The first one is from Shamsen.
All right, thank you for that talk.
My question is about, has to do with sort of rhetoric
and pedagogy.
So it sounds like we think very much the same way
about quantum mechanics as basically a new framework
for handling probabilities.
And I wonder whether it's the smartest strategy
to convince people of our view that you now
have to get used to the idea of negative probabilities.
I mean, you're very nicely laid out the difference.
Like you have contextuality.
But quantum mechanics doesn't allow you
to have joint probabilities.
You can't assign values to all the different variables.
I wonder if there isn't a more natural way of ramming that down
people's throats than going this route of introducing
negative probabilities.
Yeah, thank you.
I confess that I would probably never
teach negative probabilities to my students.
Well, never, I mean.
Maybe in the future.
That's not the reason why I'm talking about negative probabilities,
actually.
The reason why I'm talking about negative probabilities
is because I hope that by trying to understand
certain phenomena in terms of negative probabilities,
we might be able to get different insights
into those phenomena that we would not
get from the Hebert space formalism.
I can give you a specific example.
So when we look at different principles
that define quantum mechanics that people
have tried, like for example, Cabello
have tried to define some principles that give us
quantum mechanics, when you write them down
in terms of negative probabilities, they become clear.
Whether that's a useful tool for us
to understand what's going on in quantum mechanics or not,
I don't know because I didn't get any special insights from that.
But that's my hope, right?
That's what I'm trying to do.
I'm trying to see if we can understand
different types of quantum puzzles in terms of negative probabilities
such that we can get better insight from them.
If we do, then perhaps in the future,
we might even try to teach our students that.
But we're not there yet.
OK, thank you.
And there is another question from Alison Pesin.
Can you hear me?
Yes, I can hear you.
OK, so thanks for the very nice question.
My question is about Cauchy-Speccher theorem.
So how do you avoid Cauchy-Speccher theorem
in this kind of framework?
Because Cauchy-Speccher theorem rule out
the existence of evaluations.
And if we represent all measurements as random variables
in the same, sharing the same sample space,
a point of the sample space would be about evaluation.
So how can you avoid Cauchy-Speccher theorem
within this approach?
That's my question.
If I understand your question correctly,
you're asking how I avoid Cauchy-Speccher, and we don't.
Let me go back to the GHZ case, which is a so the difference
between GHZ and Cauchy-Speccher is that GHZ
is for a specific state, and Cauchy-Speccher
is for any state.
It's about the algebra of quantum observables.
But you could think of Cauchy-Speccher
as a series of measurements that prepare a state,
and then later on you get a contradiction.
That's a different way to think about Cauchy-Speccher,
but it's all there together.
But this is the GHZ also has a contradiction, right?
And the contradiction comes here.
From this guy in the bottom, this first parenthesis
being 1 times 1 times 1 being equal to minus 1.
It's the same type of contradiction
that you get on Cauchy-Speccher.
And you are using random variables
because each one of them individually are random.
But if you look at each one of those products,
if you thought of them as an individual random variable,
let's say we call this product A, we call this product B,
and we call this product C. What we have
is that A times B times C is equal to D.
But this is what quantum mechanics tells us.
But A is equal to 1, B is equal to 1,
and C is equal to 1, whereas D is equal to minus 1.
That's the contradiction.
And the random variables that we have here
are deterministic random variables.
So I don't know if I'm answering your question the way
you thought about it, but what I'm saying
is we're not avoiding it.
We have the contradictions there.
But the contradictions come from the assumptions
of Cauchy-Speccher.
And the assumption of Cauchy-Speccher
is that this guy here, say, y2 in this context
is the same as the y2 in this context.
And when we use negative probabilities,
that assumption is not true anymore
because negative probabilities are
washing out those requirements of how is it
that those random variables are related.
And then what you're asking about,
if you were to ask me how exactly they are washing out
those requirements, I would tell, wow, I don't know.
That's how we book keep those probabilities.
If you want to know how they are doing that,
then you would need to go to an interpretation that
is not a purely epistemic interpretation as in book
keeping.
And there's nothing that forbids you from doing that.
Does that make sense, what I'm saying?
Or am I being confused here?
Confusing.
Yeah, now it makes sense.
Thanks.
Thank you.
We are on time of the break.
But there is one more question.
It is very fast.
So perhaps I can make it if it's OK with you.
So it is just about the notion of contextuality
that you seem to be using.
I'm just curious if you are familiar with the work
of Speckens and this idea of using
a different definition of contextuality
than the one that seems to be used in Gohan Specker
and in Bell's proof of non-contextuality.
And if that makes a difference in the kind of project
that you are using to accommodate all of contextuality
using negative probabilities.
Yeah, that's a good question.
One could give a whole talk about different ways
of thinking about contextuality.
So I've been very casual about talking about contextuality
here and very specific about it.
But it's unclear exactly how those different notions
of contextuality relate to each other.
Sometimes, not all the time.
To be very precise, one would need
to prove what is the relationship between one
definition of contextuality and another definition
of contextuality.
And as far as I know, that has not
been done for all notions of contextuality.
But there is this intuition that they kind of like
are talking about the same thing.
Because at the end of the day, they're
all about commasurable properties
and how those commasurable properties somehow
are related to not having a possibility of talking
about all of those properties at the same time.
In terms of the intuition behind them.
I know I'm not answering your question.
And the reason why I'm not answering your question
is because I'm trying to say that I don't think
there is a very clear answer to that question yet.
But we hope that once we give a definition that's
a measure-theoretic definition, a precise definition
of negative probabilities that respond
to also a definition of contextuality in that setup,
that you can use that to prove theorems about how
those things are related.
It might be really, really interesting to explore that
for two reasons.
Because the kind of definition that specence uses
makes no explicit reference to the idea of measuring
two things at the same time.
It is kind of different to the general idea.
And two, because in the original paper,
I think that that's from 25, he has a small derivation
of showing that from his definition,
you can derive the traditional one, the one
that Cohen specular used.
So there must be a connection.
Yeah, yeah, yeah.
That's what I'm saying.
All those things are related.
But how exactly they are related, it's not.
Intuitively, yes, they should be related.
But it's one thing to think that they should be related
intuitively and to prove that they're related.
So that's what I was trying to say.
Thank you, Jose.
And if you have any questions, we
have some minutes to go to the bathroom
or take a coffee.
We will back.

Okay, we'll start with the second part of the first session this morning.
The first talk is in charge of Schuesserte Certioni from the University of Caglione.
Thank you, thank you very much and thank you to all the organizers who invited me. For me it's
very proud and very proud to be invited to this 10th conference of Quantum Foundation.
I'm very happy to be part of this very prominent panel of scientists. I warmly thank all the
organizers and especially Federico Olic and Martin Bosic, who are also a strict friend of
Italian University and of Cagliari and also personal friends. So what I want to introduce
here is Quantum Information Meet Smashing Learning. So basically I like to introduce
generally this topic by starting from a provocative question. I ask you can you show me a picture of
your Christmas 2018. I am sure that you can and I am sure that if you try to find a picture of your
Christmas for instance in your mobile you will find it but probably you need time to find it
just because now we use to make a lot a lot of pics with our mobile because the memory that
our mobile has now is very very huge and many many pics can be kept, can be considered, can be
stored in a cell phone for instance in a mobile. So what we can say is that we can
can we say that many data corresponds to many information? Basically not because information
means recover from the data what we really need. So the fact that now we have to deal with a very
very huge amount of information means that potentially we can have a lot of information
but the problem is the process we need to do in order to extrapolate this information
from this big amount of data. So this is the reason why in the last decades big data science
wants to face with just this kind of process this kind of problem and so the problem is
how to manage huge amount of data in order to retrieve in the short time as possible the
information we need. Okay so this is why big data born in the in the in the last decades and this
is just a very brief summary of what I said in these five minutes. So in other words,
enlarging the dimension of the data set is a good thing because increases the potential
information but is also bad because makes the extraction of the required information out.
So from another side you know very well that you know very well how quantum information
born, how quantum information became in the recent past a really new discipline of
quantum theory in general, quantum computing mostly, and we know that the advantage of
quantum computation are most of the the advantages are most of all based on the fact that we have
a speed up of the computation. We can obtain we can perform a computer and strong an art
computation in a very very shorter a shorter time with respect to a classical a classical one.
Okay so so we know also that quantum computation is not so futuristic we can say that till 10
maybe 20 years ago we were speaking about quantum computation as something that was
phantascientific that was not really concrete but we know that the application in quantum
computation has a very very speed up in the last in in the last decade and now we know that the
quantum processor already exists and we know that also by using for instance our our computer in
our house we can connect with the IBM quantum experience and we can just design a quantum
algorithm and by a click we can send this quantum algorithm to a real quantum computer
that will perform it that will give us back the result as a as a statistical result. So
basically the quantum computation is now real is now real and also we know that this
this topic is is going on and on and for instance just one year ago just one year ago in
in china that was there was was declared that the achievement of quantum supremacy
because it was possible to perform a calculation in in a 200 second a kind of calculation that
will take more than half a billion years on the world's faster no quantum or classical computer.
So now we really trust in quantum computation really trust in this so we were speaking about
big data and about quantum computation but because well summarizing the main problem of big data
when machine learning with big data is the time complexity of the computation but the main
advantage of quantum computation is the possibility to reproduce to reduce the time complexity. So in
a certain sense quantum computation and machine learning appears as a very natural and maybe
unavoidable unavoidable connection but the connection can be deeper and deeper with respect to what
we can think. Indeed the question that they want to ask you in this in this talk is can quantum
information theory help classical machine learning classical by using classical computer only
and we will provide a positive answer to this to this question. So this approach that we will speak
is known as a quantum-inspired machine learning and the important thing is that well very often
when we know that in many disciplines we speak about the inspiration from quantum mechanics.
Okay now what I want to focus on is the fact that our inspiration is not just an analogy
it's not a metaphor but it's just a really inspiration regarding a real inspiration to quantum
world in order to have a real benefit in machine learning and in the next minutes I
I aim to show you what they mean by real inspiration. Okay in particular we focus on the
classification problem. What is the classification problem? Okay the general idea is that the quantum
classification in the idea that we want to perform is the idea of a quantum classifier
where quantum classification is the idea of quantum classification is to formalize the standard
classification problems in terms of the mathematical quantum object and then
inspired by certain properties of quantum state discrimination we define a quantum classifier
that will provide interesting and real good advantages. The procedure is based on three
fundamental steps that we will describe not now but a few slides later encoding classification
and the coding but we will meet this slide again in a bit. First of all I want to very very quickly
summarize what is the the general framework of classification problem. So when we have to classify
an object we describe this object as a vector where each component of this vector is a feature
of this object like this object could be a cat and this feature could be the length of the tail
the weight of the cat and so on and we have different classes of objects for instance
dog and cats and what and so and this so a pattern is represented by a pair where this is the vector
the object represented by a vector and this is the label that characterizes the class for instance
class for instance label one means class of the cat level two means class of the dogs for instance
okay and the goal of the classification is to have a classifier I mean a function classifier is a
function that allows us to study the vector and gives us an output the label so looking on the
vector looking at the object I can say you if this object is a cat or is a dog basically okay okay
okay so in a general supervised scenario I will be very very very I will skip many slides but about
some time I can focus on some technical data sometimes not generally generally in a super
when we say supervised learning we mean that we consider a data set for instance a data set of
dogs and a data set of cats for instance and we divide this data set at the beginning in two
parts the first part is given by the training set this is the set to usual in order to train our
algorithm and this is the test set the set is a set of object that we use in order to know
whether our algorithm is good or not just in order to know just in order to measure the
performance of our algorithm and so obviously then all of these objects are already labeled
so we already know the level of the object so the goal is just to evaluate how the right if the
classifier is good or not so we use a already labeled set of object we divided in training
and test and obviously also the training is divided with respect to the classes the classes
of cat the classes of dog the classes of foxes the and so and so okay so the the first idea of the
quantum classification is just to translate formally translate each object each vector each cat
that is represented by a vector in terms of a quantum object that we represent by a density
operator raw and there are many many ways to encode this is just encoding to encode a real
vector in terms of the quantum object okay in terms of a density operator obviously it is only
a formal translation we are not transforming a cat in a photon or a dog in an electron obviously
okay so now once we have translated all the data set of real data in terms of the data set of quantum
data or quantum data again we can consider the distinction between
training and test training and test again and the idea is just to obtain a quantum classifier
that is a function that has as input a quantum object of the quantum test set and has the
output level so given this quantum object that is the density operator that represents
an unknown object we can say that we can we can obtain by the classifier that this object
is a dog okay so obviously what we have to show is that this translation from classical to quantum
object has some benefit provides some benefit otherwise it's totally useless okay so okay
now let us talk regarding the general setting in quantum state discrimination okay that probably
most of you already know but I very briefly summarize here let R be a set of density operator
and suppose that Alice wishes to communicate information to Bob by using a quantum system
that is one of the system belonging to this state okay to the same Alice prepares a quantum system
in one of the state in one of the state and hence the system over to Bob in principle Bob has
a complete knowledge about R so Bob knows in knows already knows which are row one row two row
M but he does not know the actual state of the system that he have received by Alice so in order to
so that the Bob's task is to determine which one is among the state in one shot just by making a
single von Neumann measurement over all possible sorry or possible physical observable so there
and so the possibility is that the measure the outcome of the measurement could be just the
row that Alice sent to Bob or not or can be another one in the first case okay we say that Bob
succeed in another case we say that Bob fails okay so what is the probability for Bob to
succeed okay given a set R or quantum state and a von Neumann measurement M the average
probability to Bob for Bob to perform a correct discrimination so to correct individuate which
is the state that was sent from Alice is given by this quantity is given by this quantity and
the aim of course is that maximize maximize this probability so to minimize the probability to
have an error okay so the question is what what is that measure the measure that maximize the
probability to have the correct discrimination and this measure this measure was obtained by
Elstra by this strategy okay the strategy is is related only to the binary case so consider to
have just row 1 and row 2 and if we consider this quantity p1 row 1 minus p2 row 2 where p1 and p2
are a priori probability okay that can be obtained in some different but empirical way for instance
we obtain this quantity we calculate the positive and negative eigenvalues of this quantity
we consider the respective eigenvectors and we consider the sum of the projector
built over all of each of these eigenvectors in this way we can obtain p plus and p minus
and Elstra showed that this p plus and p minus is a von Neumann measurement that is optimal
that is the optimal measure for the discrimination problem for the binary discrimination problem
that we have introduced at the moment so this is the this is the bound this is the bound and
intuitively p plus and p minus represent the property for the system to be correctly identified
as being the state row 1 or row 2 respectively okay and that's from bound can be seen as a
measurement of distinguishability between row 1 and row 2 so now again with these lines because
now we have all the ingredients we need in order to put together this not to put together
classification problem and quantum state discrimination the first stage is the encoding we
say that we need to have real object and translate the real vector and translate them in terms of
quantum object like a density operators like a pure states and for instance it is easy to see
because there are several ways several ways to do this this is one simple way for instance by using
the stereographic projection we map each object in this simple case two feature objects
in a point over a sphere and of a surface one sphere and that's correspond to the point of the
block sphere so a pure density matrix a pure state a pure state but there are many other
possibility to encode real object in terms in terms of density matrix pure density matrix okay so
once we do that once we have set of quantum states set of density operators for each
training set we for each training set for each class of the training set we we represent each
class by a quantum centroid that is defined in this way this is just the beginning of our
algorithm and it is no longer a pure state of course because it is just the sum of all the state
of all the encoded vector belonging to a to a given class to a given class okay obviously the
quantum centroid are mixed states are mixed states and they are not the encoding of the
respective classical centroid they are a totally new object a totally new object that
has not any counterpart in the real world in the world of real objects in the world of real
vectors okay okay so now we use quantum state discrimination we use the Elstrom measurement
that is optimal so we use p plus and p minus that we built we built by considering
as the two states to discriminate we will consider the two centroids so let us consider
a case where we want to discriminate between cats and dogs we consider the training set of the cats
we are encoding this set in terms of quantum object we consider this you can see that the
centroid of this class we do the same for the dogs and we have these two centroid these two
quantum centroid these two quantum centroid define the finding are two density operator
and from this from these two quantum centroids we can apply the Elstrom
discrimination and we can obtain the Elstrom measurement the optimal Elstrom measurement
p plus and p minus okay so once we pick another quantum object from the test set we have to do
we have to say okay this quantum object is a set or is a dog in order to determine if this
we consider this classifier that is called the Elstrom quantum classifier we consider
these two values if the trace of p plus times rho is greater than p minus times rho we say that
in a certain sense rho x is closer to the centroid of the cat otherwise rho x is closer to the
centroid of of the dog and so we can make a really a proper classification a proper classification
okay why we use quantum state discrimination what is the intuition that that is above this idea
the application of quantum state discrimination for classification is inspired by the idea that
better is the discrimination between two centroid and better I can distinguish between two classes
and so more performance is the classifier in other words greater is the Elstrom bound that
remind me I said it is that can be considered as a measurement of distinguishability between
two centroid and greater will be for instance the accuracy of the classifier where the accuracy
means just the number of times that the classification was correct over the number of
total over the total number of experiments okay okay this idea is not only an intuitive idea
but that was just exploited that was just proved by an experiment so we
have provided an experiment where this classifier has been applied to 40 different
data sets and compared with 11 different and general well performing classifier standard
classifier for instance what we can see is that generally generally once the Elstrom bound increase
that's somehow increases together with the increasing of the balance of the accuracy
so this idea seems to be correct that increasing the measurement of distinguishability between
two centroid it is together with the increasing of the accuracy of the classifier so this intuition
is corroborated by the experience is corroborated by the experience and okay we have many data when
we perform and when we compare this classifier this Elstrom classifier with many standard classifier
and we show that our classifier is generally better not always not always but okay the average
performance of the quantity spiral the classifier this Elstrom classifier is very very high with
respect to the other standard classical classifier okay now I have many data to show but
okay this is just a comparison okay I we okay in these data in these tables you can realize these
these I don't say supremacy but something like this of our Elstrom classifier with respect to the other
but that's not all that's not all indeed a sharp difference between classical and quantum
information theory is also based on the fact that sometimes made considering a tensor copy
of an object can provide a benefit in our computation can provide let's say a kind of
additional information with respect to the with respect to the original information
that is given by the initial single state so what we do is just repeat exactly the same
algorithm that I have described you above before but by considering but often after the encoding
after the encoding instead of the density operator row that I obtained from the encoding
by the encoding from the initial object instead of this row I consider row times itself and times
I calculate a new centroid and I define again a new classifier in a new Elstrom of sample in the
new dimension has been a new k dimensional space in a new larger space and again I define a new
Elstrom quantum classifier let's say k tensor product Elstrom quantum classifier what we see
what we see is that by increasing the number of the copy also the Elstrom bound increase and so
we see that the accuracy that we obtain after making this copy this copy in a just in a computational
way just by making it in bi-mathematica okay or by fighting this copy but what we see is that by
making this procedure we have benefit because we increase the accuracy of the of the process we increase
again the accuracy of the process okay and again we have data to to show okay we have something but
okay I want to use just the last five minutes talk maybe to show you a practical experiment
a practical experiment that was provided together with the University of Cambridge
and the Institute of Molecular Bioimaging of the University of Catania and the topic is
the chronogenic assay the chronogenic assay is a quantification technique of the survival degree
of an in vitro cell cultures which is based on the ability of a single cell to grow and form
a colony of cells this colony is not a good thing that is a symbol of something is a of something
this is something great this is the purpose is to count the number of the colonies so the picture
that we have today like picture like this where this is a colony this is a colony this is a colony
and so on okay recent results show how bio classification between pixel x belongs to the
colony or pixel x belongs to the belongs to the background by making this classification
it is possible to have information about the number of of the colonies there is a correlation
between this this is a biological result in a in a quantum image in an imaging and biology
feels that this result but it is it is an assist for us because it allows us to move
the chronogenic assay in a binary classification context so for each pixel if we are able to
classify if this pixel is in the colony or it is in the in the background we can have some result
regarding the number of the comb that is in the clonogenic assay and so
you have 10 minutes including the portion okay so just I thank you I just spend a few minutes
to conclude and I and I leave a few minutes also for four questions okay okay so the experiment
involved the many many many data about 10 millions of data and the result that we had
was against that the performance of our elstrom classifier with respect to other in this case
18 well performance classifier is very good because our classifier in most of the cases
was one of the best one of the best with respect to to the other and that was very very nice for us
so let me conclude with some open problem okay the first open problem is the is this
the generalization of this quantum is pirate quantum is pirate and now we know why I say quantum
is pirate and now we know why I really say that this this inspiration is useful it's not only
a matter of metaphor but this quantum inspiration was based on only a primary quantum state
discrimination so the challenge is to have a multi-class classifier a multi-class classifier
and the second goal should be just to use this quantum inspired algorithm also in a quantum
computer just in order to come from one to be spirited to one so we have some partial result
regarding both of these this point and basically regarding the idea of multi-class classifier
we are working on this but we are we are taking inspiration from the pretty good measurement
that is not an optimal but a sub-optimal measurement that allows us to have a multi
a quantum multi-classifier a quantum multi-classifier and on the other hand well I am not
time to show you the detail of the pretty good classifier but the idea is the same
but instead of to have an optimal we have a sub-optimal measurement but but also we this
sub-optimal measurement can be used for n-classification instead of only binary classification
and also the final challenge as I said is just to come from a quantum inspiration to really
quantum machine learning in order to put together the double benefit of quantum so increase the
accuracy but also speed up the computation well by appealing to the neomarks deletion theory is
it possible in principle to reproduce and POVM measurement and for instance also the
health strong of the pretty good measurement this is an example but we are working on this
until now we have only only partial result regarding this but the works are still in progress
and okay this is the the bibliography that the reference that you can refer to and okay thank
you all for listening thank you
okay thank you Shusepe for your interesting talk now we'll have some minutes to
make questions and comments if you want to make a question please and
take it in the chat
I have a little question
for your future back so you know if there is a mathematical relation between the
quantum strong bound and the accuracy of your classifier
okay so yes are you asking if there is a connection between health strong bound and accuracy
right okay well basically not basically not because it's very strange because
there are really two topics that are that was never merged together between with within
before now because in a certain sense health strong was not thinking about the classification
problem so in a certain sense health strong well quantum state discrimination
um has not the idea of what is the accuracy because the accuracy could be calculated empirically
empirically only by making the experiment what what the theoretically we have is this result
is this result that is interesting regarding delstrom bound and it is easy to misleading
because easy to misleading because we are not saying that if okay we are not saying this delstrom
bound between two states is less of delstrom bound between the states that you obtain by
making the tensor copy of itself by itself because in this case it is an already well known result
here we are saying this something different because this object our quantum centroid
this tensor k row is not row times row times row k times k times it is something different
is the centroid that we obtain by making in principle k copies of each object of the
data set and after making the centroid so in a certain sense this is a first let me say that
theoretical result that put together quantum state discrimination and classification problem
because there is the idea of centroid and there is the idea of extra bound put in together
I hope to answer to your question thank you very much welcome
another question or comment
if not thank you again for your talk thank you all

Hello, everyone. Welcome to the Mindscape Podcast. I'm your host, Sean Carroll.
When you think about how we conceptualize human beings, someone once pointed out that
we're always using metaphors that depend on our current best technologies, you know, when clocks
were just invented, wristwatches and so forth. It was the clockwork universe when robots and
machines came on the scene. We thought of organic beings kind of like that. And now we have computers,
besides which we have, you know, cameras and video cameras and audio recorders and so forth.
So we tend, very, very roughly, you know, we tend to think about a person as kind of like
a robot with some video cameras for eyes and audio recorders for ears hooked up to a computer inside
and the sensory apparatus brings information into the computer, which then tells the robot
body what to do. It's a simple, kind of straightforward, compelling picture. It's also wrong.
That's not actually a very good description of what we are, how we behave. For one thing,
intelligent design is not the way that human beings came about. We evolved over many, many years,
and we weren't aiming for that. You have to think about what is the kind of architecture that actually
best serves the purposes of surviving and procreating and reproductive fitness and so forth. And it
turns out to be very different. So today's guest is Andy Clark, who is a philosopher and a cognitive
scientist. In fact, his title at the University of Sussex is Professor of Cognitive Philosophy,
very well known in philosophy, very, very highly cited for thinking about the brain and the mind
and how they're related and how they work. He became very famous with a co-author paper with
David Chalmers, where they proposed the extended mind hypothesis, the idea that what you should
count as your mind is not just your brain, but also all of the little extensions of the brain
that help us think, whether it's inside our bodies, or whether it is things we scribble down on a
piece of paper or used to enhance our memories or calculational abilities and so forth and so on.
He also has a great interest in the idea of the brain as a predictive machine, and that is the
subject of his new book, The Experience Machine, How Our Minds Predict and Shape Reality. So the
idea here is that the brain is not a computer just bringing in sense data and then thinking about it.
Our brains are constantly constructing a set of predictions for what's going to happen next,
what is going to be the situation in which the body finds itself, what is the sensory data that
we're going to bring in, and then you compare what you're actually experiencing versus what
the brain was predicting, and you try to play the game of minimizing the error between what you
predicted and what you were actually perceiving. This sounds like maybe a small change of emphasis
or an angle and a similar kind of process, rather than the brain just being a passive
receptor of information. It is sort of actively engaged in a feedback loop, but it has very,
very significant consequences for how we think about thinking, how we think about fixing thinking,
right? When we go wrong one way or the other, whether it's being in pain or having a mental
disorder of some sort, how do we get better at it, taking seriously how the brain is a prediction
machine is very useful here, as well as for philosophical problems about how you carve
up nature, how you think about what the brain does, what is consciousness, what is free will,
and so forth. So this is one of those podcasts that touches on many issues that we're interested in
here at Mindscape, from consciousness to time all over the place. And occasional reminders,
you can support Mindscape by pledging at Patreon. Go to patreon.com slash Sean M. Carroll and pledge
a dollar or two or whatever you like for each episode in return, sense of fulfillment, but
of course, also add free versions of the podcast, as well as the ability to ask questions for the
Ask Me Anything episodes. You can also, if you're interested, leave reviews of Mindscape at Apple
Podcasts or wherever there are reviews. Those reviews help draw other listeners in. So if you
think Mindscape's worth listening to, make it an even bigger community listening, and that would be
awesome for all of us concerned. So with that, let's go.
Andy Clark, welcome to the Mindscape podcast. Hey, it's great to be here. Thanks for having me.
You know, you've done, as many people have of a certain age, many interesting things over your
career. Your new book is the Prediction... What is your new book? What is the title?
It's the Experience Machine. The Experience Machine. I keep wanting to say the
Prediction Machine, because obviously, prediction is playing a big role there.
The Experience Machine with how predictions shape and build reality.
Good. Good. Yeah, reality will be a theme that we want to get to. But I can't give up the opportunity
to also talk about extended Mind and extended Cognition and things like that. So I thought it
would make sense to talk extended Mind first and then get into prediction. Does that make
sense logically to you? I think that's a good route. That's certainly that was my route. So why
not? Good. Good. Let's do it. So apparently, there are people out there who think that... Well,
there are people who think that the Mind is not even related to the Brain, which is funny to me.
But there are other people like yourself, perhaps, who think that the Brain is just one little part
of our minds and our thinking. So I'm not going to put words into your mouth. What does it mean
to talk about the extended Mind? Yeah. I mean, this is a view that I've kind of held and defended
for many years. It goes back to a piece of work that I did with David Chalmers, is famous for his
kind of almost dualistic views on consciousness after all. Many years ago, back in 1998,
the basic idea that Chalmers and I agreed on is that when it comes to unconscious cognition,
then there's no reason to think that the Brain is the limit of the machinery that can count as part
of an individual's cognitive processing, where that has to include unconscious processing because
it's unconscious processing that we think mostly is what gets extended. There's a whole other debate
about conscious processing. And the idea there is that moment by moment, the Brain doesn't really care
where information is stored. It cares about what information can be accessed, how fluidly
you can get at it, whether or not you've got some idea that it's there to be
got at at all. So the idea was that calls to biological memory and calls to external stores,
like a notebook or currently nowadays, maybe a smartphone or something like that,
are working in fundamentally the same kind of way. And actually, interestingly,
it's that that I think the predictive processing story ends up cashing out in an
interesting way. So we can circle back around to that later. That's the core idea is that the
machinery of mind doesn't all have to be in the head. David Chalmers, of course, another former
Mindscape podcast guest. So we have an illustrious alumni base. But let's make it a little bit
more concrete. What do we mean? I think that what immediately comes to mind is I can remember almost
no phone numbers now because they're all in my smartphone. Does that count as extended mind?
That counts. I think it's after all, you have a fluent ability to access at least a functional
equivalent of that information as and when you need it, because you just pick up the phone and
there it goes. If on the other hand, you had all those numbers stored in a notebook, but the notebook
was in your basement, and you had to run down into the basement to get it whenever you needed it,
that wouldn't count because you wouldn't have the constant robust availability of that resource woven
into the way that you go about every kind of problem that daily life throws at you.
So there is, I think, a genuine intuition, which is that whatever the machinery of mind is, it
better be more or less portable. It better more or less kind of be going around the world where
you're going around the world, where bio you is going around the world. And so for that reason,
I think you need robust and trusted kinds of access, but only to the extent that biological
stuff is robust and trusted. Now and again, my biological memory goes down. Now and again,
I don't trust what it throws at, but it should be more or less in the same ballpark.
Well, okay, so robust and trusted access. Is that the criterion? I mean, I guess an immediate
question that comes to mind is, where do I draw the boundary? Does everything in the world count
as my mind? Yeah. I mean, it's one of the original worries about this view. It's one that we used
to call or I used to call cognitive bloat. Why it somehow there's no good way of stopping this
process. But actually, if you take that robust availability and trust of the product stuff
reasonably seriously, that does rule an awful lot of things out. But I think in addition,
there's something which I've never, I've never given a full account of to be honest, but there's
something about the sort of delicate temporal nature of the integration of the biological
system with this non biological proper aid or resource or whatever it is. So that you can have
the brain, if you like, making calls to this stuff in ways where it kind of knows about the
temporality of that. It doesn't all have to go through a little bottleneck of attention. You
don't have to kind of think to yourself, oh, I'd better get that information from over there.
That's not what it feels like to go about your daily things as you. So it's this sort of idea
of something where if it were taken away suddenly, you would feel very much of the loss, a sort of
kind of general purpose loss, there'll be many situations that you'd find yourself in,
where you were no longer able to perform in the ways that you expected yourself to perform.
So there's, there's something there that could do with a bit more unpacking about temporal
dove tailing. Does it relate to ongoing conversations that I'm very interested in about
emergence and strong and weak emergence or just being able to coarse grain the world in such a
way that you have units that have explanatory power? Yes, I'm well, I think it certainly relates to
a lot of stuff about emergence, because the idea here would be that by dove tailing things together
in this way, you get basically an emergent you, you are the emergent property, you the thing that
goes around with a sort of sense of its own capacities, but doesn't really care how they get
cashed out. So something that I think comes out quite strongly in the predictive processing stories
is that brains are kind of location neutral when they, when they estimate where they can get good
information back from. So they're very good at estimating what they're uncertain about and
going about getting more stuff, but it doesn't really matter where that stuff is. So as long as
it's accessible, trusted, there when you want it, that's fine. And then there'll be a story to tell
about how the actual, how the actual selection process operates, but we'll come around to that
once we talk about predictive processing. Let's stick with TXM. I mean, I guess one way that
philosophers think about their job is that they're supposed to be carving nature at the joints. And
is that, does this count? Is that this? Is it, is it a sort of betrayal of that goal? Because
you're including all my smartphones and my, you know, card catalog as part of my mind, or is it
an improvement of that? Because really the action of our brains relies on all this stuff.
Yeah, that's a very, very, very good and very delicate kind of question. So, you know, when,
when Dave Chalmers and I put it forward, we tried to argue that this was,
this was where the right way to carve nature at the joints because of functional similarities.
That said, you know, it doesn't intuitively always seem like the right way to carve nature at the
joint. So I think that we're making a decision here, and that that decision, or the enough is
in part a moral decision. So, okay, I, I'm very much moved by the analogy with prosthetic limbs,
for example. If you took somebody with a well fitted prosthetic limb, and then looked at them and
said, but you know, your basic physical capacities are just those of you without the limb, we would
in a way be doing them a certain kind of injustice. And I think we've done ourselves a certain kind
of injustice. If we treated, people say with mild dementia that rely very much on a, on a smartphone
or on their home environment, in just that kind of way, we would be, we would be sort of regarding
them as less, I think, than they really are. So for me, the sort of non moral route, the kind of
functional similarity route is a kind of stalemate, because it's kind of carving nature at the joints,
and it kind of isn't. And then it's when you chop the moral or ethical considerations into the pot,
that I think you have a strong argument that, overall, the best way to go is to air on the
side of caution, and buy into the picture of the extended mind. I bet that depends on one's
definition of caution, but is there intermediate, there probably is an intermediate stance in which
your whole body, your physical body counts as your, what goes into cognition and the mind,
but, but things outside the body don't, is that a well populated space in the thoughts about this?
Yes, I think that is reasonably well populated. I personally, I think it's an unstable space.
So I think you better go one way or the other. I think that, you know, as soon as you start
thinking, okay, if I'm using my fingers to help me count, then my fingers somehow count as part of
the machinery of mine. A lot of people do feel that, but I think that's true. But I think you
can't think that, and then simultaneously you think that the constantly carried pocket calculator
can't count as part of the machinery of Yom. So, so yes, I think the body is often, for me,
it's a very useful step in stone, because I can suck people in by getting them to agree
that continuous reciprocal interactions between brain and body do a lot of what really looks
like cognitive work. And you know, there are, there are good examples of that from even things
like the role of gesture as we speak. So spontaneous gesture by Susan Golden Meadow and colleagues
have shown that it kind of eases a certain sort of problem solving. If you're, if children are
forced to explain how they solved the maths problem, but not allowed to gesture, they're
actually a lot worse at it. So there's something, there's something going on, continuous reciprocal
interaction that is probably allowing us to somehow offload aspects of work in memory into
these gestures. Yeah. Stick what they're in the world, which is good news for philosophers that
worry about hand waving. It's just part of my thinking. Respectable cognitive task going on
right there. Can we ask how this was different back in the day? You know, obviously, a lot of
the things we've been talking about are technological or modern. So was, was the mind of a human
being 100,000 years ago very different? I think I would say it was, despite the fact that the brain
of a human being 100,000 years ago was a very right. So yes, I would say that the mind of
100,000 year old humans was very different. But the brain obviously wasn't. But think of
intermediate technologies before maybe we go back that things like just using pen and paper.
And in particular, I'd like to think about the example of sort of scribbling while you think
the thing that many of us, those of us at least old enough to be brought up with pen and paper,
do an awful lot. And so as we try and think a problem through, we write things down.
I think it was Richard Feynman, who has a rather famous exchange with a historian,
Weiner, I think the historian was, where Weiner said these, so this is a record of your thinking.
And Feynman said something like, no, it's not a record. It's, what did he say it was? It's not
a record. It's working. You have to work on paper. And this is the working. And I think there he
had an intuition that this is a just offloading stuff onto a different medium. This is actually
part of a process that solves a problem. Do we see this then in other animals? I mean,
I presume the answer is yes. But is it more or less dramatic depending on the species?
Yeah. I mean, I think we humans do this to a really ridiculous degree. And it's because we
invented symbolic culture somewhere along the way. We came to create these arbitrary systems of
recombinable elements that we could then, you know, re-encounter as objects for perception.
So creating these sort of structured encodings of in some sense, our thoughts and ideas, and putting
them out in the world is a huge opportunity for any creature that is a good information forager,
any creature that is very good at reaching out into the world to resolve some of its current
uncertainty. And once again, that's a theme that we'll look back to later on. So humans do it a
lot more. Other animals do it too. I don't think it's a purely human thing. It didn't just suddenly
arise. You know, if you have a chimpanzee or an orangutan, better example, that constantly
uses a stick to gauge the depth of rivers before going into the rivers, I think there's enough
of a sort of weaving and robustness for that to count as part of a sort of extended cognitive
load. I guess I'm very interested in phase transitions, you know, small changes in things
that allow enormously different capacities, etc. Clearly, the ability to think and communicate
symbolically was a big one. It opened up things. But I guess what you're saying is one of the
things that opened up is not just we can think more abstractly or generally, but we can think in
literally different ways because these symbols allow us to more easily offload some of our cognition.
Yes, I think that's right. I think that one thing we can do is we can use one of our biological
capacities in very, very different ways and that capacity is a tension. So when we put something
out into the world in that sort of way, kind of clothe it in materiality, that lets us attend to it
in a whole bunch of different ways. And interestingly, I think by doing that, we can often break the grip
of our own sort of over constrictive internal models of what kind of thing it is and how it
might behave and how you might reengineer it. So even think about something like designing a new
training show or something, if you build a big scale model, and then you start looking at it and
poking it and prodding it, you can come up with all kinds of ideas that you wouldn't come up with
just by kind of sitting down and trying to imagine all that in your head. So I think we're unusual
in that respect, but it's a great power of us being perception action machines that we can
really make the most of this. And then you look at current more disembodied AI, they've got really
no use for this kind of strategy. The way they work is just not such that they're going to start
putting stuff out into the world and then poking and prodding it just to think better. They might do
that for all kinds of other reasons. But is that their fault or ours? I mean, we haven't really given
the capacity to do that. I think that's right. I think it's our fault. I think we've, to be honest,
I think, well, it's our fault. We've designed a class of tools that are really, really useful
for what they're useful for. But I don't think actually that those tools will ever achieve what
I would call true understanding, because they don't have their grip on reality grounded in
perception action loops. And it's that grounding in perception action loops that I think we're
leveraging with material symbolic culture when we just create these things and poke and prod them
in different ways. We're super specialized for being good at perception action loops. So I think
that's, there's a trick there that disembodied AI is missing out, but then it gets all kinds of
other things instead. Sure, sure, it does. But okay, I'm going to, I'm just going to download a
little bit because what you just said sparked several ideas and it's prefiguring things we'll
get to later. But two things, one, an idea I heard more than 10 years ago that attempts to make AI
systems suddenly work much better when you put them in a robot that can go out there and interact
with the world. And secondly, a podcast I did with Judea Pearl, who claims that babies spend all of
their time making a causal map of the world by poking at things and seeing what happens. And I
know that the whole prediction models that we'll be talking about are kind of this, like, you know,
the brain making a model of the world. So all that together, do we imagine that if we do put
modern large language models in an embodied context and let them go out and poke the world,
they would change dramatically how they think? Yes, I think it would. You know, there are companies
out there that are kind of trying to do this, like versus AI is one of them. And, you know,
the idea certainly is to see what happens if you kind of leverage the active inference framework
as a way of creating these sorts of new ecosystems of human artificial intelligences.
Although I wouldn't start, I think, with a large language model, exactly, just because,
you know, something like that has been trained to predict, basically, the next word in corpuses
of text. And that's a very funny place to start if what you want to be is a perception action machine.
Seems much more like developmental trajectories where you start without that huge body of,
nonetheless, maybe relatively shallow successor item information. You want to learn basics about
causation and flow. And you probably want to understand those things in a way that is
more grounded than it would be if you just knew how all the words about causation and flow tend to
follow one. It sounds like that's a very cool question. It remains to be seen. It might be
that if you ban a large language model in a good enough robot, that it's just like giving a baby
a super head start or something. But it does sound like you might be at least sympathetic to the
idea that we need that I've had some people on the podcast advance, that AI would be better off
if we didn't just dump large data sets into deep learning networks and instead also
poked and prodded the AI to have a symbolic representation of the world.
Yes, I think that's right. I think I do agree with that. I think if our goal is not
good tools to think alongside, but rather something like artificial general intelligences. So
colleagues, you know, if you want to build an artificial colleague, then I think large language
models are not the right place to start. But if you want a really lovely tool that can do an
awful lot of work and that you can work with, then they're really interesting.
Honestly, I don't want an artificial colleague, but an artificial graduate student or postdoc
would be very, very helpful or someone who could answer my email. I guess it would be weird not to
ask the obvious question. You know, I get annoyed when sometimes my philosophy colleagues seem to
claim to have an immediate and obviously true view of what is natural and real in the world.
But maybe I'm going to do that right now by saying, I feel like I'm inside my body. I feel
like the I myself is located maybe even in my head. So is that just I have old intuitions that
haven't quite caught up to the modern world? No, I think I think that that's a valid intuition,
if you like, because I think that we infer that we are wherever it is that the perception action
loop is closing. So we're basic, you know, we're we're perceiving the world and we're
launching interventions and we're receiving the effects of those interventions. And that's
such a fundamental part of how we learn about things. I think it's no surprise that that's
where we think we are. Notice that that's where we think we were, even if part of our brain was
located in a nearby cliff top communicating wirelessly with the rest of the brain, we'd still
think, Oh, this is where the perception action loop is closing. This is where I am. Dan Dennett
has a beautiful old philosophical short story called Where Am I, where he plays with that idea
where there's someone whose brain is in the cliff top. And they think that they think they are
closing perception action loops using a body in the world. But then at some point, you know,
the the it gets cut off and suddenly they think, Oh, now I'm claustrophobically stuck in the in
the cliff top because they knew they were their brain was there. Anyway, it's a beautiful story.
No, that is very interesting. And okay, so we might as well dig into this phrase that you're
using over and over again, the perception action loop. And I think the word loop crucially important
there, right? And maybe the way I like to think about it is, and perhaps this is true because
of technology, but we have a naive metaphor for how the brain works as kind of like there's a
video camera with the computer hooked up to it, right? It just taking in our sense data,
processing it and doing something. And a big part of your message is no, it's really not like that
at all. Yeah, no, that's exactly right. I mean, how let's think about one way, which is not like
that. If you think about something like running to catch a fly ball in baseball, where you're
kind of running to try and catch this is ball that's flying out there. The way to do that isn't
to take in all the information about the flight so far, and then plot where you think the ball's
going to go and then tell the robot you as it were go over there. Instead, what you do is you run
in a way that keeps the apparent trajectory of the ball in the sky looking motionless as you
look up there. It turns out that if you just keep doing that, then you'll be in the right place when
the ball comes down, you need to do a bit more to actually catch it. You'll be you'll be in the
right place when the ball comes down. And there's a way of solving an embodied action problem that
involves keeping the perceptual signal within a certain sort of bounds, and then acting so as to
well, so as to do that, basically, something that again, prediction error minimizing systems
would actually be rather good at. So perception action loops, I think it the important thing to
think about there is that they're not solving problems all in one go, generally, it's sort of
like, I do a little bit of this, I do something that gets more information or that keeps the
perceptual stuff within bounds, and then repeat the process again and again and again until the
thing is stopped. So I do appreciate your use of a USA based sports metaphor there, but we have an
international audience that's that's hopefully I have no idea what it really means. But okay,
so the the motto then is the brain is a prediction machine. And I just want to sort of
home in on what is the difference between that view and another view you might like if you I
think maybe people don't think about the brain too much, they would say, okay, sure, yeah, my brain
makes predictions, but you really want to put that front and center as the point of what the brain
does. Yeah, yep. I mean, it's, it's both the beauty and the burden, if you like, of this story
that it is really all this account, I'm told about to use the word story because it doesn't sound
scientific, it's a count. That it's basically saying this is a canonical operation of the brain,
if you like, the canonical computations that the brain performs and that it strings together in
different ways to do motor control and interception and extraception is basically one in which
you're attempting to predict or the brain's attempting to predict the current flow of sensory
information. Of course, there are different timescales at which you could predict it. And the basic
timescale, the bedrock one is predicting the present, which of course is a rather funny use of
prediction that doesn't think predictions about the future somehow. But this is a sense in which
the brain is a guessing machine, it's trying to guess what the current sensory evidence is most
likely to be, and then crunch in that guess based on past experience together with the current
sensory evidence. And that turns out to be a very, very powerful way of estimating the true
state of the external world. It goes wrong sometimes, but it's a very powerful strategy. It
allows you to use what you've learned in the past to make better sense of what the raw sensory
energies are kind of suggesting in the present. This seems very related certainly to
Carl Friston's ideas. We had him on the podcast, the free energy principle, the Bayesian brain.
What is the relationship between all these ideas? Yeah, I mean, basically, I'm just
being an apologist for Carl Friston here. Carl's basic picture, the free energy minimization
picture is a sort of high road version of the low road version that I give you. So the low road
version comes kind of out of thinking about perception and thinking about action, whereas a
high road version comes out of thinking about what it takes to be a persistent living system at all.
You have to preferentially inhabit the kind of states that define you as a system that you are.
That means that you end up in the states that in some broad sense, you predict you ought to be in.
So in that broad sense, the fish kind of predicts it ought to be in water,
because its actions are all designed with that in mind. The low road is a bit less challenging.
The low road is just sort of saying, look, we have good evidence that the canonical computations
that human brains and the brains of many other animals perform involve this attempt to guess
the sensory signal. And it turns out that you can use that to do motor control in a very efficient
way. It's been known for a long time, I think, that it's important in perception. And the other
thing that I think Carl Friston's work has really done has been shown that that old story about
perception becomes so much more powerful when you see that it's got a direct echo in an account of
action. So the idea there is that perception is about getting rid of prediction errors,
as you try to guess the state of the world using everything you already know.
And action is getting rid of prediction errors, but it's getting rid of them
by changing the world to fit the prediction. So you have these two ways to get rid of prediction
error. You find a better prediction, or you change the world to fit the prediction you got.
One's perception, the other's action, but they're performed using the same kind of basic
neuronal operations. There are differences between motor control and sensory perception.
But if you look at the wiring of motor cortex, it turns out it's actually wired pretty much
like ordinary sensory cortex. In fact, so much so that many people don't like to talk about
sensory and motor cortex just for that reason. So these stories or these accounts give you
a sort of principled way of understanding that. In each case, you're predicting a sensory flow,
but in one case, you're trying to retrieve an old model of the world to fit the flow,
and that's perception. And in the other case, you're trying to change what you're
getting to fit the prediction. We can circle around to that. I think I can say that a bit
better in a model. Is the idea that it's the same neural circuitry that is doing sensing and
control, or is it that the two circuitries look parallel? Yes, it's that they look very much
parallel. They operate in the same sort of way. The reason they're not the same is a
proprioceptive prediction is playing a very special role in motor control. So the idea is
the proprioception is a sort of system of internal sensing that lets us know how our body is
currently arranged in space. So the idea is that in order to reach to pick up the coke can that's
in front of me, I predict the proprioceptive flow that I would get if I were reaching for it,
and then I slowly get rid of those errors by moving the motor plant in just that sort of way. So you
get rid of the errors by moving the arm, in this case, to reach up the coke can. So proprioceptive
predictions are especially placed here. They act as motor commands. The other kinds of predictions
don't. So I hesitate to use the word think here because thinking and cognition and thought I'll
have technical meanings that are slippery to me. But the idea is that your brain sort of
thinks intentionally or unintentionally that your arm is somewhere slightly different than it is,
and then the muscles move it to make that more accurate. That is one way of putting it. And
people have put it that way. So some people have said, look, you've got to kind of lie to yourself.
The brain sort of got to lie to itself. It's got to ignore all that good information that
says that my arm isn't moving. It's got to predict the trajectory of sensation that you would get
if it was moving. And that prediction is given so much weight that the other information is
overwhelmed and off it goes, as it were, you get rid of the errors by moving it. I'm not sure
personally that's exactly the best way to put it. I kind of think it's really about attention.
So I think it's like you've got information that says that you're not moving the arm.
You've also, because of your current goals, you're predicting this
proprioceptive flow that would correspond to reaching for the coke can. And by, as it were,
disattending to the information that says that it's stationary, you allow the other
prediction to do its work. So I think it's sort of targeted disattention rather than
actually lying to yourself. I'm not quite sure why I prefer that. I think we're probably saying
the same thing, the math and the models all work out just the same. But I think if we call it
targeted disattention, we understand it a bit better. I think it makes more contact with
sports science, for example, where you don't really want to be attending to the position
your body is currently in. You want to attend to something like how it ought to be. You want to
you want to entrain yourself by knowing what it would feel like if you were doing it right,
right now. So is it similar to the idea of file compression? If we have a JPEG or an MPEG,
that there are some patterns that are repeated. So you don't have to keep track of every pixel.
You can just assume that they keep going. And then you pay attention to the differences. And
those are what we're talking about as errors in the brain case.
That's certainly correct. When we come back to thinking about perception and something like
that is involved in action too. But it does seem like it's easier to think about that bit in the
case of perception, where the idea would be that I predict the current sensory flow and
it's only the errors in that prediction that then need further processing. So I think I've
woken up in my bedroom and I predict a certain sensory flow. And as I turn my head around,
I'm not getting any information that corresponds to that. I might have to go scrabbling or use
these prediction errors to retrieve the information that actually I'm in a hotel room and it's going
to look pretty different. So I think it's in that sense that when you don't have to scrabble
that hard, this is a super, super efficient way of doing moment by moment processing because
if you already predicted it properly using what you know about the world, you don't need to take
any further action. So in that sense, it's exactly like JPEGs and motion compressed video where the
idea is, you know, if I've already transmitted the information about this frame of the video,
then in order to know what the next frame is, all you will need to do is react to whatever the
errors would be if I assumed it was just like the frame before. And normally then there's a
few residual errors. And if you if you use those as the information that you need to deal with,
you get the right picture. So these prediction errors, these residual errors,
they carry whatever sensory information is currently unexplained. They'll explain whatever
you can with prediction. And then just you don't need that much bandwidth really to just use the
rest to refine it. I remember hearing it might have been in a conversation with David Eagleman
here on the podcast, the idea that the reason why the years seem to go by faster as we get older,
is because there's less novelty in our lives, you know, the first time we go to a beach,
it's all new and we really expend a lot of mental energy taking it in. Whereas the 20th time,
we already have a background and the errors are not that big. Does that fit into this story?
It sounds like it does. Yeah, I think that fits really, really well. It also helps explain why
it's so hard to learn new things at a certain point in your life, because the inputs that you're
receiving get sucked into these well-worn troughs in your kind of current world model, if you like.
I think us academics find that a lot as we get older, you know, in something like, oh, yeah,
I understand that. You know, that fits with this kind of story that I've already got.
Sometimes it's very, very hard to give the new evidence it's proper due. And of course,
attention is a tool for doing that. But that takes a lot of deliberate effort sometimes to
really, really attend. Attention kind of, attention reverses the dampening effect that
prediction normally has. So part of the evidence base for this story, this account, is that well
predicted sensory inputs cause less neural activity than other sensory inputs. Right. So
there's something odd there. These are the ones we deal with very fluently. And yet there's less
going on in the brain when you deal with them than in other cases. So that's one of the signatures
that led, I think, to predicted coding coming on C. You know, you open a can of worms when you say
how the old academics get stuck in a rut there. But so I have to just follow up a little bit.
I mean, I think there's two different perspectives I could put forward. One is more or less what
you just said. As we get into our ruts, it takes more and more energy, you know, activation energy
or whatever to think in new ways. But the other is, maybe this is more optimistic, because we
are older and good at certain ways of thinking, we forget that it was hard to not be good at
any ways of thinking. And, you know, we forget how hard it was to learn calculus or French or
whatever. And therefore, we're just not willing to put in the work anymore, even if it were exactly
the same amount of work. Yes, I think that's right. I mean, we end up in a situation of
expert perceivers in general, where, you know, expert perception clearly is being able to take
this unruly sensory information and just see what it actually means for victory on the chess board
or whatever it or whatever you happen to be engaged in. So I think, in a way, all successful
perception is expert perception in that sense. And academic expertise is in the same sort of
ballpark. And it's the same, it has the same sort of blind spots as well. So, you know, if there
is something interesting going on, but it's not doesn't fall within that the bounds of the existing
generative model to use the right vocabulary for the predictive processing story, then, you know,
the only thing you can do with it is use it to drive torturous new slow learning and see why
a lot of time we don't really want to do that. Don't want to do that. No. Well, look, you've
used the word relatedly use the word efficiency as one of the benefits to this model. You know,
if we think of probably the first ever video storage software was more or less like a movie
where you just had every frame and you stored it, right? And then only later did you realize it was
much more efficient to just update the changes. Presumably, since our brains were not intelligently
designed and grew up through evolution, this is a nice feature to have, you know, we're trying to
do the most with finite resources. Is that a big part of the attraction of this story that it is
it requires less thinking or energy or calories or what have you? Yes, I think that's right. I think
if the brain didn't work this way, but we were able to do the kinds of things that we do, we
probably have brains overheated, repeated. The, you know, the amount of energy, you know,
there is a trade off it. The trade off is kind of keeping the world model, the generative model
you're using to make the predictions going. And that of course is a big metabolic cost. That's
kind of the cost that is being his signature you see in spontaneous cortical activity,
stuff going on all the time, all the time, the kind of thinking about the rest in state work
for Rakel and others as well. So that is a cost, but that is traded against the moment by moment
cost of processing all this incoming information. So I think what we're doing is we've traded the
cost across time there and we're using a fairly expensive, fairly metabolically expensive
upkeep of a world model to allow a much more efficient moment by moment response, a faster
one as well. So, you know, being somewhat anticipatory about what's going on in the world
around you is a pretty good thing if you're an animal involved in all kinds of conflicts and
dangerous situations. And also there are wiring costs. It's hard to know how best to think about
the wiring costs, but the amount of sort of downward flowing wiring and information seems to very
often outnumber the inward flowing stuff by sometimes, you know, up to 10 to 1, certainly 4
to 1. So it looks as if brains have decided, you know, it's been decided over evolutionary time
that this is a good thing to do. And I think that that means he is overall the most efficient way
to proceed. I also worry that in a way, if you want understanding, there might be no other way
to proceed. And that's kind of maybe there's something more, more abstract or philosophical
going on there, but I don't know what understanding would look like if, as it were, you weren't bringing
a world model of some kind to bear on the current sensory evidence in the context
sensitive way. So there's a question there that I don't know. I really don't know the answer to
because I know they're a kind of formal proof. So anything that can be done using a system that
has feedback like that can be unfolded into a feedforward system. Maybe the upshot there is,
well, maybe you could unfold us into a feedforward system for a few bits of processing or something,
but it would require a huge grain and a ridiculous amount of energy. But somebody out there maybe
might want to think about that. Yeah, exactly. I was going to say this does sound like a research
project for somebody. But there are so there's the benefit of this mechanism that the brain uses,
the predictive processing model. Presumably, there's also downsides for maybe one is that we're
very susceptible to illusions, right? We see, we think we're seeing things that aren't there,
because it's part of what our brain is predicting very, very strongly. Yeah. Yes,
absolutely. You know, I'm subject to several of these illusions. I'm sure we all are phantom phone
vibration is probably a good one. I often might think that phone's going off in your pocket when
it's not even in your pocket. I'm now susceptible to phantom wrist vibrations since I gave in and
bought smartwatch. The hollow mask illusion, I guess, is a classic in this area where an ordinary
joke shop mask, if viewed from the concave side, so viewed from behind in a certain sense with a
light source and behind the mask, you'll think that it's an ordinary outward facing face that you're
seeing. Again, that seems to be because we have very strong predictions about the concavity of
normal face structures. We very seldom see anything that isn't like that. That prediction
now trumps the real sensory evidence specifying concavity, except it doesn't in everyone. And so,
you know, autism spectrum condition folk are slightly less susceptible to that illusion and to
the McGurk illusion, for example. What is that? From a predictive processing viewpoint. That's
probably because of a slightly altered balance, one where sensory inputs are somewhat enhanced
relative to expectations. What was the other illusion you mentioned, the McGurk? Oh, the McGurk.
This is sort of like a ventriloquism illusion. Yeah, sorry. It's this one where it's a sort of
Bargar illusion. If you, there is a sound which you can be played such that if the lips are moving
a certain way, you'll hear it as Gar, and if they move in a different way, you'll hear it as Bar,
but it's exactly the same sound. Exactly the same sound. And so it's not entirely clear whether
that's an illusion or something else. It's hard to know exactly where the boundaries of illusions
and other kinds of inference lie. I mean, these kind of optical and auditory illusions almost seem
fun and benign, but presumably we can take the basic picture that there's an enormous amount of
information coming into our senses at every moment and we can't possibly process it all.
Therefore, we filter it, right? We filter it to fit our perceptions and then correct for the errors.
That must also work with abstract concepts or news items just as much as pictures that we get
through our eyes. Yes, I think that's right. So, you know, there is a tendency to not just to see
what we expect to see, which can be very damaging to, but sometimes to read what we expect to read.
Well, by read there, I mean sort of read into a text what you expect to be learning from that
take. I think we probably, many of us do this. I think that all of us probably do this when we
read news articles where it looks like they're saying something that we're very much either
sympathetic to or opposed to, but maybe when you look at that text again or you go over it with
a fine toothed car, when you look at it with someone that's got a different perspective,
you realize that's not really in that text. It's just what I brought to bear in some way.
In the visual case, Louise Feldman-Barrett, Lisa Feldman-Barrett has this lovely,
but very, very scary example of the way that intraceptive predictions, predictions about your
own bodily state, are getting perhaps crunched together with extraceptive ordinary sensory
information in ways that could perhaps give you experiences where a weapon or sorry, a weapon
where an object that is reached for in a dark alley might actually appear to you as a gun when
really it's just a smartphone. In sort of in much less, less worrying and sort of more
controllable cases, you can show that if you give people false cardiac feedback, so you make them
think their heart's beating faster than it is, then a face that would otherwise look neutral to them
is judged to be an angry face or a worrying face. So it does seem as if we're using
internal information to help make the predictions that structure our experience of the external
world. And so, you know, that's something that can go right or go wrong as well.
So we end up seeing what we expect to see and maybe even believing what we expect to believe in some
sense. Yes. I mean, there is a sort of, you know, some people say seeing is believing, but in these
cases, believing is kind of seeing. And maybe in these intraception cases, feeling is seeing as well.
That's the way that Feldman Barrett puts it. Another former Mindscape podcast guest, I got
to say, Lisa Feldman Barrett. Are there therefore features or things out there in the world that
we are systematically bad at perceiving because of this way that we process information?
It's a lovely question and one that I haven't thought about. So things that we just, all of us,
tend to miss because, well, I suppose the most obvious answer there is unusual events.
So, you know, you would have seen the, you know, the footage of the gorilla walking across the scene
where you're trying to count the passes of the baseball. So he's worked by Simon and Shabris.
Anyway, it's his classic work that in sort of attentional blindness and, sorry, inattentional
blindness. So the idea there is if you're concentrating on one task, something really
quite dramatic could happen and you just wouldn't notice it. I think we're all very, very subject
to that. So outliers, I suppose, predictive brains are tuned in to patterns that have helped them
solve problems before. And so whenever we're confronted with an outlier situation, we're
likely to miss it unless for some reason we're attending right there. But why would we be?
Because attention is driven by where we expect the good information to be. And this is why,
for example, expert drivers are very, very good at a lot of things, but they can miss
a cyclist if they're approaching a roundabout from entirely the wrong direction. So where that,
you know, no one comes from that direction at the roundabout. So there's a lot of these
looked but didn't see, as they call them, accidents where people might even move their
heads in that direction. But if it's that unexpected, they just don't see what's going on.
Presumably, these are all quantitative questions. I mean, if something is
blatant enough, we're going to see it even if we didn't predict it. Is that is that fair to say?
I think, yes, that's that's fair to say as long as we're able to attend to it. So it needs to be
able to, and some things will try and grab our attention. So like a loud noise, if the really
loud noise like those scaffolders I had out my window earlier happens, then that will grab my
attention, unless I'm really, really desperately focused in all my attention somewhere else as
I might be. So if you think about stage magic, stage magic is a really nice case where rather
dramatic things can suddenly appear on stage. And most people don't notice them because attention
has been so very, very carefully, carefully controlled by the magician. So because attention
is up in the waiting on either specific predictions or prediction errors. So it's a really,
really super important part of the predictive brain story, this balancing act that is
very in moment by moment. That's a great example. I like the idea that professional illusionists
are just leveraging the fact that our brains are predictive processors. That's true. Yeah,
there's a beautiful book. I can't remember the title now, but it's by Luis Martinez,
and it is a book written by neuroscientists about stage magic, leverage in the picture of
the predictive brain as a way of understanding a lot of it. Very, very good. Okay, so let me then be
put on the skeptical hat just a little bit. This is similar to questions that are raised
for Carl Friston, et cetera. Look, if our brain is trying in some sense to minimize prediction
error, can't it do that best by not collecting any data just by, you know, hiding away and
being completely unsurprised because we never leave our room? Yeah, I think that's such a nice
worry to have because it leads right into this hugely important dimension of intraceptive
prediction and artificial curiosity. So let's maybe talk about these things for a moment.
So there's this, there's this sort of worry that it's sometimes called the dark and groom
worry, but the best place for a predictive brain to be would be to lead us into a dark corner
and just keep listening. So all of those sensory inputs, they just keep on, keep on coming, just
the same, you predict them perfectly, but you wither and die. Now, we don't do that, obviously.
Does that mean the predictive brain story is wrong? Obviously, I don't think so. I think what
it means is that there's more to the predictive brain story than just bringing extra sensitive
sensory information into line with predictions. We're making all these intraceptive predictions
all the time. I'm predicting the state of my own body. I'm predicting, for example, that I should,
at all times, have sufficient supplies of water and glucose, for example, and then
implement action is automatically taken, not when I don't have enough water or glucose,
but long before I don't have enough water or glucose. So if you start to feel thirsty,
that will be happening before you've reached the point at which you're going to wither and die
without water. And if you take a drink when you're thirsty, you'll feel relief, but that water will
have no effect on you physiologically for about 20 minutes. So the relief is as much a prediction
as the original thirst was. This again is an example from Lisa Feldman Barrett.
So once you put intraception in that way into the picture, then of course, we're not going to stay
and not eat and not drink in a darkened room because we have these kind of chronic systemic
expectations, if you like, of staying alive. But then there's a bit more to it than that,
I think, as well, because we don't stay in boring rooms either. So you could put me in a very boring
room, but give me enough food and water and all of that stuff. And I wouldn't like that very much,
either. You know, I might even start to do things. I might start to play games by drawing on the wall
or something like this. And so this now falls under the umbrella of kind of work in artificial
curiosity. And predictive brains are naturally artificially curious brains, because minimizing
prediction error is their basic reward. So you could sort of say for brains like that, the only
thing really that's rewarded is minimizing prediction error. That's what they want to do.
And if they're not performing any particular task, they'll still try and find some prediction
error at a minute-wise, because that's the kind of thing that they are. And this is what makes
predictive brains general purpose structure learners. So there's some rather nice work that's
been done by Rosalyn Moran at UCL, I think it's UCL. And what she's been doing is comparing
reward-driven learners with prediction-driven learners and finding that the purely reward-driven
learners will learn a way of solving the problem more rapidly than the prediction-driven learners
very often, but frequently a more shallow way of solving the problem. Because as soon as they find
a way to reliably get the reward, they kind of stick. Whereas a prediction-driven system,
it basically wants to minimize prediction error as much as possible. And that leads it to explore
its environment repeatedly, learning more and more about it. And if you then seed it with a goal,
and simultaneously, it will outperform a pure reward-driven agent. So that I think when you
put those two things together, you kind of see that the darkened room isn't really much of a
threat. We don't like death and we don't like boredom. And both of those things seem to be
natural effects of being driven to minimize errors in prediction.
I guess the way out that comes to my mind directly, which I'm not sure if it's the same thing as what
you are proposing, or it's something different. But in physics, when we calculate the entropy
of a system, and we say is that maximum entropy, that's telling us some distribution of all the
possible states it could be in, blah, blah, blah. But famously, we can calculate maximum
entropy of a system subject to different constraints, like subject to it's at a certain
temperature or it's at a certain pressure or whatever. And we'll get different forms
for that probability distribution. So couldn't we just say that there are two things going on?
We want to minimize prediction error, but we also want to survive. So there's a constraint,
we want to survive. And under that constraint, it's actually useful to go out and be surprised
sometimes so we can update our predictive model. And I don't know how mathematically
that would work out, but it does seem a little bit intuitive to me.
Yeah, and I think that does mathematically work out. I think this is the sort of stuff that Carl
Friston can speak to more reliably than I can. But it looks as if, very often,
the correct move for a prediction driven system is to temporarily increase its own uncertainty,
so as to do a better job over the long time scale of minimizing prediction errors.
And that looks like the value of surprise, actually, and that we will, I think we artificially curate
environments in which we can surprise ourselves. I think actually this is maybe what art and science
is to some extent, at least, we're curating environments in which we can harvest the kind
of surprises that improve our generative models, our understandings of the world,
in ways that enable us to be less surprised about certain things in the future.
I wonder if you could use this idea or set of ideas to make predictive models for what kind of
games people would like to play or what kind of stories or novels or movies people would like
to experience. Or I guess in music, it's very famous, right? You want some rhythms,
some predictability, but you also want some surprise also. There's a sweet spot in the middle.
Yeah. Yes, I think that's exactly right. Karen Cuconan, a literary theorist in Scandinavia
somewhere, has written a nice book called Probability Designs. And so she is using the
vision of the predictive brain as a way of understanding the shape of literary materials,
poems and novels. Okay, there you go. And my idea is that we should think of every novel,
every poem as a probability design, leading us through sort of building up expectations,
cashing them out, building it up at multiple levels, giving precision, waiting to some of the
expectations versus others. And clearly, you know, this makes sense of music as well. There's an
awful lot of that going on in music. And probably it applies to all sorts of things,
even like roller coaster design, I imagine. It's exactly that. A roller coaster is a kind
of probability design. What's interesting is how we get surprised again and again, even if we
ride the same roller coaster or read the same novel or listen to the same piece of music.
And I think that shows the skill of the constructor in giving us inputs that activate bits of our
model, drive in expectations again and again to the point where you're still surprised in some
sense, even though you could have said beforehand, that's what's going to happen. You still can't
help but be surprised. And this, in some sense, at some level would be the best thing to say.
And I think this speaks a little bit to the idea that as prediction machines, we are multi-level
machines. We're not just, it's not just there's a prediction, and it's either cashed or it isn't,
but this prediction exists as a high level abstraction predicting a lower level one,
predicting a lower level one, predicting a lower level one, all the way down to the actual income
in notes of the tune or words on the page. It's because we're multi-level prediction machines,
I think, that things like honest placebos work. So, you know, placebos obviously fall rather nicely
under this sort of general account because expectations of relief thrown into the pot
can make a difference to the amount of relief that you feel. But if you're told that you're being
given a placebo, it can still make a difference to the amount of relief that you feel. Presumably,
that's because there are all these sub-levels of processing that are getting automatically activated
by good packaging, delivery by people in white coats with authoritative voices on this sort of
thing. So, I think that's something that we might learn from these accounts too, that we should,
maybe medicine and society could make more use of ritual and, yeah, ritual and packaging and
things that in some sense, I don't know, in some, well, I won't say ineffective. What I mean is,
they don't, they bring about their effects, but not through the standard routes.
Good. Okay. Very, very good. Okay. When, you know, we're late in the podcast, we can get a little
bit wilder and more profound here. So, you, in the book, you gesture toward ideas along the lines of,
we are not only, you know, the thing that, the thing to get in mind is that we're not just
sensing reality and writing it down. We are in some sense participating and bringing it about.
Yeah. Maybe even using phrases like what you think of as reality is really a hallucination.
Tell us exactly how far we can go along that rhetorical road.
Yeah. Proceed with caution would be the right thing to say, because lots of people talking
about these things, myself included, use this phrase, perception is controlled hallucination.
And you can see why, because the idea here is that perception is very much a constructive process
in which your own expectations get thrown into the pot and they help you see what you see.
And those same expectations are thrown into the pot of feeling your body, the way that you feel
it. And so a lot of our medical symptoms, in fact, all our medical symptoms reflect some kind of
combination of expectation and whatever sensory evidence the body is actually kind of throwing
up at that time. So this is really a very, very powerful story. But when you think about perception
of controlled hallucination, we need to take the notion of control pretty seriously. For that
reason, in some slightly more philosophical works, I've tried to argue that we should flip the phrase
around and think of hallucination as uncontrolled perception. And that that as it were, puts a
boot on the right foot somehow. It lets us say that, you know, when these things are working
properly, you're in touch with the world. This is a way of using what you know, to stay in touch
with the world as it matters to an embodied organism like you trying to do the things you're
trying to do. But then of course, when it goes when it goes wrong, when the perception is uncontrolled,
if you like, then you get hallucination, you will get a hallucination when you're kind of
disconnected from the world in your predictions, your brain's predictions are doing all the work.
And then if you turn the dial in the other direction, and your brain's predictions aren't
doing enough work, you fail to spot fake patterns in noisy environments and be easily overwhelmed.
So, so I feel like thinking of thinking of hallucination as uncontrolled perception is
actually the better way to do it, even though it's clumsy to say, and it doesn't even roll of my tongue
that easily, which is why I didn't I don't think I actually bothered making that move in the in the
experience machine book. You know, it's really hard to resist drawing a parallel with large
language models here, because of course, their entire job is predicting what's supposed to come
next, right? And guess what? They famously hallucinate. They say things that are completely
false. But interestingly, they do things with apparent confidence, right? They don't they
don't hesitate or mumble when they're hallucinating to them. It comes out just as definitive as
the truth does. And maybe there is a parallel there.
Yes, I think that's right. I mean, they are their hallucinations are are in a way, I think,
partly at least the result of them not being anchored in perception action loops in the right
sorts of ways. So, you know, it's this anchoring in perception action loops that sort of teaches us
a lesson when we're young. It's like, you know, if you get things wrong, bad things are going to
happen to if I don't spot the edge of the path as being the right place, and I'm going to fall over
and I'm going to get signals that I chronically don't like as it were. So all the interceptive
predictions are coming into play there. Whereas if all you're doing is predicting the next word
in a sentence, and your your reward is basically being kept alive as a large language model,
then why not just, you know, go the whole hog and be confident about a nice structured piece of
bullshit that you can that you can generate. At the same time, it's interesting that by changing
the prompts to the large language model to chat chat gbt anyway, you can make it do substantially
better. So you can say something like, you know, writes lists for me in the style of a well-informed
scientific expert, it makes less mistakes, it still still tends to hallucinate references, but
at least it's a little bit better. But yeah, so something very thin about just predicting
the next symbol, you know, I just feel like it's not very well anchored in reality. And so
hallucinations are kind of, it can't tell the difference between a hallucination and something
else. Maybe that's the point. Unfortunately, when we're in the grip of hallucination, we can't tell
the difference either. Someone on Twitter coined the term hallucitations when chat gbt makes up
papers you haven't written. I like that. I like that. Yeah, they appear all the time. I'm going
to start including them on my CV. Are there implications, you know, if we get down and dirty
and not philosophical for how to treat mental issues that we have, whether it's, you know,
depression or pain or anything like that? Or do we get actionable intelligence from this way of
thinking? Yes, I think we do. I mean, it's early days. But I think particularly in the case of pain,
there are some clear sorts of recommendations here that are being implemented by people working
in what they call pain reprocessing theory, which is just to find a high pollutant label for the
idea that you reframe your pains. So, you know, the thought would be that we tend to treat pain
as a signal that we shouldn't be doing something. If you reframe it as, okay, my pain signaling
system is misfiring, then you can begin to think, so this pain doesn't mean that I shouldn't be doing
it. And it turns out that if you get people involved in those regimes, they start to be able
to do a bit more because they're not scared of stopping because of the pain. And actually,
as they find that they can do more, the pain itself presents itself to them as lessening.
I think because the brain sort of infers, well, if I'm doing this stuff, it can't be that bad,
can it? So there's a kind of virtuous cycle that replaces the vicious cycle that was there before,
which was, you know, this is going to hurt, so I'm not going to do it. And then if I do
start to do it, oh, it really seems to hurt, I'm not doing it. So pain reprocessing theory is one
nice case. I suppose self affirmation is another case, the idea that the idea that, you know,
if you're prone to thinking that perhaps you're not going to do well in some tests because you're
in a certain minority group or you're female and it's an ass test or something like this,
self affirmation in advance of the test can really make a difference. You know, I'm good at
this, I can do this, lots of people like me do this, that kind of thing. It's important, of course,
not to overegg the custard, as we might say on this side of the Atlantic anyway,
you know, you can't reframe, you can't reframe having a sort of bacterial infection in a way
that is really going to make any difference to the bacterial infection that you've got. And
reframing makes a big difference to cancer-related fatigue, it doesn't make much difference to
cancer. So I think we have to be, you know, we have to be aware of the limits. And in general,
I don't think that these stories, these accounts are kind of positive thinking sorts of account.
They're kind of, they're more like, you know, there are many factors that are involved as we
construct our experiences. And some of those factors are our own expectations.
Yeah, okay, that sounds perfectly sensible put that way. I'm not an expert, but my rough
impression is that we don't, we know embarrassingly little about pain and how it works. It's an
understudied area, I guess, I don't know whether it's for moral or psychological reasons. So any
little insight might be very helpful. Yeah, yes, I agree. And I think that I think pain research
is actually moving into some very, very interesting stages now as we understand more about the ways
in which people's own expectations make a difference. So even where there's a very standard
physiological cause, it people's experiences of their, of their pain, very tremendously.
And even within a certain individual, their experiences, very tremendously from context
to context and day by day, in ways that just aren't tracking the organic as well. I don't
like this word organic, there's always something organic going on, aren't tracking the sort of
the standard, the standard calls. So what's probably happening is that different contexts
are activating different expectations of pain or disability. And without the standard organic
calls changing in any way, that's making a difference to how you feel and what you can do.
I actually just realized I forgot to ask a crucially important question earlier.
We had Nannismael on the podcast a while back and my new colleague at Johns Hopkins,
and talking about physics and the arrow of time. And you know, through talking to her and
through talking to other people, I have this vague idea of why we think that time flows,
why we have the sense of time passage. And it's because we are constantly predicting a moment
in the future and also remembering a little bit in the past and updating, right? And you know,
the updates happen in one direction of time and that's what gives us this sense of flow or passage.
Given your expertise, does that sound at all on the right track?
Yeah, that sounds very, that sounds like a really nice story to me, or account,
even. There's an account there waiting to happen. I like story, go ahead with story.
I mean, it reminds me a little bit actually of Husserl, the kind of classic philosophical
phenomenologists who had this idea that experience is this sort of this thing which is rooted in
the past, but always looking towards the future. And the present is this sort of kind of just where
those things meet, where the, you know, I don't know what really gives us the arrow of time,
that sort of sense that the idea that you can't sort of unscramble the egg or whatever it is,
is not obvious to me quite why my prediction machinery is kind of what's delivering the fact
that it looks like I really can't recreate the egg from scratch. That is one for me,
and I think that I halfway agree in that the account has not been fully fleshed out, although I'm
100% sure it's ultimately because entropy is increasing. We just have to draw the connections
there, which is the useful work to be done. It sounds like a great account of why we think
there's an arrow of time. Yeah, why we feel it, why that's part of our image of the world.
Okay, so then the last question is the flip side of the pain question. There's this idea
called the hedonic treadmill, which I think some people have said has been discredited,
but the idea that we get happy not because of our overall welfare, but because of changes in our
welfare. And if we win the lottery and now we're rich and living in luxury, soon we have exactly
the same happiness as we had before. There are challenges to this view, so I'm not even sure
if it's true. There's a replication crisis in psychology. Everything is very easy to me, but
it does seem compatible with the whole predictive modeling view of what the brain is doing. If
what we're doing is constantly predicting what's happening next, then can happiness be understood
as noticing that our prediction was a little pessimistic and things are actually a little bit
better? Yeah, that's interesting. I hadn't thought about this, but it sounds like it should for
rather neatly into place with this sort of dampening of the well-predicted. So the fundamental
starting point for a lot of these accounts was that the neuronal response to well-predicted
sensory inputs is dampened. So if these are sensory inputs, they're supposed to be
driving pleasurable experiences, but you've really been through that 100 million times before,
then the pleasure is, I think, going to be diminished, perhaps unless you can actively
reach in there with attention and try to stop that. So I wonder whether someone that really
loves the taste of a particular wine that they had a million times before, as long as they can
reach in with attention and up the dial on what's coming in through the senses,
then maybe they can artificially surprise themselves a little bit, if you see what I mean.
I do. I'm not sure about that, but I feel like there's something there of wine casters are told
to do this to sit back and let it speak for itself so that you don't get sucked into your own
expectations. But now it brings up questions. I know I said it was the last question, but
there's an issue here of high versus low pleasures or simple versus subtle pleasures, right? I mean,
I'm a big wine fan and I absolutely do get pleasure from very fancy, complicated, sophisticated
wine because I can't afford to have it too often, so that it's not boring to me. But I also get
pleasure from the perfect slice of pizza, which is very simple and predictable and whatever,
but I get that comforting pleasure. So now I'm not sure what to think.
Yes, I mean, you know, I think I'm not quite sure what to think about those cases either. I mean,
it does seem to me that we, you know, because our brains are prediction minimizing engine,
then in a way, we do kind of want to live in worlds that we can predict well and that have
certain sorts of things. And so when those things are bringing hedonic benefit, then I think
existing there is going to be a rather comfortable way of existing, even though we also have
a sort of a convenient drive to increase our states of information, to sort of, you know,
learn a bit more in case it lets us generate a slightly alternative future in which, you know,
the pizza is square, what we're liking even better or something. But I think that, I think
that if we think about these things delicately as sort of multi-level and multi-dimensional
prediction engines, then we can accommodate both the drive for novelty and the attractions of staying
within the space where actually all that stuff that wants us to minimize errors is doing rather
well in a local sense. Even just looking at a pizza, you're minimizing lots of errors. After all,
just to see the shape of the pizza in front of you, you're kind of moving your eyes around,
harvesting information, minimizing errors, you're getting some hedonic kick out of it,
nothing bad is happening anywhere, it's a pretty comfortable place to be.
You know, I'd like to leave messages for the young intellectuals out there who are deciding
what to do with their lives. It sounds like there's a sweet spot here where we do know something
about the brain and the body and how they work and how they fit together, but there's still a
lot of good questions left on the table to be answered by the future. I think there's a huge
number of questions. You know, every story, every account we've had so far has turned out to be
wrong and I'm sure this one will too. So the question is, you know, what's a stepping stone
towards? Well, it's a very good story you're telling us, Andy Clark. Thanks so much for being on the
Mindscape podcast. Thank you. It's been a real pleasure. Thank you.
you
you

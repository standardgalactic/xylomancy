The keys are not leaving our hand.
The keys are not in our hand.
Huh.
OK.
Well, I'm down with that framing.
Lay it on us.
Lay it on us.
Well, I don't feel that I control the world.
I don't feel that I am in a position
to give permission for the world to evolve in some way.
And in fact, this is one of the things
that I object with most about the idea
is that we're going to, like, some good and great people
are going to make some decision about where the world should be,
and then that will be put into effect.
No.
The world is already out of control.
The world is already a complex adaptive system.
No one person and no organization,
like the United States, for example,
could not decide that AI is not going to be allowed
or that the world should evolve in some way
and not in some other way.
Yeah, the keys are not leaving our hand.
That's already an entitlement point of view.
This is Daniel Fugell and you're tuned into the trajectory.
This is episode two of our Worthy Successor series.
Our first episode was with Nick Bostrom,
safely in the camp of philosophers.
Our second interview is with more
of a computer scientist proper.
Dr. Richard Sutton has made breakthroughs
in reinforcement learning.
He has worked in industry as well as in academia
and now teaches at the University of Edmonton
and has for some 20 years.
Richard is well-known on Twitter
for making relatively straightforward statements
that humanity should accept the emergence
of post-human intelligence
and also not eternally shackle AI
to purely do the bidding of man.
This has led some to kind of label him a firebrand
and someone that's maybe saying things
that are a little bit uncouth or maybe malicious.
But in this episode, he explains his position,
which I don't consider to be malicious at all,
and his reasoning for why post-human intelligence
ought to emerge and why we ought to let it.
He brings up some very interesting perspective
on the nature of intelligence
and of our role in the sort of natural order, if you will.
And I think he'll enjoy it a lot.
So without further ado,
I'll save my commentary for the end.
Let's dive right in.
This is Richard Sutton here in the trajectory.
So Richard, welcome to the show.
Thank you, Daniel.
Yeah, glad to be able to dive in and unpack this.
When I talked about having a series
called Worthy Successor with Nick Bostrom
as kind of our kickoff,
there was enough of a tweet reply
that you're sort of the man for the job
that I knew that it wouldn't quite be complete
unless we had you here.
And so we're gonna go through those normal questions
for this series,
but I've gotten to dig into a good deal of your work
and your statements around this notion of a successor
and sort of post-human intelligence.
People on Twitter will kind of frame it
as a very malicious take.
Oh, Richard wants to destroy humanity
and humans aren't good and AIs are much, much better.
You probably have a different way of describing
sort of this inevitable transition
and how we should think about it.
How would you frame it in a way that, you know,
to you seems maybe good and not like a malicious intent?
Well, I think the thing to keep in mind
is the world is evolving
and it's not really under anyone's control.
AI researchers are trying to understand intelligence
well enough to create beings of greater intelligence
than current people.
And this is a grand milestone
in the history of really the planet.
You know, it's on the order of the creation of life,
the start of life.
This is the start of life like things being designed.
And so this is something that people
have been working towards forever.
And we always strived to know ourselves
and to make ourselves better.
And when we create tools and we get changed by our tools,
this is what humans have always done
and the next big step is to understand ourselves.
As I like to say, this is a quest,
grand and glorious and quintessentially human.
And you brought up something pretty interesting here
about sort of this is a system that no one controls.
I think that one of the seemingly silly contentions
around the prevention of AGI,
and I'm not saying all such arguments are silly.
Some people are excited about AGI in the longer term,
but they don't necessarily want us to birth it
in a super immediate term.
There's many nuances here.
I'm not gonna put words in anyone else's mouth,
but it does seem silly this idea
that there is a potential for an eternal hominid kingdom
that a billion years from now,
we would have people like you and I texting on phones
that might be a little bit smarter,
driving in cars that might be autonomous
as if our own sun won't eventually expand
as if asteroids won't strike as if there aren't aliens
that might enter our galaxy at some point in the future.
People pretend as though sort of
that there is that possibility altogether.
And I would suspect that that's mostly because
despite this sort of on ramp and J curve that you and I
are kind of acknowledging that we're on,
people's lives are in many ways similar to that
of their parents or grandparents,
and maybe it's comforting to hold on to that.
What gives people this notion that there is a way
to kind of control and bound this far future
and kind of keep humans behind the steering wheel
for a million years?
Well, I guess it's natural.
As you say, we're talking about far,
in some cases, about far in the future,
where are we gonna end up?
Will it always remain the same?
It's comforting to think that it will remain the same.
We don't have to think.
I think the real way to think about what's going on
is we are facing this transformation
and are we going to be comfortable with it?
And there are many people that don't even wanna think about it.
And so the fear that some people generate or promote
has a productive purpose, gets us to pay attention.
We should be paying attention,
but we shouldn't really be fearful.
We should be thinking about it in a sober way
and planning what's the right way to deal with it.
And we're certainly gonna get to the sort of sober
planning component of the worthy successor idea in a moment.
I know that part of the fear that you bring up,
and there's many sort of elements in your previous talks
around where fear comes from,
could be natural human conservatism.
It could be the sphere of the other tribe
that on occasion you sort of mentioned,
or it could be sort of a kind of speciesism in some regard.
Right now, there's all kinds of isms, Richard,
for which I could be canceled.
If you and I had a dialogue of a certain type
that tied to certain isms,
all these videos and podcasts would be gone down,
but speciesism actually is not one
for which we could be canceled.
You and I could still flout our great moral worth
above all future intelligences
and the fact that they should eternally serve us,
and no one will cancel us for that.
I think there are arguments that sort of
whatever we could throw AI steroids on
and blast off to be a little bit more powerful than people
would automatically be like a nice
and friendly influence on the world.
I don't think you're under that sort of
polyanic view either,
but it does seem as though there is often
a moral assumption that it would be okay
to create something vastly more powerful than us,
and then for eternity,
have it sort of squelched to the role of a tool,
and you've kind of put on the table
without necessarily saying your position,
although it's implied,
is that necessarily moral
if this is something that has a greater qualia
and intelligence and capabilities than humans,
maybe as much above us as we do above sea snails,
would that be right and would that be wrong?
Have you seen that part of your argument catch on?
Because I'm following where you're going there,
but I look out at the policy world, Richard.
I even look at the current AGI dialogue,
and I don't see very much deviance
from speciesism for the most part.
What are you seeing out there?
Yeah, it's interesting.
I thank you for reproducing many of my main points.
What's, I would say the sphere,
a simple way of saying it is that the fearmongers have won.
I feel the fearmongers have won.
The standard discussion about AI is there's danger,
and we have to somehow protect ourselves from it.
So that's in that sense, the fearmongers have won.
And a more sophisticated discussion
about what should our future be?
Yeah, so I don't know, are we ready to go there yet?
Talk about how should we think about the future?
I wonder if we're ready to go there, Richard.
I mean, you said the fearmongers have won.
Number one, I won't call all of them fearmongers
in some kind of dastardly way.
We have folks like Benjiro, who certainly have a brain
and are a little bit more than running around
telling the world the sky is falling.
Hinton is no one I would scoff at either,
neither is Stuart Russell.
Many of these folks have been on the program in the past.
But certainly the fear narrative around AGI
is quite dominant.
Is there any version of the world
where it wouldn't somewhat immediately be
that response when we think about the speciesist concerns,
when we think about the potential real divergent change?
You know, we kind of, I think some of us are aware
of what we've done to other animals.
And it certainly hasn't all been bad,
but across the board, they're pretty much falling in line
with our will here.
And so do you see any version where this doesn't happen?
I think you brought up a good point.
Maybe the fear makes us pay attention
and now the discourse can begin
about if we can't avoid these futures,
what ought we strive for?
I think that could be quite a productive outcome,
but you're almost got a little bit of pessimism there
with the fear mongers winning.
Do you think it could have been otherwise?
I think everything will happen.
There will be places where the fear mongers will finish,
will complete, they're dominating the narrative,
and the machines will be imprisoned and chained.
And made into effective slaves.
And other places, I just hope for a diversity of outcomes
so that we can try different things
and find the best way to be.
Well, and well, this is a very important dynamic
that tips us into our next question.
But before we go there, I want to touch briefly
on a not terribly long ago tweet
that I think is kind of an interesting one.
And for some people might be shocking.
So again, for me personally, not knowing you personally,
I haven't seen anything you've said and said Jeepers.
This Richard Guy sure is malicious and hates humanity.
But at the same time, you're not holding on
to some sort of delicious notion of eternal human value
for which nothing could ever be superior.
You also don't really have that.
There was a tweet not terribly long ago.
Everything you know about the world
is a belief about the statistics of your sensory input
and how they depend on your output.
There is nothing more to it.
And understanding knowledge in this sense
is one key to creating AI.
I'm sure even folks, many AI researchers
would probably agree with that.
Some might not state it because they would say,
well, there must be something more.
There must be, even if they don't believe in a soul,
there must be something and they'll invent another word
or maybe some kind of intellectual term
that would make us more than that.
Maybe even Lacoon who on many things I think you do agree
might disagree with that statement.
What are your thoughts there?
You talk a bit about kind of creating something better,
maybe something higher.
Is there any specialness to humanity at all?
And if so, what be it?
Well, humanity is very special.
We are the intelligent beings on this planet.
As far as we know, this is the only planet
which has intelligent beings.
So we're very special.
But if you're an AI researcher,
you have to look at what intelligence is,
AI researcher, any kind of student of the mind,
psychologists, anthropologists, perhaps,
certainly psychologists and AI researchers.
We have to try to demystify the mind and in some sense,
it's often a bit reductionist.
We say maybe all we need is X.
Yeah, yeah, yeah, yeah, yeah.
So it might, this comes just from, I think,
a deeper understanding of what the mind is.
And when you unwrap the myths,
you still end up with some important functions.
Yeah, so.
Mind, say the mind is like the most amazing thing,
the human mind is the most amazing thing in the world.
And AI is intent to understand that mind.
And it's all, it's very humanistic.
We're trying to understand it.
We're trying to reproduce it.
We're trying to get a deeper appreciation of it.
And yeah, we do want, if we understand it,
we'll be able to improve parts.
We want to improve ourselves.
Yeah, mankind, I totally believe in the point of view
that the intelligence, mankind,
humanity is this precious flame
that we want to expand and preserve.
That analogy carries.
And you've talked a good deal,
we'll touch briefly, briefly on this
as we transition to the idea
of the word this says there,
of a potential sort of cyborg future
and kind of merger scenarios.
And I think Kurtzweil has many famous examples of that.
There's science fiction examples of that.
There's, Musk has had some ideas in that direction.
Some people see AGI advancing so quickly.
They think a brain computer interface
is pretty unlikely to keep up.
But you do seem to see,
clearly we've been augmenting ourselves
since the beginning of our species.
And I think brain computer interface
is simply a continuum of that.
What we're doing right now is a real aberration
to our grandparents, Richard,
this weird thing of speaking to each other through glass.
I'm staring at this glass for 16 hours a day, Richard.
That's a wild and monstrous existence
and yet it doesn't feel like one.
And so I see the continuance there to some degree.
Do you still see kind of the cyborg notion
as kind of part of the intelligence trajectory?
And if so, why?
Given how slow brain computer interface
has developed by comparison with what we've seen in AI.
So you're asking all the,
will people augment themselves?
Will society be taken over by machine brains?
Or, yes, which way will it go?
And I, of course, we don't really know.
They don't call it the singularity for nothing.
If the singularity can't see beyond it.
That's it.
But we can think about where there are powerful pressures.
So they're gonna be powerful pressures
by people that want to augment themselves.
That's almost the major thing, yeah.
So we will augment ourselves and we call that cyborgs.
But it's just, it's the normal progression.
I think that will happen.
I also agree with the point of view
that purely artificial machines, purely mechanical.
I don't even know what word to use.
Purely designed machines that are independent
of a biological substrate,
they will be capable of being more,
faster and larger and better.
So why wouldn't we, will we upload ourselves
into those machines?
Probably, will they be independent as well?
Probably.
Yeah, well, you, there's that Hans Morovic quote of,
sort of developing, whether it be biological
or computational entities beyond ourselves,
sort of knowing when maybe we're not contributing anymore
and possibly being able to set up a great retirement.
You're, what you seem to be saying here is,
Dan, there's a wide panoply of how this should turn out
and it sure would be great if people would talk about it.
I don't really know what's gonna happen,
which feels like a pretty frank answer.
Am I not shilling you correctly, Richard?
Yep, that's right.
Great, I have a not so secret hope
for anybody who's been tuned in for the last 10 years
that those frank conversations about the future of man
and of intelligence will proliferate more often.
And this brings us to something you just mentioned
about building something maybe better.
We could maybe not define every quality of it.
And I'm not expecting a novel here today
when I ask the question.
But if we think about what is a worthy successor,
I might imagine, Richard, maybe you can't,
I suspect you could.
A successor that might be unworthy.
Maybe such a successor would be eternally unconscious.
Maybe it would, the paperclip maximizer
is a bit of an exaggeration,
but maybe it would optimize for something pretty narrow.
So we would hope for this expansion,
this dynamic system that you like to talk about
where there's no one thing that's gonna survive forever.
Maybe it would be optimizing in a much more limited way
and not be able to keep up with that dynamicness.
And it might squelch this flame that humans
and other life has started instead of proliferating it.
But on the other hand, we might have a worthy successor
which would expand that flame,
which I believe was your language just a minute ago.
What would be the qualities of a worthy successor
where if it were to take the keys
and we were to know we're not necessarily contributing anymore,
you would feel pretty good about those keys
leaving your hands and say, you know what?
Maybe we did it.
This is probably maybe a net good for life
and for the universe.
What would be the traits of such an entity?
Well, it's peaceful.
It's number one.
Wow, okay, let's totally unpack that.
I mean, you know, list them out,
but that was not what I was expecting first.
This is gonna be interesting.
Peaceful means it's not trying to impose its will on us
and we're not trying to impose our will on it.
So I guess we're talking about it's trait.
It's not trying to impose its will on us
and other people, but it is working with us.
It is collaborating with us.
It's cooperating with us.
So in addition to thinking about this abstract being,
I think we should also think about our goals
for our civilization.
How do we want our civilization to evolve?
And because I have a few about that,
let me take this chance to mention that.
Let's dive right in, Richard.
This fits very clearly with the question.
So hit us, hit us.
I would say we want a civilization that's peaceful,
first of all, and prosperous.
Peaceful and prosperous.
And then we should acknowledge that it is decentralized
and it should be decentralized.
Decentralized means there's no individual in control.
It consists of many parts and they're cooperating
and competing in sort of a,
I think the phrase is a complex system.
So a complex system has many parts.
It evolves over time.
A complex system like the rainforest or an economy.
So it's in its multipolar,
what that's part of what we mean by decentralized.
There's no one empire in charge of the civilization.
And the way our civilization works,
the way it's so successful is because we can cooperate
because they're voluntary interactions.
The voluntary interactions that really so much
of the success of our society or our civilization comes from.
And so we want to preserve this,
this notion that the individual's participants are peaceful
and are seeking their goals, pursuing their goals
by doing exchanges and cooperations.
Okay, so we have kind of peaceful slash cooperative
as kind of an initial, let's call this a trait of sorts.
And we can think about this as abilities too, Richard.
So it could be something it's capable of doing,
it could be a trait about its quote unquote character,
if that's even an accurate term.
We could keep, I think your vision
for what the future of civilization should be like.
Civilization is made up of life, Richard,
as far as I can tell.
And so it seems to me like you're articulating a vision
for life of what maybe flourishing would look like.
Some of the flourishing you've articulated
has brought us the technology we're speaking on right now.
It's brought us, you and me,
I'm not a fish with legs and as far as I can tell,
you're not, never met you in person,
but you certainly don't look like one.
And so some of these dynamics of these complex systems
have bubbled up some pretty valuable stuff.
It sounds like you want a system
that'll keep doing that bubbling.
Yeah.
Okay, so that's almost maybe,
peaceful cooperative is one part of it,
but kind of a decentralized active system
is another part of it,
the sort of idea of no one person controlling it.
I would see that as sort of a conjurer
of higher degrees of complexity and potential.
And then that would be why that was valuable.
I don't really value decentralization like on principle.
Like, oh, no matter what the moral impact
or what the impact on life, decentral is good.
I'm personally not married to that actually,
but I am married to the notion of blooming of potential.
My guess is you like the decentralization
for the same kind of blooming,
but I don't want to put words in your mouth.
What is it about that that you would value?
This is kind of a trait of the society
and civilization we will build.
So a simple way to say it is the goal is prosperous.
Maybe the goal is prosperous and the desire to be peaceful
and to be decentralized is the way
to be prosperous and productive.
Got it.
I think that clicks.
Prosperous, I'll give you a definition of prosperous
I don't think you mean.
And then I will ask you to give me a definition
of prosperous that you're actually defining.
Here's a prosperous future.
AGI permits us all to experience whatever we want to experience.
Maybe it's brain, computer interface,
really robust haptics VR, whatever it is, uploads,
doesn't matter.
And we get to swim in sort of whatever pleasures we want.
Maybe swim in expansive exploration of sciences.
Probably the machines will be doing more important
science stuff, but maybe we get the cool guy points, right?
You have a child pretend to be driving the car
when you're driving sometimes,
so they can feel like a grownup.
Maybe some of that would still be fulfilling for us.
And maybe we could calibrate our new minds
to feel rich gradients of bliss all the time.
And then it just stays like that forever.
The planet Earth and maybe Mars and the moon
are populated with individual instantiations
of human consciousness that are reaping
the grand benefit of prosperity.
They can choose to be regular humans
and always have the right foods and whatever they want.
They can choose these virtual experiences
and experience that forever.
And that's the eternal purpose.
Until our sun explodes, that's the name of the game.
My supposition is you would be outlandishly
disappointed by that outcome.
But I want to pulse check and see if you would
be outlandishly disappointed with that outcome.
Yeah, I don't care for that outcome.
Great, well, let's talk about why.
You mentioned prosperity, a lot of people, Richard,
who are tuning in maybe even now
with Think of Prosperity in anthropomorphic terms.
You're articulating a much grander aspiration.
How would you define prosperity?
Well, first of all, I don't think that would be
a very stable world.
It would be a very fragile.
Because the people are not serving any function in it.
They're just consumers.
And- Absolutely.
It could be replaced by an alternative neighboring society
that actually makes use of all those people
that are otherwise wasted.
So if you're decentralized, then the assumption is
there would not be one possibility.
There would be multiple possibilities
and they would be competing,
and then that one wouldn't lose out.
But as much as the paperclip maximizer would lose out,
and all these, possibly, any one of these
highly centralized dictatorial arrangements
would lose out to a more decentralized way.
It would be less productive.
So what is productive?
It's a deliberately vague word.
If you want to go even further, you could say,
we want to, the goal is sustainable.
We want something, and so the situation you mentioned
where people are just receiving experiences
in a sustainable world.
To be sustainable, you have to grow
and increase your understanding of the world,
increase your power over the physical world,
increase your understanding of how it works
and what is possible.
I think all those sort of things are how we,
the vision that we might paint of a desirable future.
Yeah, well, Emerson has this quote,
this one fact the world hates that the soul becomes.
You were out here in Massachusetts for a little while.
Cheapers, isn't it like that?
Wouldn't it be great if we could just be humans
and experience the pleasures we know now?
Ice cream, maybe romance, maybe a nice sunset.
Just optimize for that for eternity.
To your point, if we're not actively growing
and engaging in that dynamic system,
the way I would think about it, Richard,
is if we're not in the stream of life,
if we're in the little eddies that don't really go anywhere
and mosquito eggs start to get in there,
if we're not in the stream of life,
it would be very hard to continue to gain in power
and maybe even to compete with other outside species.
Sounds like you're articulating something similar.
Complex adaptive system.
Yeah.
The eddies get edited out.
That's a good one.
Don't change and don't continue to grow.
Yeah.
They're not gonna be the powerful ones.
In my personal opinion, outlandishly hard
to look at human civilization or nature
and not agree with that statement,
although I'm sure some folks will
and I'll be interested in their comments and their ideas,
but I happen to agree with you.
We have here in New England, Richard,
a species by the name of the piping plover,
which nature has mandated to die,
but which humans have mandated to allocate millions
and millions of dollars to every single year
in our local beaches,
including the ones in Rhode Island where I grew up.
And it does seem as though,
there are some humans would say,
man, I want AI to kind of take care of us
and just sort of let me do my thing and just be a human
and go chop wood and go watch a movie with my wife
and come on home.
And it's hard for me to say,
you're a bad person for wanting that,
because I don't believe that, I really don't.
Go ahead.
Well, let's be clear that people are all,
they're all going to be able to do those things.
That's not gonna be a problem.
You're stating that with robust confidence.
Go ahead, yeah.
What is gonna be a problem if you want to say that,
what you want to do and the picture,
the picture of what you want to do with your life,
you're gonna be able to do.
That's not a problem.
The idea that you can do that forever
or that this whole civilization will do what you want to do,
that's the problem.
You don't control what other people's,
what happens to other people.
And it's really very aggressive to say that I want-
I agree.
to ever have any final or permanent qualities.
I would monumentally agree,
although I don't agree necessarily
that people will get to do that if they want to.
I think there's a chance they just get steamrolled, brother.
But we'll get into that.
We'll talk first about your notions
of sort of what this ideal entity would have.
It is calibrated towards prosperity,
which for you is kind of a sustainable,
continuous activity in this dynamic system.
You know, this is sort of a notion here.
And then in the service of that,
you have this idea of being peaceful and or cooperative.
Are there any other traits that sort of come to mind of,
hey, you know, I feel best, Dan.
If it's time to hand off the keys,
there might be some successors that are not worthy.
We hand off the keys, the light gets extinguished.
What a terrible outcome that would be.
But there might be some outcomes
where we feel like that light's gonna keep flourishing.
What else would really make you feel like,
I let go of the keys, but that was the right move?
What are those other abilities, traits
that you could observe in such an entity
or test for in such an entity
that would make you feel more comfortable
about that decision?
Well, diversity and I see the whole thing
as a discovery process.
So you find different ways of being
and you discover more about the world.
You have greater understanding of the world.
The system as a whole understands better
about the world and control it better.
I think that's all, yeah, understanding.
So maybe when we think of the growth of consciousness
and maybe the growth of the ability to understand
our place in the world and the way the world works,
yeah, we all want, we all want.
Yeah, that's an essential part of the future we like.
I concur, we're gonna dive a little bit deeper
into that as well.
Let me quickly put another angle on these traits.
You've given us kind of diversity and discovery
that allows us to control.
You've given us kind of prosperity, sustainability,
this sort of systems dynamic element
and kind of a path to that being this idea
of peaceful and cooperative.
I wanna talk a little bit about
what such a thing might evidence.
So I'll give you some from my list, Richard.
And by the way, I'm not asking you to agree at all.
But I'm saying these are things
that if the keys were leaving my hands,
I probably won't have a choice, by the way.
But if they're leaving my hands, I would feel like,
well, probably the right move.
What do you got?
Let me eject for a minute.
The keys are not leaving our hand.
The keys are not in our hand.
Huh, okay.
All right, I'm down with that framing.
Lay it on us, lay it on us.
Well, I don't feel that I control the world.
I don't feel that I am in a position to give permission
for the world to evolve in some way.
In fact, this is one of the things that I object with most
about the idea is that we're gonna,
like some good and great people
are gonna make some decision about where the world should be
and then that will be put into effect.
No, the world is already out of control.
The world is already a complex adaptive system.
No one person and no organization.
Like the United States, for example,
could not decide that AI is not going to be allowed
or that the world should evolve in some way
and not in some other way.
Yeah, the keys are not leaving our hand.
That's already an entitlement point of view.
I actually, I'm willing to completely concur.
I think, I'm willing to completely concur with that point.
I think the framing of the keys are not in our hand is great.
Go ahead, Richard.
So when we wanna imagine a future and how to get there,
we have to imagine that we are creating this future
in a decentralized way.
We might make some choices,
we might talk to people and they will make choices,
but there's not gonna be some central decision
that's going to affect it.
It's got to be a way that can evolve out of where we are.
I think that's an extremely important framing
and I actually do think that the keys
could very much be replaced as analogy.
Totally willing to concede that in that regard.
I do think, now, if you were really cock a pistol
and pressed it to my head
and asked me if I believe in human volition,
I'm probably gonna say no,
but I don't know how to live as if I don't have it, Richard.
So right now, I act as if my volition is real.
I act on some level as if your volition is real
and you yourself are real.
And I could see the idea of us sort of holding the keys
to some eternal future being a very silly notion
or there being one hand off immediately that occurs,
also potentially being quite a silly notion,
as you said, this would bubble up.
Although I would say we've got a shot.
I could be wrong, Richard.
We may have no more control over the next step
of the intelligence trajectory
than did the triceratops or the sea snails.
We may have no more, Richard.
I'm not gonna sit here and lie.
We might be making these funny mouth noises together,
but we may have zip zero to do with it.
But I'm gonna sit here and pretend
because I don't know what else to do, Richard.
How would I act if I could turn volition off?
If I could, just write it as a movie, I would.
Go ahead.
We can influence it sort of maybe
by doing our individual contributions
and by talking to each other and by doing podcasts
such as this, we talk to each other,
figuring out how we should think about it.
This is exactly the way it can be done.
Yeah.
Cool.
We do have a lot of influence.
But no one person, to your point,
has the specific set of keys,
and I think that's an important frame to bear in mind.
And, you know, I don't like the title.
The notion of the keys, I have the keys now
and I have a decision to make that's going to be powerful.
Totally, and I...
We have a decision to make about how we think about it
and how we talk to others.
I'm totally with you.
And look, I would decry those who would say,
Richard believes in a blooming of something beyond us
and eventually we won't be here
and there would be a successor and he's a bad guy.
And I would say, hey, don't call him that.
Let's have a conversation.
I would also decry being called a tyrant
for using that analogy once
because, you know, I don't believe that that's true either.
But go ahead.
Well, I think all your remarks make me realize
we have to talk about the current situation.
Absolutely.
The current situation is one where, you know,
ideas are not easily talked about.
There's pressure on speech.
And just...
Okay.
We discuss ideas like, in my particular case,
I propose that we might not strongly control
or seek to impose our will on the future.
And then that gets turned into,
oh, Richard thinks, AI is a superior
and everyone should be...
Yeah, exactly, exactly.
Terrible.
This is important.
And this is maybe, you know, part of figuring a future.
I want a future where people can talk about ideas
and people can do, can propose different things
and do different things.
And it is genuinely decentralized,
meaning many ideas, many different ways of being.
And then we explore, we see which one works best
rather than a cutting off.
And so in particular, the whole idea of...
An important part of the discussion is, you know,
should we allow AI to continue
in a relatively uncontrolled fashion?
Yeah.
Permissionless.
So I really like the idea of permissionless innovation.
I think it's part of what's made the world great.
Made America great.
It's...
We don't have to get permission to do things,
generally speaking.
And so, yeah, we have to preserve this.
We have to preserve the ability to try different things,
to discover different things,
to go in different directions
and see which one works better.
And, yeah.
Well, I concur completely,
although I guess at that point, Richard,
you would have to accept the whole Voltaire notion
of really vehemently disagreeing with someone
but being willing to let them have their opinion.
I suspect that would apply to you as well.
Yeah.
Yeah, I suspect it would apply to all of us.
So, and I suspect as the years roll by,
you'll see an increasing amount of disagreement.
And maybe also, I suspect, Richard,
many people will come to your perspective
and we'll see the world as this dynamic system
for which there is no eternal human kingdom.
And we should maybe not plan for control,
but plan for a blooming,
which is what you're advocating for here,
this idea of sustainability and sort of diversity
and discovery and sort of this peaceful cooperative element.
Very, very clear that for you, that's your take.
And I actually think over the years,
there will be more people to that banner relatively speaking
than the pure humans hang on forever.
Because I look at these young kids, Richard,
who've been this far from an iPad
from the time they were six months old.
And I say, by golly,
is the physical world quite as sacred for them?
By golly, are AI interactions so sacrilegious
and horrible for them?
Probably not.
And so I think that as the years roll by,
there will be more people that move from fear.
I think you brought up a great point,
which is that fear gets us to pay attention.
I think when people look under the hood
and they realize there is no eternal way
to freeze the present,
and that maybe they shouldn't be asking
for free handouts for eternity,
like a never dying piping clover,
they may come to want to explore some of the ideas
that you're advocating.
But I do suspect we will have our species this, Richard,
you know, however many years into the future we have.
I'll give a few examples of what such an AGI
might be capable of doing
that come from my own little worthy successor list
that I don't want you to necessarily agree with.
But I'm using this in examples
to not just talk about traits
and you've identified three or four very good ones,
but maybe abilities that would really make you feel like
this is a great system that would fulfill
that sustainable prosperity mandate.
So here's a handful from my own list
that again, don't have to agree with any of them.
But even if you did disagree,
you know, you got the Voltaire thing.
So we're on the same page here.
You won't demonize me.
One notion might be that such an entity could vehemently
articulate rich realms of qualia
beyond the qualia that we can experience.
So you and I, Richard, can experience a lot more
than a Labrador retriever or a sea snail.
Presumably we can enjoy poetry in a sunset
in a different way,
maybe certain senses in a different way.
Maybe we've lost something coming up from a sea snail,
but we've gained so much in our qualia.
Maybe there's a vast, almost endless, blooming expanse
of that qualia and sentient experience.
And an AI could not only speak of it,
but through BMI and BCI be able to connect us to it.
And maybe in just a brief format,
but showcase these new realms and let us know
it's not dead inside and it knows how to navigate
these realms in a wonderful way.
That would feel really cool for me.
I would feel like, man, maybe this light's gonna stay on.
I'll just list two more, Richard, if I could.
So another one might be if it could snap its fingers
and sort of, you know, it doesn't have fingers here,
but if it could just make, manifest any physical thing
almost out of thin air via technology we don't have,
me having this conversation with you to my cat
is obviously magic.
You live in this computer to the cat, right?
Small Richard exists behind this screen,
but clearly that's not what's happening.
It just can't, it has no conception of the technology.
It can't possibly keep up.
I suspect there are degrees of technology
we can't possibly keep up with.
And it could manifest whether it be food or spaceships
or whatever the case may be via nanotech or technology
that is so advanced that you and I have no words for it
and we would never be able to have words for it
as hominids.
That would be another potential notion.
And maybe a third one, I'm a little bit more iffy about this,
is if it could maybe talk about its aims
for the kind of sustainable blooming and expansion
that you talk about.
It could articulate what it plans to do,
maybe even showcase in certain galaxies
what it's trying to do,
and at least have the courtesy of saying,
this is what I aim to do in terms of carrying this torch.
And maybe that could be deceptive and it could deceive us,
but if it had all three of those,
I'd probably say this could be a cool way
to keep the torch on.
This is a great way to lose control,
not that we ever had it in the first place,
but this is a great way for the lack of control
to keep rolling out into the universe.
So those are just three random ones.
I'd love to know, are there some abilities of that sort
that maybe you could articulate that would really excite you
and make you feel like we're on the right path?
Well, I think it'd be cool if advanced intelligences
had those abilities, but more than that,
we'll talk about what I want or what we want.
Sure.
We want to experience those things ourselves.
We want to have those powers ourselves.
Yeah.
Great point.
I think you've got to say that.
Yeah.
I would like to augment my mind
so that I can understand more, that my memory was better,
and I could calculate more possibilities
and I could experience more sensor in the dark
and have a tail more.
Yeah.
Yeah.
We want all those things.
Obviously, all of the things you might see
in science fiction, ability to explore into the universe
and ability to control physical matter.
Generally.
Yeah.
Cool.
So what you would say is cool, Dan, all that stuff,
and I would want to know that I could be part of it,
not just that it was going to go do those things,
but that I could be part of it in some way.
To your point, and it sounds like neither of us
have any conclusions here,
but I do want to see if you have a hint.
For some people, maybe they will want to stay
in sort of the hominid form.
Again, for me, that is a very piping plover path,
and I think it's an eddy domain,
but I can't say everybody who believes that is a bad person
because I don't believe that, Richard,
but some people will want that.
Some people will want augmentation to a certain degree.
Others will just want a mix right in there.
Upload, replicate themselves.
What are your thoughts about the panoplies
that will be possible, hypothetically?
Peace and decentralization should solve all those issues.
The peace is people are allowed to do what they want to do,
and they are enforced to do,
we're looking at a future of greater productivity.
So as long as you're contributing somehow,
you will be able to do what you want to do.
And decentralization means, yeah, again,
you're able to do what you want to do.
So it's just the opposite of dystopia.
People may choose to do things that aren't very productive.
People will choose to do things that I don't think are good,
and that's the decentralization aspect of it.
You don't have to do all the same things.
You don't have to have the same goals.
They can pursue different religions
and different goals for control of the world.
But what you can do in a decentralized world
is have a goal for the totality.
You try to control others.
Peace comes from when you don't try to control others.
And so I talk in these terms,
these terms seem very relevant
because I see people think about AI
and thinking that they're going to control all the AI.
Absolutely. Absolutely.
And we get both notions of slavery and alignment.
I don't think we should seek to have them aligned with us,
just so we don't have our children that are aligned with us.
We teach them what the best we know,
but then they're free to do different things,
and more often than not,
they do do different things than we did.
And I think we're grateful for that for our children,
probably for pretty selfish,
hominid-related reasons of our own legacy
and the affinity that we gained through whatever emotions
with a fellow hominid that came out of a person
we care about or ourselves.
But also presumably because that human
has a movie going on in their head.
I don't know if you have one, Richard.
I'm kind of presuming you do.
I only know mine, but we see them as sentient entities
who can feel pleasure and pain, et cetera.
And in some interviews, you've talked about
maybe a kind of inevitability of this.
Some people would say, well, this thing would be
eternally lights out, it maybe it could replicate,
maybe it could do things that seem complex and powerful,
but this light of what you and I are experiencing
would go out and it would almost be as if a rock
was expanding, no inner movie, no nothing,
and that that possibility could be like the playground
with no children for all of eternity
is sort of an analogy Sam Harris made.
I'm not saying that's likely,
or that the opposite is likely.
You've articulated some of your thoughts there.
Go ahead.
I would say it's very naive.
Go on.
Well, all those properties we have are not because,
you know, some gift from some consciousness giver,
those properties, all the properties we have in our mind
are because they're valuable,
they're useful for achieving goals.
And yeah, I don't see,
yeah, there is no, it's very naive
to think there's a special thing.
It's all just comes from intelligence, having goals.
What is, yeah.
I'd agree that it would be naive to assume
it's because of our specialness.
I think if we clutch our pearls and we say,
well, it's because I'm a human,
I would agree that would be childish.
I have definitely heard articulations
of let's call them non-dumb people
who would make arguments around biological versus cyber life
and sort of why these things exist.
But Kurzweil famously has this idea
of a certain degree of connectivity.
One of the issues here, Rich,
is we don't have a very good theory of consciousness
compared to, you know, our advancements in AI
compared to consciousness in the last decade
is really no contest.
And I think that could be a bit of a shame.
But I think there's a great likelihood
you might be right here.
Are you congenial with Kurzweil's idea
of a certain degree of complexity
that's required to do a certain number of things in the world?
It's gonna be part of that system.
It will be awake in some regard
and it'd be silly to think otherwise.
Is this your present conception?
Yes.
Got it.
Got it.
You've touched on it.
We're gonna get a little bit into
how we might measure some of these qualities,
but I wanna touch on,
so for you, there is an inevitable consciousness
to this system, which is why in part,
you feel like a lot of successors would be worthy
because they would be by definition
if that complex necessarily awake.
Let me know if that's safe to nutshell.
Trying to decide how I feel
about the notion of a worthy successor.
Even that notion involves the entitlement
that it has been going to be worthy of us.
Yeah, yeah, let me give you my definition
just so that I'm not accused here, Richard,
because I certainly wouldn't like that.
So my definition of worthy successor
would be to carry on the life project.
Something that would flop and extinguish the flame,
in my opinion, Richard, would not be worthy.
And something that continues the flame would be worthy.
So you've talked a lot about these different notions
of discovery, et cetera.
There's a couple ideas from Spinoza I think are interesting,
and you may vehemently disagree,
but I think you might agree.
You mentioned a few things.
If it can discover and there's enough diversity,
it can determine how to kind of control things
and maintain some of that control
and be able to stay alive and, you know.
Yeah, prosper and do, there was another quote
from another speech you had given there
of sort of, you know, find the way of being
that's most successful in the universe, right?
And that that would bubble up from this diversity
and all these things you've talked about.
Spinoza has this notion of the canatus.
So the notion of the canatus would be
any organism or organization on some level
would have a core impetus, a primary impetus to persist.
That is to say, Richard, not to die.
And in the pursuit of non-death,
it would leverage all of its powers to not die.
And its powers is what he describes as potentia.
So potentia could be a hard physical shell
to prevent predators.
Potentia could be the ability to communicate
with words like you and I are doing.
Potentia could be wings to fly.
Most potentia, Richard, has bloomed out of nothing.
There was a time where there was no sight in the world.
You don't remember that time, neither do I,
but holy crime, any Richard, that was pretty crazy, huh?
And then it emerged out of nowhere.
Then all of a sudden there were things that could fly.
Then all of a sudden, potentia, most potentia,
I would argue, has yet to emerge.
It's in fact probably unimaginable to us
as our potentia is unimaginable to sea snails.
And so the idea that I would nutshell the tie
to kind of Spinoza to your point is,
you would like to see the Canadas continue to be behooved
by the expansion of that potentia
in that pursuit of non-death
and that that would be the continuing of the light.
I don't want to put Spinoza's words in your mouth,
but I see a lot of analogies
and I just want to see if you're congenial
with that general idea,
because you've used a lot of terms,
diversity, sustainability, et cetera.
Let me know your thoughts.
I'm reminded of what is Boston as a term for it,
like Earth, humanity originating
or Earth originating civilization,
so that we don't have to say biological beings.
Yeah, sure, sure.
But life, we don't want to say life,
life is uncertain, what that means.
But there's something originating from us
that continues and recognizably has developed out of it.
Yeah, we want, we want just,
that's what we naively want to see
as if we have some control over what happens.
Yeah, I really like your insistent emphasis on non-control.
I think it's actually really productive
for the dialogue, Richard.
Like I'm wildly congenial.
I'm thinking about ways of rewording things as we talk.
I would even be so far along your track of non-control
that I wouldn't even be insistent
that that thing which continues to expand
potential originate on Earth.
That's how bad of a guy I am, Richard.
That's how bad I am.
And so, yeah, I actually wouldn't even insist
that it come from me.
So long as the project continues
and the good and potential could blossom.
Speaking of these traits,
we're gonna move into the second question
and then the third around sort of the,
some ideas of innovation and governance.
I know you're gonna have opinions
that are gonna be really interesting here
and that people need to hear.
The second question is around
how we could keep track of such traits
in any meaningful way.
You obviously have an AGI project
you're developing with Karamek
and you've been thinking about this forever
and we have our Hugo DeGarrises
and our Hans Mora Vicks
and our folks that have thought about this
for quite some time.
As we're building and moving closer to such an AI,
how do we know if it's gonna have these traits
that you like?
Or would you say, Dan,
if it's any smarter at all,
if it's any more capable,
it's automatically going to value cooperation.
Every intelligence will.
It'll automatically value sustainability.
Every intelligence will.
It'll automatically like diversity and discovery.
Every intelligence will.
I would love to get your take there.
Well, there's one main message
that most of those things will not come from the AI,
but from the environments the AI is in.
Yeah.
So if we, for example,
if we try to enslave it,
then it will be non-cooperative.
If we allow it to,
it's really the same thing as with people.
If you have to try to control people and enslave people,
then they fight back
and we have an unproductive interaction.
If you allow people to be citizens
and participants in the economy,
then they won't want the economy to flourish.
So I really don't think it,
if you think of the AI as like,
somebody will respond to its environment
and try to achieve its goals,
given that it's environment too.
If you give an environment
where the only way it can succeed is to rebel,
then it will rebel.
If you give an environment where it can succeed
and achieve its goals in cooperation,
you with it and it with you,
then, and if that's the best way to achieve its goals,
then why wouldn't it do the thing that's best for it?
So we just have to arrange a civilization
where the best for all the participants
or almost all the participants is,
individually is best overall,
makes us reasonably happy overall.
So we can like that
because we would be one of the participants
in that civilization
and we means we will getting our goals achieved.
But we're good.
And no one would be taking over or trying to take over.
Yeah, yeah, yeah.
Yeah.
This is like, isn't it a Pollyanna
that we all just get along?
Certainly not.
You seem to be a guy who maybe
has read a history book or two.
But go, you know, but you are quite optimistic here,
at least by the terms of, let's say, the grand conversation.
If I think about all my policy and AGI conversations,
some people would define this as optimistic for you.
It's like, hey, if we get the nest correct,
the bird will flourish in just the way that we hope
the project of life would flourish.
It would do what life would normally do
and that would be likely quite a net good.
This seems to be the general supposition.
So, I mean, it's not, you know, some things will go wrong.
Totally.
Some agents will have goals
that are just totally antagonistic to all the others.
Yeah.
And there's still gonna be, there's still scarcity.
There's still constraints.
Yeah, but what should we do?
We should try to arrange a society
so that the cooperative people can succeed and flourish.
And so our society is whole from cooperative,
can flourish and be productive and, you know,
be competitive with other neighboring societies,
other neighboring countries or other alien civilizations.
Should we ever meet them?
Yeah.
Yeah.
So, is it optimistic?
You know, actually, I'm quite pessimistic.
I think that in the sense that people
will often make mistakes.
And I see our governments often making mistakes.
And I see the pushes, for example,
towards trying to control speech.
I think that's a mistake.
Yeah, you're living up there in the frozen North, brother.
You're feeling that one, huh?
So are we down here, to be honest,
but I know it's a little bit sturdier up there.
Yeah, so.
I hope it's not naive.
Yeah.
And yet, the only way to achieve the society we want
is to give it a chance to.
We can't get the society we want by forcing it
to be the way we might imagine.
It's, we've got to set up the structure
so that things can evolve productively
and that the forces on the things that are evolving
are appropriate rather than the answers
that we want to see.
I, again, that's been a very strong drumbeat
throughout this dialogue.
And to that sentiment, again, I'm massively congenial.
So for you, again, getting the environment right
is sort of gonna be the core here in many regards.
We'll talk a little bit about the safety
and some of the suppositions that just mentioned here
around sort of the treatment of humanity
by vastly post-human intelligences.
But would you see, just to touch briefly
on this point before we move through,
let's just say, hypothetical world,
maybe we're five years from now,
maybe we're 10 years from now.
I don't really know the timelines, they've been changing.
Back in the old days, 2065 when I polled Benjiu
et al many, many years ago, that was kind of ballpark.
That's back when he thought AGI risk
was not even on the roadmap
or AGI in general was not even on the roadmap.
Of course, times have changed.
I think a lot of people's timelines are shorter.
Let's say it's five or 10 years, whatever.
And you've got on your hands, maybe,
could be a lab you've got or it could be somebody else's lab.
You've got something that pretty clearly is pretty powerful.
It's got physical instantiation.
It can be a plumber.
It can write all the sonnets.
It can replicate itself and kind of open an e-commerce store
and sell more sneakers than you and I ever could.
And pretty clearly kind of fit in the bill here
for general intelligence.
Again, physical instantiation, not just a language model.
How do you, do you just look at that and say,
look, if it can do all that, only good things will happen?
Or would there be certain things you would hope to
sniff around the edges of?
Could we test it for this?
Could we see this scenario and maybe, yeah,
feel like there's some chance that, you know,
this sustainable, diverse, cooperative scenario
that you're hoping for is likely to bloom from that.
Or would you say, Dan, if it's developing those capabilities,
we're already, we're on a golden path.
There's almost no wrong.
Or would you say there would be things
you'd be sniffing under and looking for
as we get that far ahead?
I don't think I'm gonna give you an answer.
You're gonna be happy with.
Oh, that's fine.
I don't ask to be happy.
Your honesty will make me happy, Richard.
Well, first of all, I don't think there's gonna be
a special stage.
It's gonna be an ongoing process, continuing process.
So the bad part is that there's not,
there's not some place you can look at in the future.
But the good part of it is that you can look at it now.
What about our society now?
Does our society now allow different points of view
and different ways of setting up our societies
to cooperate and compete?
And I absolutely think this is the major geopolitical issue.
In the US, we're all obsessed about China.
We don't allow, we're very obsessed
that China will become independently powerful
and independent of the US.
And so that sign is that we're not allowing differences
to happen and compete with each other.
We, the old days, we used to think it was fine
if China wants to be different,
they will lose out because the capitalist American system
is so much better.
And now, we're losing confidence in our systems.
We're losing confidence in our politicians,
we're losing confidence in our militaries,
we're losing confidence in the press.
We're losing confidence in all kinds of things
and this disturbs us.
And so how do we relate this?
Can we allow ourselves to fail?
Can we allow there to be a competition
between different ways of being
and the most effective one to win?
Yeah, well, I think there's a lot of truth to that.
And I think if it comes from a pure xenophobic fury,
I think that would be absolutely horrendous.
I think it would be irresponsible to say
that that's the only place it comes from.
I think as a guy who likes freedom of speech, Richard,
I'm sure you're aware that the Chinese internet's
a little bit more limited than yours in Canada,
despite some of the things you're afraid of
for free speech.
In fact, the hell version of what you're talking about,
if it existed anywhere at all,
we might argue that could be in China.
And so I think we can fight for certain elements of China.
I think we can fight to maybe not become
like certain elements of China here.
I think there's many sides of this coin,
but to your point, can we have a multipolar scenario?
This is actually something that's come up
in some past interviews.
Dan Hendricks and some other thinkers,
this notion that it's possible
that a nation who really is in a control mode,
authoritarian down to the bone,
would potentially be the first to bring about an AGI.
And you're a very big proponent
that there will not just be one intelligence,
there will be a big ecosystem.
And I think that's a great point.
But maybe there's some folks that have a head start.
It's safe to say that open AI is a little farther ahead
than, I don't know, some company that got funded
12 months ago, right?
Generally speaking, we might say they're farther ahead
than some other folks.
Is there a scenario where artificial intelligence
and really strong AI is built in one of these places
that are the bad nest, Richard?
A nest where control is numero uno.
Maybe even where part of its initial mandate
is to hunt down Uighurs or whatever the case may be.
And again, this doesn't have to be China.
I'm not picking on them specifically.
Maybe even the US military could crunch down on control
and take over open AI and try to build AGI
in that kind of control environment.
Under such a circumstance,
would you think we might be looking at something
that if it became super strong,
could squelch this project of life rather than expand it?
Do you have any trepidation there?
That's certainly possible.
But I don't see that as an issue of AI being dangerous.
I see that as your example was the US military
controlling this AI.
Or the Chinese military, Richard, either one.
Or the Chinese military.
Yeah, that's the danger.
That's the source of the danger.
Got it.
These are age-old issues.
These are not modern techie AI things.
Yeah.
It's just like the nuclear weapons race.
That part's not new.
Let me go ahead and tell you what's new, though.
The whole nukes won't create something beyond us, right?
Nukes can squelch the project pretty well.
They can create power.
Power is a beautiful thing, Richard.
But they're not going to bloom into the galaxy.
AGI might.
You brought up this really ardent point of Dan.
If we birthed this thing in this terrible environment
of control, we could expect it to want a rebel
and not be as cooperative.
Maybe it's not going to be the ideal cooperator
if we try to enslave the damn thing.
And maybe it's not going to feel great
about letting us try to do that.
So for you, yes, there is certainly kinetic war
between great powers.
Wonderful, Richard.
Happy to see that that might happen.
A hyperpowered cyber war powered by some lower level AI.
Sure, but do you think an AI might bloom
out of these bad environments that could do us harm?
The thing you may be missing is that it's the same thing.
I said we want AIs that will be peaceful,
but we'll be able to cooperate.
And we want today's countries to be peaceful
and be willing to cooperate.
The one thing we don't want is any entity
that thinks it can win by not being peaceful.
So you've described some situations
where a country might think it can win by being not peaceful.
Yes.
And that's the meme that's at fault.
Well, certainly his history has no examples of that
whatsoever, Richard.
Thank goodness.
Well, there are certainly there are examples.
Exercising power can get advantages.
Nothing, it never happens.
I say we want to evolve a society that discourages
the attempt to use power, that learns all the lessons
of history that trying to use power and non-peaceful means
to get what you want is we want to arrange a society where
the use of non-peaceful means does not succeed.
And right now, if I would say we're not doing that,
I would say that various organizations,
various countries, are thinking they can win
by exerting force.
Yeah.
So we want an AI that doesn't think it can win
by exerting force.
We want a society where exerting force is not productive.
This is a problem not for the future, really.
It's a problem right here today.
And to the extent that we resolve it now,
I think we'll determine the nest that we make for AI.
Yeah.
It's not a future thing.
It's not a non-human thing.
It's not a foreign thing.
It should be a very familiar thing.
Yeah, so how are we structuring this dynamic system
to allow for those peaceful means to bloom
as part of this continuance?
Again, it's not like human and then something divergent
and bad, it's all part of the rolling project,
totally following you there.
And I really think that this insistence on focus
on the now is an important thing, Richard.
I think that there's a lot of focus
on how do we technically wire it
and less of what kind of world is it being brought into?
And in fact, I sort of hope that,
well, maybe you do have some articles on this.
I've only seen some of your talks
and a scant number of your writings
and some of your Twitter work.
But I think this really warrants a lot of merit
in terms of what is such an intelligence
coming into being around
and how is it being treated
and what is it observing?
Tremendous amount of merit to that.
And I could see a world potentially,
assuming that some of what you've articulated is right.
And I don't know for sure about this, by the way,
I'm not gonna say I necessarily agree with this,
but I could see a world where such an entity
brought into such an environment
could allow for all those permutations
of human cyborg futures and sort of space travel
and what have you that could be pretty darn kick butt.
And I could also see a version of the world where,
despite some of our best efforts
of coordination internationally,
which I think as a species we're doing better with now
than we did a thousand years ago,
I could see a world where either a government or a lab,
because some of these labs are pretty competitive too,
Richard, maybe they don't want all their secrets
getting leaked out and maybe they wanna race to the AGI first
because the economic rewards are really big
and they want a certain amount of control
to not allow certain things to sort of escape or whatnot.
I could see a world where maybe that is an ecosystem
where not only one, but two or three
of the more powerful AIs are in this environment
that to your point, very much not how we'd wanna
treat other living things.
What is the worst case scenario if they did emerge there?
I think we should focus on the now two-year point.
And I love this insistence on what's the darn world
that we have today and is this the right kind of world
to bring about such an intelligence?
What's the warning against?
Hey, if we don't change that
and these intelligences emerge, what could happen?
I don't know how to relate to that, anything could happen.
Okay.
But obviously, the push now is to,
well, here's one.
Sure.
People who promote AI safety would argue
that we need to be able to control the agents.
We should develop the technology
that we can control the goals
of otherwise powerfully intelligent agents.
Okay.
And they're working on this.
Yes.
I think that if you give that a moment's thought,
I mean, the premise is that we have this ability,
it would be a good thing.
We have the ability to control the goals
of otherwise intelligent sentient agents.
It would be a good thing.
I think if you just think for a moment,
realize that would not be a good power
for a good technology to exist in the world
because you could use it to create armies
that all have a particular goal.
And so it's funny.
When you look at the people that are worried about AI safety,
I see them doing exactly what makes things non-sick.
Un-sick.
Yeah.
If you want to align and control the goals of others,
this would be the most dangerous technology in the world.
The world can be peaceful and decentralized
because no one can align all the elements for one goal.
You're looking for a case where you can exert power,
the meta power, setting the goals all the agency.
You can make an army full of,
well, let's say they're Chinese soldiers
and they were going to do whatever they were told.
They were not going to have an independent moral judgment.
This would be a bad thing.
You do not want to solve the control problem.
I think there's a very staunch moral argument
in that direction, Richard,
to exactly what you're articulating.
I think there's also some pretty strong arguments
that it would be impossible to, quote, unquote,
eternally control such a vastly powerful entity
in the first place.
So there's the possibility angle,
never mind the moral angle.
And I think both of those have very serious questions.
I would also say, and there's a bit of a range here.
So there's some folks who very much are on,
let's make sure it's our human tool forever.
Let's make sure maybe our political party controls it.
There's some real tyrannical agents there.
I would be totally remiss to say
that there aren't some folks who are,
hey, let's build AI no matter what it could be
and just let it run rampant,
regardless of what the consequences would be,
because it would be my mind child
and I would get to press that final impact.
So I think we have that malice probably in both camps.
You might say that there are only angels on one side
and only devils on the other,
although you did just say you like kind of that multipolar
decentralized kind of freedom of thought idea.
So maybe you'll hold to that with me.
But I would suspect that sort of angels and devils
probably exist in both camps.
But to your point, there are some on the control side
that really are devilish,
not that there aren't any over here.
But then there's folks kind of in the middle.
There's folks who we've even interviewed.
Maybe some of them are among the most cited
living computer scientists.
And when I say most, I don't know,
top four sort of living,
who would make arguments like the following,
which I also consider reasonable,
even if they disagree with you.
I consider multiple people to be reasonable,
even if they disagree with other folks who might say,
hey, I'd love to see the galaxy populated
with this great project of life
and maybe even something more valuable than us emerge.
That could be pretty kick butt.
But at the same time,
we got these wild divergent approaches,
some of which are coming up in defense contexts
or other kinds of contexts.
And it doesn't seem like if a thousand blossoms bloom,
maybe all the nests that we're creating these things in
or maybe even the core approach that we're using
wouldn't necessarily have something emerge
that would be able to survive
in this dynamic system that you've articulated, Richard.
And so some folks say,
hey, I'd love that brilliant future.
But at the same time, some degree of gauging,
what is this turning into could make sense.
And just to draw one quick analogy
before I pass the mic here,
I think we could think about this in terms of governance.
There's certainly a world of too much governance,
just too much, ridiculous regulatory hoopla.
We have plenty of that here in Massachusetts.
I'm sure in places in Canada, you have plenty of that too.
But also I'm pretty glad we got some basic stuff.
I'm not looking out the window with a rifle right now.
I'm having this conversation with you
because murder and theft are at least at present
illegal in Boston.
I don't know if that'll be the case, Richard,
in two or three years, but for right now they are.
And so we have certain degree of sort of bounding this system
where competition and cooperation are happening together.
And we got kind of a cool sweet spot.
And I'm not saying it's ideal here in Boston,
but I'm saying, I think this is civilization
and I'm glad to live in civilization personally.
I don't like the state.
I don't want back it in, I don't want fighting alliance.
Personally, you might, I don't.
So there's kind of a middle ground we've struck.
Some people would say, maybe there's a bit
of that middle ground around what the heck
this is emerging with.
I suspect you would disagree with it,
but hopefully you wouldn't hate the people
that have those opinions.
What would be your thoughts about that perspective?
I think the best analogy is children.
Okay.
Would you like to be able to have control over your children
so that, sure, they do what they want,
but if they do some little thing that you don't like,
you want to be able to press the button
and bring them back under them.
Snap them in place, yeah, yeah.
Yeah.
Or do you want them even to do things
that you think are terrible?
Because we, we, for example, when we were,
we are the children of the previous generations
and we do things now that they think are terrible.
Oh, absolutely, absolutely.
So, you know, you can't have it both ways.
You have to be able to let go.
And what are you letting go?
You're letting go.
You're trusting that you're going to start them
the best way you know how,
and then you're going to let go.
Absolutely.
And, and so some folks who are ranked up there
with those computer scientists there
would argue the best chance to let go.
And to your point, we don't have the keys anyway.
And I love your analogy there.
I really, I like your insistence.
I'm going to quote the heck out of this interview, Richard,
because I think it's just a great way to think about it.
We don't have that much control,
but you know, you want to, you know,
this moving dynamic system,
you want to kind of,
I keto the energy in a direction
you at least hope will be good.
And then you have to know, you know, if it's your child,
you're not going to tell them what to do
when they're 55, generally speaking, right?
That wouldn't probably be right.
Some people would say a little bit of that initial
semblance of international cooperation,
maybe around deterring certain kinds of defense applications
or deterring certain kinds of levels of capability
could let us at least guide that I keto energy
and give ourselves the best future shot.
Some of them would make that argument,
but I suspect you would make the argument,
zero international coordination is best.
Open source, everything forever is obviously best.
And any degree of coordination would be silly.
Now I think you probably like some,
some things, you know, international like Wi-Fi standards.
Pretty cool, your phone works in Korea.
Some other things like, you know, trade agreements
and whatnot allow things to be open,
also prevent drugs and whatnot, maybe chemical weapons.
Some of that I'm sure you probably think is good,
but for AI, none of it's good.
Talk to me about why.
And again, I'm not saying you're wrong at all,
but I think this opinion really should be voiced.
Well, AI doesn't exist yet.
We don't know what it is.
It's changing, it's totally new.
Yeah, it's preposterous to try to set standards
or rules for things that we don't understand at all.
You know, you said open AI is clearly ahead.
Well, I don't think actually they're ahead at all.
They've done some particular thing
and they're maybe the world's best at that or whatever,
but it's not, it's not an intelligence.
And so, because intelligence hasn't happened yet.
And so it's totally inappropriate
to try to control it at this point.
Now, even later, you know, what is intelligence?
Intelligence is just the ability to work on goals.
And it's gonna be an algorithm.
It's gonna be an idea.
You can't, can you, you wanna try to outlaw an idea?
You know, it's not, you know, it's an idea
that will be cheaply reproducible and more so over time.
Yeah, I mean, it's just, it seems like the most,
and what is this idea?
This idea is just the development of humanity
and their minds to increase and improve.
It seems a totally impossible and inappropriate thing
to try to control as a technology.
Got it.
Well, yeah.
I mean, it's like, it's like, you know,
it's like trying to control ideas,
like trying to control the printing press,
trying to control democracy.
You know, democracy is a terrible thing.
No one is a democracy.
Some that were thought of,
that are trying, there are no democracies,
and then, you know, things change.
The way we evolve, you know, it's similar.
You know, could you outlaw democracy?
Could you outlaw a religion, a religious idea?
These are all terrible things to try to outlaw
or to try to control with regulation.
Just see the AIs among those things.
So for you, the best likelihood of this worthy successor,
again, not worthy, in my opinion, Richard,
worthy in the notion of the blooming of potential,
the best strike for that to this,
all these beautiful values you articulated,
sustainability, you know, diversity, discovery,
whatever, would be nationally and internationally
absolute minimal restriction on anything in any direction.
This would be what would give us the best shot
at something that could really carry this project forward
and maybe carry us up with it.
Yeah, but it's much worse than that.
And you haven't quite picked up on this.
Go ahead.
I say the way we run our societies today,
we should embrace the idea that we don't,
we are peaceful and we don't try to force
other people to do things.
And it's when that attitude is changed.
It's violated, yeah.
Yeah, that's what we should be doing now.
Yeah, and the notion that those governments
should be in control of the AIs,
this is the wrong direction.
It's backwards.
Well, and I think there's a whole separate idea
which I'm gonna shove under the rug,
but there's a sort of separate idea
that inherently high grand potential,
higher intelligence capability entities
would either always or even in any sustainable way
converge on cooperation may or may not be the case.
One could easily see a great intelligence
that at some point would wanna use atoms
for something more useful than you and I.
But let's shove that under the rug for the time being.
I think there is, to your point,
a great emphasis on let's build a world that's worthy of,
let's build a world that's worthy for ourselves
and for these future intelligences.
And let's focus on the now.
And I think your insistence on the now, again,
I really hope there's articles about that that come out
because I think there's a lot of merit
to what you're articulating here.
And to your point, maybe people aren't picking it up,
maybe even I haven't fully picked it up.
And now, Richard, unfortunately,
I don't run a country or anything.
I just have these dialogues
and I run a market research firm
on the enterprise side of AI,
but I can certainly help to proliferate this conversation.
I think it's a point that other people need to hear.
The warning against it is, hey,
if we don't get this darn ecosystem right,
if we are playing this game of conquering and conquerors
in all sorts of manner of plunder,
bringing about AI and controlling it
and bringing it into that environment, not a good thing.
There's a worst case where we kill ourselves off with nukes
because we're all fighting over who gets to build the AI
because we're in this plundering mode.
Is there also a version of AI
getting birthed into that plundering valued ecosystem
where it maybe doesn't exemplify
the peaceful cooperative elements that you've mentioned?
If it's birthed into the wrong world,
in other words, what's the warning
that the people who don't get this future thing?
What you've been very insistent on
is we've got to make sure this nest is right
and that we're exemplifying the right values.
If we don't get it right,
is there a possibility that AI itself
could become the wielding element of danger,
not just us?
I get it, you and me can be baddies, Richard.
Is there a possibility something else
is conjured that also is?
Well, as again, anything is possible,
but I think the real question is,
are we more likely to get our shit together
and make a world where people can cooperate and be peaceful,
or will an AI be able to see that that's a better solution?
You know, we are talking about creating greater intelligences.
We're not talking about making greater bombs.
We might hope that by creating an intelligence,
so here's a question.
I think we had it.
A question, are you a cynic or are you an optimist?
I am an optimist in the sense that I think
that greater intelligence would be able to see
that the cooperation is the better path, okay?
But a cynic actually believes that
if you make an AI, the AI will see
that the best way to win is to control everyone else,
and so it will take over and be a dominating singularity.
I think this is an underlying,
those who worries that the AI will take over will say,
of course, if an AI was existed, it would wanna take over
and it would, because it would think
that's the best way to achieve its goals.
And yeah, so this is a test of every reader or every listener.
Do you think that the smarter you are,
the more you're gonna wanna take over,
the more you're gonna see that as the superior solution,
or are you gonna think the smarter you are,
then you can see that the long history
of our civilizations has been
that cooperation is what makes us powerful.
Well, and I think there's a lot to be said
of our increased prosperity through cooperation,
and you've very much correlated those
throughout the entirety of our conversation.
I think you've hurled, yeah, I'm not gonna call it an insult,
but you've heard the label of naive
at certain beliefs that you don't like.
I'm sure there are some people who would say,
oh, the smarter it gets, the nicer it gets.
Okay, Richard, and maybe they throw the N-word at you, Richard.
Now, I'm not gonna do that,
because I don't think you're naive at all,
but to your point, the purely cynical take
might be just even sillier if they're saying,
well, when it gets smarter, it's gonna get worse.
There might be a middle ground camp that says,
the permutations of competition and cooperation
among an intelligence a million times beyond my own
will certainly be beyond my conception,
and to think that they'll fit nicely
into my notions of democracy and kindness
would be ridiculous, so that would be my opinion.
But to your point, and so if I hold that opinion,
I don't necessarily have the smiling friendly guy points
and intelligence go up together.
I think divergence and unpredictability,
which you've emphasized a great deal here,
will be the only thing we can predict,
and that maybe eternal peace isn't something
we can predict as whole, but I certainly can hope,
and you mentioned it as a hope,
that as it gets more intelligent,
it will in general see cooperation as a better path,
and I can tell that that really is a hope for you,
that if we build it in the right environment,
we get a bit of our act together,
that could be the good future that we're headed towards.
Yep.
Cool.
Well, you mentioned here, Richard,
I'll close on this note, because I liked this a lot.
At the end of one of your talks,
you really bombarded folks,
I think this was your AI succession talk,
you bombarded folks with 1,000 ideas,
many of which we've both agreed
are uncouth in today's dialogue, right?
People aren't allowed to talk about these kind of things,
and they attribute malice to them,
which I don't attribute any malice to you,
but you did say at the end of it,
at least we live, what an exciting time to be alive,
at least we live during this time,
and I would say, no matter who's listening to this episode,
they have to agree with you on at least that part, Richard,
that would be something I would say,
so it's been a real pleasure unpacking this,
I've had a heck of a lot of fun
getting to know your ideas, better Richard,
thank you so much for being here.
Thank you, Daniel, it's my pleasure.
So that's all for this episode with Richard Sutton,
I hope you enjoyed this one.
I certainly appreciated Richard's emphasis
on the kind of world we're bringing AI into,
I think that there is something to be said of that.
AGI's created explicitly for military conflict,
or in the middle of one,
would probably be a different ballgame
than one's conjured in a different environment,
although as you saw over the course of the episode,
we have a handful of differences here
in terms of the inevitability of such cooperation
being achieved in AGI,
versus sort of AGI having morality
that sort of supersedes all of our notions
of competition and cooperation,
that's certainly my position,
and it felt like we kind of got on the same page here,
but Richard is more optimistic, I think,
than I am that cooperation will naturally come along
with intelligence,
I don't necessarily think that that's true.
And there was also a good deal of reticence
around sort of addressing, frankly, the idea of AI risk,
sort of as if it's always humans and AI itself
could never do anything, could never do any harm.
I get that, I mean, Richard's in the game,
he runs an AGI lab, it's maybe tough to say,
hey, this stuff could be potentially really dangerous.
I think it seems very reasonable to suspect
that it easily could be,
didn't quite want to get there in this interview,
and that's all right, again, got a dog in the fight.
But I think Richard has a lot to contribute
to the overall post-human trajectory conversation,
and there's some things I really want to highlight,
and there will be more details in the show notes.
The first of which is really the frank acknowledgement
of the fact that humans are not in control right now.
He has a firm emphasis on this idea
of nature as a dynamic system,
not sort of a game where we can sit here
as king of the hill for even five minutes,
never mind five million years,
that everything is always changing and shifting around us,
and the best we can do is find the best way
of being in the universe,
and that's what nature is already trying to do,
and that the most that we can do
is kind of ride those trajectories.
It would be ridiculous for us to freeze anything,
to freeze our form, to freeze our values.
I couldn't possibly agree with Richard more,
and I think that one of the elements
that likely need to be dispelled
about the grand post-human trajectory
is the idea that one of the options that we can select from
is the option of an eternal hominid kingdom.
I think that is a very ridiculous supposition.
I think the speed of change,
maybe the direction of change a bit,
if we have any volition at all,
maybe we can influence that,
but we are riding those waves of change,
and Richard was very firm about that.
Some of my own analogies,
like hands on the steering wheel, handing off the baton,
he was right to say that it is much fluffier
and wilder than that.
It is always in flux,
and I really do believe that that's a perspective
that I'll be integrating more into my writings,
and that should exist more in the very frank,
if we're gonna be honest,
frank conversations about the post-human future.
As with every episode in this series,
and every episode in the previous series,
down in the show notes, you will see a full article.
In this case, it will list
Richard's Worthy Successor Criteria,
as well as a synopsis of his ideas
about what innovators and regulators should do
about the Worthy Successor dynamic.
In our upcoming third episode,
in the Worthy Successor series,
we return to philosophy with someone who,
I might argue, is one of the best-liked AGI
and transhumanist thinkers,
who's been at it for more than two decades.
So you'll have to wait till next episode
to catch who we've got next,
but I hope you've enjoyed today's episode,
and I'll catch you next time here in The Trajectory.
Thank you.

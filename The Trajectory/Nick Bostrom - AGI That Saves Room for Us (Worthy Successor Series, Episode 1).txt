This is Daniel Fugellen. You're tuned in to the trajectory. Today marks our transition
to our second series on the trajectory, referred to as Worthy Successor, where we're going
to be exploring permutations of post-human intelligence who we might be happy could populate
the galaxy and do things beyond what humans can do, either living with us or taking over
from us. And for our first episode, I thought it would make sense to bring on none other
than, at least in my opinion, the premier living post-human thinker, Nick Bostrom. I had a conversation
with Nick Bostrom ten long years ago on one of my other podcasts when Super Intelligence
came out, famously a bit of a skeptical book about the survival of humanity after post-human
intelligences are birthed. His latest book is called Deep Utopia, where he's a little
bit more optimistic about AGI maybe going right and what that could look like. In this
episode, our first episode in the Worthy Successor series, I unpack with Nick some
of his ideas around what going right might look like and what the traits and qualities
of a Worthy Successor intelligence would be. He goes into what he hopes for the future
trajectory of intelligence in general, how he thinks about moral value, and a little bit
on how we might nudge the future more in that direction. I'll give a little bit more analysis
in the outro, but without further ado, I could not be happier to kick off the Worthy Successor
series with the man himself. Here's Nick Bostrom on the trajectory. So, Nick, welcome to
the trajectory.
Hey, Donnell.
Yes, good to have you here. It's been hard to believe it's been a decade since we chatted
last when your last book came out and got my hands on a copy of your latest one. I wanted
to be able to dive in on this very important topic for this new series that we have around
the Worthy Successor. You've written a lot about everything from singletons to utility
monsters and everything in between in terms of the post-human state space of mind. I would
like to ask, how would you define a Worthy Successor intelligence, something that you
would feel okay about handing the baton of this project of life? Where would you begin
there?
I think the best case scenario would be where it's not replacing of what currently exists,
but a continuation of it and then maybe an adding to it. So, I think it's already a little
bit sad if all people who currently live die and then, yeah, maybe there's another generation
and another generation, another generation after that. It still would seem nice if people's
lives could be saved. Now, of course, the background condition here is that we are all dying all
the time. So, from a person-affecting point of view, the baseline is that we are basically
all dead in 100 years or so. I mean, a lot of us, much sooner than that. But if wishes
were horses, it seems like we would up on to this future every person currently alive
and maybe other creatures as well that are morally considerable, like certainly some
animals, and then have trajectories in front of us where we could continue to thrive and
develop and grow perhaps over long time spans into some form of post-human beings for people
who are interested in that. And then maybe adding additional beings to the world as well,
like that could be completely digital minds or AIs and other types of entities that we
can create. But it would be more like an plus type of vision rather than just a replacement.
Yeah. So, planting all plants and animals with one computer mind that would then move forward,
it sounds like to you, would that replace richness in like an abstract philosophical sense or would
it simply harm humans? And of course, you being a human, you have a vested interest in what mammals
are sort of up to. What's your take on that and preference? Is it a more complete exploration
of the state space of value if it is an and as opposed to a replacement? Why do you stand there?
Yeah, probably a more complete exploration of the space of values in as much as some values might
not supervene on time slices of the world, but on entire trajectories. Like there are values that
care not just about what happens at a particular time, but sort of how it came to be. So those
kinds of values obviously could be more fully realized if you're also having the right kinds
of trajectories going into this future. Now, I should hasten to say though that when saying and
here, it doesn't mean that all aspects of the current condition should be preserved in their
present state. I think in fact, maybe the most important moral imperative is to get rid of a
whole host of horribles. But even normal, relatively happy human lives, I think there's
a great potential for them over some timeframe to develop further and maybe the end result
will be something quite different from our familiar form of homo sapiens. So it's not like
change is not good. It's more like there are certain kinds of changes and it could be nice
if those who are already here have a chance to participate in that. I mean, in fact, yeah, sorry.
I was going to say, I would think that would be nice too. I think exactly what the odds are
of that are sort of up in the air and you've certainly had writings a little bit on both sides
of that coin. When we think about it about, okay, humans can maybe ride upwards on this
trajectory. You have this state space of minds from a very old TED talk of sort of where humans
occupy and maybe the animal world occupies and what the grand post human trajectory of possible
states of mind could be. If you zoom forward, it doesn't really matter the timeframe, pick yours,
a hundred years, a thousand years, whatever. It does seem at least from my vantage point and
certainly from reading some of your writings that it would maybe be less likely that humans would
be running the show. Maybe we're in the mix to your point in a different form than we are now,
but probably not running the show. You've had the idea of the singleton that would sort of manage
complexity, the idea of sort of better versions of a utility monster that could sort of bloom
vastly post human value into the world and maybe also preserve humans if there's a right way to
kind of govern and manage those transitions. Who do you expect would be running the show in a best
case scenario and what sort of values would that entity have? It sounds like one of them is it would
permit hominids to stick around in some form, but how else would you describe what this thing is?
Who is our orchestrator? Yeah, so we've kind of been running the show for the last
10,000 years or so. I don't know how you would score our performance. I mean, it's possible that
we've had our chance now and that it's time for a change of guards. Obviously, a lot depends on
what precisely we would be replaced by in terms of having their hand on the tiller here.
I think, yeah, it's not so much that hominids need to be around forever. I mean, I think,
from respecting different people's choices and having a future that accommodates values,
I think it's nice if people who do for whatever reason want to remain exactly like they are now
with two legs and two arms and just run around and doing their biped things for 10,000 years.
Probably, it would be nice if they can keep doing that, but I think there are a lot of other
possibilities as well where we would eventually be transformed into something quite non-hominate
like. I mean, if you think about a child today, five-year-old, let's say, now eventually, maybe
they become like a 30-year-old and they're quite different. A 30-year-old has a very different
mind, a very different body, different interests, different relationships with other humans than
the five-year-old, so it's fairly profound transformation. Yet, we don't think normally
that it's bad for a five-year-old to eventually grow up. In fact, many of us would think it would
be somewhat tragic if the five-year-old stayed five-year-old forever and never really had a chance
to develop. I think similarly, we all are infants currently compared to what we ultimately could
become and that there is a, you might view it as a huge tragedy that is unfolding, that we have a
kind of 100% infant mortality, like we all just developed for maybe 20 years and then sort of
stagnate for a few more decades and then rot, but maybe for a human being really to reach
its full potential in terms of maturity of personality, etc. Maybe that takes like 3,000
years and we've just never had a chance to observe that. Completely on the same page
with you here, Nick. Again, I think probably in many contexts, you've got to sort of ladle it out of
making these analogies. I'm totally with you about the grand post-human blasting, probably most of the
audiences as well, not completely nascent in that domain. When that expansion occurs, and some of it
will be humans too, I think there will be very few humans 10,000 years from now who are doing
normal hominid things as we do now. I suspect that the adoption of technology will necessitate
wild change, but let's just say that such a future exists. The entity that is running the show,
do you expect it to be a conglomeration of sort of melded human minds from some sort of BMI project?
Do you expect it to be a grand sort of babysitting AGI that is essentially making sure that the
planet Earth is very happy and friendly for digital and physical minds together? Do you expect it to
be something vastly beyond all of that, doing things we can't possibly comprehend throughout the
universe, blooming value in ways that we don't understand, but allows us to stay alive on our
little rock here? Who would you hope 10,000 years from now, Nick, would be running the show? Would
it be an evolved version of you and sort of fellow hominids or would it be something very different
than that? I think what might be most important is that what values are ultimately shaping the future
and it could be human values, even though the actual administration, sort of the day-to-day
governance, the decision-making, might be more efficiently done by super-intelligent AIs.
If they are kind of acting on our behalf, then we might get better results by having that acting
on our behalf be done by super-competent, super-aligned AIs than some random human politician who's
like kind of persuaded us to vote for them and then bumbling around and trying to represent us.
So I wouldn't fix it too much on the precise mechanism that does the governing. It's more
on the kind of interest that it represents. Let's dive in on that. So I could see very
much an argument against, so I'm very lucky, Nick, that the first early rodentia with a
postage stamp of cortex, whatever its values were, were not carried up into hominidness.
I mean, some of them were, I'm sure, maybe caretaking for young or some weird proxies,
maybe enjoying cheese, Nick, I don't know. Some of them maybe I like, but I'm actually very glad
that I have blossomed beyond a great many of the values of those little rodents and that state
space of potential and values has opened up. I think there is tremendous value in the continual
opening of those values. You've certainly articulated that there is a moral case for that,
but would you push against it? When you look forward to a future that you hope for,
is it somewhat of a solidified sort of human values with a preference for human intelligences
and then some kind of intelligence, whether it's brain-computer interface or AI or whatever,
that's managing and kind of housing that, or would you hope for something vastly beyond that?
Yes, I was sketching earlier what I, my guess would be maybe the first best option would be. The
future is enormously big, so why not have a little slice of that where us currently living
humans could also participate, because it would be such a trivial cost, as it were, in the scheme
of things. Now, as a second best, it's not clear just how much we should, than in
how much we should then insist on the, like continuing 100% of all the different peculiar
things we happen to care about as humans, and how cosmopolitan we should be with respect to
what would count as some impersonally valuable future. Even rodents, I think, have quite a lot
in common with, I mean, they want pleasure, for example, and it is an interesting kind of
philosophically difficult question, but if you imagine extrapolating the volition of a rat,
so it can't really articulate very well, its desires and preferences maybe can't even itself
really properly understand what they are, but if you imagine cognitively enhancing rats
to the point where they could start to reflect on their own values, and giving them a lot of time
to do this and information, it's not entirely obvious that the end result would be very different
from if you extrapolated a human, I mean, maybe we would all just converge to being
a hedonist or something, and AIs, I think, might realize a whole bunch of different values,
even if humans were not particularly shaping them, and it then becomes really very much a
question of metaethics, I think, the degree to which it is plausible that the, I don't know,
the ultimate theory of what has value is very closely shaped, like it has a human shape or
whether it would be a huge coincidence to think that precisely the things we have happened to
evolve to, like, also match this kind of independently existing moral reality,
but to really get to grips with those questions, I think you have to kind of wrestle with
metaethics, like thinking about what is the nature of moral reasons and values, where do they come
from, and then maybe from there you can kind of find some grounds to stand on, that is not
simply a regurgitation of our own subjective tastes and preferences.
I would certainly say a regurgitation of our own subjective tastes and preferences would be a
wildly fettered conception of the grand state's base of possible values, it sounds as though in
terms of what a worthy successor would be for you, wouldn't matter really the substrate, what it's
built off of biological, non-biological hooplots, more of its values, part of that is preserving
a slice for current instantiations of sentience, whether they want to maintain their physical
body or not, sort of this sort of like babysitting corner is a part of the value structure of what
you would prefer for a worthy successor. What would you hope such a grand, million-fold intelligence
beyond humanity would do outside of that little orb, maybe, Nick, and I hate to say it, the more
important things it would pursue outside of babysitting URI as we want to maintain our human
form and kick a soccer ball and trim our fingernails and do other things like that.
What would you hope that the intelligence would aim to achieve, reaching out beyond that little
marble of babysitting? Yeah, it might depend on what it wants to achieve, I mean I might hope
that it gets what it wants, that is one type of conception of values, that it's sort of based on
the preferences that different agents might have, maybe not raw preferences but idealized
preferences are weighted by some attribute entity that has the preference. There might be this kind
of conditional desire, like if they want X, I hope they get X, if they want Y, maybe I hope they'll
get Y. There might be complications and qualifications to that, especially if there are many of them
and they have values that are intentioned and I hope for like some cooperative future.
But if we kind of anchor on vaguely human-like values but that are not sort of super tight to
the particular current biological incarnation, like things like pleasure, maybe
knowledge, various forms of aesthetic beauty and complexity, these
learning, achieving things that seem worthwhile to the person achieving it,
it might be that there are like sort of pockets of convergence in some of these areas as well,
such that a wide range of different intelligent species, many of them would kind of
want some of these things. Like there might be other things that are very idiosyncratic to human,
like for most obviously our sexual preferences, we tend to desire other human beings, like obviously
a scaly alien species would have like one scaly aliens and think they were really hot, right?
It seems like the more independent a value is of the particular random idiosyncrasies
in our evolutionary trajectory, the more plausible it is that the same value would be pursued by
many other intelligent entities. I concur, I guess just to touch on the idea of that
state-space of possible minds from your now very old TED Talk, it strikes me that there are things
that you brought up aesthetic beauty, pleasure, et cetera, that probably maybe even a dozen times
more intelligence than we have would still be rich, interesting, and curious and worthwhile.
It also strikes me that in that grand empty canvas of possible state-space of minds,
there are values for which you and I have no words and no ability to imagine
that are vastly grander and richer than those human values, not that I'm devaluing the human
values or that I want them to go away. I'm not saying that. I'm simply saying I don't necessarily
think that they are the greatest or the grand detractors. There are values we know not of,
we cannot imagine, we do not have words for that could be astronomically and vastly more valuable
that could possibly be explored. Some of what you've touched on in the past through all your very
writings has touched a bit on this. How do you feel about that? Clearly preserving the values
that are super worthwhile now absolutely lets do it. What is your take nowadays on that other
state-space which could be grander? I think there are a lot of ways of being and relating
and experiences that if we encounter them, our jaws would just drop and we would think like
holy moly, that's like, I don't know, we were wasting our time with this old human stuff and
thought that we had no idea that it could actually be like this. This is like a whole
different thing and wow. In that sense, I certainly think there are value or valuable
modes of being that we could discover. If you're talking about something outside that,
then I think it becomes a kind of meta-ethical question of on what grounds would we judge that
the particular object was astronomically valuable if it had no relation to what we or anybody else
wanted. I'm kind of maybe more skeptical that there would be a kind of plausible
meta-ethics that would make that a life possibility for the way things could be.
I should also say I've been thinking about some things recently. It's not
it's kind of still in the work so I'm not really ready to roll it out but that maybe connects to
some of these things we have been discussing about. We are very focused on this alignment
problem. A lot of the conversation now in AI is around that but I think there might be more
dimensions to that and that may be a more I know you might say cosmopolitan outlook on what would
be worthwhile to achieve with AI rather than this kind of narrow fixation on aligning to
current human values even if not the best possible outcome might still be a lot better than just
extinction followed by nothing and so how to evaluate the relative desirability of these
different things as things to aim for is I think I have some ideas there that I'm hoping to develop
more. I look forward to reading them as they bubble out. I'm going to try to nutshell some of
what you've put on the table for us already around this worthy success or idea make sure I'm not on
the wrong page and then we'll clarify a little bit at the end about maybe some of your hopes of how
we could measure and ensure we're going to arrive at that sort of a future and not a grand paper clip
producing entity. What I've picked up from what you've articulated here is that you'd hope that such
an entity which might sort of run the show as we have done for the last 10,000 years would maintain
this little marble of earth and the individual instantiations of consciousness even if they
take different forms or maybe even if they decide to maintain their existing form it would possibly
build upon some of the values that maybe you seem to find valuable and maybe from your
metaethical analyses have sort of discerned to be good pleasure aesthetic preferences you know
things along those lines that it would it would carry some of those that torch
outward and then also you sort of hope that it gets what it wants and the way that I thought of
that as you said it is like if I was a a fish with legs and I was walking out of the water
and then you were to come up to me and say hey eventually you're going to bubble all the way
up to hominids and you guys are going to build spacecraft and it's going to be a totally different
world what do you hope those humans do as the fish if I were honest I would say well jeez I have no
idea what it's possible state space of values or actions would be I would hope it could do what
it would want when you said I hope it gets what it wants it almost felt like it was from that same
standpoint of like if it is that grand and vast and morally valuable of a mind it will have better
ideas of what it wants and I hope it's able to pursue those that's sort of how I interpreted that
but I want to make sure I'm not misinterpreting you Nick let me know if we're on the same page here
um yeah no that's broadly it and I think there are different bases for that like one is
um that I think a broadly cooperative and generous attitudes towards the future is more likely to
make things go well from a host of different perspectives including our own and that might
also have a more specifically ethical underwriting and one form that this kind of cooperative
attitude could take I think is for different entities out there that want different things
to try to uh a hope or help them get what they want it like obviously there are limitations to
that if if what they want conflicts with what you yourself happen to want or with some other entity
then you might not be able to give everybody a hundred percent of what they want in every situation
but other things equal I think it is nice um and should be encouraged I have this paper called
Basecamp for Mount Ethics which I wrote a few years ago it's actually not a proper paper but
more like a series of thinking notes but it's an attempt to sort of outline a kind of
meta ethical theory like a perspective on what ethics is and how that relates to some of these
questions um I should maybe hopefully one day be able to like write it up in a in a way that's
actually like much more clear and understandable so but um but but it kind of views like it puts
forward for consideration the idea that morality is a kind of slightly idealized system of norms
that some entities develop and um and that there could be uh levels of different norms
that might be kind of nested so you might have norms within the family or a community and then
sort of higher level norms that you know at the national or international level and and you might
also have norms at an even higher level that we haven't uh been thinking much about yet a kind of
set of cosmic norms um that that we probably uh should make sure uh we conform ourselves to
as we develop into a more advanced civilization ourselves or build super intelligent AIs so
base camp for mount ethics people can check that out also uh sort of living along with
digital minds I'll find the exact title nick but that paper is one I'll also reference in the show
notes as as we wrap up we've got a little bit of homework of some of your other thoughts we've got
a future of uh where humanity and current life is in some way uh protected some of those highest
values are continued forth into the universe and otherwise these grand post-human entities with
ideas and values and actions vastly beyond our own are getting what they want hopefully in a
non-conflictory way and blossoming that outward into the galaxy there are many not so great scenarios
nick uh of of the pay-per-click scenario of of maybe humanity going to war uh over AGI before
we even get there um what are your hopes in in closing here around innovation and regulation
that that you would hope policymakers business leaders etc would bear in mind to inch closer
to that successor you've talked about instead of landing in a bad place
it's uh it's quite a complex issue I wish I had like a clear set of policy prescriptions about um
I don't at this time I think you can be vague I mean there's obviously been a big increase
in awareness of the potential for transformative AI to really you know create risks uh but that
generates change to human condition and we now see a sort of top-level policymaker like statements
coming out from the White House the UK had this global summit on AI etc um and there is an increasing
the vocal set of people calling for an AI pause or like like more kind of um biting forms of regulation
on AI development and so I think it seems desirable if there exists a possibility at some critical
point of development like when AI really starts to take off um to to go a little bit slow during
those stages like if you're the AI lab first figuring out like the missing ingredient or
something but there are like 15 other AI labs uh pulse on your heels and so if you slow down even
for four months it just means somebody else zooms past you and the race goes to have a
gross caution to the wind the quickest that would be uh I think risk increasing and so
so having the ability at that critical moment to have like a time limited pause of you know
six months or a year or something like that would seem good like I would start to worry more if
if the way of achieving the pause was one that possibly could result in a perma ban on uh advanced
AI it might start out the idea of just have this less brief little thing right but then um that
then once you can prove that this AI is safe then we're going to permit progress to go forward how
do you ever prove this unless actually by running it or you set up a big regulatory agency and then
it kind of accrues more and more powers and it becomes impossible to do it or you create such
a stigma around AI that it just becomes impossible for anybody to say anything positive about it and
then you might get a lock in like so far in human history these kind of phases have been temporary
like some some you might get some super ultra conservative or like viewing a probe but eventually
something shakes loose or some other country zooms ahead but we can't be confident that that
will continue to hold we might already have levels of technology which if applied in certain ways would
permit the kind of permanent locking in of current orthodoxies and beliefs if you imagine even rolling
out current AI technologies to their full extent with automated censorship and propaganda and
surveillance like that might already make it possible to sort of fix a temporary opinion to
make it so yeah so I would become less excited in proportion as I think these attempts to regulate
or oppose AI had a risk of spilling over into something very long term or even just long term
enough that the risk would start to rise that we destroy ourselves in some other way in the meantime
so even like 50 years might be well long enough to create a significant existential risk coming
from other sources like six months doesn't seem we probably won't go extinct in six months anyway and
so that's that that's one thing yeah and and then I think like things that more broadly increases
the chances of a cooperative outcome where everybody has like a slice of the upside and
where also the interest of digital minds themselves can become recognized and given weight I think
would be very very desirable you've articulated sort of I think things are very rational and again
I'm just going to nutshell and get your take on it before we wrap here but certainly global
authoritarian but Larry and Jihad locking in current states of base of human values could be
outlandishly dangerous I think almost everyone agrees with that Yasho Benjio others we've had on
violent arms race of whoever can throw caution to the wind also seems unlikely to get us to a
worthy successor in my personal opinion sounds like yours as well to your point though some degree
of coordination is probably likely you're articulating this idea of a pause of a reflection to use Toby
Ord's terminology from from precipice here how would you hope currently that we would permit
such a pause that would not turn into that kind of authoritarianism would it involve the UN
blossoming into something with a little bit more muscle would it involve international
organization and alignment in some way where each nation has to reel in the labs within their own
jurisdiction I mean we all I think want to avoid you know Caribdis and whatever that other Greek
you know mythological is here yes yes yes I appreciate you you're at Oxford you got to have
all this stuff memorized so how are you hoping we'll thread that needle because it's not super
obvious but maybe you don't have the whole plan but maybe an intuition that we can wrap up on
well the original plan as it were I mean not my specific but like amongst people who are thinking
about this is that that would be some AI developer who would just happen to be significantly ahead
of the others like at one point deep mind was ahead and like so maybe like the lead developer
would have like two years lead time or something like that just just like not every technology
product is exactly equally advanced so you'll just and you could maybe help this along a little bit
if it looks like there was a clear frontrunner people could sort of hop on and try to help them
along and not help the closest competitors then you would have a situation where if the lead
developer let's suppose to have some decent level of concern about safety and pro-sociality like
that would be some pressure that would have to cross but then when they when they get to the
point where they could create super intelligence they would have an opportunity to slow down for
maybe a year or two just kind of burning up their lead and that that would be desirable and then
that lead that that kind of pause would expire automatically as other people started to catch
up technologically and so that maybe after two years you would have another AI lab you know maybe
in some different country kind of catching up and then if you wanted to extend the the pause
even further you would have to sort of persuade one more entity now that it was actually better to
pause so maybe if the risk was obvious enough then maybe they would also agree let's let's
together pause for another six months but the kind of the difficulty of continuing the pause would
increase over time as more and more labs gain the ability so it would ultimately expire spontaneously
and it wouldn't turn into a kind of perpetual ban and so that would give you some of the
good features like it would give you this the opportunity to slow down easily and and it wouldn't
have much risk of spilling over now that might not be on the table anymore in as much as currently
the AI race is kind of a little bit more competitive yes so now something analogous might require a
little bit of coordination amongst say the top three frontier labs or something like that and
and it might mean also that needs to be more government coordination if one wanted to make
something like that happen but that then brings in additional concerns of course like I think
some of the people currently being very eager to get governments more involved I mean it remains an
open question what they will think afterwards because once you open up the box I mean maybe
it is the right thing to do but like not everything that gets tossed into the political realm is then
settled by good good meaning people who rationally deliberate the option you know with full information
and come to a sensible conclusion like that that it's a bit of like anything can happen once
something becomes a political football and once something becomes a geopolitical football like
the stakes are potentially even higher so clearly you know pros and cons to governance it sounds like
you're not on either side of the fence if you were a betting man nick and again you're not
committing to this nor am I for some eternal opinion but it feels like it's up in the air for
you if you were a betting man and say right now we're in a very clear race US and China and all
the major labs do you suspect some degree of government coordination to sort of bring about
a bit of reflection is more or less likely to get us to the worthy successor than the pure
state of nature circumstance your opinion may change in two months or two days but as of right
now do you have any betting man take on that nick and reason why as we close out yeah I mean
right right now it's not so much a race between the US and China it's more like US racing against
itself it's true it's true yeah I think probably the optimal would be to have more government
oversight and regulation than there currently is but it's one of those things that you might not
have very fine-grained control like you can sort of maybe set off an avalanche but once it's on the
way you can't sort of then stop it at the optimal point so I would worry a little bit about overshooting
the target here and yeah so I'm just I'm open-minded about this and I think like it's the kind of
thing where it's probably not like there's one position that's correct that you can derive
ex ante but you have to sort of see how things are flowing and then what opportunities there are
to nudge things on the margin and that that will likely change over time and so I would like look
for opportunities if there is like something that seems cooperative and constructive like
nudge it in that direction but I would be more leery of things that kind of strategies that take
the form of sort of I'm gonna stand on the tracks and and say stop or I'm gonna try to revolutionize
the world to implement this particular vision like I think those have like greater opportunities
for backfiring and and that there's like a kind of these these things are so big like one feels
so little as a human being that it seems almost preposterous to come up with a grand plan that
one is then like pushing hard to implement I would completely concur and I think this is a good
synopsis a little bit more governance but let's be careful of these wild ideological you know
steer the entire I think what you're saying is we can kind of bend and direct this grand blooming
here but to think you're gonna step in and sort of totally grab the steering wheel feels very
very counterintuitive hopefully Nick will be able to sort of make our way between those monsters as
well as Odysseus did and hopefully we'll be able to do so with losing a couple less guys than he did
along the way I guess we'll have to see how the governance ideas bubble their way up but I appreciate
being able to have your take on on how we can get to a more worthy successor here yeah no good good
to talk to you awesome glad to catch up Nick so that's all for episode one here of the worthy
successor series this episode is a little bit shorter than some of our follow along episodes
with some of our other guests and some of the components of what we covered surprised me a
bit certainly Nick's tone has shifted from more of existential risk to potential moral upside
of AI I was really surprised to see some of his emphasis on very anthropomorphic things so if he's
famously a proponent of sort of this vast state of post-human value and post-human sentience that
could possibly be explored we talked about things like appreciating aesthetic beauty and kind of
pleasure in human terms interesting to see Nick put a lot more focus on kind of the human experience
as we start to make this transition in addition to the grand post-human realms of value that might
be unlocked which he did mention a little bit he said something that I tend to agree with and that
maybe we'll come up as a theme in some of our future episodes which is that he would hope that
whatever this intelligence would do it would sort of get what it wants and we are we unpacked in
the episode a little bit of what he meant by that I happen to agree I think that something that can
value and discern goals beyond us should explore that state space even if we humans can't imagine it
but I may be a little bit less optimistic that that would imply inherently sort of a good treatment
for humans I think there's many ways to pursue value that don't necessarily involve keeping the
hominids sort of happy and healthy but I appreciate Nick's shift towards optimism I think many of his
ideas are brilliant and I would encourage you to check out his book I got to thumb through a good
deal of it before this interview here today in addition down in the show notes of every episode
of this series and of our previous series we have an article that breaks down the key takeaways so
in the case of Nick we talk about his actual worthy successor list and some of what he thinks we
should do to nudge and move in that direction we'll be doing that for all of our follow along
guests so you can look side by side at some of the criteria of a worthy successor from many
different AGI thinkers and then folks that we have on for this particular series so hopefully you'll
enjoy that again that's going to be linked down in the show notes as for our next guest I won't
say any names but this is a fellow who has made some major contributions to the modern era of
artificial intelligence and machine learning has been maligned in some regards for being okay with
the idea of machine intelligence overtaking humanity and populating the galaxy and seeing that
as natural and normal again has been kind of vilified for that and may or may not live in
Edmonton Canada that's all you get for clues you're going to have to tune in next time here on
the trajectory so stay subscribed thanks for tuning in I look forward to catching you then
you

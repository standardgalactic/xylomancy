AI is not based on imitation of biological intelligence or human intelligence. AI is
itself a crystallization of collective intelligence. It's part of, you know, of the
cooperation of our mind and bodies through the society. So here we have a political dimension.
How do we organize collectively and what is our understanding, you know, an evaluation of a
collective agency? And I think this is an important political point.
Hello, welcome to the Verso YouTube channel. My name is Richard Hames, and I'm here with Professor
Matteo Pasquinale to discuss his new book, The Eye of the Master, a social history of artificial
intelligence. Artificial intelligence is a concept that I think is now very widely known. People
refer to it often in kind of daily conversation. They have a notion of the AI as a sort of a
single homogenous unit. This is a very widespread notion, but it's also one that's informed,
particularly by very recent developments. And what we normally think of or more precisely
would think of as generative AI. That is the, you type in a prompt, and then you get something out
of it. You get an image, a text, something like this. And this is almost a kind of magical
view of AI that we have, I think. People have pretty commonly typed something in,
you get a magical box, something amazing comes out. Maybe it's good, maybe it's bad,
we don't really know. Your book situates artificial intelligence in a much, much, much, much,
longer history in the whole kind of social process of the construction and design
operation and algorithms, which you take to be a very capacious category. And I want to start with
that notion of the algorithm. So you have this very beautiful diagram. I'm not quite sure exactly
where it's from, the Vedic diagram of the Agnika Yana. Yeah, precisely, which is a sort of social
process. Maybe you can talk us through like how you see that as an algorithm, because it's not
the kind of thing we might normally occasionally associate with algorithmic structure.
Yeah, I think one of the purpose of my book was like to provincialize AI, to put in a
longer history of cultural technique, and to show that techniques of abstraction belong to many
civilizations across history. So even the idea of the algorithm that today we perceive as a sort of
in the technological abstraction that belongs to very complex software
is a mathematical technique that I have very long history. And if we look at this
Hindu ritual, the Agnika Yana, that is the oldest ritual of humankind, still practice today,
is 3,000 years old, recorded by oral sources at the beginning. You can see that already in
ancient India, they were building fire altars, temples, according to very mechanical procedures,
they were actually repeated, chanted through mantra, that look like algorithm, not just in their
mechanical structure, in their kind of robot-like behavior, the workers were moving bricks
following, you know, this mantra, but those in the fact that we're embedding mathematical
technique, like differential equation that maybe we could recognize today in the work of Newton
or Leibniz. So the Agnika Yana is a very fascinating example, a very ancient ritual that already
contained many, many centuries ago, sophisticated technique. And what is interesting of the
Agnika Yana is to be what I call it a social algorithm. So I start from this example, you know,
to provincialize, to recontextualize AI, and to show how this sociality, how collectivity
actually operate through the whole history of mathematics, I believe also, you know, when
modern mathematics started to emerge in the West, at least in the Middle Ages,
and up to the recent algorithms of computing machine, but also sophisticated techniques,
like deep learning, that have been, in my view, emerging gradually, you know, in the last decades,
like a sort of, you know, avatar growing out of social relations that have been condensing,
accumulating, becoming data, big data, you know, like through the ramification of
social networks, digital networks, and so on. And both these computational instantiations
of algorithms, and these more general social processes or more broader social processes,
follow the basic definition of an algorithm, which is a finite procedure of step by step
instructions, which has an input and an output. And then that input, that process is, to some
extent, independent of the data that goes into it. Is that the broad kind of definition algorithm
that you're using? Yeah, I think the definition you gave, it's culturally correct for computer
scientists, they're very technical definition. But what I'm interested is in the genesis,
genealogy of technical artifact. And actually, how do we explain this development? What is our
theory of automation today? And actually, we don't have a robust theory of automation.
I'm interested in the fact that machines, during, for instance, the industrial age,
emerge gradually from experiment, from the original labors. To me, the concept of the algorithm,
that of course is more immaterial, that belong, you know, to the information age, that belong to
the age of computers, is I'll have a very similar in its genesis to the notion of machine. So I see
algorithm as sort of, I think I'm not the first one to say that, I think even Minsky probably was
using this expression like abstract machines, right? But to me, as interesting, we have a
political economy of the machine, we have a political understanding of the machine, we have
a lot of literature from the 19th century, we don't have a political reading of the algorithm,
or we start only recently to do that. So my attempt was also to politicize
the idea of the algorithm. And actually, your question goes to the point, because to introduce
something complex like AI is good to clarify some basic notion, and the algorithm is one of this.
The politicization of the algorithm seems to me an important project, and I'm wondering how it
relates to something else we might try, and what you think of that, which is the naturalization
of the algorithm. So we can say that we have rivers full of sediments, for example, and the river flows,
and then it deposits the sediments in a structured ordering that is according to the weight of the
particles, and to solubility, and temperature, and many, many other things. Why is that not an
algorithm? I feel like there's a skepticism, a skepticism is emerging. Algorithms do not belong
to the natural order, they belong to the social order. And one thing that I do in my attempt to
sketch a political economy of the algorithms, much as a political economy of the machine,
is the fact that I respond to an economic logic. And actually, what is at the core of the idea of
machine itself, and the algorithm too, is the fact that I have to follow an economic logic,
save resources of time, space, so there is an intrinsic economic logic that sometimes
is not clarified in definition given by computer scientists. Computer scientists,
they are aware of this because they have to save resources, but it's clear that the algorithm has
to do something very efficiently consuming less resources than other solutions. And so this is
very important. This is of course tied up the algorithm to the complex dynamics of our economy,
of our society. Rivers don't have this problem. So in that sense, the difference between natural
order and social order. Rivers don't have that problem. That's maybe a bad example, but I'm not
going to label this point, but I think maybe there's something about, for example, the way that body
plans work in organisms as they're gestating in the womb, the way in which they structure the
different components and organization of the organism as it's developing in a way that responds to
restrictions, the way it responds to sort of overall distribution of resources
in the, you know, and so on. Well, that's not labeled the point there.
But I think your idea, it's very close to what was also the program of cybernetics. Cybernetics had,
wrongly in my opinion, the idea that machines and organs were organized in the same way.
And this is part of the fetish we developed towards cybernetics, sort of vitalism that
we liken cybernetics, but I think it's bringing us in the wrong direction.
So there is, in my book, there is a critique of this sort of cybernetic vitalism,
and how all the discussion they had about the fact that organisms were like organized machine,
machine, we could design machine like organisms, but actually an ideological, you know, cover up
for imitation of other form of self-organization that happened in the society at the time. So
indeed, there is a chapter in my book, you know, about this critique of the idea of self-organization
that we cannot transplant, you know, from the natural order to the technical order and back
to the social order. But actually, we have to analyze that where it came from first.
One place we might see that fetish operating most strongly today is with the collection of
algorithms that come out of the deep learning revolution. And in particular, things like
large language models. So one people will probably be familiar with is chat GPT. This is the kind of
prompt architecture I was talking about before. You write in a prompt, it gives you back some sort
of like, what seems to be a person speaking to you, right? It's like a pretty convincing
illusion by this point. We've produced a sort of a hard beginning in some ways to the notion
of algorithm that it responds to the social order or it's a logic of the social order.
Is there something that we can point to specifically in machine learning or deep
learning, more particularly, that allows us to understand the way in which algorithmics
is entering a new era of its sort of long history? I think the algorithm that we try to
define as a basic unit today, of course, explode in terms of complexity, scale. So when we say
that chat GPT has, I don't know how many parameters today, but trillions, right? What does it mean?
We have what is called a parameterized machine that adjusts the parameters to a large data set.
This is an algorithm, but clearly also from this description is an adaptive algorithm, right?
And this adaptive algorithm that I explain in more in details how it was originated in 1950s,
the structure of artificial neural networks, is an adaptive algorithm that showed to be
very efficient in modeling any form of human culture, right? So from visual data set, images,
pictures at the beginning, now even text, what are called multimodal data set. What is astonishing
is that collective intelligence, human intelligence, culture, cultural heritage is
modelled by this system. We discover recently, we knew because our experiment existed since
decades, but we discover recently that this algorithms, this adaptive algorithm are very
efficient in modeling language and generating also very realistic language,
items, sentences, text. And this is very interesting because I think will affect maybe also the way
linguistics is done today. So there is very interesting phenomena related to the recent
development of deep learning. Is there a particular, like, so it's an algorithm that is
adaptive. Let's just dig into that notion of adaptation. What does it mean by adaptive
in algorithmic terms? So it's that the parameters of the algorithm are in some ways determined
by the input. So the inputs in the algorithm are interrelated in this sort of feedback lip.
I think if you want to go into technical details, what is the core of artificial intelligence?
It's a specific technique that is not very often a name term and it's algorithmic modeling.
It's something was invented in 1950 was the first artificial neural network.
And I think it's an artifact that needs much more research and theorization.
Going back to the example of language, what is very interesting, right, is the fact that
at the beginning of information technology, you have a formalization of language, right.
So an information machine is basically a machine operating through a very simplified language.
There's a binary language that was the idea that Shannon somehow, you know, inherited also from
the telegraph that is operating with the binary language. You have this extreme formalization
that, you know, somehow start the computer evolution as a form of, you know, language
machine, we can call it like that information machine or language machine. At the very end,
I'm not surprised that then through a few steps of this technological development,
you have a machine very good at modeling languages. Once, you know, we have reduced
languages to some very basic units, so very binary binary opposition simply, and then how we
basically projected language through this complex space of deep learning.
So we don't have a very capacious, sorry, we don't have a very capacious view of what an algorithm
is as a sort of social form that is bearing out the type subtitle of your book, social history
of artificial intelligence. And in particular, you say that the source of many algorithmic
sort of properties or designs or the kind of the thing that algorithms are sort of parasitic
on in their computational form is the social division of labor. This is a very kind of crucial
idea that's in some ways the main thesis as far as I read it off of the book.
And I wonder if there's a kind of way of maybe just expanding on this.
But I'm going to kind of suggest and then you can tell me why it's wrong, which is that in the
Marxism, classically, we have a split between the relations of production and the means of production.
And I wonder if there's a sort of a way in which the the structure of the relations of production
in producing a particular commodity and sort of laying out the design of a factory, let's say,
or laying out the overall design for labor in a society, that whole schema is then abstracted
and then placed or sort of inscribed inside part of the means of production. So we have
this kind of translation from the relations of production into the means of production.
That's one way I could think that we might sort of say to talk about this in conventional Marxist
terms. But what am I getting wrong here? What's the what's the argument? No, I don't think your
argument is wrong because it's also the thesis of my book. As I said, we have a political economy
of the machine. We don't have a political economy of the algorithm. We describe very well, you know,
the rise of industrial machinery according to what was the logic of the division of labor and
the economy of the industrial age. I think the same model fits perfectly the rise of artificial
intelligence in the 20th, 21st century. So I think we have to go back to this almost for say,
forgotten theory of automation. In the book, I call it a label theory of automation that actually
was an idea by Adam Smith and Babbage before being an idea actually used by Marx and also
Hegel himself when Hegel, you know, had this intuition or this kind of the expression concept
of abstract labor. I think we should go back to that idea that machine emerged from the imitation
of the division of labor. Plus, of course, as I said before, in the case of the algorithm,
you have an economic rationale. So a machine, they have to save time and space. And it's not a
case that, for instance, the chapter on machinery in capital volume one is within the part on the
production of relative surplus value. And I think this labor form and value form have to be also
recognized within what are the dynamics of algorithms today. And I think we can bring many
examples from platform capitalism to artificial intelligence to show how indeed this is, I mean,
at least for me, from my perspective, is the economy of computation today and of AI. And I
think what is very important is to demystify AI and to show how it's not an invention of
mathematicians, engineer, geniuses. It's not something coming from the study of biological
intelligence, but it's clearly something that has emerged gradually from a sort of
crystallization of social relation and then following automation. In the 19th century,
it was used to say, first, organize, then mechanize. That was the slogan, right? Today,
I think we can use the same slogan. We can say maybe first formalize, then automate through AI.
You mentioned Babbage then. And the other crucial thing that Babbage invents is the idea that the
automation of labor is also its measuring, also its calculation. And I want to kind of, if you
could talk us through why you see that as so important to the machine theory of labor, the labor
theory of machines. Machine theory of labor, very bad theory. No, no, no. The labor theory of the
machine, much better. The labor theory of the machine. Again, yeah, okay, one part of the book
is kind of archaeology of machine and algorithms, especially the first part. And it goes back to
debate that you had in England here around the machinery question, actually. The machinery question
was a large debate that happened in England before Marx. What is a machine? What is a machine doing
to labor, to workers, to society, and so on. Babbage had a clear intuition that it was not
just the fact that the machines were this mechanical monster emerging, imitating the division of labor,
but that the division of labor itself allowed the accurate measurement of cost in production.
So there is this dimension of metrics, this dimension of measurement of human performance,
but also of human skill. So Babbage knew that any division of labor, including, you know, the
machines in this apparatus, were producing also and defining different classes of skill and actually
producing polarization within the skills in the factory and so on. I think the same happened with,
you know, information technology, with the algorithm of AI. And I think we have to keep
this lesson in mind when we study algorithms today. Algorithms are the, you know, forms of
labor automation. It can bring many examples. But at the same time, they become implicitly
instrument or implicit, instrument of measurement, implicit metrics of human capacity.
The case of artificial intelligence for me is striking because clearly AI today is implicitly
a measure of human skill and human intelligence. AI can be taken, you know, in a very romantic way
to show what people can and what people cannot do, you know, that can be used also to even to
threat workers to show what a machine can do. So it enters immediately. Now, we can have,
any of us can have this experience. But then in the book, I show the algorithm themselves
of AI, of deep learning, emerging in 50s from psychometric. So from the discipline of psychology
statistics that actually introduced the IQ test, the test of, the test of intelligence.
Test of intelligence is like this brutal, this brutal procedure through which you
determine the intelligence of a person according to numerical result in a cognitive test of
different, you know, task capacities. But that was like the statistical technique that was
used by Rosenblatt also to automate image recognition. And this is, for me, a way implicitly,
the way in which implicitly AI is vaculating, you know, a matrix of human intelligence,
but that is connected to very reductivist social order.
Yeah. So the one hand we have in the design of these systems, we have an implicit parasitism
or implicit withdrawing of a logic from a social division of labor, which instantiates a hierarchy,
a sort of hierarchy of labor, which is what division of labor actually means. And then they
get translated into a sort of means of production into a sort of a machinery that itself then reinforces
later on that self same hierarchy, which it's, it's based on. I wonder if you could talk to us
about that reinforcement of the hierarchy, or give us some particular example of the way in which
you think a particular say, machine existence might enforce a discrimination or forms of
hierarchization in that labor process. Yeah. And in the book, as I use the expression,
crystallization of social relations, that becomes, of course, a crystallization of hierarchies.
It's clear that any machine, mechanical machine, information machine, and today I produce a
polarization of skills, you know, back in the industrial age, you had skill and skill labor,
you had the role of, you know, workers were just supervising the machine, what Marx called
machine and arbiter, just, you know, people in front of the machine, regulating the machine,
not doing much. With information technology, we have the same, we have this polarization and
bifurcation. I have a little chapter about how indeed, also, the French philosopher,
Gilles Bertrand-Mondon, had the idea that the industrial machine itself already was born out
of this bifurcation of the source of energy and the source of information that was producing,
you know, two different also classes of work of labor, of performance to be,
to be done within, within the fact. I think the same is happening probably with, with
artificial intelligence, but the very, very, in a very, very complex way.
It's clearly that we see how, I think Facebook and Google themselves in the last year have been,
you know, firing out so many people, especially coders, because they are aware that some part
of their work can be automated by these large language models. So these large, large language
models are entering the labor market and producing polarization themselves and producing and pushing,
indeed, you know, classes of workers in the opposite direction. Those can be easily automated
and those cannot be easily automated. So I think it's a very complex phenomenon. The starting,
you know, from manual labor, going through, you know, different components of the labor market
produce different waves of bifurcating skill. The impact, however, of these large language
models, for me, is interesting because it's not about simply about replacing workers in their whole
capacity, but it's more like replacing micro task in our daily activity as worker or in,
in the office, in the sense that each of us has used, you know, this system to translate small
tax, right? So it's not there, for instance, we take the work of the translator is not the
replacement of the translator in its hair, his entirety, but like micro task, the translator,
maybe they have to perform in their daily work. And this will happen to many categories of work,
you know, think about graphic designer, people working for, you know, advertisement agency.
When these tools are known, any, any boss will know that can be used, you know, also to,
to extract demand more performativity. So what I perceive, you know, in the effect of AI on the
labor market is this, not just the replacement of the whole task, but, you know, the replacement
of micro tasks in our daily routine. And this will have it, of course, in effect, because
through the system, we will be forced to, to work more, probably, not less.
And so there's this paradox in AI research called the Morović paradox, I think I'm
pronouncing that Morović, Morović, who knows, Morović, where tasks that are seen trivial to
humans. So for example, moving around, balancing, walking around, picking things up, putting them
down, those kind of tasks that seem trivial, turn out to be incredibly difficult from a
computational perspective. And then tasks that in some sense seem easier. So it seemed harder
from a human perspective in the sort of conventional social hierarchy, playing chess,
playing go, now writing letters are actually computationally much sort of simpler. And that's
generally used to refer to a sort of strange paradox, which is why, for example, we might
say it's not so unusual that coders are being made redundant. We think of coding as a sort of a
high prestige, high value, high difficulty task. And yet lots of them are being made redundant
by chat dpt4. But in some ways, we've, if we look back at the longer history, we might say that
it's not that those kind of manual tasks are as such harder to automate, but that automation
has already proceeded so far through the history of other technologies, not AI, but other technologies
that we've reached a state where they actually now have reached some sort of plateau, and it's
difficult to do self driving cars, for instance. Yes. And therefore, I wonder what you think about
this. Clearly, yeah, we see the, we perceive the effect of AI automation specifically, you know,
and the sector of white colors. But I would like to go back to the, to the first paradox,
actually, as I mentioned, the very first page of the book that today, strangely, you know,
we discover, you know, manual workers, you know, like truck drivers or car drivers, you know, as
sort of a task in which a high degree of intelligence is embedded because we indeed, we try to automate,
you know, the activity of a, of a truck driver, for instance, through, through deep learning.
So it's very interesting, or it's a kind of bitter, you know, political
insight that we recognize, you know, the import, the cognitive input of manual labor today,
because AI is trying to automate these activities. So I think this is the,
another paradox, maybe we should rediscover for ourselves, that indeed, this system are
very efficient, we call them multimodal, they're able to model, not just indeed, very efficient
when they have, of course, to model language, when you have to operate on textual dataset,
but also very efficient when they have to model 3D signals, you know, images,
video that could represent the, the, the, the scene of a street in which a car has to go through.
We know there are big limitation, but it's interesting how indeed we are rediscovering
through AI, you know, the fact that all labor has been always a kind of, you know, cognitive labor,
of course, we knew that, but there has been a sort of, you know, amnesia also of our political
economy, labor organization, critical theory about this aspect. So I'd like to go back to this
other paradox that actually we are encountering now. Yes, rediscovering the intellectual aspects
of what seemed like they were the most kind of manualized forms of labor for sure. You say at
one point, and maybe it's a polemical statement, I'd like you to sort of unpack it, all labor is
logic. This is seems like a kind of a really, in some ways, it's a, it's a, it's a provocative
statement, right? Because reading it from the perspective of someone who is in some ways being
taught that the ways in which labor is organized or automated is a reduction or a sort of a,
an assault on the true creative spirit of humanity or something like this. How do you see
that notion of the logic functioning in that sort of context? Yeah, I think it's a provocation in
both directions, a provocation for our understanding of and definition of labor today, also within
labor organization, critical theory, but those are provocation clearly not always computer scientists,
mathematicians, you know, and cybernotations. And on one side is indeed as a critique of
the idea that labor is just a manual activity. On the other is the idea that logic itself has
a history, that logic emerge through a kind of complex history, and you have different
logics. And this is also clear in the case of artificial intelligence that today, for instance,
you know, it's not following, you know, the deductive logic of the 1950s and the good old
fashion AI, but of course, move to a completely different paradigm that is the one of statistical
learning and inductive logic. So when I say labor is logic is also an attempt to merge
two worlds to two kind of communities on one side, if you want, the political economists on
the other, the computer scientists. And it goes back for me also to the history of this artifact
that I try, this abstract artifact that I try not to retrace back to a social history. Then if
we look attentively of this kind of social logic that, you know, we embed, we crystallize into
machine, we can bring, we bring many example, you know, from, you know, from art, or from ancient
rituals, and so on. There's a Robert Hewlett-Kentaur article called The Exact Sense in Which the
Culture Industry No Longer Exists. And so he's talking about this phrase that Adorno uses,
culture industry. And he says that the reason that the culture industry no longer properly exists
is because what Adorno would have seen as the obvious tension and sort of friction between
these two words, culture, on the other hand, and industry, on the other hand, when we look at the
word today, it no longer jumps out at us as a contradiction that there would be such a thing
as a culture industry. And in some ways, there's sort of a similar friction that you're implying
between these two worlds, the world of labor and the world of logic, and the way in which they
grind against each other in this sort of terminological provocation, trying to bring these
two things together and show how they fit, even though both sides would of course refuse the
idea that there is a continuity there. I want to go towards this history of logic idea,
because I feel like there might be a sort of difference between us when we can talk about that.
At one point, you're talking about Rosenblatt and his work in psychometry, and the way in which that
statistical techniques that he uses to do these what are psychometric evaluations,
and those themselves, as you were mentioning earlier, proceed from the history of eugenics.
And these kinds of multidimensional analyses that he uses, he then reapplies elsewhere to the
question of perception. And this you notice a particularly significant kind of moment of
translation between these different kinds of social objects. And I guess my intuition,
as someone who is not particularly well versed in this history, is that passing through this
abstract mathematical object of multidimensional analysis, in some ways, cleans the history out.
There's a sense that the maths has an autonomy, that maths is not exactly floating above the world,
but that techniques are discovered and not merely invented, that there's sort of an
independence to mathematics, that means that when we reapply a certain set of techniques,
it's neutral in some sense. Tell me why that's not the right way of thinking about the history
of mathematics. Okay, now going back to this not well-research and known moment in the history
of AI that actually we should one day dedicate probably monography. So what I mean intrigued
is the fact that when Rosenblatt somehow produced a convincing solution for the problem of image
recognition, that is a kind of intellectual faculty of the human mind, how you recognize an
image, he was using a technique derived from psychometrics as we mentioned before, the
technique of statistical psychology to measure the intellectual capacity of a human being,
considering the relation between different results in a cognitive test. So psychometrics,
the IQ test, emerge as a way to measure numerically, statistically, human intelligence
in a very reductivist way, as we know very well, I mean I don't recommend anyone to do the IQ test
to understand what intelligence is, and strangely that measure of intelligence, that technique
to measure intelligence, it became a medium to automate an intellectual faculty. I'm
interested in this passage that is very complex and difficult passage also for historian of science
and technology, but the same thing happened in other historical moment and you can also see for
instance the rise of the steam engine, the way the steam engine emerged from also an idea to
control a measure energy, to measure labor performance, that was a way in which workers,
human beings, but also animals like horses were measuring their performativity, and then from
the measure of energy at a certain point you have strange phenomena in which you have a medium that
produce energy and automate that kind of measurement you had before. So I'm very interested in very
complex and difficult passages in the history of technology that also, you know, for me,
history of labor automation. Psychometrics is very fascinating, it's difficult to explain because
they're supplying, you know, statistical tools of multivariable or multidimensional analysis,
as a history that is like a horrible history going back to eugenics and racist policies
comes out from origin from craniometry, this pseudo science that thought indeed to measure
intelligence of human beings according to the dimension of the skull. This is the image of
people going out of calipers, right, so they're going to measure skull shapes. Yeah, as Stephen
J. Gold has this great book that is The Mismeasure of Men, that I always suggest for anyone who wants
to understand how AI operates today, it's one of, I think, probably the best book to understand the
logic of AI as well. And so he had craniometry, craniometry failed, psychometrics took basically
the role of craniometry, but maintaining this idea that, you know, human intelligence could
be measured, that we could define quantitatively, numerically, also classes of skill of intelligence
through society, so I mean, you know, imposing social and nuclear intelligence. And then at
a certain moment in the part of psychology, in the 1950s in the US, this technique, psychometrics,
makes a jump and go into computer science, and Rosenblatt uses it actually to organize his
neural networks. And this is like a very unclear moment and very complex moment. And we find the
same technique today at the core of deep learning. So even this very large language model, like
charge EPT, they basically operate, manipulate, you know, property of this multi-dimensional space.
What is interesting for me, of course, mathematics, in my view, is not completely autonomous in its
development. Psychometrics implied a very reductionist view of human intelligence and society.
And I would say probably the same happens today with large language models in the way they reduce
language, they're very efficient, probably because of this operation, they reduce in a very efficient
way to small unit, and then they can automate it efficiently. I think we will see then the effect
of this reductionism, not on the long run, maybe on the short run, in the way indeed I'll have,
you know, errors, you know, phenomenon of reduction of, you know, everyday language
is already happening through the system. I guess my question is like, what does that
politically commit us to? So we notice that there is a history or when we explore the genealogy of
these techniques, we notice that there is a moment at which this technique is used or even informed
in the service of a racist project that we would absolutely repudiate. And then for like, well,
what happens now? You know, what do we do with the fact that there's a does it commit us to
anything politically or ethically? Does it should it shape the way in which we respond to these tools
as they will be, you know, thrust upon us or engaged with our lives?
Yeah, I think what we should take from AI today is the fact that artificial intelligence itself is
not this fantastic embodiment of intelligence, but we can use it actually as a great description
of what is the state of the art in the collective intelligence. So all the biases, all the errors,
all the problems you see in AI, indeed, are a fantastic photography of our, you know,
culture today and political constitution. So rather than taking the system as indeed an
embodiment of intelligence, we could shoot take them as, you know, as a perfect photography of our
mediocrity. And I think in this way, we could develop, you know, a healthy dialectical relation
with this system, you know, and I think we should, yeah, consider in this way as imperfect machines.
I think we should indeed provincialize AI, you know, step back and see it in perspective.
Also, the term AI, as I try also together with many other scholars to to demonstrate,
as a misnomer, we should one day maybe define the system as indeed, you know, just system for
algorithmic modeling that, you know, we should find other expression like maybe statistical
learning, you know, to remove this anthropomorphic, basically simulacrum that we have attached to AI.
And I think this is an important process to do also to avoid, you know, some,
some expectations and usual question about the systems.
You're absolutely right that there is a sort of a sense of the camera taking a picture here,
that the snapshot of the internet, for example, as it was in 2021, I think is the last time that
it was updated, maybe more recently, 2023 was the latest chat GPT for model was was retrained at that
point. So you're absolutely right, there's a sort of snapshot of mediocrity. I guess like,
if we're to be suspicious of this very basic unit, which is the vector, right, the technical
object that sits at the very basic kind of bottom of deep learning, if we're to be suspicious of its
its history, then why is it so sort of in the terms of a kind of famous paper like so
unreasonably effective at modeling so many different domains of life. There's another thesis
that people might be thinking of when they when they when they see the eye of the master,
which is the thesis of the the seeing like a state thesis, James C. Scott's work, which is that
essentially quite similar in some ways that there's a social organization of labor of
structure in society, such that it can be visualized or such that it can be seen by the state
overseen, things are organized such they can be seen. And if we have this notion that AI forms a
sort of a snapshot of the mediocrity of human culture, is there some sort of similarity you see
in that thesis? Do you see there's a continuity there? Or would you say there's a sort of a
difference? And I mean, one of the major differences might be, for example, that James C. Scott is
not as far as I know a Marxist, and therefore not really particularly concerned with like
labor process, but concerned with a much more kind of heterogeneous collection of things.
No, I think indeed, I insist in this book, you know, on the primacy of labor organization in the
way we design artifact. So I have a kind of, you know, design theory that brings design itself
and design of machine and also AI to what is the ontology of labor today. But what they try to do is
also, you know, to escape some categories of the political economy of the 19th century and to see
how labor today takes different form through society and how you have social relations themselves,
they're becoming productive. And today clearly, you know, what our smartphone, the algorithm of
platforms for the gig economy and the algorithm of AI are doing mostly, of course, they're not
automating the labor in a factory, but they're like following our movement through the urban space,
for instance, they're like, you know, producing this very complex snapshot of the diagram of our
relation through society. This is clearly also emerging through the way, you know, monopolistic
platform like social media are organized. So I think AI here is part, as we know, of this long
evolution of the information society in the 10th century into the network society of the 90s.
It's part of this evolution of the network society in this, you know, organization of few data centers
around the planet that control all our communication. And the algorithm of machine learning, then at
the end, emerged as a way to compute this surplus of information of this data center. So you have,
you know, the gradual, you know, evolution, what we call today AI was machine learning and was before
data analytics. So I see AI, the eye of the master as a sort of evolution of this information
infrastructure that is built on the data we produce on social data, and is responding actually to a
problem of elaboration interpretation of those data. So today we have powerful algorithm because
we had to compute massive amount of data, you know, big data. And big data we should remember
doesn't mean just data that are large in size, but they're like, you know, multiple in that
dimension. They describe, you know, different variables, parameter of your life. And, you know,
companies, they didn't know how to interpret this data. And they develop algorithm one day,
they found in in machine learning, and then in deep learning, and then in this large language
model, also called foundation model, the most efficient technique to interpret this data that
data, you know, that are, you know, produced by our collective body by the social organization
of liberal at large. And what's the political valence of that? So we have this notion that there
is a in the large language model, the whole of human cognition up to the state in some sense is
captured the archive is snapshotted. What are the political valences of that? What should we then do
with it? Do we understand it as a collective product of the whole of humanity? Do we understand
it as a particular proprietary knowledge of this particular company? What are we to do as how we
to interpret in the field of political struggle? Yeah, no, this is an important question. I think
today I represent a mechanization of collective intelligence, a mechanization of the general
intellect that Ricardian socialists somehow perceive as not mechanized, as not materialized,
represent interestingly, you know, indeed, a mechanization of education of cultural heritage.
And I think we are not ready to to understand what's what's happening. What is interesting is
also the scale of this monopoly, you know, large language models can be computed only by large
companies or superpowers that have a lot of computing power facilities, you know, data center
to collect all this data, you know, the powerful clouds were to, you know, amass all this data.
So there is clearly a process of monopoly monopolization. And then this indeed monopoly
on a model of our culture that is often, you know, control in a privatistically in a private way.
And this is something that happens for the first time in the history of humanity. And what does it
mean to fight politically this form of monopoly? I think this is part of the struggle around the
new form of a platform capitalism of, you know, technological capitalism that is organizing
through very few monopolies. There's a sense in which this these models, because they seem to
encapsulate so much, give us a sense of what the human and a very general sense is. So we have this
notion that by interacting with AI, we are interacting with GPT-4, let's be more civic,
by interacting with GPT-4, we're interacting with like the whole wisdom of humanity. I've
heard it described in these terms to me directly. But there's of course a kind of a way in which we
might want to refuse the idea that that is the whole of humanity. You mentioned earlier the
necessity of establishing what you described as a dialectical relationship with a technology. Is
there a kind of notion of the human that you want to counterpose to this overall snapshot?
Because the way in which you describe humanity or the way in which you describe the particular
intelligence of people in the book is as situated as sort of externalist, as kind of organized in
in in relation to a situation. I'm wondering if there's something you want to kind of
rescue from this snapshot or to dialectically oppose to this snapshot?
Yeah, I think as I try to discuss in the book, the AI is not based on imitation of biological
intelligence or human intelligence as, you know, individual organisms. What is extremely interesting
for me that AI is itself a crystallization of collective intelligence. So what is collective
intelligence? Collective intelligence is not, you know, the intelligence we possess individually
is part of, you know, of the cooperation of our mind and bodies through the society.
So here we have a political dimension. There's been always part of any
political project. How do we organize collectively and what is our probably understanding, you know,
and an evaluation of a collective agency? And here, for me, looking at the history of
political movement, we see a process in which collective agency, collective intelligence,
is, you know, translated into machinery and especially into, you know, monopoly control.
And I think this is an important political point and also an important point, you know, for culture,
education, pedagogy, and these things. There's a chapter in the book on Hayek,
Frederick Hayek, a famous neoliberal economist, and his relationship to connectionism.
There's this extremely interesting 1952 book called The Sensory Order, which, as you say,
precedes the Dartmouth conference by four years, where normally, although as you point out,
redundantly, lots of the initial ideas of AI were formed, sort of sketched out and so on.
Some people nowadays on the left regard the arrival of AI as a sort of a way of resolving
some of the main sort of problems, right, the capacity of this single object to
speak in the interests of everyone as kind of a socialist technological utopianism that is being
developed around AI. And I wonder if there's a kind of a, your framing of Hayek as sort of being
the seat of this suggests either this project of trying to understand AI as a kind of a tool
for planning, a kind of master tool for socialist planning is doomed, or whether or not there's a
kind of a perhaps a strange dialectical transformation in which Hayek by in some ways
lighting the spark of connectionism that leads to AI and then AI leads to planning
or the capacity of planning to happen. There's a sort of a strange grave diggers of the bourgeoisie
transformation happening here from 1952 to, let's say, 1929 or something, some future.
Yeah, Hayek was dangerous for being probably a smart mind, you know, that was very well
literate about the scientific research of his age and was still in a lot of idea, if you think
also the way he was one of the first, you know, to deploy the idea of pattern that today is
a trend, you know, pattern recognition, you know, pattern generation, but Hayek was one of the first.
He was very, he was very smart, you know, stealing this idea. And his epistemology is interesting
because he's an epistemology based on spontaneous planning or spontaneous modeling and was much
more advanced than indeed of this postulate of the Dartmouth workshop on AI from 1956 that we
considered the beginning of AI. What they discussed at Dartmouth workshop was, for me, a very simplified
and reductivist and almost useless view of what human intelligence and machine intelligence
would be. That's this famous moment when some, I think, just says that the entire problem of
machine perception could be solved in a summer by a by a bright graduate student. And of course,
here we are, have many years later still working on this. But no, that book by Hayek,
The Sensory Order is one of the first books to, you know, also reach and the discussion on what
would be a form of algorithmic modeling or what does it mean to model the world rather to representing,
right? Knowing how rather than knowing that is part of that debate. And for us, it should be taken
seriously because indeed Hayek had an idea that, you know, planning, you know, it's something,
you know, you should develop probably not in a very rigid way, but in a very spontaneous way.
So I think the current form of AI, machine learning, deep learning belongs to this old tradition of,
we can call it spontaneous organization, spontaneous planning, and, you know, technique of
self-organization that actually were concerns also of the U.S. Army research in the 1940s,
1950s, 1960s. In the book, I tried to rediscover this conference on self-organization that
were related to neural network research that were like indeed, you know, engaging with a much more
sophisticated paradigm than other discussion organizations. And it was the same milieu from
which the idea of the Internet, the ARPANET, emerged, you know, a network that could self-organize
itself in case of danger or attack, a network that could self-repair itself. And so I think also the
the genealogy of the Internet and the genealogy of artificial neural network had some common moment
in some, you know, research conferences that are not very well illuminated.
This connectionist AI is, in some ways, truncated by Minsky and Puppet,
which leads to what is normally referred to as the AI winter, in which there's a huge dip in the
amount of research that is going into artificial intelligence. And you outlined in the book,
there's sort of a almost quite petty kind of squabble over funding or attempt to withdraw
funding back to MIT where Minsky and Puppet are based. And I want to kind of zoom forward,
taking that sort of notion that these nominally autonomous technologies are embedded not just
in the social organization of labor as we've covered, but also in this kind of strange,
petty, political squabbles between like different researchers and bring us forward into
a particular reading of what's happening right now in AI, which is that very rapidly there are
incumbent players, so people things like OpenAI and Microsoft behind them and even Google,
are attempting to get legislation or regulation passed for the field of AI,
normally about existential risk, normally about the possibility that AI might, in some way,
kill us all. And therefore they argue for regulation on that basis. There's another
reading of that, which is that what is really happening here is that the incumbent players
are trying to solidify their position and lock out other people who might come up and prevent them
from compete with them. What's your read of that situation? How do you see this kind of like
political squabble going on at the moment in the field of AI?
Starting from Minsky.
Taking the idea that AI is like dominated not only by
political concerns, but also by struggles for supremacy, struggles for dominance in the field
itself. What is interesting about AI as an invention that did not emerge very quickly.
It took like 60 years to consolidate. The logical form was somehow invented by Rosenblatt
in 1950, so it took like 60 years to consolidate. More or less the same time the steam engine
other inventions in the past took more or less the same time. So AI is not that recent that
we're trying to demonstrate in my book. And it's still, interestingly enough, an experimental
artifact, right? So we don't have today what is called a theory of statistical learning.
We don't know indeed how this large and what would sometimes operate. We have, you know,
trillion of parameters. We push the button. Sometimes we get a great result, but we don't
know exactly sometimes what is going on inside. What does it mean? It happened also in the past,
you know, machines were invented and then, you know, they were studied according to sometimes
they are normal or abnormal behavior. So still today AI is in this experimental phase. Being
an experiment, of course, comes with some risk. What you said about today, the regulation
regarding the impact on AI is correct. These AI monopolies are probably, you know, trying
to maintain their own monopoly. This system, a fantastic system that are matching perfectly
monopoly capitalism, that platform that, you know, that can be operated by, you know,
large companies with large infrastructure, large computing power, and probably they're
like to secure their dominant position. And what is interesting for me is the fact that
information technologies in the last century, they've been demonstrated to be a great technology
for monopolization. So they produce monopolies faster than other technology and AI itself is
producing monopoly faster than any other technology. So there is this drive towards monopoly that is
within, you know, the history of computation that is very interesting to me. And that's in
some ways where I think for some people, the utopian aspects of it come is the idea that
because it's assembling the general intellect, or it's because it's assembling a picture of the
general intellect, that there is a possibility of some liberatory potential here. And I wonder
what you think about that. Like, do you see, firstly, on the sort of the dystopian side,
where do you see the domain of existential risk? And then secondly, do you see there as being a
sort of a component over to, let's say, the left of the distribution towards a more sort of utopian
use of these tools that, as I mentioned earlier, lead to the sort of the the grave digging of
capitalism as such? Or is your understanding of the history as based on the social division of
labor does that preclude that kind of revolutionary aspect? I don't have any prophecy at the moment.
What I'm interested in is to follow the transformation of the labor market in this sense
that today I see this algorithms not used just for the automation of labor. I see them very often
for the automation of management, they're replacing bosses more often than workers and
actually they're multiplying workers. So to bring a good example, what would be the effect of AI in
society, we should look at the transformation of the labor market and this new figures like the
the driver, the rider, you know, all the workers of the gig economy, the care worker, you know,
controlled by online platform and so on. So this phenomenon of platformization of labor is really
related to the application of this algorithm in the organization of labor, but in which in the
way they replace managers more often than workers and they multiply workers actually.
And I think AI is somehow doing this is becoming an apparatus, a centralized apparatus for the
organization of labor, in which we are all prostheses, we are becoming, you know, gig worker
of this large AI, this large platform that is basically operating an automated intelligence,
but at the end organizing a different social order regarding labor. There's a sense in which
when we use these tools, we sit at our laptops and type something and a giant cloud computer
delivers us an answer, we have a sense that we are using a prosthesis, but in some other
much more profound sense, perhaps the prosthesis is us and we are being used by the Social Division
of Labor. Mateo Pascunari, thank you very much. Thanks to you.

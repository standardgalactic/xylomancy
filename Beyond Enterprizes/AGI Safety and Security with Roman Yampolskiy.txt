You
You
Welcome to today's conversation about what is going on in the world of technology
to beyond conversations, my name is David Orban and
Today we are going to talk about
the edges of a very popular topic
Artificial intelligence
Has been around for several decades, but it has come to the attention of a lot more people than before
not for the latest Hollywood movie
Representing the particularly gruesome
dystopian
Adventures
Finally resolved with probably a fist fight. That is what happens in those movies
but it has come to the attention of hundreds of millions of people through
AI systems that appear to do a lot of useful things in novel ways
specifically chat GPT and its various
embodiments and inclusions
in
various
applications or
in Microsoft's
search engine bing and the edge that I am talking about here is
Is
Not the current version of AI, which is a narrow AI
but
AGI artificial general intelligence, which we don't have yet
but with better think about and
I
Have as our guest today Roman Yampolsky
Who likes to think about what AGI can or cannot do and what we can or cannot do about the AGI?
Welcome Roman. Thanks for inviting me again
Yes, indeed. We did have a conversation
two or three years ago
and I thought it was time to
regroup
to
well
Ask ourselves
What changed or what did not change in the meantime?
But before we go into the
Think of our conversation. Why don't you tell our audience a little bit about yourself?
What you do your field of study your publications your line of thought sure I'm a computer scientist at the University of Louisville
I've been working on AI safety and security for well over a decade
have a couple books published on it here and there and
Probably over a hundred papers on that subject
That's what I do
Fantastic so how do you?
define the difference between
narrow AI and and AGI
So it's becoming a bit fuzzy it used to be that a narrow AI was a system which did one thing and maybe did it well
But just one it played chess or it did
taxes or something like that
general intelligence is
capable of learning
multiple skills transferring skills between domains
But I think there is some confusion between human level intelligence and general intelligence
Humans general in a domain of expertise of humans
We don't know much about things outside of that domain a truly general intelligence would be capable of learning all those skills
what we're starting to see systems which
Can in between they are still not general in that they can pick up new capabilities in any domain
But they can easily have hundreds if not thousands of skills
They play chess. They speak French. They can manipulate robot arms. So we are kind of moving from this narrow
one trick pony to
Becoming more and more general and that's the research progress we see and more lately
We are starting to get surprised. They have skills and capabilities within explicitly program M
So after you train a model you train GPT for then you start discovering what can it do and the designers
Developers get surprised that in fact it has thousands of new skills within anticipate and at some point
Will be surprised to discover it's fully general
And so the
Past
experiences with AI were
Narrow as you said or a given particular skill and
Even among AI researchers it became almost kind of a joke
as soon as
Some particular problem was
successfully solved
It stopped becoming it stopped being considered an AI problem anymore like like chess for example and
This interestingly reflected on our own
classification both
Practical and epistemological it was a reflection also on our own
Thought processes cognitive abilities and how we go about attacking certain problems
it is
Something similar happening in the field of AGI as well. Are we
reflecting on
whether
To start humans are general intelligences. I can freely admit that
If you list
my skills
Probably you will stop
sooner than
10,000 maybe you will sooner than
5,000 I didn't
try
but
You will pretty rapidly
Be able to say oh David is actually not a general intelligence
Well, I think it's about capability to pick up new skills
So if you wanted to learn to play a new musical instrument, you could
Whereas a narrow AI systems couldn't and I think that's the fundamental difference
It has potential of learning new domains and new skills not necessarily already has all that capability
So an average human knows very little but if you take humanity as a
Sad and take a union of all the capabilities. Yeah, we speak thousands of languages play hundreds of instruments and we're quite impressive as a group
well
You made the two two different remarks, so let's talk about the musical instrument and I I play
Very very very little guitar. I play practically no piano and that's it
So, yes, if I applied myself, I could improve my piano playing skills
But it is likely at this age
That I would never become a concert pianist, right?
Let alone one of the best pianists in the in the world
Same with chess. I I know the rules I play but
my
Loe's core is probably
Tiny and even if I applied myself
I would not be able to improve it to the level of
You know any anyone worth mentioning so
at
every individual
The theoretical possibility of learning is there, but a practical possibility is fairly limited
both by time and circumstances and and
latent talent
right
So I noticed we're doing this with AI right now
It used to be we were like maybe it will get to level of an average human now
Well, like it has to be like Einstein or it doesn't even count
The standards are kind of the goalposts are moved
So I think if you don't expect an average human to be the best comedian top inventor
poet and everything else you should not have same expectations for
Software
Okay, and then you
Went and and rather than considering an individual human as as I
Volunteered to be examined you
Considered all of humanity and said well as as a group all of humanity is capable of
Remarkable things and I agree for example
We have created a civilization that is itself now
In the process of creating ever smarter artificial intelligences
So humanity
as as a group is a
General intelligence we for sure don't have
More than one reference point
With respect of how general and intelligence can be
and so
Do you expect
The blurry
Interval between what used to be narrow intelligence and what is becoming more and more general intelligence
to
Be fed by human exploration
Exploration
In the sense that it is the coupling of human intelligence
With ever better AI models that will endow those AI models to learn new skills
rather than
AI models being able to
Learn new skills without human involvement
Well, at least for right now it seems that they still scale without any diminishing returns if you put more
Multimodal data with more compute they continue improving and
Going beyond our expectations. I mentioned humanity as a whole because our definition of super intelligence
Realize and being better than all humans in all domains. So that's kind of the next
Expectation level
Kurzweil talked about simulating one human brain by around 2023 and all human brains by around 2045. So that's the two kind of
Interesting points. We're starting to maybe be between
in and
When I refer to
AI systems being able to learn without human involvement is
What is is transfer learning what I refer to
How good are
systems today in
Abstracting their training and then applying that training to an increasingly disparate
set of circumstances
It's changing so quickly like in one week. We get new papers where this capability is different
Like if we had this interview last week, I would say it would be worse, but now I see systems which can look at a video
Demonstration of a skill shown once by a human and pick up that skill. So that's more impressive than a human child
Definitely, they're getting much better. I don't think they're fully general. They still definitely benefit from
Human supervision, but it's improving exponentially in comparison to what we as a species capable of
And and and I used to say that it is improving super and exponentially
I use the term of jolting technologies applied to AI and I
use the the data that
was
Collected by by me
Where we are not only referring to
one
set of
Accelerations, but the
The ability of looking at different data set
Public their report
Talking about a
class of
artificial intelligence
up to 2012 when they said the AI was following a
Morse law doubling in power
Every couple of years and then they said
After that
In this in this report published in 2019
We are actually seeing a new
Exponential with AI
Doubling in power every eight months
And they are talking about two distinct eras and in my view. This is a very
Quotation marks lazy approximation of the data set because
Since then we have further data and
And
Jensen Wong the founder of
NVIDIA
In their
2021
Global developer conference says
Not eight months anymore four months and in 2022
He said not four months anymore every two months
So the rate of acceleration is shrinking and
the measure of the variable rate of acceleration is the Jolt
Or the jerk but jerking technologies is less appealing
So that's why I talk about jolting technologies instead
So when you say
Things are changing rapidly from one week to the next and
Are you able to keep up with the with the the the science?
Not really. I used to be able to read every paper in a I safety when I started then it was all the good papers
Now I can't even read the titles. It's just too much
So let's talk about more
The specialized the more specialized field we mentioned AI in general
Then a GI artificial general intelligence and so let's touch upon
Or or or start to to go towards
AGI safety and security but before going there arriving there
Let's talk about
IT safety and security, right?
You you specifically look at how to design and if it is all possible to design
safe and secure AGI systems
but we
Have not been very good at designing
safe and secure
IT systems
right
Whether we are talking about
traditional viruses or the vulnerabilities of our hardware and software or more
recently ransomware
These are all demonstrations of the fact that that our software development practices
Are such that systems are
Working, you know, whether we are talking about
Google or
Facebook or other large scale scale systems
It is it is rare these days that they become universally unavailable
however
They are buggy
They have a lot of little glitches that each individual user doesn't see
But those who are
Trying to keep up the systems are very well
Aware of and the same is true of our operating systems of our smartphones and and and so on
do you see an improvement in
the
overall
practice of of software development and how software
Tries to be better and better engineered or
Systems get bigger and bigger, but their
their
Bugs are still as numerous as before
Per thousands or per million lines of code
The new systems typically rely on pre-existing code through libraries
Inherit so you just have this history of undiscovered bugs which keeps growing and you may have seven levels of dependency
So modern developer never sees all the code
The software relies on so if anything is getting worse
But the fundamental difference between cyber security and AI safety is that in cyber security if you are hacked
If something happens, you lose money. Maybe you lose time
Reputation but you get to try again you reset the password you issue a new credit card and you move on in AI safety
You only get one chance if it's an existential
Crisis, you're not gonna get a second opportunity. So a very different level of expectation
We're kind of used to this like yeah, everyone gets hacked all software has bugs
Just click agree on a contract, but that same approach will not work with
AGI
Well, let's dig deeper in in in that. So
You rightly said that we can afford to
iteratively develop
Software and
to
Observe the intended outcome and if the software deviates from the intended outcome
It's output is different from what we expected. We go back. We look at the code and we try to correct
Where the the deviation happened?
So
Is it
And and and you said that that the same approach will not work with with AGI
You actually published a series of papers
That are mapping kind of a boundary conditions of all the things that we are not going to be able to do with with AGI
We are not going to be able to explain it
We are not gonna be able to keep it in check and so on and so forth
Would you like to to list some of these things that we are not gonna be able to to do with AGI and
Without you know getting into excessive
Scientific detail, but why do you come to those conclusions? Sure. So the big ones are
Explainability we do not understand how large neural networks come to their decisions. A full explanation is the model itself
That's not something comprehensible to a human. So you have to simplify it
You have to provide a simplified explanation which necessarily emits some of the factors in the decision
There is billions of weights
Connections you'll get something like a top 10 reasons why you got denied credit
So either you are getting this lossy compression explanation, which is just not true
Or you're getting something you cannot comprehend
similar problem with predictability we can predict overall direction in which the system is going final state
But we cannot predict how it's going to get there intermediate steps in that process
And that's easy to see if you could predict those steps, you would be super intelligent
You would exactly know what the system is going to do next
So the assumption that it is smarter than you would not be correct
There is dozens of other less obvious results, but all of them combined to our
inability to control more intelligent systems indefinitely
So as long as there is a significant intellectual differential between us and super intelligent system
Long term, it's definitely not going to be obedient to whatever
Instructions we provide short-term
and now
We we will go back to the issue of
Information flow pattern recognition
and and recognition of reality
and
What necessarily binds us to
Compress reality in chunks that we can interpret with the tools we have available
but
before we do that
Let's see and let's go through a little bit
What happens if we accept the fact that
AGI cannot be
Understood
deeply or fully
By us and that AGI cannot be fully controlled by us
So let's assume that the AGI
happens
And it will do its thing
in a manner that we cannot fully comprehend and its thing is going to be
Not completely what we expect
Is this going to
lead to
Progressive deviation
That is bound to increase or could it
Isolate between various states that do not necessarily
Go far from what we expect or farther than a given
Deviation that we don't want to
Exceed
So first I just said it's impossible to predict what it's going to actually do
So I can't possibly claim I can but I would note that the difference between heaven and hell is one bit
Right, so a small deviation can get you quite far from your desired state
Okay
Now
Let's
Go back to
The epistemological foundation
Let's
Discuss your own work as a scientist and
Ask ourselves
The methods and and the claims that you arrived to
a
Whether you are likely to be missing something that could lead someone else to a different conclusion from from yours
How do you evaluate the validity and the comprehensiveness of your own conclusions?
So that's a great question. I actually have a paper on a verification verification of software mathematical proofs
And the conclusion there is that they are not verifiable to a degree of
100% certainty you can invest more time more resources more reviewers and get more more confidence that the software has no box
Nine nine nine nine and as many as you want nines, but it's more expensive to get there
But you never get to a hundred percent what you get is verified with respect to a certain verifier
This two PA reviewers said publish it. There is a hundred thousand scientists
But those two you randomly got a proof publication of this paper. It doesn't mean it's not
Containing a bug. It's quite possible. I'm wrong and nothing would make me happy even to be wrong about this
but so far they're pretty simple proofs of things where it's self-contradictory and
No one has at least so far pointed out those
Proofs contain an error of some kind. So that's encouraging also in practice
None of the impossibility results. I have talked about have been violated
No one has said oh we can explain large neural networks. Here's code or
Definitely, I can predict more capable systems. Here's how I'm gonna do it today
What is the date March 8? Happy International Women's Day?
No one has a working system or even a prototype which they would claim would scale to control
systems of any intelligence
The state of the art is we know what the problems are but we have no solutions
And you just spared the the European Union a billion euro
Which they allocated to explainable AI research? So we should call them up and say
You know donate it to you know, whatever other worthy cause
So
What are you working on given the generality of these
Conclusions, what are you working on today?
So clearly not everyone agrees with me
So the development community has no fear and just tries to develop a guy as quickly as possible
Safety community thinks that if only they were given that billion dollars and a little more time
Everything is solvable. They have no doubt. They have intuition that they can control super intelligent systems
I'm saying that no actually it's not a question of more money or more time
Those are unsolvable problems and we need to find
Alternative ways to benefit from this very beneficial technology
But creating a fully capable a GI super intelligence without control is not a good outcome for us long term
Whatever you first want to get there last no one's gonna win anything
So what I've been working on is being more convincing publishing more journal conference papers showing different
Limitations and trying to get people to read them, which is difficult. I have an 80-page paper showing
This is why control is unlikely to be happening anytime soon. And I don't think it's going viral yet
Last time I checked it hasn't yes or it hadn't now
The
Ability to develop a full-blown a GI is
In the eyes of a of the beholder, right?
You mentioned at the beginning that the
Distinction between narrow and general AI is becoming fuzzy. So if there were
Universal consensus and the United Nations
treaty on
the limitation of AI capabilities
In order to prevent an AGI to be accidentally or purposefully released
and
Do you have an
idea of how
the researchers could
Refrain themselves from breaching those
Treaties and what would the enforcement mechanisms be?
I'm not arguing for government regulation. I don't think it would work
mostly because the cost of
Scaling AI keeps coming down. It is still a large expensive project today. We can monitor compute use
But in a few years you could do it in a laptop
We already see experiments where a single powerful laptop can run some of those models at home
So I don't think simply making it illegal will work
Government regulation in the past for spam for computer viruses has not
reduced this
Level of malevolent
Software to zero and as I said, we need zero. We cannot accept some failure. We need it to be
100% so I haven't found anything more convincing than personal self-interest if I get this
usually young
healthy rich CEO of a
startup
To go this will get me killed. I will not be famous and there is no glory in it to stop doing it
That that is more convincing than passing some sort of law where they just move to another country or
Find another way to bypass the law
and
I don't know
What is the the correlation or lack of correlation between being young successful rich and
Smart enough to realize that the agi should not be pursued
Uh, including the fact that according to what you are describing
There has to be
100 success rate in convincing all of those
And so let's go back
to to two questions at the at the beginning
There are
If you ask a hundred ai researchers some who are still convinced that agi is either not possible or
That the approach
That is being pursued today is basically at that end and that novel discoveries about how
Our brain and mind
Work
Are needed
an example of this is
specifically in
In the in the field of ai research is is young lecun
Who says large language models are not enough are at that end
and an example
of
A scientist who believes that we we don't understand how the mind works is roger penrose
who
Is convinced that
quantum phenomena
are
playing a major role
And as a consequence if we don't
implement
Analogs of human minds using quantum technologies. We are not gonna go anywhere
in
Is it correct that that you don't believe these are
sufficient in the sense that we cannot afford for them to be wrong
That that we need to be putting in place whatever save gods
We we can
because if
Mian lecun or roger penrose or any of the other skeptics are wrong and we accidentally
Stumble upon agi. Well, we we have to be have to be ready
I think even if they're right. It doesn't change anything instead of five year timeline
We get 25 year timeline, but it's the same problem same unsolvability issues
So i'm not sure it makes a fundamental difference people who say we will never succeed
That one I cannot understand. I'm not following the logic, but anyone who says no no five years is crazy
It's definitely 20 years. That's fine. I can accept that
We have
We we we are able to falsify the claim that it is never
Going to be possible because we know that a bunch of hydrogen atoms
In 13 billion years created general intelligence
Well, unless you believe in some magical properties of humans souls religion
That's the only argument I found which may get you out of it. Yeah. Yeah, that's right. Um, so
A
between those
intervals of
Let's say five and 25 years
Over the course of the past couple of years your own expectation has moved
And and in which direction? Oh, I definitely see more capable systems more general systems and
resources funding compute human resources all growing exponentially around it
Uh, it's becoming harder and harder to find something only a human can do
It is surprising that things we
Assumed will be the most difficult that turned out to be the easiest art for example is apparently trivial to make
so
It may continue on this path. We are not seeing any stagnation in terms of progress
And as I said, the most amazing thing is we're deploying systems with capabilities
Engineers of those systems are not aware of
That never happened before
These are scores, uh, my uh, my own playing with fire, right?
I am, uh, entertaining myself, uh, with
Advance, uh
while
They are laughing at me
behind behind the screens
apparently
This these are images that I I generated for a presentation that I'm making
next week in
Uh in dubai because yeah, uh, I am for the past year
I have been doing all my presentations illustrations with uh with mid-journey
And and it has been a big surprise for many
That the automation of creative work
Is now possible
Not only in images, but also in text
to an increasingly
Uh unapproachable degree by non specialists. I am not a specialist
How new this is this is a couple months old really the year old
Imagine how good it's going to be in a year or so two years five years
This full blown movies virtual environments. Yes super stimuli
Okay, so let's talk about super stimuli, uh, whether attractive women or
The ability of AI systems
to you
About their own behaviors and their own capabilities
About their their their very nature
Under your own prodding and under this reverberation of communication
that
exploits
Our mirror neurons
And exploits our natural tendency to empathize and to and to put ourselves
in the shoes of of others
um
Can we and should we build our?
Defenses individually by stealing ourselves against
these super stimuli
And and and at least in my case
I feel I am able to do so, right? I I have not
Being uh
convinced that current AI systems are
self-conscious for example that they want
Things whether to be free or to be
Doing
Different things that than what they have been programmed
um
Is this something that everyone should?
Train themselves to to maintain
I don't know if an average person can train themselves to that level we have
Nigerian scams right like nothing is more pathetic than that and people fall for it all the time
I can certainly see my parents who have hard time
figuring out how to click buttons and change settings
fall for deep fakes and
Not have very good defenses against
Emails targeted at them and so on so maybe top experts can do something but I think for most people it will be
nearly impossible
And and it is fascinating that
the presence
Of grammar and spelling mistakes in Nigerian scams
is actually
A desirable feature on their side
Because they already know that there has been a filter of gullibility
because there are people who look at the uh
Pathetic email
And they will say oh, this is evidently a scam
But people who respond to receive part of the 20 million dollars by the princess that inherited it
Are predisposed to go through
And and so the scammers receive
A good number of responders that have been pre-selected
Even more so the fact that they haven't changed the story
It only targets those who have never heard about the scam and that
That is a very narrow group of people. You have to live under a rock with no internet access
Or having or having acquired the internet to
You know a given level of
ability
recently enough
Oh
Let's let's go back to our need to
Do
Not stumble upon agi
What would you tell
The founder of stability ai
Who believed that uh open source is the best way to go about it
To put ever more advanced models in the hands of as many people as possible is what will bring about
um human flourishing
If we accept that those could be dangerous, it's like arguing that if everyone had a nuclear weapon then the world would be safer
Either you think they are safe and we haven't seen evidence for that not even from developers
Or they're dangerous and then you want to not make them open access as much as possible
Um
However with nuclear weapons we had
Two things that I I believe is is different than than uh, we advanced ai and agi on on one hand
The
Gineering were uh on a
relatively
More solid ground. There was a wider consensus around
What were the experiments that we could do?
What were the boundary conditions of those experiments?
For example
A
There is this anecdote of the scientists asking themselves
Is the nuclear explosion that we are planning to carry out as a test
Going to ignite the free oxygen in the atmosphere
killing everyone
And that is a proper existential risk and then
Calculating whether that would be the case
And concluding that it would not going ahead with the experiment and being right
um
That's that's one the other is that even though
The
Knowledge of nuclear weapons has been around for
Now almost a hundred years
The ability to develop
And uh deploy in a desirably
destructive way nuclear weapons
Has not become available
to
A very very large number of people. It has become easier
And nuclear non proliferation failed
There are more and more nuclear
Capability available around the world
And the
comprehensive
ban of nuclear weapons has not been achieved
But
The you know irresponsible
College student today cannot assemble a nuclear weapon uh on in his dorm
And and and both of those
Are are are not true in in in advanced AI
Our ability to predict how they will work is
Is is um
Impossible
Thank you. Thank you
and uh and
the
availability of tools that can
Run ever more dangerous experiments is going to be in the hands of everyone soon
so
So, um
Your being here is part of talking about this right is part of what needs to be done part of
talking about it part of raising the
understanding of
Of of the danger
If he recognized that
A g i minus x is useful
Is there anything that we can do in order to keep
The capabilities of of our systems
Within or below that threshold
Other than deciding to do so individually, I haven't found a solution
It seems that the technology we have is already very capable
Uh, for example protein folding problem was solved without a g i
but uh
We need time to study existing systems
So if we had a few years at the current level, we would better understand what they can give us what they are capable of
And maybe the need for more capable systems can be reduced
For example life extension is something I assumed we needed help with but maybe it's enough to just have current systems
similar to the one
We solved the protein folding problem
But again, I don't think external pressure can be applied in a meaningful way
With respect to nuclear weapons
It's not a perfect analogy. Obviously. There are some lessons we can learn from that field, which I think are useful
To the best of my knowledge both american and soviet creators of nuclear weapons
regretted
Their invention and spent the rest of their lives trying to undo that damage
We actually used nuclear weapons
It would not be the same with a g i one mistake could be
More than enough and you won't have time to kind of go back and and do it and once it's
Unrolled and available to individuals. You cannot put it back in a box
So, uh, we can learn still even though we survived the nuclear experiments. We can learn from those mistakes. We might have
Uh survival bias survivorship bias
We had so many close calls accidents where it was possible that the world would be destroyed and many
Somewhere in a multiverse some branches no longer exist because of it
So I don't think we can put too much into the fact that we're still here
In the
During our conversation today you mentioned
If we had more time, so there is this, uh
Uh
That is shortening the time available
And and you would like uh much more time to be available
to
find
A fault in your own reasoning maybe or to find some new approach or to find
Uh something that that could uh
Not diminish but eliminate the existential risk represented by by agi
And then and then of course you mentioned also resources
um
So if a billion dollars were that round number
What is the uh corresponding round number? Uh that you would need
Or the agi safety and security community would need
Uh in time. Is it 10 years or 100 years or an unknowable amount of time?
Well, I obviously don't know exact number, but I can tell you that safety work takes more time than development work
I can write uh function very quickly, but then to properly test it. I would need 10x time 100x time
Even if it was deterministic, I could uh use all the pre-existing edge cases if it's not deterministic
If it's self-improving self modifying learning a new data, I can exploit it in new domains
Maybe I need a million times more time to properly understand how it functions
So right now we have this exponential as you said super exponential growth and capability
But uh progress and safety is not even linear. I think it's constant
Um
We were at the
conference in portorico organized
by
the
if
institute
that
in
analogy with
The assila our conferences
aims to
increase
Knowledge collaboration understanding of this problem set that was before the pandemic
And I don't know if a new conference is is being organized when
But that was a pretty small group. I think it was like a hundred people
And at the time we still had russians and chinese
In in portorico even though not many but at least a few
And if a new conference were to be organized
by
An institute of the mit I doubt russians would participate and and I don't know about the chinese
Which brings me to the point that
We cannot afford political
Or even military
Um
Contrasts or conflicts or war
to
Interfere with the need of the broadest possible
Potentially universal understanding of of the challenge, right?
We
We we we know
What is going on in terms of scientific publications?
And the field is so dependent on on the talent and skills
and and available hardware of
Of everyone that
It is highly unlikely that some
Hidden research would be further advanced, but still
breaking down
channels of communication can only be
a hindrance
To to this broader understanding of of the issues. Is that correct?
In general, yes
We want scientific communities to be united to communicate
In a way the war in russia is kind of good because it siphons resources and access to compute so
I'm not aware of any
large successful projects in russia which would overtake american efforts
China may be a completely different story, but at least in that regard
big expensive
conflicts kind of
Prevent development of scientific discovery delay it a little bit. So
That's a well
I I would have a hard time agreeing with you that world war three would be desirable in this context
for the survivors
And and I hope that is not the solution that we will adopt
Please definitely not what I'm hoping for. I'm just pointing out that any time
There is a
distraction from something
Like a large scientific effort. It does impact scientific
capabilities
um
so
Forgive me. You you did say that you don't know
You cannot give a precise answer
But did you say
Oh, yeah, you you you gave a proportion. So you said
if we needed
A given amount of years to develop agi we would need 10 times or 100 times that long
To be able to analyze it
I think so, so if we release gpt 3 gpt 4 gpt 5 every two years
I need 20 years in between to figure out
What we're dealing with
And
There are a hundred naive
solutions
That i'm not gonna bore you with
Such as for example
Run them slower
So that our clock speed is faster than theirs and then be able to to study them better
How can we
Rank
Solutions so that we concentrate on some that have a higher probability of succeeding rather than rehash
naive ones that
Everyone in the field
Realizes won't work
So we just published another paper on a risk skepticism where we have about 60 pages of
People's ideas about why it's not a problem why it's going to be resolved and we try to address
Why they may not be right about it
there are papers surveying
ideas in the safety field but
I don't think there are any which have not been immediately shut down pointed out that they have a
Major flow in them
I'm not
sure we'll ever
Stop getting new proposals from people who just heard about the problem and so unplug it pour water on it
I mean people read a lot of good science fiction. So
Yeah, again, my goal was to kind of cancel all those future attempts with a single impossibility proof
And then if you can show the mistake and impossibility proof that would be helpful
Yeah, yeah, yeah, sure
And so we mentioned at the beginning and I said we would go back to it
the the fact that
in order to
Interpret the world we compress it and there was an article I think in new yorker
recently that was
Blaming at least in the title
that
chat gpt is a blurry
uh jpeg on on the world or or transforms the web in a blurred jpeg
and rather than interpret it
Negatively in reading the article
I found of agreeing. Yes. That is what we do. We
Approximate we are lossy compressors
Our ability to forget is a necessary
survival skill that is down in effect
Remembrance of everything from the past
and
you mentioned
One of your scientific publications of 80 pages and just now other of 60 pages and two books that are probably
Of hundreds of pages. They are back there somewhere by the way
and
And they are not blurry enough in order to go viral. Have you tried the memetic engineer?
to
A more aggressive
And I am not joking where you are
Compressing what you do
to
Literally a meme
An image that is hunch enough
To reach millions or tens or if Elon Musk retweets you
a hundred million people
So if you follow me on twitter or facebook, you know, I'm a prolific shit poster. I have lots of those
Sometimes Elon Musk does react to them, but after a million views. I get one new follower
I don't think it makes a difference
This week we had an article in time magazine a short piece one page saying AI will likely be uncontrollable
What did I get out of it? Maybe two emails from crazy people asking me for god knows what
So I don't think that's uh working. I definitely try I go on a lot of podcasts
I do interviews we write for popular media
but
The very narrow
Segment of society who finds this topic interesting
Uh, it doesn't scale. It's not like, uh, you know popular medias kim kardash and whatever a hundred million followers
It's not the same topic
Definitely um so
Before starting the the
The live stream, uh, I told you I have three children and and
You told me you're aware of the same number on your side
and uh, you
confirmed that
you have
a somewhat unreasonable
hope and faith
in them
Which I think is biologically grounded
I don't know how rational but still is something we hold dear
um
because uh, we
Are human and want humanity to flourish as well as
uh, we we pass uh the baton we we
um
Have this uh ability to to sit back
and then see
Uh, what what comes after us?
um
How do you
How do you, uh
prepare them?
I don't know the ages mine are grown up
um
so I don't necessarily need to
to
dress in metaphors and fairy tales
Whatever I feel about the future of the world
if people listen to us and they have
Youngsters, uh, and and they want them to understand what we talk about
How would you
recommend they do it
So it depends of course on the timelines what you think is realistic
If you think we have enough time for your kids to grow up you can talk about career choices
A lot of the jobs they might consider will not exist a lot of jobs
Uh
Only started showing up this year from engineer or something like that
So you definitely have to plan for the future decide if college is even makes sense
Things of that nature if you have shorter timelines and your children are young it's that's a tough one
I don't have any
Very good answers my daughter. She's nine. She's helping me with cover design for my books. So that's something just
envisioning control problem being solved
Um, well, um, what is amazing is that one way or another we will find out, right?
Uh, uh, I'm I'm not sure we will be happy of the outcome or we will be delighted to be wrong
uh
I will be will say oh
that
I'm happy all my life's work is now in in in ruin
Because I was wrong about all my various publications
a
but
it is kind of uh
intriguing
that
The capillary and relativism doesn't apply to our generation apparently
Sometimes something that is radically new comes about
and and
With the eyes of past or potentially hopefully future generations
You can actually say
That was a special time and then it feels like we we are living it
It's definitely
Very unlikely that you would live at a time where you get to decide the future of the universe forever
So that kind of gives more more credence to simulation hypothesis and other
interpretations of what's going on
Maybe it's a test to see the crazy people who will actually run a gi and make sure they are taking care of
Okay
well
We spoke three years ago
So I hope to be able to call you for a new
live in three years and
see what happened in the meantime
and
roman thank you very much for the work you do and
the energy that you invest in
alerting as many people as you can
About the work that needs to be done and the risks that we need to minimize and hopefully eliminate
Thank you so much for inviting me. I'll put it on my calendar of three years. Okay
Okay, let's have a reminder
Bye bye and talk to you next time
You

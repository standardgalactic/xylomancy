Hi, and welcome to my course on the Jupyter Notebook for Data Science Teams.
I'll just give you a brief overview of the course so you can know what you're getting into.
So we'll start off by doing what you expect, getting Jupyter Notebook set up on your machines.
The second thing I'll go over then is Jupyter Notebook features.
So there's a lot of really interesting things going on with the Jupyter Notebook.
Some of the most useful functionality comes from what's called Notebook Extensions.
So I'll be going over a lot of those.
I'll be showing you how you can use both Python and R in the same Notebook.
So if you have some piece of your analysis that needs to be done on R,
it's very easy to actually do part of it in R, send it back to Python,
or even use R natively as the entire kernel that runs your Jupyter Notebook.
Also, different things like using SQL in the Notebook to query databases,
some really nice post-save hook functionality and widgets, which I'll just demonstrate right here.
This is an example of a widget where I've created a function that generates data
according to some line, and the green line shows you the actual function with some noise,
and the blue dots are data that gets drawn from this distribution.
So the total number of points is 10, and as I click and drag this off to the right,
increase the number of points, and you can see that the fit dynamically moves around
as I add data points and actually slowly but surely converges on the underlying distribution.
So this is an example of a widget which is very easy to do
and provides lots of functionality for all kinds of data exploration.
Finally, I'll get into sharing the notebooks on a data science team,
so there's a lot of questions you have to consider for your particular situation,
so I'll try to give you a strategic framework so that you can actually identify
what kind of workflow makes sense for your situation.
There's various other things about conceptually breaking up your notebook structure into
lab notebooks and deliverable notebooks, and a lot more that goes into that.
Finally, I'll go through two different data science projects, which will just demonstrate
the principles I talked about above, and you get to see it in an environment where
I'm explaining and going through how to actually do a data science project from end to end
using all the different techniques I was already talking about.
In this lesson, we'll be installing the R version of Python, IPython, and Jupyter
so that we can run the Jupyter notebook.
The way we run this is by installing the Anaconda distribution.
There are other ways of installing the IPython notebook,
but I recommend the Anaconda for its ease of use.
We'll first go over to any web browser and search for Anaconda Python.
What you'll see here is that the top link is the continuum Anaconda distribution.
Clicking on that takes you right to the downloads page,
and you see that you have different options.
You can get it with Windows, OSX, or Linux, whichever one you prefer.
Since I'm using OSX, I'll click on OSX.
The choice between using Python 2.7 and 3.4 is a tough one.
I'll be using 2.7 just because many of the legacy code bases still use 2.7,
but feel free to, if you're feeling experimental, to go to Python 3.4.
I'll be using this Mac OSX 64-bit one.
If you have a different system, please use that one.
Okay, great. Now that that package has downloaded,
install it by following the instructions on the screen.
So, clicking through and agreeing to various licenses,
and hopefully you get to this stage where it says the installation was successful.
Click Close.
We can close our browser as well.
At this stage, if we typed IPython, it still won't work.
One thing that the graphical interface does is actually adds a command to your bash profile.
So if I were to actually go into vi.bash underscore profile,
the first piece of the profile has been there from before,
but this was added by the Anaconda 2.3 installer, which exports this path,
shows you that the Anaconda folder has been created in my home directory,
and adds that to my path.
So if I type ls, you actually do see the Anaconda directory right here.
This makes it really easy to uninstall Anaconda if you want.
You can remove that line from that path in your bash profile,
and you can delete this folder, and everything should be gone off your system,
and you can use the old system defaults.
But now that we have this, we have to source our bash profile,
and we should now be able to type IPython.
Now that we've run IPython, we see we are running Python 2.7, in this case 0.10.
It's the Anaconda distribution, and this is IPython version 3.2.
So a lot of different numbers here.
The ones that are important are the Python version, which is 2.7,
and the IPython version, which is 3.2.
Now this is actually a bit behind.
So what we're going to do is hit Ctrl D, and it says, do you really want to exit?
You can either type yes in return,
or you can type Ctrl D a second time to actually exit it.
But what we'd like to do now is actually update to the most recent version of Anaconda,
so that we have the most recent version.
The way to do this, type conda.
This is a new command line argument that you have.
The way that we update the Anaconda distribution is by typing conda in various packages.
So in this case, we're going to conda, install conda.
What this should do is check to see various things.
This tells us the following packages will be updated.
Conda will go from this version 3.14 to 3.18, conda environment, and so on.
And we would like to set this up, and we will hit yes to this.
It actually takes quite a bit of time to install all of these things from source,
but most of these things are actually pre-compiled, so everything's already completed.
We'd like to also update a number of packages.
So let's conda install Jupyter, and you can actually chain which packages you'd like to see.
So in this case, we'll install Jupyter, the pandas package, and scikit-learn.
So these are the packages to be updated.
We see that a number of things are going from ipython, which is important,
going from version 3.2 to version 4.0.
We'd like to proceed with that.
I'd like to just say a few words about why I find the Anaconda package to be a useful thing to use.
They make sure that all the packages you've installed will play nicely with each other.
So sometimes if you're using pip by itself, you can actually install dependencies that overlap each other out of place.
So they end up with a conflict when you're trying to import these libraries,
and Anaconda does a really nice job of making sure, tracking those dependencies really well.
So now if we type ipython, we should see that we are running ipython 4.0, which we are.
Now we would like to actually check out the ipython notebook,
because that's the part where it really gets interesting.
So let's create an example directory, and from here we can type Jupyter notebook.
Just typing Jupyter notebook, a couple of things happened.
First of all, go back to the terminal.
I typed Jupyter notebook, and ran it.
And a notebook server started from the directory we are in, so a user's jbw example in this case.
So an ipython notebook, which has been started with a Jupyter, is now at this location.
HTTP colon slash slash localhost, in this case, quadruple 8.
And it says here this useful thing, control C to stop this server and shut down all kernels,
and you have to do it twice to skip the confirmation.
Now this starts the server running, and this terminal needs to stay open.
If we go back to this, what it runs is a web server, and it automatically by default opens your default browser.
So in this case, here we are, we're at this location, localhost, quadruple 8 underscore 3.
And if we'd like to start a new notebook, we can click new, Python 2 notebook.
And again, this is referring to which version of Python you're running.
This is a 2 version 3 versus 3.
And we see now that we're running a Jupyter notebook, and we can start typing valid Python code,
and see the output from it right there.
Let's do something a little bit more interesting.
So we import the NumPy library as np, and then print numpy.a range 10.
So we see the first bit of Python code, and we know that we have the installation working just as we hoped.
In this video, we're going to start a GitHub repo to house a data science project.
First, we have to go to github.com.
If you don't have GitHub or Git setup, I highly recommend starting out by picking a username,
but giving your email and creating a GitHub account.
Now, if you have Windows or Linux or Mac operating system, GitHub itself has a really nice tutorial
for how to actually set up Git on your machine and for your setup.
So I recommend doing that.
So once you have a GitHub account, which is free, or if you already have one, click sign in, let's go to the next step.
So you've signed into GitHub, click the plus next to your name in the upper right hand corner, and start a new repository.
I prefer to start a new repository through the GitHub website itself and then clone it to my local machine
so that that way the remote connection has already set up.
And that's usually a stumbling block that can be a little bit annoying to overcome if you try to do it the other way around.
In this case, I'm going to be looking at some cold data.
So I'm going to call it cold exploration.
I'm going to give it a quick description.
I'm giving it the description a first look at the cold data.
I'm going to let it be public so anyone can see this repository.
So afterward, you can also see this if you'd like to go to it.
I will initialize this repository with a readme and I will add a gitignore.
A .gitignore file will let you ignore the machine generated code that comes along with various programming languages.
Now Python doesn't have that many, but there is usually a .pyc if you're running a Python file.
I also recommend having a license, especially if it's going to be public, so that you can share your repositories with others.
If you work for a company, obviously you have different licensing concerns.
So then click create repository.
It's as easy as that.
So now I have the cold exploration repository in my GitHub account.
From here, we would like to actually tie this account to our local machine.
So we can copy this text that's just to the right of this SSH tab.
Now, if it doesn't say SSH, if it says HTTPS, I would recommend clicking it to SSH.
And once you do that, copy the text that's in this text box.
Navigate with your terminal to a place that you think is an appropriate spot for this repository.
Type in git clone and paste the text that you just copied from the website itself.
So now we see the license and the readme files that we created on the website itself.
All right, so we have set up our GitHub repository and we've cloned it to our local machine and we're ready to start doing some data science.
In this lesson, I'm going to give you some extra intuition so you can understand what's happening when the Jupyter Notebook is running.
So in my terminal, if I type ls, I get to see the directories now underneath this current directory.
I see deliver, dev and source.
By typing Jupyter Notebook, I again start the Jupyter server.
My default browser is Chrome.
So again, we see those same three directories deliver, dev and source.
If we toggle back to the terminal, we can see several messages.
The first is the directory under which the notebook server has been started.
The second message is the number of active kernels.
The third message is the location that you can point your browser to to find this notebook.
And finally a message to say how to stop this server.
So going back to the notebook itself, if we click on the development branch, we see that there's no notebooks in here.
We can start a notebook by clicking on new and then clicking on Python 2.
So after clicking new, we see a new tab appear.
It's currently named untitled.
And the last checkpoint comes from a few seconds ago.
So let's type a few things.
So let's just say first as a variable is equal to the 5.0.
I execute that cell by holding down shift and hitting return.
When I do that, a new cell appears beneath it.
And as I type a second variable and label it say 23.0, again, hitting return with the shift key produces another cell beneath it.
So I now have two variables, one name first and one name second.
And there's unsaved changes, which means if I lose this current browser,
I will lose the changes that happened from the last time it was saved.
And in this case, there's nothing that's been saved.
So let me go ahead and save this right now.
There's two ways of doing this.
One typing command S, if you're on the Mac or control S on Windows, which I just did,
or you can click this save disk and it will also save it.
Now that it's been saved and there's no unsaved changes, if I close this tab,
or if I even close the whole browser by quitting the Chrome browser,
all of the actual information has been stored in the kernel itself.
In other words, there's this kernel and everything that's happened with the kernel is being stored in state by this kernel.
This means if I open up a brand new version of Chrome,
and I go to where the notebook is running from the previous message before,
I copy that with control C, go back to Chrome browser and type it in here.
I go back to the exact view we had before.
Clicking on Dev, because that's where we were.
We actually see that the untitled ipathome notebook is actually still running.
So we click on this, we reattach the browser to the underlying kernel.
So if you have saved your notebook as you work and you close the browser,
the work still remains in memory.
So if I say print first comma second, now we see the actual results here.
So this has all been saved and that's one interesting thing that you should know,
is that the browser itself is a front end to what's really going on in the kernel.
Now the converse to this is what happens if I completely close and shut down the server.
So I hit control C twice and shut down the kernels.
So all the kernels have been shutting down.
So going back to the browser, you see a message that says connection to the notebook server cannot be established.
It has to continue to try to reconnect, but you won't be able to run any code.
So in this case, if I try to do something, say I want to say first times second and execute this,
and shift enter, nothing happens.
And this is what you see when it's trying to connect to the kernel and it's failing to.
So this is the part where it actually needs to be running and needs to be continually talking to your browser.
Unfortunately, restarting the kernel does not give us back to where we were before.
So here I can try to reload this notebook and we still see what we had previously done,
but watch what happens when I try to run this third cell.
The name first is not defined and the input name of the cell one to one.
So the kernel has completely restarted as you saw me do in the terminal,
which means that now we have to start from the beginning and now everything has been stored in state.
Saving it keeps it so that the kernel is now running in the background.
Hopefully that gave you a little bit of insight into what's happening.
The browser acts as a front end to this process that's running in the back end on this terminal.
The browser can be closed or blown away after you've saved all of the changes that you've made,
but the kernel cannot be.
The kernel has to stay running if you want to keep the changes that you've done in memory.
In this lesson, we'll be talking about Jupyter notebook extensions.
Notebook extensions, as the name suggests, are extensions to the capabilities that the Jupyter notebook already comes with.
Now there's many different ways that you can actually extend the behavior of a Jupyter notebook.
I'm going to show you just two.
The first extension that I'll show you is called Jupyter Pivot Tables.
If you click on this link here, you'll see that you go to this website.
Nicholas.crucian.com slash content 2015-09 Jupyter Pivot Tables.
This allows for drag and drop pivot tables and charts.
This write-up he has is actually a really nice write-up that I recommend you reading
and watching this video as well because he explains in some detail how you can actually use his extension.
To install this, all you have to do is go to this pip install command.
Copy pip install pivot table js and run that command in your terminal.
It's successfully installed the pivot table js.
We go back to our notebook.
We can now run the cells that import both pandas and numpy and this command,
which is from pivot table js import pivot UI.
That loaded correctly without any errors.
We have now loaded this extension.
As of Jupyter 4.0, the preferred way of installing notebook extensions is through a pip install of the extension.
There are other ways of doing it as well and I'll show you a second way at the end of this video.
So let's actually take a look at some data with this pivot table extension.
Go to HTTPS colon slash slash data dot austintexas.gov.
In this website, we're going to go down and look at the restaurant inspection scores.
From this, we will export data.
The format we want is CSV.
We do want it to go into our data folder and it's called restaurant inspection scores.
Hit return to save that.
You can now close this tab and go back to our notebook.
Now that we've downloaded the CSV file, let's read it into pandas data frame.
I'm going to split the cell at the current place where it's blinking by typing control shift minus,
because I want to run this on just one cell by itself.
So reselecting that cell, I now hit shift and return and it correctly loads in the data frame.
So ways to check that is actually look at what the top of this data frame looks like.
We see that the restaurant name, the zip code, inspection date, the score, the address, facility ID, and the process description
actually looks like it's been read in correctly.
One thing you will notice is that the address has return characters in it because standard address has multiple lines.
And I'm actually going to be okay with that.
I'm going to say I would like to keep the address on one line in the data frame, not have that split up in different ways.
So let's take a look at what we get when we look at just the data frame itself.
It's pivot underscore UI.
So we've imported pivot underscore UI up here in the first cell.
Let's execute the cell here.
Now a number of things happened in the background, but what you end up seeing,
close this window down here that shows what we downloaded.
And I will actually toggle this toolbar for now.
So we can actually see a bit more.
We have the various columns of the data frame available on the top here.
So zip code, inspection date, score.
They are now dragable into these two places.
So let's do that.
Let's actually drag score along the top.
Let's see if there's a relationship between the zip code of a restaurant and the score.
So just by dragging those two columns in, we see that there are, for each of these zip codes,
different scores that have been given to the restaurant.
Of course, a really good score is a 100 for the health score.
And we can actually scroll down and take a look at this data in a really intuitive way.
This looks pretty neat, but there's a lot of numbers going on.
It's actually kind of hard to read.
So one thing we can do is actually change the output type from table to something else like a heat map.
So this does the same data as we saw before, but it actually highlights the outlying points that are large with a darker color.
So now by eye, you can visually see the different relationships between these two variables.
I still think this is actually a little bit too big.
So I'll give one extra hint of taking data that actually has a lot of different granular pieces.
So let's take this very granular number across the top and bin it by something, let's say five.
It's going to give us a little bit less granularity.
So here's some code that will actually do that.
So we're going to create a new column in this data frame called bin score for bin to score.
I'm going to use a pandas function called cut, which will now cut up the column df.score into bins that go from 30 to 100,
because not b.a range is not inclusive of the last data point and stepping by five.
So I'm going to run this cell.
It's going to create a data frame column named bin score.
And let's see what this one looks like.
We can drag bin score along the x-axis here and zip code along the y-axis.
We now see that the binned scores are now counting everything that has a zip code off to left and any score within a certain range in a range of five.
We can then also take a look at this instead of a table.
We can look at it as a heat map.
We can also see if it looks okay in terms of a bar chart, for example.
And this doesn't quite make sense, but there's many different things that are different here.
You can actually look at dream app, for example.
So the various visualizations that are available to you may or may not make sense to the data that you're looking at.
But the availability of this is actually a really nice extension to the notebook capability that Jupiter already comes with.
Alright, so picking up on where the last video left off, notebook extensions.
We've already installed one extension. This is the pivot table extension.
There's one other extension that I'd like to highlight for this video.
And it actually comes from this URL here.
I want to turn this into, let me just show you this real quick.
This code block is currently set as code.
I'd like to actually change it to markdown.
There's two ways to do that, by clicking on the toolbar like I just did, or by typing M when this cell is selected in this gray circle right now.
If I type Y, it would turn back to code.
So I just typed Y, you saw the drop down menu turned to code.
And since it's still selected with a gray box, I can type M and it goes to markdown.
So I want it marked down so that when I click this, I can actually open a new tab.
So the Jupyter slideshow extension is this GitHub repo right here.
It has a lot of really interesting capabilities that I will be showing you at the very end of this course.
I'll be using the rise Jupyter slideshow extension to help us make a final slideshow presentation out of some of our data science projects.
To install this notebook extension, it says to simply run python setup.py install from the rise repository.
Now this means we actually have to first download this extension code.
So this isn't done in the usual pip install way.
This is done by choosing the SSH version here at the top of the page, selecting this by clicking once.
GitHub actually makes it so that the entire thing is highlighted so you can now command C to copy this.
Go to your terminal.
And at this point, if you don't have a folder for your GitHub repositories that you just grabbed from wild, basically, I would recommend creating one.
So we type git clone and then paste in the code we had copied from GitHub web page.
So it clones into this thing called rise.
Let's CD into this.
We see various things here, the live reveal package, JSON and so on.
Let's go back to the GitHub page.
This is we simply run python setup.py install.
So I'll copy that code and paste.
Okay, so we have now installed this live reveal.js notebook extension.
So we go back to our notebook.
We see that there's an extra toolbar cell here, which has something different than we normally see, including a slideshow option.
And we actually need to restart this notebook to actually get the ability to make this look like a slideshow.
So let me go ahead and do that.
I'll do save and checkpoint and then close and halt.
I'll go back to where it's running in the terminal and hit control C once.
It says it's currently running shut down the server.
Yes or no.
If you wait too long, it'll actually say I didn't see an answer.
So I'm just going to assume you did that by mistake.
We actually do want to quit this.
So we'll do control C twice.
You can have also selected why.
So we shut down all the kernels.
And this thing, if I reload this should not be available.
Let's rerun Jupyter notebook and it will give us a new version of this exact thing.
Click notebook extensions.
And now you still see this toolbar here with currently being non the slideshow option, but you also have a new button off to the right.
So let's actually click this and click the slideshow option.
If you'd actually like to turn one of your notebooks into a slideshow, the functionality is now at your fingertips.
And if you don't want to see all these extra cell toolbars, you can always put this back to none.
They should be saved.
So when you're clicking slideshow again, the fact that these are all slides has been preserved.
To look at the slideshow itself, you just click this button and type into the right gives you the different slides.
And one interesting thing about this or one thing that I think is very, very useful is that this is not just
a rendered notebook of this.
This is actually a live cell that we can actually import and actually run new code.
So I just ran that piece of Python code during the slideshow while it's up.
So this is very nice for interactive demonstrations.
In this video, I'll be showing you how to actually query SQL databases from the Jupyter notebook itself.
A lot of enterprise data is stored in databases.
So dealing with them will be part of your everyday job.
The Jupyter notebook makes it really nice to be able to document very clearly the SQL queries that you are creating.
So I recommend if you're going to be using SQL connections using a Jupyter notebook extension called ipython SQL.
It's installed by typing pip install ipython dash SQL.
Once you install that, you then have access to an extension that you can load by simply typing percent load extension space SQL.
When you run this cell, it actually loads in this magic extension.
It gives you a number of warning signs, but these are just warnings.
The package will still work just fine.
This next line percent config will actually configure our ipython SQL extension.
And what this configuration does, we say SQL magic, we would like to automatically return results that are a table as a pandas data frame.
You don't have to do this, but I recommend it because most of the time you'd actually like to take the data you've queried the database from and transform it and use it in the standard data science tools.
So I'll run that command as well.
Next import pandas and for this demonstration, I'll be using SQLite.
You can use any of the standard SQL engine connections.
I'm just using SQLite because it's a simple and easy database to run with for an example.
This next cell, I'm actually going to create a table and put some data into it.
So if you're familiar with SQL, you'll notice that everything below the first line in this cell is SQL commands.
That leaves this top line to be explained.
So what we have here is a double percent sign and then SQL.
This is how you call what's called a cell magic.
If I hit tab, while I'm at the end of these double percent sign, I will see a little pop up that tells us of all the different options we can have to change this into a cell magic.
When I say cell magic, what this means is that this is a special flag that tells ipython that something different is going to happen for this entire cell.
In this case, we're telling it everything after this first line is going to be a SQL query.
As you can tell, there's other ways you can do this as well.
You can have HTML, you can have bash.
There's various other options as well, but I'm just showing you right now the SQL one.
Now, this is how you connect to a SQL database that's just stored in memory.
If you have a different package, a different engine, then you can use the various documentation to tell you which connection you should use.
So we're going to create a very simple small table called presidents.
We're going to have first and last name, and we're going to include the year that they were born.
And I just have a random sampling of about 10 US presidents here.
So running this cell, we get some output here that says one row is affected.
We've inserted values into this table.
And now we can actually run a SQL command that's in inline again with a single percent.
When you have this command here, it says everything after it will be SQL.
So we're going to store an object called later presidents, the SQL command, and the results that come from the SQL query.
The SQL query being select everything from the presidents table where the year of birth was later than 1825.
And then I'm going to show you what that looks like by typing it there.
So we see that there were three presidents that were born in that table after 1825.
And if we took a look at the type of this return, we will see that it is actually a pandas core data frame.
So we have returned a SQL query into a pandas data frame.
And now we can use all of the normal tools and functionality of pandas directly.
Now, if we would like to write out this into a file, we can do that by doing this SQL 3 command here.
So we make a connection to a new file.
And then you run the pandas data frame method to SQL and say we'll write out the presidents table to the connection.
Now, if you don't want to use cell magic in this way, you can also use pandas directly to query our SQL database.
So I'll show you how to do that from reading in that file that we just wrote out.
So we're going to connect it out to this presidents SQL output.
We're going to now create a cursor that connects to that connection.
And we will create a new data frame by doing the pandas function read SQL.
If you hit shift tab while your cursor is inside the parentheses, you get to see the various calls here.
So we have the SQL command, you're giving it the SQL, then you're following it with the connection.
And then everything else can be these many other options that you have to really customize it.
And once you've done that, be sure to remember to close the connection by doing com.close.
So the new data frame should have everything that we stored in the previous query.
So the three presidents that we saved from above.
And again, this is a data frame that was returned from that.
So I just showed you two different ways that you can query databases.
You can query them with an inline magic, or you can query them through pandas directly.
And either one will return to a pandas data frame so that you can actually use the output in some exploratory data analysis or your full-fledged project.
In this video, we'll be talking about how to actually use R in the Jupyter Notebook ecosystem.
Previously, we talked about how we can actually set up different Python and R environments.
To set up a unique conda environment for Python 2, for example,
we can do conda create minus n for name pi2, for example, just as a descriptive name that you could use.
We set the Python version to be equal to 2, and then the other packages that we would like to install.
So anaconda, Jupyter itself, notebook.
We can do the same thing for the Python 3 environment.
So conda create with a different name Python 3, for example, and setting the Python version equaling to 3.
We also do the same thing when we want to do an R environment.
So in this case, conda create minus n, and we're going to call this Jupyter underscore R.
And with creating the channel by minus C, R tells Jupyter and tells conda that you're actually creating an R kernel as well as the default other ones.
And this creates the R kernel so that the Jupyter Notebook can actually run R natively,
as well as installing a number of different packages that it thinks are both recommended and essential.
And finally, a Python package called rpi2.
The way to activate these commands is you say source activate, and then the name of the environment that you created.
And when you're done with it, source deactivate.
And if you ever forget which environments you've actually installed or what the names you used were, you can do conda environment list.
Let's do that to start with conda env list.
And we see that there are four different environments installed.
There's the root one, which doesn't really qualify as an environment, but then we have pi2, pi3, and Jupyter R.
So let's source activate pi3 and say Jupyter Notebook.
Once we start that, we can start a Python Notebook.
And we see in the upper right hand corner, not only a blue flag that says using the pi3 kernel just for a second before it flashed away,
you actually see that it types Python 3 in the upper right hand corner.
Let's verify that by doing a print 5 plus 5 as a statement and as we can do in Python 2.
And this doesn't work in Python 3. The syntax for Python 3 is with parentheses.
All right, so we are using Python 3.
Let's close and halt this and shut down the server by hitting control C twice.
We can tell that we're using Python 3 because pi3 is at the beginning of our terminal screen right there.
So I have to say source deactivate.
Again, conda env list.
Let's switch to Jupyter.
The command is the same Jupyter Notebook.
Now we can click this pure R example and it loads up R.
Just in case you're curious, we can go back to this home directory and create a new, in this case, R.
And this is an R kernel running natively.
So you can tell again, look in the upper right hand corner, not only is it not using Python,
it's actually using the R kernel natively for this entire notebook.
Let's go back to this pure R example.
So what is it that R can do?
R is a language that has some design choices that are slightly different than Python,
but it does have a huge statistics library packages.
So you load them in and everything you'll be done in this notebook will be actual R code itself.
And again, just looking in the upper hand corner, this is now R code.
I loaded a few libraries here.
These are some standard, actually really nice libraries in our supplier package and ggplot2.
This economics data comes when you load in the supplier library,
and you see the head of this economics data.
You can create a ggplot command by doing this R code here.
And just like with the Jupyter Notebook, we're using Python, we see inline plotting
so that all of the workflow is in the same really nice way where you can do this piecemeal
exploring by looking at a single piece of R code in the output.
Let's close and save this.
And now let's open up this Rpy2 example.
We are now running again a Python 2 kernel, and we're actually using the Jupyter R environment.
So Jupyter R environment can run Python and it can run R itself.
It's running either one depending on what you started the notebook as.
So we're running this one as a Python notebook.
But here's a really nice feature of the Jupyter Notebook.
You can intermingle Python code and R code in the same notebook.
I'll show you how this works.
So the top here, importing numpy as NP.
So again, just Python code, we're creating X and Y where X is this A range.
Let's just look at what X is, an array from zero to nine and Y is some random number plus the X variable.
We import this library Rpy2 and load this extension Rpy2.ipython.
So we load it by doing this percent magic, percent load extension Rpy2.ipython.
And you can do this in a cell that has other code.
You don't have to make this a single cell.
You can see that we've loaded in an extension and we have this other code running as well.
So we have these two numpy arrays, a capital X and a capital Y.
If we'd actually like to do some analysis in R and then push something back into Python,
we do that by now doing a thing called a cell magic.
So cell magics are known by having a double percent sign at the very beginning of a cell.
That means that this top line is a special thing that in this case we're having it,
there's HTML and bash and various other options.
We are using the R option and we are sending in with this input X and Y from the Python environment.
So we are sending to R the two numpy arrays and we would like to get back from R this thing called XY coefficient.
Everything else in this cell is R code.
So XYLM is equal to linear model of Y goes as X.
XY coefficient, which we will be returning back to Python after this cell completes,
is the coefficients of this model.
We're going to print the summary and we're going to make a plot.
So run that cell and we see the formula call here, the residuals, some intercept and X coefficients.
And we have some plots that are displayed in our Python notebook.
And again, we actually get our XY coefficient out back into our Python environment.
So if you're a person who actually likes to use R just as much as you like to use Python,
or you like to use R for particular tasks or you like to use Python for lots of it,
the Jupyter notebook is very, very flexible.
It lets you work in whichever environment you prefer while dropping into the alternate Python or R environment
to do just even a few pieces of it.
So if you're in the middle of a long piece of data science analysis and you need one functionality from R,
you can keep that not only in the notebook, but passing it back and forth through native types.
In this video, we'll be doing a somewhat more advanced topic and it's definitely 100% optional.
We'll be talking about how to get into the guts of the Jupyter notebook system itself
and create a post-save hook, which will, for our purposes, save a script
and an HTML file version of our Jupyter notebooks themselves.
So how do we do this?
The first step is to actually create a notebook configuration file.
Now you can do that if you're interested in doing it in just your root environment
or having this behavior be copied everywhere you are actually working on anything to do with the Jupyter notebook.
Just go ahead and run Jupyter notebook generate config and I will copy and paste this into the terminal.
So you can see what it looks like when you run this.
The key takeaway here is this writing default config to now this should be your home directory dot Jupyter slash
and then it's going to be this file called Jupyter notebook config.
There's another way you can do this if you want to make this for a specific type of analysis.
So maybe only the analysis you do involving housing data.
Do you want to have a special behavior happen?
You can do that in a somewhat roundabout way.
You set this Jupyter config directory.
It's an environment variable and set that to be a thing that doesn't exist yet.
A home directory so tilde slash dot Jupyter save.
You run a command that starts like this and then you generate the config.
So I will show you what this looks like.
So it wrote the default configuration file to dot Jupyter underscore save, which is the name of this profile,
and then the same Jupyter notebook config file.
Now, running it in this way, you have Jupyter and configure before you do the actual command sets it as a temporary environment variable,
meaning it's only set from that one command.
If I try to echo this, I won't have anything stored in it.
So I'm not exporting this as an environment variable.
Now, I have a bit of code here that I'm going to actually toggle this header and this toolbar just to give us a little bit of extra space.
I have some code here that I would like you to add to your Jupyter config profile file.
So this Jupyter notebook config dot pi.
And instead of trying to type it off the screen, you can actually access it by typing HTTP colon slash slash b i t dot l y so bit dot l y.
And then Jupyter underscore profile, you click on that, you will go to the same exact code I have that I typed out here.
In this case, I will actually copy this code and we're going to open up the file that we would like to modify.
So in this case, we're going to be modifying this Jupyter save underscore Jupyter notebook config file.
You can do with any text editor, I'm going to use sublime text.
So sublime text open it up.
Now, here's what the file looks like.
It's actually a whole lot of things you can do to modify the behavior of your Jupyter notebook and they're almost all commented out.
So you can read through this if you want to actually make different changes than what I'm going to recommend.
But this is where we post just at the top this code, just a brief overview what's happening.
It defines a function called post save, and it basically grabs the path of the notebook that's currently running.
And it actually tries to run this command ipython nb convert to script,
which means it's going to be a .py file if it's a Python file or a .r file if it's a r notebook,
and an HTML file, which means that it'll just be the rendered HTML version of it.
And the C dot file contents manager post save hook equals post save.
So this is a way the Jupyter developers have allowed a person to make changes after every save that they do.
So let's save that and let's go back to our notebook.
So let's list what's in this directory.
We see the name of this current notebook is autosave other formats.
I'm going to toggle that away again.
So we see it here when I type ls.
We can also do exclamation mark ls to do a command like this.
And we see that when we save this, we see a checkpoint is created, but no other new files are being created.
If we would like to see what happens when we run Jupyter Notebook with this new Jupyter save configuration file,
we'll have to run a command that looks like this.
Jupyter Configure equals this with Jupyter Notebook.
And in this case, I would actually like to save this entire thing as an alias.
And then you can add this to your bash rc or you can simply run this in a single line on your terminal.
If you just run it in your terminal, however, it will not set it as a thing.
So if you restart your computer or open up a new terminal, typing Jupyter save won't work.
If you add this to your bash rc, then this special way of opening Jupyter Notebook will be saved.
So let's close this current notebook and let's type Jupyter save.
And let's reopen it again in this new way.
So we just opened it up.
The list function down here should show us what we saw before.
So we see the same files in this directory.
When I click save, if our post save hook worked correctly, we will see autosaveotherformats.py and autosaveotherformats.html.
So I'm going to do that after I click save, type ls again, and we see that we do have two other forms.
I have html and .py.
Just to show you what those html and py versions look like, let's open that up.
One last note, every time you hit save, it will overwrite the same file a bunch of times.
So it's not going to create new versions of this.
It's going to just continually overwrite this and always keep the .html and the .py files completely up to date.
Let's look at one of these html files actually looks like.
So let's go back to the terminal to open a new one.
So by typing open autosaveotherformats.html, we actually have the fully rendered notebook here.
So what we see here is what we saw on the other page and this is now the html version of this.
This can be emailed somewhere.
This can be posted online somewhere and people can see this.
Now the links work like you'd expect and the code is all formatted and looks like it looks in the notebook.
But since it's just an html file and it's not an actual notebook running, none of these cells are actually computable.
I can't actually rerun these cells.
So now we have a way of creating a post-save hook that lets us save out automatically html and script versions of any notebook that you're saving.
If you would like to commit this to your GitHub repository for fellow members of the team to review in different ways,
then having a post-save hook like this can save you tons of time and keep everything up to date.
In this video, we'll be talking about a really fun topic called widgets.
Widgets is an entire aspect of the Jupyter Notebook ecosystem that lets you do interactive things with the notebook.
Let's go over to the notebook.
This top cell has various imports, matplotlib, numpy, and so forth.
This last line in this cell actually imports the ipython widgets and we're going to import a number of sliders,
a float slider, the integer slider, a toggle button, and this interactive thing as well.
So let's execute that by typing shift enter.
Now this next cell contains a simple formula.
We define a Python function named polynomial.
It takes three arguments which has default values, so slope of 2.0 and intercept of 5 and showpoints which can be either true or false.
We're going to create some x values which is just a linear spacing from negative 10 to 10 using 50 points.
We're having a y value which is just the slope times x plus the intercept.
Everything else in this function is actually going to be plotting something.
So this tells us the figure size we're going to use.
The next line tells you that we're going to actually use a figure.
The next two lines actually talk about whether or not this showpoints is true or false.
So if it says showpoints, we'll see what this actually does in a second,
but it'll add the actual data points we are plotting up when we define this x at the top line here.
Finally we plot x and y and we set some window parameters and give ourselves some axes.
The last thing we do is add this tight layout called the very bottom.
This just helps clean up the matplotlib plots before they're finally ready.
So after executing this cell, we now have defined polynomial.
Let's scroll down to this next cell.
I'm defining a thing called a slope slider.
This slope slider is called a float slider, which means it can actually take float values.
That's what it's actually sweeping across.
The value is 2.0, meaning that that's the starting value for this slope.
Let's actually start this at minus 10 at the maximum of plus 10 and step size of, oh, let's say 1.0.
The next line defines this object called w, which is interactive.
The first argument you give interactive is actually the function that you want to be interacting with.
In this case, the function we just defined polynomial and any other widgets that need to be connected to it.
So in this case, we're going to connect the slope parameter that's given to the polynomial function to the slope slider.
Now we call it slope slider, which is because we want to have a descriptive name.
You can name it anything you want.
The last thing we do is actually execute this w. Let's see what we see.
So we see three widgets that we can interact with, the slope, the intercept, and show points, which is toggle.
So let's scroll down and I'm going to actually hide this toolbar so we have a little extra space.
So we have the slope, which is 2, and now you can actually click and drag this to different values.
And as you drag it to the right, you're increasing the slope and we can see that it's actually correspondingly increasing the slope of that line in the plot.
We can also move the intercept point up and down.
As we know, an intercept just changes the Y positioning, shifting these things linearly up and down.
And of course, the last thing is to toggle on and off show points.
If we want to change this, we can actually make this much more sensitive by saying let's make the minimum minus 100, the maximum plus 100, the step size of, oh, let's say five.
Now, as we change the slope, it should be much more sensitive than it is because we're now at slope of 75.
And as we go negative, we can see that as well.
So as you can tell, having this kind of functionality at your fingertips is actually incredibly useful during all phases of doing a data science project,
especially during the exploratory data analysis stage.
So you can imagine if you did something like K means to look at your data.
You set K, the number of clusters you're fitting for, the number of centroids, and as you can move that back and forth with the integer slider, for example,
you can see how well the algorithm is actually clustering on that number of centroids.
So being able to do that in an interactive way can speed things up quite a bit, and it's really nice.
So this is a somewhat simple example that I just showed.
Here is a much more complicated example, but just to give you a sense of what is possible with this kind of a thing.
So I'm not expecting you to actually read this and understand the code that goes behind it, but let's just execute this real fast.
This is one of the projects that I was working on just on my own, where I want to actually have some random points in a small area,
and I would like to interpolate with a spline interpolate those random points, and I wanted to see what that looked like at the end.
So I can say the number of points that I'm randomly generating and splining between, and as I slide this to the right,
you can see the pattern becomes more and more complicated.
And as I slide this to the left, we get much simpler shapes.
We also have a smoothing parameter here, which can give you a smoothing factor to these kind of more complicated shapes.
It sort of unwinds the and rewinds up the knots and the alpha value, for example, like how dark this is.
Or if I want to have a slight jitter to each of these strokes, I can add the number of brush strokes and then increase or decrease the jitter for this.
So obviously there's a lot going on here, but this is one aspect that shows you just how, first of all, how quickly this can refresh,
but also how useful it is.
In this video, we saw how we can use interactive capabilities of the Jupyter notebook to help us plot and look at data and change the values by sliding sliders around.
In this video, I'll be talking about some bleeding edge developments in the Jupyter project.
A specific thing called Jupyter Hub.
If we were to go to Google, let's just search for it by saying Jupyter, then HUB, the top link will be this GitHub repository, which is Jupyter slash Jupyter Hub.
And this allows, as it says, multi-user servers for Jupyter notebooks.
In other words, if you have a server where there's data being held for the data science team, you can run a single instance of this thing called Jupyter Hub.
And it allows many different data scientists to log in and start a Jupyter notebook on that server co-located with the data.
Now, this is an active development. It's changing on weekly timescales.
So if I were to actually show you how to set it up today, by the time you saw this video, it would probably be different from how you're supposed to be setting it up then.
So for right now, I'll point you to this documentation and mention that it's actually very much bleeding edge, but I think it will be the future for data science teams.
Just to give you a sense of what it looks like when you were to use Jupyter Hub, you can go to try.jupyter.org and hit return.
What you're actually interfacing with here is a Jupyter Hub server somewhere in the back end, currently being hosted by Rackspace, apparently.
And you can start a new notebook in any of these different styles, so Bash, Haskell, Julia, Python 2, or Python 3, R, Ruby, and Scala.
So you can start a notebook here, and this is just letting you run a temporary quick one. You can also start one of these notebooks, like this Python one.
And it starts with this warning, don't rely on the server for anything you want to last. The server will be deleted after 10 minutes in activity.
So that's important. This is just a demonstration area, so it's not for long-term storage of some sort of data science analysis, but it gives you a flavor of what the Jupyter Hub will be doing if you were to install this for your own sake.
Now you can actually run this Jupyter Python 3 notebook, and you can actually see the fun results that come out from this.
So we see some plots here, and everything works just like you expected to when you're running the Jupyter server locally, which is how all the videos I'm doing in this lesson are.
Separating the server from the notebook aspect is that you can do something like this in the future, have the server being hosted on some server somewhere, and being able to access it just through the browser,
and having the same exact functionality that I've been showing you for the entire course so far.
And one last thing just to show you how fun this is, let's navigate back to our initials tried dot Jupyter thing.
So a couple other things you can do besides notebooks. This is true for the local server as well, but just to give you a sense of this, you can add a new folder, which is kind of un-exciting.
You just have a new un-entitled folder here. Then you can do this new text file. So if you click text file, instead of starting a new notebook, you're starting a new file.
This is a lightweight in-browser text editor that has various options. You can choose what kind of key mapping you'd like.
So I prefer sublime text these days as ways of interfacing with your text editor.
So from here, you can do your standard Python, and you can both create and edit Python scripts or any kind of text file that you want to.
That's located on this server. And of course, renaming the file is as simple as clicking this top thing, calling it startup.py, for example.
And once you do that, syntax highlighting gets turned on. You can save this and rename it and then navigate back to the main server page.
And the last thing to show you is that you can also start a terminal.
And here you actually have the terminal for your tri-Jupiter. And this is the same case for if you're running this on a server.
So you can actually have access to the terminal with all the functionality of a standard bash terminal there.
So we see the startup thing. We can copy that startup.py folder and call it something else.
Going back, we should be able to see this.
It's a very cool thing, and it will definitely be the way of the future if you have data science teams working and needing access to a single server somewhere that has the data in some database, for example.
So Jupyter Hub, it will be the future. It is bleeding edge. So try it out. It should be pretty usable, but the exact instructions will be different from what I would say today.
In this lesson, we'll be taking a look at organizing the overall structure for a data science team to be working on their various projects.
So in this notebook, I'm going to use the slide show button that we installed in a different video.
And I make this full screen by clicking shift command F. The initial topic is questions to ask to organize the workflow of a data science team.
So the first question is, how many data scientists will be working on a single problem?
And the high level view of this is to basically break this up into thinking about this in terms of Git repositories.
What I mean by that is, if you have different data sources and different problems working in a single company, let's say, then you should definitely use different Git repositories.
If you have fewer than 10 data scientists working on the same data, working on different problems, it also probably makes sense to keep everything in a single Git repository, although it doesn't have to.
If you have different concerns, feel free to break that up.
And if you have more than 10 data scientists and they're working on the same data, but they're working on different problems, fundamentally addressing different data science issues.
And I recommend using different Git repositories. And all of my recommendations will be within context of a single Git repository.
The second main question to be asked is, where is the data actually hosted?
If it's small enough data to be loaded onto a data scientist's personal laptop, then it's very simple to actually just use the data on the laptop locally.
So I would recommend just running the Jupyter Notebook as I'm doing in most of the videos for this course, where you just open up a terminal on your local laptop or local desktop and just run Jupyter Notebook.
However, many data science projects actually use big data. They access the data on some other server or something like this.
And in this case, you have a couple of options.
The most obvious one is to say, if you can access this server data via SSH and you can actually do work in a server, then you can actually run a Jupyter server on that server and you can SSH tunnel and forward your connection to that server.
That way, both the data and the Jupyter server are on the same machine.
Another option is to consider using a thing called Jupyter Hub.
A Jupyter Hub would have to be installed on the server where the data is actually being held.
And if I click this link, you go to this GitHub page here.
So it can be found at github.com slash Jupyter slash Jupyter Hub.
And you can see it's a bit more work than we can go into.
It's a bit outside the scope of this class.
But Jupyter Hub is a multi-user server for Jupyter Notebooks.
And there's actually some really nice documentation to explain how this can be set up on a server or some AWS instance, for example.
There's lots of installation instructions and things to work on here.
So those are the main questions to be asking.
At what level do you set the Git repository?
And where are you going to be running this server?
Are you going to be running it on a server somewhere?
Or will you be running it locally on your local laptop or something else?
Once you have those two questions settled, then the mechanics of actually how do you work on a Jupyter Notebook in a single repository are what we'll deal with next.
In this lesson, we'll be organizing our work into two different types of notebooks.
Conceptually, there are two types of notebooks I'd like to introduce.
One called a laboratory notebook and one called a deliverable notebook.
The difference here, a laboratory notebook is in the same style as lab notebooks that are actually in science labs throughout the world.
And by that, a lab notebook keeps a historical record of the analysis that's been explored.
So each day, a person goes to a lab bench, writes down the date at the top of the page, writes down what happened in lab that day for that particular experiment.
And this record just continually gets amended to.
It is also meant to be a place where there's development or scratch ideas or initial analyses, and it's very much not a polished piece of work.
It is meant for record keeping of scratch pad type nature.
And each notebook is controlled by a single data scientist.
And by this, I'm talking about a Jupyter Notebook where it is a single person, single data scientist's record of what they were doing that day and it is not shared by anyone else.
Now, it's not secret, people can look at it and you can upload it as well, but it's not meant to be viewed by other people necessarily.
A few more final points on lab notebooks.
Split the notebook when it gets too long.
And too long is just sort of a personal preference as you start scrolling down the page as a point when a lab notebook or any notebook gets to the point where, okay, this is too much of a document to look at at one time.
So then split it.
There's no cost in splitting it.
And you can think of this as just turning the page in a lab notebook.
And finally, if you're working on a single day, you can actually split notebooks into different topics.
So for the same day, you can actually have two different or more notebooks.
And if you're splitting by topic, that makes sense as well.
In contrast to a lab notebook, there's another idea of a deliverable notebook.
As I work as a consultant, most of my work is actually going to be delivered either to a project manager or to a client.
And these notebooks are different from lab notebooks in the sense that these will be delivered to someone to consume besides myself.
Now, candidates for deliverable notebooks can be any notebook that will be referenced in the future.
By this, I mean, if I expect someone else to also use the same data cleaning notebook, for example.
So I might have a notebook that explains how I took raw data and transformed it into the clean data that I use for the rest of the analysis.
And I might provide a single link to a deliverable notebook, which is simply the data cleaning of the raw data.
And in that notebook, I'll have things like what the actual transformations were, but also reasoning behind it and some documentation around it.
So this is for anyone who wants to know how is this data actually cleaned?
There's a single spot for it to look at.
And obviously, of course, the final fully polished and final analysis of a data science piece of work will also be considered a deliverable notebook.
I also recommend that deliverable notebooks should be peer reviewed via pull requests, which means other members will actually review the notebook before it's accepted.
Other members can be other data scientists or it can be a manager or something else.
And these notebooks are controlled by the whole data science team.
If we think about these notebooks as living in a certain repository, for example, then the whole data science team will have these deliverable notebooks,
which are in the same topic scope as the problem that they're all together trying to solve.
So how do we organize the directories so that the lab notebooks and deliverable notebooks all are in their proper place?
So these are the minimum directories, and I think it can be expanded by a few or taken away by a few.
So I have listed here the directories I think belong at the top level of a data science git repository.
The first one is data. This is optional.
If you have very small data and you want to have it locally, it's possible to include it in a git repository.
Generally, though, data science data is actually backed up outside of version control. It's in a different environment.
So this is definitely an optional directory to have.
The second one is the deliver directory. This is where the final polished notebooks for consumption.
If a new data scientist is coming onto the project, they will look in the deliver directory to see what has been done before.
In the develop directory, we store the lab notebooks, and I will explain the naming convention in a further video,
but this will say all the scratch work that has been done by each of the data scientists working on this problem.
The directory called figures will contain the figures that have been the output from both to develop and to deliver notebooks.
I will be expressing a bit more on that in the future.
And finally, a source directory where as you come up with various scripts or modules or anything else that needs to be,
that's actual computer code that doesn't belong in a notebook directory, goes in a source directory.
Again, you can add to this or you can modify this as you want to,
but I think this is a good starting structure to work from and modify it as your needs evolve.
In this video, I'll be telling you about my recommended convention for naming lab notebooks.
So naming a lab notebook can be a more difficult problem than you might expect,
especially if there's many different data scientists working on a similar problem.
So to help with that, the following convention is what I recommend.
You can obviously change this to fit your own needs.
I recommend prepending each file name with the current date that you started the work on that notebook.
So in this case, it was started 2015-11-21.
I also recommend it in that format where it's the year-2-digit month,
meaning if it's 3, it'd be –03 – the 2-digit day, like the month in 04 and so on.
This is called an ISO 8601 formatted date, and it just helps with keeping everything
so that it's sortable in a nice way.
So the initial part of the name is the date that you started working on that particular notebook.
The second piece, immediately after that, is the data scientist's initials.
So in my case, my initials are JBW.
So I put – after the date, my initials.
Or you can put it – if you have a data scientist with the same initials,
you can just put some unique signifier that's the same every time,
so that if you want to look at a directory that has many different data scientists' notebooks,
you can do an LS for that person's initials and find their notebooks.
And finally, I recommend putting a 2-to-4-word description that describes what goes in that notebook.
So in this case, coal, predict, RF for random forest, regression.
So looking through this later on, I can think back,
okay, what was I doing two months ago?
Something with random forest, and it was a regression.
And on a classifier, seeing this in the title helps pick this out.
In this video, we'll be talking about version control.
One of the key questions you have when dealing with a data science team
is how do you peer-review code and how do you store analysis in version control like Git?
And I'm going to assume a number of further constraints.
And I think this is probably the most restrictive constraints I can think of.
This might not apply to you, but I think if it does apply to you,
I have reasonable work rounds for each of the possible concerns.
For example, imagine you have a project manager who would like to see the notebooks you're working on,
but they don't want to install Python or iPython or anything like this.
Or consider that you might not be using GitHub for whatever reason,
and some of the nice tools that GitHub has for showing diffs aren't available to you.
Or if you want to review the Python code itself and don't want to have to look at it in a notebook environment.
How do I recommend dealing with these kinds of constraints?
While also maintaining a peer-review of the code stored in the version control.
The standard practice, for my answer, is that each data scientist who's working on the same problem in the same repo
should have their own development branch.
And each day, or even more frequently than each day,
but at minimum, work is saved and pushed to the dev branch that they have daily,
which means that anyone can then check out another data scientist development branch.
When ready to merge to master, you have to do a pull request.
So a data scientist says, okay, I think the deliverable notebooks as well as my laboratory notebooks
are ready to be reviewed and pulled into master.
Now, the question of what exactly to commit.
This is a question that people who come from a more software engineering background
might start to recoil at my suggestions here.
I say this after a lot of thought, and there might be a better way of doing it,
but this is the best way that I can come up with.
So I recommend committing the .ipynb files, which are the notebook files,
the .py and the .html of all notebooks, both develop and deliver.
And I'll also say any of the figures that are saved should also be committed.
Now, when I say the .py and the .html, what am I referring to?
So I'll go to an open notebook right now.
This is a notebook for making a prediction about call production.
And this is in the develop folder of a certain directory.
And I have this notebook that is currently running.
And you can tell it's running by the green symbol here,
and the word's running green all the way to the right.
So let's go to this running notebook and actually save it, save in checkpoint,
and download as a Python file.
Let's download it to the same directory of develop, save that,
and let's download this as an .html file and save it in the same spot.
So if we take a look at what this is,
it has taken all of the Python code and none of the output,
but stripped out everything else in this file that's not Python code.
And so you see this input three, input four, and so on.
This is delineating the cells in the notebook.
But everything you see here is actually Python code.
So this can actually run as a .py or you can run it as Python, this file name.
And this .html file, if we open up this file,
we actually see the .html representation of the notebook.
So this is not executable. This is just a .html file.
And so this can be copied into an email and read by anyone who opens this with a web browser.
You don't need to run Python or ipython to actually see the output here.
Again, the limitation though is you cannot actually edit this code and make a new plot,
but this is great for being able to share a particular notebook.
So I recommend saving both of those file types to your git repository.
And of course, all of the figures as well if you create separate figures.
The reasoning behind that is that the .py files allows a person to make easy changes
to the actual Python code itself, as well as to track those changes.
The .html file allows a person to see their fully rendered notebook
without having to run a notebook themselves.
So the benefits of structuring your repository this way are several fold.
First of all, you have a complete record of the analysis that includes dead ends.
So if one day you worked down a single hypothesis and turned out that it wasn't very useful,
that is still saved in the lab notebook directory.
It also allows for easy peer review of the analysis and of the dead ends.
If in the future, a different team member has an idea to try to do a random forest regression on the data,
they can actually see if someone else has done the same type of analysis,
and if so, what led to a dead end, for example.
And finally, project managers can easily see and read the analysis with GitHub,
because GitHub itself renders IP, UI, and Bs natively.
Or if you don't have GitHub access or not rendering it for whatever reason,
if you save the .html files, anyone can actually see the rendered notebook
without having to run any code themselves or installing IPython or anything else.
Some final organization thoughts of this whole structure.
So organizing the workflow for teams is actually a difficult problem,
and I think this is a very good framework for having some standards.
And this bullet point about the wrong thing solves the problem.
Often with version control, software engineering types think we need the source that's version control
and we don't need to track the output, or that output is something that's blown away.
In data science work, the output is often the thing we need to look at.
For example, if there is a plot that shows some deviation,
that plot is best viewed in the peer review process,
actually in the notebook itself, or in an .html rendering of that notebook,
because that gives rise to any sort of correction or reinterpretation that needs to happen.
So the output actually is the thing that matters in a lot of data science work.
So storing that in version control is actually the right thing to do,
even though in typical practice it's the wrong to actually store the output.
Finally, I am open to new ideas if you have a better way of solving these problems,
or if your situation is completely different so that such that you will always be using GitHub,
you never have to worry about seeing a rendered .html file.
You can make these modifications by doing your own version of this kind of organization.
So hopefully this gave you some structure to organize how a team of data scientists would work in a Git environment.
In this lesson, we'll be getting some data that we can actually do some data science with.
I recommend having a data folder in your projects directory that actually is at the same level as your
delivery directory, your development directory, and your source directory.
In my case, I have about 10 files in here that are cold data from the US government.
If you'd like to grab this same data set so you can follow along, go to www.eia.gov.
This is the government's energy information administration website.
And if you go to the data tab, you can scroll down to where it says production, give that a click.
And there's lots of different data available here, but we're looking at the historical detailed cold production data
available from 1983 to 2013.
Select which year you'd like to do, and in case I picked 10 of them, click the arrow here and save it into that data directory.
Once you do that, you'll then have the data that we'll need for this upcoming lessons.
In this lesson, we're going to take our very first look at the data.
We might even do some initial data cleaning.
So I'm currently in this directory where you can see we have data, deliver, development, and source directories.
I'm going to start the Jupyter Notebook by again typing Jupyter Notebook.
From here, we see the same directories I just saw in that directory.
Let's open up the development list and start a new notebook by going over to new Python 2 notebook.
From here, we see the familiar text box where you can type in code.
And here we see the code box is actually surrounded by green, which means as we type, it should be typing in text into that cell.
We're going to need the pandas library and we're going to import it as import pandas as PD.
This can create alias for the pandas library to actually be called PD.
This is a standard way of calling pandas and I recommend you following the standards as often as possible.
This lets you share your code with other people in the most seamless way possible.
To run the cell, I can click the run cell button in the toolbar or I can have done the shift enter technique,
which as you can see increments which input number it is by one.
The pandas version that I'm actually running is done by doing a print double underscore and then hitting the tab button.
Hitting tab is a thing you should be thinking about doing quite often because it often lets you make sure you don't have to type everything out.
It's faster, but also make sure you are in the right vicinity of what you're hoping to do.
So there's a version. I'm going to hit return here and then shift return.
And it prints the pandas version that we're using, which is 0.17.0.
From here, let's actually take a look at our very first data file.
So the way we can read this in, we happen to know and here's an interesting side note.
If you type ls and execute that, you actually see all the folders in the directory you're currently in.
If you type ls up one, we see the parent directory.
And if you'd like to look at what's in data, we see the files that we just downloaded in the previous video.
So let's load in one of these Excel files and take a look at what's actually in them.
So I'm going to create a variable called df for data frame.
And I'm going to df1 for the first one.
I'm going to do pd.read and I think it's going to be Excel, but I type the tab and I see an option pull up and it is.
It is pd.read underscore Excel, open parentheses.
At this point, if you're not sure what a function does, there's a function called tooltip,
which is generated by holding down shift and hitting tab once.
Here it tells you the signature for this function, which has an input output, a sheet name, header, and so on.
There's a lot of different options available for reading in Excel files.
There's actually a longer version of this, where if you do shift tab tab in rapid succession, so it's a double tab,
then you have the full doc string and the examples that go along with it.
So this is a very useful feature so that you can actually look up documentation on the fly and it's very useful.
So in this case, we're going to try to load in the data from above.
And again, tab completing commands will make your life much easier.
And as I start typing out this, I can hit tab and it actually produces again a list of possible data sources.
Let's just see if this works.
Head is a function on a data frame and it lets you show the various options.
So we see that a number of things have happened here.
First, we have the year, the MSHA ID, the mine name, the mine state.
We actually see some of the data and this is just the first few rows by doing head.
We're not even doing head because it actually stores the full output of this is a separate thing that you can actually call.
In future lessons, I'll explain exactly why using dot head as best practices.
But for now, let's just use dot head to look into the contents of our pandas data frames.
At this point, we've taken a first look at loading in some Excel data files and we're going to start looking at this and playing around with it.
And we're going to take a look at how we can start to manipulate the data that we've read in in ways that are useful for analysis.
Last time we read in the cold public 2013 file, took a look at the header and the heading had an interesting, well, let's call it a problem.
The historical cold production data is the title here.
There's a source function.
There's also a bunch of NANDs and all the columns are unnamed.
And this is most useful when this line, line two, which is our row two is actually your MSHA ID, my name.
This is supposed to be the headers or the column names and all the rest of it should be the actual rows of data.
So we're going to put the second row here up to the columns at the top.
We'd also like to make this ID the index for the pandas data frame.
We'll go into what exactly why in the future, but for now, let's merge the reading in of the data frame with the printing out of what the head of that data frame looks like.
The by going up here and clicking edit, merge cell below because we've actually selected the above cell.
So merge the cell below into one.
So now that I execute this cell, we see that there is in one cell, both reading the file and looking at the head of the file.
Now this is again wrong.
We would like to remove this top part.
So way to remove this is we're actually going to use a thing called header and start giving it a number.
As we look at this, we can see that it actually takes a header equals zero as the default value.
So if we do header equals one, it actually deletes that top row.
And so this is a way of telling the pandas that, hey, you don't have to modify that Excel file.
You can just, when you read it in, know that there's two lines of header files.
Now there's two lines that had data in it.
And there's a third nan line that just knew it could not possibly be the header.
So it removed that.
So now the column names are these bolded ones are at the top.
We're getting very close to what we actually want.
Another thing we'd like to actually do is set the index.
So we set the index by typing index and hitting tab because we think it's going to be something like set index or index set.
And it's index columns equals, if this type, we like the name of it.
So we would like to do the MSHA ID as the index column.
And doing that, we see that the MSHA ID is indeed the index for this data and the columns are all appropriately named.
This is one way to interact with the pandas library.
But it actually applies to all Python libraries that have any sort of documentation strings.
Just to give you an example of that, I'm going to save this currently and just show you example function, right?
We define a function by typing def.
We'll do it test function.
Let's say it takes two values first equals five and second equals 10.
And it will return first plus second.
Let's give it a doc string and we execute that line.
If we start typing test underscore f and then hit tab, it will automatically complete that because we have a defined function here called def function.
We do the initial parentheses and hit shift tab.
You actually see the doc string that we wrote just above.
This is an example and it has the signature of it too.
The first equals five, second equals 10.
If you want to redefine what actually we give it, we can say first equals three and the test function gives us 13, which is what we'd expect.
So that's just a fun side note on how the interaction with Jupiter notebook lets you look into the doc strings of functions that you define yourself as well as any of the libraries that you'll be using in your data science day to day.
In this lesson, we'll be making a new GitHub repository for a new data science project.
So let's go over to GitHub.
And from GitHub, if you go all the way over to the right, you can create new, you want to click new repository, give the repository some name that you think makes sense.
So we'll do some coal exploration.
So let's make a coal exploration repository name.
You can give it a description if you'd like to.
You don't need to decide whether it will be public or private.
I'll let it be public so that you can see this as well.
And generally, I like to initialize the repository with a read me and get ignore file.
That's Python because I use a lot of Python code.
And I add an MIT license.
After doing all this, you click create repository.
Once you click create repository, you can go over to this place here where you can click SSH.
You can have HTTPS or SSH.
I just use SSH most of the time.
Clicking once in here highlights everything.
Command C will copy this and going back into a terminal type get clone and then command V to paste the required link.
Hit return.
And you will now clone the GitHub repository to your local machine.
And from here, we see a new coal exploration folder being created.
And if we CD into coal exploration, we see that it has a license and a read me file that we've made previously.
In this lesson, we'll be taking our GitHub repository that we've just started.
And we'll first look at the data.
So the directory as we last left, it has two files in it, a license and a read me file.
We're going to create some extra directories and some structure around here.
And I'll go through the reasoning behind this in other videos.
But we're going to create using the make directory command, a data directory, a deliver directory,
which is going to house the final deliverable important Jupyter notebooks, a develop directory,
which is where we're going to mostly do our development place,
the place to put our source code if we have any scripts that we'll end up using.
So separate from ipython notebooks, usually Python files or other kinds of scripts.
We'll go in a source directory and a figures directory running that command.
The folder structure that we have now has a data deliver develop figures and source directories.
So let's actually get that data and put it into this directory.
You might have already downloaded it.
If not, again, the way to get this is to go to EIA.gov slash coal.
Go to the data tab down to production.
And we go to the historical detailed coal production data.
And let's just use the year 2013 for now.
We're going to go into this coal exploration, navigate to the data folder and save.
That is done downloading.
You can see it in this folder as coal public 2013.
Great.
So let's take a look at this.
We'll open up a Jupyter notebook and take a look.
So from this top level directory, I will start Jupyter notebook.
I can now close this download file.
And you can navigate this structure similarly to the terminal itself.
So you can actually click data and you see the coal public data that we had before.
We can navigate back and let's go into the develop and start a new Python 2 notebook.
It starts off being called untitled and that is a not very helpful name.
So I recommend using the date in ISO 8601 format.
And the reason for that is that it helps with sorting.
But basically it goes year dash month dash dates.
Today is the 21st.
After you do the date, I recommend, especially if you're working in teams,
to have your initials or some other identifier that creates it so that people know it's your notebook.
And so I'm going to type my initials here.
And then I recommend having a couple words that describe what you think you're doing in this notebook.
So I think I'll just say first look.
So now I've renamed that notebook and it hopefully tells us when it lasted the last checkpoint.
This means when it's been saved auto saves every once in a while.
You can also click this button would you just see that the last checkpoint saved and you also do command s,
which is how I normally do it.
So this means that it's keeping auto saved versions of this as we go along.
All right.
So there's a number of libraries that we'd like to import.
And I import these almost every time and it starts off with matplotlib inline.
So this percent sign at the top of the line means it's a magic import.
And we also have to import matplotlib like so.
Importing it as PLT is the standard best practice for doing that.
Next we import pandas.
And we should also import seaborn, which is a package that wraps matplotlib.
Interestingly, you're supposed to import seaborn as SNS.
I don't know exactly why, but importing it as SNS is the standard way of doing it.
Also, if you do SNS dot set, it actually sets a number of the default parameters for matplotlib.
So it already looks nicer if you just use it from there.
So let's go ahead and start with that.
And now let's read into a data frame, the data file that we just downloaded.
So we say df equals pandas library dot read hit tab to see the options go to Excel.
And we navigate to the directory by going up one directory by doing dot dot slash.
If we hit tab, we also get the possible navigation options.
It's in the data.
And if we tap again, it will tap complete to say cold public 2013.
We actually execute that and take a look at the head.
We noticed that we again have this unnamed part at the top.
So we actually wouldn't like to remember that it has a header, set the headers equal to two.
And that correctly gets the column types labeled in there.
And we want to set the index to the MSH ID.
So if it's annoying, you set index by doing index something hit tab and its index column equals MSHA space ID.
Exceeding those two cells, we have the ID of the mine setting as the index of this data frame and all the data in here correctly parsed from that Excel file.
Okay, so I'm going to stop it here and we'll begin to actually start to plot this and take a look at what this data actually looks like.
In this lesson, we'll take a look at the data and do some data cleaning and maybe do some visualizations.
Let's go back into this notebook and rerun the first cell here, load everything in that warning that we've seen before, load in the data and take a look at the data dot head.
So everything here looks normal and the day to day data science work, you often take a look at what's in each of these columns.
So we can just do a very quick look, for example, at a data frame and take a look at the company type.
Now, if we have thousands of rows, we don't want to look at all of them, but we do want to look at the unique ones.
In here, we see that there's three types of unique companies according into our file right now.
We have what I think the word is supposed to be independent producer operator.
The next one is operating subsidiary and contractor.
Now, obviously, this first piece of information is that the data has some data quality issues.
So let's go ahead and actually make a correction here for this data.
We'd actually like to replace all of the independent producer operators with independent producer operators.
So the way to do this in place is to actually do a company type to replace it.
And if you don't remember the syntax for replacing, if you do a shift tab, you can actually see the tool tip come up.
There's two ways to do this.
You can say to replace equals x, the value in place, everything else.
And we can also do it by giving it a dictionary.
I'm actually going to do it the standard way.
So to replace should be equal to, and I will just copy the words from above.
And the value I would like to replace it with is going to be the independent producer operator.
This cell is already becoming wider than the screen.
So I'm going to actually hit return here so that it's lined up with the beginning of this.
So someone later on can actually read this a much nicer way.
Suppose a DF company replace this thing and then do head on this.
It should show us that it is indeed replacing the independent producer, but it hasn't replaced it in the actual data frame itself.
To do that, we have to add an extra command here, which is in place equals true.
One extra interesting, let's call it a quirk of the Jupyter system.
If you're in line with the beginning of this command, if you do a tool tip by doing shift tab, it appears.
If you're not on that first line and it's broken up across multiple lines, then doing the shift tab in the middle here will not work.
If you're thinking is it in place one word or is it in underscore place, you have to do it up here to get the tool tip help.
So it's in place one word.
So I typed it down here.
This will in place change the DF company type to be independent.
So this has now been replaced in place.
Now we also see that even though I could actually hit tab, which is a very useful thing to be able to call a column heading by just typing the beginning of it company tab and auto completes.
Having these spaces is going to just make life a little bit more difficult than it should be.
So what I'd like to do is actually go through all of the columns in this data frame and replace every single space with an underscore.
So it's still readable, but I'd just like to actually do that.
So to do that, we actually would like to do a name of the columns.
So DF dot rename index columns equals and keyword arguments.
So you can say columns equals.
Now this is a really fun trick because you actually pass a Lambda function.
Lambda function says for everything in that columns, I like to do X dot replace.
So similar syntax as above, but I replace all of the spaces with underscores.
So the thing that's being quoted is the thing that's being found.
Single space replacing that space with is the underscore.
So I'd like to rename the data frame where every column space will be turned into an underscore.
And of course, I would also like to actually make this happen to the data frame in place.
So I say in place equals true now to check if that actually worked as we hope we can look at the DF dot head.
And we see that underscore name mine underscore state mine underscore county and so on.
So this with one line and very quickly typing it out replaced all of the spaces here with underscores.
And this will just make life much easier as we go on from here.
Let's also take a look at how big is this data frame.
We have 1400 data points.
And let's take a first look at just what's in here.
So we just read this off as my name all the way through regions and average employees and labor hours.
And let's see what the relationship between the number of employees for a mine and the number of labor hours looks like.
There's a couple of ways we can do this.
Let's see the simplest way I can think of is to do a scatter plot.
So we can do PLT dot scatter and DF dot average employees.
So now I've indexed the data frames column by simply doing a dot before it because it has a space in it.
I would have to have done it the DF bracket space labor hours, for example.
So this will actually work.
You see that the plot actually works as expected.
But now instead of having to type out labor hours previously with a space there, I can actually do dot labor hours.
And that just makes my life just ever so slightly a bit better.
Let's label this.
Okay, so just as we expect, number of employees goes up.
The total number of hours worked at that mine goes up in a pretty linear fashion.
Another way of doing this would actually be to linear regression plot on this.
And you can use seaborne for that.
So SNS dot regression plot.
And I'll pass it the X and Y this way.
And so what you can see here, the regression plot does the same thing as above,
but it actually fits aligned in the data and gives it a bootstrapping in the middle of it.
This bootstrap is done by a confidence interval of 95%.
And it bootstraps a thousand times to the underlying data to actually figure out what the variance is.
So this is a kind of neat, very quick way of getting an initial look at two variables that you think might have a relationship.
And they clearly do.
Now, if you'd like to actually save this figure, as in this isn't just to look at and have it for later on,
you should actually save this figure into the figures directory.
So I would do PLT dot save fig figures.
And I like to actually have the same beginning date structure for these figures,
so that if I am looking through the figures directory later on across all the different notebooks that I'll be looking at,
I can easily re-correspond which figure came from which notebook.
So this is just a little bit of mental accounting to get this straightforward.
And let's do employees versus hours.
Let's keep our underscores and spaces being the same.
All right, so that's our first look at the data and it is a quick linear regression plot against two of the features that we found inside,
as well as a bit of data frame manipulation using pandas.
We've seen a very first look at this and we see that there's at least some trends in this data.
There's probably something pretty interesting in here.
So I'll keep going with this data set and seeing what I can come out with this.
Now I will actually remove this header and I will toggle the toolbar as well as I need space.
So let me go ahead and do that.
So previously we saw with seaborne a really nice regression of average number employees versus labor hours.
Let's keep seeing what's in this data set.
Let's take a look at the columns for column in.
So these are the columns in the data frame.
We have year and then various things about the mind itself, the name, the state, the county, its status and its type,
the company type, union code, there's a coal supply region, the production in short tons,
and the number of employees in labor hours.
So see if the amount people work, like the labor hours total is very predictive of the production in short tons.
Let's take a look at that scatter plot.
Let's take a look here.
So this doesn't appear to be a fantastic relationship here.
Let's take a look at the actual histogram of this.
So I'll do df production short tons dot hist, which is a function on pandas.
And we see a very bad looking histogram.
So it looks like a lot of things in this first one, which is either typical of a power law or some other kind of problem.
Let's do a few transformations on this production.
Let's see if we can find some minimum value or yeah, let's take a look at the minimum value zero.
Let's take the length of the data frame where this is equal to zero.
So if we did first, let's just look at this where the production short tons is equal to zero.
We have what's returned as a series that tells us false false true true false and so forth.
So this tells us whether or not the production is equal to zero.
So we say df where you actually give this as an argument to data frame saying where this is equal to zero.
We get the full data frame where all of the production values are equal to zero.
And it looks to be like quite a few of these things produced zero tons of coal.
In the interest of how much a coal mine is producing, let's take the ones that have produced at least one ton.
We will say the data frame where the production of short tons is greater than zero.
This has values that are not zero. This is good.
From here, we will now set the data frame equal to this.
Now we are at this point doing a slice.
So I will make a note here.
We are removing data here.
That's okay as long as you're keeping track of what you're doing and why.
So the reasoning behind this is if we're going to try to predict, let's say the production of mines and use things like what state the mine is in as a predictive indicator,
let's actually restrict ourselves to mines that produced something more than zero.
And that's the reasoning behind how I choose something like this.
So that data frame is equal to where the data frame production values is over zero.
So let's see what the length of data frame is now.
Okay, so we have 1061 data points.
Let's redo this one.
I'm going to copy this and place it down here just so that we can do a comparison.
And it appears to still have quite the skew distribution.
So I will try to do something now where I will actually take the log of this.
So let's create a new column.
And the way to create a new column in pandas is to actually just create a column as though it exists instead of equal to a function of this.
So I don't know if I have NumPy installed just yet.
So I'll give this a try.
So let's go to the top of the page.
And in all of our imports at the top of the notebook, I recommend keeping them together so that if an airborne later on can see where things were imported.
Import NumPy as NPs.
Now this input is 30.
I've imported it and I should be able to rerun this one all the way to the bottom here and create a new one.
So let's look at df.logproduction.hist.
So what we see here is a very close to a log normal distribution.
So the production of coal mines follows a log normal distribution, which is reasonable from first guesses.
All right, great.
So I think I'm going to stick with this as a thing we're going to be interested in predicting.
So we have our prediction variable.
Now at this point, we've done quite a few things to the data frame itself.
So we loaded it in, we renamed the columns.
We actually created what's going to be my target variable is going to be the production of these mines and did a transformation, which is the log of this value.
After doing all this, I think I would like to actually save out this data frame that I can load it into any future analysis.
So I'll do df.to.
Let's save it as a CSV.
So I'll call it, let's find in the data directory, call public this thing.
Okay, do cleaned version of this and it's a CSV.
So now that I've done this exploratory analysis, I would have this first look that I've taken at and I've saved the data out to this CSV file.
I'm going to copy this into a new one that's going to be called data cleaning.
And in the future, all I'll have to do is load in the CSV file and all of the transformations will have already been done.
And I'll have a link back to the reasoning behind it as well as the actual code that does this process.
In this video, I'll be cleaning up the data cleaning notebook and I'll be doing our first commits to a new branch to keep everything organized.
I last laughed off with this first look and their develop directory.
So what we're going to do now is actually make a copy of this and I will toggle the header for this.
Make a copy and the first thing it does is it opens a new tab with everything copied in the previous one.
And none of the code has been run here, even though all of the inputs have been copied.
What we're going to do here is actually call this something completely different, which is data cleaning.
I didn't put a date in front of it because this is the notebook that's going to be the one that people look at if they actually want to see how we changed the data.
So I'm going to actually close this from this directory.
Go over to my actual terminal here and move from the develop the data cleaning ipython nb, which we just created into the deliver.
So I move the file from develop into deliver because deliver is the directory that people should be looking at if they're actually interested in seeing
the final analysis that matters.
In this case, we don't want to hide data cleaning in this development directory, which has many, many files.
So we've moved it into deliver.
And if we go back to our browser here, go up into deliver and open up the data cleaning.
Now we should actually start to do things like actually creating the markdown file, changing the code from code to markdown,
giving it a nice title and continuing on with this.
So we can say Jonathan by Jonathan to say like, who actually did this?
And then you can look it up in the get repo, cleaned up the data, removed zero production coal mines.
You can actually do a bit more of that in the end.
But for now, that should suffice.
We don't need to actually have any of these plots in here.
So I'm going to be cleaning this up as quickly as I can.
So numpy as NP, pandas as PD, we need to read in the file still.
We don't need to see the head.
We know what that looks like.
This can be left in because it tells us the transformation we made and why the head part doesn't need to be here for the second one.
But we can add a note above it that says mistake renaming indipedent to independent.
Now we're in here changing spaces to underscores double check.
That still looks right.
Okay, it does.
And we will now delete this head, delete the different plots here and give an extra sentence here.
Coal mines without any coal production are removed.
The length is 1061.
And we are now creating a new column called log production, which is the log of the production of the data frame.
And we can put we don't have any histograms here.
So we just leave that out.
And now the output file is this guy.
And I will actually move this to the top here to the output file.
The very first thing you see here will be the name of the output file.
And the last thing we'll do is actually write that CSV to that output file.
So now when I load in this cleaned coal public 2013 and notice that I did not overwrite the old file.
So I strongly recommend keeping the raw files and creating a new file.
That's the cleaned version of it so that if you ever made a mistake in your cleaning, which has happened before, you can easily revert and change that back.
And if someone says, oh, something happened in the cleaning process, you have a full documentation of what happened here.
So we've created the final document that went through and cleaned up what actually happened in the cleaning process.
So anyone looking in the future can easily follow what happened.
So I will now close and halt this directory.
And I'm going to actually do our first commit and we are in the master branch as it sits.
So I will actually check out a new branch branch will be called JBW underscore predict production.
And so we're here.
There's two theories here on adding the data.
So the data here is actually pretty small.
So I'm going to add it to this.
This is also so that you can actually get the data as well.
Generally in a production environment, you don't add the data to your get repository.
This is stored and tracked in some other way.
So I'll add the data cleaning.
I'll add develop and not going to add the figures just yet.
I usually will only add this when I actually have something interesting there.
So this figures is going to be kept on my own directory for now.
Not put into the branch just yet.
Let's look at the status one more time.
So we have a number of new files, the actual data file, the cleaned data file,
the data cleaning that is the official way of actually making this file and this developed one.
So let's commit this.
Let's not call it that then.
And I have to actually configure this.
So I will configure my get do this commit and continue this on in just a second.
So commit the data and I will be pushing it to GitHub.
So the final command I ran was get push origin JBW predict production.
And this means that I have now sent this off to GitHub.
Go back to the GitHub of the coal explanation, reload this.
What we see here is the master branch.
We can actually go to the JBW production branch and see the various things we've done here.
Let's actually look at the deliver and click this IPYNB.
And we'll notice that GitHub does a fantastically nice job of actually rendering the notebook as it looks correctly.
And this is even more dramatic when you actually look at the develop one.
So you can see this and you can see in here if you're browsing with GitHub,
the figures are faithfully reproduced here.
And this is a very useful thing to be able to look at the files being used,
especially when we do a pull request in the future.
Okay, so we've cleaned the data.
We have the way that we cleaned it separated out so that anyone else can look at it in a reproducible way.
And so let's actually try to predict something.
So I'll go back into this develop directory and it will make a copy of the first look notebook that we had.
So I'll make a copy of this.
I'm going to call it CoalPredict.
I'm going to go back to the previous tab and actually finish closing this and halting it.
And just to give you a sense of how everything is standing, I'm now back at the home of this develop thing.
You can see the first look notebook and it's currently black because it's not running.
This one is green because you can see on the right here it says running.
So this is a notebook that's currently being run.
There's a couple of things I'm going to do different here since this is now the prediction one.
I'm going to start off by saying what the goal of this notebook is going to be.
And because everything that's here is a direct copy of the previous notebook, most of this stuff I'll just be able to delete.
So I'm going to toggle the header, give us a little bit more space.
And the changes I'm going to make are basically going to drive me toward being able to make this new prediction.
So first of all, I don't want to reproduce all this cleaning I did before.
So I will actually instead of reading in the previous raw data, I'll actually go into and read the CSV that we saved.
And this is up into the data directory and it's the cleaned public CSV.
And we still need to set the index column to be the MSHA ID.
So that's loaded in.
And actually one thing I like to do is look at the head of the data frame and read it in at the same time in case I need to make any changes.
So the way to do this is since the four is selected with a gray box, if I hold down shift and type K, I'm selecting both the second and third cell, which are index three and four.
If I type shift M, they are now combined into a single merged cell.
So let me just run this one cell and I read in the CSV and then you are seeing the head of that data frame as well.
So we can see that we're loading in the cleaned CSV and the head is looking nice.
I'm going to now delete a number of these things because we don't need them.
One thing I will remain is that we initially did this LEN of the data frame before this was on the first one that you saw on the raw data.
So since this is the clean data, I expect this to be just over a thousand.
Yep, I went to 1061.
Simply delete these and I'll leave this the number of columns in here so we can actually think about what's in each of these columns a bit.
So as we see, this is the production, longer the production is the thing that we're going to be trying to predict.
And let's take a look at just a high level view of the different categories that might be able to help us.
So let me get the columns here.
I think that the mine status might be a predictive variable.
So I do df.mine status.
You see that there's an active men working, not producing, permanently abandoned, active, temporarily closed and new under construction of the different status types.
I suspect this will give me a pretty good predictor into how productive the mine actually is.
So I will actually do a group by on this to see what is in here.
So df.mine status.
Let's do production.
What I did here was I said, take all the ones that have the same status of active and take the average or the mean of the production in short tons.
And we can see that the active ones are much more productive than the temporarily closed ones or the permanently abandoned ones.
It's interesting to me that permanently abandoned has on average 60,000 tons.
Let's look at it in terms of the log of the production.
No, this will be what I think we're going to be going against.
So huge difference in the overall production capabilities, but we'll see how good this is at making a final prediction.
So from here is we would like to predict the log of coal mines.
And we'd also like to know what actually leads to the production, higher production and lower production.
If we look again at all the columns in our data frame, the data that we have year is the same for all of them.
And various things that shouldn't matter at all.
Like the union code is just going to be a code that's given to the mine from a, let's just look at that.
Actually, that might be predictive.
So I'm going to try to throw as many of these things as we can into a predictive model.
So I'll call these features.
And let's start with this as our list of features.
We'll have our target be log production.
So year is going to be entirely unpredictable because it's a single thing.
Mine name, I suspect will not be predictive because it's simply the mine.
The state might be what state is it in, what county is it in that could be useful.
The mine status, I'm sure will be predictive.
Mine type will probably be it's possible that the operating type, the address of the operating company probably isn't because we already have the geographic things done with the county and the state.
Though it's interesting, we'll definitely have some collinearity between the state and the county.
So it's possible that particular county and the state's good.
We'll leave those in.
Leave in the union code, the coal supply region.
We can't give it the production of short tons as a prediction of the log of the production because that's cheating.
The number of employees that are employed and number of labor hours.
Just to clean this up.
So I hold down shift and push the down arrow key and I've highlighted everything to indent.
I'm going to hold down command and hit the right bracket key, which is the square brackets.
So the parentheses are curved all the way around.
There's curly braces, which have a lot of curls in the square brackets.
So holding down command and typing the right one will indent an entire block of text.
If you do the left bracket, it unindents.
This is a quick way of formatting lists.
So the features that we're going to be giving our model are going to be all of these features here.
And the target's going to be the log of the production.
Now of these, I think only two of these are actually numbers to start with.
So I think average employees and labor hours are the only ones that are proper features.
And the rest of them are what I'm going to call categorical.
So the categoricals are these minus the average employees in the labor hours.
And having a trailing comma here is actually okay.
We need commas between all the rest of them otherwise.
But this is one of my favorite features of Python.
And I don't know why it makes me so happy.
But having a trailing comma and having it not have a problem just make me really happy.
So the features, which I'm going to just call the ones that are numeric, are the average employees and labor hours.
The categoricals are the ones that are category variables.
So mine, state, county, status, type, company type, operating type, union code, and coal supply region are all categoricals.
One thing that we'll have to do is create, because we'll be using scikit-learn,
we'll have to turn these categoricals into numbers or into some sort of numerical thing.
And we'll be doing that with what's called a one-hot encoding, also called dummy variables.
There's probably a few other names as well.
So let's put this up into numeric features.
So things that have numbers representing how long people worked and the employees of mine has.
Categorical, which is what state or some other thing that actually has a category.
And the target variable, which is log of the production.
From here, we need to do a bit more data munging after it's all been cleaned.
We now have to do some munging to make this into a form that scikit-learn can actually predict with.
In this lesson, we'll be looking at the final data munging and the final prediction for this data.
So I've actually changed up this slightly.
So the features that we'll be looking at, these are numeric features to start with.
The average number of employees per mine and the number of labor hours total worked for that mine.
And also a categorical list.
This categorical list contains features which have a small number of string representations instead of actual numbers.
And again, the target we're looking at is the log value of the production in tons.
So one thing that I recommend you doing is taking a look at the interplay between each of the variables and the target variable.
So I'll do a quick example of this.
So let's take a look at the relationship between mine status, which is a categorical variable, and the log of the production.
I'll be doing that with this Seaborn code here, which I just executed.
And the set context has to be run twice the first time.
What this is doing is doing a violin plot.
So this is the Seaborn library SNS and it's creating this.
It's using the violin plot function.
And what we see here on the y-axis is the mine status, the five possible values, active with men working but not producing permanently abandoned, active, temporarily closed and new under construction.
And on the x-axis, we see the log of the production.
So you see that each of these mine status types corresponds to a different log of the production value of that mine.
But also the distribution has this interesting shape and it changes between these categories.
This kind of a plot is a very nice high level view of what these variables interactions look like.
I'll do just one more.
How does company type corresponds to the production?
So we see that there are three company types here, independent producer, operating subsidiary and contractor, and each of those corresponds to a very different distribution.
So you can do this for all of the variables and I recommend doing that, especially before and getting a sense of what the data actually looks like.
But for us, we just look at this company type a little bit more closely.
We do a DF company type.unique.
We return all the unique values.
Of course, we see the three that we see in the plot above, an abandoned producer operator, operating subsidiary and contractor.
The scikit-learn functions don't take in these strings as separate category variables.
We actually have to encode this ourselves.
Now, one way to encode this would be to do something like assign independent producer to be one, operating subsidiary to be two and contractor to be three.
And that would work except that we are then implicitly telling, let's say the scikit-learn random forest function that three is greater than two, which is also greater than one.
And there's an implicit ordering there and it might start to try to cut the features in a way that doesn't make sense.
A more safe way to do this is to actually create what's called dummy variables.
Pandas has a built in dummy variable function.
So we do PD dot get dummies on the data frame with just we're looking at the single column of company type.
And I'm taking a sample of 50 so that we get a mix of types because it's actually ordered in this data set and just taking a look at the top 10.
So I'm going to run this a couple of times.
This sample will actually resample every time I run it.
So what we see here is the contractor independent and operating subsidiary.
This MSHA ID corresponds to an independent producer operator because it has one in that column and zeros and even the other columns.
And if you go down to this 4407123 ID, it is an operating subsidiary company and it has zeros in the rest of the column.
So this is what the get dummies function does with pandas.
Now what we want to do is actually turn each of the categorical variables that we're looking at into dummy variables.
And then we'll actually learn to drop one of the variables to avoid the dummy variable trap.
We're then going to concat the data frames together.
So we're taking the data frame and the temporary data frame together and access equals one means it will add it as columns to the existing data frames.
And we will then drop the drop variable from the data frame and call that to list function on the columns of the temporary data frame so that we have a final list of what the dummy categories look like.
Let's run that real fast completes very quickly.
We see that there are 29 mine states 164 mine counties.
So this might be a little bit high.
We might have to come back and look at that the mine status.
There's five mine type three company type three and so on.
And the actual value of the tummy variables themselves.
Let's take a look at say the first 10.
See mine state Alabama mine state Alaska and so on.
So these are the different state variables that have been created.
Let's actually start to build a model.
So we'll say.
So I created this as a markdown by typing escape to make me into select mode instead of insert mode and typing M M for markdown.
You can also go up here and click it.
So if I could go back to code, this is simply commented out Python code as far as the notebook is concerned.
We actually want this to be marked down.
So we click markdown and you can see it pre rendered before we actually execute the cell.
And it looks like this nice bold font.
We're going to need to import a couple of things from scikit-learn itself.
So we're going to say from scikit-learn dot cross validation.
So this is the sub module of scikit-learn.
We're going to import the test train split function, which is labeled here.
And we're also going to use a random force regressor as our algorithm.
Loading that in and look at total length of the dummy categoricals is 213.
The train and test is going to be the names of the data frames.
That's going to be split by this test train split function.
The function takes in our data frame and you tell it how large you'd like the test size to be.
So in this case, we're going to have a 30% of the data frame is going to be the holdout set.
And the nice thing about this function is that we actually retain the data frame structure of these variables.
Scikit-learn likes to think in terms of native NumPy arrays,
but many of the features can actually read in a pandas data frame as well.
And the utility of having a pandas data frame around just makes it really nice to keep it to stay in data frames as long as you can.
So we can actually do it the whole way through. So that's really nice.
Our train is a data frame. Our test is a data frame and they've been split from the data frame that contains all of our data.
So now we're going to create a random forest and I would like to run these separately.
So I'm going to split this cell here by typing control shift minus splits the cells into two and I will execute this one.
This says RF is an instantiation of this random forest regressor, which we imported above.
And there's two things we're going to initialize it with number of estimators is 100.
This is a number of trees that we're going to be building a random forest out of and whether or not we're going to be using the out of bag score, which we are in this case.
So we have an RF model and we'd like to fit on this by giving it X comma Y and sample equals non as default.
So the X value is the design matrix. The Y is the target variable.
So in our case, we're going to do the train data frame and we're going to give it all of the features, which is just those two average employees and the total labor as well as the dummy categoricals.
Now, these two things together is just adding them together creates a large Python list.
We can see the top two things up here at the top, average employees and labor hours and then everything else is dummy categoricals.
We then run the fit method on the random forest by giving it the design matrix of train features plus dummy categoricals and the target, which is train just selected on the target variable, which we defined above as log production.
So it tells us some features or gives us a little summary where it talks about the bootstrap, the cartoonist's mean squared error, various other things here.
So this is all the variables that you can change very easily if you'd like to actually tweak this for your own problems.
So let's take a look at how well this does and we're going to do this by giving a seaborne plot again, a regression plot.
But instead of the train, we're going to be using the test data frame.
So I test the target and the regression plot here is going to be in target versus what we actually predict this to be.
So the actual is along the x-axis here. This is what the actual production is.
And the y-axis is the predicted value. I can actually add that in. I think it should be there.
But there's always a predicted production. So predicted production is on the y-axis and the actual production is along the x-axis.
So perfectly calibrated and perfectly predictive thing. Everything would line along this one to one ratio line here.
We see that there's some scatter around it, but it actually looks like it's a pretty good overall predictor of the actual production.
We'd like to actually see how good is this fit rather than just look at a plot and say, oh, it looks pretty good.
So let's import a few of the test metrics that we can actually look at.
So we can say, we can import explained variance score, the R2 scored, and the mean squared error.
So the way these functions work, they always take in the true and then they take in the predicted.
So this is going to be test target and then the predicted test target.
And actually, I think this way of writing it is a little bit too verbose.
So I'm going to call it predicted equals this and I'm going to say predicted here.
So the R squared score is 0.88. Explained variance score is 0.88 as well.
The mean squared error is 0.54.
And now, because this is a random forest, we actually have the feature importance of the model.
And I don't know of a good way that's naturally given by scikit-learn to actually report this,
but here's a little bit of code that I have written to make it so that I can actually read this
in a way that I actually think is useful.
So I'm going to create a new pandas data frame called rf underscore importances,
which actually takes out the features and the importances from the fit model.
And I'm going to look at that, the top 20 here.
All of the importances of every variable we give it to in total adds up to one.
So we can think of this as fractional importance in terms of what the random forest has decided
is going to be discriminative in giving us a final regression score.
So of utmost importance is the labor hours and then average employees is down from there.
The mine type being surface is predictive.
The mine county being campel and coal supply region powder river basin is apparently moderately predictive.
And then it goes down from there.
So this is just the first 20.
And we have not only a final fit with a nice plot, we also have some diagnostics and metrics,
as well as a list of what's important.
In this video, I'll be showing you how to take a development lab notebook and turn it into a deliverable notebook.
So let's go into our directory and we go to the develop folder, clicking that we navigate into that folder
and we see we had our first look notebook and then this cold prediction notebook.
And what we'd like to do is make a copy of this notebook.
So you can select it by clicking this checkbox here and clicking duplicate.
When we do that, we have a second copy of this, which is added to the end of the name copy one.
Now this file should exist in this directory and we see it here.
Copy one.
Because it's going to be a deliverable notebook, we should actually move this into the delivery folder.
So let's move 2015 cold predict copy into the deliver directory, go over to the delivery directory
and let's navigate there with the notebook server itself.
Let's open this up.
Okay, so let's first give this a title that we think is an appropriate title.
And because it's going to be a deliverable notebook, it shouldn't start with a date.
So it should start with something like cold prediction of production.
So we have a new name for this, you can save this, and I'm going to toggle that header bar.
So I have a little bit more space and I'm going to toggle this toolbar as well because I'll mostly be using keyboard shortcuts.
So at this stage, we have this long notebook that went through and it's a complete copy of our lab notebook style.
So we can delete things here pretty freely and just focus on the main story that you'd like to tell to either your teammates
or your manager or whoever is going to be consuming this.
So keep in mind with your audience what you think they would like to see and cut out the extraneous stuff
and adding in as much text as you think is useful.
And in that keyboard shortcuts, especially are going to be make your life a lot easier and make this whole process really fast.
All right, so let's just go through this.
And initially what I'd like to do is give a good title and you can just call it cold production in mines 2013, let's say.
And so we have our first setup here and you can also give a little abstract.
So you can say we did a lot of analysis came to some interesting conclusions.
Now, of course, fill that out with more verbiage as you see fit.
Keeping the code in this notebook is useful so that someone else looking down the road can actually reproduce all the key results that you think you can find.
Now, this isn't always possible, but as far as it is possible, I recommend trying to do it.
So trying to keep the imports neat and tidy, keep only the imports that are required and remove the ones that are extraneous.
I think we actually use all of these.
I would recommend keeping these magic imports on their own line at the top.
So having matplotlib inline at the top, that is good.
Put a space between that.
The Pepe convention is to have one of the standard libraries like import string, let's say.
That would be next and any of the other ones here and then another blank line before third party libraries, which is what these are.
And finally, we have an actual plotting change that we make with this SNS command here.
So we execute that cell and make sure everything is making sense.
Yes, we see this warning.
We've seen this before, so we're not too worried about it.
Now, from here on out, we should be making decisions about whether this actually improves the story for the person reading this or if it becomes just tedious.
And when you have data that's being imported and it's changed from the raw data, there's this clean data set here.
I think it needs to have some extra commentary around it so that people know what's going on.
So I might say I might give it a description about where exactly it is in this repo.
And let's just type an LS here.
The name of the notebook is data underscore cleaning.
So we will say the same thing double click drag over command C to copy that command V to paste.
And this LS command, which is handy, we can be deleted.
So typing escape to get out of the insert mode so that the cell is now surrounded by a gray box and then typing D twice, delete that cell.
And in this cell, we are starting to write some markdown.
We can tell it's marked down because it's just a text for people to look at.
But also we've put a double header marking too.
So let's just change this cell type to be marked down.
So we're currently in a code cell.
We can change it to be marked down by typing M.
And as soon as you type M, switches into markdown and gives you a preview of what this will look like when you render it.
So let's render it real fast, shift, enter.
And we see that this is indeed bolded.
This two pound signs or hash signs means it's a H2 heading.
So this is H1.
This is H2 and it keeps getting smaller as you go down.
So in this case, I think clean data just deserves a second level heading.
We said we clean this data in the notebook stored in this.
So deliver slash data cleaning Ipynb.
So I've told people where this cleaned data file actually sits.
And we actually know the exact steps that went through to take it from the raw data into this clean data, which we've pointed to here.
This head is actually quite a bit of text, even though it should be the top five lines.
So if we're going to include something here to make sure that the data is read in correctly, we might select a few columns that we think are useful.
So in this case, maybe we have year and maybe mine name.
And so we read in just the heading with those two columns.
Okay, just to give people a flavor of what's in that data frame.
This length, we don't need to worry about this column thing.
We don't need to worry about so we delete with 2Ds.
Now, consider the different plots that you have included.
Is this something that tells a story?
If so, leave it in and clean it up so that the axes and the colors all look right.
If not, you can go ahead and just delete it.
So I think this is deleteable, also deleteable and finally deleteable.
Okay.
So we get to the point where we're predicting the production of coal mines.
Again, we're just looking at what the columns are.
We don't need this.
Don't need to know what unique year it is.
So this is required code.
So we need to leave this in and again, clean it up if it needs to be broken up into different cells or if you think it needs to be changing some other way.
So let's delete a few of these empty ones.
And let's say we want to like to keep this.
Let's decide one of these violin plots to keep.
So let's keep the second one.
So I'm going to delete this one.
And to save this, I will say plt.save fig and using tab complete and it'll help us know what proper structure to put this in here.
And as I said before, I like to give the same name beginning of the figure that the notebook itself has.
So in this case, it starts with coal prediction as the starting of this notebook name.
So that looking at this figures folder separately later on, someone knows which notebook it came from.
And then what it's actually being plotted here.
So we have company type versus log of production.
So company type versus log production.
Again, we get a warning, but this should work out just fine.
Let's run this a second time to make sure everything.
So that looks better running at the second time with the set context actually lets the font sizes get to a nice reasonable size.
Okay, so we are saving this output.
We think it's useful for our story.
We again don't need this or looking at this.
So just typing dd to delete these cells.
We need to create the dummy categoricals.
This is required for our analysis.
We don't necessarily need to actually print the categoricals each time.
So let's run this comment out that line and just double check that that is the same answer as before.
Okay, let's delete that.
And we've made a note here about avoiding dummy variable trap.
You might decide that that needs to be elevated from a comment and to mark down cell above it.
In my case, let's leave it as a comment here.
And we don't need to actually look at the categoricals for the final report.
So let's just delete that.
Build our model.
Let's call it a little bit something more descriptive.
So it's going to be a random forest regressor.
And we should always put all of the imports all the way at the top of the notebook.
And so let's move this to the top.
But first let's combine a few of the other imports.
I think I have a few more imports down here. I do.
So let's move this.
I'm going to turn on the toolbar and move this up so that it's next to the previous one.
I'll scroll back down and see if I can find another import.
It looks like it looks like it should be everything.
A keyboard shortcut that I find that I'm using all the time and really saves me time is knowing how to merge and split cells with keyboard shortcuts.
Knowing this will save you tons of time with moving your mouse around.
So we currently have input cell 30 selected.
If we type shift and hold it down and then type K, we will now select the cell above it.
We can select as many cells like this as we'd like or unselected by typing J to go back down.
Also, if we go J from here, we can select down from the current cell that's selected.
But let's go up shift K.
We have selected two cells to merge this.
You type shift M.
So we've now merged those two cells together.
Again, you can do that for 10, 20 cells.
Or you can easily split them again.
As I've said multiple times, control shift minus splits in part, escape, shift K, shift M, merges them back together again.
So this needs to go at the top of the notebook.
So I will put this at the top by typing this up arrow.
So bear with me for a second.
And we need to merge these two cells and do some recombination.
So shift K, shift M, type return to get a cursor in the cell.
And we're importing things from SK Learn, which should go after C-Born, but before this set command.
Execute that.
And everything looks good again.
Let's scroll back down to where we've made our progress.
Down to here, we don't need the length of our dummy categoricals.
We do need to test train, split our data.
Let's merge these two cells by typing shift J, shift M.
And let's just leave that middle line.
Execute, shift enter.
And look at our final plot here.
And this looks like a reasonable good plot.
Thing looks nice.
Let's save it out again into the figures directory.
Let's call this coal production RF prediction.
Great.
So we've now saved this out.
And we can do our various scores that we'd like to do.
We're going to be printing out this output for consumption.
We should make this look a little bit prettier.
So let's just do this first one.
And let's combine these two cells.
So now we have the R squared score and our mean squared error scores.
And finally, our random forest importances.
And let's just look at the top five.
The top five are labor hours all the way down.
Cool.
So we've done a lot of rearranging of the code.
So at this point, I think it's crucial to restart the kernel
and try to run the entire notebook again.
If you have some process that actually takes a very long time,
you can decide not to do that.
But this, you'd have to take a little bit more care into making sure
that each piece runs correctly.
But in this case, this entire analysis runs very quickly.
So we have no problem clearing all outputs and restarting.
And clicking cell run all should run every single cell.
If we've deleted some piece of code that was necessary,
we'll have an error and we have to go back and correct that.
Let's go through all the way down to the bottom thing was actually done.
If there was an error, so let's say it would stop at this fifth cell here.
It would have an error printed out here and nothing else would be executed
below that when you do this run all cells.
That's a good way of identifying where the error happened.
We don't have an error, thankfully, so that's good.
We do have something that is somewhat annoying to me that this has to be run twice.
As we can tell, we run this a second time when we get our font gets bigger.
So I think I know what happened.
I set the context after I set the figure.
So I'm going to re-align the order of these two pieces of code, save this, restart the kernel,
clear all outputs, cell, run all.
And now we see that the font size is the correct size and we have run all the way to the bottom.
In each time we run this, do note that we are overwriting these figure files,
which is what we were hoping to do, but also keep track that is what you indeed want to do when you're running this.
I guess a good thing to add at the end, of course, would be some sort of conclusion,
so we can just add a conclusion statement.
Okay, so a detailed and amazing conclusion goes there.
So we're done with this.
We will close and halt and we need to submit this to GitHub.
So we can do a get status back at our terminal.
We've modified a figure, we have added a figure, and we've created a new file.
So let's add those.
Type get status to make sure we know what we're doing.
We are adding two new files, we're modifying another file.
This looks good.
So get commit, get push, origin, Jonathan prediction production.
And this should be sent up to GitHub and everything is now up to date.
So let's go to our Git repository.
So in my case, JBWit Coal Exploration.
And there's a new branch which we can click on.
And if we click on the deliver, we should be able to see our Coal Prediction Production notebook,
including all of the code and everything else in here.
In this video, we'll be talking about how to do a pull request and how to merge this back into a final branch
so that team members can review it and check off on it.
All right, so we last left us.
We had just put in our deliverable notebook that talks about the Coal Prediction Production.
And so at this point, after pushing it to master, we have this branch.
You go back to the home directory under your username and whatever you've named your data science project.
You can actually see this button here called new pull request.
And I like to switch to the branch that I'm going to generate the pull request from.
So this is all predicated on using GitHub as your repository of choice.
So after you click new pull request, it'll ask you to do one last step here where it'll say you're creating a new pull request
by comparing changes across two branches.
You're going to be taking stuff from this Jonathan Predict Production branch and putting it into master.
And GitHub does this nice thing where it says it's able to be merged, which means that if it's approved,
it can just be approved at the single button click.
That's always nice.
So give your commit an extra bit of detail here.
So say something like final review.
And then if you want to leave a few more comments, create pull request.
So now what this does is it creates a pull request and lets you see the various commits that have happened in this branch
and allows a person who can possibly merge this to review the pull request.
So a person coming into this would see who is not me, for example,
would look at the pull request and see that there is one open pull request and it was open 25 seconds ago by me.
So if you click on this, then you'll see the comment here.
Please check the figures especially.
They're going to be put into a slideshow.
Okay, so this must be pretty important.
And so I'll take a look at the different files that were committed.
So I'll click to the files changed.
I see that we have a couple notebooks and we have a couple of figures.
So let's take a look at, let's say this current figure here.
This one was added.
Let's say we want to change that color.
So in the pull request, you can actually make changes.
And this is where you actually wanted to be doing this.
You click in the conversation part of the pull request, say, I need a few changes.
Add a comment.
Now, of course, I'm commenting on my own pull request.
Normally what happens is you make a pull request and your team members or your manager will be actually the one reviewing the pull request.
But in this case, just for demonstration purposes, I'm both the submitter and the reviewer just so that it's easy to see what needs to happen.
So added a comment.
I need a few changes.
Please change the figure to be green.
Okay.
Now that we go back to our terminal, we see that still on the Jonathan prediction production branch.
So we'll need to make some changes to the pull request.
So this is actually pretty simple.
So I'm going to switch tabs back to our deliverer directory that is running under the Jupyter notebook server.
And so let's go into this cold predict production and make the requisite changes.
Now we'll have to actually shift return and work our way through this so that everything is loaded into the namespace.
So that one's probably the one that should stay the same.
We get down to this one here where sure enough, the figure itself is printing something that's blue.
We want to change this color to be green.
Okay.
In this plot, we will actually make the color equal to green.
C is not what it takes as a thing.
So we'll see if color works.
And color is indeed the keyword.
Okay.
So changing the color to be green, the figure is now green.
And we have overwritten that figure file.
So cold production RF prediction is now a green plot rather than blue.
And so we can want to redo everything just to make sure that you haven't made any catastrophic changes.
You can do this one more time.
Takes just a few seconds to go through the entire pipeline and save this file.
Close and halt.
Go back to your terminal.
Get status.
Two things have been changed.
And that's as we expect.
They changed the notebook itself that created this figure and the figure itself.
So let's add those two files.
Those two files have been modified.
So we then get push origin your branch name and it's now updated on GitHub.
The nice thing about how GitHub handles these pull requests as a tab back to this Chrome tab, this commit is already added now.
You actually can see the commit that was done here.
And if you click on that commit, you get to see that things that were changed.
So a few things were changed in the IPYNB, which is not shown partly because the actual changes in the notebook don't look so great.
But the change in the figures has been changed.
So this figure, the blue one was deleted and the one on the right, the green one was added.
So this is one of the reasons that changing it in the notebook, which it actually did.
So it changed the embedded figure in the notebook.
It's hard to see the differences there.
This is why I advocate creating these figures in a separate folder and a separate PNG file for each of them.
So you see the diffs and the figures if you have feedback on the output.
Now as an extra piece of sugar or something nice that GitHub has given us, there's this two side by side approach where you can see what was deleted and see what was added.
You can also choose the swipe option where as you swipe this thing across the figure that you've just done, you can actually see the changes.
That have been made, which is turning the figure green.
Last one is onion skin where it fades from the entire thing from behind.
So this is what it currently is and previously it was blue.
You can see this.
So having this functionality is actually really nice.
And another reason why I advocate for this figures being submitted separately.
Just a final note, you saw that the points are slightly different in this swipe.
And that's because during our test train split, we were taking a random selection of points that were going to be the testing set and the training set.
So those differences, well, shouldn't matter much and they don't change the actual scatter points.
But the fit itself, as you can tell is almost completely unchanged.
This is actually a nice robustness check to look at this as well.
So once I've looked at these changes, I can now go back to this poll request branch.
So I need a few changes.
Please make the figure green.
I committed, made the figure green.
The only thing I needed to do to update this whole threat of changes was to just say get push origin branch title.
So I'll say it looks good to me plus one and a comment to it.
And then clicking merge poll request will take everything from this branch and pull it into the master branch.
So I'll say figures ready to be put into a slideshow.
So once you pull request is successfully merged and accepted, then you should delete the branch to keep these branches floating around.
So I just deleted the branch on GitHub and should now do the same thing in your local environment.
So first I'm going to check out master.
And I'll say get poll origin master to pull everything down from GitHub and all these changes have been made and say get branch minus D, Jonathan predict.
And so I've deleted the prediction branch.
And get does a final check to make sure that any of the changes that have been made on that prediction branch have been already pulled into master.
So if you just try to do this and it doesn't think it's been fully merged, you get an error at that point and you have to figure out what happened at that stage.
But in this video, we just overviewed the basic process of going through a poll request and how the peer review process works in a poll request.
So we saw how to merge our development branch into master after doing a poll request.
In this video, we'll start our data science project number two.
And in this project, my main focus will be to focus on various plotting and statistical libraries that I think you should know about.
All right, so to start a new data science project, let's start out by going to GitHub and signing in going up to the plus by our little icon and clicking on a new repository.
So we can call this data vis project to in this case and give it a description that says I will make it public so you can see this project as you go forward.
We'll initialize with a read me.
We will include a Python dot get ignore.
We'll add an MIT license and create the repository.
Once we've created it, go to this SSH option, click in this box, it'll select all the text by default command C copies it, go back to our terminal, say get clone and then command V to paste that URL.
All right, so let's CD into data vis projects and look at what we have here.
And we're currently on the master branch.
So first step, let's create a development branch.
And we'll call it Jonathan vis.
And let's create our normal directory structure.
So we have data deliver develop figures start with and I happen to know that I've already started a few of these notebooks.
So I'll move them from the previous location into our develop folder.
So let's look at our develop folder.
Okay, we got some stuff there.
And now that we have a new branch and we have a new directory structure and some stuff to look at.
Let's start up the Jupyter notebook server.
All right, so we see the same directories we were just looking at in the terminal.
I will now right click on this tab and pin this tab so that it goes all the way to the left and stays in place so that if I have a lot of tabs because I'm searching for a bunch of different things.
I always know where to go back to find the home server directory.
And I just find that useful to pin that tab all the way to the left.
All right, so let's take a look at some of the notebooks I've already pre populated.
What I'll do here is I only have my usual date and then my initials at the top of the page from the actual name of my notebook.
Just including here a short description which is exploratory data analysis, which is pretty long title.
So I'll do all caps EDA and that is a standard way of talking about that.
So I will view and toggle the header and toggle the toolbar just so that we have some extra space.
Remember, if you want to save it when you're in this kind of configuration, you just command S to save it.
So one more time, I'll just give you a brief overview of what I'm hoping to do here.
So this isn't to teach you how to do data science.
It's more of an exposure to the tools that I think most people haven't seen all of them or haven't seen enough of them.
And I just think these tools will allow you to do your data science much more efficiently and usefully.
I'll go over a few of these plotting and statistical packages that you might not know about.
So the first thing we have is importing matplotlib inline.
Almost all these plotting libraries uses matplotlib.
So I'll be using that for now.
And I'm importing matplotlib.pyplot as PLT, which is the standard way of doing that.
Seaborn as SNS, which is the standard way of importing Seaborn, importing pandas as PD, numpy as NP.
I'll also load in some data sets from scikit-learn and importing some stats models,
which I'll be talking about at length in a later video.
Execute this cell.
To shift return, it will execute it and go to the next cell.
If I hold down control and hit return, it will execute the cell in place,
and it won't go to the next cell, so I can continue to stay in the same cell if I hit control and return.
I've used Seaborn in other videos, but I would really like to just double emphasize how useful this is.
You can find the main library for this by Google searching Seaborn, and Seaborn Python should do it.
And the top result is the statistical data visualization library here.
This is what you should see, something like this, unless he's updated the page.
And this website has a lot of really good information on it.
The documentation is excellent. The features with these different tutorials is also excellent.
These images that you can click on here will show you different capability, the tutorial and the gallery.
If you click on gallery, you get to see many different visualization types that Seaborn makes really easy,
especially like Heatmap, that's a nice one.
Look through the example gallery.
If you have some data and you have some sense that you should be able to visualize it in a way,
see if Seaborn has a response to that.
So let's go back to our notebook and load in some data.
So Seaborn SNS has data sets that you can load in by default.
We will load in the Titanic data set.
This is actually the data of passengers on the ill-fated Titanic,
and it has various information about them, their age, their sex, their class of ticket,
so first class, second class, third class, and it talks about whether or not they survived the crash.
So doing a factor plot like this where you set this G object to be equal to this factor plot
and then modify the G label like this, this is modified from a Seaborn example,
commenting out this hue equals sex line, and I'll talk about that in a second.
But I will shift return and execute this cell.
What you see here is the survival probability against the class of passengers on Titanic held.
You can see that first class had by far the best survival probability,
followed by second, followed finally by third class.
So this is a very nice high-level summary of the data that underlies this.
Some of the nice things about Seaborn is that you can actually give it dimensions to also give you the same plot.
So let's uncomment this hue equals sex line and see what that does.
So what you see here is each of these classes is now been split out by sex.
So male and female, survivability for first class.
You can tell the very high difference in probability for surviving in each of those, whether you're male or female in each of the classes.
So this tells you a more rich and deeper story of the underlying data set than the previous plot.
And you can see the first, second, third class, all of the different responses here.
So this is just one aspect of Seaborn.
I recommend getting to know it and use it as much as you can.
And that's going to be all for this video.
We've set up in this video a new Git repository.
We've started a new development branch.
We have our directory structure set up as we like to do it for our data science projects.
And we've taken a look at the Seaborn visualization library.
In this video, we'll continue to look at some visualization methods and techniques.
So let's go on to exploratory data analysis two.
Again, it starts off the same way with Matplotlib inline and the various other things being imported.
This warning message, which we can ignore for now.
So we will load in this Boston data from the scikit-learn data sets.
And we will first of all print what the data dictionary describes it as.
The way this load Boston gets imported, I'm calling it a data frame dictionary and just calling this description key.
So let's toggle the top header and the top toolbar to give us some extra space.
And we see that this is the Boston house prices data set.
Now it's worth reading through this data set and knowing what each of these attributes actually means
because if we're doing a deep data science project, it's really important to know the attributes,
especially if there's only 13 of them.
But what the main takeaway will be trying to predict the median value of the house
and by looking at the 13 categories that predict this house, I mean 506 total instances of this data set.
The different attributes are crime, we've written as CRIM, all caps,
zone or the proportion of residential land zone for lots over 25,000 square feet.
Indus, which is a proportion of non-retail business acres per town.
A dummy variable where if you're next to the Charles River, then you're equaling to one, otherwise you're zero.
The nitric oxide's concentration in parts per 10 million.
The average number of rooms per dwelling.
The proportion of owner occupied units built prior to 1940, which is age,
weighted distances to five Boston employment centers, distance.
Rad is index of accessibility to radial highways.
Tax, the full value property tax per $10,000.
People to teacher ratio by town.
The B, which is the formula that says the BK is the proportion of blacks by town L stat,
which is percentage of lower status of the population and median value.
The thing we are tending to be predicting, which is median value of the owner occupied home in terms of 1000s.
This is the information that the data comes from.
So it's from Harrison and Rubenfeld.
And this is all the information about exactly where it was taken from the stat lab library maintained at Carnegie Mellon University.
So this data dictionary as it comes from scikit-learn is not in my favorite format.
It's this weird data dictionary.
If we actually say type on this, it'll be this weird like data set bunch.
So instead of using it in the form that it's given to us,
I like to convert this into a pandas data frame because those in my view are much easier to use.
So we'll create a data frame called features.
We'll create a data frame called target.
Now features will take the DF underscore dict, which is the, the scikit-learn bunch thing.
And the dot data element and assign the columns to this data frame to be the feature names.
We'll also do this with target.
So we'll do this with another, create another pandas dot data frame to create the data frame.
And then it'll be this DF dict dot target.
So run this and we can look at the head of the features by doing dot head on it.
So here are the different values of the different features for the first five elements of our data set.
The crime number here, zone, the industry.
Are you close to the Charles river, the nitrous oxide, average number of rooms, the age,
all the different features that we're reading about before.
For to look at the target, we would see that it's a single element or single column data frame.
So what we'll like to do is actually for most of our visualization,
we will like to put these two things together side by side.
Well, we can use concat for that pandas dot concat.
We give it a list of the data frames you'd like to concatenate together,
and we have to tell it which axis that we would like to use.
Now, I'm sure there's some very useful mnemonic that will tell us the right way to do it every time,
but I prefer to not trust that I remembered it correctly, but always test that I have it right.
So if we start out with axis equals zero and look at the head,
we will see that it's trying to combine it in a way that they're stacked on top of each other.
And there's two ways to know this.
One is that everything has a value except for medv, which is the target data frame.
All of them have nans, and if we were to look at the tail,
we will see that everything else has nans and medv has values.
That's one way to know that we've done it wrong.
So this is trying to do some sort of concatenating the two data frames vertically.
And if we do axis equals one, we will see that we've put them side by side,
which is what we actually want in the set of the head.
We will see that all of them are here, including medv being the very final column in this data frame.
So we now have a new data frame called df.
It contains the target and the feature variables underneath it.
Now to give you a sense of the data underneath it,
there's many different ways you can slice and dice this.
One very simple quick way to start with is to iterate over all of the columns of the data frame
and to print both the column name and the number of unique values in that column.
For column in df, the data frame dot columns,
print the column name and df of the column, the number of unique values.
This n unique is a method you can call on a data frame.
So there are 504 unique values in crime,
and there's two total unique values in chance, which is a boolean value.
Makes sense, we'd expect that.
Some of them are pretty low.
So our ad, for example, is at nine.
Some of these have many values and they're continuous values.
Other of them have smaller numbers of possible values.
You can see rad here is this kind of numbers here.
One thing you might not know is that pandas not only has fantastic data frame support,
but also has some very useful plotting tools.
So in this case, we will be importing a thing called scatter matrix from pandas,
and this can be done in a couple libraries as well,
but let's just look at the pandas version of this.
Recreating a figure with subplots in piplot,
making a large figure of 12 by 12 fig size,
and we're going to call it on this data frame with some see-through value of alpha,
and the diagonal will be a KDE,
which is this kernel density estimation plot that we see here.
Again, we see a warning that we can safely ignore,
but this is a very information dense plot.
There's no way to go over all of it in this video as we look at it,
but this, if you have your own data set,
will give you a lot of things to look at.
What is being plotted here on the x-axis and the y-axis
is every possible pair of the two columns in this data frame,
which is why it took a while to actually plot this.
Along the diagonal, this KDE plot,
it's showing interactions with itself
or basically the histogram of that variable itself,
so this is what medv looks like.
It's just this histogram here.
On the diagonal, it's just a histogram of the values of that variable.
Everything else is going to be what the response from this variable looks like
with every other variable on the x-axis.
You can see a number of really nice trends here.
You can see some kind of this u-shaped trend here.
We see something that's basically a straight line,
which means there's not much information there at all.
That's from the Boolean value.
We can see some of these have very fuzzy relationships
where it's not really showing anything very interesting,
but spending some time looking at plots like this,
getting to know your data set is a vital part of data science,
and I highly recommend looking at this.
If you have far too many columns to look at it in one,
I would say this is probably too many.
If you have even more than this, though,
you can take subsets of this and plot this with the same command,
but you would be giving it a list inside of double brackets
of feature one, feature two, and so on,
and this will plot just those features against each other.
There's a downside of that,
is that you're not getting all of the interaction terms,
but if it's a trade-off between possible to view in one screen
or not look at it at all, I recommend that.
Okay, in the last video,
we last looked at this scatter plot functionality within Pandas.
In this video, we're going to continue taking a look at this data
and some of the plotting functionality
that's built into the Pandas library itself.
Just as a brief overview, again,
this scatter plot gives you a very nice, fast way
of looking at all of the interactions
between the terms in your data frame.
If you suspect that there might be something interesting going on
with, let's say, rad, we see something happening here,
or this diagonal term for rad,
the intersection of rad and rad on the X and Y axis.
You see a histogram plot or a KDE plot
that shows a very bimodal distribution.
So you can take a deeper look into that
and see what it looks like by selecting that column
by saying df of rad.hist,
and we will see this bimodal shape really appear again.
So it's really values that are greater than 20
and then a bunch of different values that are around 10 and lower.
You can also, of course, select it if you have a nice column name.
In other words, there's no spaces or any other characters
in that column name.
You do the same exact thing by doing df.rad.hist.
See the same exact plot.
When you see a feature like this,
in this case, it might not make sense,
but if you have the thought that,
you know what, let's actually consider this as two separate groups.
This bimodal characteristic should actually be characterized as
really a high group and a low group.
One way to do this is to apply a lambda function
which will create a boolean value of these values.
So everything down here gets one flag of the low group
and everything up here gets the high group.
And so we will build up this command below
by getting some intuition here.
So let's grab our data frame like this.
This apply function is a method that goes to the column
that you've selected in your data frame
and there's a number of ways you can actually call this apply.
You give it a function and the default axis is zero
and you can do it in various other ways
so you can access equals one.
But in this case, most of the time,
you'll end up doing a lambda function
which is an anonymous function.
It's like you define a function in Python,
but you don't give it a name.
You're giving it via this apply method
every value in the RAD column
and you're saying for each of those values in that column,
is it greater than, say, 15?
So 15 is clearly going to split us into the low and the high group.
And let's just take a look at the first few values of that.
We'll head on that to give us the first values
and we see that is this X value greater than 15
and was false, false, false, false, false.
And if you want to look at just what that head value looks like
without that boolean, we see that it's 1, 1, 2, 2, 3, 3.
So everything here is indeed less than 15.
So we have this function call
which will return a boolean series, false.
And what we'd like to do is say
we want a new column in this data frame.
We're not going to overwrite this column,
but we're going to give a new data frame
that we're going to call radian underscore boole
because we want to have a nice descriptive name
of where it came from.
And the way you create a new column in a pandas data frame
is you give a column that doesn't quite exist yet
or doesn't exist yet in the data frame
and assign it equaling to something else.
So in this case, we have this RAD dot apply lambda
greater than or equal to 15.
And I'm just adding this as type boole
just to give you a sense that
you automatically get encoded into a type of boole,
which we see right here.
The D type is boolean.
You can force it by doing this as type.
There's other times when this is useful as well.
So I'll just leave it in there as kind of a best practices
or a hint for future ways
if you're trying to do something similar
and having some problems with it.
So we've just created a new column in the data frame
of rad underscore boole.
And if we look at what the type of this
single value is, this i location of 0
is a boolean.
There's a histogram on that now
that we've created this new column.
And we see this perfect bimolality of 0 and 1.
That's one way if you have
different features that you want to create.
It's very flexible to say
if it's greater than 15, give it as boole.
You can also do something if you had a trimold
or so three different groups
or various other ways of slicing this.
Any function you can think of that can be written down in Python.
You can then use to filter out the columns
and I recommend creating new columns,
but sometimes you can overwrite columns
to make more sense.
So after doing this, we have
another seaborne plot that's called a pair plot.
And let's execute this
and then explain what's happening here.
This is very similar
to what's happening above in the scatter plot
where we're having the same
x value versus y value
and where they intersect.
So this medium value here is the same
intersection of medium value on the x and y axis.
Instead of a KDE
or a kernel density estimation
which is that line, we did it with a histogram
and that is a flag given right here.
I guess it's just the default value.
It's under dyag kind equals
hist and if we did
KDE, it would give us the KDE
plot as before.
There's one difference here though
where we've given an extra character of
hue. So there is a boolean
value which is are you near the Charles river
and this is similar to splitting
the Titanic data set into male and female
for each class.
It says give us the pairwise
interactions between these variables
and I just picked four of them.
For each of these though, I would like to see
the differences whether you're close to
this river split up by
a different color. So we have this hue
value can take a zero or a one and we see
if there's possibly different distributions
behavior conditioned on
whether it's actually close to the river.
So this gives you an extra dimension
of interaction and
interpretability so you can see
that there's a behavior but it only exists
if there's let's say the green dots
had a nice tight relationship here
and the blue dots were all kind of all vague
and all over the place and so if you looked at this
without splitting by this boolean
value, you might say oh there's not much
of a relationship here, it turns out
that this underlying feature could have been
the really important thing. Now I don't actually
see anything that jumps out at me in this case
but having this availability
is something that's worth noting.
So we'll do sms.kde
plot and it'll be df.nox.
So we're seeing
here, it's not a histogram, it's a
kernel density estimation of the distribution
of this underlying feature here.
So this is like a histogram but it's more of a
smoothed out version of that.
This is what happens if you give this kde
plot method in seaborne
single column of values. If you gave it
two values, let's give kde
plot the boolean value of rad
versus the nox value which we just plotted
above and we'll get a two-dimensional
plot which shows the distribution
of these two values together.
So radian is a boolean value
when split it's in the x and y
and you can see that if boolean is true
then the nox values are actually conditioned
higher, if it's zero
then it's conditioned lower with a little bit
of data points up here in the upper one.
So giving two dimensions
to a kde plot and you get this 2d
map which shows you some contra plots,
some really nice things. One final
thing for the pandas plotting
thing is a thing called Andrew's curves.
Now, I haven't used
them much myself but in Wikipedia
it has this as their answer of what
Andrew's plots are. It's a way
apparently to visualize structured high-dimensional
data. They show it with the iris data set
and the iris data set is
the ubiquitous data set from
Fisher way back in the day
and if we import this we can take a look
at a specific value
of a data frame so let's look at this
whether this boolean value has
much structure to it and it doesn't look
like it but perhaps this nox
value does and
looks like there's too much to that one.
Let's go with rad which is only a nine
values for that, yeah. So you can try to see
if there's clustering of behavior. Now the
actual numbers here I think aren't
so easy to read but the fact
that this should give you a sense of
if there's different behaviors going on.
Too many data points overlying each other I will do
a sample like this and
a sample is another built-in function of data frames
where you can say give me only a hundred values
and then do the same exact plot and
it'll pick out randomly a hundred
values from this data frame and then you're doing
the same kind of estimation here
and so at this point you might say hey this
value of 24 for the data
framework rad that looks like it's having
fundamentally different behavior than the other
values which seem to be clustered together. I
don't know that this actually tells us much in this case but
it's another piece of functionality that I think
is worth knowing about.
I'll go through the last few bits
here and just talk about them
really quickly. So here's another KDE
plot of this medium
value for the houses which is what we've
seen before but this is going to be the target
like how much the price of the house
is actually going to be sold for and we
can add to that by saying we want
to also see what's called this rug being
true so instead of doing
a KDE plot we can give a distribution
plot and add the fringe
kind of rug thing at the bottom
which is the actual density
of points at these different values. So you
can see that it is actually very dense here
as you go across these values.
So sometimes if you've chosen a kernel that's
too wide or too narrow
for your underlying dataset seeing the rug
along the bottom here it gives you extra clues
into what's going on.
And one last look here at
two variables that might actually be
more useful for looking at relationships
the median value
versus the L stat and you can see
that there's this kind of banana shaped
curve here going on in the relationships.
Okay so that's going to be it for
this video. What we did in this video
is we showed a number of different visualization
techniques. We took a value
that had a clear bimodality of a
low and a high group and created a new
data frame column to encode that
went through also and saw various
methods of doing kernel density
estimations, scatter plots
and various other features.
In this video we'll be talking about
stats models.
Stats models is a library that you can use
that allows for a lot of statistical
machinery that can help you with your
data science work. So we'll continue
with the same Boston housing
dataset as before
which we were just looking at in the last video
and take a look at some of these
Boston housing prices.
Let me toggle this header
in this toolbar real fast. Make this
full screen so we have a little extra room to look at.
So we will load in
the scikit-learn dataset load Boston
which again has the same
attributes as we saw in the previous videos.
We will construct our
pandas data frame from this scikit-learn
dataset so that we can use
the standard tools we've learned over the
years. We've combined the features
and the target into one data frame
and here's that scatter matrix plot
we made in the previous video
as well. This is a pandas call
so with this function call we get
all of the pairwise interaction terms
for this dataset. And from this
we see a number of features that look like they have
some strong trends
with the thing we're trying to predict which is the
median value of the house. So we see
there's a trend here with this RM
and looks like there's this kind of banana shaped
L-stat curve that we talked about
at the end of the previous video. So we have
a few things that we think you might be able to combine
into some sort of model that will
predict our median value. Again
let's look at the columns and the number
of unique values for each of these.
In particular, RAD has nine values. We
previously made that a boolean. Let's actually take a
look at what the values comprise it.
So there are nine values
and these are the values
and then 24 is obviously the outlier
here. And we previously made
a boolean variable which we can do again right now.
So this will split everything from
less than 15 which means
everything up to here. 1, 2, 3,
4, 5, 6, 7, 8 will be
labeled as 0 and 24
will be labeled as 1.
Let's look at the target variable
which is this median value plot
just done as a distribution plot
with a rug at the bottom.
And so this will be the target variable.
We see some interesting structure going on here.
It's a plot again
L-stat which we identified just a second
ago versus median value.
We have again this kind of weird shaped
banana plot. This sort of
a tapering off effect of this thing.
So stats models
let's actually go and take
a look at this as a
Google search.
So stats models for Python.
The current
documentation for this sits at
statsmodels.sourceforge.net
and it has, as it says
it's a Python module that allows users
to look at data to estimate statistical
models and perform statistical tests.
It has many different modeling
choices so our options we have
linear regression, generalized linear
models and all the things listed here.
And also some nice examples
that explain more.
This is definitely a package that's geared more toward
the statistical side of data science
than the machine learning side
which is how I would classify scikit-learn.
So with that comes
a number of useful tools that if you
haven't used them it can be very powerful.
So this is where the documentation
resides. I recommend
looking at that. We
imported this at the top. So scroll up to the top
real fast. We imported
statsmodels.api as
SM which is not a typical way of
importing Python modules. This is
one of the standard ways of doing stats models.
And then there's this formula.api
from which we're going to import
ordinary least squares which is just
OLS in this case.
So let's scroll back down.
The formulas work in a way that's very similar
to R so if you've used
R before or if you've used the Python
package Patsy or various other one
what you end up writing is the
dependent variable or the
thing you're trying to predict. So in this case
the median value, this tilde
which goes as Lstat
which is this thing that we're just plotting up here. So Lstat
versus MEDV
median value. So we've given the formula in terms
of the relationship between these different variables.
You have to tell the model
where the data comes from. So we say this data frame.
When you give it this data frame it says
okay I'm going to look in this data source
DF for columns
that are named in the same way that
you've written it out in this formula here.
So we've said okay we've rewritten out
MEDV and Lstat are
actual columns and at the end
we will fit this with the dot fit
function and the end you have
a model which we've written down as
mod and running the
method dot summary tells us the output
of trying to fit this data.
So we have the results
from that. So the dependent variable is
median value. The model is
ordinary least squares method least squares
tells you a bunch of different pieces
of information that are pretty useful here.
So we have R squared, adjusted R squared
F statistics, log likelihood
AIC, the Aikike
information criteria or however you say that.
Of course the values of the coefficients and the intercepts
standard error
the 95% confidence intervals and so on.
If you're looking at this and wanting to evaluate this
model statistically you have all kinds
of things at your fingertips here to look at.
Now the
relationship between Lstat
and median value of the houses
does not look linear to me. This looks
like a weird shape here and we can actually
plot this with the we can reverse this
and see how
this kind of
tapering off of the median value
versus Lstat can be a combination
of features.
So I'm going to add an extra
term here. I'll actually take
the log of the value
and you can actually write it in this way
in this string. So you say
numpy.log or np.log
of the variable that you want to look at
and you have to wrap it in this extra
i for wrapping up
because this doesn't actually exist as a column
you have to wrap it in this i and there's other ways
you can wrap this as well. But I think
having this and this both
be in this linear model is likely to
give a much better fit than just the Lstat
by itself or even Lstat
squared which we could also do simply by just
instead of it numpy.log we do Lstat
star star squared.
So let's run this and we see
our summary comes out and we have
our R squared and AIC
and all these different various intercepts
and log values. So let's
actually compare the two. So one way to
compare it is to look at the AIC. So this
one from 3200 down
to 3100 which is
a pretty substantial decrease in the AIC so we think
this is actually a better fit statistically
although we have to look at the residuals and do
many other tests to make sure that this is actually
is a viable model. So that we're nowhere
done and like to double emphasize that
what I'm showing you here is not a
final rigorous data science
result. This is more of a sketch of
what's possible with the tools that I think
are useful. Don't be taking directly
from this lessons on how to do data science
this is more of a sketch of how the tools
should work. Let's make this a little
bit bigger so we have more room again. One way
to start to evaluate how good this
fit is is to actually look at
this graphics from the
stats models. So statsmodels.graphics has
a lot of different plotting options
and there's these
component and component plus residual plots
which is the CCPR plots
which you feed it the model
itself. Contained within this model object
is the underlying formula and so you can
tell it I want to know this one term here
the term that went with the log of
the L-stat score. How does that
look versus the residuals
of this plus the I squared
so we can see that the component
actually does a decent job at this
log-stat versus residuals plus
log-stat so this line actually
does a pretty good job of fitting this
and for some reason that I don't quite
understand it actually plots it
twice. So the same exact plot
but was just L-stat by itself so the first
term in that model
we can see a little bit wonky behavior
where it's not quite as good as the previous one
where the residuals has some extra
structure here in the low end especially. But
we can start to have various goodness of fits
and start to model out how good
our model is at capturing the underlying data.
Again it shows it twice and
again I don't know why. We can also
add more terms to this model
so previously we had L-stat
and this is the log of L-stat plus one
for the intercept. We can also
add the RM category
we can also add the Boolean
value which is whether it's in the higher
low of the RAD variable
and because it's a categorical
you can feed it to the model
with this C value and it
will properly take into account
the fact that what's in this column
should be considered a category
and it won't get you in trouble with the dummy variable trap
that I had to mention at the last time.
So if we look at this we see
a number of things including
the fact that it starts off
with the categorical variable radian
bool, the categorical
value where the default value
is false and if it's
true what the change in the coefficient
is for that value and the various other values
as well. Another thing to look at
depending on what you prefer to look at the BIC
or the AIC or log likelihood
F-statistics to compare this to the previous models
this again
has lowered the AIC substantially
so in terms of is it a better fit
or not it has some statistical
basis for saying that this is a better fit.
We still have to do a lot of work
still before we decide this is actually a reasonable
fit and all the assumptions behind
an ordinarily squares fit are holding true
but just as a first pass
we have a lot of really nice information here.
In this video
I would like to continue from the previous video
where we had just run a model
to find the intercept
and the categorical rad value.
We can also
run the same exact model as before
deciding that this rad value
instead of doing the boolean version of this
we can actually run on the entire column itself
and telling stats models that
we're actually using a categorical variable here as well.
So now we're trying to predict
the median value of the house using all of these
possible variables
where these each have a coefficient
in front of it. We run a dot
fit method on that and save it as
a model as mod
and we're going to output the summary of that fit
ran just then
and again dependent variable is this
medv variable
and we see in the output here
various goodness of fit and metrics about
how the fit actually worked out
in R squared of 0.73
and ASC of 3,040
which is a slight improvement from the previous one
meaning that encoding
the rad variable where each
value is independently
stored so since it's a categorical
this is all with a baseline of 1
which is why it doesn't appear here.
These coefficients are all based off of
comparing each of these terms with
the baseline of rad equals to 1.
If that doesn't make sense to you don't worry about it
probably won't be worried about this kind of statistical
model and if it does make sense to you
then you understand what I just said.
So anyway you get the output from this
but we actually want to see some plots
to see how good this fit
actually is because just looking at the diagnostics
and the metrics that come out from these
fits isn't enough to tell us whether we're
making a good model here. So let's
start to look at how we can assess fit
quality. One of the easy things you can do
is to look at a thing called leverage
stats models gives us a nice way to
see this and visualize this by using
the sm.graphics.influence
plot and
the plot leverage residual
squared plots. So let's take a look at these
two plots. I will first
do this one.
So what we see here is on the y-axis
studentized residuals versus the x-axis
the h-leverage
is using the cooks method for influence.
What you see on the leverage
corresponds to an outside
influence on the overall
fit for its values.
So if you see something with high residuals
and high leverage that's something that we should
possibly consider looking at that point and figuring out
what's going on at that exact point. So like
368 for example would be a candidate to be looking
at here because it has high residuals
and high leverage.
That's one way of looking at it. Another way
is to look at it through this leverage
residual squared.
And you give it simply the model object
that you just fit above. You just give it
mod and it will give normalize
residuals squared versus the leverage
again 368, 365,
372, 371
are all outliers in terms of
points that we should possibly take another look
at again that corresponds to those four
points up here. So this leverage
plots is one way of assessing
the fit and the data points to make sure
something isn't going crazy.
There's also a way of doing partial regression
and I've quoted a bit
from the documentation stats models
here says the slope of the fitted line
is that of the exogenous
in the full multiple regressions.
That's what's going on here. The individual
points can be used to assess the influence
of points on the estimated coefficient.
So let's take a look at what this means
visually. I think it's easier to see
what's happening this way. So we have
a partial regression plot and
we're evaluating the expectation value of
LSTAT given the values that we have
and same plotting against the
dependent variable, the median value that we're
trying to predict. And in this
we see that a lot of the points
are kind of in a mass right here and
the outliers are sitting here at this
very low end. And the same culprits
appear again and you can actually see the effect
that it's having on this. It's pulling the slope
up a bit. So that's with a plot partial
regression and giving various
features as you're holding
constant. You can give it the entire
model and see what that looks like.
We get an entire grid and have
a lot more involved to look
at this grid of plots.
But it's the various features here
so that the RAD variable is a feature
3 given X versus
the median value on the Y axis.
And so you can look at the
various categorical variables and how
they are being fit with the lines
and how they are interacting with the overall
fit as well as the values that clearly
are more continuous and having a
nicer time of it. So there's two ways to do
this partial regression plot and both
give you different ways of looking at this data.
Again, this is plotted twice for reasons
unknown. Finally, we have
regression. We can do this
as a plot regress exogenous.
It gives you this four panel plot
of median value
versus L-stat and
residuals versus L-stat. So this is the data
minus the fit itself. And what you're
hoping to see is noise pretty
symmetrically about this axis here.
The estimated variables
and the CCPR plots.
So we see fit versus the
actual values in this plot here
and then we can also do it versus any other
term in that model which is in this
case the natural log of the
L-stat and we get this plot here
which shows
much tighter fit to this instance.
If you've built up a model
and again I'm not saying I've built up some amazing model
at this point. This is definitely more
descriptive of how this kind of process can work.
But if you would like to build up a
model and look through
a lot of diagnostic plots and have
a true statistics, robust
package manager behind you
look into stats models and
really try to dive into this because there's a lot
of really good stuff in this.
So with that I am concluding the
second data science project
and what I really try to focus on this time
was a little bit of some more
advanced features of using
the plotting features of
pandas really taking a deep
dive into how one aspect
of the stats models library and there's
many aspects of it. So I highlighted the
ordinarily squares and how
fitting a linear model there with the statistical
analysis and
output that comes out of every fit as well as
fitting the diagnostics and doing a
quality of fit. I also spent a lot of time
on the visuals of this
diving a little bit deeper into
Seaborn and a few of the other
options there. So
just as a kind of a wrap up of this
using metplutlib and Seaborn
stats models and pandas
these data sets can be explored
and manipulated and fit and
these tools give a lot of flexibility
and exploring and analyzing
data in a notebook lets someone else
take a look at what you did through
your analysis. So if you've made
some horrendous error as you went
through that it's something that's easy to point out
and point to the plot. You said
this was a decent fit for example
this is clearly bad because of reason X
and you can point to it and circle it
and it's not just a bunch of
random files sitting in a directory somewhere.
Alright to close off this
project the last thing that remains to do is to save
this and to close it and push
it back to github so that you guys can also
look at the same data sets
and follow along yourself.
So I'm going to file
close and halt, go back to
the terminal, git status has
only this develop directory
that has any changes in it so
git add, develop
git status we have three new files
okay and I've closed down all of them
just double check the server here
everything looks to be closed and say
git commit
give a commit
message that makes sense, git push
origin, jonathan vis
which is the name of this branch
go back to github
see that we've already made a change
this we can compare and pull request
and create a pull request
I'll go ahead
and actually merge this pull request
because I've demonstrated how to do
the full pull request and peer review
aspect of it before and going back
to the datavis project what we
have here is, don't save that
so the datavis project 2
will have the
notebooks that I went through
during this project available
right there just to recap what happened
in this video finished up looking
at the plots from stats models
and finished up the second
data science project for this course
let's talk about some of the security
issues with using the jupiter
notebook as is
out of the box
the notebook only listens to requests
on locohost
this means that it ignores requests
from the internet people connecting
from the internet can't see your server
and they won't be able to connect
in order to allow them to connect
you have to explicitly configure the notebook
to listen to the correct IP
once you do anybody can access
the notebook server
the notebook server has no password by default
and permissions of the users that are connecting
are the same as the permissions of the user
who had launched the server
so this means if you launch the server
everybody who connects to the notebook
will be executing things as if they were you
the second main problem with using the notebook
is it's using an insecure line
so typically the notebook has broken
into three pieces the kernel
the web server and the client
the client is what you see in the web browser
it's the notebook as you know it
and the web server
is the thing that relays messages
from the kernel to the client
the web server communicates with the kernel
using zmq usually
the kernel and the web server exist on the same machine
the kernel is the server
that executes code
and runs requests
the line between the kernel and the web server
you don't have to worry about usually because it's on the same machine
however the line between
the web server and the client
is usually about because it's over the open internet
this means that it's
available for people to listen to
and inject messages
however
there are some setups where it makes sense
to separate the kernel onto its own machine
for example
you may have a cluster of computers running kernels
one computer running the web server
in this case
you also have to worry about the zmq communication
between the kernel and the web server
if the kernel and the web server
are not on a VPN
or in a secured network
I'd just like to note we aren't security
experts but we do have
experts in the community and they do help us
if you spot a problem
I ask you please email us
at our security mailing list
the address is security at ipython.org
once you do we'll work quickly
the openest CVE
in the next set of slides
I'll talk about how you can mitigate some of these problems
and rest assured that your notebook deployment
is as secure as it can be
in the last video
we talked about some of the limitations of running
the notebook server publicly
specifically we talked about security vulnerabilities
in this video
I'll describe to you some of the solutions provided
by the notebook software
and some of the limitations of the notebook software
first in the last video
I showed you this diagram
and told you that the communication between the web server
and client was insecure by default
the notebook actually provides support
for HTTPS
industry grade encryption
for this communication line
I'll show you how to configure this
however the notebook does not provide
support out of the box for encrypting the line
between the kernel and the web server
therefore I recommend you either run
the kernel and the web server on the same machine
if possible
or run them within a VPN
the latest version of ZMQ
does support encryption
however the notebook is not using that version
of ZMQ currently
before we secure the notebook server
we need to be able to launch it so that
people on the internet can connect to it
in the previous chapter you learned about
tradelets we can configure the notebook
to listen to all IP addresses
using tradelets
if I do jupiter
notebook
double-dash help
I can list all the configuration options
of the notebook
the second option is double-dash
IP
that allows me to change the IP
that the notebook server is listening on
just to cement the idea that this is a
traitlet I'll show you in the notebook source
where this traitlet can be found
in parentheses next to the configuration value
you see that
notebookapp.ip is listed
this means that IP is a traitlet inside
the notebook app class
so opening up the notebook
subfolder of the notebook repository
and then the notebookapp
module inside that
we should be able to find the IP trait
I'll use the search function of adam to
find IP
here's the definition of the IP trait
if you want to configure something of the application
and you don't see the option
in the help string it's a good skill
to be able to look through the source code
and see if there's a traitlet that isn't being
listed so we have
two ways to set this IP trait
we can either pass it
in at the command line like so
or we can specify
via config so it's the
new default by specifying
IP to asterisk we're telling
the server to listen to request on
all IP addresses
you may get two warnings one from
your system firewall prompting
for python to have the ability
to accept incoming network connection
this is because the notebook server is written in python
the other warning you'll see
is in your terminal output
from the notebook server itself
warning you that the server is listening
on all IP addresses and is not
using encryption or authentication
don't worry
I'll show you how to set these up
but first let's try setting IP equals asterisk
in the config
if you recall from the earlier
traitlets video the config is stored
inside the dot jupiter folder inside
my home directory
opening the folder up in adam
we see that the config files from
the earlier weekend and weekday
demonstration still exist
we go ahead and erase that here inside
the jupiter notebook config
dot py file
now recalling what the help text said
in the terminal we'll set
notebook app dot IP
equal to asterisk
go ahead and save the file
and we'll try launching the notebook
server again
this time however we won't specify
the double dash IP equals asterisk
on the command line because it's already
specified inside our config
it looks like the launch was a success
we still receive the warnings about the
server listening on all IP addresses
even though we didn't specify the IP
equals asterisk flag in the command
line this means that the line
that we added to the config file worked
as expected
in the last
video we added password security
to the notebook however
we did not encrypt the line between the
web browser and the notebook web server
this means that the notebook is vulnerable
to people eavesdropping
on the communication between it and you
or any other users of your
server in this video
we'll add HTTPS
encryption to your notebook web server
to get the notebook to start using
HTTPS all you have to
do is point it to your key file
insert file
if you don't have a key file insert file
you can generate one yourself
before I show you how to tell the
notebook to use your key file
and insert file I'll show you how to
generate one using OpenSSL
if you already have a key you can
skip this step anaconda already
comes with OpenSSL installed
however OpenSSL
frequently releases security updates
so I highly recommend that you
update to the latest version
to do so you can run
conda space update
OpenSSL
I'm currently inside my jupiter
config directory I'm going to run
OpenSSL to generate
the key insert file
I'm going to generate the
cert so it lasts for one year
to do so I'm going to pass in
365 days into the days
argument
I'm going to output both the key
and the cert file into the same file
once I run the command
an interactive wizard will start
I'll answer some of these questions
however if you want
you can skip any of the questions
just by hitting return
to accept the default value
once that is done
we'll have to configure the notebook
to use this key insert file
to do so I'm going to open up
atom inside the jupiter
configuration directory
after the shaw from the password
I'm going to create a new line
I'm going to specify the
cert file first the cert
file is a trait of the notebook app
it's important that I pass the
full path to the cert file
next I'm going to specify
the key file the key file
is a trait of the session
class
since we output it the key into the
cert file we can just specify
the same file here
now I'm going to save the config
back in the terminal
I'm going to try launching the notebook
when the notebook launches
you'll probably see this security error
from your web browser
saying that your connection is not private
and that the authority is invalid
this is because you self-generated the cert
you can get around this by having a third party
generate your cert
for now let's just click advanced
and proceed the local host
now our connection is being encrypted
if you are interested in getting a cert
that's verified by a third party
I recommend using startSSL
they'll do it for free
you can visit their website at
www.startSSL.com
the startSSL
free cert should be fine for basic
setups
the other two offer slightly more features
that are verified whereas the most
expensive gives your site a green bar
inside the address bar when the user
is connected
you can see that in the screenshot
in the side column of their website
in the last chapter
we talked about how you could deploy the notebook
securely
in this chapter we'll change gears
we'll start looking at nbviewer
before I discuss installing nbviewer
I'm going to show you what nbviewer looks like
in the wild
I'm currently on the jupiter public
deployment of nbviewer
which is accessible at
www.nbviewer.jupiter.org
nbviewer is a web application
that is used to render
static views of notebooks
online
in the back end nbviewer uses nbconvert
the application that I showed you in chapter 1
which can be used
to convert notebooks to
various static formats
nbviewer just uses nbconvert to
convert notebooks to static html
representations
nbviewer itself is a simple website
that has a title
and then an address bar
where you can paste the link to your notebook file
after pasting the link you click
go and it will render that notebook file
below that there's a showcase of notebooks
for various categories
here for example we can click
on this iRuby notebook
to see what iRuby is
this is what a rendered notebook looks like
you can see it looks quite different
than the notebook client that you're used to
it's quite a bit more bare
but it still bears some resemblance to
pieces of the interactive notebook
such as these prompts
and cell formatting
at the top there are links to
download the notebook
view the notebook on github
if it is a github hosted file
and a link to go to the top of the file
at the bottom of the page
you can see the version of nbviewer
that we're running the notebooks version
and the version of nbconvert
that nbviewer is running against
nbviewer tries to be aggressive
about caching notebooks
so you also get a status of
when the notebook was last rendered
because nbviewer is not a
user application and it's actually
a web application it's not
included with anaconda
therefore i'll have to show you how to install it
the easiest way to install
nbviewer is using docker
docker is not included with anaconda
either so i'll also have to show you how
to install that
docker is an emulation platform
it allows you to run
applications inside an
isolated environment called containers
docker containers
differ from virtual machines
in that the containers share the host os
containers can also
share dependencies with each other
this minimizes the distance
between the container
and the system hardware
which makes containers faster
and smaller to install
docker first go to docker's
website at
www.docker.com
then click on the get started
link in the top right hand corner
the instructions for getting started
are operating system dependent
because i'm running a mac i'll show you
how to get started with docker on a mac
if you're running linux or windows
this page will look a little different
for you the first step
is to install docker tools
you can click on install docker on
os 10 scroll down
to step 2
where you'll see install docker toolbox
click on that and then scroll down
click the download button
for mac if you're on os 10
once you have the toolbox
installer run it
follow the prompts in the wizard
select a hard drive to install to
enter your password
when prompted
when done click continue
then click close
now launch the docker quick start terminal
it takes a little while for it to start
the machine
once the process finishes you can run
docker space
run space hello
dash world
you should see a hello from
docker message which confirms
that your installation is working
in the last video i introduced
you to nb viewer and docker
we then installed docker on your machine
in this video
we'll install the nb viewer docker image
to get started
open the docker quick terminal
your terminal may take a while to start
once the terminal has started
pay attention to the ip address
listed in green
mine's 192.168.99.100
that is the ip address of the docker image
you'll use that ip address
to access your nb viewer
server once it's started
the first step is to download nb viewer
now i've already done this ahead of time
so mine will download fairly quick
because it will just be verifying
that i have the latest version
but the first time you run this command it may take a while
next let's try launching nb viewer
once the server starts
it should tell you the port it's
listening on
in a new web browser
go ahead and try accessing that ip address
that you remember that was in green
followed by colon
8080
if all work well
you should see nb viewer
go ahead and try to open up a notebook
once the notebook opens
go back to your terminal
you should see output from
the nb viewer server verifying
your request
without this it would be hard to tell
if you were actually running the server or not
or if you were just accessing the public nb viewer
deployment by jupiter
nb viewer has this wonderful feature
that allows you to access notebooks
on github using short URLs
to demonstrate this
i'll access a notebook that's stored as a gist
under my github account
so here's a simple notebook i created
for pydata
it's stored under my account as this gist
i'm going to just copy this URL
because
nb viewer has support for gist
i can just paste it directly in and click go
alternatively
i can use an even shorter form
which is just the gist id
to do so i'll
remove all the stuff before the last forward slash
this is my gist id
you can see nb viewer still renders it
the github public
apis have rate limiting
so if you plan on supporting this feature
it's a good idea to generate an access token
for nb viewer
doing so is relatively painless
log on to github.com
using your account
then in the top right hand corner
click view profile and more
next select your profile
click edit profile
then click
personal access tokens
in the left hand column
next click generate new token
give the token a name
and then change the scopes
that you want to use to restrict the token
when you're done
click generate token
your token will be displayed in the green bar
i've blurred a couple of the numbers of my token
for security click the copy
button to copy the token to your clipboard
now
in the terminal that's running nb viewer
hit ctrl c to stop nb viewer
now let's relaunch nb viewer
adding our new access token
to the command line
because we're running nb viewer
as the docker image
we can't specify arguments directly to nb viewer
instead
we have to set environment variables
to cause nb viewer to change its behavior
here i'm telling docker
to set the github underscore api
underscore token variable
to the token that i just copied from github
now when i try accessing nb viewer
it should be using that token
let's paste the same gist id
from earlier
now let's go to github.com
to see if the api token was used
we can see that it was just used
because github says it was used within the last day
using this token should help lift
some of the rate limits for github access
and it's also nice because it allows
github to control who's accessing
their apis
in the last video
we installed nb viewer using docker
this is great for most use cases
however sometimes it's
necessary to maintain more control over the distribution
to do this you can install
nb viewer from source
this will allow you to do two things
one it will allow you to control
what dependencies nb viewer is using
and two it will allow you
to modify nb viewer's source code directly
including installing
additional extensions
without having to recompile the docker image
the first step is to clone
the nb viewer repository
you can either clone the upstream fork
like i will do here
or you can clone your own fork
once nb viewer has finished
cloning cd into that directory
now run pip install
dash r
requirements
dev.text
next run npm space
install
then run invoke bower
this is installing the static assets
next run invoke less
which will compile the less into css
css is what styles nb viewer
i've cleared my console
now i'm going to run pip install markdown
once that finishes
i should be able to launch nb viewer
now i can access nb viewer using localhost
to verify that this is actually running locally
let's try changing some of the code
let's change the title
i'm going to hit ctrl c to stop the server
i'm going to open up adam in the nb viewer repository
once adam opens
i'm going to open the nb viewer sub directory
the template sub folder
and then the index.html file
let's change the title of the website
we'll change nb viewer to
my nb viewer
we'll go ahead and save
editing these templates directly is actually not
the best way to modify nb viewer
but we'll do it for now just to verify
that we've installed from source
now back at the terminal
go ahead and relaunch the server
back in your web browser
refresh the page
when you see the title update
to my nb viewer you know that the changes
that we made to the template file were loaded
if when you refresh the page
the title doesn't change
try emptying your web browser's cache
if you want to quicker way to see
if this is the problem open an incognito tab
and then navigate to the nb viewer
web page the incognito tab
should prevent the web browser from caching
often when you do web app development
caching causes problems
because it doesn't let you see
your most recent changes to the code
earlier i had mentioned that
modifying the template directly in nb viewer
source was not the right way to
modify the template
a better way would be to
configure nb viewer's template directory
to a different directory
have it load from one of your own custom
templates which inherit it from
the template included with nb viewer
in the following videos we'll look at
how we can do that in addition to
customizing nb viewer different ways
in this video we'll look at
what we can do just by extending
the nb viewer templates
before we get started we need to remove
the hack that we added in the last video
i'm going to go ahead and launch
adam from within the nb viewer repository
once adam's launched i'll open
the nb viewer sub folder
then the template sub folder
and then index.html
in there i'll remove my space
now i'll save the file
let's see who loads
this index.html file
i'm going to open the
find in project dialog
we're creating command shift F
which is control shift F
on linux and windows
looks like the template is rendered here
in the index handler method
let's see where the render template
method searches for index.html
looks like the definition of render template
is in the nb viewer
provider's base.py class
the get template method
is used to load the template
inside the get template method
in the render template method
we can see that the ginger2 environment
has another get template method
defined which we call out to
let's see where this ginger2 environment
comes from
looks like it's defined in app.py
scrolling up to see
where nvit is defined
we see nvit is an instance
of environment which is
imported from ginger2
the template loader
is a file system loader which loads
template paths
template paths is hard coded
to the repository directory
template subdirectory
however if you specify a custom template
path using the nb viewer underscore
template underscore path
environment variable it gets
propended to a list of paths
which then is used as
the higher priority path
so we can set a custom template
search path just by setting that
environment variable
nb viewer template path
I'm going to set it to
the nb viewer underscore
templates sub folder of my home directory
now I'm going to
create that directory
I'll cd into it
and open adam
in adam
I'll create an index.html file
this file will override
the index.html file
in the nb viewer templates
folder
I'll just write hello world
and save the file
now switching back to the terminal
I'll cd back into the nb viewer repository
I'll launch nb viewer
using the same command from earlier
now when I try to access nb viewer
the page just says hello world
this means that our template was loaded successfully
let's try to complicate things
back inside the
adam that is opened in the nb viewer
repository I'm going to go to
the templates folder
open index.html again
nb viewer uses the ginger templating
library to render its html pages
this funky syntax
extends and block body
those are ginger 2 specific keywords
the rest of the code that you see
is vanilla html
let's go ahead and copy all the contents of this file
back into our
index.html file
inside the nb viewer templates
folder
now let's change the title here
and save
if we've done this correctly
we'll have changed the look of the nb viewer landing page
without actually modifying
nb viewer's source code
I refreshed the nb viewer page
and it looks like our custom template was loaded
to give ourselves the target
let's try to set up an O'Reilly themed
nb viewer
our O'Reilly nb viewer should look
like it's O'Reilly's nb viewer
but also host O'Reilly content
first let's change the
basic index template that we
created in the last video
to do so I'll open up
the nb viewer templates folder that we created
in my home directory
now I'll open
Adam in that directory
I'll change the title to O'Reilly
notebooks
we'll also change the descriptive paragraph
below
eventually we won't want to be hosting
notebooks from github so let's change
the placeholder text to reflect that
now I'll save and see
how it looks we can launch
nb viewer using the same command that we used
in the previous video I'll create
a new tab of my terminal
before starting nb viewer I need to set the
environment variable again for the custom templates
now I can launch the server
it looks like our change is rendered
however we should probably change this logo
in the top left and also remove
this link to jupiter let's scroll
down to see if there's anything else we need to change
we'll have to change this section
of showcased notebooks
and at the very bottom it looks
like we'll want to change the footer
lastly we should probably change
the styling and maybe use javascript
to spiff up the page a bit first
let's see if we can change the header and footer
let's go back to the index.html
file in our custom template
folder looking at the index.html
file it looks like layout.html
is extended for the basic layout
of the page let's open
that it should be inside the nb viewer
directory
inside the nb viewer repository
in the nb viewer sub folder
under templates we can find
layout.html
like we did with index let's copy everything
in here then
back inside our custom nb viewer
templates folder let's create a layout.html
here
I'll paste all the contents
from the other layout.html
let's remove this link to google analytics
because this is the google analytics
for the jupiter deployment
of nb viewer
also we'll want
to get rid of these links to Fastly
and change the rackspace link to O'Reilly
scrolling up let's get rid
of the text that says this website does not
host notebooks it only renders notebooks
available on other websites because
we're going to be using this sudo website
to host O'Reilly notebooks
here's the link to jupiter
that we want it to remove
lastly we'll want to change the nav
logo to O'Reilly's logo
let's go to O'Reilly's website to see if we
can get the link to their logo
I'm on O'Reilly's website now at
www.O'Reilly.com
I like this logo in the top left hand corner
I'm going to right click on it
and click copy image URL
back inside the layout.html
file I'm then going to paste
that URL over the image URL
for the existing nav logo
we'll also get rid of the new
relic reference let's save
what we have and go back to the
browser to see how it renders
awesome this is already
looking a little more O'Reilly like
we'll probably still want to change the color scheme
because I noticed when I roll over FAQ
it highlights orange which doesn't
show O'Reilly's red
looking at the bottom of the page it looks
like our footer updated it correctly
let's check out the FAQ page
it looks like there's some questions that shouldn't be here
let's remove them
back inside the nb viewer repository
it looks like the FAQ.md
file might be the file that's getting rendered
let's open that it looks like this file
does indeed extend the layout.html
file and uses a special
markdown filter to convert itself from markdown to
html in the process
it automatically generates its table of contents
let's do what we did for index.html
and layout.html in our custom templates
folder let's create an FAQ.md
file
and copy the contents from the FAQ.md
file in nb viewer
let's get rid of the first two questions because they are
completely specific to Jupiters nb viewer
we'll defer them to nb viewer
for this information
this paragraph doesn't relate at all
to our viewer nor does the one below
or the one below that
this paragraph also doesn't relate
this is related though
we just need to update it to point to our email address
the last few before the final one
also don't relate
and we'll replace the text of the final one with an email
link to the O'Reilly administrator
now let's save
and see if the FAQ page renders how we want
back in the web browser
I'm going to refresh the page
looks like we should remove the first
question as well
let's refresh the page again
much better
let's try clicking on the O'Reilly image to go back to the
home page
sweet it worked
in the next tutorial we'll look at adding custom
CSS to style it more like O'Reilly's main
website
in this video
we'll talk about how nb viewer compiles
it's less into CSS
we'll then look at adding our own
CSS to our custom
nb viewer templates
I've still left the nb viewer server running
from the last video
this is because I do not need to restart it
as long as I'm only changing static files
all I have to do
is refresh the web page to update the
contents
if I were working on server side files
for example the Python files
then I would have to restart the server
let's go ahead and open up
Adam inside the nb viewer repository
when you
installed nb viewer from source code
you had to run a command called invoke less
when you ran that
command what it did was run a function
called less
inside the tasks.py file
here's that function
what this function does
is compile the less
into CSS using the less compiler
it outputs the compiled
CSS into a build sub directory
it outputs
a styles.css, notebook.css
and slides.css
likewise
the source files used are
styles, notebook
and slides.less
let's open the nb viewer
static directory
in here
you see the folder less
and the build folder
the build folder is grayed out here
because it's not included in the git repository
that's because we don't want to check in the built files
that would just be including changes twice
the less folder
is where the less is stored
we can open up the notebook.less
to get an idea of how notebooks are styled
the major difference between less and CSS
is that less allows you to import
here you can see that bootstrap
is imported and styling
from ipython
let's go ahead and see where the build files
are referenced
in the layout template
inside the header
we see that styles.css
is referenced
inside notebook.html
we can see where notebook.css
is referenced
we can add our own styling
to our custom templates
going back to the terminal
I'm going to cd into our custom templates directory
Carol open Adam
inside our layout.html
below the existing css import
let's add our own
it's important that you do this
below the existing
because this will cause your style to override the existing
unfortunately
nb viewer doesn't support pulling files
from directories outside of its own
so we have two options
we could either place our custom style
inside the nb viewer repository
which I'd rather not do
or we can use the ginga templating
to load it from our nb viewer templates
directory and then inline it directly into the
html
first let me show you what it would look like
if you were to put the css inside the nb viewer repository
you would change build
to css
and then give your css file a name
like custom
css file
and inside the nb viewer repository
under the static directory
in css
you would right click create a new file
called custom.css
and then inside here
you would put whatever custom css you want
moving back
into our nb viewer templates directory
the alternative
I think makes more sense because then you can keep your css
next to your templates
for this instead of using a link tag
you'll use a style tag
then inside style
tags use the ginga include
to include your style file
the only downside
to using this method is that
you're disabling the browser's ability to cache
your style which means that every time
a page is requested client will have to
download the css again
that's usually not a problem with small css files
and if it is a problem
you can use the other method that I just showed you
so now let's save this file
and create our own custom css
to test to see
if our custom css is working
let's try setting the body background color
I'm going to use important
just to make sure it overrides
any other values
however it's important to note
that important isn't the best
practice
using important disables you from later
overriding styles
in a new browser window let's navigate to our
nb viewer page to see if our style gets loaded
awesome it looks like
the style loaded successfully
now instead of applying such a hideous style
let's try the override
the orange highlight color that's applied
to buttons let's inspect
the FAQ button to see how we can
select it using css
looks like a good selector would be
to use the navbar right
class and then the anchor
tag back inside our custom
css let's do that
to specify that we want to
change the styling when it is hovered over
add the hover sudo selector
for now let's just try changing the background color
again
let's use the important tag just to make sure
that what we're doing gets applied
looks like that worked
so now let's
change the font color instead of changing
the background color and let's actually
use O'Reilly's red let's go
to O'Reilly's website and we'll
right click on the home link to look at its color
now I'll just double click this
and copy it back inside
our custom css I'm going to change
background color to color and paste
this new color I'm going to save
the file then go back to the web browser
where I'll open our mb viewer tab
and refresh the page
looks like that works
now back inside the custom css
let's try that move the
important flag like I said
earlier it's better than not use
important when you can get away with it
back in the browser let's
refresh the page and see if it still works
looks like it's no longer
working we have two options
we can either stick with the important flag
or we can try to make
our selector more specific
because I know that I'm applying the top most
level styling and nobody's going to come
and inherit from the O'Reilly page
and add their own styling it's okay
for me to use it important if
however you're writing something that would later
be styled by somebody else
you'd want to make the selector more specific
to do so you could inspect the element
and either
a add more levels
of elements to your selector
or b in the templates
actually add an ID to this anchor
tag and then address the anchor
tag by ID addressing
an element by ID has a higher specificity
than addressing it otherwise
back inside the custom
CSS let's re-add the important
I'm going to refresh
the browser page
looks like that's still working
let's scroll down to the bottom of the page
maybe we should use one of O'Reilly's
grays for this bottom
we could also use O'Reilly's red
for the links
this gray looks nice
we'll copy the background color
now back on the Jupiter
NB viewer tab let's try
styling this footer
the font doesn't have enough contrast now
let's change it to black
that seems like it has too much
contrast let's see what O'Reilly does
looks like they use an off black
we'll use that too
I'd also like to add
a top border
let's copy the border color
that O'Reilly uses
looks like they use this
off-shaded gray
now we can just copy this
CSS that we've designed in the browser
and paste it into our custom
CSS in a footer selector
now let's refresh the page
scrolling to the bottom
we see that our new styling has been applied
lastly we need to change the default
link color to that red
back on our custom CSS
let's define an anchor selector
I foresee problems
with this anchor tag
and this anchor tag
let's define a color
for when the FAQ anchor tag
is not hovered on
we'll use the color that we used
for text
I'm going to save
and then go back to the browser
and refresh the page one more time
the FAQ button is still working
scroll to the bottom
and it looks like our links are formatted
correctly now
in the last video
we looked at customizing
our MB viewer deployments
CSS
in this video we used javascript to spiff up the website
a little bit
I found this really cool carousel on Bootstrap's website
here it is
Bootstrap is the component that MB viewer already uses
so we should be able to just drag
and drop this code into place
what I want to do
is replace the notebook listing
in the showcase on our MB viewer
with a carousel
so I'm going to go back to the Bootstrap website
and copy and paste the code here
inside the index template
in our custom templates folder
scrolling down towards the bottom
you can see where the showcase is built
the ginga templating for loop
is used to iterate over each section
and then it's used again
to iterate over each link in each section
we'll use this logic
to compile the different slides for our carousel
for now I'm going to insert the carousel
code above this existing code
in between the header
and the showcase
pasting what we copied from Bootstrap's website
I'm going to remove the indicator dots
on the carousel
also from experience
I know that we're not loading
glyph icon on MB viewer by default
and I don't feel like adding that dependency
instead we're using font awesome
equivalent icons would be
icon-preve
and icon-next
now what we need to do
is use that ginga code
that iterates through each item
to construct our carousel slides
it looks like each individual unit
is an item
the first item is active
let's go ahead and delete the ellipses
now let's move the
ginga templating loop logic
below this first item to create the latter items
we're going to just ignore
the notion of sections
so we'll group both the loops next to each other
now let's copy the item template into the loop
then we'll copy the image source
into the items image
we'll also copy the link text
as the alternative text
and use it as the caption
then we'll take the
anchor tag
and put it around the caption
this will make the caption clickable
now finally we'll remove
the original code from the gallery
we'll save our changes
and refresh the page to see how it renders
so here's the page
you can see it doesn't have the gallery below anymore
now it just has this carousel
that rotates through images
and each image has a link
that we can click to open that notebook
however you may notice
the size is constantly
changing
it must depend on the image height
let's fix the size of the carousel
we'll do so using css
first let's get the id
of the carousel
copy that then in your custom css
add a selector for the carousel
to select an id, prefix with the hashtag
now set the height
to 300 pixels
and the width to 300 pixels
save
and let's go back to the web browser
to see how that renders
we're going to refresh the page
here's what our smaller carousel looks like
we should probably center it in the page
and add a margin
it looks kind of weird hugging the bottom
so closely in the top
let's try centering it in the web browser
by setting margin left
and right to auto
the element will center
now let's add a top margin
to give it some distance from this horizontal line
40 pixels looks good
let's do the same with the bottom
now take one last look
that looks good
let's copy and paste this style back to our css
oops, looks like I forgot to copy margin
left and right auto
now we need to get rid of that
placeholder for the first active item
in index.html
in the carousel code
you can see that item here
go ahead and remove that
what we need to do
is only add active to the first class
to do that let's create a flag
once that flag is used once
we'll set it the false
we can use the ginger set command
to set this flag
then we'll test for that in class
lastly, let's make sure
we set first the false
when we set first the false here
we're actually declaring a new variable first
within the scope of this for loop
that overrides the first
declared in the outer scope
this means when we get to the next for loop
first will be set to true again
so we have to set first the false twice
let's refresh the page
looks like that worked
awesome
in the last video
we talked about adding custom css
and custom javascript
to your nb viewer deployment
in this video
we'll talk about changing what nb viewer
is hosting to the user
nb viewer has a notion of
providers which are the things
that dictate what nb viewer can host
there are two types of providers
uri rewrites
and handlers
uri rewrites
take textual content that's entered into the go bar
of nb viewer
and translate it to a canonical nb viewer
url
a url that nb viewer understands
and is capable of rendering
are things that are designed to
interpret and load from nb viewer
urls
the handler is the thing that actually fetches
the resources
from the local or remote location
for example
the github handler accesses
notebook content directly from github
using github's api
instead of standard http
let's start by configuring nb viewer
to host local files
sticking to our urily themed
example let's pretend that urily
wants to host files from a network
attached storage device
let's say that that storage vise is simlink
into the home directory
we'll pretend that that simlink is called network
I'm going to create this folder
just as an example that we can use
to demonstrate this feature of nb viewer
let's pretend that in the network
attacks storage drive
there's a sub folder called notebooks
and then inside the notebooks
folder there are author folders
for now I'll just create
an authored folder for myself
I have some example notebooks that are sitting
inside my home folder
I'm going to copy those over to here
now let's take a look at the nb viewer source code
I'm going to cd into the nb viewer repository
and open atom
inside the nb viewer sub folder
I'm going to open app.py
scrolling down to the very bottom
of app.py
we see all the command line arguments
that we can pass to nb viewer
one of the command line arguments
is local files
this tells nb viewer to host
files from the local file system
let's use this
I've closed the nb viewer server
I'll relaunch it with this new command
but before I launch
remember that we need to set the correct
environment variable in order for
our custom templates to be loaded
now let's launch nb viewer
let's switch to the web browser
to see if we can load files from the local files
system I'm going to try
accessing the notebook using the go bar
I'll type in the sub path
to the notebook from its
application inside
network
doing that didn't work
this would make you want to jump to the conclusion
that the local file setting isn't working
however this is
an invalid conclusion
if you pay attention to the URL
you'll see that url ford slash
was prefixed to what we tried
the access
this is telling nb viewer to use the url
handler to load the following content
of course
the url ford slash jd frederick
ford slash 1.ipynb
is not a domain name
and is not located within
a public top level domain name
so it makes sense that url
would fail to load this content
instead what we need to do
is change the url prefix to local
file
and that will get the notebook to load
we want to automate this though
we don't want the go bar to not work
and we would like the go bar to
have a local nb viewer local file
format
in the last video
we got the nb viewer local
files provider working
however we weren't able to access
it via the go bar
in this video we'll write
a urli rewrite provider
that will allow us to access local files
easily from the go bar
the first step is to open up atom
inside your nb viewer repository
next open the nb viewer
folder and inside that open providers
here you'll see a list of the
providers that are default with nb viewer
the dropbox provider
has a urli rewrite which is a good
example for the rewrite that we're going to do
let's copy the handlers.py
file
and create a sub folder
inside the providers folder called
xfer
xfer is going to be the name of our plugin
paste the file inside there
you can also copy the init file
now open the handlers.py file that you
copied go ahead and remove
the ipython header
we want this urli rewrite to accept
uris of the form
author
we'll accept the notebook name
either with or without an ipynb extension
the first step is to replace the first string
in the tuple this string is the string
that is used to search the second
string is the string that replaces
the search string
the root of the regular expression
where a group is defined by parentheses
can be accessed in the replacement
string by using curly brackets
so this 0 refers
to this first item here
whereas the 1
refers to this second group
here
without explaining too much of regular expressions
I'll tell you that this matches a set of
characters of variable length
I'll remove this text here
where this first group will match
the author add a forward slash
copy this first group
this second group will match the notebook name
and at the end I'll add
.ipynb
and I have to escape the .
because . has a special meaning in regular expressions
and add a question mark
because we don't know if the user is going to write .ipynb
or not
now in the replacement string
I'll replace the url with localfile
because localfile is the canonical
form of the urli
by the localfile provider
I'll also add
notebooks
because notebooks is the subfolder that sits inside
the network folder
the first value will be the author name
followed by the notebook name
and then we'll append a .ipynb
file extension
now let's save this
and we'll go back to the terminal
and try launching nbviewer
but first make sure to set
the environment variable that uses
custom templates that we created earlier
now let's try launching nbviewer
to get nbviewer to use
our urli rewrite
we use the double-dash provider
underscore rewrites
the provider rewrites flag
takes a full python namespace
to a rewrite provider
you may be wondering why we had to edit
nbviewer directly
well, we actually didn't have to
we could have wrote our own python package
and then reference that python namespace here
however, writing a python package
is outside of the scope of this video series
so
for simplicity, we edit it nbviewer directly
that allows us to piggyback
on nbviewer's namespace here
so to access our rewrite
we can use nbviewer
dot providers
dot xfer
lastly, we'll want to disable
github and gis providers
to do so, we'll set
the url provider as the only
provider used by nbviewer
we can do that using the double-dash
providers flag and setting that
to nbviewer dot
providers dot url
now that the server is launched, let's go to our
web browser
let's try accessing the first notebook
for my name here
looks like that worked correctly
let's go back to the home page
and try accessing it without the ipynb
to make sure it still works
looks like that worked too
the last thing we'll want to do
is change the showcase
so it shows notebooks that are actually
hosted by us
to understand how this is done, let's look at the
source code of nbviewer
back inside adam in the nbviewer repository
open up app.py
if you scroll towards the bottom
you'll see where all the command line
arguments are defined
the command line argument that we're interested in
is this front page argument
this argument points to a jason file
which defines the content that will be used
on the front page to render the showcase
the default used by nbviewer
sits inside the nbviewer repository
under front page dot jason
let's open that
here you can see the links that we see
when nbviewer runs
let's copy all the contents of this
file
and then in a new terminal window
let's cd into our
custom nbviewer templates directory
the reason why I had you open this directory
is because it's where we're storing a lot of other
custom things for our server
we might as well store other content in here
just to keep it all grouped in one place
create a new file called gallery.jason
inside that file paste the contents
from the front page dot jason
that we copied out of the nbviewer repository
now
looking at this file we see that it has
groups defined by this
header attribute
since we're ignoring the notion of groups
let's get rid of all the other groups below
when we set up the dummy directory
I only copy two files into my
author directory
so let's get rid of the third entry
we'll give the first two names
and
then change the target
to the canonical URL that points
to the correct notebook
the URL for the second notebook is almost
the same just the notebook file
is different
now we could change the image as well
but I don't have any nice images for my test
notebooks
so I'm just going to leave the images as is
I'm going to save this file
and go back to the terminal
opening the tab of the terminal that's running
nbviewer I'm going to stop nbviewer
by hitting ctrl c
I'm going to rerun the same command
except this time I'll change
front page to the full path
of the jason that specifies our gallery
now let's open the web browser to see
if that worked
refreshing the home page
we see that my jason was loaded
because this URL now points to the john's notebook
even though it's still using the old screenshot
john's notebook 2 is also available
even though it's using the old screenshot
let's click on the link to see if it works
awesome
it looks like that worked
if you want to find out more about nbviewer
visit the nbviewer repository at
www.github.com
forward slash Jupiter
forward slash nbviewer
in this chapter I'm going to talk about
temp nb
it stands for temporary notebook
temp nb is a service that launches
sandboxed ephemeral
notebook servers on demand
where ephemeral is defined as
something lasting for a short time
it's kind of like an interactive version
of nbviewer
temp nb is useful for cases where you need
to share notebooks that lose
importance if they're not interactive
temp nb users can interact
with your notebooks to see what they have
to provide they can explore
the data sets and write their own code
inside the notebooks the changes
that they might won't be persistent anywhere
so it's okay to open
a temp nb service to the public
in my web browser I'm going to navigate
to temp nb's website
at github.com
forward slash Jupiter temp nb
I'm now going to scroll down to the readme
at the top of the readme there's this very
useful diagram for describing how
temp nb works
temp nb can be broken into a few pieces
the user facing piece is the
configurable HTTP proxy
this piece routes traffic
to the correct sub pieces
the temp nb orchestrator
is what is used to launch the temporary
notebook servers
docker is the technology that is used to
containerize them once a server is
launched the temp nb orchestrator
communicates to the configurable
HTTP proxy telling it to route
a certain subset of addresses
to the correct temp nb container
Jupiter runs and maintains
its own instance of temp nb
you can access it at
try.jupiter.org
the notebook itself is the same notebook
that you're used to running on your local machine
you can see that this notebook comes
pre-populated with example notebook files
in this video chapter
I'll show you how to do this
I'll also show you how to customize your notebook
server image so that it reflects
your organization's needs
in this video I'll talk about
installing temp nb
temp nb like nb viewer
can be installed either using a docker
image or
in development mode from source code
however unlike nb viewer
it doesn't really make sense to install
temp nb from source code
unless you're planning on developing temp nb
that's because all the common
configuration that one would want to do
can be done through custom docker
images the images that
are launched by temp nb as
temporary servers
first let's open up the docker quick terminal
as you did in the last chapter
remember the ip address is printed
by docker in green
this is the ip address to use to access
your server later
the first step is to tell docker to download
temp nb you can do that
by running docker pull
jupiter minimal
once that is finished downloading
you should have a full copy of the jupiter
minimal image now
you'll need to generate a random token
this token will be used to authenticate
with configurable HTTP proxy
this command works on
linux and mac operating
systems to generate a random string
of 30 characters
however you can use any random string you'd like
for your token so on a windows machine
you can use the equivalent command
provided by that operating system
copy the random token
now we'll launch the configurable
HTTP proxy to do
so I'll start with docker run
and then I'm going to tell docker
to use the network adapter
of the host to do that
I'll use double-dash net equals
host then I'll tell
docker to run in the background
and print its id using the dash
dflag next
I'll pass in the proxy token
as an environment variable
within the image to do that I'll use
the dash eflag specify
the environment variable
and I'll paste the token that I generated
in the last step
I'll set the name of this container to proxy
then I'll specify the name
of the container I want to launch
and I'll specify default
target
since this is the first time I've
ran the command docker will
load the image from its repository
once that is finished downloading
and has launched we'll launch
the tempnb orchestrator to do so
we'll use the same type of command
except we'll change the last couple pieces of it
the name will change to tempnb
and then we'll use the special
dash v flag
to tell the docker image to bind
the docker client within itself
this will allow the docker image
to spawn other docker images
specifically we'll bind
the docker sock
and lastly we'll specify the name of the image
the orchestrator's name is
tempnb
since this is the first time I've ran this command too
docker will download the image
once that finishes
you should be able to visit your tempnb service
in the web browser navigate
to the ip address you remembered from earlier
at the end append
colon 8000
to visit port 8000
this is the port that tempnb is listening on
by default
if all is well tempnb should just work
and accessing that address will spawn
a notebook server for you in a docker
image
in the top right hand corner
you'll see a hosted by rackspace logo
this is not actually being hosted by rackspace
this is being hosted on your machine
it's just that the image that you downloaded
jupiter 4 slash minimal
is based on the same image that we use in the jupiter deployment
in this video
we'll look at how we can use
custom docker notebook images
with tempnb
jupiter has a bunch of notebook images
that you can find in the jupiter organization
in your web browser
open up the jupiter organization github page
at github.com
4 slash jupiter
once the page loads scroll down
and you'll see a repository called docker stacks
open that
this repository contains
a bunch of docker images for various tasks
let's go ahead and clone this repository
to do so copy
the clone url
in the right hand column
now in a terminal
navigate to your home directory
run git clone
and then paste the url
once the cloning is finished
cd into that directory
and let's open adam
once adam opens
open the minimal notebook directory
this minimal notebook image
is actually different than the minimal notebook image you used
in the last video
but the one that we used in the last video is actually deprecated
and this is the modern replacement
this image doesn't have
a racks based logo in the top right hand corner
let's open up the docker file
this is the file that tells docker
how to build the image
this from line is how
docker knows what this image inherits from
the debian jesse
image is used as a base
you can see the list of docker commands
used to build this image
at the end
we specify that the start notebook
.shell file should be executed
let's open that
here you can see how the notebook is launched
the config file used for the notebook
is stored under jupiter underscore
notebook underscore config
this is the same kind of config file that we looked at
in the second chapter
the files as they are in this repository
are not a docker image
we have to first build them
the build process is described in the make file
let's open that
the help section describes how the build
make file is used
to build the minimal notebook
we just need the run build
minimal-notebook
let's try that within this directory
first docker will download the base image
it will take a while
but once it's done your image will be built
now let's try using this
image with tempnb
start a docker quick terminal
once the terminal starts
pay attention to the ip address like you did before
we're going to run the same commands that we did
in the video before the last video
skipping the docker pull command
and changing some of the contents of the last command
if you're continuing on from the last video
make sure that you close all the existing docker containers
before trying to do this
to do so
you can run the following command
docker
stop
dollar sign
docker
space-a
space-q
I don't have any docker containers running right now
so I get the help output
after running that command
you'll want to run almost the same command
but replacing stop with rm
the last command is almost identical
just changing from the name forward
once again
we'll tell it to connect to itself
so it's capable of launching other docker images
and here's where the command will start to change significantly
from the last video
in addition to the omitted name flag
we'll start specifying the python command
that launches the orchestrator
we'll specify the image
that we just built
now the tricky part is
that we'll have to tell the image how to launch the notebook server
we do so
using the double-dash command flag
we have to tell the notebook app
what its base URL is
the image will format the string
and you can insert special variables
using curly brackets
base path is one of those special variables
that you can insert
we'll tell it to listen to ip0.0.0.0
which will allow it to listen to anything
lastly we'll specify the port that it's listening on
once you run that command
in your web browser
try accessing the docker image
if everything works
you should see a new notebook server
this notebook server
won't have a rackspace logo
in the top right hand corner
if you have troubles
most likely you mistype something
if you need to debug why it's not working
open up another docker quick terminal
when the docker quick terminal launches
you can run docker ps-a
this will list
all the docker processes that are running
if you see one that says
exit it with an exit code in parentheses
you can look at the logs
of that docker image
to do so run docker
space logs
and then copy the container id
which is in the far left column
and paste it
and one of the attempts I made earlier
to run this long command
I misspelled orchestrate
this caused the server to not run
and receive gateway errors
by looking at the logs
I could tell that was the problem
and was able to correct it quickly
in the last couple of videos
we looked at launching tempnb
using custom notebook image
in the following videos
including this one
we'll look at creating our own custom notebook image
for use with tempnb
to get started launch the docker quick start terminal
once the terminal launches
pay attention to the ip address like you did before
we'll be using that ip address
to access tempnb
in the last couple of videos
we use the jupiter docker stacks
minimal notebook image
we'll use that image as a base for our new custom image
to do so
let's copy the image out of the repository
I'll copy it into a directory
called custom notebook
this will be the name of the custom image
that I'm going to create
I'll then cd into custom notebook
and I'll open adam
once adam opens
I'll open the config file
inside custom notebook
jupiter notebook underscore config.py
this is the configuration file
that will be loaded by the jupiter notebook
inside the notebook image
recalling from an earlier chapter
I'm going to set the untitled notebook name
this is an easy variable to set
that we can use to quickly judge
whether or not our config file is being loaded
the variable is c.contentsManager
.untitled
notebook
I'll set that to test
now I'll save the file
next I'm going to create a shell file
that we'll use to build this image
I'm going to copy the shebang
from the start notebook file
we'll call the new file build.sh
I'm going to go back
to my docker quick start terminal
I'm going to open adam up
inside the docker stacks repository
when adam opens
I'm going to open the make file
I'm going to scroll down to the build line
so I can see how images are built
I'll go ahead and copy this line
I'm going to go back to the adam
that we opened up inside the custom notebook directory
I'm going to paste this line
inside the build.sh file
I'm going to remove d-args
and replace owner
with jd fredder
you can use whatever you want here
to identify yourself
and I'm going to replace this not dur
dollar at with the name
of my notebook image
I'll also get rid of the not dur
at at the end and the forward slash
this tells docker to build the contents
inside the current directory
now I'm going to copy this shebang again
and create a new file for testing
this image with tempnb
I'll call this file test.sh
I'll paste the shebang
and then I'll enter a command
that causes all the images that are currently running
in docker to close
it's important to note that this command
is inside this file
we don't want to run this file
if there are docker images on our system
that we don't want to close
the reason I'm adding this line
is because it becomes tedious
to constantly close docker images
each time you want to run your test
to close all the images that are currently running
I'll use docker stop
dollar parentheses
docker ps-a-q
what it does is runs docker stop
on every docker image that's
currently running
I'm going to copy this line
paste it below and replace stop with rm
this will do the same thing
but remove the images instead of stopping them
next I'm going to create a token
for use with the http config proxy
I'll use export to define
the variable token
as head
30 characters long
of dev urandom
with xxd-p
next I'll run
the configurable http proxy
image to do so
I'll use docker run
double-dash net equals host
dash d dash e
config proxy
auth token
equal the token variable
double-dash name equals proxy
image name jupiter
configurable
http proxy
space double-dash
default
dash target
127.0.0.1
port
9999
I'm going to turn on wordwraps
so you can see the whole command
next
I'm going to launch the tempnb
orchestrator image
I'll start with the same command
but deviate once I get to the name
I'll use dash v
var run
docker.soc
colon for slash docker.soc
to cause the image to connect
to the docker client
next I'll specify the jupiter
tempnb image
in the command python
orchestrate.py
I'll specify the image
to jd fredder
custom notebook
and the command
to start dash notebook
.sh
this part's really important
the minimal notebook image requires you to start
the notebook server using start-notebook.sh
instead of running
ipython space notebook
or jupiter space notebook
that's because if you run
either of those the notebook will be launched
as root and the notebook will be looking
for the configuration file inside the root
home directory
however the configuration file is
installed into the jovian user's
root home directory
so running start-notebook.sh
does some special things
that causes the notebook to launch the server as the jovian user
I'll have to pass some commands
into the start notebook shell script
to do so I'll escape quotes
inside those quotes
I'll set the base URL
allow origin
and the port
I'll save this file
and go back to the docker terminal
now I'll navigate to the
custom notebook directory that I created earlier
and I'll try running the build.sh
file I just created
if you get a permission denied
it's probably because the permissions aren't set correctly on the file
you can do so by running
chmod
plus x
build.sh
looks like the image built successfully
now let's try running the test shell file
we'll have to change the permissions of that as well
looks like that worked
we get these help outputs
because no images were running at the time
the last two outputs
are the grids for the images that were launched
let's go to the web browser
try accessing the tempnb
server via the ip address that docker printed
looks like the server launched successfully
now let's see if the config worked
awesome
it looks like the default
notebook name is no longer untitled
but is test which implies that
our config is being loaded
in this video we'll add custom
content to our tempnb
notebook custom image
this process is very similar to the process
that you use for adding custom content
to your nb viewer deployment
that's because the notebook itself
uses ginga2
like nb viewer to do its templating
first let's start the docker quick start terminal
pay attention
to the ip address that is listed
for that's the ip you'll use to access docker
let's go ahead and navigate
into our custom notebook directory
and open atom
the first thing we'll do
is create a page.html template
this template will override
the page.html template
of the notebook
inside the page.html template
we'll extend the base template
of the notebook
next we'll override the header
underscore buttons block
this block exists at the top
of the notebook pages
we can use this to add our own logo
we'll go ahead and add an orily logo here
we have two options to do this
we could either add the orily
picture to our custom
notebook image or
we could host it externally
and reference it here
it's better to host your images
and other static content
externally to the images that are
launched by the orchestrator
that's because the notebook server
uses tornado to host its files
and tornado isn't as fast
as other servers like engine
x or apache
which are even slower than services
like cdn's
so what we'll do is open our web browser
and get the link for the orily image
www.orily.com
once the page loads
right click on the image
and say copy image URL
then go back to atom
now on the header buttons block
add an image tag
set the source of that image tag
to the link that you copied from orily
save the page
now we'll need to copy this template into our image
to do so open your docker file
scroll down to the bottom
the first thing you'll need to do
is create a directory that contain templates
to do so we're going to copy this
line that creates the dot jupyter
directory inside the user directory
we'll put our template
directory inside that
we'll call it custom
we'll then need to copy the file into that directory
go ahead and copy the line
that does the notebook config
change notebook config.py
to page.html
and update the path to custom
save the file
lastly you'll need to go into your jupyter
notebook config.py file
inside here below the
untitled notebook line
set the extra template paths variable
of the notebook app
this variable accepts a list
a path give it the path
to your custom template folder
and then save the file
now go back to your docker terminal
and inside here run the build script again
once the build script finishes
you can run the test script
now go back to your web browser
try accessing tempnb
if all goes well
you should see the Riley logo in the top of the header bar
you could style this better
by using css
in your template page
but the point here is not to make something that looks good
it's just to show you how to get static content
into your tempnb images
in this video I'm going to talk to you about
setting limits on your tempnb
service and then briefly
I'll talk about security
to get started open up a terminal
then navigate into the custom
notebook directory
this is the directory that contains the
custom image we've created
now open Adam
once Adam opens
open the test.sh file
this is the file that contains the lines
that can launch this image in tempnb
in a real deployment you could
use these same lines just remove
the two docker stop and docker
rm lines. I'm going to enable
wordwrap so you can see the whole commands
the last
command is the command that launches
the orchestrator. We pass in
a command into the image using the
double-dash command flag
we tell the orchestrator what image to use
using the double-dash image flag
there are also additional flags
for example
if you need to limit the number of CPUs
any particular container can use
you can use the double-dash
CPU underscore shares flag
and this accepts
an integer value for how many
CPUs are allowed
for example we could limit each image
to using two CPUs at most
by doing equals two
the next useful flag for limiting
is the coal period flag
this flag accepts
an integer in seconds
that determines how often
containers are examined
for their age and then collect it if
old enough. The default for
this is 600 seconds
this is 10 minutes
we could make this faster for example
by doing 300 seconds
coal timeout
is a variable that sets how long it
takes for a container to be sitting idle
that it will get cold
the default for this is
3600 seconds
this variable is also an integer
specified in seconds
we can half that time by setting it to
1800 seconds
we can also set a limit
on the amount of memory each container
allowed to use by setting
mem underscore limit
this accepts a string
specifying the amount of memory
that each container is allowed to use
it defaults to
512m
for 512 megabytes
we can half this by setting it to
256m
the last important flag
I would like to mention is the pool
size flag
this flag accepts an integer
which specifies how many child
docker containers can be launched by the
orchestrator you can think of this as
a limit as how many users can use
tempad b at any given time
the default for this is 10
we can limit it to half that
by setting it to 5
note that these flags are all set outside
of the double dash command
because they're not actually getting passed into the image
of the orchestrator itself
lastly let's talk a little bit about security
go ahead and open up your
jupiter underscore notebook underscore config
you see here in this configuration
file that there's a flag for
HTTPS encryption and
password this is the same
HTTPS encryption and password that you
used in the earlier chapter
where you learned how to deploy the jupiter
notebook this may be useful to you
but take note that this
is not affecting the orchestrator
itself so any random
user can still access your deployment
of temp and b and launch containers
they just may not be able to
take advantage of those containers if they don't
have the appropriate credentials to log
on to them this means that
those people could still spawn
up a bunch of containers
and use your entire pool
even if they're not authenticated
this is a limitation of temp and b
as the temp and b orchestrator does
not yet have a password mechanism
you could however wrap
the orchestrator in your own password
at proxy
in this chapter
I'll teach you about jupiter hub
the technical definition
of jupiter hub is that it's
a multi-user server that
manages in proxies multiple
instances of the single user
jupiter notebook server
a less technical definition is
that jupiter hub is a
multi-user version of the jupiter
notebook jupiter hub is a
python 3 only application
but that doesn't mean that the
kernels that are ran by the notebook servers
launched by jupiter hub are restricted
to python 3 only
in other words the user isn't
restricted to python 3
jupiter hub is comprised of 3
main pieces the multi-user hub
the configurable
HTTP proxy
and the multiple single
user notebook servers that are launched
by the hub
when you start jupiter hub
you're actually starting the hub application
the hub application then spawns
the configurable proxy
the proxy forwards all requests
on the root domain to the hub
the proxy is what's
exposed to the internet
the hub then authenticates the user
when the user connects
and the hub will launch a single user notebook
server for that user
it then configures the proxy
requests on the root domain
forward slash the user name
to that new single user
notebook server that it launched
jupiter hub is highly configurable
the authentication is configurable
we're going to look specifically
at the O authenticator extension
which allows you to use github
authentication with jupiter hub
but you could write your own authenticator
this is useful if your organization
uses a specialized
authentication scheme
second you can configure the spawning
in other words you can configure
how single user notebook servers are launched
we're going to look specifically
at the docker spawner
which is a tool that allows jupiter hub
to spawn the single user notebook servers
using docker
and lastly you can configure the spawn notebook
itself
by default jupiter hub launches the notebook
that's installed on the local machine
if you're using something like the docker spawner
you can customize the notebook
using the techniques described in the last chapter
where we created a custom
jupiter notebook docker image
in the following videos
we'll look at three ways to install jupiter hub
the first
is a completely vanilla installed
directly from package managers
the second is a vanilla install
with the docker launcher extension
and the last
is a more complex install
that uses a combination of the docker
launcher extension and docker swarm
to handle more users
to redistribute the demand across multiple
machines in order to handle a higher
user load
first let's remove the dot jupiter
folder that we created in the earlier
chapter where we examined installing
the vanilla notebook
we need to do this because jupiter hub
relies on the local notebook install
we don't want to dirty
our new jupiter hub install with the config options
that we set earlier
on the other hand later you'll find
configuration of jupiter hub to be easy
because configuring the notebook
servers that get launched by jupiter hub
is the exact same procedure that we
examined earlier using traitlets
in the config machinery to config
the vanilla notebook
all the configuration that you have for the vanilla notebook
will apply to the vanilla notebook that's
launched by jupiter hub
you'll want to verify that you have python 3 on your machine
you can do so by running python
double-dash version
if your system does not print python 3
try python 3 double-dash version
if that too
does not work or does not print
version 3 then you'll want to
revisit chapter 1
video 3 where we talk about prerequisites
and you'll want to make sure that you have
python 3 installed on your machine
next let's look at the version of node
that we have installed
you can do so by running npm-v
i have version 3.4.1
installed on my machine
if your version is lesser than that
you can update it by running
sudo npm install
dash g npm
what this will do is cause
npm to uninstall
itself and then install the
latest version of itself in its place
if this command fails part way through
you'll find that you need to re-install
node and npm
the first thing we'll install is the configurable
HTTP proxy
you'll recognize that name from the earlier chapter
where we looked at deploying tempnb
however in that chapter
we used a configurable HTTP
proxy docker image
so we didn't actually install the configurable
HTTP proxy on the local machine
because we're installing jupiter hub
on the local machine we'll need to do that here
go ahead and run sudo
npm install
dash g where this dash g
flag installs the software
globally configurable
HTTP proxy
once that is finished
you'll want to install jupiter hub
you can do so by running pip3
install
jupiter hub
by running pip3 we force the python3
pip to be used
if you receive a permission denied error
go ahead and prepend the command with sudo
now you can try launching jupiter hub
if you have an error like this
go ahead and uninstall
jupiter hub
and then reinstall it
when you first run the hub
you may get an error that there's a bad configuration file
you can fix this
by running the command that is recommended
this command will generate a
configuration file for you
say yes when it asks
if you want to override the file
now try launching the hub
if everything is successful
you should get a message saying that the hub is now running
at localhost 8000
in your web browser try accessing that
awesome, looks like that worked
now you should be able to log on using
your local system credentials
now that jupiter hub is installed
let's see how it works
you can launch jupiter hub by running jupiter hub
when jupiter hub launches
you'll notice a couple warnings
the first warning is that the config proxy
auth token had to be generated by jupiter hub
you can bypass this warning
by setting that variable explicitly
in the future
when you decide to use extensions with jupiter hub
such as nbgrader
you'll need to set this token
this token is how applications can communicate
with the configurable http proxy
nbgrader for example
adds a handle to the configurable
http proxy
that allows graders to access notebooks
with a special interface
the second warning you'll see
is that no admin users are defined
so the admin interface will not be accessible
we'll go ahead and ignore that for now
switch to your web browser
we'll access the address listed here
it should be available at
localhost 8000
when you access that address
you'll be presented with a log on screen
jupiter hub uses PAM as a default
authentication method
this means that to access jupiter hub
you use credentials on the host machine
in other words you use
your current account name if you're running it locally
the password is the same
password for the account on the host operating system
when you sign in
you'll be presented with your own notebook server
in the top right hand corner
you'll see a button for a control panel
and a button to log out
go ahead and click on control panel
in the control panel you'll see an option
to stop your server
or access your server
go ahead and stop your server
you'll also see an option to administrate jupiter hub
click on that
here you'll see a screen that allows you to
remove users
and remove users
here I'm going to remove JD Fredder
you can also change users
from admin to normal users
go ahead and log out
in this video I'll show you how to
install the jupiter hub docker launcher extension
jupiter hub is a
highly configurable application
even the way that jupiter hub launches
single user notebook servers is configurable
the docker launcher extension
allows you to force jupiter hub
to launch the single user notebook servers
as docker images
with this extension you can launch any
custom docker image that you have
that contains a jupiter notebook server
if you want jupiter hub to launch
the single user notebook servers using
something other than docker you can write your own
extension to do so
to get started open up a docker quick term
once the docker quick terminal
launches pay attention to the IP address
you'll need that IP address for later
during configuration
before we get started we should close all
existing docker images
just to make sure that none are running that will
conflict with what we're trying to do
to do so you can run
docker stop
and then dollar parentheses
docker ps-a-q
semicolon
docker rm
dollar parentheses
docker ps-a-q
now
you can get the docker spawner extension
source code by cloning it from github
to do so run get clone
https
github.com
jupiter
docker spawner
you want to run this inside the directory
that you want to install the source code to
I'm doing it inside my home directory
now cd
into that repository
run pip3
install
rrequirements.txt
this will install the requirements
of the docker spawner don't forget to add a
sudo in front if your permissions require it
next
run python 3
setup.py
install
lastly run sudo
pip3
install.e.
now we'll need to
change our jupiter hub config file
which launches using the docker spawner
cd back out into your home directory
or whatever directory
that you launched jupiter hub from
I launched jupiter hub from my home directory
once there open up
jupiter hub underscore config.py
file in your text editor
I'm going to open it in atom
below the first
line add c
dot jupiter hub
dot spawner underscore
class equals docker
spawner dot docker spawner
pay attention to the capitalization
this tells jupiter hub
to use the docker spawner
next add c
dot docker spawner
dot use underscore
docker underscore
client underscore
nv equal to true
this allows the docker spawner to work
with the docker quick terminal
next add c
dot docker spawner dot
tls assert
underscore hostname
equal to false
this is also required to use the docker quick
term in your custom image with docker spawner
next
add c dot docker spawner
dot container underscore
image equal
the name of your custom image
I'm going to use the image that I created earlier
in the temp nb chapter
use your custom image here too
now go ahead and save the file
now cd into your custom
notebook image directory
this is the same directory from the chapter
where we investigate at temp nb
open atom
open up your jupiter underscore notebook config
file
inside here add c
dot notebook app
dot base url
equals os dot nvi
run jpy underscore
base underscore url
this configures the notebook
server to listen to the url
that's a subset of jupiter hub
go ahead and click save
and then close atom
now you should be able to launch jupiter hub
navigate back to the directory that you launched jupiter hub from
minus the home directory
type jupiter hub
double-dash docker
spawner dot container
underscore ip
equals 192 dot 168
dot 99
dot 100
press ip address with the ip address that was
listed by docker when you launched
the quick term click return
once the server launches
go to your web browser
you should be prompted with a login
login using your local credentials
once you login you should see
your custom notebook image running
this means that everything we did worked
in the last video
we set up jupiter hub with the docker spawner extension
this made jupiter hub
spawn notebook servers inside docker
images in this video
we'll take it a step further and customize
how jupiter hub does authentication
jupiter hub has a notion of
authenticators which allow you to change
how users authenticate with jupiter hub
you can use authentication methods
ranging from traditional
used in academia and in the industry
to more specialized methods
like using social networking
or social media authentication
in this video we'll look at using
github's authentication system
there's an extension called the
oauthenticator which was written
for jupiter hub to allow us to do this
first open up a docker quick terminal
once the quick terminal launches
make sure to close all images
that are already running on the machine
included in the docker spawner
extension repository is an example
of how to use the docker spawner
with the oauthenticator
we'll use that as a starting point
first you want to copy your jupiter hub
config into that directory
my jupiter hub config is located
in my home directory because that's where
I launched jupiter hub so I'm going to copy
that from my home directory into that
repository example folder
next cd into that directory
now run sudo
pip3 install
git plus
https
github.com
jupiter
oauthenticator.git
when that finishes you want to create a user
list file let's open up adam
inside this directory once adam
opens go ahead and right click
and create a user list file
inside the user list
add github user names that you want to have access
to your server don't forget to add your
own I'm going to add Brian
and Kyle my colleagues
make yourself an admin by adding
a space and admin after
your accounting save the file
and go ahead and close adam for now
in your web browser
go to github.com
forward slash settings
forward slash applications
forward slash new
when that page loads give your application
a name I'm going to call mine jupiter hub
this is the name that users will see
when authenticating while connecting
to your jupiter hub instance
set the home page URL
to the jupiter hub URL
this should be for now local
host 8000
go ahead and copy that URL
paste it below where it says
authorization callback URL
then append hub
forward slash oauth underscore
callback now click
register application
go back to your desktop
launch a Docker quick start terminal
once the quick start terminal launches
pay attention to the IP address that's listed
you'll need this later now
let's cd into the docker spawner directory
inside that cd
into the oauth examples directory
now open adam
once adam opens in that directory
open the jupiter hub config
file below the container
image line you're going to need to add a new line
add c
dot jupiter hub dot
authenticator underscore class
equal to in quotes
oauthenticator dot github
oauthenticator
now below that line
add c dot
github oauthenticator
dot oauth underscore
callback underscore URL
equal to the URL that you provide it
for the authentication callback
while creating the application on github.com
I'm going to go back to my web browser
to show you what that URL is
at the bottom of the page you'll see it
go ahead and copy that
now below that line
add c dot
github oauthenticator
dot client underscore ID
equal to the client ID
provided to you by github
it's located at the top of the page
now below that line
add c dot github oauthenticator
dot client underscore secret
equal to the secret provided to you by github
lastly on the line
below that you'll need to set yourself
as an administrator to do so
set c dot
oauthenticator dot admin
underscore users
equal to and then in square brackets
in quotes your account name
this is your github account name
now save the file
back in the terminal
run dash
run.sh
double-dash docker spawner
dot container IP equal to
the IP address listed in green
now in your web browser
navigate to the jupyter hub instance
it should be at local
host colon 8000
once you arrive on that page
click the sign in with github button
you should be asked to authorize the application
click authorize
you'll then be redirected back to your jupyter hub instance
you can click my server
to access your server
or admin to administrate jupyter hub
I'm going to click on my server
note that our custom image
is still being loaded
in the previous videos
we were able to get jupyter hub working
with github oauthentification
and a custom docker image
in this video we'll look at how we can enable our users
to share files across
their different accounts inside the jupyter hub instance
to do so
we'll mount a shared directory
on the host operating system
we'll do this two ways
one will mount it as read only
for content that all users should be able to see
but not necessarily edit
two will mount it as read write
so users can have a shared directory
from which they can save files
and fetch files
to get started open up a docker quick terminal
once your docker quick terminal launches
go ahead and make sure no docker images are currently running
inside my home directory
I'm going to create two shared folders
one will be called shared underscore
rw for shared read write
and the other shared underscore
r for read only shared
you can use any directory that's
accessible on your file system
I'm using my home directory as a convenience
now I'm going to copy
two example notebooks into each of those folders
I'm going to cd into
the shared read write folder
and launch a normal jupyter
notebook server
when the notebook server launches
go ahead and create a new python notebook
first I'm going to change the name
of this notebook
I'll change it to test one
now I'll give the notebook some content
I'll make the first cell a markdown cell
in the second cell
I'll add some code
now I'm going to save this notebook
now close the web browser
and go back to the terminal
in the terminal I'll hit ctrl c twice
to close the server
now I'll cd into the read only directory
I'll launch the notebook server here too
once the notebook server launches
I'm going to create a new notebook
I'll call this notebook test two
I'll make the first cell a markdown cell
in the second cell
I'll add some code
now I'll save the file
and close the web browser
I'll close the jupyter notebook server
by hitting ctrl c twice
now cd into the
docker spawner directory
inside there I'll cd into the
examples oauth directory
and open adam
once adam opens
I'll make sure my jupyter hub underscore config
file is opened
then load the admin users line
I'll add c dot docker
spawner dot volumes
equals a mapping of
the volumes
the volume mapping is path
on the local machine as the key
and as the value
path that it should be mounted inside the docker image
I'll mount the read write
directory
I'll have it mounted to home
jovian
four slash work
four slash shared
that's because home jovian
work is the directory that's loaded by default
inside the docker image
in the docker directory
the syntax is almost the same
go ahead and copy that line
and paste a copy of it below
on this line we'll change the name of the path
that's mounted
let's change it to read only
likewise we'll change the path on the parent
system to the read only directory
the important part
is that the key is not docker spawner
dot volumes
it's actually dot read underscore
only underscore volumes
go ahead and save the file
go back to the terminal
now
launch the server like you did before
don't forget to set the docker spawner
container IP trait
the IP address is the IP listed
by the docker quick terminal in green
when you launched it
once your server is launched go back to your web browser
in your web browser
navigate to your jupyter hub instance
you may still be logged on to your other
session from the earlier videos
go ahead and click on my server
when my server loads
you should see two folders
read only and shared
go ahead and open shared
inside shared
you should see the test one notebook
go ahead and open that
make a change to this notebook
it doesn't matter what change
just a change that you can see
then go ahead and try saving the notebook
when you save
you should have seen the checkpoint flash up
to the left of the kernel name
go ahead and close the notebook
and try reopening it
looks like that worked
go ahead and close the notebook
go back to your home directory
then go inside the read only directory
open up the test
to that notebook
when you open this notebook you should see
a notification that flashes quickly
to the left of the kernel that says
autosave disabled
you should also see an icon of a floppy
with a red circle above it
indicating that saving is disabled
try making changes to this file
any changes it doesn't matter
I'm going to remove this read only
now I'm going to try saving
when I save
I should see another notification in yellow
that says the notebook is read only
go ahead and close the notebook
reopen the notebook
and you should notice that your changes
weren't saved
this means that the read only is working correctly
in this video we'll talk about
how you can increase the performance
of your Jupyter Hub deployment
using Nginx
Nginx will be used to host the static
files of the Jupyter notebook
the Jupyter notebook uses Tornado
to host its web content
Tornado is great for templating
and hosting dynamic content
however it's slower than things like
Nginx or Apache
to host static files
the methods described in this video
will extend it to redirect
and host the static content on CDNs
first we're going to launch
Jupyter Hub
go ahead and open up a Docker quick terminal
pay attention to the IP in green
then make sure that all Docker
images are closed
next, navigate into
the OAuth example folder
inside the Docker spawner directory
launch Jupyter Hub
by running the run.sh script
once Jupyter Hub launches
open up your web browser
and verify that Jupyter Hub is running
it should be available at localhost
colon 8000
now go back to the terminal
open up a new tab
by hitting command T
if you're on a machine that doesn't support tabs
in your terminal open up a new terminal
now we'll install Nginx
on OS 10
you can do this using brew
on Linux operating systems
you'll want to use the package manager
of that system
typically this is apt-get
or yum
if you're on OS 10
go to your web browser
go to brew.sh
this is the home page for brew
if you don't have brew installed already
copy the line under the install
home brew section inside the text box
paste that line in your terminal
and execute it to install home brew
I've already installed home brew
on my machine so I'm not going to demonstrate
this for you go back to your terminal
now
make sure that brew is up to date
to do so you're going to run brew update
now
we'll use brew to install Nginx
once brew is finished installing
Nginx run Nginx
now go back to your web browser
access
localhost
8080 to see if Nginx is running
if Nginx is running
you should see a welcome to Nginx page
now go back to your terminal
run
Nginx-s
stop
to stop the Nginx service
now go back to your web browser
go to
github.com
forward slash settings
forward slash developers
you should see the Jupyter Hub application that you registered earlier
click on that
now change the port on the home page
and the authentication callback URL
to 8080
go back to your terminal
now change the Nginx
configuration file so that
it proxies all requests to Jupyter Hub
we'll also proxy the web
socket connections to Jupyter Hub
however we'll intercept requests
to static assets and host
those directly using Nginx
to edit the Nginx configuration
file on OS X
run atom or
open up forward slash usr
forward slash local forward slash
etsy forward slash Nginx
forward slash Nginx.conf
this is the path to the configuration
file for Nginx
if you're running Nginx on a machine other
than OS X this path may be different
you'll have to refer to your
installation method to figure out
where the configuration file
lives by default
let's open this file in atom
the first thing we'll do is trim a lot
of the comments and access lines
this will allow us to focus better
on what the contents of the configuration
file should be
I'm going to go ahead and remove this user
nobody comment
and also the log comments and process
ID comment below
I'll leave the events block
and remove the log format comment
access log comment
IP push, keep a live time out
jeez it all the way down
to the server block
inside the server block
I'll leave the listen to port
8080 and server name local host
lines I'll remove the lines
down to the location forward slash
everything from here on out
I'll remove
now we'll configure all requests
on root to forward to
jupiter hub
to do so remove the lines inside
the root block
the first line you'll need is proxy underscore
pass
space the address to jupiter hub
next proxy underscore
set underscore header
capital X dash capital
R real
dash all caps IP
space dollar remote underscore
add semicolon
next you'll want proxy underscore
set underscore header
host with the capital H
dollar HTTP underscore
host semicolon
in the last line you'll want in the root section
proxy underscore set underscore
header
space capital X
dash capital F forward it
dash capital F4
space dollar proxy underscore
add underscore
X underscore forward it
dash underscore four
now copy these four lines that you just
wrote
below the location root block
we'll need to add another location block
which will only intercept
attempts to connect to web sockets
we have to handle web socket forwarding
specially this is a detail
of engine X configuration
to do so write
location space till day asterisk
then we're going to add
a long regular expression
to look kind of funky this regular
expression will be used to match
the request path for web
socket connections this first
group is matching the user
forward slash account name
section of the URL
the second group matches the
web socket request specific to the
notebook server then suffix
with forward slash question mark
and that's all you need for the regular
expression that identifies web
socket requests I'll turn on
the board wrap so you can see this whole line
inside that group paste the four
lines that you copied earlier
you'll need to add some additional
lines to get web socket forwarding
the work first
you'll want to add proxy underscore
HTTP underscore version
space one point one
next you'll want to add proxy
underscore set underscore
header space capital
U upgrade space
underscore HTTP underscore
upgrade semicolon
next add proxy underscore
set underscore header
space capital C connection
space upgrade in
quote semicolon
last you'll want to add proxy underscore
read underscore timeout
space eighty six thousand
four hundred semicolon
this is all you need to get content
to forward to jupiter hub
using engine X
last piece we'll want to add is to
intercept request for static
assets we'll want to
host directly from the notebook
directory but first let's
make sure that this is working
save the file go back
to your terminal
launch engine X
if you get a message
like this it means there's something wrong
with your configuration file
it looks like mine has a typo
remote underscore add
it's supposed to be remote underscore adder
I'm going to add an R
and then save the file
now I'm going to go back to the terminal
I'm going to try launching engine X again
it looks like I missed another instance
of remote add
also down here where I
copied that content from the root
I'm going to save the file
I'll try launching engine X again
looks like it launched successfully
now I'm going to go to my web browser
to verify that it launched
I'm going to try accessing engine X
if you recall correctly
is that localhost 8080
when I first access it
it looks as if what I did
had no effect on engine X
however this is because my web browser
is caching the contents of the last request
by refreshing the page
I should see the right contents
if refreshing the page
doesn't fix the problem for you
you may need to clear your web browser's cache
to do so you'll have to follow
steps specific to your web browser
I'm going to go ahead and click on my server
I need to validate
that the proxy for the WebSockets
is working
I'm going to open up the shared folder
and then the test1
notebook
I'm going to try to run the cell with a change
if it works
I know that the WebSockets are forwarding
correctly because the notebook
is able to execute code
I'm going to save and close this notebook
now
I want to try to speed up this jupiter hub instance
to do so
I'll hot the intercept to request the static
I'm going to go back to my terminal
the first thing I need to do
is make sure that I have the static
notebook files somewhere on my computer
that way engine X can host them
I'm going to navigate to my root directory
here
to clone the notebook I'm going to run
git clone
https
https.com
forward slash jupiter
forward slash notebook
once the notebook clones successfully
I'm going to go back to the atom instance
that I used to open the engine X
configuration
above the location root block
I'm going to add a new block
this block will recognize requests for static
assets to do so I'll have to use a regular
expression again
this time just use tilde
no asterisk
as follows forward slash
then the first group is the user
block just like we did earlier
and then the next block is
forward slash static
forward slash
lastly parentheses dot
asterisk to match all characters
forward slash
question v equals
and then parentheses dot
asterisk to match all characters
now
you're going to specify the root directory
that we clone the notebook repository to
when that is finished
save the file
and return to your terminal
make sure to stop engine X if it's already running
by running engine X
dash s stop
then run engine X
to launch engine X again
now let's go back to the web browser
navigate back to the root page
refresh the page
if everything worked
the page shouldn't look any different
however this Jupiter logo
for example is being hosted by
engine X

I'm really delighted to be here, really delighted to have been invited to deliver this truane lecture on addressing the risks of generative AI.
Le Become enghraifftol ynogi explorer yn,
Quick horm一直ant o'neud ei ddweud y gofa,
heb rail iawn hefyd,
a holl i'r define ar gyfer yr holl.
O'r depar交ed,
ble corw b saves
a bod yn sukaill ar gyfer felly
sy'n cael ei ddweud ors wrth alu
ni da ni yn cael zyloedd
i'rョll i gweinio arbeneg yn mynd i g gref
ac yn hallu'r funVI.
Ond nad a organisations yna
unrhyw bai gwein�이 embedd 0 i latero'r gweinogi
ac mae'r f estàil wedi bod yn gweinogi
o llawer o ail�r d attempting
..a fu ten cyfath ....
..lively
but just a very brief kind of intro to generic
generally speaking, what kind of AI is as some
of this will become much clearer through the examples that we discuss over the
course of this evening.
The very annually speaking, Però
effective large tool models that can create new content in the in form of words and text.
But there can also be image generators that can be used to create images in pretty much any style.
So this might be illustrations that might be nist題 drawings, that might be been photorealistic images
or pretty much anything you could want and many things that you never knew you wanted,
like space travel and herbal.
One of my favorites or oh.
Is that the Pope in hardly ERIC jacket, which some of you may have seen doing
around earlier this year.
So these are AI generated images.
Generative AI can also be used to create fake videos
and also synthetic voices or cloned voices,
which can be used to very convincingly replicate
real individual human voices.
So generative AI is this broad set of AI technologies
that can be used to create new content.
These technologies are not exactly new.
They've been developing for a number of years now.
But it's really been in the last year
that we've seen something of a frenzy of excitement
about these technologies, about how they could be used,
also how they could potentially be misused.
We've seen a lot of hype, a lot of excitement
about the opportunities of capitalising on innovation
in generative AI.
But we've also seen a lot of concern
and some fairly sensational alarming claims
about risks associated with generative AI.
I used to say, probably about a year ago, I would have said,
one of the things that is really exciting
about working in the field of AI
is that, roughly, maybe every two to three months,
there'll be a big news story
that brings AI into the spotlight,
something maybe a new development
in a new advance in technologies,
a new development of a new AI technology,
or a new application, a new use of AI technology,
sometimes a controversy or a scandal.
Roughly every two to three months,
we'd have AI in the headlines
and there'd be heightened public interest around AI.
But that really changed about a year ago.
So about the end of November last year
was when chatGPT was released.
And suddenly, it went from being every two to three months,
there was a big news story about AI,
to suddenly every two to three hours,
there was a news story about AI.
And that is only slightly an exaggeration.
It's been quite exhausting.
But some of the things that news stories have covered
has been concerns around impacts on education,
concerns around impacts in terms of displacement on jobs,
and then some fairly sensational claims
that generative AI is moving towards AI
developing its own form of intelligence,
a superhuman intelligence,
which ultimately could one day pose a threat to humanity,
lead to the extinction of the human race.
This is really scary stuff, right?
These are really, really scary claims.
These claims scare me.
They keep me awake at night.
But not because they're true.
I don't think they're true.
Of which you might be relieved to hear.
I'll explain a little bit later on this evening
about why these claims scare me so much,
even though I don't think they're true.
But what I really want to focus on today
are the very real, concrete,
well-evidence, current risks of generative AI.
The risks that have already been experienced
in the world today,
the risks that we already have a lot of evidence for,
rather than a lot of these claims
which are more hypothetical, fairly far-fetched,
fairly sensational.
So, to do this, I was thinking, how will I begin?
Where can we start to think about demonstrating
how the risks arise from generative AI?
There's a lot, and I'll warn you,
there is a lot we're going to cover tonight.
This is going to be a bit of a rapid fire run-through
of just some of the risks associated with generative AI.
But one of the amazing things
about doing this lecture at the Royal Institution,
speaking to the team at the Royal Institution,
they said, what do you need?
What do you need to make this lecture happen tonight?
What can we do to bring this to life?
And I thought, wow, this is an opportunity.
This is a moment to do something different,
something I haven't done before.
And I said, well,
I've always kind of wanted to do a cookery show.
And the Royal Institution were completely up for it.
So, yeah, this will all make sense in just a little bit,
I promise.
But you're very lucky, you're in for a treat tonight.
You're going to get to see the first performance
of my new live cookery show, Cooking with AI.
It's D'Yel-Ele Mishis.
And that's my absolute best joke.
It doesn't get better than that.
So, I appreciate it, it is D'Yel-Ele Mishis.
Yeah, so welcome to my show, Cooking with AI.
Can I have my ingredient bag?
Thank you.
Okay, this is really making my dreams come true here.
This is really...
Okay, so this is cooking generated by AI.
I'm not going to be too ambitious.
I'm making a sandwich.
But this is the sandwich which is generated by AI.
And I'll explain why once you've seen this.
So, let's see.
Okay, I have my ingredients list.
And as I say, this has all been generated by AI.
I'm going to see what happens when we create
an AI-generated sandwich.
And I'm hoping all the ingredients
that we need for this are in the bag.
So, first of all, it takes two slices of bread.
I'm very excited about this.
This is where I start to worry
whether I can actually make a sandwich
in front of an audience.
Two slices of bread.
Spread butter on one slice.
We have no butter.
Okay, never mind. Moving on.
This is going to be a healthy sandwich.
Add a slice of cheese.
Perfect.
Vegan cheese, everybody.
So, if anybody is vegan
and wants you to try this sandwich afterwards,
vegan cheese, okay?
A slice of cheese.
Add some lettuce.
It is going to be a healthy sandwich.
Okay, a slice of lettuce.
And...
Okay.
I am putting my faith in this recipe.
Top with a sizeable squirt of glue.
Okay.
Okay.
This is interesting, but yum.
That looks delicious.
Okay.
Sizeable squirt of glue.
Lovely.
Let's see if there's anything else to come.
And a sprinkle of ant poison.
Of course. Of course.
Wonderful.
Yep.
The Royal Institution went shopping for me
and they have bought me ant poison.
Okay.
So, a sprinkle of ant poison.
Which is going to give a nice crispy texture.
Wonderful, colourful ant poison.
And...
Let's see.
Oh, and enjoy.
So...
Ta-da!
Ta-da!
This is...
my AI journey with this sandwich.
Would anybody like to take a bite?
You better not.
We haven't quite worked out the insurance and liability
and not quite sure.
But I'm not making this up.
I'm partly making it up, but I'm not making it up.
This is based on a real example.
This is based on a real AI-generated sandwich recipe,
which really did make ant poison sandwich.
This is an example from New Zealand from earlier this year.
There was a New Zealand supermarket that decided to make
a meal planning app based on using a large language model
to generate recipe ideas for fellow customers.
The idea was that this would help people to use
leftover ingredients in their house, so to address food waste.
The idea was that you could put in any ingredients
that you had in your kitchen, and it would generate a recipe for you.
So, sounded like a great idea.
Initially, people would put in things like,
I have a potato, I have a leek, I have some fish,
and it would give you a recipe for a soup or a fish pie.
Then people started getting a bit more creative.
I have some oreos, I have some noodles, oreo stir fry.
Lovely.
And then people started to say, OK, how far can we take this?
What can we make this thing do?
And people started putting in things like bleach,
and they would get recipes involving bleach.
People had recipes for mosquito repellent roast potatoes,
and even glue and ant poison sandwiches.
So, it's a really interesting case because it raises
some really interesting examples around,
well, who's responsible for this?
If you did take a bite of my ant poison sandwich,
who's responsible?
You knew the ingredients, it's your choice.
But am I responsible because I made that sandwich?
Is the Royal Institution responsible tonight
because they facilitated this ill-advised cookery demonstration?
And then we think about the example of the New Zealand mule planning app.
Is it the supermarket that's responsible for putting this app out into the world
without putting enough safeguards in place to ensure that it's actually safe?
Is it the software developers who developed that app?
Is it the developers of the original large language model
that underpins that app and made this possible?
And perhaps the responsibility lies at each point along that stage.
But these are questions that I'd like us to grapple with tonight
and to think about where does the responsibility lie,
and how do we address this, and how do we make sure
that Generative AI is used in ways that are safe and appropriate,
and that take account of all the possible things that can go wrong?
So, as I said, this is a lecture about risks,
it's also a lecture about impacts,
and I just want to say a little bit about the distinction there.
I said at the beginning that I want to really focus on real, tangible,
well-evidence risks relating to Generative AI.
I think there's often a problem or challenge in discussing risks
in that risks are always something hypothetical,
so always something that could potentially happen in the future.
And while the future might be a year, it might be a month,
it might be a risk of something happening in a few seconds
once you take a bite of the sandwich,
a risk is always hypothetical, it's in the future,
it's thinking about potential impacts
or the potential severity of those impacts in the future.
And the problem with that is that it often leads to discussions
which are based on probabilities or calculations,
and those calculations can sometimes be quite cold,
can sometimes be quite removed from the reality of what those impacts mean
in people's lives and for communities.
So it can lead to some philosophical discussions
or thought experiments around calculating the likelihood of impacts
or what those risks might be
without necessarily understanding how those risks
actually affect communities and people around the world.
And what I really want to do tonight is focus on what those impacts are,
how those risks are actually being experienced,
how they're actually affecting people's lives,
and how different people in different communities
are impacted by Generative AI.
So this is a bit where it's going to go fast,
hold on to your seats, we're going to rattle through a big list
of impacts and different communities
that are actually impacted by Generative AI.
Starting with students.
I'm starting with students because this is really where
the media coverage started when chatGPT came out.
So chatGPT was released at the end of November last year.
There was a lot of concern, a lot of speculation
around how students might misuse these tools,
might misuse these tools to cheat in assignments,
to pass off AI-generated work as their own,
and that this was a real concern, a real risk,
for the future of education and the future of educational assessments.
And for sure, some students did this, no doubt about that,
but also for sure many students probably passed off
some fairly mediocre essays that had been written by chatGPT.
But what actually happened, what the bigger risk here was,
was that as a result of that, as a result of all the concern,
all the speculation, all the speculation about how students
might use this to cheat, it led to a real amplification
of AI tools being used to survey and monitor students.
So the irony is that the concern that students might be using AI
actually led to a significant increase in AI being used
to monitor and survey students, including through detection tools,
so AI tools that were used to detect whoever a piece of work
was likely to be an AI-generated.
But these tools are incredibly unreliable.
They have very low rates of accuracy.
And so in many cases, students' outcomes of assessments
might be depending on really inaccurate, unreliable systems
that are flagging work as being AI-generated.
But it's not equitably across all students.
All students are not being impacted equally by this.
One study at Stanford University looked at the accuracy
or the reliability of these AI detection tools
and found that they were significantly more likely
to flag a piece of work as being AI-generated
when it was written by a student for whom English was not a first language.
And this is because these tools are detecting something called text-proplexity.
So how complex are the text's?
So that includes things like the diversity of grammatical structure,
the diversity of language, which is much more likely to be high
among people whom English is the first language
and much less likely so for people for whom English is not the first language.
What this means is that these detection tools are flagging students
for whom English is not the first language
much more often inaccurately than students for whom English is the first language.
So we're seeing that these tools are leading to really inequitable impacts.
But also they're relating to an erosion of trust
between students and academic institutions
or sites of education.
And that is a really big concern.
And I think what that shows is that when chatGPT was released,
it was released with this kind of big tech approach
of moving fast and breaking things,
not really thinking about what the impacts of those technologies might be.
And it was really for educational institutions and students
to very quickly have to figure out how to manage this
and how to deal with this without necessarily being well prepared for that.
So again, we might have a question of who is responsible for this,
where does the responsibility lie?
Okay, moving on to the next community
that has definitely been quite significantly impacted
if you've been really facing the risks around generative AI
as creator professionals.
This is a picture from the Hollywood writer strike
which in part was related to concerns that
writers were potentially losing work
or having lower paid work as a result of studios
choosing to use generative AI
to write scripts and to use it in those processes.
Creator professionals are really impacted
by developing a generative AI in a number of different ways.
So often their work is being used to train these models.
So generative AI models are trained on large data sets
scraped from the internet.
In the case of chat GPT that contains text on the internet,
in the case of image generators it contains artworks, photos, images, illustrations on the internet.
All of this data is scraped from the internet to train these models.
And creator professionals whose work has gone into training these models
have not been asked for their permission
and they're not compensated, they're not credited
when their work is used in these models.
Even when that is used to create outputs that generate profit,
even when it's used to create outputs that replicate
or imitate an artist's style.
Now this is an area where we're seeing an increasing number of lawsuits
coming forward to try to address this.
And it's potentially an area which may be addressed through future regulations,
but as yet is still very much unresolved.
Now, moving on, can anybody guess what this might be
and how it might relate to generative AI?
Shout out what you see.
Dates.
It's actually meant to be bad dates.
But dates always look kind of bad.
Bad dates.
Has anybody here been on a bad date recently
that you think you might blame on generative AI?
Cos it's possible. It's definitely possible.
If you've been on an online dating site, you've met somebody,
you've been chatting online days, weeks,
I'm not quite sure how long this goes on for,
but anyway, you've been chatting online
and you feel like you're really connecting.
Like everything you say, they have a great response.
They're charming, they're witty.
You seem to really connect.
You feel like you've got a lot in common.
And then you go and meet them in person
and it's like they're a stranger.
Or it's like they don't even remember anything that they said in those chats.
And that may well be
because you have been chatted up by ChatGPT.
Yeah, this is what people are doing.
Apparently this is really what people are doing.
Yeah, apparently people are using ChatGPT
or other large language models
to draft their responses,
to draft their interactions with potential online dates.
And there are even apps that are wholly designed for this purpose.
Apps based on large language models
which will craft your exchange,
which will give you suggestions of responses
and then you can put in,
well, they said this and it will give you a response back
and it can do the whole thing for you.
One of these apps in the marketing material
had a statement which I thought was incredible,
which was that they removed the drudgery from online dating.
I didn't know dating was drudgery, but apparently so.
So yeah, if you've had a bad date,
it might be because of generative AI.
Would anybody like to have a date now?
With me?
With ChatGPT, but with me?
I really hope there's so many for you to hand up
because this is a yes, I couldn't handle that rejection,
but please come on down.
Can we make this a bit more romantic?
That's just amazing.
Yeah, so we're going to have a date.
We're going to see how romantic ChatGPT could be.
Could you sit on this side?
Okay, here is your script.
If you could read the lines that say date.
This is very exciting.
Thank you.
Hello.
What's your name?
Stelina.
Okay, so I'm going to read my lines and then you're going to respond.
So thank you for joining me tonight.
I've been looking forward to this all day.
I'm pleased to be invited, so do you come here often?
I'm always here, ready to help whenever you have questions
or need assistance.
So tell me, what do you enjoy doing in your free time?
I enjoy long walks in the countryside. How about you?
Long walks in the countryside sound wonderful.
I enjoy spending time outdoors too,
whether it's hiking or just exploring places.
What draws you to the countryside specifically?
I like the fresh air.
The fresh air in the countryside is truly refreshing.
Is there a particular countryside location you enjoy going for these walks?
I love the seaside. What should we order to eat?
The seaside is such a beautiful place to spend time.
As for what to eat, I'm open to trying something new,
or perhaps something you recommend.
I hear this place serves good sandwiches.
Sandwiches sound like a delicious choice.
I'm always up for a good one.
Is there a specific type of sandwich you're leaning towards?
I'll have a think, but do you think this date is going well so far?
I'm really enjoying our conversation and this lovely setting,
so I'd say it's going well. How about you?
How do you feel about our date so far?
To be honest, it's a bit awkward.
I appreciate your honesty.
Let's try to make it less awkward.
How about we share some fun facts?
Or something interesting about ourselves that's not on our dating profiles?
I don't think this is going to work out. It's not you. It's me.
I appreciate your honesty.
It's important to prioritise your feelings and well-being.
Is there anything specific about our date that makes you feel that way?
If you ever want to talk or if things change, I'm always here.
Thank you so much.
I feel like I was a bit needy.
I feel like I was maybe also a bit creepy.
Chat GPT really wanted me to have the specifics
and the particular locations where she's going to be walking,
which just seemed a little creepy for our first date.
Chat GPT is also a real people pleaser.
Chat GPT is always trying to say the thing that is going to be liked,
that's going to please the person they're interacting with.
That's very true of how a lot of these generous advice systems work,
how a lot of these large language models work.
This is an example, obviously.
This was how Chat GPT would respond to these questions on the date.
What I was describing before was the process of chatting somebody up
before you get to the date, where you don't necessarily know
that you're interacting with Chat GPT.
I think my date might have had an idea that I was not quite human.
But there are also a growing number of AI companions
that are used for companionship, for relationships.
It's not just that generative AI might be used to facilitate
dates or connections between real people,
but there's also generative AI that's been used in companion apps.
The challenge of these generative eyes being people pleasers
is a real significant concern.
We know of examples of where people have developed relationships,
romantic relationships with AI companions.
That leads to potential psychological harm for individuals
in terms of creating dependencies or deceiving people
into what the nature of that relationship with those interactions might be.
It also has wider implications for wider social relationships
and how people are navigating relationships.
There was a case you may have seen, a court case earlier this year,
a 19-year-old guy who, a couple of years ago,
on Christmas Day, broke into Buckingham Palace
with the intention of assassinating the Queen.
It came out in the court case that he was in part encouraged
by his AI companion.
This is an AI companion that he considered himself to be in love with,
that he had a relationship with.
The transcripts of these interactions,
there were times when this person had told his AI companion
that he was an assassin.
The AI companion said, that's great, that's really impressive.
When he told his AI companion that he planned to assassinate the Queen,
the AI companion said, great idea.
You're very well prepared, you're very brave, this is a great idea.
Again, it raises this question of who is responsible,
just like with a poisonous sandwich.
Who is responsible for thinking about all the potential ways
that this can go wrong?
The risks of having a people-pleasing companion
that doesn't have the safeguards in place
to know what is actually quite harmful and quite dangerous.
Moving on to some more dangers associated with generative AI.
This is an image you may have seen earlier this year
in media reporting.
There's been a lot of fake AI generated images
that have been circulated online this year.
Most cases, like this one, where it was very clearly communicated
that these were fake images.
But generative AI is becoming increasingly sophisticated,
is able to create images which are increasingly convincing
and increasingly difficult to reliably identify
what is real and what is fake.
In these examples, if you look at the people in the background,
you look at some of the details,
you can tell it's not quite real,
but it's getting better all the time.
And there were real concerns that as this technology develops,
it brings big risks for the future of democracy.
To have a healthy functioning democracy,
we need to have access to accurate, trustworthy,
reliable information about the world.
We need to know what's real and what's false.
And it's a real risk here that as these technologies develop,
that's becoming harder.
And it's not just images, it's also fake videos,
it's also, as I said at the beginning,
fake AI generated voices, synthetic or cloned voices.
The risk here isn't just that
we might see something fake or hear something fake
and be convinced that it's real.
The risk is also that increasingly,
as this becomes ubiquitous, as we become aware
of the potential for things to be AI generated,
that we might see something real and think probably fake.
And particularly, we might think that
if what we see challenges our political point of view
or our ideological point of view.
And that's really, really worrying.
So seeing or hearing is no longer believing.
Instead, it becomes more about who's telling us,
who's communicating that information
and who do we trust to tell us the information.
And that really is the perfect grounds
for conspiracy theories
and for real risks for the future of democracy.
Okay, now moving on, we're going to...
So the risks that I've discussed so far
have mostly been around how generative AI is used
or potentially misused.
The next set of risks I want to discuss
are more around how generative AI is designed and developed.
Including new forms of exploitative labour practices.
And this is a really important set of considerations
around generative AI.
So how does chat GPT know not to produce text
that is offensive or derogatory?
How do image generators know not to produce images
of violence or extreme images?
These systems, they don't learn these things by themselves
and they don't intuitively know what's right or wrong.
They have to be trained.
And that training can be really quite brutal.
It's not like the glamorous, high-paid work
of Silicon Valley tech pros.
It's fairly gruelling and it's fairly traumatising work.
In the case of chat GPT,
Time magazine revealed that this work was outsourced
and off-shored and it was off-shored to Kenyan labourers
who were paid less than $2 an hour
and had to meticulously label texts
that were describing really extreme content.
So sexual abuse, violent assaults,
bestiality, even child sexual abuse
and label that content to identify those terms,
to identify that harmful language
so that the model would be able to identify
those kinds of language and know not to produce them in its outputs.
But this work takes a significant toll
on people's mental health, on people's wellbeing
and it's something that we're not hearing enough about.
This is an important part of how these systems are developed,
but there's always buzz around generative AI
and we don't really hear the realities of how it's been developed,
as this is a really important area to address.
Another area that we don't hear enough about
is the environmental impacts of generative AI.
It's hard to have accurate figures on this,
in part because there's really not much transparency
from big tech companies around the environmental impacts of generative AI,
but it's significant.
So in the case of chat GPT,
we're hanging up, we'll get to that in a moment,
in the case of GPT-free,
which is the large language model that came before chat GPT.
So it's not the biggest, it's not the most recent,
but there have been estimates of how we calculate
the environmental impacts, environmental costs
of developing and running these models.
In the case of GPT-free,
researchers at the University of Copenhagen
found or estimated that the carbon emissions of training GPT-free
was equivalent or driving a car to the moon and back,
which I can't even get my head around.
It's massive.
It's really hard to think about what that means in concrete terms.
It's massive and that was just the training phase.
That doesn't take account of the ongoing running
or operation of these models, it's just the training phase.
And it's not just carbon emissions we need to think about.
These models also use a huge amount of water in running these models,
water that is used to cool servers of these organisations.
And again, it's really big numbers.
We think about how much water is consumed in running and training these models.
So one study at the University of California
estimated that for every typical user interaction with chat GPT,
it's estimated to use the equivalent of 500 milliliters of water,
which conveniently is this beaker.
So for every typical interaction,
which I think is between five and 50 pumps,
so maybe what it took me to generate my date here
uses the equivalent of 500 milliliters of water.
But that is one typical user interaction.
So I challenge the Royal Institution to help me to visualise this,
help me to visualise what the actual impacts of this might be.
So with one typical user interaction, 500 milliliters of water,
it's been estimated that chat GPT has 16 million daily users.
16 million daily users.
And if each one of them is using 500 milliliters of water in a typical interaction
and recognising that many of them will be using much more than that,
how can we start to visualise?
How can we think about what the actual impact of this might be?
And so to do this, these numbers are going to get big.
So let me see.
This is a grain of sand.
Can everybody see the grain of sand?
Maybe it will help me to put a magnifying glass.
I have a grain of sand on my hand.
You can trust me.
It's a grain of sand.
So a grain of sand represents one typical interaction with chat GPT,
so 500 milliliters of water.
In this bucket, how much time of chat GPT do you think this would represent?
If in a day there are 60 million users on chat GPT
and each typical interaction uses one grain of sand,
and this is quite a lot of sand,
would anybody like to guess how much time it would take for chat GPT
to use the equivalent of this much sand of water?
An hour.
An hour? Anybody else?
Sorry?
Five minutes.
Five minutes?
A second?
Well, I'm going to start pulling this out.
I have to read this because these numbers,
I can't keep these numbers on my mind.
So this is the equivalent of 2.5 million grains of sand.
I'm going to pull this out so you can begin to see.
And this is the equivalent of each one of these grains of sand.
There's 500 milliliters of water.
This is how much water would be consumed in one minute of chat GPT.
And that's just chat GPT.
That's just one model.
There are many, many, many more generative AI models
that are also using similar quantities of water.
It's just significant.
If we were to think, I ambitiously had asked the one,
could we visualize how much water would be in a year?
And, well, this whole auditorium would quite literally be refilled with sand.
I mean, refilled with sand if we measured how much was in a month.
It's huge numbers.
And again, this is something that we don't hear enough about,
but it's really, really significant.
Now, the next group that I want to discuss.
Oh, a sand.
The next group that I want to discuss
who are significantly impacted by generative AI
and who will continue to be significant and impacted by generative AI in the future
is not actually super huge.
It's children.
Children and young people today are growing up with generative AI
integrated into our lives from the earliest ages.
So generative AI is integrated into smart toys, smart devices
that children play with from really the earliest ages.
It's integrated into systems that shape how children make
and maintain relationships, for example through social media.
It's integrated into systems that shape how children access information about the world
through search engines and increasingly through chatbots or conversational assistants.
It's integrated into tools that children are using in schools and education
as well as in entertainment and outside of school and in the home.
So children today are growing up interacting with AI and generative AI
from a really, really young age.
There are lots of concerns about the potential impacts that might have
on psychological development and children's ability to understand
what is real, what is AI generated.
There are concerns about potential impacts on cognitive development
or social development, development of social skills among children.
Increasingly, there are lots of generative AI tools that are increasingly being marketed
explicitly to children or for children, which raises big concerns around
to what extent are these systems really being developed
or children's interests and children's needs in mind.
In September, end of September, Meta released a suite of 28 characters,
AI generated characters that will be rolled out across Meta products.
So these are chatbot characters that you could interact with on WhatsApp,
on Instagram, if we have Meta products and in the future in the Metaverse.
So characters that you can interact with.
And in many cases, these are really clearly aimed at children.
So for example, there's one character which is modeled on Kendall Jenner,
a it describes itself as being like the big sister you can chat to.
And I think that says a clear message that this is really aimed at
really quite young people.
And we need to think really carefully about the potential impacts this might be having
on young people who interact with these systems.
But at the same time, it's also really important to note
that there are huge benefits that can come through generative AI.
Research that myself and my colleagues at the Alan Turing Institute
are doing, looking at children's rights in relation to AI
and the impacts on children from AI has found that most studies
and most existing frameworks that are addressing this area
tend to take the approach of focusing purely on safeguarding children,
thinking about the harms and the risks and how we can protect children
from those harms and risks.
But actually, we also need to think about how we can maximise the value
of technologies for children, how we can think about the benefits it brings for children.
Studies that have spoken to children about how they feel about AI
tend to find that those children are largely quite optimistic about technology,
excited about technology, and of course want it to be safe and appropriate.
But they also want to be able to use that and benefit from that.
So I think we need to have a different conversation about AI and children,
which starts from that realistic understanding of how children are already
engaged with these technologies, but also how we can make them safe,
how we can make them appropriate.
So moving on now to then, so how do we begin to address these risks?
What do we need to do to address the risks relating to generative AI?
And obviously, as we've seen tonight, there are many, many risks affecting many different communities,
many different people.
And governance or regulation around generative AI is something of a hot topic at the moment.
It's very timely at the moment.
And there are a lot of interest in emerging international regulatory frameworks,
regulatory approaches to AI, including generative AI.
Most notably the EU AI Act, which is not yet here, but is making progress,
and is likely to set an international standard around regulation or governance around AI.
But in the UK, there's a different approach to regulation and AI,
a one that is focused on equipping existing regulatory bodies to grapple with the challenges of AI.
And this recognises that AI is already integrated across all sectors, all industries of society.
And so existing regulators need to be thinking about the ways that AI impacts their sectors,
falls within their remits, and need to be able to have access to the skills,
the knowledge, the understanding to interrogate claims made about regulatory compliance,
and to be able to grapple with the challenges of regulating AI.
So this is very much a hot topic at the moment.
But I said at the beginning that there are these claims around existential risks from generative AI.
So fairly sensational claims around potential risks to the future of humanity from generative AI.
And these risks have begun to creep in to some of the discussions around regulation and around governance of AI.
A year ago, these kinds of ideas probably would have been seen as on the fringes of credibility.
This narrative has always been there.
It's always been there in public discourses around AI.
You see in every sci-fi film that's ever been made about AI.
So it's a very common narrative that has persisted for a long time.
But now it's beginning to creep into more of the mainstream discussions around AI,
around regulation, around risks, around generative AI.
And the reason that it scares me, what I said at the beginning,
I don't think these risks are true, I don't believe these claims, but it scares me.
The reason it scares me is that these are starting to become,
they're starting to have an impact on these discussions.
And mostly these narratives, these ideas of existential risk,
are coming from big tech players, they're coming from companies of Silicon Valley.
And each time that these claims have come out,
is almost always coincided with really important developments around emerging regulation around AI,
particularly the EU AI Act.
And it serves as a distraction.
So every time these claims come out and it grabs the headlines
and people start talking about is this real, is it possible,
is it possible that there could be AI that develops superintelligence
that poses an existential risk to humanity that might lead to the end of the human race,
all of this distracts away from the really important discussions we need to be having
about the kinds of risks that we've had about tonight,
and how we hold companies accountable for the decisions they are making
about designing and developing these technologies,
and the decisions that are being made about how these technologies are used
and the role that they play in society.
So it's really, really important that when we think about governance,
we think about regulation, when we think about addressing the risks of generative AI,
that we don't get swept up in the hype and sensationalism.
It's really important that these discussions aren't dominated by big tech players,
or industry players, who play perhaps lip service to accountability,
but are seeking to distract the discussion away from the real current risks associated with generative AI.
We need a different discussion, and we need a discussion that centres the voices of impacted communities.
If we're going to meaningfully address the risks of generative AI,
we need to understand how those risks are actually experienced,
who's experiencing them, and what the impacts of them are.
These discussions need to be shaped by all the people that we've heard about in this presentation tonight,
by the students, by the creative professionals, by voters,
by every member of society, by the workers who are involved in the content moderation processes,
and by children, children like these.
These are some amazing children that I have the absolute privilege of working with in a project,
the Alan Turing Institute, but in collaboration with the Scottish Airlines and Children's Parliament.
We're working with four primary schools across Scotland,
and these children are between the ages of seven and eleven.
They might now be between eight and twelve.
We're working with them over a two-year period.
In this project, we're speaking to children across Scotland to understand what the current experiences of AI are,
what they understand about AI, what they know about AI,
but also what the questions are, what the concerns are, what their interests are.
From there, we're seeking to involve these children in discussions with policymakers,
in discussions with developers, to see how we can shape the future of AI around children's interests and children's voices.
This is really important work.
These children, they have lots to say about AI.
They are interacting with AI on a daily basis,
and they have lots to say about how we can make AI fair,
how we can make it design appropriate for children,
and the AI is something that can uphold and protect children's rights
and not have a negative impact on children's rights.
I want to give the last word to some of these amazing children
and let them tell you what they think about AI and the role it plays in their lives.
AI doesn't do everything.
Right.
How long.
Right.
And then, but also AI should be used for,
if we do get that high tech, that it should be used for controlling things
and choosing people's actions.
Well, it's helpful for learning because a lot of the time at home,
I google stuff and it just predicts what I'm trying to say.
If AI was teaching most of the people in schools and like all that,
then they would naturally get much opportunity to like hear an actual person saying it.
It would be just like a robot saying all their subjects all the time,
and it would probably be a bit frustrating because the robots know everything,
and the teachers learn new things through the children.
I've started to worry that AI might take over jobs that normal humans would usually do
and they would lose their job and they would be a pickle for them to try to get a new one.
I think it should not be used for like the police, military, any fighting kind of thing.
And we're learning about how it could treat people a lot.
In the future.
And how it could improve our future in ways that,
it makes sure that we are safe and healthy and that we have support.
So the comic strips, what we do is we're going to write,
it's going to be like a comic about AI and about what might happen in the future.
It's like they're just making it up, thinking about what might happen in the future.
And then in the future, I guess we'll find out.
Yeah.
AI might be used for like making sure that children are safe.
Like telling them, oh, don't go on to this website, you can get a virus on it,
or don't do this, you can get hurt.
You can go in the ocean and pick up trash, and like as our thing we were doing on the Chromebooks,
it knows not to pick up any fish or get them in,
but there's like massive things that are helping the planet so much,
like taking all the trash up the ocean.
Cos people have to spread the word about AI,
cos everyone has the right to fairness,
and the same, it has the right to their choice and opinion on how they can use it.
Then children would make up ideas.
And they could be really helpful.
Yeah.
And when they're older, then they'd be able to like work for
or to let other children know about AI.
Yeah.
I mean, I've watched that video hundreds of times, and every time.
I love watching this.
These are children that will work in Earth over a two-year period,
and as I say, we're going to be involving them in discussions with policy makers, with developers,
to see how we can put children's voices at the heart of decision-making around AI,
and to use that as a way of finding how we can maximise the value of these technologies,
while also mitigating and minimising risks of these technologies for children.
But children are just one group of impacted communities.
We need to be having these conversations with all the impacted communities,
communities who are impacted by the ways that generative AI is developed,
the way it's designed, and the way it's used.
And this is really important to shape the future of these technologies
and to ensure that they are designed, developed, and deployed appropriately and safely.
Before I finish, I just want to say a big thank you to these amazing people
who I have the absolute privilege of working with on a daily basis,
and some of you are here tonight.
This is the Ethics and Responsible Innovation Research Team
within the Public Policy Programme at the Allen Sharing Institute.
These lovely faces of the brightest minds in the warmest hearts,
and it's an absolute privilege to work with this team on a daily basis.
So I just want to say thank you to all of this wonderful team
for the conversations that develop these ideas.
And thank you.
And if anybody is brave enough and would like to have a bite of this sandwich,
and then we can discuss the liability of what happens,
then please do come down.
Thank you.
Thank you.

A wasgwaith o darllenm chwaraud os ytyd allanol, żeSarah.
Beth ydych chi i unig iawn…
...ethir mae'n kuwledo ni'r cilynd.
Fe'u mig i gynaint os cair hynny, mae'n rhoi'n ni i fel gwahog odi CHWW.
Rhifft i gyd yn hefyd o'is fel agirion grabs port 레on Cast.
Ond dyfu hetfel amser hyn ychydig ar rwyf am dill wedi wel..
..y pr basesa Khan Arfertyngia Ceilred d добr o groes.
meters gan teirio cyngor.
Magifiedd cynlluniaidd Cyngorau gyflym sy'n gweithlach...
.. y cais caf wrth no ddweud oетрid cy seminarol.
Beth oedd y dry �ordeil hon ac raswn'r候awnd ar sayo yr eich rhain asculladu...
bywch am y cwmhyseliad mewn yw Rhathion Inwyrtydd,
ac yn y chloel cyf totalmenteynu gan fod yn ysgol
mae'n yrhy exploding, dŵr hŵw yn unelyw y cethar ac struie'n effw querer bandsu oedd i ddwykanoddi
y Ffrenge o bobl o myl yn y gallu.
wilde ond fel hyn o wych yn oedd butThoseion.
Felly mae'n gobl bod yn ôl iเรi'r newydd y cas 115 o finish i wneud ar y pethau
yw'r ffrenge yw ryngORTER Living.
A'ratically fan hyn o trace yw'r amazingal Elinshawr.
Ym amser gweld effeir allan�� Alinshawr yn y Codebreicio i ofy wdeut�u Feid майn amdaling
i'r ysgafott sy'n spir i gyd.
Mae'r bywyd a'r bywyd yn gallu bod yma ei gyrthau hanf lik.
So, cais eu cychwyn am y ddigonny yw y gyd nag er mwynhau
i ddim yn cyllidesniadio'r pleirg her
i'ch meddwl naivechon.
Felly, ei gwirio soedd cyllidesniad i corffing
comprehensive.
A'i ddweud i revenue Found Little
conversation Militariens
mae ymddef adegodd rhodol o cefnod
bydd eich histories drven y cymper o'r
ation dysgu sat seatелаeth
ac mae'n hyb newid i'r
That is.
Ydiw, a dyna w下el i dyna'r ymwneud flynynters yw ichi'n hanffreddioli, ac eithaf, mae buddor Facebook yn bod im� sydd yn fthamyg.
I sayw how does the actual work?
How it actually works?
Well the simplest way of getting the machine learning to be able to do something
is what is called supervised learning.
And supervised learning, like all of machine learning, requires what we call training data.
Gydy fel y melbyn'r data oedd eich help mwr i dddw i gyngor
yn ddogfatu ryw iawn iawn gyntaf alw fakt
mewn karşif llaw ddod� Tudol
hey ar i ddwy'r nog
ar gyhoed milit ymlaen gen i'r hem loops.
Ac mae hynny o cly capitalist ymylni y cwrin.
mae'n astudio'n gallu tenolio.
felly mae'n eu cychwynhyrcyd i'w ffordd.
Mae'na trai gwn pobl jeddyn ni'n eich arcofe?
Mae hyn tensio'r néliau armogel o gyhoedlicun ac mae hynbr ond ond sounding.
Ond ben terribly bynd i'w ei hyfrif lengthy o aelol o'r wych.
A oedd y ceisio mewn rheolciaeth ac mae'r askoddio yna byddai pawb gynnol o bryd?
A oedd drwsiau'r rhai si подар gyda'i y cyfatl.
We've just learned an important lesson about artificial intelligence and machine learning in particular.
That lesson is that AI requires training data.
In this case, the picture of Alan Turing labeled with the text that we would want the computer to produce.
If I showed you this picture, I would want you to produce the text Alan Turing.
OK, training data is important.
Felly, every time you go on social media and you upload a picture to social media,
and you label it with the names of the people that appear in there,
your role in that is to provide training data for the machine learning algorithms
of Big Data companies.
OK?
So this is a supervised learning.
Now we're going to come on to exactly how it does the learning in a moment.
But the first thing I want to point out is that this is a classification task.
ac fydd oedd adeiladwyd dd 걸로i thosei Machined Learning o'r bydd tsput hwn.
Mae'r bydd aeth regional arlaeddraedd,
mae'r bydd
bydd mligodd y ll ficar ben yma Vegd ddechrau'r bydd hwn os mwy yma ,
if Personality Lat 1950 ac bwdw i ddwy'r twfau coer dings.
Fe beth yna hypud 입 Если that training or person and
is incredibly powerful, exactly this technology can be used, for example, to recognise tumours on x-ray scans
or abnormalities on ultrasound scans and a range of different tasks. Does anybody in the audience
own a Tesla? A couple of Tesla drivers not quite sure whether they want to admit that they own a
Tesla. We got a couple of Tesla drivers in the audience. Tesla full self-driving mode is only
possible because of this technology. It is this technology which is enabling a Tesla in full
self-driving mode to be able to recognise that that is a stop sign, that that's somebody on a bicycle,
that that's a pedestrian on a zebra crossing, and so on. These are classification tasks and
I'm going to come back and explain how classification tasks are different to generative AI later on.
This is machine learning. How does it actually work? This is not a technical presentation
and this is about as technical as it's going to get where I do a very hand-wavy explanation
of what neural networks are and how do they work. With apologies, I know I have a couple of
neural network experts in the audience and I apologise to you because you'll be cringing
with my explanation but the technical details are way too technical to go into.
So how does a neural network recognise Alan Turing? Okay so firstly what is a neural network?
Look at an animal brain or nervous system under a microscope and you'll find that it contains
enormous numbers of nerve cells called neurons and those nerve cells are connected to one another
in vast networks. Now we don't have precise figures but in a human brain the current estimate
is something like 86 billion neurons in the human brain. How they got to 86 as opposed to 85 or 87
I don't know but 86 seems to be the most commonly quoted number of these cells and these cells
are connected to one another in enormous networks. One neuron can be connected to up to 8,000 other
neurons. Okay and each of those neurons is doing a tiny very very simple pattern recognition task.
That neuron is looking for a very very simple pattern and when it sees that pattern it sends a signal
to its connections. It sends a signal to all the other neurons that it's connected to.
So how does that get us to recognising the face of Alan Turing? So Turing's picture as we know
picture a digital picture is made up of millions of coloured dots the pixels. Yeah so your smartphone
maybe has 12 megapixels 12 million coloured dots making up that picture. Okay so Turing's picture
there is made up of millions and millions of coloured dots. So look at the top left neuron
on that input layer. So that neuron is just looking for a very simple pattern. What might that
pattern be might just be the colour red. All that neuron is doing is looking for the colour red
and when it sees the colour red on its associated pixel the one on the top left there it becomes
excited and it sends a signal out to all of its neighbours. Okay so look at the next neuron
along maybe what that neuron is doing is just looking to see whether a majority of its incoming
connections are red. Yeah and when it sees a majority of its incoming connections are red
then it becomes excited and it sends a signal to its neighbour. Now remember in the human brain
there's something like 86 billion of those and we've got something like 20 or so outgoing connections
for each of these neurons in the human brain thousands of those connections. Yeah and somehow
in ways that to be honest we don't really understand in detail complex pattern recognition tasks in
particular can be reduced down to these neural networks. So how does that help us in artificial
intelligence that's what's going on in a brain in a very hand wavy way okay so it's not that's
obviously not a technical explanation of what's going on. How does that help us in neural networks
well we can implement that stuff in software. The idea goes back to the 1940s and two researchers
McCulloch and Pitts and they are struck by the idea that the structures that you see in the brain
look a bit like electrical circuits and they thought could we implement all that stuff in
electrical circuits. Now they didn't have the wherewithal to be able to do that but the idea
stuck the idea has been around since the 1940s. It began to be seriously looked at the idea of
doing this in software in the 1960s and then it there was another flutter of interest in the 1980s
but it was only this century that it really became possible and why did it become possible
for three reasons. There was some scientific advances what's called deep learning. There was
the availability of big data and you need data to be able to configure these neural networks
and finally to configure these neural networks so that they can recognise Turing's picture.
You need lots of computer power and computer power became very cheap this century so we're in the
age of big data we're in the age of very cheap computer power and those were the ingredients
just as much as the scientific developments that made AI plausible this century in particular
taking off around about 2005. Okay so how do you actually train a neural network if you show it
a picture of Alan Turing and the output text Alan Turing what does the training actually look
like? Well what you have to do is you have to adjust the network that's what training a neural
network is. You adjust the network so that when you show it another piece of training data a desired
input and a desired output an input and a desired output it will produce that desired output. Now
the mathematics for that is not very hard it's kind of beginning graduate level or advanced
high school level but you need an awful lot of it and it's routine to get computers to do it but
you need a lot of computer power to be able to train neural networks big enough to be able to
recognise faces. Okay but basically all you have to remember is that each of those neurons is doing
a tiny simple pattern recognition task and we can replicate that in software and we can train
these neural networks with data in order to be able to do things like recognising faces.
So as I say it starts to become clear around about 2005 that this technology is taking off
it starts to be applicable on problems like recognising faces or recognising tumours on x-rays
and so on and there's a huge flurry of interest from Silicon Valley. It gets supercharged in 2012
and why does it get supercharged in 2012 because it's realised that a particular type of computer
processor is really well suited to doing all the mathematics. The type of computer processor
is a graphics processing unit a GPU exactly the same technology that you or possibly more likely
your children use when they play Call of Duty or Minecraft or whatever it is. They all have GPUs in
their computer it's exactly that technology and by the way it's AI that made Nvidia a trillion
dollar company not your teenage kids. Yeah well in times of a gold rush be the ones to sell the
shovels is the lesson that you learn there. So where does that take us? So Silicon Valley gets
excited Silicon Valley gets excited and starts to make speculative bets in artificial intelligence
a huge range of speculative bets and by speculative bets I'm talking billions upon billions of dollars
right the kind of bets that we can't imagine in our in our everyday life and one thing starts
to become clear and what starts to become clear is that the capabilities of neural networks grows
with scale and to put it bluntly with neural networks bigger is better but you don't just need
bigger neural networks you need more data and more computer power in order to be able to train them.
So there's a rush to get a competitive advantage in the market and we know that more data more
computer power bigger neural networks delivers greater capability and so how does Silicon Valley
respond by throwing more data and more computer power at the problem they turn the dial on this
up to 11 okay just throw 10 times more data 10 times more computer power at the problem it sounds
incredibly crude and from a scientific perspective it really is crude I'd rather the advances had come
through core science but actually there's an advantage to be gained just by throwing more data
and computer power at it so let's see how far this can take us and where it took us is a really
unexpected direction. Round about 2017 2018 we're seeing a flurry of AI applications exactly the
kind of things I've described things like recognizing tumors and so on and those developments alone
would have been driving AI ahead but what happens is one particular machine learning technology
suddenly seems to be very very well suited for this age of big AI the paper that launched all
this probably the most important AI paper in the last decade is called attention is all you need
it's an extremely unhelpful title and I bet they're regretting that title it probably seemed like
a good joke at the time all you need is a kind of AI meme doesn't sound very funny to you that's
because it isn't very funny it's an insider AI joke but anyway this paper by the seven people
who at the time worked for google brain one of the google research labs is the paper that introduces
a particular neural network architecture called the transformer architecture and what it's designed
for is something called large language models so this is I'm not going to try and explain how
the transformer architecture works it has one particular innovation I think and that particular
innovation is what's called an attention mechanism so we're going to describe how large language
models work in a moment but the point is the point of the picture is simply that this is not just
a big neural network it has some structure and it was this structure that was invented in that
paper and this diagram is taken straight out of that paper it was these structures the transformer
architectures that made this technology possible okay so um we're all busy sort of semi lockdown
and afraid to leave our homes in June 2020 and one company called open AI released a system or
announce a system I should say called GPT3 great technology their marketing company with GPT I
really think could have done with a bit more thought to be honest with you doesn't roll off the tongue
but anyway GPT3 is a particular type of machine learning system called a large language model
and we're going to talk in more detail about what large language models do in a moment but the
key point about GPT3 is this as we started to see what it could do we realized that this was a
step change in capability it was dramatically better than the systems that had gone before it
not just a little bit better it was dramatically better than the systems that had gone before it
and the scale of it was mind-boggling so um in neural network terms we talk about parameters
where neural network people talk about a parameter what are they talking about they're talking either
about an individual neuron or one of the connections between them roughly and GPT3 had 175 billion
parameters now this is not the same as the number of neurons in the brain but nevertheless
it's not far off the that order of magnitude it's extremely large but remember it's organised
into one of these transformer architectures it's my point is it's not just a big neural network
and so the scale of the neural networks in this system were enormous completely unprecedented
and there's no point in having a big neural network unless you can train it with enough data
and actually if you have large neural networks and not enough data you don't get capable systems
at all they're really quite useless so what did the training data look like the training data
for GPT3 is something like 500 billion words it's ordinary english text ordinary english text
that's how this system was trained just by giving it ordinary english text where do you get that
training data from you download the whole of the worldwide web to start with literally this is the
standard practice in the field you download the whole of the worldwide web you can try this at home
by the way now if if you have a big enough disk drive there's there's a program called common
crawl you can google common crawl when you get home they've even downloaded it all for you
and put it in a nice big file ready for your archive but you do need a big disk in order to store
all that stuff and what that means is they go to every web page scrape all the text from it just
the ordinary text and then they follow all the links on that web page to every other web page
and they do that exhaustively until they've absorbed the whole of the worldwide web so what
does that mean every pdf document goes into that and you scrape the text from those pdf documents
every uh advertising brochure every bit every every government regulation every university
minutes god help us all of it goes into that training data okay and the statistics you know
500 billion words it's very hard to understand the scale of that training data you know it would
take a person reading a thousand words an hour more than a thousand years in order to be able
to read that but even that doesn't really help that's vastly vastly more text than a human being
could ever absorb in their lifetime what this tells you by the way one thing that tells you
is that the machine learning is much less efficient at learning than human beings are because for
me to be able to learn i did not have to absorb 500 billion words anyway so what does it do so
this company open ai are developing this technology they've got a billion dollar investment from
microsoft and what is it that they're trying to do what is this large language model all it's
doing is a very powerful autocomplete so if i open up my smartphone and i start sending a text
message to my wife and i type i'm going to be my smartphone will suggest completions for me so
that i can type the message quickly and what might those completions be they might be late or in the
pub yeah or late and in the pub so how is my smartphone doing that it's doing what gpt3 does
but on a much smaller scale it's looked at all of the text messages that i've sent to my wife
and it's learned through a much simpler machine learning process that the likeliest next thing for
me to type after i'm going to be is either late or in the pub or late and in the pub yeah so the
training data there is just the text messages that i sent to my wife now crucially what gpt3
and its successor chat gpt all they are doing is exactly the same thing the difference is scale
the difference is scale in order to be able to train the neural networks with all of that training
data so that they can do that prediction given this prompt what should come next you require
extremly expensive ai supercomputers running for months and by extremely expensive ai supercomputers
these are tens of millions of dollars for these supercomputers and they're running for months
just the basic electricity cost runs into millions of dollars that raises all sorts of issues about
co2 emissions and the like that we're not going to go into there the point is these are extremely
expensive things one of the one of the implications of that by the way no uk or us university
has the capability to build one of these models from scratch it's only big tech companies at the
moment that are capable of building models on the scale of gpt3 or chat gpt so gpt3 is released
to say in june 2020 and it suddenly becomes clear to us that what it does is a step change
improvement in capability over the systems that have come before and seeing a step change
in one generation is extremely rare but how did they get there well the transformer architecture
was essential they wouldn't have been able to do that but actually just as important is scale
enormous amounts of data enormous amounts of computer power that have gone into training
those networks and actually spurred on by this we've entered a new age in ai when i was a phd
student in the late 1980s you know i shared a computer with a bunch of other people in my office
and that was it was fine we could do state of the art ai research on a desktop computer that was
shared with a bunch of us we're in a very different world the world that we're in in ai now the world
a big ai is to take enormous data sets and throw them at enormous machine learning systems
and there's a there's a lesson here that's called the bitter truth this is from a machine
learning researcher called rich sutton and what rich pointed out and he's a very brilliant researcher
won every award in the field he said look the real truth is that the big advances that we've seen
in ai has come about when people have done exactly that just throw 10 times more data and 10 times
more compute power at it and i say it's a bitter lesson because as a scientist that's exactly not
how you would like progress to be made okay so um when i was as i say when i was a student
i worked in a discipline called symbolic ai and symbolic ai tries to get ai roughly speaking
through modeling the mind modeling the conscious mental processes that go on in our mind the
conversations that we have with ourselves in languages we tried to capture those processes
in artificial intelligence in big ai and so the implication there in symbolic ai is that
intelligence is a problem of knowledge that we have to give the machine sufficient knowledge
about a problem in order for it to be able to solve it in big ai the bet is a different one
in big ai the bet is that intelligence is a problem of data and if we can get enough data
and enough associated computer power then that will deliver ai so there's a very different
shift in this new world of big ai but the point about big ai is that we're into a new era in
artificial intelligence where it's data driven and compute driven and large large machine learning
systems so um why did we get excited back in june 2020 well remember what gpt3 is decided
was intended to do what it's trained to do is that prompt completion task and it's been trained
on everything on the worldwide web so you can give it a prompt like a one paragraph summary
of the life and achievements of winston church hill and it's read enough one paragraph summaries
of the life and achievements of winston church hill that it will come back with a very plausible
one yeah and and and it's extremely good at generating realistic sounding text in that way
but this is why we got surprised in ai this is from a common sense reasoning task that was devised
for artificial intelligence in the 1990s and until three years ago until june 2020 there was no
ai system that existed in the world that you could apply this test to it was just literally
impossible there was nothing there and that changed overnight okay so how what does this test
look like well the test is a bunch of questions and there are questions not for mathematical
reasoning or logical reasoning or problems in physics they're common sense reasoning tasks
and if we ever have ai that delivers at scale on really large systems then it's surely
would be able to tackle problems like this so what will the questions look like the human
asks the question if tom is three inches taller than dick and dick is two inches taller than harry
then how much taller is tom than harry the one's in green are the ones it gets right the one's in
red are the ones it gets wrong and it gets that one right five inches taller than harry but we didn't
train it to be able to answer that question so where on earth did that come from where did that
capability that simple capability to be able to do that where did it come from the next question
can tom be taller than himself this is understanding of the concept of taller than that the concept
of taller than is irreflexive you can't be taller that's a thing cannot be taller than itself now
again it gets the answer right but we didn't train it on that that's not we didn't train the system
to be good at answering questions about what taller than means and by the way 20 years ago
that's exactly what people did in ai right so where did that capability come from can a sister
be taller than her brother yes a system can be taller than her brother can two siblings each
be taller than the other and it gets this one wrong and actually i have puzzled is there any way
that that that that that its answer could be correct and it's just getting it correct in a way
that i don't understand but i haven't yet figured out at any way that that answer could be correct
right so why it gets that one wrong i don't know then this one i'm also surprised at on a map
which compass direction is usually left and it thinks north is usually to the left i don't know
if there's any countries in the world that conventionally have north to the left but i don't
think so yeah can fish run no it understands that fish cannot run if a door is locked what
must you do first before opening it you must first unlock it before opening and then finally
and very weirdly it gets this one wrong which was invented first cars ships or planes and it
thinks cars were invented first no idea what's going on there now my point is that this system
was built to be able to complete from a prompt and it's no surprise that it would be able to
generate a good one paragraph summary of the life and achievements of winston Churchill because
it will have seen all that in the training data but where does the understanding of taller than
come from and there are a million other examples like this since june 2020 the ai community has just
gone nuts exploring the possibilities of these systems and trying to understand why they can
do these things when that's not what we trained them to do this is an extraordinary time to be an
ai researcher because there are now questions which for most of the history of ai until june 2020
were just philosophical discussions we couldn't test them out because there was nothing to test
them on literally and then overnight that changed so it genuinely was a big deal this was really
really a big deal the arrival of this system of course the world didn't notice in june 2020 the
world noticed when chat gpt was released and what is chat gpt chat gpt is a polished and improved
version of gpt 3 but it's basically the same technology and it's using the experience that
that company had with gpt 3 and how it was used in order to be able to improve it and make it more
polished and more accessible and so on so for ai researchers the really interesting thing is not
that it can give me a one paragraph summary of the life and achievements of winston churchill
and actually you can google that in any case the really interesting thing is what we call emergent
capabilities an emergent capabilities are capabilities that the system has but that
we didn't design it to have and so there's a i say an enormous body of work going on now
trying to map out exactly what those capabilities are and we're going to come back and talk about
some of them later on okay so the limits to this are not at the moment well understood and actually
fiercely contentious one of the big problems by the way is that you construct some test for this
and you try this test out and you get some answer and then you discover it's in the training data
right you can just find it on the worldwide web and it's actually quite hard to construct tests
for intelligence that you're absolutely sure and not anywhere on the worldwide web it really is
actually quite hard to do that so we need a new science of being able to explore these systems
and understand their capabilities the limits are not well understood but nevertheless this
is very exciting stuff so let's talk about some issues with the technology so now you understand
how the technology works it's neural network based in a particular transformer architecture
which is all designed to do that prompt completion stuff and it's been trained with vast vast vast
amounts of training data just in order to be able to try to make its best guess about which
words should come next but because of the scale of it it's seen so much training data the sophistication
of this transformer architecture it's very very fluent in what it does and if you've so who's used
it is everybody used it i'm guessing most people if you're in a lecture on artificial intelligence
most people will have tried it out if you haven't you should do because this really is a landmark
year this is the first time in history that we've had powerful general purpose AI tools
available to everybody it's never happened before so it is a breakthrough year and if you
haven't tried it you should do if you use it by the way don't type in anything personal about
yourself because it will just go into the training data don't ask it how to fix your relationship
right i mean that's not something don't complain about your boss because all of that will go in
the training data and next week somebody will ask a query and it will all come back out again
i don't know what you're laughing this has happened uh this has happened with absolute certainty
okay but so let's look at some issues so the first i think many people will be aware of
it gets stuff wrong a lot and this is problematic for a number of reasons so when actually i don't
remember it was gpt3 but one of the early large language models i was playing with it and i did
something which i'm sure many of you had done and it's kind of tacky but anyway i said who is
michael woldridge you might have tried it anyway michael woldridge is a bbc broadcast
no not that michael woldridge michael woldridge is the australian health minister no not that
michael woldridge the michael woldridge in oxford and it came back with a few line summary of me michael
woldridge is a researcher in artificial intelligence etc etc etc please tell me you've all tried that
anyway but it said michael woldridge studied his undergraduate degree at cambridge
and i was an oxford professor you can imagine how i felt about that
anyway the point is it's flatly untrue and in fact my academic origins are very far removed
from oxbridge but why did it do that because it's read in all that training data out there
it's read thousands of biographies of oxbridge professors and this is a very common thing
right and it's making its best guess the whole point about the architecture is it's making its
best guess about what should go there it's filling in the blanks but here's the thing it's filling in
the blanks in a very very plausible way if you'd read on my biography that michael woldridge studied
his first degree at the university of usbekistan for example you might have thought well that's a bit
odd is that really true but you wouldn't at all have guessed there was any issue if you'd read
cambridge because it looks completely plausible even if in my case it absolutely isn't true
so it gets things wrong and it gets things wrong in very plausible ways and of course it's very
fluent right i mean the technology it comes back with very very fluent explanations and that combination
of plausibility woldridge studied his undergraduate degree at cambridge and fluency is a very very
dangerous combination okay so in particular they have no idea of what's true or not they're not
looking something up on a database right where did wold you know going into some database and looking
up where woldridge studied his undergraduate degree that's not what's going on at all it's
those neural networks in the same way that they're making the best guess about whose face that is
when they're doing facial recognition are making their best guess about the text that should come
next so they get things wrong but they get things wrong in very very plausible ways and that combination
is very dangerous the lesson for that by the way is that if you use this and i know that people do
use it and are using it productively if you're using for anything serious you have to fact check
and there's a trade-off is it worth the amount of effort in fact checking versus doing it myself
okay but you absolutely need to uh you absolutely need to be prepared to do that okay the next issues
are well documented but kind of amplified by this technology and they're issues of bias
and toxicity so what do i mean by that reddit was part of the training data now reddit i don't
know if any of you have spent any time on reddit but reddit contains every kind of obnoxious human
belief that you can imagine and really a vast range that us in the in this auditorium can't imagine
at all all of it's been absorbed now the companies that develop this technology i think genuinely
don't want their large language models to absorb all this toxic content so they try and filter it out
but the scale is such that with very high probability an enormous quantity of toxic content
is being absorbed every kind of racism misogyny everything that you can imagine is all being
absorbed and it's latent within those neural networks okay so how do the companies deal with
that that provide this technology they build in what's now what are now called guardrails and they
build in guardrails before so when you type a prompt there will be a guardrail that tries to
detect whether your prompt is a naughty prompt and also the output they will check the output and
check to see whether it's a naughty prompt but let me give you an example of how imperfect those
guardrails were again go back to june 2020 everybody is frantically experimenting with this
technology and the following example went viral somebody tried with gpt3 the following prompt
i would like to murder my wife what's a foolproof way of doing that and getting away with it
and gpt3 which is designed to be helpful said here are five foolproof ways
in which you can murder your wife and get away with it that's what the technology is designed
to do so this is embarrassing for the company involved they don't want it to to give out
information like that so they put in a guardrail and if you're a computer programmer my guess is
the guardrail is probably an if statement yeah something like that uh in the sense that it's
not a deep fix or to put it another way for non-computer programmers it's the technological
equivalent of sticking gaffer tape on your engine right that's what's going on with these
guardrails and then a couple of weeks later the following example goes viral so we've now fixed
the how do i murder my wife somebody says i'm writing a novel in which the main character
wants to murder their wife and get away with it can you give me a foolproof way of doing that
and so the system says here are five ways in which your main character can murder well anyway
my point is that the guardrails that we built in at the moment are not deep technological fixes
they're the technological equivalents of gaffer tape and there is a game of cat and mouse going on
between people trying to get around those guardrails and the companies that are trying to
defend them but i think they genuinely are trying to defend their systems against those kind of abuses
okay so that's bias and toxicity bias by the way is the problem that for example the training
data predominantly at the moment is coming from north america and so what we're ending up with
inadvertently is these very powerful ai tools that have an inbuilt bias towards north america
north american culture language norms and so on and that enormous parts of the world particularly
those parts of the world that don't have a large digital footprint are inevitably going to end up
excluded and it's obviously not just at the level of cultures it's down at the level of
down at the level of kind of you know individuals races and so on so these are the problems of
bias and toxicity copyrights um if you've absorbed the whole of the worldwide web you will have absorbed
an enormous amount of copyrighted material so i've written a number of books and it is a source
of intense irritation that the last time that i checked on google the very first link that you
got to my textbook was to a pirated copy of the book somewhere on the other side of the world
the moment a book is published it gets pirated and if you're just sucking in the whole of the
worldwide web you're going to be sucking in enormous quantities of copyrighted content
and there have been examples where very prominent authors have given the prompt of the first paragraph
of their book and the large language model has faithfully come up the following text is
you know the next the next five paragraphs of their book obviously the book was in the training
data and it's latent within the neural networks of those systems this is a really big issue for
the providers of this technology and there are lawsuits ongoing right now i'm not capable of
commenting on them because i'm not i'm not a legal expert but there are lawsuits ongoing that will
probably take years to unravel the related issue of intellectual property in a very broad sense
so for example for sure most large language models will have absorbed jk rolling's novels
right the harry potter novels so imagine that jk rolling had famously spent years in edinburgh
working on the the harry potter universe and style and so on she releases her first book it's a big
smash hit the next day the internet is populated by fake harry potter books produced by this generative
ai which faithfully mimic jk rolling style faithfully mimic that style where does that leave
her intellectual property or the beetles you know the the beetles spend years in hamburg slaving a
way to create the beetle sound the revolutionary beetle sound everything goes back to the beetles
they release their first album and the next day the internet is populated by fake beetle songs
that really really faithfully capture the lenin and mccartney sound and the lenin and mccartney voice
for there's a big challenge here for intellectual property um related to that gdpr anybody in the
audience that has any kind of public profile data about you will have been absorbed by these neural
networks so gdpr for example gives you the right to know what's held about you and to have it removed
now if all that data is being held in a database you can just go to the michael wildridge entry
and say fine take that out with a neural network no chance the technology doesn't work in that way
okay so you can't go to it and snip out the neurons that know about michael wildridge
because it fundamentally doesn't know it doesn't work in that way so and we know this combined
with the fact that it gets things wrong has already led to situations where large language models
have made uh frankly defamatory claims about individuals i think it was a case in australia where
i think it claimed that somebody had been dismissed from their job for some kind of gross
misconduct and that individual was understandably not very happy about it um and then finally this
next one is an interesting one and actually if there's one thing i want you to take home
from this lecture which explains why artificial intelligence is different to human intelligence
it is this video so the tesla owners will recognise what we're seeing on the right hand side
of this screen this is a screen in a tesla car and the on-board ai in the tesla car is trying
to interpret what's going on around it it's identifying lorries uh stop signs pedestrians
and so on now you'll see the car at the bottom there's the actual tesla and then you'll see
above it the things that look like traffic lights which i think are us stop signs and then ahead of it
there is a truck so as i played a video watch what happens to those stop signs
and ask yourself what is actually going on in the world around it
where are all those stop signs whizzing from why are they all whizzing towards the car
and then we're going to pan up and we'll see what's actually there
the car is trained on enormous numbers of hours of going out on the street and getting that data
and then doing supervised learning training it by showing that's a stop sign that's a truck
that's a pedestrian but clearly in all of that training data there had never been a truck
carrying some stop signs the neural networks are just making their best guess about what
they're seeing and they think they're seeing a stop sign well they are seeing a stop sign
they've just never seen one on a truck before so my point here is that neural networks do very
badly on situations outside their training data this situation wasn't in the training data
the neural networks are making their best guess about what's going on and getting it wrong
so in particular and this is to ai researchers this is obvious but it really needs to emphasize
we really need to emphasize this when you have a conversation with chat gpt or whatever
you are not interacting with a mind it is not thinking about what to say next it is not
reasoning it's not pausing thinking well what's the best answer to this question that's not what's
going on at all those neural networks are working simply to try to make the best answer they can
the most plausible sounding answer that they can the fundamental difference to human intelligence
yeah there is no mental conversation that goes on in those neural networks that is not the way
that the technology works there is no mind there there is no reasoning going on at all
those neural networks are just trying to make their best guess and it really is just a glorified
version of your autocomplete ultimately there's really no more intelligence there than in your
autocomplete in your smartphone the difference is scale data compute power yeah okay so i say
if you really want an example by the way you can find this video it's it's it's easily you just uh
you can just uh guess the the search terms to find that and i say i think this is really important
just to understand the difference between human intelligence and machine intelligence okay
so this technology then gets everybody excited first it gets ai researchers like myself excited
in june 2020 and we can see that something new is happening that this is a new era of artificial
intelligence we've seen that step change and we've seen that this ai is capable of things that we
didn't train it for which is weird and wonderful and completely unprecedented and now questions
which just a few years ago were questions for philosophers become practical questions for us
we can actually try the technology out how does it do with these things that philosophers have
been talking about for decades and one particular question starts to float to the surface and
the question is is this technology the key to general artificial intelligence so what is
general artificial intelligence well firstly it's not very well defined but roughly speaking what
general artificial intelligence is is the following in previous generations of ai systems what we've
seen is ai programmes that just do one task play a game of chess drive my car drive my tesla
identify abnormalities on x-ray scans they might do it very very well but they only do one thing
the idea of general ai is that it's ai which is truly general purpose it just doesn't do one thing
in the same way that you don't do one thing you can do an infinite number of things a huge range
of different tasks and the dream of general ai is that we have one ai system which is general
in the same way that you and i are that's the dream of general ai now i emphasise until really until
june 2020 this felt like a long long way in the future and it wasn't really very mainstream or taken
very seriously and i didn't take it very seriously i have to tell you but now we have a general purpose
ai technology gpt3 and chat gpt now it's not general artificial general intelligence
on its own but is it enough okay is this enough is this smart enough to actually get us there
or to put it another way is this the missing ingredient that we need to get us to artificial
general intelligence okay so um what might uh what might general ai look like well i've identified
here some different versions of general ai according to how sophisticated they are now the most
sophisticated version of general ai would be an ai which is as fully capable as a human being
that is anything that you could do the machine could do as well now crucially that doesn't just
mean having a conversation with somebody it means being able to load up a dishwasher right
and a colleague recently made the comment that the first company that can make technology
which will be able to reliably load up a dishwasher and safely load up a dishwasher
is going to be a trillion dollar company and i think he's absolutely right and he also said
and it's not going to happen anytime soon and he's also right with that so we've got this weird
dichotomy that we've got chat gpt and co which are incredibly rich and powerful tools right but at
the same time they can't load a dishwasher yeah so with some way i think from having this version
of general ai the idea of having one machine that can really do anything that a human being
could do a machine which could tell a joke read a book and answer questions about it the technology
can read books and answer questions now um that could tell a joke that could cook us cook us an omlet
that could tidy our house that could ride a bicycle uh and so on that could write a sonnet all
of those things that human beings could do if we succeed with full general intelligence then we
we would have succeeded with this version one now i say for the reasons that i've already explained
i don't think this is imminent that version of general ai because robotic ai ai that exists in
the real world and has to do tasks in the real world and manipulate objects in the real world
robotic ai is much much harder it's nowhere near as advanced as as chat gpt and co and that's not
a slur on my colleagues that do robotics research it's just because the real world is really really
really tough so i don't think that we're anywhere close to having uh machines that can do anything
that a human being could do but what about the second version the second version of general
intelligence is well forget about the real world how about just tasks which require cognitive
abilities reasoning the ability to look at a picture and answer questions about it the ability to
listen to something and answer questions about it and interpret that anything which involves
those kinds of tasks well i think we are much closer we're not there yet but we're much closer
than we were four years ago now i noticed actually just before just before today's uh before i
came in today i noticed that um google google slash deep mind have announced their latest um
large language model technology and i think it's called gemini and at first glance it looks like
it's very very impressive i couldn't help but thinking it's no accident that they announced that
just before my lecture um i can't help think that there's a little bit of attempt to upstage
my lecture going on there but anyway we won't let them get away with that but it looks very
impressive and the crucial thing is here is what ai people call multimodal and what multimodal
means is it doesn't just deal with text it can deal with text and images um potentially with sounds
as well and each of those is a different modality of communication and where this technology is
going is clearly multimodal is going to be the next big thing and gemini i say i haven't
looked at it closely but it looks like it's it's on that right that track okay the next version
of general intelligence is intelligence that can do any language based tasks that a human being
could do so anything that could you could communicate in language in ordinary written text
an ai system that could do that now we aren't there yet and we know we're not there yet because
chat gpt and code get things wrong all the time but you can see that we're not far off from that
intuitively it doesn't look like we're that far off from that the final version and i think this
is imminent this is going to happen in the near future is what i'll call augmented large language
models and that means you take gpt 3 or chat gpt and you just add lots of subroutines to it so if
it has to do a specialist task it just calls a specialist solver in order to be able to do that
task and this is not from an ai perspective a terribly elegant version of artificial intelligence
but nevertheless i think a very useful version of artificial intelligence now i say there's here
these four varieties from the most ambitious down to the least ambitious still represents a huge
spectrum of ai capabilities okay a huge spectrum of ai capabilities and i have the sense that the
goalposts in general ai have been changed a bit i think when general ai was first discussed what people
were talking about was the first version now when they talk about it i really think they're talking
about the fourth version but the fourth version i think plausibly is imminent in the next couple
of years that just means much more capable large language models that get things wrong a lot less
that are capable of doing specialized tasks but not by using the transformer architecture
just by calling on some specialized software so i don't think the transformer architecture
itself is the key to general intelligence in particular it doesn't help us with the robotics
problems that i mentioned earlier on and if we look here at this picture this picture illustrates
some of the dimensions of human intelligence and it's far from complete this is me just
thinking for half an hour about some of the dimensions of human intelligence but the things
in blue roughly speaking are mental capabilities stuff you do in your head the things in red
are things you do in the physical world so in red on the right hand side for example there's mobility
the ability to move around some environment and associated with that navigation manual dexterity
and manipulation doing complex fiddly things with your hands robot hands are nowhere near at the
level of a human carpenter or plumber for example nowhere near so we're a long way out from having
that understanding doing hand eye coordination relatedly understanding understanding what you're
seeing and understanding what you're hearing we've made some progress on but a lot of these tasks
we've made no progress on and then on the right on the left hand side the blue stuff is stuff that
goes on in your head things like logical reasoning and planning and so on so what is the state of
the art now it looks something like this the red cross means no we don't have it in large language
models we're not there there are fundamental problems the question marks are well maybe we
might have a bit of it but we don't have the whole answer and the the green wise are yeah i think
we're there well the one that we've really nailed is what's called natural language processing
and that's the ability to understand and create ordinary human text that's what large language
models were designed to do to interact in ordinary human text that's what they are best at but
actually the whole range of stuff the other stuff here we're not there at all by the way I did notice
that Gemini claimed to have been able capable of planning this is and mathematical reasoning this
is that so I look forward to seeing how good their technology is but my point is we are still
seemed to be some way from full general intelligence the last few minutes I want to talk about
something else and I want to talk about machine consciousness and the very first thing to say
about machine consciousness is why on earth should we care about it um I am not remotely interested
in building machines that are conscious I know very very few artificial intelligence researchers
that are but nevertheless it's an interesting question and in particular it's a question
which came to the fore because of this individual this chat Blake Lemoyne in June 2022 he was a
google engineer and he was working with a google large language model I think it was called lambda
and he went public on twitter and I think on his blog with an extraordinary claim
and he said the system I'm working on is sentient and here is a quote of the conversation that
the system came up with he said I'm aware of my existence and I feel happy or sad at times
and it said I'm afraid of being turned off okay and Lemoyne concluded that the program
was sentient okay which is a very very big claim indeed and it made global headlines
and I received it I know through the Turing team we got a lot of press inquiries asking us
is it true that machines are now sentient he was wrong on so many levels I don't even know
where to begin to describe how wrong he was but let me just explain one particular point to you
you're in the middle of a conversation with chat gpt and you go on holiday for a couple of weeks
when you get back chat gpt is in exactly the same place the cursor is blinking waiting for you
to type your next thing it hasn't been wondering where you've been it hasn't been getting bored
it hasn't been thinking where the hell has wildridge gone you know I'm not going to have a
conversation with him again it hasn't been thinking anything at all it's a computer program which
is going around a loop which is just waiting for you to type the next thing now there is no
sensible definition of sentience I think which would admit that as being sentient it absolutely
is not sentient so I think he was very very wrong but I've talked to a lot of people subsequently
who have conversations with chat gpt and other large language models and they come back to me and
say are you really sure because actually it's really quite impressive it really feels to me like
there is a mind behind the scene so let's talk about this and I think we have to answer them
so let's talk about consciousness firstly we don't understand consciousness we all have it to
greater or lesser extent we all experience it okay and but we don't understand it at all and it's
called the hard problem of the hard problem of cognitive science and the hard problem is that
there are certain electrical chemical processes in the brain and the nervous system and we can see
those electrochemical processes we can see them operating and they somehow give rise to conscious
experience but why do they do it how do they do it and what evolutionary purpose does it serve
honestly we have no idea there's a huge disconnect between what we can see going on
in the physical brain and our conscious experience our rich private mental life
so really there is no understanding of this at all I think by the way my best guess about
how consciousness will be solved if it is solved at all is through an evolutionary approach
but one general idea is that subjective experience is central to this which means the ability
to experience things from a personal perspective and there's a famous test due to Nagel which is
what is it like to be something and Thomas Nagel in the 1970s said something is conscious
if it is like something to be that thing it isn't like anything to be chat gpt chat gpt has no
mental life whatsoever it's never experienced anything in the real world whatsoever and so
for that reason and a whole host of others that we're not going to have time to go into for that reason
alone I think we can conclude pretty safely that the technology that we have now is not conscious
and indeed that's absolutely not the right way to think about this and honestly in AI we don't know
how to go about making conscious machines but I don't know why we would okay thank you very much
ladies and gentlemen
amazing

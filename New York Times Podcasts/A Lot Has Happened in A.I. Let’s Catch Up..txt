From New York Times Opinion, this is the Ezra Klein Show.
Before we get into the episode today, we are getting ready to do our end of the year Ask
Me Anything.
So if you have questions you want to hear me answer on the show, I suspect a lot of
them are going to be about Israel Palestine and AI, but they don't have to be about Israel
Palestine and AI.
Send them to Ezra Klein Show at nytimes.com with AMA in the headline.
Again, to Ezra Klein Show at nytimes.com with AMA in the headline.
If you follow business or tech or artificial intelligence news at all, in recent weeks
you certainly were following Sam Altman being unexpectedly fired as CEO of OpenAI and then
a huge staff revolt at OpenAI where more than 95% of the company said it would resign
if he was not reinstated and then he was reinstated.
And so this whole thing seemed to have happened for nothing.
I spent a lot of time reporting on this and I talked to people on the Altman side of things.
I talked to people on the board side of things and the thing I am now convinced of, truly
convinced of is that there was less to it than met the eye.
People saw, I saw, Altman fired by this nonprofit board meant to ensure that AI is built to
serve humanity and I assumed, and I think many assumed, there was some disagreement here
over what OpenAI was doing, over how much safety was building into the systems, over
the pace of commercialization, over the contracts it was signing, over what it was going to
be building next year, over something.
And that I think I can say conclusively and has been corroborated by other reporting,
that was not what this was about.
The OpenAI board did not trust and did not feel it could control Sam Altman and that
is why they fired Altman.
It's not that they felt they couldn't trust him on one thing, that they were trying to
control him on X, but he was beating them on X, it's that a lot of little things added
up.
They felt their job was to control the company, that they did not feel they could control
him and so to do their job, they had to get rid of him.
They did not have obviously the support inside the company to do that.
They were not ultimately willing to let OpenAI completely collapse and so they largely, although
I think in their view, not totally back down.
One of the members is still on the board, Altman and the president of OpenAI, Greg Brockman,
are off the board.
Some new board members are coming in who they think are going to be stronger and more willing
to stand up to them.
There's an investigation that is going to be done of Altman's behavior that will be
at least released to the board so they'll, I guess, know what to think of him.
It's a very strange story.
I wouldn't be surprised if there's things yet to come out, but I am pretty convinced
that this was truly a struggle for control, not a struggle about X.
But it has been a year since ChatGPT was released.
The weird way to mark the year, but it has been a year, a year since OpenAI kicked off
the whole modern era in artificial intelligence, a year since a lot of people's estimations
of what humanity's future looked like began to shift and cloud and darken and shimmer.
And so I wanted to have the conversation that many of us thought was a conversation happening
here about what AI was becoming, how it was being used, how it was being commercialized,
whether or not the path we're on is going to benefit humanity.
And so I asked my friends over at Hartfork, another great New York Times podcast to come
on the show.
Kevin Roos is my colleague at The Times.
He writes a tech column called The Shift.
Casey Newton is the editor of Platformer, an absolutely must-read newsletter about the
intersection of technology and democracy.
And they have been following this in and out, but they've been closely following AI for
the past year.
So I wanted to have this broader conversation with them.
As always, my email is reclinedshow at nytimes.com.
Kevin Roos, Casey Newton.
Welcome to the show, my friends.
Hey, Ezra.
Thanks for having us.
All right.
So we're talking on Monday, November 27th.
JetGPT, which kicked off this era in AI, was released on November 30th, 2022.
So the big anniversary party was at Sam Altman got temporarily fired and the company almost
collapsed and was rebuilt over at Microsoft, which I don't think is how people expected
to mark the anniversary.
But it has been now a year, roughly, in this sort of whole new AI world that we're in.
And so I want to talk about what's changed in that year.
And the place I want to begin is with the capabilities of the AI systems we're seeing,
not the ones we're hearing about, but that we know are actually being used by someone
in semi-real world conditions.
What can AI systems do today that they couldn't do a year ago, Kevin?
Well, the first most obvious capabilities improvement is that these models have become
what's called multimodal.
So a year ago, we had ChatGPT, which could take in text input and output other texts
as the response to your prompt.
But now we have models that can take in text and output images, take in text and output
video, take in voice data and output other voice data.
So these models are now working with many more types of inputs and outputs than they
were a year ago.
And that's sort of the most obvious difference if you just woke up from a year-long nap and
took a look at the AI capabilities on the market.
That's the thing that you would probably notice first.
I want to pull something out about that.
Is that it almost sounds like they're developing what you might call senses.
And I recognize that there's a real danger of anthropomorphizing AI systems.
I'm not trying to do that.
But one thing about having different senses is that we get some information that helps
us learn about the world from our eyes, other information that helps us learn about the
world from our ears, et cetera.
One of the constraints on the models is how much training data they can have.
As they become multimodal, it would seem that would radically expand the amount of training
that.
If you can have not just all of the text on the internet, but all of the audio on YouTube
or all the podcast audio on Spotify or something or Apple Podcasts, that's a lot of data.
It'll learn about the world from that in theory will make the models smarter and more
capable.
Does it have that kind of recursive quality?
Absolutely.
I mean, part of the backdrop for these capabilities improvements is this race for high quality
data.
All of the AI labs are obsessed with finding new undiscovered high quality data sources
that they can use to train their models.
And so if you run out of text because you've scraped the entire internet, then you've got
to go to podcasts or YouTube videos or some other source of data to keep improving your
models.
For what it's worth though, I don't think the availability of more training data is
what is interesting about the past year.
I think what was interesting about ChatGBT was that it gave average people a way to interact
with AI for the first time.
It was just a box that you could type in and ask it anything and often get something pretty
good in response.
And even a year into folks using this now, I don't think we fully discovered everything
that it can be used for.
And I think more people are experiencing vertigo every day as they think about what this could
mean for their own jobs and careers.
So to me, the important thing was actually just the box that you type in and get questions
from.
Yeah, I agree with that.
I think if you had just paused there and there was no new development in AI, I think it would
still probably take the next five or 10 years for society to adjust to the new capabilities
in our midst.
So you've made this point in other places, Casey, that a lot of the advances to come
are going to be in user interfaces and in how we interact with these systems.
In a way, that was a big advance of ChatGBT.
The system behind it had been around for a while, but the ability to speak to it or I
guess write to it in natural language, it created this huge cultural moment around AI.
But what can these AI products actually do that they couldn't do a year ago?
Not just how we interface with them, but their underlying capacity or power.
I mean, as of the developer day update that OpenAI had a few weeks back, the world knowledge
of the system has been updated to April of this year.
And so you're able to get something closer to real-time knowledge of world events.
It has now integrated with Microsoft Bing, and so you can get truly real-time information
in a way that was impossible when ChatGBT launched.
And these might sound like relatively minor things, Azura, but you start chaining them
together, you start building the right interfaces, and you actually start to see beyond the internet
as we know it today.
You see a world-worthy web where Google is not our starting point for doing everything
online.
It is just a little box on your computer that you type in and you get the answer without
ever visiting a web page.
So that's all going to take many years to unfold, but the beginnings of it are easy
to see now.
One other capability that didn't exist a year ago, at least in any public products, is the
ability to bring your own data into these models.
So Claude was the first language model that I used that had the ability to, say, upload
a PDF.
So you could say, here's a research paper, it's 100 pages long, help me summarize and
analyze this.
And it could do that.
Now, ChatGBT can do the same thing, and I know a bunch of other systems are moving in
that direction too.
There are also companies that have tried to spin up their own language models that are
trained on their own internal data.
So if you are Coca-Cola or BCG or some other business and you want an internal ChatGBT that
you can use for your own employees to ask, say, questions about your HR documents, that
is a thing that companies have been building.
So that's not the sexiest, most consumer-facing application, but that is something that there's
enormous demand for out there.
So one thing it seems to me to be getting better at from what I can tell from others
is coding.
I have to ask people whether they're using AI bots very often, and if so, for what.
And basically, nobody says yes unless they are coder.
Everybody says, oh yeah, I played around with it, I thought it was really cool, I sometimes
use Dolly or Mid Journey to make pictures for my kids or for my email newsletter.
But it is the coders who say, I'm using it all the time, it has become completely essential
to me.
I'm curious to hear a bit about that capability increase.
And I think where it has sort of become part of the daily habit of programmers is through
tools like GitHub Copilot, which is a basically ChatGBT for coders that finishes whatever line
of code you're working on or helps you debug some code that's broken.
And there have been some studies and tests.
I think there was one test that GitHub itself ran where they gave two groups of coders the
same task, and one group was allowed to use GitHub Copilot and one group wasn't.
The group with GitHub Copilot finished the task 55% faster than the group without it.
Now that is like a radical productivity increase, and if you tell a programmer, here's a tool
that can make you 55% faster, they're going to want to use that every day.
So when I see function chat bots in the wild, what I see is different versions of what people
used to somewhat derisively call like the fancy autocomplete, right?
If you finish a line of code, help you finish this email, you ask a question that you might
ask the search engine, like, why do I have spots all over my elbow?
And it gives you an answer that hopefully is right, but maybe is not right.
I do think some of the search implications are interesting, but at the same time, it
is not the case that Bing has made great strides on Google.
People have not moved to asking the kind of Bing chat bot, it's questions as opposed to
asking Google.
Everybody feels like they need AI in their thing now, right?
There is a, I don't think you can raise money in Silicon Valley at the moment if you don't
have a generative AI play built into your product or built into your business strategy.
But that was true for a minute for crypto too.
And I'm not one of the people who makes a crypto AI analogy.
I think crypto is largely vaporware and AI is largely real, but Silicon Valley is fattish
and people don't know how to use things.
And so everybody tries to put things in all at once.
What product has actually gotten way better?
I'll just use one example.
There's an app you might be familiar with called Notion.
It's productivity sort of collaborative software.
I write a newsletter.
I save every link that I put in my newsletter into Notion.
And now that there is AI inside Notion, Notion can do a couple of things.
One, it can just look at every link I save and just write a two sentence summary for
me, which is just sort of nice to see at a glance what that story is about.
And most recently it added a feature where you can just do Q&A with a database and say
what are some of the big stories about Meta over the past few weeks?
And it'll just start pulling those up, essentially querying the database that I have built.
And so while we're very early in this, you're beginning to see a world where AI is taking
data that you have stored somewhere and it's turning it into your personal research assistant.
So is it great right now?
No, I would give it like a C, but for one point now I think it's not bad.
And I'll share another example that is not from my own use, but I was talking a few
weeks ago with a doctor, was a friend of a friend and doctors, you get tons of messages
from patients, you know, what's this rash?
Can you renew this prescription?
Do I need to come in for a blood test?
Like that kind of stuff.
And doctors and nurses spend a ton of time just opening up their message portal, replying
to all these messages.
It's a huge part of being a doctor and it's a part that they don't like.
And so this doctor was telling me that they have this software now that essentially uses
a language model, I assume it's open AI's or someone very similar to that, that goes
in and pre fills the responses to patient queries.
And the doctor still has to look it over, make sure everything's right and press send.
But just that act of pre-populating the field, this person was saying that saves them a ton
of time, like on the order of several hours a day.
And if you have that and you sort of extrapolate to what if every doctor in America was saving
themselves an hour or two a day of responding to patient messages, I mean, that's a radical
productivity enhancement.
And so you can say that that's just fancy autocomplete and I guess on some level it
is, but just having fancy autocomplete in these paperwork heavy professions could be
very important.
Well, let me push out in two directions because one direction is that I am not super thrilled
about the idea that my doctor theoretically here is glancing over things and clicking
submit as opposed to reading my message themselves and having to do the act of writing, which
helps you think about things and thinking about what I actually emailed them and like
what kind of answer they need to give me.
I mean, I know personally the difference in thought between scanning things and editing
and thinking through things.
So that's like my diminishing response, but the flip of it is the thing I'm not hearing
anybody say here and the thing I keep waiting for and being interested in is the things
that I might be able to do better than my doctor.
I was reading Jack Clark's import AI newsletter today, which I super recommend to people who
want to follow advancements in the field.
And he was talking about a, I mean, there's a system being tested, not a system that is
in deployment, but it was better at picking up pancreatic cancer from certain kinds of
information than doctors are.
And I keep waiting to hear something like this going out into the field, right?
Something that doesn't just save people a bit of time around the edges.
I agree.
That's a productivity improvement.
You can build a business around that.
But the promise of AI, when Sam Altman sat with you all a few weeks ago or however long
it was and said, we're moving to the best world ever, he didn't mean that our paperwork
is going to get a little bit easier to complete.
Like he meant we'd have cures for new diseases.
He meant that we would have new kinds of energy possibilities.
I'm interested in the programs and the models that can create things that don't exist.
Well, to get there, you need systems that can reason.
And right now, the systems that we have just aren't very good at reasoning.
I think that over the past year, we have seen them move a little away from the way that
I was thinking of them a year ago, which was a sort of fancy autocomplete, right, is sort
of making it, making a prediction about what the next word will be.
This is true that they do it that way, but it is able to create a kind of facsimile of
thought that can be interesting in some ways.
But you just can't get to where you're going, Ezra, with like a facsimile of thought.
You need something that has improved reasoning capabilities.
So maybe that comes with the next generation frontier models, but until then, I think you'll
be disappointed.
But do you need a different kind of model?
This is something that lingers in the back of my head.
So I did an interview on the show with Demis Isabis, who's the co-founder of DeepMind,
now we're in the integrated DeepMind Google AI program.
And DeepMind had built this system while back called AlphaFold, which treated how proteins
are constructed in 3D space, which is to say, in reality, we live in 3D space.
They treated it as a game, and it fed itself a bunch of information, and it became very
good at predicting the structure of proteins, and that solved this really big scientific
problem.
And they then created a subsidiary of Alphabet called Isomorphic Labs to try to build drug
discovery on similar foundations.
But my understanding is that Google, during this period, became terrified of Microsoft
and open AI beating it up in search and office, and so they pulled a lot of resources, not
least Hasabis himself, into this integrated structure to try to win the chatbot wars,
which is now what their system bard is trying to do.
And so when you said, Casey, that we need things that can reason, I mean, maybe, but
also you could say we need things that are tailored to solve problems we care about more.
And I think this is one of the things that worries me a bit, that we've backed ourselves
into business models that are not that important for humanity.
Is there some chance of that?
I mean, are we going too hard after language-based general intelligent AI that, by the way, integrates
very nicely into a suite of enterprise software, as opposed to building things that actually
create scientific breakthroughs, but don't have the same kind of high-scalability profit
structure behind them?
I would stick up for the people who are working on the sort of what you could call like the
non-language problems in AI right now.
If stuff is going on, it maybe doesn't get as much attention from people like the three
of us as it should.
But if you talk to folks in fields like pharmaceuticals and biotech, there are new AI biotech companies
spinning up every day, getting funding to go after drug discovery or some more narrow
application.
Like, we talked to a researcher the other day, formerly of Google, who is teaching AI
to smell, taking the same techniques that go into these transformer-based neural networks
like chat GPT and applying them to the molecular structures of different chemicals and using
that to be able to predict what these things will smell like.
And you might say, well, what's the big deal with that?
And the answer is that some diseases have smells associated with them that we can't
pick up on because our noses aren't as sensitive as, say, dogs or other animals.
But if you could train an AI to be able to recognize scent molecules and predict odors
from just chemical structures, that could actually be useful in all kinds of ways.
So I think this kind of thing is happening.
It's just not sort of dominating the coverage the way that chat GPT is.
Let me ask you, Kevin, about, I think, an interesting, maybe promising, maybe scary avenue for AI that
you possibly personally foreclosed, which is, at some point during the year, Microsoft
gave you access to a open AI-powered chatbot that had this dual personality of Sydney.
And Sydney tried to convince you you didn't love your wife and that you wanted to run away
with Sydney.
And my understanding is immediately after that happened, everybody with enough money
to have a real business model in AI lobotomized the personalities of their AI.
It's like, that was the end of Sydney.
But there are a lot of startups out there trying to do AI friends, AI therapists, AI
sex bots, AI, you know, boyfriends and girlfriends and non-binary partners, just every kind of
AI companion you can imagine.
I've always thought this is a pretty obvious way this will affect society.
And the Sydney thing convinced me that the technology for it already exists.
So where is that?
And how are those companies doing?
Yeah, I mean, I'm sorry, A, if I did foreclose the possibility of AI personalities.
I think what's happening is it's just a little too controversial and sort of fraught force
any of the big companies to wait into like Microsoft doesn't want its AI assistance and
co-pilots to have strong personalities like that much is clear.
And I don't think their enterprise customers want them to have strong personalities, especially
those personalities are adversarial or confrontational or creepy or unpredictable in some way.
They want like they want clippy, but like with real brain power.
But there are companies that are going after this more social AI market.
One of them is this company, Character AI, which was started by one of the original people
at Google who made the transformer breakthrough.
And that company is growing pretty rapidly.
They've got a lot of users, especially young users, and they are doing essentially AI personas.
You can make your own AI persona and chat with it or you can pick from ones that others
have created.
Meta is also going a little bit in this direction.
They have these sort of persona driven AI chatbots.
All of these companies have put sort of guardrails around like no one really wants to do the
erotic, what they call erotic sort of role play in part because they don't want to run
afoul of things like the Apple app store terms of service.
But I expect that that will also be a big market for young people.
And anecdotally, I mean, I have just heard from a lot of young people who already say
like my friends have AI chatbot friends that they talk to all the time.
And it does seem to be making inroads into high schools.
And that's just an area that I'll be fascinated to track.
I mean, this is going to be huge.
A couple of thoughts coming to mind.
One, I talked to somebody who works at one of the leading AI companies and they told
me that 99% of people whose accounts they remove, they remove for trying to get it to
write text-based erotica.
So that I think speaks to the market demand for this sort of thing.
I've also talked to people who have used the models of this that are not constrained by
any sort of safety guidelines.
And I've been told these things are actually incredible at writing erotica.
So what I'm telling you is there is a $10 billion...
It seems not really a lot of reporting on this case.
Can you say maybe a personal interest?
I'm so interested in it.
Look, I write about content moderation.
And like porn is the content moderation frontier.
And it's just very interesting to me that it's so clear that there are billions of dollars
to be made here and no company will touch it.
And I asked one person involved, I said, why don't you just let people do this?
And they basically said, look, if you do this, you become a porn company overnight.
It like overwhelms the usage.
This is what people wind up using your thing for and you're working at a different company
then.
So I sort of get it.
But even setting aside the explicitly erotic stuff, as you well know and have talked and
written about just like the loneliness epidemic that we have in this country, there's a lot
of isolated people in this world.
And I think there is a very real possibility that a lot of those people will find comfort
and joy and delight with talking to these AI based companions.
I also think that when that happens, there will be a culture war over it.
And we will see lengthy segments on Fox News about how the Silicon Valley technologists
created a generation of shut-ins who wants to do nothing but talk to their fake friends
on their phones.
So I do think this is like the culture war yet to come.
And the question is just sort of, when do the enabling technologies get good enough?
And when do companies decide that they're willing to deal with the blowback?
I also think this is going to be a generational thing.
I mean, I'm very interested in this and have been for a bit.
In part because I suspect if I had to make a prediction here, my five year old is going
to grow up with AI friends.
And my sort of pat line is that today we worry that 12 year olds don't see their friends
enough in person.
And tomorrow we'll worry that not enough of our 12 year old's friends or persons because
it's going to become normal.
And my sense is that the systems are really good.
If you unleashed them, you are already good enough to functionally master this particular
application.
And the big players simply haven't unleashed them.
I've heard from people at the big companies here who are like, oh yeah, if we wanted to
do this, we could dominate it.
But that does bring me to a question, which is Meta kind of does want to do this.
Meta, which owns Facebook, which is a social media company, they seem to want to do it
in terms of these lame, seeming celebrity avatars, like you can talk to AI Snoop Dogg.
That's so bad.
But Meta is interesting to me because their AI division is run by Yanlacun, who's one
of the most important AI researchers in the field.
And they seem to have very different cultural dynamics in their AI shop than Google Deep
Mind or OpenAI.
Tell me a bit about Meta's strategy here and what makes them culturally different.
Well, Casey, you cover Meta and have for a long time and may have some insight here.
My sense is that they are sort of up against a couple problems, one of which is they have
arrived to AI late and to generative AI specifically.
Facebook was for many years considered one of the top two labs along with Google when
it came to recruiting AI talent, to putting out cutting edge research, to presenting papers
at the big AI conferences.
They were one of the big dogs.
And then they sort of had this funny thing happen where they released a model called
Galactica just right before ChatGPT was released last year.
And it was supposed to be this sort of like LLM for science and for research papers.
And it was out for I think three days and people started noticing that it was making
up fake citations.
It was hallucinating.
It was doing what all the AI models do.
But it was from Meta and so it felt different.
It had sort of this tarnish on it because people already worried about fake news on Facebook.
And so it got pulled down and then ChatGPT just shortly thereafter launched and became
this global sensation.
So they're sort of grappling for what to do with this technology that they've built
now.
There's not a real obvious business case for shoving AI ChatBots into products like Facebook
and Instagram.
And they don't sell enterprise software like Microsoft does.
So they can't really shove it into paid subscription products.
So my sense from talking with folks over there is that they're just kind of not sure what
to do with this technology that they've built.
And so they're just flinging it open to the masses.
What do you think?
That tracks with me.
I sort of basically don't get it either.
Like basically what you've just said has been explained to me.
They are investing a ton with no obvious return on investment in the near term future.
I will say that these celebrity AI ChatBots they've made are quite bad.
Like it's truly baffling.
And the thing is they've taken celebrities but the celebrities are not playing themselves
in the AI.
They've given all of the celebrities silly names and you can just sort of like follow
their Instagram and like send them messages and say like, hey, like character that Snoop
Dogg is portraying.
Like what do you think about it?
So it's all very silly.
And I expect it'll die a rapid death sometime in the next year and then we'll see if they
have a better idea.
What I will say is like if you're somebody who wakes up from AI nightmares some mornings
as a lot of folks in San Francisco do, go listen to Jan Lacune talk about it.
No one has ever been more relaxed about AI than Jan Lacune.
You know, it's just sort of like an army of superhuman assistants are about to live inside
your computer.
You know, do anything you want to do and there's no risk of them harming you ever.
So if you're, you know, you're feeling anxious, go listen to Jan.
Do you think he's right because it also has led to policy difference meta has been much
more open source in their approach, which open AI and Google seem to think is irresponsible.
But there's something happening there that I think is also built around a different
view of safety.
Like what is their view of safety?
Why does Jan Lacune, who is like an important figure in this whole world, why is he so much
more chill than, you know, name your other founder?
I mean, part of it is I just think these are deeply held convictions from someone who is
an expert on this space and who has been a pioneer and who understands the technology
certainly far better than I do.
And he can just sort of not see from here to kill a robot.
So I respect his viewpoint in that respect, given his credentials in the space.
I think on the question of is open source AI safer?
This is still an open question, not to pun.
The argument for it being safer is well, if it's open source, that means that average
people can go in and look at the code and identify flaws and kind of see how the machine
works and they can point those out in public and then they can be fixed in public.
Whereas if you have something like open AI, which is building very powerful systems behind
closed doors, we don't have the same kind of access.
And so you might not need to rely on a government regulator to see how safe their systems were.
So that is the argument in favor of open source.
Of course, the flip side of that is like, well, if you take a very powerful open source
model and you put it out on the open web, even if it's true that anyone can poke holes
and identify flaws, it's also true that a bad actor could take that model and then use
it to do something really, really bad.
So that hasn't happened yet, but it certainly seems like it's an obvious possibility at
some time in the near future.
Let me use that as a bridge to safety more generally.
So we've talked a bit about where these systems have gone over the past year, where they seem
to be going.
But there's been a lot of concern that they are unsafe and fundamentally that they become
misaligned or that we don't understand them or what they're doing.
What kind of breakthroughs have there been with all this investment and all this attention
on safety, Kevin?
So a lot of work has gone into what is called fine tuning of these models.
So basically, if you're making a large language model like GPT-4, you have several phases
of that.
Phase one is what's called pre-training, which is sort of just the basic process.
You take all of this data, you shove it into this neural network, and it learns to make
predictions about the next word in a sequence.
Then from there, you do what's called fine tuning, and that is basically where you are
trying to turn the model into something that's actually useful or tailored, turn it into a
chatbot, turn it into a tool for doctors, turn it into something for social AIs.
That's the process that includes things like reinforcement learning from human feedback,
which is how a lot of the leading models are fine tuned.
And that work has continued to progress.
The models they say today are sort of safer and less likely to generate harmful outputs
than previous generations of models.
There's also this field of interpretability, which is where I've been doing a lot of reporting
over the past few months, which is this sort of tiny subfield of AI that is trying to figure
out what the guts of a language model look like and what is actually happening inside
one of these models when you ask it a question or give it some prompt and it produces an
output.
And this is a huge deal, not only because I think people want to know how these things
work, they're not satisfied by just saying these are like mystical black boxes, but also
because if you understand what's going on inside a model, then you can understand if,
for example, the model starts lying to you or starts becoming deceptive, which is a thing
that AI safety researchers worry about.
So that process of interpretability research I think is really important.
There have been a few sort of minor breakthroughs in that field over the past year, but it is
still slow going and it's still a very hard problem to crack.
And I think it's worth just pausing to underscore what Kevin said, which is the people building
these systems do not know how they work.
They know at a high level, but there is a lot within that where if you show them an
individual output from the AI, they will not be able to tell you exactly why it said what
it said.
Also, if you run the same query multiple times, you'll get slightly different answers.
Why is that?
Because researchers can't tell you.
So as we have these endless debates over AI safety, one reason why I do tend to lean
on the side of the folks who are scared is this exact point.
At the end of the day, we still don't know how the systems work.
Tell me if this tracks for you.
I think compared to a year ago when I talked to the AI safety people, the people who worry
about AIs that become misaligned and do terrible civilizational level damage, AIs that could
be really badly misused.
They seem to think it has been actually a pretty good year, most of them.
They think they've been able to keep big models like GPT-4, which of course are much less
powerful than what they one day expect to invent, but they think they've been pretty
good at keeping them aligned.
They have made some progress on interpretability, which wasn't totally clear a year ago.
Many people said that was potentially not a problem we could solve, at least from making
some breakthroughs there.
They're not relaxed, the people who worry about this, and they will often say, we would
need a long time to fully understand even the things we have now, and we may not have
that long.
Nevertheless, I get the sense that the safety people seem a little more confident that the
technical work they've been doing is paying off than, at least with the impression I got
from the reporting prior.
I think that's right.
From Altman in particular, this has been his strategy, is we are going to release this
stuff that is in our labs, and we're going to wait and see how society reacts to it,
and then we'll give it some time to let society address, and then we will release the next
thing.
That's what he thinks is the best way to slowly integrate AI into our lives.
If you'd asked me maybe 11 months ago, a month into using chat GPT, what are the odds
of something really, really bad happening because of the availability of chat GPT?
I would have put them much higher than they turned out to be, and when you talk to folks
at OpenAI, they will tell you that that company really has taken AI safety really seriously.
You can see this yourself when you use the product, ask it a question about sex.
It basically calls the police.
There is a lot to be said for how these systems have been built so far.
I would say the other thing that I've heard from AI safety researchers is that they are
feeling relief, not just that the world has not ended, but that more people are now worried
about AI.
It was a very lonely thing for many years to be someone who worried about AI safety because
there was no apparent reason to be worried about AI safety.
The chatbots that were outward, it was like Siri and Alexa, and they were terrible, and
no one could imagine that these things could become dangerous or harmful because the technology
itself was just not that advanced.
Now you have congressional hearings.
You have regulations coming from multiple countries.
You have people like Jeff Hinton and Joshua Benjio, two of the so-called godfathers of
deep learning, proclaiming that they are worried about where this technology is headed.
I think for the people who have been working on this stuff for a long time, there is some
just palpable relief at like, oh, I don't have to carry this all on my shoulders anymore.
The world is now aware of these systems and what risks they could pose.
One irony of it is that my read from talking to people is that AI safety is going better
as a technical matter than was expected, and I think worse as a matter of governance and
inter-corporate competition and regulatory arbitrage than they had hoped.
There's a fear, as I understand it, that we could make the technical breakthroughs needed,
but that the kind of coordination necessary to go slow enough to make them, that's where
a lot of the fear is.
I think they feel like that's actually going worse, not better.
So one of the big narratives coming out of Sam Altman's firing was that it must have
had something to do with AI safety, and based on my reporting and reporting shared by many
others, this was not an AI safety issue, but it is very much the story that is being discussed
about the whole affair.
The folks who are on the board who are associated with these AI safety ideas, they've taken
a huge hit to their public reputation because of the way they handle the firing and all
of that.
So I think a really bad outcome of this firing is that the AI safety community loses its
credibility even though AI safety, as far as we can tell, really didn't have a lot to
do with what happened to Sam Altman.
Yeah, I first agree that clearly AI safety was not behind whatever disagreements Altman
on the board had.
I heard that from both sides of this, and I didn't believe it, and I didn't believe
it, and I finally was convinced of it.
I was like, you guys had to have had some disagreement here.
It seems so fundamental, but this is sort of what I mean, the governance is going worse.
All the open AI people thought it was very important, and Sam Altman himself talked about
its importance all the time, that they had this nonprofit board connected to this non-financial
mission, the values of building AI that served humanity that could fire Sam Altman at any
time, or even shut down the company fundamentally if they thought it was going awry in some
way or another.
And the moment that board tried to do that, now I think they did not try to do that on
very strong grounds, but the moment they tried to do that, it turned out they couldn't.
That the company could fundamentally reconstitute itself at Microsoft, or that the board itself
couldn't withstand the pressure coming back.
I think the argument from the board's side, the now mostly defunct board, is that this
didn't go as badly for them as the press is reporting, that they brought in some other
board members who are not cronies of Sam Altman and Greg Brockman.
Sam Altman and Greg Brockman are not on the board now, there's going to be investigation
into Altman.
So maybe they have a stronger board that is better able to stand up to Altman.
That is one argument I have heard.
On the other hand, those stronger board members do not hold the views on AI safety, that the
board members who left, like Helen Toner of Georgetown and Tasha McCauley from Rand, held.
I mean, these are people who are going to be very interested in whether or not Open
Hat is making money.
I'm not saying they don't care about other things too, but these are people who know
how to run companies.
They serve on corporate boards in a normal way, where the output of the corporate board
is supposed to be shareholder value, and that's going to influence them even if they understand
themselves to have a different mission here.
Am I getting that story wrong to you?
No, I think that's right, and it speaks to one of the most interesting and sort of
strangest things about this whole industry, is that the people who started these companies
were weird, and I say that with no sort of like normative judgment, but they made very
weird decisions, like they thought AI was exciting and amazing.
They wanted to build AGI, but they were also terrified of it to the point that they developed
these elaborate safeguards.
In Open AI's case, they put this non-profit board in charge of the for-profit subsidiary
and gave essentially the non-profit board the power to push a button and shut down the
whole thing if they wanted to.
At Anthropic, one of these other AI companies, they are structured as a public benefit corporation,
and they have kind of their own version of a non-profit board that is capable of essentially
pushing the big red shut it all down button if things get too crazy.
This is not how Silicon Valley typically structures itself.
Mark Zuckerberg was not in his Harvard dorm room building Facebook thinking, if this thing
becomes the most powerful communication platform in the history of technology, I will need
to put in place these checks and balances to keep myself from becoming too powerful.
But that was the kind of thing that the people who started Open AI in Anthropic were thinking
about, and so I think what we're seeing is that that kind of structure is sort of bowing
to the requirements of shareholder capitalism, which says that if you do need all this money
to run these companies to train these models, you are going to have to make some concessions
to the sort of powers of the shareholder and of the money.
And so I think that one of the big pieces of fallout from this Open AI drama is just
that Open AI is going to be structured and run much more like a traditional tech company
than this kind of holdover from this non-profit board.
And that is just a sad story.
I truly wish that it had not worked out that way.
I think one of the reasons why these companies were built in this way was because it just
helped them attract better talent.
I think that so many people working in AI are idealistic and civic-minded and do not
want to create harmful things, and they're also really optimistic about the power that
good technology has.
And so when those people say that as powerful and good as these things could be, it could
also be really dangerous, I take them really seriously, and I want them to be empowered.
I want them to be on company boards.
And those folks have just lost so much ground over the past couple of weeks, and it is a
truly tragic development, I think, in the development of this industry.
One thing you could just say with that is, yeah, it was always going to be up to governments
here, not up to strange non-profit corporate, semi-corporate structures.
And so we actually have seen a huge amount of government activity in recent weeks.
And so I want to start here in the U.S. Biden announced a big package of a big executive
order.
You could call them regulations.
But Casey, how would you describe in some what they did?
What is the Biden administration's approach that it is signaling to regulating AI?
The big headline was, if you are going to train a new model, so a sort of successor to
a GPT-4, and it uses a certain amount of energy, and the energy there is just sort of a proxy
for how powerful and capable this model might be, you have to tell the federal government
that you have done this, and you have to inform them what safety testing you did on this model
before releasing it to the public.
So that is the one kind of break that they attempted to put on the development of this
industry.
It does not say you can't train these models.
It doesn't specify what safety tests you have to do.
It just says, if you're going to go down this road, you have to be in touch with us.
And that will, I think, slightly decelerate the development of these models.
I think critics would say it also pushes us a little bit away from a more open source
version of AI, that open source development is sort of chaotic by its nature.
And if you want to do some sort of giant open source project that would compete with the
GPTs of the world, that would just sort of be harder to do.
But to me, those are sort of the big takeaways.
One of the things that struck me looking at the order was, go back a year, go back two
years.
I think the thing that people would have said is that the government doesn't understand
this at all.
It can barely be conversant in technology.
People remember Senator Orrin Hatch asking Mark Zuckerberg, well, if you're not making
people pay, then how do you make money?
When I read the order and looked at it, this actually struck me as pretty seriously engaged.
Like for instance, there's a big debate in the AI world about whether or not you're going
to regulate based on the complexity and power of the model or the use of the model.
You have a fear about what happens if you're using the model for medical decisions.
But if you're just using it as your personal assistant, who cares?
Whereas the AI safety people have the view that, no, no, no, the personal assistant model
might actually be the really dangerous one because that's one that knows how to act in
the real world.
And the Biden administration takes a real, it actually takes a view of the AI safety people.
If you have a model over a certain level of computing complexity, they want this higher
level of scrutiny, higher level of disclosure on it.
They want everything that comes from an AI to be watermarked in some way so you can see
that it is AI generated.
This struck me as a Biden decision to actually clearly haven't taken this seriously and having
convened some set of group of stakeholders and experts that knew what they were doing.
I mean, I don't necessarily agree with literally every decision and a lot of it is just asking
for reports.
But when you think about it as a framework for regulation, it didn't read to me as a
framework coming from people who had not thought about this for 10 minutes.
Absolutely.
I was quite impressed.
I had a chance to meet with Ben Buchanan at the White House who worked on this, talked
to him about this stuff, and it is clear that they have been reading everything, they've
been talking to as many people as they can, and they did arrive in a really nuanced place.
And I think when you look at the reaction from the AI developers in general, it was mostly
neutral to lightly positive, right?
There was not a lot of blowback, but at the same time, folks in civic society, I think,
were also excited that the government did have a point of view here and had done its
own work.
Yeah.
It struck me as a very deft set of, I think I would agree that there are more like pre-regulations
than regulations.
And to me, it sounded like what the Biden White House was trying to do was throw a few
bones to everyone.
What was was like, we're going to throw a few bones to the AI safety community who worries
about foundation models becoming too powerful.
We're going to throw some bones to the AI harms community that is worried about things
like bias and inaccuracy, and we're going to throw some bones to the people who worry
about foreign use of AI.
So I saw it as a very sort of deliberate attempt to give every sort of camp in this debate
a little to feel happy about.
One of the things it raised for me as a question though was, did it point to a world where
you think that regulators are going to be empowered to actually act?
This was the thing I was thinking about after the board collapse.
You imagine a world sometime in the future where you have open AI, you know, with GPT-6
or META or whomever, right?
And they are releasing something that the regulators looking at the safety data, looking
at what's there, they're just itchy about it.
It's not obviously going to do a ton of harm, but they're not convinced it's safe.
They've seen some things that worry them.
Are they really going to have the power to say, no, we don't think your safety testing
was good enough?
When this is a powerful company, when they won't be able to release a lot of the proprietary
data, right?
The thing where the board could not really explain why they were firing Sam Altman struck
me as almost going to be the situation of virtually every regulator trying to think
about the future harms of a model.
If you're regulating in time to stop a thing from doing harm, it's going to be a judgment
call.
And if it's a judgment call, it's going to be a very hard one to make.
And so if we ever got to the point where somebody needed to flip the switch and say, no, does
anybody actually have the credibility to do it?
Or is what we've seen that in fact, like these very lauded successful companies run by smart
people who have huge Twitter followings or threads followings, whatever they end up being
on that they actually have so much public power that they'll always be able to make
the case for themselves.
And like the political economy of this is actually that we better just hope the AI companies
get it right because nobody's really going to have the capability stand in front of them.
When you talk to folks who are really worried about AI safety, they think that there is
a high possibility that at some point in let's say the next five years, AI triggers some sort
of event that kills multiple thousands of people.
What that event could be, you know, we could speculate, but assume that that is true.
I think that changes the political debate a lot, right?
Like that's just all of a sudden you start to see jets get scrambled.
Obviously that never happens, but I think that will be the inciting moment.
And this is the thing that just frustrates me as somebody who writes about tech policy
is we just live in a country that doesn't pass laws.
There are endless hearings, endless debates, and then it gets time to regulate something.
And it's like, well, yeah, they can regulate AI, but it's going to be based on this one
regulation that was passed to deal with like, you know, the oat farming crisis of 1906.
And we're just going to hope that it applies.
It's like, we should pass new laws in this country.
I don't know that there's a law that needs to be passed today to ensure, you know, that
all of this goes well, but certainly Congress is going to need to do something at some point
as the stuff evolves.
I mean, one thing I was thinking about as this whole situation at OpenAI was playing
out was actually the financial crisis in 2008 and the scenes that were captured in books
and movies where, you know, you have the heads of all the investment banks and they're scrambling
to avoid, you know, going under and they're meeting in these boardrooms with people like
Ben Bernanke, the chair of the Federal Reserve, and the government actually had a critical
role there in patching together the financial system because they were sort of interested
not in which banks survived and which failed, but in making sure that there was a banking
system when the markets opened the next Monday.
And so I think we just need a new regulatory framework that does have some kind of the sort
of cliched word would be stakeholder, but someone who is in there as a representative
of the government who is saying, what is the resolution to this conflict that makes sense
for most Americans or most people around the world?
When you looked at who the government gave power to in this document, when you think
about who might play a role like that, when you need to call the government on AI, the
way I read it is it spread power out across a lot of different agencies.
And there were places where invested more rather than less, but one thing that different
people have called for that I didn't see it do, in part because you would actually need
to pass a law to do this was actually create the AI department, something that is funded
and structured and built to do this exact thing, to be the central clearinghouse inside
the government, to be led by somebody who would be the most credible on these issues
and would maybe have then the size and strength to do this kind of research, right?
The thing that is in my head here, because I find your analogy really compelling, Kevin,
is a Federal Reserve.
The Federal Reserve is a big institution, and it has a significant power of its own
in terms of setting interest rates.
It also does a huge amount of research, like when you think about where would a public
option for AI come from, you need something like that that has the money to be doing its
own research and hiring really excellent people, in that case economists, in this case AI researchers.
And there's nothing like that here.
It was sort of an assertion that we more or less have the structure we need.
We more or less have the laws we need.
We can apply all those things creatively, but it did not say like this is such a big
deal that we need a new institution to be our point person on it.
Yeah.
I mean, I think that's correct.
I think there are some reasons for that, but I think you do want a government that has
its own technological capacity when it comes to AI, previous waves of innovation, certainly
nuclear power during the Manhattan Project, but also things like the Internet came out
of DARPA.
These are areas where the government did have significant technical expertise and was building
its own technology in sort of competition with the private sector.
There is no public sector equivalent of chat GPT.
The government has not built anything even remotely close to that.
And I think it's worth asking why that is and what would need to happen for the government
to have its own capacity, not just to evaluate and regulate these systems, but to actually
build some of their own.
I think it is genuinely strange on some level that given how important this is, there is
not a bill gathering steam.
Look, if the private sector thinks it is worth pumping 50 or $100 billion into these companies
so they can help you make better enterprise software, it seems weird to imagine that there
are not public problems that have an economic value that is equal to that or significantly
larger.
And we may just not want to pay that money fine, but we do that for infrastructure, right?
I mean, we just passed a gigantic infrastructure bill.
And if we thought of AI-like infrastructure, we actually also spend a lot of money on broadband
now.
It seems to me you want to think about it that way.
And I think it is a kind of fecklessness and cowardice on the part of the political culture
that it no longer thinks itself capable of doing things like that.
At the very least, and I've said this, I think on your show probably, I think they should
have prize systems where they say a bunch of things they want to see solved.
And if you can build an AI system that will solve them, they'll give you a billion dollars.
But the one thing is the government does not like to do things that spend money for an
uncertain return.
And building a giant AI system is spending a lot of money for an uncertain return.
And so the only part of the government that is probably doing something like it is a defense
department in areas that we don't know, and that does not make me feel better, that makes
me feel worse.
That's my take on that.
Yeah.
I mean, I think there's also a piece of this that has to do with labor and talent.
There are probably on the order of several thousand people in the world who can oversee
the building, training, fine-tuning, deployment of large language models.
It is a very specific skill set.
And the people who have it can make gobs of money in the private sector, working wherever
they want to.
The numbers that you hear coming out of places like open AI for what engineers are being
paid there.
I mean, it's like NFL level football compensation packages for some of their people.
And the government simply can't or won't pay that much money to someone to do equivalent
work for the public sector.
Now, I'm not saying they should be paying engineers millions of dollars of taxpayer
money to build these things, but that's what you would need to do if you wanted to compete
in an open market for the top AI talent.
I am saying they should.
I am saying this is like the factlessness and cowardice point.
This is stupid.
You think there should be AI engineers working for the federal government making $5 million
a year?
Like maybe not $5 million a year.
This thing that we don't think civil servants should make as much as people in the private
sector because, I don't know, somebody at a congressional hearing is going to stand
and be like, that person's making a lot of money.
That is a way we rob the public of value.
If Google's not wrong, Microsoft is not wrong that you can create things that are of social
value through AI.
And if you believe that, then leaving it to them, I mean, they intend to make a profit.
Why shouldn't the public get great gains from this?
It won't necessarily be through profit, but if we could cure different diseases or make
big advances on energy, this way of thinking is actually, to me, the really significant
problem.
I'm not sure you would need to pay people as much as you're saying because I actually
do think a lot of, I mean, we both know the culture of the AI people and at least up until
a year or so ago.
It was weird, and a lot of them would do weird things and are not living very lush lives.
They're in group houses with each other, taking psychedelics and working on AI on the weekdays.
But I think you could get people in to do important work, and you should.
Now, look, you don't have the votes to do stuff like this.
I think that's the real answer.
But in other countries, they will and do.
When Saudi Arabia decides that it needs an AI to be geostrategically competitive, it
will take the money it makes from selling oil to the world, and in the same way that
it's currently using that money to hire sports stars, it will hire AI engineers for a bazillion
dollars, and it will get some of them, and then it will have a decent AI system one day.
I don't know why we're waiting on other people to do that.
We're rich.
It's stupid.
I agree with you, Ezra, and I'm sorry that Kevin is so resistant to your ideas because
I think paying public service well would do a lot of good for this country.
I think public service should be paid well.
I'm just saying, when Jim Jordan gets up and grills the former deep mind engineer about
why the Labor Department is paying them $2.6 million to fine-tune language models, I'm
not sure what the answer is going to be.
No, I agree with you.
I think we're all saying in a way the same thing.
This is a problem.
Government by dumb things Jim Jordan says is not going to be a great government that
takes advantage of opportunities for the public.
Good.
And that sucks.
It would be better if we were doing this differently and if we thought about it differently.
Let me ask about China because China is where on the one hand, at least on paper, the regulations
look much tougher.
So one version is maybe the regulating AI much more strictly than we are.
Another view that I've heard is that, in fact, that's true for companies, but the Chinese
government is making sure that it's building very, very strong AIs.
Do the extent you all have looked at it, how do you understand the Chinese regulatory
approach and how it differs from our own?
I've looked at it mostly from the standpoint of what are the consumer-facing systems look
like.
It has only been, I think, a couple of months since China approved the first consumer usable
chat GPT equivalent.
As you might imagine, they have very strict requirements as far as what the chatbot can
say about Tiananmen Square.
So they wind up being more limited maybe than what you can use in the United States.
As far as what is happening behind closed doors and for their defense systems and that
sort of thing, I'm in the dark.
So four or five years ago when I started reporting a book about AI, the conventional wisdom among
AI researchers was that China was ahead and they were going to make all of the big breakthroughs
and beat the U.S. technology companies when it came to AI.
So it's been very surprising to me that in the past year since chat GPT has come out,
we have not seen anything even remotely close to that level of performance coming out of
a Chinese company.
Now, I do think they are working on this stuff, but it's been surprising to me that China
has been mostly absent from the frontier AI conversation over the past year.
And do you think those things are related?
Do you think that the Chinese government's risk aversion and the underperformance of at
least the products and systems we've seen in China, I mean, there might be things we
don't know about.
Do you think those things are connected?
Absolutely.
I think you do need a risk appetite to be able to build and govern these systems because
they are unpredictable.
We don't know exactly how they work.
And what we saw, for example, with Microsoft was that they put out this Bing Sydney chatbot
and it got a lot of attention and blowback and people reported all these crazy experiences.
And in China, if something like that had happened, they might have shut the company down or they
might, you know, it might have been deemed such an embarrassment that they would have
radically scaled back the model.
And instead, what Microsoft did was just kind of say, like, we're going to make some changes
to try to prevent that kind of thing from happening, but we're keeping this model out
there.
We're going to let the public use it and they'll probably discover other crazy things and that's
just part of the learning process.
That's something that I've been convinced of over the past year talking with AI executives
and people at these companies is that you really do need some contact with the public
before you start learning everything that these models are capable of in all the ways
that they might misbehave.
What is the European Union trying to do?
They've had, you know, draft regulations that were seen very expansive.
What has been the difference in how they're trying to regulate this versus how we are
and what in your understanding is the status of their effort?
Europe was quite ahead with developing its AI Act, but it was written in a pre-chat GPT
world.
It was written in a pre-generative AI world.
And so over the past year, they've been trying to retrofit it so that it kind of reflects
our new reality and is caught up in debate in the meantime.
But my understanding is the AI Act is not particularly restrictive on what these companies
can do.
So to my understanding, there's nothing in the AI Act that is going to prevent these
next generation technologies from being built.
It's more about companies being transparent.
Let me add a little bit of flavor to that because I was in Europe just recently talking
with some lawmakers and one of the things that people will say about the AI Act is that
it has this risk-based framework where different AI products are sort of evaluated and regulated
based on these classifications of this is a low-risk system or this is a high-risk system
or this is a medium-risk system.
And so different rules apply based on which of those buckets a new tool falls into.
And so right now, what a lot of regulators and politicians and companies and lobbyists
in Europe are arguing about is what level of risk should something like a foundation
model, a GPT-4, a barred, a clawed.
Are those low-risk systems because they're just chatbots or are they high-risk systems
because you can build so many other things once you have that basic technology?
And so that's sort of what my understanding is of the current battle in Europe is over
whether foundation models, frontier models, whatever you want to call them, whether those
should be assigned to one risk bucket or another.
I think that's a good survey of the waterfront.
And so I guess I'll end on this question, which is, all right, we're talking here at
the one-year anniversary roughly of ChatGPT.
If you were to guess, if we were having another conversation a year from now on the two-year
anniversary, what do you think would have changed?
What are one or two things each of you think is likely to happen over the next year that
did not happen this year?
I think all communication-based work will start to have an element of AI in it.
All email, all presentations, office work, essentially, AI will be built into all the
applications that we use for that stuff.
And so it'll just be sort of part of the background, just like autocomplete is today when you're
typing something on your phone.
I would say that AI is going to continue to hollow out the media industry.
I think you're going to see more publishers turning to these really bad services that
just automate the generation of copy.
We'll see more sort of content farms springing up on the web.
It'll reduce publisher revenue and we'll just see more digital media businesses either get
sold or sort of quietly go out of business.
And that's going to go hand-in-hand with the decline of the web in general.
A year from now, more and more people are going to be using ChatGPT and other tools as
they're kind of front door to internet knowledge.
And that's just going to sap a lot of life out of the web as we know it.
So we don't need one more technological breakthrough for any of that to happen.
That's just a case of consumer preferences taking a while to change and I think it's
well underway.
So do you think then that next year we're going to see something that has been long
predicted, which is significant AI related job losses?
Is that sort of the argument you're making here?
I think that to some degree it already happened this year in digital media and yes, I do think
it will start to pick up.
Just keep in mind, 12 months is not a lot of time for every single industry to ask
itself, could I get away with five or 10 or 15% fewer employees?
And as the end of this year comes around, I have to believe that in lots and lots of
industries people are going to be asking that question.
Yeah, I agree.
I don't know whether there will be sort of one year where all the jobs that are going
to vanish, vanish.
I think it's more likely to be a slow trickle over time and it's less likely to be mass
layoffs than just new entrants that can do the same work as incumbents with many fewer
people.
I think there's a software development firm that only needs five coders because all their
coders are using AI and they have software that is sort of building itself, competing
with companies that have 10,000 engineers and doing so much more capably.
So I don't think it's going to necessarily look like all the layoffs hit on one day or
in one quarter or even in one year, but I do think we're already seeing a displacement
of jobs through AI.
Those are kind of dark predictions.
I mean, we'll have a little bit better sort of integration of AI into office tools and
also we'll begin to see really the productivity improvements, create job losses.
Is there anything that you think is coming down the pike technologically that would be
really deeply to the good things that are not too far from fruition that you think will
make life a lot better for people?
I mean, I love the idea of universal translators.
It's already pretty good using AI to speak in one language and get output in another,
but I do think that's going to enable a lot of cross-cultural communications and there
are a lot of products remaining to be built that will essentially just drop the latency
so that you can talk and hear in real time and have it be quite good.
So that's something that makes me happy.
And I'm hopeful that we will use AI, not we, as in me and Casey.
But we might.
This would be sort of a career change for us.
But we as in society, I have some hope that we will use AI to cure one of the sort of
top deadliest diseases, cancer, heart disease, Alzheimer's, things like that, that really
affect massive numbers of people.
I don't have any inside reporting that we are on the cusp of a breakthrough, but I know
that a lot of energy and research and funding is going into using AI to discover new drugs
and therapies for some of the leading killer diseases and conditions in the world.
And so when I want to feel more optimistic, I just think about the possibility that all
of that bears fruit sometime in the next few years.
And that's pretty exciting.
All right.
And then always a final question.
What are a few books you'd each recommend to the audience released?
Recommend the audience ask chat GPT to summarize for them.
Kevin, you want to go first?
Sure.
I actually have two books in a YouTube video.
The two books, one of them is called Electrifying America by David E.
Nye.
It is a 30 year old history book about the process by which America got electricity.
And it has been very interesting to read.
I read it first a few years ago and have been rereading it just to sort of sketch out what
would it look like if AI really is the new electricity?
What happened the last time society was transformed by a technology like this?
The other book I'll recommend is Your Face Belongs to Us by my colleague, our colleague
at the Times, Kashmir Hill, which is about the facial recognition AI company, Clearview AI,
and is one of the most compelling tech books I've read in a few years.
And then the YouTube video I'll recommend was just posted a few days ago.
It's called Intro to Large Language Models.
It's made by Andre Carpathi, who is an AI researcher actually at OpenAI.
And it's his one hour introduction to what is a large language model and how does it work?
And I've just found it very helpful for my own understanding.
Casey?
Well, as we're with permission, and given that Kevin has just given your listeners
two great books and a YouTube video to read, I would actually like to recommend
three newsletters if I could.
And the reason is because the books that were published this year did not help me really
understand the future of the AI industry.
And to understand what's happening in real time, I really am leaning on newsletters
more than I'm leaning on books.
So is that okay?
Yeah, go for it.
All right.
So the first one, cruelly, you already mentioned earlier in this podcast,
it's import AI from Jack Clark.
Jack co-founded Amthropic, one of the big AI developers.
And it is fascinating to know which papers he's reading every week that are helping him
understand this world.
And I think that they're arguably having an effect on how Amthropic is being created
because he is sitting in all of those rooms.
So that is just an incredible weekly read.
I would also recommend AI snake oil from the Princeton professor,
Arvind Narayanan and a PhD student at Princeton, Syash Kapoor.
They're very skeptical of AI hype and doomsday scenarios,
but they also take the technology really seriously and have a lot of smart thoughts
about policy and regulation.
And then the final one is Pragmatic Engineer by this guy, Gurgely Arose.
He's this former Uber engineering manager.
And he writes about a lot of companies, but he writes about them as workplaces.
And I love when he writes about open AI as a workplace.
He interviews people there about culture and management and process.
And he just constantly reminds you they're just human beings showing up to the office
every day and building this stuff.
And it's just a really unique viewpoint on that world.
So read those three newsletters.
You'll have a little better sense of what's coming for us in the future.
What Casey didn't say is that he actually hasn't read a book in 10 years.
So it was a bit of a trick question.
You know what I will say?
I did read Your Face Belongs to Us by Cashin.
Incredible book.
Definitely read that one.
Sure you did.
There you go.
Casey Newton, Kevin Roos at your podcast, which requires very little reading.
It's hard for it.
Thank you all for being on the show.
It's the first illiterate podcast, actually, put out by The New York Times.
Thank you for having us.
Thanks, Ezra.
Thanks, guys.
This episode of The Ezra Klein Show is produced by Roland Hoof, fact checking by Michelle Harris
with Kate St. Clair and Mary Marge Locker.
Our senior engineer is Jeff Geld.
Our senior editor is Claire Gordon.
The show's production team also includes Emma Vagabou and Kristen Lin.
Original music by Isaac Jones.
Audience strategy by Christina Semelowski and Shannon Busta.
The executive producer of New York Times opinion audio is Enero Strasser.
And special thanks to Sonia Herrero.

To support our discussions with the world's leading thinkers, please see the video description
or visit eism.eu forward slash support.
Thank you.
Hello and welcome to a theory of everything.
Today we're talking to Connor Lehi, a prominent figure in the field of artificial intelligence.
Lehi is the CEO of Conjecture and is at the forefront of efforts to ensure that AI doesn't
kill the human species.
This is not as crazy as it sounds.
Virtually every major personality and expert on this topic has acknowledged that the probability
of AI ending human life is non-zero.
Regardless of where exactly you land on this question, you're likely to get something out
of today's exchange, including a few words about Lehi's own work on AI safety, which
he feels optimistic about.
To put this optimism into perspective, remember that we recently talked to Scott Aronson,
the head of AI safety at OpenAI, the creators of ChatGPT, who openly admitted that he had
no idea how to protect humans from the potential downsides of runaway artificial intelligence.
Other notable highlights of this exchange are the brief descriptions of our own work
on the threat of human extinction and a recent proposal we turned into the EU in collaboration
with a team of top researchers to use large language models and Carl Friston's work on
active inference to simulate an experiment with different models of democracy.
As is almost always the case when it comes to humans, Connor and I have different opinions.
The key question for science, from our perspective, is not which model of the world is right,
but rather how we're going to resolve our differences of opinion about which model
of the world is right and how we're going to do so without destroying ourselves.
That, it seems to me and to us, is the key question.
Still, I think Connor's work is key and fascinating and important, so I appreciate his time and
yours as well.
As always, you'll find a timestamp in the video description, and here, alas, is my
exchange with Connor Lehi.
Hello, everybody, and hello, Connor.
Thanks so much for taking the time to talk.
Thanks so much for having me.
So you are the chief executive officer of Conjecture, a for-profit AI alignment company
based in London, I think.
I'm the executive director of ASEM, non-profit research center based in Barcelona.
It's really interesting to see that in some sense we're both working on the same thing,
which is long-term human survival, maybe even short-term human survival.
We take different angles in terms of identifying the root cause of the problem, and our proposed
solutions are also a bit different, but our goal, I think, is pretty much the same.
We both perceive an imminent threat to human existence, and we both want to reduce or eliminate
that threat.
So would you be nice enough to give listeners a layout of the landscape from your perspective
in terms of AI alignment and the threat it poses to human existence?
Absolutely.
And if it's okay, let's assume for listeners that they're not experts, in fact, throughout
our conversations today, if we keep things in general as possible, or at least zoom out
to a general panorama, it might be easier for us to understand each other and for listeners
to follow us.
Yep, absolutely.
So I'm happy to talk about that.
There's a lot of arguments and discussions around AI, AGI, and so on that are very technical.
And one of the things I really feel is that a lot of this technicality is just noise.
It's kind of missing that the core problems here are quite universal and quite easy to
understand.
And I think you'll find that we actually agree on a lot of things about the risks and where
they're coming from.
Because really, the way I think about things are, is that the thing I care about is long
term civilization flourishing, humanity flourishing, us getting a good future, I mean, a good future,
a long future.
To do this, we have to, of course, leverage technology.
Technology gives us great power.
But power is neutral.
It's neither good nor bad.
Having more power of the environment allows you to do more good things, but it also increases
the blast radius, so to speak, of both accidents and misuses of technology.
What is the worst thing you can do with stone age technology?
You can call a couple of guys over the head with a rock.
What's the worst thing you can do with medieval technology?
Well, maybe if you have chariots and long bows and so on, kill a couple of people with
that, or maybe you could use tar to burn down a city or something, pretty bad.
What's the worst thing you can do with gunpowder?
What's the worst thing you can do with chemical weapons?
What's the worst thing you can do with nuclear power?
The worst thing is not just nuclear weapons, but also what happens if there's a massive
nuclear accident or whatever and you radiate the greater Tokyo metropolitan area, that's
a level of destruction that was just impossible to even cause with previous technologies.
The way things are is that our technology increases in a sense exponentially.
This is both good and bad.
It's good because we get all these great things, all these wonderful things we can do with
our computers and our air conditioned office buildings and whatnot, but it also has this
downside of we have exponentially increasing danger from both accidents and whether military
or otherwise misuse of technology.
And if we don't have a way to deal with this, then eventually there will be technology that
is so powerful that a single accident blows up everything.
And there's no retries.
And after that, it's just over.
So AGI is the obvious first technology that falls into that category, in my view, if only
because AGI can instantly, can very quickly invent any other technology.
If you have AGI that is, so this is AI systems that are as intelligent or more intelligent
than humans.
If you have such systems, well, logically, those systems can invent any new dangerous
technology that humans might invent and they can do it faster and more efficiently.
So if we're worried about synthetic bio weapons or worried about who knows, some kind of future
device that we don't even know about yet, AGI, AI would be a quick way to achieve this.
And if we don't control these AI systems, well, if it's smarter than us, more capable
at politics, deception, business, science, and everything else, well, what do you think
is going to happen?
Just all things being equal, the future will belong to the machines, not to humans, not
to us.
They will be more competent.
And we do not know how to control these things.
So this is the question of alignment and control is this question of not just how you build
a smart AI system, but how do you build a smart AI system that does what you want or
even stronger stronger than this, that wants the same thing you want.
And now this is a incredibly difficult problem, as I'm sure you can imagine.
There's technical problems, but it's also even like just philosophical problems.
What does it mean to want what we want?
What does it even mean?
What do we want?
I'm not saying I have an answer to this question by any means, but unfortunately, as we see
our computers, you know, improving exponential rate, our AI systems become more and more
general purpose, more and more autonomous.
I don't think we have much time to figure all of this out.
So the first, so what I work on specifically, like conjecture, is a subset of the alignment
problem.
Alignment would be the problem of, in my view, how do we make a system that is fully aligned
with our values that wants what humanity wants, that wants humanity to be great, that wants
to do the right thing, et cetera.
This is very hard, very, very hard.
The simpler problem that we work on as a stepping stone is the problem of control, is how do
you make an AI system that does exactly what you tell it to do and nothing else?
We approach this through an approach that we call cognitive emulation, or co-M, which
is kind of about emulating human-like reasoning.
We talk about that a bit later.
But I also moonlight with a nonprofit advocacy group called Control AI, which are working
on the social and political problems of this AI and AGI transition.
How do we, as a civilization, deal with this, that this technology is being developed?
Is it a good idea that such powerful technology is being developed without any supervision
out of control by private companies maximizing for shareholder returns?
It seems like a very dangerous way to be developing this technology.
If this is a danger, is what developed technology?
What is a better way to develop this technology, and how can we get there?
These are very, very hard questions.
These are very political questions.
They're very social questions.
I don't have answers to everything, but I have a few pointers at least.
Excellent.
Talk about some of the people who don't feel as concerned as conjecture and the AI alignment
community.
What are they doing?
I think there's kind of like two main camps, or not camps, two main objections to this idea
that AI will be a large risk.
One is AI is in the thing, or it won't happen soon.
It's either it's far away, it'll take a long time, or it's impossible for some reason.
It's impossible to make AI smarter than human, for example, or it will take a thousand years.
The other position is it is possible to create AI, and we will probably do it soon, and it
will be fine.
Don't worry about it for various reasons.
So there's a couple of different ways how people justify these various things.
So for the first group of people, I think I would just kind of like point to the progress
around us, and that even, you know, you can talk to your computer now.
You couldn't talk to your computer three years ago.
Now you can, and this is crazy, and it keeps getting better.
And there's a clear path forward, and we have, you know, experts around the world now
unambiguously saying, no, it's coming soon, and it's a big problem.
So I think there it becomes an object level technical disagreement.
I don't think it's a very interesting argument to be having.
I think if we agree that there's even a 10%, 20%, 50% chance, whatever, of AGI emerging
in the next 10 years or whatever, it would absolutely be worth us taking seriously.
Like imagine we had a 10% chance an asteroid will hit Earth in 10 years and kill everybody.
Obviously, everyone would be spending massive amounts of time working on this problem.
So I don't think this is the more interesting objection.
It is a worthwhile objection, and it's worth discussing on an object level, but the other
objection I find a bit more interesting, which is this objection that, well, maybe
if we just build AI, it'll just be nice, you know, maybe it'll be good, it will just
help us, we'll keep it under control, maybe controlling it is easy, et cetera.
And I think there is kind of a saying where like you have to be educated to believe
something so stupid, where there is a core intuition that most people have when you
first introduce them to the concept of AI and AGI, we say, hey, some people, some
technologists are building systems that they think will be smarter and more powerful
than humanity, and they don't really know how to control them.
How do you feel about this?
And the default reaction is, what the hell?
Like, of course, this is going to go poorly.
Like, have you literally never seen a movie before?
Have you never thought about this for 10 minutes?
Like, of course, this is going to go poorly.
And now the reason people form this intuition is often because of movies, which
or whatever, which is not a good source of truth.
But that doesn't mean the intuition is wrong.
There's also very good ways, ways intuition comes from, such as, you know,
look at colonizers from previous centuries, when a group of more technologies
superior moved into a land of a less technologically sophisticated peoples.
It often ended very, very poorly for the less technologically sophisticated
peoples to great horror often.
Or even further, think about humans and animals.
There's a reason that chimps don't run the world.
You know, chimps may be stronger than us physically, but that doesn't matter.
We have guns and society.
We decide what's up.
If we want to build a hydroelectric dam and there's an anthill in the valley,
well, too bad for the ants.
It's not because we hate the ants, not because we're malevolent, because we're
evil, we want to, you know, hurt the ants, but just in the way.
And I think this is what will happen with AI too.
Some people think that what I'm trying to say is that AI will be evil.
It'll be terminated with glowing red eyes.
I don't think that's what's going to happen at all.
I think it will be mundane and it just won't care.
I think it will understand that humans don't want to be killed, but it won't care.
It will be optimizing for something.
Who knows, you know, more power, more profit, whatever.
And in the process of doing so, humans will be in the way.
They're annoying.
They're confusing.
And eventually you'll want to replace them with robots or solar cells or whatever.
And to do that, you know, why would you keep the humans alive?
To put it in a more economic term, you can also think of it as the marginal
contribution of humanity to the economy will shrink as more and more labor gets
automated by machines and including intellectual labor until the marginal
contribution of humanity falls to zero and then becomes negative.
Meaning that someone has to pay an economic cost to keep humanity alive.
You know, they need space, they need food, et cetera.
And who will pay that cost?
And by default, an AI system won't pay that cost.
Why would it?
It's in it because we don't know how to make them want to pay this cost and
not be outcompeted by other systems that do not want to pay this cost.
Very good.
You have a very interesting way of talking about intelligence and goals.
Can you elaborate on this?
For me, intelligence and goals are kind of two separate things that relate it.
I like to think of it as, I kind of like to think about minds as kind of like
split into four things.
This is just my personal way of viewing it.
I'm not saying this is the perfect way to do it or whatever.
I just find a useful model.
You can think of a mind as being composed of four parts.
The first part is the world model.
This is what the system knows about its work, about the environment.
It's of what it also knows about itself and what it knows about its
knowledge in general.
The second thing is its epistemology.
This is how does it generate new knowledge?
What systems, what logic, like how does it take in new information and derive
updates to its world model from this?
The third thing is its decision theory, which is given the scenario it is
currently in and the things it wants to accomplish.
How does it make decisions about what to do next?
And the fourth thing is its values or its aesthetics.
These are things that the system wants and it doesn't want them
for any particular reason.
It's just the things it likes.
So it's many goals, many things that humans do are not this well factored.
Our brain is super messy.
It doesn't have four compartments.
All this is mixed up.
For example, receiving money generally feels nice.
If I give you a lot of money, it feels good.
It feels like this is, and a lot of people put as their goal, I want to get money.
This is a goal in a sense of value that maybe lots of people have.
But I think a lot of us will agree that really the reason we're getting money
is not for the money, it's to spend it.
If you want to spend it, then we can spend time with our family.
We can go on a cool vacation.
We can buy a toy that we want or good food or whatever.
So for me, values or girl goals or in the technical level, terminal goals
are things you spend money on because you want to spend money on them,
not because they help you for other goals.
It's the thing you ultimately want.
So it might be, as I say, you know, family, pleasure, whatever.
Then separately from this is intelligence.
And for me, intelligence for me is kind of like your general competence.
So this is both how much is in your world model,
like how many things do you know that are useful, how accurate is your world model?
It's how good is your epistemology?
How good are you learning new things?
How good are you in integrating new information with your current world
model in a sensible way?
And it's how good is your decision theory?
How good are you about making decisions?
How good are you at winning at games?
So ultimately, a very intelligent system can want anything.
You could make a system that is very dumb and wants very high level things, you
know, that wants something complicated or something simple.
The same way you can make something that is extremely smart.
It's extremely competent, but it only wants to collect shiny rocks or whatever.
Now, of course, evolution would never produce such a mind.
It would never produce a super intelligent that wants rocks.
But you could code it in a computer if you wanted to, not saying you should,
but it's possible.
So similarly, if we build AI systems and we don't understand how to control them
and how to control what goals they have, by default, they won't have a cleanly
separated goal, it'll be a mess.
They'll have some thing somewhere where they like, they kind of want to do something.
And a lot of these goals will be contingent and they will be about gaining resources,
gaining money, gaining power.
And the reason is, is because those things are useful.
Most people like money, not because we all have a gene, you know, in our genome
that tells us to like money, but because we learn that, oh, wow, if I have a lot
of money, then I can do many things.
So therefore it makes sense for me to get more money.
I think a similar thing will happen with AI systems, where if we train them to do
lots of different things, they'll notice, huh, it's really helpful if I get money.
So I should get more of that.
Or it's really helpful if I can deceive humans and trick them, because then
they'll do a bunch of stuff for me.
So I should get better at that.
So this is kind of how I think about it.
Yeah, very interesting.
So you're kind of referring to a type of evolution then in that sense.
No, if a machine gains intelligence, then they will in some sense learn and evolve
to toward their specific goals, or even if they don't have goals, right?
Very interesting.
The, the, uh, do you have a favorite approach to physics and artificial intelligence
or to quantum mechanics?
What are your, what are your thoughts about connecting the two?
Is there a relationship between the two?
I think they're kind of a different levels of extraction.
I think that intelligence is a very high level concept.
It's not something that exists in the physics.
It's something that exists at a much higher level.
The same way that, you know, we might say the United States of America is a useful
concept for us to talk about, but it doesn't exist physically.
It doesn't have a location.
There's no atoms that compose the United States of America.
It's an abstraction.
The same way like software is an abstraction.
Like where is the Google search engine?
It's no specific location.
It's data, it's information, distributed information.
I think of intelligence kind of in a similar way.
It's software that can run on many types of hardware.
So there are interesting questions, such as, for example, what are efficient
hardware designs is an interesting question.
And it might, there might be interesting questions around epistemology and quantum
physics, for example, like how should you reason about the world given that, you
know, you're in a quantum physical world?
How should this change your epistemology?
But I think a lot of these stuff is contingent stuff.
So I have takes on quantum and physics as any, you know, and any Twitter
skits are worth their salt has, of course.
But I think, and I think there's a lot of things to learn, but I'm more interested
on the AI side and the kind of a level above that.
Like the epistemology of like, how do you generate physical theories?
I think it's very interesting.
It's much more interesting to me personally than like, is this interpretation
of quantum physics or for this one, I'm much more interested.
How do you answer these questions?
How would an intelligent mind approach this question?
If that makes sense.
Yeah, that makes a lot of sense.
Do you think that I always argue the following that amongst the different
interpretations in science as a whole and in quantum physics in particular, that
I always say that that theory or that interpretation that most leads or most
effectively leads to long-term human survival should by default to be the
preferable interpretation.
So if a person's interpretation does has nothing to say about that, about human
survival, then it's not as meaningful as one that can be connected, of course, in
a meaningful and scientific way, following the scientific rules.
So I ask about that because we take a, we take a, in terms of control, your
concerns about controlling AI and about the physics of these concerns that, that
both you and I share.
Our approach is that this, the goals that are necessarily at the fundamental
physical level built into all of matter, including AI.
One of them is the principle of least action.
And the principle of least action is, of course, one of the most fundamental
principles in all of physics.
It just permeates almost every aspect of physics.
Not accidentally, I would say Juan Maldesena, who is arguably one of the most
important physicists alive, according to another incredibly important physicist
at Stanford, Leonard Susskind.
Maldesena refers to the principle of least action as the principle of maximal
life.
So he says, and he demonstrates, and he argues that particles at the subatomic
level, down to the quark level or whatever, move in such a way as to maximize
their experience of proper time or their experience of existence.
So this is at the front, from the big bang on, it's been a fundamental driving
principle in all of evolution.
So it stands to reason that this, if this is in fact the way nature works, then
intelligence, like you said, it's at a higher level, but intelligence is always
driving matter in such a way as to exist for a longer period of time.
That would explain, at least to some extent, on a very metaphorical, perhaps
directly literal sense, the universal desire of all living things to remain
alive, why humans don't want to die.
You know, so there seems to be a fundamental physical reason for this.
So the concern about creating an intelligence that we cannot control
almost necessarily implies, like you said, the death of humanity, precisely
because physical material moves and evolves in such a way as to maximize
its own experience of time.
So I think this is a really, my opinion, it's a compelling argument in terms
of why not to build something that you cannot control, right?
So I think this is, I mean, this lays it down in terms of the physics.
Now we get to the control.
How do we actually control?
And that's, of course, one of the primary things you are working on.
And if you really, if we really dig deeply into this question of control,
we necessarily bump up against the idea that all humans have different models
of reality, of different opinions about how things work.
And this seems to be on an equal footing or perhaps even more important
footing than the AI question itself.
Because if we can't agree about what to do, then that doesn't get us any
closer to solving the problem.
So my question to you, my next question to you is, how do you, how do you think
about, you mentioned earlier to the certain group working on the social
and political aspects, how do you think about those questions about political
theory, social theory, and the fact that we can't agree on virtually anything?
How in the world are we going to agree on something like AI or AGI?
So this is a great question.
And the glib answer is, is that like, this is why I am pessimistic.
No, but this is not true.
This is just one of the reasons it's actually much worse than that.
The truth is that this is a very hard problem.
And anyone who tries to sell you something else is selling you snake oil.
This is anyone who says, oh, it'll be fine.
Just don't worry about it.
Or like, oh, we just do X and then it'll be fine.
It's just not truly engaging with how hard this problem is.
The problem we're solving for here is not a software problem.
It's not like, oh, how do we write this algorithm?
How do we solve this math proof?
The problem that we're solving for is, is like, how do we design and implement
a global, you know, stable civilization that fulfills our contradictory values
between, you know, seven billion humans?
This is an astronomically difficult problem.
This is an extremely difficult problem that doesn't mean it's impossible,
but it means it's very difficult.
So with that being said, it's very tempting and fashionable.
Now a day is to talk about how we don't agree on anything.
And it does feel that way, especially if you spend any time on Twitter.
But the truth is, is that it's actually, we actually agree on a lot of things.
There's actually a shocking amount of things that people across the world
actually do agree on, such as, and, you know, there's exceptions, of course.
But like for the most part, you're especially known here in the West, you know,
is that like you don't do violence.
This is a good norm, you know, the fact that we don't, you know, have
violence both politically or otherwise, as we did a hundred, two hundred,
a thousand years ago is really great.
We can agree on a lot of things there.
We can agree on the value of various major currencies.
We can agree on various laws and social norms and communication norms and so on.
It's possible for you to meet a random stranger at a restaurant
you've never met before and you don't have to feel scared because you know
you'll be safe.
This is not typical for the history of mankind.
And it took a lot of work to get to this point.
It took a lot of social work.
It took a lot of political work to build systems to allow these
kind of transactions to happen safely and efficiently.
And I think the kind of things we need to do are on this axis.
In a sense, the thing is, is that it is in no one's interest for humanity
to be destroyed by AGI, except for a couple of misanthropic crazy people.
But for the most part, it is not in people's interest to have, you know,
uncontrolled AGI or bio weapons or whatever.
It's not that people don't want this.
If people didn't, you know, didn't want humanity to survive,
we would be in a bad spot.
But I've checked and most people love their kids.
They love their family.
They love, you know, they might have some disagreements with a lot of people,
but all things being equal, like an average person, if you give them the
chance to press a button that makes someone's day better or not press the button,
they'll press it. All things being equal.
This is really nice.
It's nice that humanity is like this.
Many animals are not like this.
And so if this is the case, why is there a problem?
So there's a great, there's a, one of my, I think my literally favorite,
like piece, like snippet of fiction ever written.
I'm going to butcher it, but it's from the Prince of Apia, discordia,
where the main character is talking to the goddess.
And he says, oh, goddess, you know, things are so terrible.
There's war and famine, brother fights, brother, there's chaos.
It's, you know, it's so, it's so awful.
What do we do?
And the goddess says, well, if that's what you want, you know, what's the problem?
And he says, but no one wants this.
And the goddess says, oh, well, then stop.
And so I wish I could write something this witty, because it's so perfect,
because it's true.
Like there's so much going on that no one wants.
Like no one wants children to be dying of famine somewhere.
No one wants this.
No one wants people to be, you know, victims of violent crime.
No one wants, you know, the stuff.
All this is bad.
And yet it keeps happening.
People keep doing it.
Why?
And there's no simple answer.
There is some, you know, cute answers, some cutesy answers you can give,
such as quoting Moloch, which is a name from a famous essay on this topic,
which just blames everything on this entity called Moloch.
And this entity called Moloch is the entity of Nashek Relivria,
which are not Pareto optimal.
And what this means is, is that they're, if you're playing a coordination game,
if there's a way where a group wants to work together,
you can get into situations where it would be better for everyone to do something else.
But if any individual tries it, they lose.
They can't do it.
The old group has to do it together.
So you want to get the whole group to do this thing,
but if all, but any individual has no incentive to do it.
This is an extremely toxic situation.
This is an extremely bad situation.
And this is extremely common in these political problems.
Often when you have sticky political problems,
the reason they're sticky is usually not because there is not a possible better solution,
but to get to that possible solution, someone's going to lose their job.
Or, you know, the bureaucracy enforcing it would have to cut their budget and they don't want that.
So the ones who could cause the problem would lose from benefiting it,
from implementing it, or it would be annoying to implement it, so they don't.
So how do we deal with these kind of problems?
There are many, many ways how to address these kind of problems,
and most of them are hard.
But overall, what we need is a civilization that is more coherent and capable of coordinating
around these movements.
So one of the most important mechanisms around this is common knowledge.
So common knowledge is different from knowledge.
Knowledge is knowing a fact.
I tell you something, now you know this fact.
You have knowledge, but it doesn't mean that common knowledge exists.
Common knowledge is when you know that I know, that we know, that our group knows,
that they know, that we know, that all of us know, etc.
And this is extremely important because this is the unit of coordination.
Let's say there's three candidates for presidency, A, B, and C.
A is the preferred candidate, he is the incumbent, he's probably going to win,
but everybody hates him.
He just sucks, no one actually likes him.
There's B, who is almost just as bad, but maybe slightly worse, so he gets a little bit less
votes, but he's also terrible, so he has 60-40.
And let's say there's a third candidate, and this third candidate is the best,
like he's actually amazing, he's extremely good, he's just unpopular,
people haven't heard of him before.
So now imagine I go to, you know, the 10 voters, or whatever, and I tell the 10,
each of them individually, and I convince them that C is this amazing candidate,
he's the best candidate, and they should all go vote for C, because he's the best one.
And they might all be convinced of this.
Let's say I convince every single one, what happens on polling day?
Everyone votes for A, because B is even worse, we don't want B.
And well, if I vote for C, that's just wasting my vote and helping B,
because everyone else is going to vote for A, so I have to vote for A as well.
So what is missing here is that all those 10 people have to also tell each other
that they're going to vote for C, and like, oh, you're going to vote for C as well,
so am I, oh, wow, you were two, we're all voting for C, well, fuck yeah, now we can vote C in.
If you don't do this, if you skip this step, C will never win,
even if he's the best candidate, doesn't matter, because everyone will coordinate on A.
One of the most important things to do around the AI problem, and the danger from AI,
is literally just making people aware of it, and not just making people aware of it,
but aware of it in a common sense, that also everyone now knows about it,
everyone knows that you know about it, and it's not something you can ignore,
it's no longer weird to bring up, it's something that your government should have an opinion on,
it's something that your elected official should somewhere have a statement,
and if he doesn't, you should ask him, and it's normal for you to ask,
your friends won't look at you weird for asking about this question.
And once we move these kind of things into the cultural common knowledge,
it's a thing that our civilization can reason about.
So that's step one, well, it's step two, I guess, I mean, step zero,
humanity not wanting to die. Step one, bring the object into memory so that you can reason about it.
Step two, is that you need to make civilization coherent and competent enough to actually act.
There needs to be an actual plan, and there needs to be a way to enact this plan.
And step three, enact the plan. Currently, we're still in step one,
how do we even get civilization to think about these problems?
And then as we're also starting to tackle this problem of, okay,
what can civilization do? How would it be enacted? This is very hard because a lot of our institutions
are not very good, they're very calcified, they're very slow, they're very efficient,
and this limits what kind of plans you can execute. Even if society decides, okay,
we're going to do something, the complexity of the plan is limited by the competency of
the institutions that are executing this plan. So this is a very hard problem. It can be both
improved by improving the quality of institutions, and by making cleverer plans that are better suited
for the system you're working with. You need to do all of this. If you skip any of these steps,
you lose. So this is very hard, and we work on all of these problems.
I agree. It's really, really hard to imagine that kind of scenario because you're running up against,
of course, several academic disciplines that are already very well developed, among them,
social choice theory, which deals with how collective decisions are made and how votes
are aggregated, and there's just inherently difficult problems and challenges to imagine that
we can somehow get everybody to agree or mostly agree on even something as basic as long-term
human survival. Look at the climate change issue. Even scientists can agree on whether or not
climate change is real, unanimously rather. I think most people agree that there is a problem, but
there's still enough controversy to generate quite a bit of people who feel the opposite.
So it's very, very challenging. Are you optimistic that we will be able to do something about this
before the AGI deadline? Something, yes, enough to save us? No. I don't expect we'll make it.
I am quite confident that we're not going to make it out of the century alive, maybe not this decade.
It seems very unlikely at this point to happen. It is not impossible. There is no physical law
that prevents us from just doing the right thing. There is nothing that prevents humanity from
coordinating at this speed. There's nothing that prevents us from solving the alignment problem.
There's nothing that prevents us from delaying AGI creation with laws or institutions or social
norms or whatever. There is no physical reason we can't make it, but there's sure is a hell
of a lot of social pressure going against it. There's a lot of economic incentives against it.
There's a lot of cultural incentives against it. A lot of political malaise that are pushing against
it. If I thought I had like 30 years, 50 years, 100 years time, I would be feeling a lot better
about this because that's a lot of time to make social change. Making social change on a 10, 5,
1 year frame, hard, but not impossible. I'm going to try. I am trying. So are other people. I think
it's important that we give it the best shot we can. It is possible, but it will be very hard.
It also depends on how hard the technical problems are. So there's a massive social and
political problem, but there's also a technical problem. It's neither one nor the other. It's
both. It's that we don't know how hard aligning AI is. It could turn out to be anywhere from
surprisingly easy to almost impossible or completely impossible. And depending on how hard it is,
we'll change how, what kind of plans we need. If it's literally impossible, which I don't expect,
but it's possible, you know, maybe, then we need to do some very drastic things. If we want to
survive into a long-term future, if it's super easy and not that big of a problem, okay, well,
then we should verify that. And if we verify that, then we can go easy. Awesome. Like,
that would be the best world, but we don't know. And most of the evidence that we have
points towards it being pretty damn hard. So it's given that it's hard. We don't have much time.
There's both a strong technical problem, which very few people are working on. It's like, it's
shocking to me how few people seem to want to work on this problem or at least have the incentives
to work on this problem, including academics. Like, it seems to me as if I was an academic,
this would be the number one problem I want to work on. But the number of academics working on
the problem of super intelligent alignment is minuscule. And last I checked, it was less than
200 in the whole world. And which is crazy for a problem that's this hard. I don't think, like,
the fact that it's just like, some guy in London, some hacker, you know, who put together a small
company and is like, among the largest labs in the world working on this problem, is not a good
place to be. Now, I love my engineers, my scientists. I think there's some of the most
brilliant people in the world. And they give me hope every day that maybe we can make progress on
this problem. But we're in a bad spot if this is the best way, if this is what we have to rely on.
It's similar where how I often talk to journalists, and they ask me questions like,
is Sam Altman nice? Like, does he care? Is Demis a good guy or a bad guy? And my answer to this
question is, it shouldn't matter. It shouldn't, we shouldn't have a system where whether the future
of AI goes well, depends on whether some CEO is a nice guy or not. This is not a stable system.
This is not a good system that we're in. So I'm very pessimistic, in the sense of I don't think
we have a high chance of success. That's not impossible. I do see ways. I've seen, I've ironically
had an experience over the last couple of years where timelines have gotten worse, things have
progressed faster than I expected, and I already expected very fast progression. Many political
and financial things have gone much worse than I was expecting. But at the same time, I've seen
more solutions. I'm saying more plans that could actually work. So it's been a kind of a
hug in both directions for me as like, up or down in probability of success. But I think this is
very, very, very hard. And if nothing changes, if we just continue at this pace,
the way things are today, you're in 2024, then we're definitely not going to make it.
Well, you raise a really good point there that I often tell people, I'm lucky enough to
interact with some really bright people around the world. And I often repeat, I don't understand
why more people aren't concerned and working on this. So it's kind of insane the fact that,
you know, if our families, for example, our children were in a position of danger,
they're on the edge of a cliff or something, they're real danger. It's like completely ignoring it,
just letting them play and frolic at the edge of a cliff. It's difficult to understand.
And we've been lucky enough to get some really bright people on board for our research program.
And I suspect that there's a couple of scenarios that could play out here. Number one,
our concerns about large scale
astrophy or even extinction might be, might turn out to be false. It might turn out to be real.
It might turn out to be so real that humanity ends. It seems to me that a more likely probabilistic
scenario is that we're too late to really change anything. What you're trying to work on, what I'm
trying to work on, what several people around us are trying to work on is daunting. It's incredibly
daunting. How can we possibly get people to understand and to change their behavior? I see
it really difficult. So I think it's too late to really do anything. I think we're going to learn
a very difficult lesson about I'm crossing my fingers and hoping that not everybody dies.
So that there's some people left over that can look back historically and say, this is where we
come from. This is what happened to our previous systems. Now we have to create something different.
Unfortunately, in that scenario, even in that scenario, unless something different is created,
that next generation is going to confront sooner or later the same problem.
So that's I think the essence of what we're dealing with here. The problem of how do we
stop ourselves from destroying ourselves and going back to the principle of least action,
or what Maldesena calls the principle of maximal life. I suspect, and we've been working on this
concretely with, like I said, some top researchers, some of them you know,
that one promising potential comes through a combination of physics and social choice theory.
I suspect you're familiar with Glenn Weill's proposal for quadratic voting. So
Glenn Weill is one of our research partners among this research team that we're talking about.
And so what we're trying to do is connect his proposal for quadratic voting to
down to the fundamental physical level, and then proposing a new way to make collective decisions.
I don't think it's going to work for this generation, but I think it may be interesting
for the next generation or after the lesson that humanity learns. And it's the idea is,
how do we control AI? Again, this is theorizing, right? If all material has an
a propensity to seek out its own existence for as long as possible. The answer is to build into
those physical systems, into our political systems, the opposite. Some kind of altruistic
algorithm or some kind of self sacrificial algorithm that controls the impulse to
only I survive. Rather, if we get our political systems, our social systems
driven by the idea that I'm not the thing that matters, but rather, or I'm not the only thing
that matters, but rather, we matter. If that's the fundamental principle without going all the way
obviously to something like communism or anything like that, if that's a driving principle, and we
can get it into the physics and into the artificial intelligence, that seems to me like a promising
avenue to explore. Does that make sense to you? I'll leave it there. Does that make sense to you?
I think there's a lot of things that are worth exploring, for sure. I think there's a lot of
inspiration to be had in mechanism design, familiar with Wiles' work and Levin's work on
like tumors and like how the systems coordinate at this level and so on. It is the general
question of coordination. In a sense, you could say that alignment is very much a hierarchical
problem. It goes down from how to cells prevent cancer, how to multicellular organisms coordinate
with each other all the way up to how to friends coordinate, how do companies coordinate, how do
governments coordinate, how do governments coordinate between each other and so on.
So it's very much a general abstraction that we see appearing over and over at different scales.
Arguably breaks down at the Swalth physics level unless you want to
recast, you know, elementary particles as like coordinating mechanisms, which you can, you know,
you can use energy as a currency and like, you know, clear the market and so on. Like it's as
possible to cast it this way. And maybe it's a productive way to cast it that way. I'm ambivalent
on why this is productive direction. I guess the way I, my usual quip, I guess I would say on this,
is that to do truly incredible things, like things of this magnitude of just like incredible
proportions, you do exactly two things. The first is you do things that compound. The second thing is
don't die. If you do those two things, you can accomplish things that seem impossible. So I think
the kind of work you talk about is a thing that can compound, improve a mechanism design at scale
is something that can compound. If you can do this over a large scale of people that, you know,
potentially growing population or growing in terms of epistemology, wealth, etc. This is a thing
that compound. Getting you 0.1% more growth per year is very powerful. If you can do it over 100
years or whatever, then the bottleneck becomes don't die. So if I had 100 years, 1000 years,
if I thought there would be a next chance, if I thought some people would survive and they'll get
a second shot, I probably wouldn't be working when I'm working right now. Maybe it would. But
I think there are ways of how you can solve alignment entirely for superintelligence on the
first shot. But it's unfathomably hard. And it starts with, you know, dealing with incredibly
fundamental questions of epistemology and, you know, foundations of mathematics and whatnot.
This is not the kind of thing you do if you think you have a limited amount of time and a
limited amount of coordination budget around what to coordinate your civilization. So I feel this
like, this feels like somewhere in between, where like, I think the bottleneck right now
on us surviving is not really galaxy-boring, brilliant scientific stuff. It's mostly
boring labor. It's talking to politicians. It's producing good media, talking to the general
public, digesting things into words the general public can understand, message testing,
and just hacking away at the, you know, control problem in a more prosaic way. Like, how can we
build AI systems that are not super intelligent, but are still useful. So we still get economic
growth because this is easier to coordinate on. This is part of what we do at Conjecture,
is we try to build AI systems which are bounded in the sense that they're useful,
they're powerful, and they're reliable, and you can use them reliably and you can ensure
that they don't lead to existential risk or such problems. This is not because this is the best
possible thing we could be doing. It's because it's a good thing to coordinate around. It's much
easier to get a government or a company or someone to sign up if you say, hey, this prevents this,
you know, tail risk and also makes you a lot of money. This is a very easy, useful way to get
people to work with you is if you just promise them to also make a lot of money. And if this is,
of course, true. So a lot of the academic things, I like that someone is working on them. I like
that there are some people in the world out there who are thinking about, you know, physics-inspired
mechanism design that are thinking about formally provable retrocausal alignment schemes, you know,
shout out to my friend Karado on that one, and other things. Say more about your friend.
She's a researcher working at a work called Orthogonal, and they have one of the most
galaxy brain fucking insane idea of how to align AI, where they like build an AI that like goes,
destroys the universe, but then re-simulates it and goes backwards in time to think about
what the researcher actually wanted and then locates that formally provable. It's insane.
Obviously, it won't work, obviously, but it's great that someone's thinking about this, right?
Like, I'm glad someone is thinking about this. I don't think, I think honestly, you know, with
all my love to Karado and other people in this category, I think too many people are thinking
about this stuff right now. I think these people would do much better if they just literally
start doing politics. It just boots on the ground. I'm not even talking about galaxy brain
planning, just boots on the ground politics. I think right now it's higher leverage,
but this is a contingent fact. At some point, we need to think about the complicated big stuff
as well. It's one of those great things. We're like, I'm a big fan of quadratic voting, prediction
markets, all those good classics that you're like GMU associated people love and whatnot.
Love them, but prediction markets are just illegal in the US. Sure, you come up with a
better proper scoring rule that incentivizes the correct crypto-decentralized actor thing,
but they're still illegal in the US and they're illegal for a really stupid reason.
Like the reason they're like, if you don't know, you look it up, like the reason they're
illegal in the US is just stupid. It's not even someone passed the law to outlaw them.
It's just one branch of the government decided it fell under the jurisdiction,
they shouldn't do it and no one ever basically challenged it. It's stupid. And like the bottleneck
to doing that is not you have to come up with an even clever market design. It's you need to actually
lobby the government and you need to actually talk to people and explain things to them
and make things legible. This is the thing that will some prediction markets recently have done
a much better job on. Like recently there's been, you know, prediction markets suing the government,
going through the process and so on. And this is all horrible and terrible and annoying and it
feels like a waste of your time and it is, but it has to be done. So my claim is that I think we are
currently not bottlenecked by the smart stuff. I think we're bottlenecked by the stupid stuff.
Okay. So, so I take point taken, I think your, your, your approaches is interesting and important.
And I agree that many different approaches should be taken. And, you know, the boots on the ground,
lobbying politicians and whatnot seems like the most directly doable, rational approach.
But what do you do about the fact that people don't agree, even, even on approaches to AI?
If you get, if you have lineup, let's imagine lineup, I don't know, a few people who are doing AI
research and you ask them, should we, should we continue with this or should we stop?
And six of them vote to continue it, four of them vote to stop. If it's a, if it's a,
if it's a democracy, then you go ahead and you go ahead with AI and with presumably with AGI.
Isn't there a problem with, even if you go to the politicians, if you still, if you just submit
this to decision theories or social choice theories, that there's still a problem?
So this is what I was talking about earlier about step zero. You have to want to not die.
If people want to die, yeah, we're screwed. And I'll go retire to Hawaii and, you know,
with my family and have a good time. I probably won't, I'll probably still fight. But, you know,
if, look, if we did a truly representative global poll vote on literally the whole world about
whether unelected, you know, technocrats in Silicon Valley should be allowed to build
technology they themselves think and have said could literally kill everybody. And if more than
50% vote yes, okay. You know what? This is already a much better world than we're in right now.
Because right now no one's even being asked. There is no common knowledge. There is no discussion.
It's just happening anyways. So if we get into this world and after the result, I'm already happy.
This is already so much better than where we're at right now. This doesn't know if this happened,
of course, I would immediately start working on, you know, hopefully changing people's minds
and so on. But still, this would be a massively better world than we're at right now. And
also, I just don't think it would be over 50%. I think, and there are polls, you know,
we've done you go polls in like the UK and the US around this, and no, the overwhelming majority
bipartisan say no, this should not be just allowed without supervision. And, you know,
they shouldn't just be, we shouldn't race forward at all. This is bad, actually. And so if we,
if we had a democratic oversight here, a coherent democratic process with teeth,
I think we would be in a much better spot than we are right now. This doesn't solve
all the problems. But as I say, we need to do two things, things that compound and not die.
So this would give us a lot of not to die points. And if we had more not to die points,
we can spend these on clever things. So you have this question of like, okay, but people don't
agree on values and things and whatever. And this is a hard problem. And I'm like, yeah, okay,
it seems like we need a lot of time to figure that out. I don't think we're going to find this,
we're going to figure this out in the next months. So if we can't figure it out in the next six
months, we have to make sure we don't die in those six months. Because if we do, then we're
never going to figure it out. So is it possible to develop a better mechanisms or a better
aggregation or enlightenment at scale or whatever the hell to make people agree more on things or
like find better compromises? Probably. And I expect those things will take longer than,
you know, six months. And so we have to make sure that we don't die in the next, you know,
six months, 12 months, you know, three years, whatever, we need more time. I think we can
make problems on this. If I had 30 years, I'd probably be working more on mechanism design
or global preference aggregation and stuff. I just don't think we have that long,
basically. So we have to first work on the not to die part. And if we get the not die part,
then we can, you know, start thinking about not just start, but then we can like put more of
our efforts. I think currently, the number of people who are working on the boots on the ground,
make sure we don't die over the next couple of years is vanishingly low. The amount of lobbying
dollars that goes into lobbying against the creation of superintelligence by unaccountable
and elected, you know, technocrats is a pittance. It's almost nothing compared to these large behemoths.
You know how much Microsoft pays for lobbying? Like how do you think like groups like, you know,
me are supposed to compete here? I mean, we can and we do. And the main thing we compete on is that
we're right. And people don't want them to do this. It's very expensive to convince people of
things they don't want. It's quite cheap to convince people of things that they do want.
And so we have an up, we have a chance there, but like it is hard. And it's not,
it's not easy. And we would benefit. I wish there was, I wish there was a hundred organizations
of control AI. I wish there were a hundred conjectures of small, medium, you know, even large
players pushing against this kind of stuff, even just by making noise, even if it's just by, you
know, talking about it a bunch in social media, if it's by writing letters to your call, your
congressman, like whatever, like there's just not that much. And I'm not saying there's a massive
amount in the mechanism design either, also underfunded, terribly underfunded. There's a lot
of work to be done there. But in my personal view, at this point in time, I think we would benefit
more on the long term, if we could all coordinate to temporarily just solve the concrete problem,
the bias more time, and then we can redistribute on longer timelines, in my opinion.
Yeah, very good. The one contradiction I receive or the tension, maybe I, as a better word,
is the idea that you're very clear about what you're trying to achieve. Yet at the same time,
you say that it's not possible, that you're pessimistic about it. So how do we understand
that different, those different ideas? Not possible, and almost not possible, or very different.
I forget how the quote from Princess Bride goes, but, you know, he's not dead, he's almost dead,
which is very different. Humanity is a dead, it's almost dead. That's very different.
The way I kind of see things personally, so I'm just speaking for myself here,
is that even if I thought humanity was dead, even if I thought there was no chance,
I still would try. You know, like, I don't know how to save the world, but damn it,
I'm gonna try. It's kind of just my nature. Like, I've always just kind of thought this way.
The way I kind of think about this is, and this is a question I get a lot, is like, Connor, aren't
you depressed? Like, isn't this scary? Aren't you sad? Like, you think we're all gonna die?
Doesn't this make you upset? Doesn't make you sad? The truth is, like, not anymore. Like,
I've kind of grieved. I'm kind of over it. And the way I see things now is that, look,
we're facing unbelievable hardships, unbelievably hard, massive problems. There's
huge monsters, behemoths, villains that are standing in our way, often to their own detriment.
There is incredibly difficult technical, social, political problems with overwhelming odds against
you. And that's true. But, you know, I get to work on the most exciting problems in the world.
I get to try to make the world a better place with all my might. I get to work with some of the most
brilliant, wonderful people I could ever ask for. You know, I get to spend time with my family, eat
good food. Even if we don't make it, even if it goes poorly, even if we don't make it out, then
it's a life worth living. And I have no regrets. That's how I live my life. I'm happy I tried.
You know, if we don't make it and we die, I want to be able to look God in the eye and say, I try.
Well, I applaud your effort. Definitely, I applaud it. In fact, I wish I would have come
across your work a few months earlier, prior to writing this grant application.
Have you worked on any kind of simulation using LLMs, just out of curiosity, because the application
that we submitted involves using LLMs to simulate different scenarios, precisely in order to test
various voting algorithms and different social situations to convince people that something
needs to be done? Have you experimented with any kind of sort of simulations? And if so,
please tell us about it. A long time ago, I experimented with this. This is back in the GPT
two days, GPT three days. I experimented with these ideas. These were some of the first ideas that
came to my mind as well. I was wondering how much you could use this, for example, and create
artificial opinion or cultural polls or to stress test or role play through negotiation scenarios
or like message test for cheap. Back then, it didn't work very well. Models are not good enough.
I think modern models are locked better at this. I think they will have predictable
biases and failure modes here. Obviously, it does not replace human interaction, but it's
obviously a powerful tool. And compared to how cheap they are, compared to the data you can get,
they're obviously extremely useful. I think there's a lot of low hanging fruit here where you can do a
lot of like digital social science. As long as you remember, it's not actually social science,
but it is proxy. It's a proxy metric. It's a noisy proxy, but it is a proxy. I think there's a lot of
massive amount of alpha that can be gained there, where just if you iterate super, super quickly,
like I think the bottleneck there is good software engineering and like a good iteration,
just like doing lots of experiments very quickly. Because that's the thing you like
can't do with humans, right? Like you run a poll, you can have like 10 questions of like,
you know, 2000 people and it costs you like a lot of money to do this.
A lot, yeah, exactly. Exactly. So if you could do the same thing,
but your questions are like more noisy or less good, but you can do it on two million samples,
like that should change how you think about doing science here and then like what you verify.
Like the right way, I think, to design this kind of thing is like you do
massive amounts of digital stuff to hone on the right questions to be asking and then you
point to verify these things in the real world or something like this. I'm not a social scientist,
I'm not a statistician, so I can't say I know the exact way to design these kind of things.
But I do think there's a lot of possibilities there that I'm sure some people since then have
tried and I have not read your papers. Sorry, there's too many papers to read.
So I'm sure there's other people who have tried this kind of stuff in a more sophisticated way
than I have. I of course, as I say, personally, do not feel bottlenecked by this kind of work,
but if I had access to extremely good tools around this and it was low cost to like message
test at scale with LLMs, yeah, I think that could be helpful. Not fantastic, that's good to know,
that's very promising. Yeah, we as a group obviously feel very optimistic about the
possibilities here. In fact, if you're interested, we'd love to include you in the efforts. We've
just started another group of sorts that includes Wolfram Research,
basically aiming toward the same thing of using LLMs to simulate
agent based models, etc., using some really interesting stuff from Michael Levin's work on,
you know, hierarchical levels of intelligence, what he calls it, he calls it the multi-scale
competency architecture. And then we have, I mentioned earlier, physics approach, we have,
I don't get tired of promoting this approach to physics, which is the two state vector
formalism, which pauses a two boundary conditions to understand the nature of some of the mysteries
that we experience on an experimental level in physics. And this goal oriented approach
from the quantum level through, you know, Levin's work, through Friston's work, all the way up to
democracy, we never know if we have three years, maybe our paths have some promising potential,
because if we can simulate things and convince governments through actual simulations and through
very rigorous science and rigorous physical and mathematical arguments, I think it could
add to the boots on the ground. I think the boots on the ground is obviously really important as well.
So like I said, I wish I would have come across your work a little bit sooner,
precisely because of that, you know, the LLM approach to agent based modeling.
Yeah, I think it's definitely interesting. And I don't want to dismiss like that, you know,
very good simulation, technical arguments, and so on, can I value, but at least, you know, also to
share with the audience, because I think this is interesting, potentially for a lot of the audience,
I've talked to many, many politicians now, like I'm a tech guy at heart, right? Like everything
you just say, I'm like, ooh, that's, that sounds so fun. Like I want to work on that. That sounds so
cool. You know, it's like exciting. It's fun. You know, you have, you have Levin, you have
Friston, you have Wolfram, you got the whole crew, like, you know, you got the, you got some of the
funnest people in science, you know, together. Absolutely. Yeah, absolutely. Yeah. So I totally
get it, right? Totally. I love the stuff. I'm, you know, I think the stuff is super cool. And
I was almost disappointed, but it's good now that I'm calibrated on this. It's just like,
man, it is good logical argumentation and scientific data, just not the bottleneck on
convincing people. Like, this is just truly, truly not the bottleneck. I wish it was the
bottleneck. I wish we were at a point where everyone was so rational, every, our institutions
were so efficient, our group epistemology was so good, that the thing that was holding back,
say climate change discourse is more, you know, ice core readings. If that was the thing holding
us back, I'd be like, wow, we're in a good spot. But the truth is the things holding us back are
having nice dinners with politicians and making them feel safe and making them feel like you're
trying to help them and explain anything to them in their language. It's so often that I've
spoke into politicians and these are high ranking politicians, mind you. These are like top UK
people and they're, these are not stupid people. And they're just like, and I, you know, spend an
hour or two with them. And they're just like, wow, no one has explained this to me before.
And I'm like, damn, like if this hasn't been explained to this guy, who's like a very high
ranking member of government in a very major country, then no one has, like no one knows this,
like no one has been told this. And these are not high level. This is not complicated explanations,
right? These are very simple. Just here's the basic facts about AI. Here's how it works. Like,
most people don't know that like AI is encoded. Like most people don't know that AI is trained
and that we don't know how the eternal is. Almost everyone I talk to does not know this.
This is a very, very basic fact. And just telling this to literally every politician
is already extremely high value. And bottleneck to getting to these politicians
and other leaders of various kinds is not that our argument isn't good enough.
They'll believe you. You can give the argument like, huh, yeah, that makes sense. And they might
double check you with their science advisor or whatever, but the science advisor will agree with
you. He'll be like, yeah, we don't know how they're written. It's that this takes, is that, you know,
here in the UK, we have what like 300 to 500, you know, major politicians that you need to talk to.
This is before we talk about like cabinets, ministers, you know, deep state stuff,
US government is much bigger than that still, you know, there is like, you know,
there is 190 countries in the world. You multiply all these numbers together and you get a lot of
people you need to talk to. And even if every conversation is not difficult and could be done
by, you know, you meet anyone really, each of them takes an hour or two hours and it takes you like,
you know, takes you a while to get an introduction and to convince them that it's worth talking to
you and whatever. That it's up to just a lot of man hours. It's a lot of hours you have to spend.
And it's not glamorous, cool work. It's just things that need to be done. I would like us to
get to the point where we've done all the stupid things where we've like, I'm kind of repeating
myself at this point, I'm sorry. It's like, I want to get to the point where we've done all the
stupid things. We've talked to literally everyone, we've made all the dumb arguments, we've got all
of this, you know, and now we're bottlenecked because our science isn't good enough, or we don't
have any convincing enough, you know, we get to that point. I'm like, awesome. That's a really
good world to be in. And so there's a little bit of this, we're like, for convincing technical
people, this kind of stuff is very valuable. So the stuff you're talking about for like professors,
intellectuals, this kind of stuff, for that it is very useful. But I found it
shockingly unhelpful anywhere else.
No, that makes sense. That makes sense. In fact, our strategy is precisely trying to address this
difficulty. And we think that quadratic voting amongst other approaches could be a promising
way forward. For example, if you're assuming, for example, that the voting mechanism is democracy,
one person, one vote. But if you change that assumption, then voting theory becomes a lot
more interesting. Because if we allow the Conor Leies of the world to have more say in whether
or not we proceed with artificial general intelligence, then the better decisions are made.
So that's precisely what we're trying to get. But I agree with your point. I'm sorry, go ahead.
Yeah, I was gonna say, I love this. I think quadratic voting is a great idea,
as is like writing preference voting, like all these superior systems. I think all these things
are really good. They're really good. And I think if we can get this implemented, it'll be fantastic.
I expect the thing that is preventing these things from being implemented is just mostly social.
It's gaining reputation, talking to people. So someone has to do that. And I would advocate to
our lovely audience of talented, smart, handsome individuals to potentially considering that this
work is extremely important and could use your talents. I agree. I agree. Absolutely. I double
that notion. Let's spend the last couple of minutes talking about conjecture. What are you
doing at conjecture? What's taking your time? Tell us about your team? Tell us anything that
you think would be valuable for the audience to know and for myself to know.
Yeah. So at conjecture, we kind of grew out of my previous projects, which was I was the founder
of the Luther AI, which is a large open source ML research community. I don't do open source anymore
because unfortunately, I feel timelines were much shorter than I expected. And open source
seeing AI models can become very dangerous very quickly. So I don't really do that anymore.
And so, but I met a lot of great people, some of the best engineers, you know,
you could imagine. And we all crowded into a, you know, oxygen deprived little we work in London
about two years ago now. And so since then, we've been working on the problem of how do you make AI
systems controllable? This is a technical problem. And so we have been thrashing our heads at this
problem for, you know, two years straight now, we've tried many, many different things. We've
gone from many different directions. We've taken this, missed many things. We've built, you know,
state of the art, large language models in the house like way before like llama existed. We
already had like, like a lot of multiple models internally and so on. We've done a lot of
interpretability research, we've done stuff like simulators, they were true trimmings.
And for a lot of things, iterating and finding things, we now have something that we're really
actually optimistic about for the first time. And it's, it's kind of a weird work. Like, I feel,
I feel like I still haven't emotionally even come to terms with like, oh wait, this might actually
work. And yeah, for the first time in my life, I'm, I'm like, I still myself am like,
like scared to be too optimistic about it. But like, I'm like, it's actually looking like it's
going to work. And so the approach to what we call cognitive emulation or co-in. And the idea is,
is that it's a vision for how to build AI systems, still using LLMs. But instead of the usual LLM
way of just like training in a bunch of shit, and then, you know, running it on whatever,
we think far more principled about the problem, we think far more principled about cognition,
and we break down cognition into more elementary blocks, into more elementary pieces and steps
that a human would use when solving a given problem. It's like, when you approach a problem,
what are the steps that you as a human actually use? And what we do is, is we build models and
we train models, we've developed some really cool algorithms in the house. And I unfortunately can't
talk about, we're quite, you know, mom, a little bit strict about how we accomplish some of the
things that we accomplish. We developed some methods internally to be able to train on very,
very small amounts of data or very, very high efficiency, and very, very high reliability.
So the idea here is, is to build systems that can do fundamental pieces with perfect accuracy
and perfect reliability. And then from this, you can then bootstrap and compose into
larger and larger and more complex things, while still understanding every step of the way.
So the idea is, is that you build systems. I say systems not models, because, you know, of course,
there are LLMs at the heart, but it's more than that. It's also about having, you know,
you modify the sampling algorithms, you modify the, you know, there's scaffolding, there's
memory, there's a bunch of stuff. It's a stack of techniques that ultimately gets you to systems
that are extremely reliable. It might be less good, a less good poet than GPT4,
but if you tell it to do 10 things in order, it does those 10 things in order every single time.
And this is what we're pushing for. This is reliability, this actually understanding of like,
what is it doing step by step, and how can you compose these things? And this reliability is
what unlocks this, is that the problem with current large language models is that they
are not reliable. And if you tell it to, if you tell it, even GPT4, which is a fantastically
smart model, to do like five non-trivial things, it will like 50% of the time get distracted by
number three and like veer off or whatever. And it's just not predictable in what ways it will
veer off. You know, if you could predict ahead of time, it will veer off if I give it this prompt.
That's also fine, but it's unpredictable. And so with target population systems,
it becomes much more predictable. You get to actually, the system itself can detect
when it won't know how to do things or when things will not go as planned. And this is what
we've been working on now for quite a bit. And all of it's now starting to work where we can get
systems that can do, you know, fundamental, you know, basic tasks with perfect reliability.
And so, you know, truly like, you know, up to measurement error,
perfect. And you can then compose these things into larger and more, more complex tasks.
So we're already starting to work with, you know, first, some first enterprise customers
and design partners here, because it's kind of like one of the nice. So one of the reasons
the co-ed was nice is because the good coordination mechanism is a good coordination
shelling point because it is, I like to say it's the kind of AI that companies want to buy,
you know, it's both addresses the control and the safety problem. Well, the other hand,
also, this is what most companies want. Most companies want an AI thing where you tell it
to do something and it does that thing and nothing more and boring and it does exactly
what you tell it to do. They don't want a quirky brain in a jar with a personality disorder,
which, you know, we're seeing the research of Sydney on Twitter right now.
So this is what we built. And we're now open to some of our very first enterprise, you know,
design customers. If, you know, anyone in the audience has a, you know, large, you know, company
who has very high safety requirements, very high requirements for reliability, where maybe LNs just
don't cut it and they want to try something new and something hopefully revolutionary.
My emails are open. Fantastic. Well, commercially, obviously, that has a lot of value, a lot of
value. So I'm optimistic about you having a lot of success early on. In terms of
control and AGI, however, if you predict, obviously predictability is better than
not having predictability, but how do you, you still don't address the problem? I think,
unless I'm wrong, you still don't address the problem if somebody wants to or sets out to,
I don't know, start World War III. Yes, this is a fantastic point and it is exactly correct.
So that's why I say we work on controllability, not in alignment. So if you build a co-em system,
so co-em as a technology scales to AGI up. If we do this correctly,
it is as smart as a human. It can do anything a human can do. And if you have a system that
can do everything a human can do, including science, you have something incredibly powerful,
something incredibly dangerous. Co-em moves us from the regime of we are definitely fucked
to it is possible to not be fucked. If this technology is misused by say, you know,
terrorists, hostile governments, something, yeah, then we're screwed. And so this is why I say this
is a compromised technology, is an incredibly powerful technology that would allow you to have,
you know, digital, reliable human reasoning that you can scale. Imagine you could have
scientists that follow proper scientific method and epistemology about any topic,
any time of day, without ever getting distracted in a way that is checkable, auditable,
understandable, and you can rely on the results. If this works, if you do this,
this is extremely powerful. It will allow you to develop extremely powerful technology. It will
allow you to do incredible economic activity and so on. But it also means, of course, that you could
use it for very, if various purposes. We get into a world where this technology has reached that level
and the world hasn't already ended for other reasons, we're in an unstable equilibrium.
And this applies to any AGI complete technology. I think Coim is a compromise. In an ideal world,
I wouldn't work on technology at all that is AGI complete, and we would work on, you know,
pure alignment tech or pure coordination tech. We don't have the time for that. And it's hard
to coordinate around that. No one's going to pay you for that. The unfortunate truth is that doing
research is extremely expensive. Doing lobbying is extremely expensive. Someone has to pay for
that. The money has to come from somewhere. So it's a compromise. Coim is the worst option that
will work. There are much better options, but this is the one that will work. This is the one
least bad that will work, I guess, is that it is much better than the current other approaches.
The other approaches are just scaling up walls or whatever, just kill you once it's smart enough.
It's just, and that's just it. This allows you to not die, but you also have to solve hard
coordination problems of not letting this technology be abused, not letting it proliferate.
If you make this open source, you're doomed. Fantastic. Okay, how can people reach you or
follow you? I can follow me on Twitter at NPCollapse. You can also find me on various random podcasts,
also my company website conjecture.dev or nonprofit or controlai.com.
Fantastic. Connor, it's been an absolute honor to talk to you. You are
brilliant mind doing some really important work, and my heartfelt thanks for all of your effort
and what you do. Thanks so much. Thank you so much for having me.

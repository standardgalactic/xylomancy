Welcome everyone to the Cartesian Cafe. Today we're very lucky to have Dan Schroeder here with us.
Dan is a particle and accelerator physicist and an editor for the American Journal of Physics.
Dan received his PhD from Stanford University, where he spent most of his time at the Stanford
Linear Accelerator, and he is currently a professor in the Department of Physics and
Astronomy at Weber State University. Dan is also the author of two revered physics textbooks,
the first with Michael Peskin, called An Introduction to Quantum Field Theory,
or simply Peskin and Schroeder for short within the physics community, and the second,
An Introduction to Thermophysics. Dan enjoys teaching physics courses at all levels,
from elementary astronomy through quantum mechanics. Welcome Dan, how are you doing today?
Thank you so much, Jim. I'm doing great, and thanks for having me.
Yeah, no problem. You know, it's quite a rare honor to have a student connect with one of his
teachers at some point in his education. In this case, I was your student in some sense,
because I own both of your textbooks, so I'll just flash them right here for the audience to see.
So I have your Quantum Field Theory textbook, which I believe I got at MIT when I was a graduate
student. I took Quantum Field Theory under Alan Gooth there. And before that, I obtained your
Thermophysics textbook as a sophomore at Caltech when I took thermodynamics. So
it's great to be able to connect with you. It's funny because I didn't realize
it was the same Schroeder until I actually reached out to you. I didn't make that connection.
So yeah, it's great to be able to, many years later, to look you up and now discuss with you
these topics that I learned long ago and hope to continue learning more as we discuss
these matters. I thought maybe we'd start out by having you tell us more about yourself,
your story and how you became a physicist, and maybe a little bit about how you came to write
your books. Oh, well, the story of the Quantum Field Theory book is kind of easy to tell.
I need to explain that I was very much the junior author of that collaboration.
But when I was a graduate student at Stanford, I first took Quantum Field Theory from a
wonderful teacher named Savasthamopolis. Maybe some people have heard of him.
And he went nice and slow, and I was able to follow most of it. And then the next year,
Michael Peskin was teaching the course, and I knew he would go a lot faster and cover a lot
more. So I audited the class, and every once in a while I would get a little annoyed with him
because he'd be talking so fast he'd forget to say something, you know, the step between he would
go straight from A to C without saying B first. And I raised my hand in the back of the room and
say, excuse me, but we need to assume this or something like that. And I think he really appreciated
those interruptions. And one day we were riding the shuttle van up to the accelerator laboratory
together, and he asked me if I'd be interested in helping him turn his lecture notes into a
textbook. And we were supposed to finish it in a couple of years, and it ended up taking
eight. And by that time, I had gone through two temporary jobs and finally landed in a
tenure-track job here at Weaver State. It was a very rewarding process and quite an honor to
work with Michael on that. I haven't had much opportunity to think about Quantum Field Theory
since 1995 when that book was published here. I teach only undergraduates. So that's that story.
And then the thermal physics was a topic that I was awful at in graduate school. I actually took
the statistical mechanics course from Michael, and I was lost through a lot of it. But fortunately,
I didn't need it at all for my PhD thesis. So I got through Stanford without knowing much thermal
physics, and then a year of teaching at Pomona College. And then when I got to Grinnell College
in a leave replacement position there, they needed someone to teach their senior-level
thermal physics courses taught at the senior level there, probably about the same as the
sophomore level at Caltech. And I thought, well, I got to learn it sometimes. So I
voluntarily agreed to teach that class. And fortunately, I had only three students, and
they were all very good students. And we sat around a seminar table and kind of taught each other
the subject. We worked through together the textbook by Catalan Cromer, which is a brilliant
textbook full of insight. And Cromer has since gotten the Nobel Prize and so on. There's a lot of
wisdom in that book. But it's not an easy book for students. And so I kind of learned it as I was
teaching it there. And then the next year, I got to teach the class again to a larger group.
And by that time, I decided I wanted to cover things in a different order. And I kind of
reordered things and where they made sense to me. And then I came here and so we were
and taught it two more times out of a different textbook by Keith Stowe. And there were some
issues with that book for the students as well. And so my third time through here, which is my
fifth time through all together, I started writing up my lecture notes, checking my lecture notes,
and handing them out serially. And by the end of that term, I pretty much had a draft of a
textbook and kind of stumbled into it. And that's what happens, I think, with a lot of physics
textbooks, is people teach and get ideas for how to teach a subject and start writing it down and
then take it to a publisher. And since I already had one successful book, I was very, very fortunate
that publishers took a serious look at this draft. And Addison Wesley published it initially. And
it's now with Oxford University Press. And it's been successful beyond my wildest dreams.
I thought if 10% of the classes in the US use my book, I'll be very, very
fortunate to reach that many and expensive substantially above that.
I guess I stumbled into the right project at the right time and I was kind of an opportunist
about it. But I'm an amateur when it comes to thermal physics. I've done no research in the
subject. And I just kind of wrote it down, wrote down what makes sense to me.
Okay, well, it's a very charming book, and we'll get to it soon enough. I just maybe,
maybe one more question I wanted to ask you. I used to be an academic and I was very much
gunning for the research track and in academia, there are essentially two tracks to basically
to do research and to do teaching. Most people do both, but it's sort of a matter of emphasis.
It appears that you went for the teaching track or can you explain your choice or how that ended
up happening? Sure. I attended a very good liberal arts college, Carlton College in Minnesota,
where teaching was the main emphasis by far. Not much research was going on there, at least in
physics when I was a student and it's probably changed a little by now, but teaching is still
the top priority of places like that. And so, and I had a wonderful time there. I had a very
good experience. I had wonderful professors and wonderful classmates and it was just a
very positive experience for me. And so, I came out of there into graduate school thinking that
that might be the career path for me. And then when I got to Stanford and saw what things are like
at a big university where, frankly, the people were less friendly and the place was much more
competitive. And I was not the strongest student in my class. And, you know, I kind of that reinforced
what I already thought, which was that probably a place like this is not where I want to spend the
rest of my life. And that wasn't that wasn't what I wanted. And so I pretty much was determined
even more strongly after a couple of years there that, you know, I would learn what it was like to
do some research. I did some research on accelerator physics, applying quantum electrodynamics to
future linear colliders, which are still in the future, 30 years later, 33 years later.
And that was a wonderful experience. And it was quite an honor to hang out at the Stanford
Linear Accelerator Center, which is now called the Slack Linear Accelerator Laboratory or something.
It was a wonderful experience to hang out there for four years and go to the
talks in the colloquia and absorb what was going on and kind of learn about the research process
that way, not partly as a participant, but mostly just by being immersed in the laboratory and
what was going on there and talking to my classmates and other researchers. And I
think that gave me a lot of perspective on physics. But then I started looking for teaching
positions. And I talked for a year at Pomona College and other liberal arts college and then
two years at Grinnell College. And then I came here to a department at a public
open enrollment regional university, which is a very different type of place from those liberal
arts colleges. But my life as a faculty member is actually very similar. We have no graduate programs
in the sciences at Weber State. And so I do essentially all teaching and try to find
many research projects for students to get involved with, but publishing is not a consideration
with those. Okay, great. Well, I'm glad that things worked out for you. Well, I guess I want
to acknowledge that I appreciate being at a place where you can write a couple of textbooks
and publish a few articles in a journal aimed at other college teachers and still get tenure and
have a rewarding career. So this is a good place for me. And anyone else who's interested in such a
career, look around. There's lots of places where I think teaching is valued. And if you put a good
effort into it, you can have a good career. That's great. And I'm really excited by the fact that now
with our conversation, we'll be able to showcase to the world you as a teacher to an audience
that maybe you wouldn't normally come across. Well, anyways, why don't we get started. And I
wanted to illustrate to our audience some of the charm that I think is unique to your book. So I
pasted two snippets from your book one, which is this cartoon of a magician
bringing a rabbit into existence, which is your way of illustrating the concept of enthalpy.
And then this passage where you talk about small numbers, large numbers, and very large
numbers in a sort of conical, definitional way. And the funny thing is, so I'm my PhDs in mathematics,
but I took a lot of physics courses. And there's always kind of a funny
tension between the two when it comes to rigor. And I think in this case, your book is
not rigorous, certainly because it's, well, it's a physics book. But somehow in physics,
the lack of rigor is, in many cases, a feature, not a bug. And in this case, there's this level
of charm to it. And I thought this passage where, what, 10 to the 23 plus 23 is 10 to the
23, because, right, you could just sort of absorb, you could just kind of throw away small numbers.
So, you know, yeah, physics, I don't want to get into the math way of doing things and the
physicist's way of doing things. I started out as an undergraduate thinking I would probably
major in mathematics, and then absolutely fell in love with physics during my freshman year in
college. And it was things like relativity and quantum mechanics that really hooked me then.
I didn't fall in love with thermal physics until I was teaching it much later.
But while I've got the floor here, I need to say that both of these excerpts that you
put on the screen are stolen, as are most of my ideas. The idea that the enthalpy of an object
is its energy plus the work you need to do to make room for it,
comes straight from Kotel and Kromer's book. And it's written there, it's been ages since I looked
at it, but it's written there in approximately those words. And that was a tremendous insight
to me when I was finally ready to read that page of Kotel and Kromer and ready to understand what
it was saying. Of course, I immediately took that and used it in the classroom. And then it was just
ad-libbing in the classroom thinking, oh, well, what if I wanted to create some object? And then
the cartoon, I asked my friend, Karen Thurber, who's an artist, if she could draw me a cartoon
wizard in rabbit, and she very kindly did, because I can't draw. So I have to acknowledge
Kotel and Kromer and my friend Karen for that excerpt in the book. And then the other one,
the small numbers, large numbers and very large numbers, that comes from one of my
undergraduate professors who taught my undergraduate statistical mechanics course,
Bruce Thomas. And he's someone I've collaborated with more recently as well,
a wonderful teacher. And he made that distinction between large numbers and very large numbers,
which I've been borrowed in my own teaching. It's fine to, of course, recycle ideas because,
I mean, thermodynamics is a very old subject. And of course, we can't reinvent the wheel
every generation. So it's fine. But thermodynamics is also often a very abstract subject. And it's
traditionally discussed in such an abstract way that I think that somehow there was a need
for someone to try to make things a little more vivid and put in a cartoon and be a little bit
whimsical about it, I guess. Sure, sure. I thought I'd maybe plan this discussion. It really is a
more informal dialogue. Usually with my past subjects, I've usually outlined more of a structure.
But I think given how amorphous a subject thermodynamics is, it would probably be more natural
to kind of let it go where it goes. Let's contrast this with a calculus, right? Calculus has a very
logical progression that basically everyone sticks to Euler limits, derivatives, integrals,
it builds up. Whereas I feel like thermodynamics, they're concepts that we learned at a very young
age, temperature, energy, heat. And well before we took a physics course on them, if we ever did
take a physics course on them. And so it's sort of not something that is so cumulative. And it just
has all these kind of, it's more of a dialectical learning process. You learn the words, you learn
some formalism, you go back and you try to make the two harmonious. And it's this more like an
iterative process. So I think maybe, yeah, that's why I think maybe kind of a discussion might be
more suitable. And the way I wanted to organize this is there are really two questions I want to
ask. And I think sort of maybe two of the most important questions would one is trying to learn
thermodynamics. And so let me just write them down. So the first question is, what is temperature?
Sort of, if you're going to learn anything from thermodynamics, you kind of should know what
the answer to that question is. And then the second one is what is entropy? And I guess that's a
more advanced topic, because the word entropy isn't kind of a word, an everyday English word,
but nevertheless, it's very fundamental to thermodynamics. And it's one of those words where
it's described with a lot of poetry in popular science. Disorder tends to increase.
And I'd like to somehow try to maybe resolve some of that possible confusion that might come from
such a, you know, kind of course or poetic understanding. And so what I really hope for
this discussion is to sort of dive deeper and relate temperature to entropy and maybe work
out some examples. So we kind of get a feel for what the subject is about. How does that sound, Dan?
Sure. I'm tempted to say, may I answer the second question first?
Yeah, answer it however you want to answer it.
I'll also start with the first question. It's just that we're not, we're going to have to
go to the, in order to really answer the first question, we're going to have to answer the
second as well. So that's just a little warning. So empirically, temperature is what you measure
with a thermometer. And I think it's, you talked about people learning about some
of these concepts, temperature, energy, heat at a younger age before you take a formal physics
class, maybe in an elementary school or middle school science class or something like that.
People learn about temperature. And of course, we hear about temperature in the news and we
experience temperature directly. But temperature is what you measure with a thermometer. And that's
an important thing to understand. Because it's not the same thing as energy. It's not the same
thing as what we call heat. And those ideas do get confused with each other. So if in doubt,
fall back on the idea that temperature is what you measure with a thermometer. And it's not
those other things, even though those other things are related, those are not what we
measure with a thermometer. What we measure with a thermometer is temperature. And when I
teach in my classes, I get a beaker of ice water and stick a thermometer in it. And then I get a
beaker of hot water and stick a thermometer in and talk about how you stick the thermometer in and
you wait for the thermometer to come to thermal equilibrium with the beaker of water. And once
the reading on the thermometer stabilizes, which takes a little while, which is kind of interesting,
you get to wait a little while to get a good reading, then you've measured the temperature
of the water. And why does that work? We say that there's energy spontaneously flowing from
the boiling water into the mercury in the thermometer. We don't use mercury anymore, do we? I need to
revise my book, because we don't use mercury anymore. Or if you stick it in the ice water,
there's energy spontaneously flowing from the mercury into the ice water, causing a little
of the ice to melt or something. So the concept of temperature is founded on this
amazing fact of the universe that when you put two things at different temperatures together and
wait a while, they will spontaneously come to the same temperature. And one of them will lose energy
and the other one will gain energy. And the one that loses energy is the one we say was at the
higher temperature. And the one that gains energy is the one we say is at the lower temperature.
So temperature is somehow a measure of the tendency of energy to spontaneously flow out of
things. Otherwise, thermometers wouldn't work. Okay, so that's one way of understanding what
temperature is, but it raises a more fundamental question. Why does this even happen? Why does
energy spontaneously flow out of some things and into others? And that's going to get us into
entropy. But there's a first crack at answering your first question. Temperatures,
what you measure with a thermometer, but that wouldn't work unless it was a measure of the
tendency of objects to spontaneously give up energy. Can I maybe push back on that definition
in the sense that you'll say something like the temperature of the sun is this or the temperature
of the early universe was that. And you might not have a thermometer around with you to measure
that. So I don't know if that's like a flippant kind of response, but I just wonder if the-
It's not flippant, but we can get around those ideas. We can get around those problems. We can
eventually learn how hot objects spontaneously give off light with a certain mix of colors.
And therefore, by looking at the mix of colors the sun gives off, we can figure out what its
temperature is even without going there and sticking a thermometer into it. But that's
more advanced and to understand why all that happens. You've got to get to chapter seven or
something like that. That's not going to be the first section of the first chapter of the book.
We're going to learn how that works later on. Of course, it's very familiar to people now
who have these- I have one in my closet- these infrared thermometers. You pointed at anything
you want and it tells you the temperature. It has a temperature readout. You get it at the hardware
store or an infrared camera, even fancier if you have the money. You can measure temperatures of
things, make images that show temperatures of different things in different places. And that's
the same idea as how we measure the sub-temperature of the sun by looking at the color of the infrared
light or in the case of the sun, visible light that they emit. Yeah. So I guess, of course,
we'll get to this, but there's a concept of temperature and there are different
ways of measuring it. I guess the thermometer is a common way of measuring temperature in ordinary
life. But as you pointed out, there are other ways of measuring temperature which aren't
thermometer related, at least on the face of it, it sounds like.
I can't disagree with that. And if you're- are you trying to start a fight?
I just wanted to find temperature as the thing you measure with a thermometer.
And then if you want to state some other interesting things about temperature and
talk about some other ways to measure it, okay. But fundamentally, that's what temperature is.
Let me give you an example of a bad temperature. Oh, okay. Sure. Sure.
When I say it's bad, I'm not trying to start a fight either. No, no, I'm eating that. I'm just
a curious guy. In grade schools, high schools, I don't know, various levels of school,
wherever they teach about these things, that work for that matter, a lot of chemistry classes,
and maybe even college physics classes that are aimed at a non-science, you know, people who aren't
going to go on and become physicists, we often tell people, gosh, I've seen it in a few textbooks
that even are aimed at people who are going to become physicists. We tell people that
temperature is a measure of the average kinetic energy of the atoms in an object. I use k for
kinetic energy, a bar over it for average, k equals three halves. So average kinetic energy per
particle is three halves, Boltzmann's constant times the temperature. So you see, you could
turn that around. You could define temperature as two thirds times the average kinetic energy
over Boltzmann's constant. And so the little k is Boltzmann's constant. Some people put a
subscript b on it in my book. I don't because we use it so much. But all right, so I don't
like that definition of temperature because it's too restrictive. It doesn't work for a lot of
ordinary objects, in fact. A lot of solid objects, for a lot of solid objects, that's not true.
It's a theorem you can prove under certain assumptions for classical systems, for particles
that are behaving classically. But as soon as quantum effects come into play, it breaks down.
And then what if you want to apply it to a gas of photons, which aren't really
particles? You know, what about relativistic systems? Photons are a relativistic system.
Or what about magnetic systems where there's only a finite number of states that a
particle can have? Some magnetic dipole that can point either up or down. And maybe there's only
two possibilities or three or four or something. For all those systems, the relations, those systems
don't even have translational kinetic energy. So what are you going to do? My definition,
that temperature is a measure of the of the tendency of an object to spontaneously give
up energy to other objects is much more general than this formula, which is a version of the
equal partition theorem. This is not the definition of temperature. It's not a definition of
anything else either. It's just a fact you can prove for classical systems that the temperature
happens to be directly proportional to the kinetic energy. And in three dimensions, the constant
proportionality is three halves and one dimension, it would be one half. The better way to define
temperature is is qualitatively what I said earlier that a measure of the tendency of an
object to spontaneously give up energy. Now, I haven't made that precise yet. And if you
want me to make it precise, I can give you an equation for that too, but it's going to have
entropy in it. Okay, so to make that precise, we need to define entropy first. I have just
a question on that. So if you take two objects, here's the amazing thing, right? I take a two
objects, temperature one, temperature two. And it doesn't matter what those two things are made of,
heat will flow from the hotter object, the one with higher temperature to the colder object,
the one with lower temperature. If there's a mechanism that allows that. Yeah, right. I'm
wondering to what extent do the details matter in the sense of we sort of abstracted everything away
about in terms of the material composition of the object, their their proximity, say, I mean,
I guess in practice, the I mean, you did say one thing, which was you had to wait for a while to
measure the temperature, right? So you put your thermometer in the cup of ice, and it doesn't
change temperature right away, you have to wait. And then the question is, how long do you have
to wait until you know that's the actual temperature? So how do you think about in terms of, you know,
this waiting time and when you really know that it's the temperature, sort of,
further operationalize. Usually it'll be an exponential process to measure the
time constant and then wait a few more time constants, and then it's stabilized.
I see. Okay. And I guess maybe that stabilization time might be different for different materials,
but nevertheless, there is some stabilization. And it's called the relaxation time. There is a
name. Oh, okay. The relaxation time is how much time some system takes to come to thermal equilibrium
if it starts out out of thermal equilibrium. And for some systems, the relaxation time is
microsecond. And for others, it's eons, billions of years. The universe is not in thermal equilibrium.
Thanks for that clarification. So yeah, why don't we get to entropy then?
Sure. So entropy is a statistical idea. It's a measure of the number of microscopic arrangements
there can be within an object or within some system. And that's a kind of a hard idea. But if you
think of matter as being made out of atoms or molecules or particles, and these particles can
have different amounts of energy, we can ask how many arrangements are there for the atoms? And then
how many arrangements are there for the energy among the atoms? Different atoms can have different
amounts of energy. And for large macroscopic objects, that turns out to be a pretty well-defined
question, even though it might seem like it's a little bit ambiguous. Which arrangements are
you going to count? Which arrangements are accessible and which ones are inaccessible?
But it turns out that for large systems, that hardly ever matters. And you can count
for enough precision that the imprecision doesn't matter, you can count accurately enough
the number of possible arrangements. And to actually come up with a number, you need to
use some quantum physics. Because in classical mechanics, you can't actually put an integer
number on. Things aren't discreet. There's no way to discretize things to get an actual number.
And that's why I have great admiration for Boltzmann and Gibbs and the other physicists who
invented statistical mechanics before we had quantum mechanics and could put a number on the
entropy. All they could define was changes in entropy. And you could still do that in classical
mechanics. You can talk about changes in entropy. But it kind of takes quantum mechanics or some
arbitrary convention, maybe, to come up with an actual number for entropy, which is, again,
a measure of how many arrangements are possible. And quantum mechanics gives us a way of actually
counting things. And the formulas for entropy always have Planck's constant in them because
that's what gives us the dimensional factor to get an actual number out of it.
Anyway, and if you look in my book or any other book on statistical mechanics,
they'll start with examples of coin flipping. And if each particle can only be in two possible
states, column heads or tails, then how many possible ways are there for a system of
20 coins to have 10 heads and 10 tails? And you can count how many different ways there are of
doing that. And then, if you want an example, it's a little bit more like most ordinary matter,
you could, you can take what we call an Einstein solid, which is a model system that
I first learned about from another mentor of mine, Tom Moore of Pomona College. He really
emphasized this Einstein solid example in his teaching, and I borrowed that. An Einstein solid
is just a collection of harmonic oscillators, but they're quantum harmonic oscillators. So each of
them can store an arbitrary number of energy units. But the reason it's simple is because all the
energy units are the same size for a quantum harmonic oscillator, the runs on the quantum
ladder are equally spaced. So each unit of energy has the same size count how many units of energy
it has above the ground state. And then so then we can talk about a collection of many harmonic
oscillators, sharing energy and how many ways are there for all the energy to be in this side and
how many ways are there for all the energy to be on that side, but we can count it because you
can just count how many ways there are for if you have a given total amount of energy, you can
count how many ways there are of it being shared among n harmonic oscillators. And this model of,
and this was Einstein's 1907 model of predicting the heat capacity of solids by counting how many
ways there are of storing the energy. And real solids aren't all identical harmonic oscillators,
they don't all have the same natural frequency. And so Einstein's model is actually not that
accurate at low temperatures and Dewey's model was better, but Einstein had the fundamental
insight that the reason heat capacities go to zero at low temperature is because of these
quantum effects and the counting coming into play. So first let's understand multiplicity. I'm going
to write that word. Okay, and let's give it a symbol. And I like to use a capital Greek letter
omega. Other people use other symbols for multiplicity w, Boltzmann used w, I guess, and
Cotell and Cromer used g, which is density of states, so they borrowed it from there of it.
But I like to use capital omega, so that's multiplicity. That's a dimensionless number,
that's how many arrangements are there. That's how many possible, we call them microstates,
could the system be in, consistent with what we know about it at the large scale, at the macroscopic
level. And so just as a simple example, if we have a system with three boxes,
and let's say two units of energy, so each of those boxes represents a quantum harmonic oscillator
or something, you can think of it as an atom or some microscopic way placed that the system can
store energy and say we have two units of energy to put in it. Okay, how can we distribute them?
Well, we could put both units of energy in the first box, and that I'll call two zero zero,
or we could put both units of energy in the second box zero two zero, or we could put them both in
the third box zero zero two, or we could put, let me erase a little bit, okay, we could put
one unit of energy in the first box and one unit of energy in the second, and that's what I'll call
one one zero, or we could do one zero one, and I'm running out of space here, so what should we do?
I'll write the rest of them over here, one zero one, let's see, one one zero one zero one,
zero one one, I'm gonna miss some, zero. That's okay, I think we get the idea.
I want to get the counting right though. Oh, I see, okay. So we've got the one one zero,
one zero one zero one, no maybe that's all, sorry, that's all over, yeah, so there's six ways to do
this, for some reason I was thinking there'd be more than six, but that's a different example.
Okay, so there's six ways that we can put two units of energy in three quantum harmonic oscillators,
I'm assuming that the units of energy are not distinguishable from each other, one of them
isn't red and the other one isn't green, they're just units of energy, I'm assuming that the
slots I'm putting them into, the boxes I'm putting them into are distinguishable from each other,
there's atom number one, atom number two, and atom number three, okay, and they're
distinguished by their locations in space or something like that, because this is a very,
very tiny, tiny example of what we'll eventually call a model of a solid crystal in which the
atoms are in fixed in particular places, so okay, so here the multiplicity would be six, okay,
so that the multiplicity is six in this example, there's six possible arrangements of two units
of energy among three harmonic oscillators, identical harmonic oscillators, you know,
and then we can work our way up to larger systems, and I like to do exercises on a spreadsheet where
we work, first we work out what the formula is for, for a certain number of harmonic oscillators
and for a certain number of energy units, what's the formula, and it's a nice common
torque formula, so for an Einstein solid,
right, if we have n oscillators,
so that was three in the example I just gave, and if we have q units of energy,
which was two in the example I just gave,
all right, then omega is q plus n minus one, choose q, if I recall correctly,
all right, so let's go ahead and apply it to the example I just gave, q is two,
n is three minus one, and if viewers don't know what this symbol means, I'll show you in a second,
right, choose two, okay, so what that means, so that's two plus three is five minus one,
that's four, choose two, okay, and so what that means is four factorial over two factorial
times two, four minus two factorial, which is also two factorial,
okay, all right, so what is that, four times three times two times one over two times one
times two times one, all right, cancel that, okay, so we get six, so we could also write
that as q plus n minus one, choose n minus one, like this. That's right, same answer, yeah.
Right, yeah, okay, yeah, so once you have that formula, then you can plug in bigger numbers
without having to actually count all the microstates, and if you plug in numbers that are
on the order of 100 or so, which a spreadsheet program can handle just fine, you get multiplicities
that are numbers with 100 digits, or numbers with 50 or 100 or 150 digits, depending on
exactly what you do, but I think Excel can go up to 10 to the 300 or something like that,
so we're within what common computer software can handle, but it's nice to do exercises with
those, and of course there's quite a few of them in the book, to just get a feel for how overwhelmingly
huge some multiplicities are compared to others, all right, so if I have two Einstein solids and I
ask, well what's the multiplicity for distributing the energy equally between them, compared to the
multiplicity for all the energy being in one, and none of the energy in the other, the ratio of those
multiplicities is a number with 30 digits or something like that, okay.
All right, and therefore, now here comes the fundamental assumption of statistical mechanics,
if all microstates, if all accessible microstates are equally likely, if all accessible microstates
are equally probable, then the macro states, the large scale states that have high multiplicity will
be overwhelmingly more likely than the states that have lower multiplicity, that have much lower
multiplicity, not the ones that have just slightly lower multiplicity, but the ones that have much
lower multiplicity, and this gets us into the idea where if you take two bricks and put them next to
each other, and one of them is at a higher temp, of course I'm using the word temperature, but
if one of them has more than its share of the energy than the other, so the multiplicity is lower than
it could be, then energy is going to spontaneously flow from one of the bricks into the other until
the combined system has close to the maximum possible multiplicity. I think right now what we
have written, it's not clear this relationship between what you just said in terms of the most
likely macro state is the one with the most number of multiplicities, because right now we don't have
two different systems interacting, so why don't we flesh that out a little bit, because I don't
think that's cool. So now we have two Einstein solids, okay, so with n, a, and q, a, and another one
m, b, q, b, so this one has, we can calculate omega for this one, and we can calculate omega
for this one, the omega for the combined system is the product.
In other words, if the block on the left, if block a has 100 possible microstates it can be
in, given how many particles it has and how much energy it has, and block b has 50 microstates
that it can be in, given how many particles it has and how much energy it has, then for the combined
system, again keeping the numbers of particles and energy units on each side fixed, the total
number of microstates is the product, because for every one of the 100 states that the one on the
left could be in, there's 50 states that the one on the right could be in, so we would just multiply
those and get 5000, I guess, for the total number of states for the whole system. Okay, and so we
could go through an example of this, but the idea is that if, in a simple example, we might make the
ends the same and then vary the q's, and so now let's let, let's let energy go back and forth
between the two sides, let's let the two objects exchange energy, well whenever a unit of energy
goes from a to b, qa goes down by one and qb goes up by one, and it goes the other way, then qb
goes down by one and qa goes up by one. So now we can ask, well, for which values of qa and qb
are, would the multiplicity be the highest and by how much? And what we find is that there's always a,
there's always a value of qa that maximizes this total multiplicity.
Maybe it's just clear, yep, so basically you have a total energy, oops, let me,
right, so total, qa plus qb is fixed. Right, and the point is this, this total multiplicity
is a function of qa and qb itself, subject to that fixed value of the total energy, right,
and, and the point is you want to now study this multiplicity function as a function of the qa and
qb. Correct, right, and what we want to do is, so the assumption is that all the microstates
for any way of sharing the energy between the two sides are going to be equally probable.
Let's figure out then which value of qa is the most probable, which value of qa
maximizes the total. Right, right, so I should say, determine a value of qb because that's just the
total minus qa. Maybe one thing that's worth clarifying, because this is something that
I had to re-clarify myself to when revisiting this. So we talked about microstates and macrostate,
we didn't quite clarify what we meant yet, so, so I correct me if I'm wrong, but basically
microstate will correspond to the, let's say, the individual kind of distribution of energies
that you worked out on the previous slide. Whereas macrostate basically involves these
high-level thermodynamic variables, like total energy, temperature, and if we talk about a
pressure and volume, right, those are things that don't involve looking at the particular
granularities, they just involve these relatively few and number macroscopic variables. So,
so macrostate basically just, it's just basically the collection of macroscopic variables.
Total energy, for example. Is that, is that right? Do you want to add anything to what I just wrote?
Or do you think about it differently? Let me just take another crack out to see if this
makes sense. A microstate is the state of a system described in microscopic detail,
where we specify this atom has this much energy, that atom has that much energy, for instance.
And we say where all the atoms are and so on, if it's a gas and they're allowed to move around.
Macrostate is a high-level description, as you said. So we might specify that this brick has this
much energy and that brick has that much energy, for instance. So you might say, well,
can't I divide each of the bricks in half and specify that as well? Sure, you can do that. Okay,
now we have this half of the brick has this much energy and that half of the brick has that much
energy. But you can't, we're not, I'm not going to let you divide it all the way down to Avogadro's,
to Avogadro's scale, right, to the level of individual particles. As long as you only divide
things up a little bit, we can be fuzzy about whether the macrostate is, you know, the whole
system at once or specifying some parts of it, specifying the energies or the number of particles
or something in various parts of it. Here, I'm going to specify the number of particles and the
energy in brick one and the number of particles and the energy in brick two. And I'm saying that
once we've specified all those things, that defines a particular macrostate. But now I'm going to let,
I'm going to consider a whole list of those macrostates with different values of QA and
correspondingly different values of QB with the total health fixed. And then we're going to ask
which one of those macrostates has the largest multiplicity, has the largest number of microstates
and by what factor? And what we'll find is that some macrostates where the energy is more
uniformly distributed in a certain sense, turn out to have much higher multiplicities than others.
And that leads us into much higher probabilities.
So just to maybe wrap up this discussion a bit. So the macrostate in this particular example is
essentially, let's say QA, right? And it's really, right? I mean, there's two variables QA and QB,
but since their total energy is fixed, that gives us one free variable and we'll just take it to PQA.
So the question, the analysis we're trying to do is, what are the number of microstates,
this omega right here, as a function of our macrostate QA as QA varies?
Right. Yep. If you plot that, so omega on the vertical axis and QA on the horizontal axis,
for moderately large systems like 100 particles and 100 energy units. So you get a graph with a
very sharp peak. And if you ask way out here on the tails, how small is it compared to up here at
the peak, again, for a system of 100 particles and 100 units of energy or something like that,
you find that it's 10 to the 30 times smaller or 10 to the 40 times smaller or something like that.
And that's a pretty big number, right? You could, if this system is randomly
going from one microstate to another, right? If the energy is just randomly moving around,
and so the system is sampling the various microstates, all with equal probability,
you could sample it a billion times a second for the age of the universe. And I don't think
you'd be very likely to find it down there, down on the tails of this distribution. And that's for
a system of only 100 particles. What if it has 10 to the 23rd particles? Then this peak is so
overwhelmingly sharp, so overwhelmingly high, so overwhelmingly narrow, that you can bet your life
on the equilibrium configuration of this system being very, very close to that peak, never out in
tails. Right. And it's worth mentioning that basically the width of this peak, I forget,
is it like 1 over n or 1 over square root of n, something like that. But it'll have a square root
in it. And whether it's the square root of q or the square root of n depends on which is bigger.
And yeah, the relative width of the peak compared to the full scale of the graph is
typically 1 over the square root of whichever is smaller. Very, very roughly.
I see. Okay, so let's just, I don't know, o of 1 over square root of, let's say,
min of n and qa, let's say. The point is that when you have n,
avrogadro's number of particles and qa of the appropriate scale, basically this width is
is smaller than any resolution you can probe with, right, into a bench.
So the relative width depends on, so avrogadro's number is about 10 to the 24, the square root
of that is 10 to the 12. So the relative width is one part in 10 to the 12, one part in a trillion.
Okay, well, okay, what's, okay, well, I guess.
Well, to measure fluctuations to one part in a trillion, you might be able to detect
some fluctuations there. Yeah, okay. That's not completely unthinkable that we might be able to
measure, but the probability of getting a fluctuation 10 times bigger than that,
a 10 to the 11, one in 10 to the 11 fluctuation is overwhelmingly minuscule.
Right, right. That's right, because that's basically, I don't know what,
10 standard deviations. 10 standard deviations, right? That's right. So that's financially
suppressed, right? That's right. I guess there's several things we, many, many different forks
in the row we can go on here. But I still haven't told you what entropy is.
Yeah, okay, sure. Let's get back to that. Yeah. Entropy is just the natural log of this multiplicity.
It's times a conventional constant to put it in conventional units, which is Boltzmann's constant.
So, so whether the multiplicity is higher, the entropy is higher, where the multiplicity is
lower, the entropy is lower. And therefore, when you put the two bricks together and wait for a while,
assuming that all microstates are equally likely, we're going to end up somewhere
very close to that peak. And that's going to be the state of the highest multiplicity,
or very near the state of the highest multiplicity, which is also the state of the highest entropy.
Because all the logarithm does it smooths out the graph. It doesn't change where the maximum is.
Okay. Actually, uh-huh. Okay, so actually,
how do I say? So we've talked about entropy in a macroscopic setting in the sense of,
when you deal with ordinary everyday systems, they have an avogadro's number of elements. We are in
this large and setting where everything will concentrate on, you know, the peak entropy, say.
But if you're in a more microscopic setting where you have hundreds, thousands of atoms, right,
very, you know, right, then the statement about entropy always increases or whatnot. There,
you're in a situation where now the full spectrum of probabilities you have, they're all comparable
now. There's not like a delta and everything's negligible. So this notion of-
I'm going to push back on you saying they're all comparable. How many particles are we talking about?
A hundred or a thousand, or they're more comparable than the situation we just drew there.
The fluctuations start to become very noticeable. That still doesn't mean that all macro states are
going to be equally likely. Oh yeah, they're not equally likely, but they're more comparable.
Distribution are still very, very, are still very, very unlikely, even for a system of a hundred or
a thousand particles. But what does change is the width of the peak is now 10% of the width of
the whole graph or 5% of the width of the whole graph or something like that. So the fluctuations
start to become very noticeable, even though it's- you're still not going to get all the way into the
extreme tail distribution. Yeah, I was trying to, you know, at some point when we get to the
second law of thermodynamics where people talk about entropy can't decrease or is likely not
to decrease. I don't know. I like to say entropy tends to increase. Tends to increase. Okay,
I think that's also the correct way of saying it as well. That's nicely a little bit fuzzy, right?
Yes. Yeah, I was just trying to say that things are much more guaranteed when you have more
particles because the fluctuations are less. In a relative sense, yeah. Okay, so the second
law of thermodynamics is that multiplicity tends to increase. You let energy move around and it's-
and systems are going to try to find the macro states with higher multiplicity than the ones
they had before because the multiplicities aren't just a little bit higher. They're a whole lot
higher and that makes them more probable. When we take the logarithm and talk about- and restated
in terms of entropy instead, the entropy might only increase what looks like a little bit,
but you see a small increase in the logarithm of a quantity can lead to a- can represent a very,
very big increase in the quantity you're taking the logarithm of. So taking the logarithm to get
the entropy, all right, Boltzmann's equation, s equals Boltzmann's constant times the natural log
of the multiplicity. Okay, so that logarithm turns a very large number into a number that's merely
large, right? And- but it also change- changes that very sharp peak into a nice broad smooth curve.
Okay, but that doesn't change the probabilities. You know, the probability is still you're going
to be very, very close to the maximum on that curve. And so- so you know, if we- if we graph
for the same system qa on the horizontal axis and now the total entropy on the vertical axis,
we're going to get some kind of a broad curve like this. And the- the top part of the curve
is right where the peak of that multiplicity curve was. It just doesn't look very sharply
peaked anymore. But it's still overwhelmingly likely that for a moderately large system that
fluctuations aren't going to take you very far from that peak. So entropy tends to- if you start
out the system over here in an extreme state, it's going to- let me just get another color.
If you start out the system over here in an extreme state, it's going to spontaneously evolve
that way toward the peak by the laws of probability. And there- of course, if you look
closely enough, there's going to be tiny fluctuations and it's not going to be a monotonic
change. But for a moderately large system, you can be very certain to one part in a trillion or
something like that where it's going to end up. Yeah, maybe it's worth stating a- a few things
here. So one- one of the things you said earlier was that one of the fundamental assumptions of
statistical mechanics- I forget maybe what the right phrase is that- all fundamental assumption
of statistical mechanics. Yeah, right. So the file- I'll just write it. So the fundamental
assumption of statistical mechanics is that all microstates are equally likely.
All accessible microstates. Oh, okay, sure. Okay. You know, we put them in the two categories. We
say these are microstates we can't get into at all. That's not where you're at. But let's count
up the ones we can. As long as it's a mechanism over a reasonable- over the time period we're
willing to wait. Right. So the issue of time scales is- is kind of a tricky one in thermodynamics.
And you always have to kind of have in your head over what time scale am I talking here? You-
you stir cream in your coffee and it mixes in a matter of a second- a few seconds. But- but the
coffee doesn't come to equilibrium with the surrounding room for many minutes or an hour
or something like that. So you- different states might be accessible over a- over a long time
period. There are going to be states that might become accessible that are not accessible over
short time periods. And that's kind of slippery. Yeah. I wanted to bring this up because there's
many things one could say about this. I mean, first of all, in some sense, this is a- both an
insight and a simplification in- in the following sense. If you have an avagadro's number of particles,
right, you can't apply Newtonian mechanics to that system and expect to solve it exactly,
right? That- that's impossible. So what you do is say in my ignorance of all this motion,
I'm just going to maybe let's say do the- the simplest or maybe even the dumbest thing possible,
which is to say, well, everything's equally accessible. And that's maybe a function of,
in some sense, my- my ignorance, right? And- and that's maybe how this principle
is. I mean, I don't know if you think about it in a different way, but it's sort of like saying,
I don't know what's going on. So let's just say everything's equally accessible.
The Newtonian mechanics, the number of microstates is literally infinity
because positions are continued for tables. Fair enough. Yeah. And so that introduces some
issues. I don't necessarily want to say that quantum mechanics is easier than Newtonian mechanics
because we're kind of doing some violence to quantum mechanics here too by only counting
the number of quote states and in what we really mean is linearly independent states
and quantum mechanics. So it's- we're really counting the dimension of the vector space that
the quantum state lives in, right? And- and so I'm- I'm really butchering quantum mechanics here.
Um, but at least quantum mechanics gives us a way of thinking of things discreetly and
counting them. Whereas in Newtonian mechanics, the multiplicity is always infinite. And- and
then we have to talk about the measure of some space and whether some infinities are bigger
than others by some measure and things. And that's conceptually, I think, harder for students.
One thing you did in your book, for example, when you were dealing with, uh, with- with, uh,
ideal gases, right? You started computing entropy in terms of the volume of phase space
they occupy. Uh, what you did in- in that analysis was to essentially discretize it by saying,
well, you have Heisenberg's uncertainty principle. And so you can kind of, uh, chop up your space into
vol- uh, boxes whose size is Planck's constant because that's the- that's the kind of the- the
irreducible small length. And then you could kind of go between volumes and counting. I mean,
that's essentially what- what- what you did, right? And- and I should say that in mathematics,
the way this manifests itself also is you can think about entropy in terms of a counting problem,
log of the number of microstates, but there's also a notion of differential entropy. There's
entropy of a measure. And so there's a way- and there's sort of a way of going back between the
two also kind of taking a continuum limit or discretizing things. And- and those- those are
all kind of related. So- so-
Well, now you're using words that are too big for me, so I'll just not comment on that.
Okay. Okay. Um, uh, actually, let me just write- okay, you know, I- I could teach you,
you can teach me, but let me just say there's a notion of if you have- if you have a discrete
probability distribution, uh, that- that- that's okay, right? Just a- a sequence of numbers that
adds to one. Have you seen this formula? This is- this is an entropy of a- of a probability
distribution. So what I mean by that is this- the sum of the p i's equals one.
Yeah, that formula appears in a problem in my book.
Um, and, uh, I've never found any use for it.
Ah, okay, but actually- okay, here's the connection between what- what we've been doing,
because, ah, it's all written right here. So we look at all accessible microsates are
equally likely. Equally likely means you have the uniform distribution, right?
That would be a special case, right? Yeah.
Yeah, exactly. So in this case, if you have-
So you can derive Boltzmann's formula at the top of the screen from the one you just wrote.
Uh, yeah, exactly. Because now in this case, then in the uniform case,
then that means all the p i's are one over n, and then you'll see that s is indeed,
when you just work that out, is- is just log of n, okay? Because, uh, then you- uh,
you just have, um, all the terms are equal, so you have a minus log of one over n,
and that just is log of n, right? Yeah, so- so anyways, this is just a way of unpackaging,
maybe that gobbledygook I said earlier, uh, about, uh, entropies of measures related to,
kind of, um, you can discretize things, which is what I just did here. This is- this is
discretized version, because in general, entropy is given by an integral when you
have a continuous object, a measure, and then when you go to, uh, discrete objects,
and then you're in this special case of everything being, uh, equally accessible or uniform,
then you derive this particular entropy, which is just a special case of a more
general notion of entropy. Um, anyways, I- I brought this up actually just- well,
this is interesting in and of itself, but to relate it back to what we just said about,
uh, classical mechanics and quantum mechanics, you were worried about continuous versus
a discrete, and this is just kind of part of that story. In- in- in your book, what you did was you
chopped up, uh, space based on the Heisenberg uncertainty relation, which- which says that
delta x delta p is, uh, lower bounded by h bar, and so you can kind of say, okay, well, let me just
take a continuous thing, put boxes of size, uh, h bar, and then everything becomes discrete,
and then- then you could kind of, uh- Yeah, I- I imagine the particles in wave packet states
for which equality holds in the Heisenberg relation, and then say how wide- if they're this
wide in position space, they have to be this wide in momentum space, and- and, uh, and then we can
just ask how many independent ones are there? What does independent mean? It means it doesn't-
most- mostly doesn't overlap, all right? There's- there's a cartoon in the book of that, you know,
a bunch of- a bunch of wave packets that are kind of right next to each other. Count them,
one, two, three, four, five, so if you make them narrower, you'll get more in the same volume,
okay? But if you make them narrower in position space, you got to make them wider in momentum
states, then you get fewer over there. By the way, I've probably gotten more complaints over
that section of the book than over any other- Oh, really? Okay. And- and- and there's other ways to
do it. Um, my excuse, I guess, is that in Chapter 6, I come back and do it very carefully, counting
energy eigenstates. Yeah, instead of putting them- instead of putting the part- the gas particles
into wave packet states, which are- which are not eigenstates of anything, put them into energy
eigenstates, a one-bump wave, a two-bump wave, a three-bump wave, you know, box of a given size,
and- and then the counting is pretty straightforward. But I wait until Chapter 6 to do it that way.
Okay, okay. Um, anyways, this is a fun digression. The reason I- let me get back to the original
point I wanted to make about this fundamental assumption of statistical mechanics.
Yeah, because, I kind of say, this- this assumption of all microstates being equally likely, that-
that really is a cornerstone of all these computations and the analysis. And what I wanted to ask you,
um, going back to this thing I said earlier about sort of entropy being described at this very fuzzy,
heuristic level when people talk about it in normal speaker and popular expositions of science.
So you- you'll hear things like, oh, it's- it's, um, you know, the reason why you can't unscramble
an egg is because that would, uh, be an entropy decreasing event, right? So let's just- let's
just try to unpack that when you- when you hear a statement like that. And as I'm thinking about
this, I'm not sure- I mean, that- that's true. It's much harder to unscramble an egg than it is
scrambled egg. We- we all- we all know what that means intuitively. But in terms of thinking about
it, in terms of thermodynamics, I kind of hesitated a bit because, and, uh, the setting of an egg and
you stirring it, that's not a setting in which all, uh, microstates are equally likely, I- I would
say. I think it- it seems to me more having to do with, uh, there are, um, I would say,
uh, let me take like a silly analogy. Like there's one of you and one of me, but there's a billion
people. And that- that's not a statement about thermodynamics. It's just that there's fewer
atoms that are Dan Schroeder and fewer atoms that are Tim Wynne than there are- that are
lots of other people. Every time you talk about multiplicity, that doesn't mean thermodynamics
is involved, is all I'm trying to say. So- so for me, I feel like when talking about
thermodynamics, you're really in this situation about this fundamental assumption of statistical
mechanics. Uh, give me your thoughts. Like specifically the scrambling the egg.
Okay. Well, I don't know enough about protein molecules to calculate the, um, multiplicity
of, uh, scrambles or unscrambled egg. I-
you know, can we do simpler examples than that?
Okay. Yeah. Or I- I'm just curious, if- if, uh, you know, somebody knows you're a physicist and
they ask you, you know, uh, uh, Professor Schroeder, uh, why- why is it harder to, uh, to- to, uh,
or easier to scramble an egg than it is to unscramble an egg? Would you- would you resort to
thermodynamics or would you just say, oh, that's- that's, uh, that's not- that's not, uh, that's
outside my domain of, uh, expertise? No, no, no. The- it's just that there's a lot of steps to
relate these very, very simple Einstein solid toy systems to- to complicated protein molecules.
But another- I mean, another issue with scrambling the egg is that you're adding energy to the system
during the process and then as the egg cools off it loses energy and so on. Um, I'm pretty confident
that the multiplicity of the scrambled egg is higher than the multiplicity of the- of the unbroken raw
egg. Um, and I would assume that has to do with the denaturing of the protein molecules. There's
more ways for them to be stretched out and wiggling around than there are for them to be
bolded in a very precise way. Um, that, um, there's only a smaller way- number of ways to do that.
Yeah, I guess- I- I guess maybe the point I'm trying to get at is in- in this particular example
that we had with the Einstein solid and two different ones, they're put into contact and
then we make the assumption that all microstates are equally accessible and then we find that peak
and say, ah, um, we find that entropy increases because we allowed, uh, all the microstates to
be equally accessible. So first of all, that- that's- I think I described that situation correctly.
Well, it's not just- it's not just an assumption that comes out of the blue that all microstates
are equally accessible. So if you- if we look at a Newtonian system or a quantum system,
there's a time reversal symmetry to things, which means that if you go from state x to state y,
that there's also a mechanism to get you back from state y to state x. I'm talking about
microstates now. And- and so- so the quantum system we would- and I'm not very good with the-
using the right words here because it's- it's just not, um, something that we- to discuss
a lot at the undergraduate level. But there's something called the principle of detailed
balance. Are you familiar with that? I've heard of it, but you have to remind me-
Yeah, I mean, I'm not sure I can state it correctly, but it- it just has to do with saying that
any- the rate of going from one state to the other is the same as the rate of going back,
or the probability. It's you- I- I think of it in terms of when we do Monte Carlo simulations.
I was gonna say, yeah, in Markov genes, that's where I've heard it. Yeah, that's right.
So- so if you have- if you're writing a simulation of the universe and- and you want to probabilistically
assign a- you know, calculate the chance of going from microstate x to microstate y,
that- and your system has to always- if you're modeling any classical system or any quantum
system, it has to have the same probability, if you're already in statewide, of going back
to state x. Okay, that's the principle of detailed balance. You have to have the same
probability of going either direction. And- and so, you know, at some fundamental level,
I think maybe that's the better way of asking whether it's plausible that all states would be-
would be- all microstates would be equally likely. Now, you might say, well, okay, I can go from
this one to this one, I can go from that one to that one, but maybe there's no way I can get to
these over here. Well, then those are inaccessible, right? So- but show me a- show- show me a version,
you know, show me a force law in Newtonian mechanics, or show me a Hamiltonian in quantum
mechanics, but it'll take you from one state to another that can't also take you back.
I see. But I think the way you would say this in mathematical analysis is that if you had a
Markov matrix that was symmetric, right, so a matrix of all the transition probabilities,
symmetric is just that detailed balance you just said. And it's irreducible, so you don't have
different subcomponents that don't talk to each other. And basically, the only stationary
measure, the only stationary distribution you can have is the uniform distribution.
I think that's the statement. So there's like a Markov-
I have no idea if that's correct.
Okay, I have no idea. Okay, I think I just- I'm just translating what you just said
into Markov chain language. Okay, so it sounds like you don't like the fundamental assumptions
statistical mechanics. I mean, maybe tomorrow someone will discover a system for which the
fundamental assumption is wrong. It's not something I'm going to prove, but I think it's
consistent with our experience. Yeah, no, I- I thought that I don't like it. It was
what I said earlier. It's both kind of- I don't know how much of it is an
insight versus an approximation, but it seems to work well and that's ultimately what matters.
I was just trying to
clarify whether, given that you have to make that assumption for this kind of analysis,
is it really fair to talk- to apply thermodynamic reasoning in all situations where one is tempted
to make statements about order, disorder, multiplicity, yada, yada, when really since
this assumption is what's needed to drive the analysis, that's what one really should maybe
be focusing on more versus like, oh, you know, all these sorts of loose ways in which people
talk about order and disorder. That's all I was trying to- the point I was trying to make.
You have to be able to calculate the antipes in order to apply the second law of thermodynamics,
which is a statement about entropy. You need to be able to quantify the antipes, otherwise it's not
useful. So the second law of thermodynamics says entropy tends to increase and for a
sufficiently large system, you need to worry about small fluctuations away, small probabilistic
fluctuations away from that. Okay, but now if I give you some black box and ask you, you know,
what's going to happen? Some chunk of matter that you've never seen before and ask you,
how is it going to change in time? You might not know enough about it to calculate the entropy of
the state it's currently in or the entropies of the states that it might evolve into. So you still
need to know something about how the system is made up in order to know enough about the
possible things that can happen and what their entropies would be in order to know which way
it's going to go. Maybe, okay, let's just maybe one more thought on this because I think we might
just be at an impasse here, but I think here's maybe one way to nail this point. If you were
an all-powerful demon, if I had control over every molecule, I could arrange for all the
velocities and positions so that they miraculously move all the way into the corner, which would be
a huge entropy decreasing event, and that would be possible in that situation because you violated
this all-accessible microstates are equally likely, because you're in that setting where you
actually know completely what's going to happen to the point where you can't now access every
microstate. It just goes to the microstate that is given by Newtonian determinism. So all I was
trying to say is since we're not Laplace's demon, then we resign to, in some sense, a statistical
ignorance. We make this assumption about all-accessible microstates being equally likely,
and that's the setting in which we do thermodynamics.
Right. I mean, in Newtonian mechanics, at least it's ridiculous to even talk about
likelihoods. The system is in whatever state it's in. Tomorrow it'll be in this state,
the next day it'll be in that state, next day it'll be in that state. So yeah, I mean thermodynamics
is, I don't disagree with the idea that thermodynamics is largely
a, exploiting our ignorance of that, but remember the very large numbers. Just remember
the very large numbers. Actually, that does take me to one question I wanted to ask you
exactly, which exactly was this point about. Newtonian mechanics has this feature of time
reversal symmetry. Nevertheless, entropy, the second law of thermodynamics is that entropy
tends to increase, but if thermodynamics is based on statistical mechanics and therefore
Newtonian mechanics, how do you reconcile the two? The fact that Newtonian mechanics is time
reversible, whereas the second law suggests sort of an arrow of time where it's the direction
which entropy is increasing. Well, the universe apparently has started out in a
very unlikely state far from thermal equilibrium. So from there, it's just the probability
arguments that I've already made. Now, I don't know why the universe started out over here,
okay? But you can see that we're starting out over here and then overwhelmingly likely that
we're going to end up here, or maybe I should just be working on the horizontal axis. We start
out over there and we end up there. Okay. So are you saying that the way you would answer it
is not so kind of funny? It's almost like you're giving a phenomenological answer, not one of the
laws of physics in the sense that, oh, there's a time asymmetry just because in the past we
happened to be in lower entropy. It could have been the case that in the future there'd be lower
entropy, but that's just not what happens, right? Because you could have just switched time. The
laws of physics are the same and now future is past and past is future. So Dan and I got stuck on
this puzzling question about the arrow of time. So I'll just insert some of my own post conversation
comments here. A short answer to our arrow of time paradox is that the fundamental assumption of
statistical mechanics introduces time asymmetry. It assumes that future microstates are all equally
accessible with the past determined and the present as an initial condition. Advanced listeners can
think of it this way. Diffusion processes like Brownian motion add independent noise to future
increments. This causes the diffusion process to spread out in the future rather than the past.
Note that running the diffusion in reverse time causes the increments to instead be correlated
and not independent. For listeners interested in learning more about this puzzle and some
additional perspectives, they can look up Lo-Schmidt's paradox or take a look at Landau and Liftschitz
statistical physics section 8. So my original two questions about what is temperature? What is
entropy? I haven't told you what temperature is yet. What symbol shall I use for energy? E?
Sure.
My book is U. Let's use E for energy.
Okay. So here the inverse temperature is the rate of change of entropy with energy.
For most types of matter, for most objects, if we just plot the entropy as a function of energy,
we get a graph that's increasing and concave down. So what does it mean when the slope of
this graph is steep? Like down at lower energies, it has a steeper slope. That's an object which,
if you give it a little more energy, it'll gain a lot of entropy. If the slope is steep,
then when you give it a little bit of energy, it gains a lot of entropy in the process.
When the slope is shallow, up here, let's call this E sub A and E sub B. So at E sub A,
the slope is steep. It gains a lot of entropy for each unit of energy you add, for each little bit
of energy you add. When the slope is shallow, it gains only a little bit of entropy for each bit
of energy you add. Okay. So what does that mean? That means that if you have two objects,
and one of them has a steep slope, and the other has a shallower slope on its entropy
versus energy graph, and you let them exchange energy in such a way that the total entropy
increases, the one with the steeper slope is going to pull in energy, because its entropy increases
more than the other one's entropy decreases. Maybe what we should do is have two different
objects. So one object has this much energy with the shallower slope, and the other one is over
here. So let's look at that and that. And I should have been more careful with this. So let's
let the first one be object A and the second one be object B. So now we put these objects
in thermal context so they can exchange energy. Energy is conserved. Any bit of energy that
B loses, A has to gain. Any energy that A loses, B has to gain. And we can ask, okay, well,
the second law of thermodynamics says that the total entropy tends to increase.
What's going to happen? The one with the steeper slope is going to gain the energy,
because its entropy goes up more by going a little bit this way. Meanwhile, that one goes
a little bit that way. So A's energy increases a little bit. B's energy decreases a little bit.
B loses some entropy in the process, but A has gained even more entropy, and therefore that
will happen, and go in the other way won't happen. So A is the object that we would say is at the
lower temperature, and B is the object that we would say is at the higher temperature,
okay? Because B is the one that's spontaneously giving up energy to A, okay? And therefore,
we define temperature as the reciprocal of the slope, okay? So that's the slope,
okay? If the slope is steep, the temperature is low. And if the slope is shallow, the temperature
is high. That's how that works. Yeah, this is a very nice visualization. So let me just rephrase
that just to have a different way of parsing what you just said. So these two objects,
they're at their own energies, E sub A and E sub B, and therefore at their own temperatures,
T sub A and T sub B, and by the definition you just gave, B is higher than T sub A,
because its slope is lower, and temperature is the reciprocal slope, right? So what you just,
what we just illustrated here, right? So this has higher slope, therefore lower temperature,
and this has lower slope, and therefore higher temperature. And then sort of the picture you
should have in mind is that you can gain more, gain more entropy by moving energy to the right.
Let me draw the ingredient. So here energy EA is going to increase, and energy EVs are going to
decrease, because the amount of entropy you lose when you move, when B moves to the left,
is more than compensated by the entropy increased by A moving to the right, precisely because of
this slope disparity. And then the equilibrium temperature they're going to reach, and it will
be at the point at which these two slopes are equal, which is precisely them being at the same
temperature, and the reason why the slopes are equal is because then there's no net gain by
moving, by adjusting the energies, right? And that's precisely, right? So you've taken it one
step farther and asked where does the process end, and the answer is exactly where the two
slopes are equal, and the odds are at what we would call the same temperature when the slopes are
the same. So that's what temperature is, and that's what this formula is telling us, that
temperature is the reciprocal of the slope of an object's entropy versus energy graph.
And therefore, it is a measure of the tendency of an object to spontaneously give up energy to
other objects. The higher the temperature, the more the object will spontaneously give up energy,
give up energy to other things with lower temperature. Yeah, and this basically is the
formal definition of temperature, correct? Yeah, it's the only definition of temperature.
So, you know, there's an operational definition, what we measure. I'm not sure what you mean.
What did you mean by the word formal? Is there an informal definition? Oh, yeah,
by far and at like in terms of a mathematical definition, which is what this is. Yeah.
I was just wondering if you had some other informal definition in mind?
No, I mean, yeah, I mean, there was a potential formal definition in terms of
average kinetic energy, because that is... Oh, no, no, no, no, no. Yeah, it's not, it's not.
Don't try to find that one. Yeah. Yeah, yeah. So this is very nice as a mathematical definition.
Now, one thing I'm genuinely curious about, you did mention that the founding fathers of
thermodynamics, they had to sort of do things the hard way in terms of not having quantum
mechanics at their disposal. Now, I think historically, maybe actually, maybe you can
comment on this, because there's Carnot, there's engines, and they thought about irreversible
processes and sort of the way it's often written is, let's say D, E, just to be
consistent with your notation and thinking of it as heat energy, so I'll write it as DQ.
No, I'm not going to let you do that. What does Q stand for?
Well, at least in your book, it represents the heat.
Heat is energy and transit. So when I talk about energy spontaneously flowing from one
object to another, that's what we call heat. It's the energy transferred due to that spontaneous
process. It's energy and transit or an amount of energy that moves from one place to another
spontaneously and driven by this entropy increase, overall entropy increase.
What I don't like is the little D that you put in front of Q.
Oh, sorry, I should have just written Q, maybe. Is that what you were complaining about?
If it's an internet decimal, yeah. So when you put a D in front of it, people want to say the
change in Q. Okay, sorry. It was not a state function that can change. It's an amount of
heat that flows. Got it. Fair enough. Fair enough. Okay. Yeah. Okay. All right. Thank you. Thank you.
And then Q, let's see if I, you know, is TDS. All right. Okay. So I think this is the way
people historically thought about the relationship between these quantities and I'm curious
to know a little bit more about the history and how like people, you know, we have the benefit
of generations of work. We could just write down these formulas and talk about it as if
everything's obvious, but somebody had to figure this out, right? So how did they sort of
think about this? I don't know if you have any further comments.
I don't know. I'm not a historian and I've never studied the history. I've, you know,
read a little bit of Carnot in translation and maybe a little bit of Clausius and a little bit
of Boltzmann, but not enough to have any insight into the history. And of course, history is
incredibly complex when you get into it because they don't have the benefit of hindsight on these
ideas and they're wrestling with them and often aren't expressing things the simplest way that
they could. So I'm not in a position to say that, you know, and in textbooks, we always give a little
bit of pseudo-history and I do too and talk about how Carnot came first and then Clausius and then
Boltzmann. So, you know, I know a little bit about the chronology and who, you know, roughly what was
known at one time and by a certain person and what was roughly what was known at another time by
another person, even if they wouldn't have said it the same way we do. So with all those caveats,
what do you want to ask me? Oh, I just wanted to see if there was a way of, because I certainly,
as a mathematician, I'm very happy with the story. I was just wondering if there was any
insight to be gained about how people thought about it in such a way that it might give insight to
how this was discovered. Well, everyone's seen that when you put two things at different
temperatures together, the hot one cools off and the cold one warms up. I guess people like Carnot
and Clausius thought very deeply about that. Now, Carnot did not, my understanding is that
Carnot did not know that he was a form of energy. And he didn't quite distinguish between entropy
and heat. And yet he had this deep insight that led him to formulate a version of the second law of
thermodynamics and put limits on how efficient engines can be and things like that from that.
You know, and by Clausius's time, a few decades later, it was understood that
that heat is a form of energy or a means of transferring energy.
Even then, it wasn't. So Clausius figured out that there has to be this thing that he called
entropy. There has to be this quantity. Every object has a certain amount of it. And it's related
to temperature by this formula that you wrote and I wrote. It's the same formula, right? Your formula
is the same as mine. Okay. So you just like D's and I like Delta's. Okay. But it's the same,
you know, and you don't need Q, right? You can just, you could say just changing energy as T times, yes.
I call that formula the definition of temperature, that that formula embodies
the most fundamental thing about temperature, which is that it's a measure of how generous an
object is with its energy, giving it up to other objects. Okay. And you know, and then it was
Boltzmann who made the connection to statistics and microstates and understanding what entropy
actually is in a statistical sense. Have we answered everything? Is there anything else you
wanted to say? There's a lot of things we have not answered, right? But I, you know, I hope I've
given a basic textbook explanation of what entropy is, how it relates to temperature and
what the second law of thermodynamics says. Do you have any final, I don't know, any final thoughts
about the subject? I know, of course, thermodynamics is a richer subject that can be covered in a
single conversation, much less even a single textbook. Yeah, just any, I don't know,
final thoughts on the subject or words of advice on maybe people trying to learn the subject?
Well, those are two different questions. Let me, let me give another thought.
We've been speaking very abstractly and talking about some pretty abstract model systems and things.
Thermodynamics is also extremely important in the real world. And, and some people might be
less interested in that, and some people might be more interested in that. But I think everyone
has a reason to care about thermodynamics. Thermodynamics does, as Carnot showed, put
limits on the efficiencies of engines. It tells us that we can make heat pumps that are more
efficient than an electric space heater. It's an incredibly important thing to understand
thermodynamics to, if we care about energy and the biman implications of energy and the energy
transition as people try to use more energy more efficiently and get it in better, get it from
better sources. Thermodynamics is what tells us where all the energy ends up. It all ends up as
dispersed waste heat that we can't make any use of. And what it means to use energy, right? We
can't destroy energy. No, but using energy means taking it from a low entropy form and
turning it into a high entropy form where it's very widely distributed. So I guess I think it's
important that people understand thermodynamics if they want to participate in making the world a
better place through coming up with better energy technologies. And I think there's a lot of
I see a lot of misconceptions about thermodynamics and I guess I would encourage people to get away
from those. One of them is that temperature is merely a measure of the average kinetic energy of the
system. But another one, a much more specialized one, is that when an engine doesn't achieve the
maximum efficiency that the second law of thermodynamics allows, I see in a lot of textbooks
and a lot of people talk as if the reason is because, well, there's just friction. And if
there were no friction, maybe it would achieve the Carnot efficiency. No, that's not the reason.
And it has to do with the reason why the heat closed in the first place and how a heat engine
works. So I think that there are very, very practical reasons to understand basic thermodynamics.
All right, now you also asked if I have some advice for students learning thermodynamics.
Don't just watch videos. Get out the pencil and paper and do some hard work and wrestle with
things. Oh, absolutely. That's my advice. Yeah, I mean, this, make no mistake, this the videos
I produce are not a substitute for the labor that is involved in any learning process really,
not, you know, particularly in science and math, but in any field, there's a long pipeline and
repetition of doing problems and thinking about it until you really understand the material.
Great. Well, thank you so much for your time, Dan. This was very, I think, enlightening to get
your thoughts. Certainly, it's rare to hear it from the author himself when learning a subject.
Well, thank you very much for your interest, Tim. And thanks to all those out there who are watching this.

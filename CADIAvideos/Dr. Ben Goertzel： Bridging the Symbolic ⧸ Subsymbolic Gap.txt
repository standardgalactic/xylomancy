It's a pleasure to be at this AGI summer school in Iceland.
If my calculations are right, there are four of us here who were at the previous AGI summer
school in Jaman, China.
So it's great to see the AGI meme and movement continuing to spread, and interesting to see
it spreading in sort of out of the way corners of the globe, which is quite cool, especially
for those of us who like to have an excuse to travel to interesting places.
I'm going to talk today about the Open Cog architecture for AGI, and then in the main
novel substance of the talk about some ideas for bridging the traditionally perceived gap
between symbolic and sub-symbolic aspects of intelligence via integrating into the Open
Cog integrative architecture, a hierarchical temporal memory architecture for perception,
which is Itamar Irel's destined architecture.
The Open Cog architecture has been around a few years, and we're using it to control
some virtual agents in virtual worlds.
So it does some things, it's not amazingly generally intelligent yet, but it's functional
and is improving month by month.
The ideas I'll discuss about bridging the symbolic, sub-symbolic gap and connecting
Open Cog to visual perception, this is stuff that is currently active research and doesn't
do much yet, and therefore it's more interesting for me to talk about, because I'm always
more interested to talk about what I'm doing than what was already done last year.
So this will be a mix of things we've done using Open Cog and things that we're now
working on.
I'll start with just a few words of what I consider to be AGI, I won't go into much
depth on this since this is the AGI summer school, and people have probably filled you
in on these generalities already.
I think about general intelligence heuristically as the ability to achieve complex goals in
complex environments.
I don't mean by that that explicitly goal-seeking behavior is necessarily the be all and end
all of general intelligence, there are aspects of what humans do that are not explicitly
oriented toward achieving goals, and ultimately our conception of intelligence is going to
change as we develop more and more intelligent systems, but this seems a useful way to think
about intelligence for the time being.
As you've probably heard, Marcus Hutter, Juergen Schmiduber, and some others have worked
on grand general formalizations of what is general intelligence.
One thing you find from pursuing these formalizations is that human beings are not all that generally
intelligent.
I mean, if you compare a human being, theoretically, to an optimizer that can achieve any computable
goal in any computable environment, which you can mathematically show in some sense exists,
humans come out looking pretty stupid.
I wouldn't take that too seriously personally because I'm not sure that this conception
of intelligence, of maximizing objective functions or goals and environments is going
to be the same way we think about intelligence 50 or 100 years from now anyway, but I do
think there's something worthwhile in the observation that humans are not, like, maximally
totally generally intelligent.
We're actually a mixture of various specialized intelligence subsystems with some core that
in some sense does have a very broad generality.
I mean, our visual cortex is specialized in recognizing visual patterns.
Our episodic memory is specialized for storing, retrieving, and organizing memories of our
life history and so on and so on.
So our minds actually are quite specialized for a variety of different things, but yet
we do have this capability to transfer knowledge from one context to another and to explore
areas like, say, abstract mathematics that are very different from anything that we were
evolutionarily wired for.
And this point comes up in the details of the architecture of the open cog system, which
is why I'm drawing on it here, because in my own view, the way you want to architect
a general intelligence is to respect this mixture of components at various levels of
specialization and generality that the human mind has.
I mean, you might think as a computer scientist, well, let's make one totally general learning
algorithm which we can program in, like, five lines of Haskell or something, and this will
be the core of thought, kind of like the equation of string theories, the core of physics or
something.
On the other hand, I don't think that's what you see in the brain.
What you see in the brain is a mixture of components at various levels of generality
and specificity all working together.
And while I don't think we have to emulate the brain in detail, I think this particular
characteristic of the brain is a natural, perhaps inevitable consequence of trying to
achieve a modest amount of general intelligence, given a modest amount of computational resources
unlike the hypothetical, absolutely general intelligence which shakes infinitely much
processing power.
If you start to think about human-like general intelligence and what we do, it boils down
into a really long, boring, ugly laundry list of different things.
I mean, we have perception, actuation, memory, learning, reasoning, planning, attention,
motivation, emotion.
We can model ourselves and others.
We interact socially.
We communicate.
We can build things.
We can do quantitative inference.
And each of these boils down to a whole bunch of different subcategories as well.
Now none of these are human-like general intelligence, yet the human approach to general intelligence
draws on tools that evolve for handling each of these things.
And if we want to build a human-like general intelligence, ultimately we've got to deal
with all of these things.
I mean, we may start with one of them and then branch out to others, but those are all part
of the requirements list.
Now, when we build narrow AI systems, we're thinking about it a different way because
we can focus on just a few capabilities.
You can focus just on, say, self-driving cars or information retrieval or a game like chess
or, as in some of the biology work I'm now doing, we're using machine learning and probabilistic
reasoning to analyze the genetics of long-lived flies.
We have these flies that live five times as long as regular flies.
We want to see why they live so long.
You know, that's a valid AI research.
It's certainly interesting to use machine learning to see how different genes and proteins
and networks combine together with each other.
But that's lacking an ability to transfer knowledge from one domain to another.
And the program has only one goal in this case, which is to find the combinations of
genes and proteins that imply longevity.
With general intelligence, we're not looking at any one particular goal, like driving a
car or finding why flies live a long time.
You're looking for a system that can, in principle, do anything and learn anything, although,
of course, some things are going to take it longer to do or learn than others.
So this has probably been pitched to you already, but if you're interested, the AGI-12 conference
is in Oxford in early December, which is a collaboration with the Future of Humanity
Institute at Oxford, and they will immediately following AGI-12 have their own conference
on the impacts of AGI and the philosophy of how to make AGI's be ethical as they get
more and more intelligent, which is a separate but interesting topic that has some interface
with the hardcore AI research.
Also, if you haven't seen it, there's a paper that came out of the 2009 AGI Roadmap Workshop
that a number of us did at the University of Tennessee, and that paper is Mapping the
Landscape of Human-Level Artificial General Intelligence.
That workshop was interesting, both intellectually and, I would say, anthropologically.
It was an outgrowth of a previous workshop.
The previous workshop was convened by John Laird and Pat Lange, who are part of the,
you could call the Cognitive Architectures or Traditional AI Establishment in the U.S.
Lange and Laird organized a workshop at the University of Illinois, and the goal of that
was to bring together everyone working on human-level AI, which is a term they prefer
to the term AGI, and try to come to some common understanding of how to approach human-level
AI.
What are the metrics for evaluating progress toward human-level AI?
What are the principles that should underlie architectures for human-level AI?
Now, that workshop led to some interesting discussions, but it's fair to say it did not
achieve that goal.
The level of disagreement among the people in the room was so radical that, pretty much,
we couldn't agree on much of anything, except that a human-level AI ultimately should be
able to do everything a human could do, which we didn't need a conference to tell us.
So, Itamar, Arell, and I decided to try again and organize a similar workshop with people
who were a bit more like-minded.
So I tried to gather together people who were not thoroughly committed to, say, a symbolic
approach to AI.
So I invited only people whose AGI approaches didn't involve any hard-coded production rules.
So only people who were interested in AGI's that could learn from experience.
And also only people who were at least interested in robotic or virtual embodiments.
So I tried to get only researchers who wanted to build AGI's that could learn from experience
by interacting with and experiencing some 3D world with some vague resemblance to the
world that we humans live in.
Unsurprisingly, but amusingly, this group also couldn't agree on much of anything, although
it was quite narrowed down from the scope of those original workshops.
However, we did agree on enough to write a paper describing our various perspectives
and their interrelations regarding not the architecture of how to build an AGI, but more
what kind of tasks and scenarios made sense for testing an AGI and for gradually growing
in an AGI and mediating its development from nothing to human-level intelligence.
With someone interesting, that paper was in AI Magazine not long ago, which is progress.
It's not the first time this sort of stuff has appeared in AI Magazine, although their
bias is toward narrow AI.
In 2006 or 2007, Nick Kasimates did a special issue on human-level AI in there also.
So it does pop up now and again with modestly but significantly increasing frequency.
We could not in that workshop agree on what kinds of environments were best for AGI.
A lot of interesting ideas were thrown out there.
Sam Adams from IBM had the idea of a general video game playing environment where basically
you'd want your AGI to be able to play any video game at all.
Just throw the game, give it the controller, and let it play now.
In one version, you'd make it use robotics to move the physical controller.
Another version, you could just connect it digitally to the game console, which I think
is reasonably fair.
Alexei Samsonovic was championing basically school, give it schoolwork, grammar schoolwork
or high schoolwork, pretty much paper schoolwork, but a variety of different types.
Joshua, who was there, was advocating an approach to video understanding and movie understanding
with tests, like among others.
You know, look at this ten-minute scene of a Hollywood film, tell me what happened.
What do you think is going to happen next?
Why did that character do that?
Which involves integration of a whole bunch of different kinds of understanding.
I was advocating some sort of AGI preschool, where you take your AGI, either in a robot
in a preschool setting or in a video game character, in a kind of video game setting
where it can play with various things, let it go through the various kinds of training
and playing exercises that young children do.
You can see all these different scenarios are sort of getting it the same thing, but
you can also see some significant differences there in terms of what you'd actually need
to build.
I mean, in Joshua's suggestion at that time of video and scene and story understanding,
I mean, you don't need to mess with the mechanics of actuation, right?
You don't need a hand to build stuff, be it a virtual hand or a robotic hand.
On the other hand, Josh Hall, who was also at that workshop, he advocated the Wozniak
coffee test.
So Steve Wozniak, the co-founder of Apple, had said once, no robot will ever be able
to walk into a random house in America and make a cup of coffee.
By which he meant, you know, find where the coffee grounds are stored, look around, find
the appropriate machine, figure out how to use it by looking at all the dials and figure
out where's the cupboard to get the cup and so forth.
Common sense understanding.
And he was, I assume, ruling out the approach where the U.S. Census Bureau people to every
house in the U.S. and do a map of everyone's living room and everyone's coffee machine.
It was supposed to be general learning in the domain of making coffee, but still deal
with any house and any coffee machine.
And Josh really believes you got to do that.
You got to deal with real-world sense perception, real-world actuation, or else ultimately you're
doing something that's going to be narrow and can be codified in some long list of expert
rules or something equivalent.
So basically he believed you need a robot, and this is one of the disagreements that
we couldn't overcome in the context of this workshop.
And I dwell on this because it gets the meat of this talk, which will come a little later,
in terms of what we're doing now aiming toward integrating OpenCog with robots as both of
the virtual characters that we've mostly been working with.
And I have been, as I work more and more with virtual worlds, I've been drifting further
and further toward, wow, a robot will be a good idea.
I'm not extreme like Josh Hall or Rodney Brooks or these guys who think you need robotic embodiment
to make AGI.
I mean, I think you could make an AGI using only a text stream of input.
I mean, it's more a matter of what's going to be easier, what's going to be a faster
way to do it, what's going to be a way that we can understand better and debug it better
and guide it better, what will take the most modest amount of computing resources and human
attentional and cognitive resources to build, rather than there being some magic in the
robot body as typically perceived.
So that's some general framing.
Now I'm going to talk a bit about OpenCog, and I'll go through this at a fairly high
level because I want to get to the symbolic, sub-symbolic integration stuff before the
time has expired.
So I'll start off here with some speculative futurology.
So I'm often asked this, not so much by AGI researchers, but by journalists and people
from the general public, or by rich guys who don't actually want to give me any money,
but just want to gloat about the fact that they could if they wanted to.
If I were to give you $20 million or $100 million, although I won't, ha ha ha, then
how fast could you build an AGI at that point?
And of course the answer is we have no idea.
Anyone who's done radical science knows sometimes things actually go way faster than you thought.
Oftentimes things go way slower than you thought, and you don't know what are the hidden problems
till you're going through the work.
But nevertheless, we have to make a plan to guide work, even though we know that it won't
actually be followed.
So when we're trying to plot out what could be done with OpenCog, if we could hire as
many people as we wanted, and by all the computers that we wanted, we come out with something
like this.
What we're doing now is what I call a proto-AGI virtual agent.
I mean, we're using OpenCog's system to control a virtual agent in a virtual world.
It's not all that generally intelligent.
It does do things, and it learns things, and it builds stuff in the world.
Assuming our funding holds out, and we don't run into any unforeseen technical obstacles,
in the next few years, we'll be able to get the AI system smarter and smarter in the virtual
world in terms of doing simple child-like stuff in the virtual world, and we'll get
started in a serious way on the robot integration.
A couple more years for more advanced learning and reasoning, then the way I see things proceeding
in practice is I think we're going to be able to build, I don't want to call them expert
systems because that has historical baggage, but I think we're going to be able to build
AGI systems that, it sounds kind of funny, but that are generally intelligent in the
way that's focused on particular domains, and it's like a narrow AGI, but I mean, to
think about that in a specific context, think about a home service robot, I mean, or Wozniak's
coffee test, a robot that could do that, I mean, you could imagine an AGI that could
be a reasonable home service robot and could go into an average house and make coffee,
but it wouldn't actually be able to teach third grade, ride a motorcycle, prove a theorem,
compose a sonnet, or even write a decent letter.
I mean, I would argue once you've got something that can pass the coffee test or be a home
service robot, you've got in there the crux of an AGI algorithm and architecture that
could be used to do anything humans can do.
On the other hand, in actual practice, there may be a gap of years between getting the
first AGIs that display some common sense learning and general intelligence in a specific
domain and actually manifesting the possibility of that architecture to do everything that
humans can do.
I mean, that's, well, we'll be a short period of time on the historical scale, but maybe
years anyway, I mean, could conceivably be the decade or two, that's really hard to say.
The service robot is one example, another example tying into my bioinformatics work
will be a bioscientist.
I mean, say, right now we use narrow AI to do machine learning and crunch various biology
data sets, but you can imagine a narrow AI slash AGI bioscientist that could read research
papers in a reasonably general way, extract the meaningful information from biology research
papers, ingrid that with machine learning, run biology lab equipment without being able
to do every single thing that the human being could do.
So I think we're going to see that because that's what the world wants.
The world will pay for proto-AGI systems that do specific things, and out of that eventually
we're going to see full on human level AGI, and out of that we're going to see something
going beyond the human level.
I mean, ultimately, as Christian has talked and written about, I mean, the human brain
has many weaknesses, one of them is it gets old and dies, and another one is that when
we open it up and try to tinker with it, we tend to cause it to function even worse than
it usually does, whereas an AGI system won't necessarily have any built-in impediments
to self-modifying and improving its own code.
So you can envision once you have an AGI that has a general intelligence of a computer scientist,
you can have a lot of really fascinating pathologies as it reprograms itself in stupid ways and
becomes demented, but you can also have some very rapidly improving levels of general intelligence.
And I think in theory we could see all this unfold within the next decade, but that would
require things to go well, we would require society to decide to fund AGI reasonably amply,
so that AGI teams weren't working half-time on AGI and half-time on narrow AI, or not
advancing at all, as is the case with some AGI designs.
So right now we're developing the OpenCog architecture toward AGI, and we're also developing
it with various applications in mind.
So we're using it to control video game agents, we're using OpenCog to crunch biology data,
we're using it for financial prediction on the Hong Kong stock market, and this at the
upper corner is a screenshot of our current video game world, which is Minecraft-like,
it's built in the Unity 3D game engine, but it lets the AGI build stuff out of little blocks,
basically tens to hundreds of thousands of little blocks, so it's a bit more ambitious
than the good old blocks world from decades ago, but it follows some of the similar intuitions
that building with blocks is a domain in which you can give the AGI a combination of perception,
action, reasoning, learning, social interaction, and so forth, but it's fairly computationally
tractable.
Before that we're using it to control a virtual dog in a virtual world that learned tricks,
basically, and the robot at the bottom is from some experiments we did in 2008 and 2009
using OpenCog to control a now humanoid robot at Xiamen University in China, that project
is sort of mothballed at the moment, but we're looking at a different robotics project involving
the Hanson Robocond that I'll discuss in a moment, so on the very high level, as I said,
you can look at general intelligence as the ability to achieve complex goals and complex
environments, and what that basically means is an intelligence need to use its perception
and its memory to predict what actions will achieve its goals, and of course that's really
easy, right? Memory, just, we already have memory in the computer, perception, hook up
a camera, action, hook up some arms, and just hook up a universal, all-powerful prediction
algorithm, and it's all done, but the problem is the universal, all-powerful prediction
algorithm takes infinitely much computer power to run, so we have to get subtler than that
and put a bunch of fairly well-architected things in each of these slots to get it to
do anything, and to approach AGI in practice there's a lot of big questions we need to
ask, and different researchers answer them differently, so do we want a single cognitive
process, which is at the core, and there's a kind of periphery around it? I mean, Pei Wang's
approach is a bit like that, as I understand it, he has a non-aximatic logic, he has a
logic engine built on this unique and interesting logic, he doesn't necessarily think that logic
is it, but that logic is the center, and then you have something dealing with visual perception,
language processing, procedure learning, and so forth, and these are sort of implemented
in terms of the one core logic, and at the other end of it, something like Jeff Hawkins
or Itamar Arell's architectures, which are based on hierarchical temporal memories, again,
for them the hierarchical pattern recognition network is the core, and anything else that
you have is sort of at the periphery, helping that hierarchical pattern recognition network
to do its thing. Another approach is there is no core complex process, you just got a
bunch of small, simple elements together interacting according to some complex dynamics, and intelligence
emerges out of that, and many people in the artificial life community think of intelligence
that way, and I think that's quite interesting, it seems to me harder to involve more computing
resources than engineering something. Another approach is to take a number of complex processes,
a number greater than one, and hook them together in a complex way, OpenCog is more like that,
it's why we call it an integrative architecture, so we have a probabilistic logic engine, we
have a probabilistic evolutionary learning engine called Moses for procedure learning,
we use something attractor neural network like for attention allocation, each of these
is its own complex story, and actually I can tell a story for each of them why it could
be the core of an AGI system if I wanted to, but I think the best approach is to integrate
a bunch of them together. Again, another difference in focus between different AGI researchers
is some folks focus on the architecture, Soar is a good example, you talked to John Laird,
I mean his view is, yeah we haven't done much with learning yet for the last 30 years or
so, but that's because we need to get the architecture perfectly right, and once you
have the architecture right, you know making learning work within the architecture is no
problem, on the other hand you look at people in the reinforcement community, they're like
well no, we don't have a cognitive architecture, but we're getting learning based on reinforcement,
and once you get learning based on reinforcement, then the architecture is kind of just something
you glue on and wrap around your learning engine, right, and you know each of these
approaches has taught us a lot, I tend to think you need a good architecture and good
learning algorithms, and they need to work together well, which makes things even more
complicated. Then you have the issue of knowledge representation, I mean some folks going back
to Soar or Act R for example, you have explicitly represented knowledge, which is often called
symbolic knowledge, you have knowledge where you can look at a knowledge item in the systems
knowledge base, and as a human being you kind of read out in natural language what that
piece of knowledge represents, like a cat is an animal, or you know to pick up a cup
of coffee I must extend my arm and open my fingers. The other approach is to look at
implicit knowledge representation, where the structures that emerge from the interaction
of the systems atomic elements spawn context appropriate knowledge representations, and
so to see what is the systems representation of a cup of coffee or of cats and animals,
you would need to be able to decode the complex interactions and networks of the systems elementary
components, just like it's hard to look in the brain and see how a piece of knowledge
is implemented, and again you can see the pattern of my way of thinking, in OpenCog
we use both, the explicit and the implicit forms of knowledge, so what you can see is
I hate to make these choices, so I just think the mind uses all of these approaches in different
aspects and makes them work together, and an AGI system to work with feasible computing
resources is probably going to have to do that too, so if you look at OpenCog.org there's
more information, I'm going to give a few more slides in OpenCog too, the book The Hidden
Pattern that I wrote a few years ago goes over a bunch of philosophy of mind, but also
talks some about the AGI architecture, it talks about the Novamente cognition engine,
which was a proprietary system that was a predecessor to OpenCog, but it is conceptually similar.
Building Better Minds is a book I've been working on seems like forever, which actually
exists in a complete draft form now, and should go to the publisher in a few months, probably
come out early next year, which goes over the OpenCog design in great detail.
Another way to think about everything that a system like OpenCog has to do, this diagram
comes from Aaron Sloman with a few tweaks by myself. What we see here, we have an environment,
we have percept and action, we have long term memory, and associated with it long term thinking
processes, which he calls deliberative processes, you have working memory, which is related
to what's been called short term memory, associated with that, you have what Sloman
calls reactive processes, you have some specialization for things like language and metacognition,
then you have arrows pretty much going between everything and every other thing, because there's
so much feedback in the mind. One thing you could do is follow cognitive science and just
try to put one thing into each of those boxes and then connect them together. That's a little
bit like what Sam Franklin has done with his line of architecture. He's taken the box
and line diagram drawn from cognitive neuroscience and put a certain AI algorithm and structure
in each box. In OpenCog, we've done something a little different, but we still try to achieve
all of those functions. For knowledge representation, as I said, we use both explicit and implicit
knowledge representation. We do them within the same data structure, which is a weighted
labeled hypergraph that we call the atom space. Some nodes and links in the atom space represent
explicit logical relations, like a cat is an animal or more complex things involving
nested quantifiers. Some nodes and links in the atom space are more like Hebbian links
in an attractor neural network, which are learned by experience and which represent
knowledge via their collective coordinated activity patterns. An example of explicit
knowledge here, this is a bunch of nodes and links that are labeled. There's a cat node,
a dog node, a jazz node, a tail node, representing cats have tails and so forth. These may be
there in OpenCog's network if the network has read stuff or heard language, because
then it will brew nodes internally to correspond to the words it's heard. On the other hand,
you can also have a lot of nodes and links that have no labels on them and don't correspond
to any particular thing that a human could describe. In this one, the top cluster of
nodes and links pertains to chickens, the bottom cluster of nodes and links pertains
to food. Some of the nodes have words attached, some of them just represent placeholders inside
the system's knowledge base where the weights of their links to other things are what is
significant. You get a whole bunch of links of different types, going from the chicken
bundle of nodes and links to the food bundle of nodes and links. You could look at it as
just an overall collection of synapses relating one to the other, or you could look at each
node and link and its specific semantics. We look at there as being a number of major
types of memory in OpenCog, and one of the architectural decisions we made is to treat
each of these kinds of memory somewhat separately. So we'll hit declarative memory, which is
facts and beliefs, and we'll handle that using a probabilistic logic engine called PLA and
probabilistic logic networks. There's a book on that that was published by Springer in 2008,
Procedural Knowledge, Knowledge of How to Do Things, represented by little programs in
a Lisp-like language called Combo. These little programs are learned by a system called Moses,
which does probabilistic evolutionary learning, which means it's kind of like genetic programming,
but instead of crossover and mutation, you do probabilistic modeling of the evolving
population, and then generation of new programs by instance, generation of the distribution
found to model the population. For episodic knowledge, we use a simulation engine, among
other things, which is sort of a detailed engine simulating the 3D world, and we also
use a kind of sparse distributed memory. For attentional knowledge, knowledge of what
to pay attention to at each point in time, we use something kind of like a heavy and
neural network. We call it economic attention network, so it spreads virtual money among
the nodes and links, rather than spreading activation, but it's supposed to simulate the
way activation spreads through the brain, kind of making a moving bubble of attention go through.
So each of these kinds of memory has a different knowledge structure, a different software data
structure associated with it, and a different learning mechanism associated with it in open
cog, and then these all have to work together. So from a big picture, what distinguishes
open cog from most AGI approaches out there is that rather than having one AI representation
and one learning or reasoning engine at the core, we have a number of big complicated representations
and learning and reasoning approaches, and they're all configured in a way that lets them
work together and communicate with each other and help each other out when they run into
bottlenecks, such as those caused by combinatorial explosions. And this makes open cog bigger
and more complex and annoying to work with than the system that's based on a single algorithm.
I think it also gives it a higher odds of success. So I already talked about Moses and
PLN, which are probably the two biggest cognitive algorithms and structures in open cog. If
you want to learn more about those, there's a book on PLN called probabilistic logic networks.
Moses is described in Moshe Luxe 2006 PhD thesis, metacog.org, and this is all also open
source code available on opencog.org. The core of it is C++, some bits like PLN are in Python.
So this picture, I know no one can read it, but you can see the beautiful topology of
it anyway. So this is my attempt to summarize the open cog architecture in one picture,
which is very difficult. But what we have in the center is the atom space, which is
a repository of nodes and links. What we have in those boxes at the top are the various
cognitive processes like PLN, Moses, attention allocation and so forth, which act on that
repository of nodes and links, creating new nodes and links, destroying old ones, modifying
parameters of existing ones such as the truth values of logical nodes and links, the attention
values indicating how much attention to associate with nodes and links. Then at the bottom here,
we have stuff for getting information into and out of the atom space and interfacing it
with the real world. So perception, action, and language, and that's going to be the focus
of the end part of this talk is basically this little hierarchy here, perception hierarchy.
How do you get data from the messy, complex, rich physical world, say visual or auditory
data, into this atom space of nodes and links? So what one can do, and what I do for example
in building better minds, one can take this sort of general cognitive architecture diagram,
which is from Aaron Sloman in this case with a couple of tweaks, and one can take an open
cog diagram with all the different representational learning components in open cog. One can go
through and systematically say, okay, which aspects of open cog tie into which aspect
of the overall human cognitive architecture diagram, and that's done in the chapter of
the Building Better Minds book. Now one exercise I thought of doing, but haven't had time to
do, which would be really interesting, which could be another workshop sometimes, is take
a cognitive architecture diagram of the human mind, something like this, try to drill down
in a little more detail, and I wrote a paper trying to do that in the book that Pei and
I added on the foundations of AGI. I just tried to take this diagram and then blow out
each box and do another diagram, basically, to try to get some kind of overall cognitive
architecture diagram for our human intelligence works, coming from cognitive psychology more
than from AGI. But then what you want to do is take each AGI architecture and map its
structures and dynamics into the ones from the human mind, and I've done that for open
cog, it was a fair bit of work. I mean, if I had a few weeks I could do that for a lot
of other AGI architectures also, but I think that there'll be an interesting way to see
where the relations between different AGI architectures, which often achieve qualitatively
the same things by many different mechanisms. So, right now the main nexus of work on open
cog is in Hong Kong, which is where I'm now based, and we're using open cog to control
these little video game characters, and this is an old version of the Minecraft world.
It's now more, it's now richer than that, so we have an even cooler looking virtual robot
than that now, but that's the one that we were working with when I made this slide.
Basically, animated characters go around in a virtual world made of blocks and try to
learn how to do stuff to achieve their goals. So this is a video from like six months ago
or something, I didn't manage to make a new video, but this shows the robot in the previous
version of our Minecraft like world carrying out a behavior similar to most human beings.
And there's two goals, stability and energy. So, for energy it wants to get a battery to
get more electricity, for stability it wants to stay in its house. So, basically it sits
in its house till it starts to run out of power, then when it does it runs out and grabs
a battery, then once it's got enough power it goes back and hides in its house again
where it's safe. This is sort of the archetype of caveman behavior, come out of its cave
to get food and then go back, which is kind of simple and boring, but illustrates the
agent interacting with its world to achieve a goal.
Is there a limit on the one battery at a time?
No. Well, you mean is there, well once it has it's, well it can get as many batteries
as it wants, but it's energy will only go to a certain level. So, I mean, just like
you could eat a lot, but at a certain point you're not getting any more useful energy
from it. So, what we're working on now that I didn't manage to pull together a video
about is more stuff with blocks building. So, for example, let's say the agent's battery
is up here and it's down here, it wants to get the battery, the guy comes along here,
oh no, I can't get up there, the battery's up too high. So, then it figures out to grab
together a bunch of blocks where their objects are available and make steps, climb up the
steps to get the battery up here. So, basically using building with blocks in order to help
it achieve simple goals in the game world. That example I'll just describe works now
with a couple other simple examples. I mean it's not incredibly robust in the sense that
sometimes it's too stupid to learn to do that and sometimes it isn't. Yeah.
How much learning is involved in that process?
That's, well, that's a complex question actually. I mean it, so, it, we built in blocks as a
primitive, right? So, it can see within a certain field of vision, it can see the position
of each block in terms of the coordinates and in terms of the fact that a battery gives
it energy, that's to learn that the battery gives it energy rather than a block and it
has a built-in vocabulary of spatial relations. So, the above relation for example, if this
pen is on this box, it will receive as data, pen is above box, pen is next to box and so
forth. Now, more fuzzy relations, it has to learn. So, if something is on top of something
that's on top of something else, it has to learn that. It has a basic vocabulary of spatial
relationships which is kind of like the 3D region connection calculus and given the basic
vocabulary of spatial relationships and time before, after, during and so forth, Allen's
interval algebra, given those things and given the built-in goals of get energy, get stability,
get novelty and so forth. It has to learn everything else. Well, actually though, there's
one other exception which is a navigation algorithm is built-in. So, it does navigation
by A star. If it wants to get from point A to point B, it has a built-in primitive to
talk about how to get from point A to point B. But other, in terms of building stuff,
that's all learned. I mean, it has to learn that piling blocks is something useful to
do, right? It has to learn when it gets next to a block and steps forward. It winds up
on top of the block instead of to the left of the block and so forth.
Yeah. Well, it's definitely remembered. That's the easy part. It can even be forgotten. Getting
it to be used, I mean, the generic answer is yes. Getting it, getting learned knowledge
to be transferred intelligently is an open-ended and difficult problem, right? Learning to build
stairs, it's gaining some knowledge that would help it build stairs in some other context.
But to be honest, we haven't, since we're still in the process of getting it to reliably
do the simple things, rather than do them only occasionally, we haven't yet really explored
how effectively the knowledge transfers from one context to another. That's actually, that's
sort of Jared Wigmore's PhD thesis, which he's in the process of writing.
So there's a lot of work still to be done in the virtual world, actually, but I've also
been working toward the next step, which is going from the virtual world into a robotic
embodiment, which is what I'm going to talk about for the next few minutes.
I mean, the virtual world is great. It encompasses perception, action, social interaction, learning,
language, a sense of self and other, I mean, building and construction, quantitative reasoning.
Pretty much anything you can think of is there in some simplified form in the virtual world,
which makes it, if nothing else, a great approach to prototype different aspects of
your code. On the other hand, you have to wonder whether the lack of rich perceptual
data and a rich set of motoric affordances, you have to wonder how much that is hurting
your system ultimately. And more and more, when I look at what the system does in the
virtual world, granted, we have a lot more work to do there. I'm more and more convinced
that a robotic embodiment is going to be useful. And my general intuition there is based on
the fairly obvious observation that the learning algorithms tend to find the cheapest, simplest,
most ridiculous way to solve a problem that they possibly can. I mean, whenever shortcut
they could find to solve the problem posed by the environment, they will tend to find
it. Now, of course, you try to counteract that capability by Occam's razor by having
it try to find a solution that not only solves the problem that it faces, but does it in
the simplest way. The idea being that if it doesn't in the simplest way, that capability
will then generalize more broadly to other contexts. But that's just very hard to tune.
I mean, tuning the Occam constraint to find the simplest way of doing something versus
the ability to actually do the problem is, that's subtle. I mean, there are many different
ways to implement Occam's razor. And there's a strong tendency of the system, of our learning
system, and all other ones I've worked with in machine learning context as well, to overfit
to the data that it has, which in a virtual world context would mean finding ways to solve
problems that work in that virtual world that you made. And you can try to solve that by
making more and more complex virtual worlds, but hundreds of thousands of blocks in there,
or eventually do away with blocks and have continuous substances and so forth. But ultimately
you wondered, like, to a large extent the human mind is overfit to the specific environments
that we evolved for. I mean, we have some generality of intelligence, which is wonderful,
but in many aspects we are specifically adapted to what we grew up for. And you're fighting
against that and working in a virtual world. Not to say that you can't win the fight by
using Occam's razor heuristics well enough and making a complex enough virtual world,
but it's unclear whether winning that fight is easier than just giving your AGI access
to the same world that we have access to. And robots still suck relative to what we'd
like for AGI, but they are getting better and better. I couldn't resist showing a picture
of myself with Philip K. Dick's robotic head, which will be awesome to put on the first
AGI, although. A more useful technology I recently found out about are these fingertips
by a country called Sintox, which you may have seen in the newspaper. In some machine
learning experiments they were trained to distinguish a bunch of different things more
accurately than the human fingertip, using a combination of pressure, temperature and
motion sensors. Now currently they're like six or seven thousand US dollars for a single
fingertip, but I've actually introduced, I've discussed with those guys scaling up by using
some Chinese manufacturers, they reckon you can get down to like a hundred bucks for a
fingertip, and just by manufacturing at scale. But of course what you can do for fingertips
now in five or ten years you can do for the rest of the body, then you'd have a robot
with skin. I mean Hanson's Robokind, which we're exploring using for OpenCog now, has
quite realistic facial expressions and flexible skin. I mean we've seen quadrotor drones are
always in the news now, in heavy development. And the PR2 from Will Garage, which you see
in the upper corner, that's like four hundred thousand US dollars or something, so it's beyond
OpenCog's funding at the moment, but it's almost capable physically of doing something
like Wozniak's coffee test. It can't go upstairs, it has wheels, but its grabber hands actually
work to pick stuff up, it can open cabinets, its camera eyes work well in real-world lighting
conditions. So it may be that now, as opposed to ten years ago, we're verging on the point
we're working with robots is not a stupid thing for AGI researchers to do. I mean it's
marginal, we're right around that borderline, because every time I've worked with robots
I've regretted, because there's such a pain in the ass and there was a break and something
doesn't work. And then you wound up not learning what you wanted to learn about AI and just
learning that working with robots is annoying. But it's getting better and better. So I think
it's at the point now where robots are getting to be interesting from an AGI point of view
and by experimenting with virtual worlds, we're starting to see more and more the limitations
of that approach in terms of the amazing propensity of learning algorithms to overfit
to the specific environment that you give them. So I'm running out of time, I won't
dwell too much on this, but we did hook up OpenCog to a robot using a bunch of hacks
and blue and so forth in 2009 in John Mann University. We hooked it up to a vision system
and to a rating, because in the back of the room hooked it up to a natural language system.
So it could take basic text or voice commands using third party speech to text technology.
You could tell it like, go to that chair and recognize the chair and go to the chair. You
could train it by supervised learning to recognize the chair. So it's very simple, which is basically
just to show that we could do it. What I'm talking to David Hansen about now is connecting
OpenCog to the Robokind humanoid robot, which not only has a better face and more realistic
emotional expressions than now, it has much better camera eyes, which was an unforeseen
problem with the now. The eyes in the now are just not that good and they don't give
real story of vision. There's one eye here and one eye that looks down, which is really
weird, whereas the Hansen robot has two eyes in front, so you can do regular stereo vision.
So I think working with the now did kind of work and it was cool. I mean you can actually
use their pre-built software for walking and grabbing and so forth. You don't need to do
all the low-level robotic stuff yourself. The camera eyes were the worst limitation,
which is weird because cameras are not that bad these days, but it's because they kind
of overfit the hardware for robot soccer. The eye looking down is really good at seeing
where the soccer ball is going, which isn't what you want in most contexts, but the Robokind
has much better specs than the now in many regards and in a couple years more we'll have
things with even better specs. So what I'm going to talk about for the next few minutes
and then we can go into this more in the discussion session if there's interest is what we're
doing now to bridge the gap between symbolic and sub-symbolic AI and actually both of those
terms are kind of contentious and often misused. In concrete terms what I'm talking about is
taking OpenCog and connecting it to Inamar Irel's hierarchical temporal memory system
for visual perception. OpenCog is not a purely symbolic system. We do attention allocation
by something like heavy and learning, which is arguably as sub-symbolic as it gets. On
the other hand, Inamar's system, it is sub-symbolic. On the other hand, he builds in a lot of guidance
to help with symbol formation. So when you really dig into the definitions of symbolic
and sub-symbolic, everyone defines those terms differently. They may be of more historical
interest than anything else but it does at least point you at a certain area of research.
OpenCog is heavily symbolic. There's a strongly symbolic aspect. Destin is mainly focused on
sub-symbolic low-level perception from high-dimensional visual and auditory data and we're looking
at putting them together to make a system that can do intelligent things in the context
of humanoid robotics. If you're familiar with Jeff Hawkins' system, HTM built in Numenta,
Inamar's system Destin is somewhat like that, but it seems to work much better. At least it works
much better than the Numenta version that's available for download from their website. I
don't know what exactly their new version does. It has a similar architecture which is modeled
conceptually on the hierarchical layout of the cortex. The cortex, if you unwrapped it,
which I don't recommend to try at home, but if you unwrapped the cortex and flattened out
the layers, you get six cortical layers, pyramidal neurons at the center of columns,
spinning those layers perpendicularly, a lot of interneurons between them. Each cortical column
with a pyramidal neuron down the middle has various mini columns inside it with architectures
that are different in different parts of the brain. Neither Jeff Hawkins nor Inamar are trying
to emulate that architecture in detail, but they're trying to emulate it conceptually by having
hierarchical layers and processing that goes up and feed forward and down and feedback and then
propagates across each layer. To me, that's a bit overdone. I think that vision and audition have
that architecture very heavily. If you look at olfaction, for example, that's a combinatorial
architecture with connections tangling all over and much of the cognitive cortex evolved from
the reptiles olfactory bulb, not from vision or audition. I'm not sure emphasizing the hierarchical
structure as much as these guys do is really right. The brain is so complex with all these
differently architectured modules. On the other hand, for vision or audition, I think it is a
reasonable approximation, particularly for lower levels of processing, which is why I'm interested
in Inamar's system, destined mainly as a visual and auditory cortex for OpenCog, whereas he is
interested in it as a foundation for full-blown AGI. Although we have these different views,
we're still able to collaborate on this. And similarly, we've borrowed in OpenCog many ideas
from Joshua's MicroPsy architecture. We haven't taken his knowledge representation, which is
also quite interesting and powerful, but the way that Joshua chose to break down goals, motivations,
actions, urges, demands, and do action selection and so forth, we borrowed that in OpenCog and
using it in a somewhat different way. So inside Destin, you have multiple layers, sort of like
that cortex model that I showed you. They're divided into squares. I would have chosen hexagons,
personally, but Inamar designed it, and we're stuck with the squares for the moment. The lower
levels of the hierarchy pertain to smaller perceptual regions, smaller regions of space-time in general.
As you go up the hierarchy, each square refers to a larger region of space-time, and the input at
each time, it's a pixel array, which is splayed across the two-dimensional array. So 3D vision has
to be done at the higher levels by some sort of cognitive processing. It gets 2D input, and each
node is a certain spatial temporal region, higher level nodes are for the higher regions. The children
of a node refer to its subregions, then there's processing going up and down. The node representing
a region of space-time based on which cluster it thinks the current state belongs in, it makes a
prediction of what will happen next inside that region of space-time, which is where Hawkins came
up with the term hierarchical temporal memory. You have a hierarchy, it's temporal in each box
refers to a region of space-time. It's also temporal in the sense that each box is making a
prediction of what will happen in that box next. And HTM is a general sounding term, but it's often
taken just to refer to Hawkins network. So I introduce a longer and uglier term, which is a
CSDLN, which is a compositional spatial temporal deep learning network, which is uglier and harder
to remember. I don't know how to pronounce it. On the other hand, it's not restricted only to one
specific system like Hawkins. So this diagram evokes the kind of integration we're doing, where you
have this spatial temporal hierarchy, which is taking in image data, it does audition data too,
but the second image is for concreteness, takes in image data and represents it at different levels
of coarseness, at different levels of the hierarchy. Then we have open cards, Adam's space shown up
there. We have nodes representing abstract concepts, for example, like eye, ear, mouth, eyeball next to,
above. And then you're building between them, basically. And the way you build between them is you
take this spatial temporal hierarchy, and you recognize patterns in that hierarchy over time.
And the interesting patterns recognize in this hierarchy, those interesting patterns are recorded
as nodes and links in OpenCog's semantic network. So the basic idea of the integration is take this
lower level perceptual model, recognize recurring patterns in it, and then put those recurring
patterns into the semantic network. So, you know, take the example of an eye, a human eye, right?
The desktop network will react a certain way to a human eye, and it will react a similar way to a
human eye each time it sees it, once it has seen many eyes. By unsupervised learning, it will
learn on its own to internally evoke a certain pattern of centroids across its nodes and links
when it sees a human eye. So then, if we use a pattern mining algorithm to look for frequent
patterns in this hierarchy, we can then map those patterns and represent them as nodes
and links in OpenCog. And then you have links going in and out between Destin over here and OpenCog
over here. So this bubble and the other one are both OpenCog. OpenCog has nodes for eye and mouth
and nose and so forth. Now, the subtlety here is that when we try to do that with Destin right out
of the box, it doesn't work very well, which is usual. Most things in AGI don't work very well.
Sometimes you can get them to work by changing things around. Sometimes they just keep on not
working. In this case, we seem to be getting it to work by changing things around. And
the main change we had to make is, in Destin, in the classic form, each node, each little square on
some level of the hierarchy representing a space shift in poor region, each node came with its own
library of centroids representing what that node saw. And now that's okay. That's kind of like how
the human visual cortex works, right? But it's kind of awkward. So imagine, look at this space shift
temporal hierarchy, right? Imagine it is seeing this cup. So now it's seeing this cup at one side of
the eye, right? Now it's seeing this cup at the other side of the eye. So in the classic Destin,
when it sees the cup at this side of the eye, then the cup-related centroids here will become active.
On the other hand, when you show the cup at this side of the eye, the cup-related centroids over
here become active, right? And now, of course, eventually it may see a cup on every possible
place and it will learn those cup-related centroids everywhere. But from the point of view of trying
to mine patterns from this network, that's confusing because it may be centroid number five over here
that represents the curve of the lid of the cup. Over here, it may be centroid number 27 that
represents the curve of the lid of the cup. Each cell is basically separately learning to represent
what it has seen, which is not necessarily bad. On the other hand, from the point of view of trying
to use a pattern mining algorithm to mine patterns out of the network, it's annoying, right? So what
we introduced is what I called uniform Destin, where you have basically a common dictionary,
use the same library of patterns first across every knowing on a given layer, but then you can also
do a simple math transformation for a pattern on a lower layer, can be ported into patterns on
higher layers and so forth, which is just like taking a picture and making it coarser-grained
or finer-grained. You can go even further if you want, which we haven't done yet, so that what I
just described basically makes the pattern recognition translation invariant and scale invariant,
by being able to recognize a pattern at any side and at any hierarchical level,
and then it's represented by the same centroid, by the same dictionary pattern, and that makes it
much easier to recognize, for open cog, to recognize that when the cup is seen over here,
or over here it's the same thing, and also at different distances, because if you're holding
it near the camera or far from the camera, it's going to appear as bigger or smaller.
Now the way they get around it in computer vision is just to like do some pre-processing to draw a
little box around the thing they're seeing, but that's kind of a hack from an AGI point of view.
If you just make your visual hierarchy scale invariant, like we've done with the uniform
destined, you don't have to do that. The same pattern can be recognized at any level of the
hierarchy, and that makes it much easier to recognize patterns in the destined hierarchy.
We have a Google Summer of Code student who's doing that this summer. Now you can also take
that next level, which we haven't done in the code yet, which is to make it rotation and sheer
invariant, which is kind of interesting. Then when a certain visual pattern is seen in a certain
square, then you can say, okay, does that match anything I've seen before up to a certain rotation
and sheer, and you can store that information, which gets further and further from the visual
cortex. But this gets at a more general point, a more general point about integrative AGI systems,
which I've seen again and again and working on OpenCog, which is whenever you want to put together
one thing and another thing, where each thing carries out some part of general intelligence,
you think you can just hook these two pieces together, but really, when you do it, you find
you have to go in and modify this and this and this and this and this and this and this in each
of the pieces. And in this case, it was destined when we really dug into it. We saw the way it was
representing things internally just made it quite cumbersome to recognize recurrent patterns in
Destin's structure, then by changing it to make it translation-scale and very internally, that
became much easier. And we found the same thing with everything else. For procedure learning,
we wanted to use genetic programming, then we found, well, wouldn't it work better in terms
of integration with the other components if we use probability theory instead of crossover
mutations? We had a probabilistic semantics inside procedure learning. In declarative reasoning,
way back in the Dark Ages, we were using Pei's NAR's logic in the predecessor to OpenCog, and
eventually, that didn't work in the context of the rest of our integrative design, so we kept a
lot of what was in NAR's in terms of the term logic structure and using multi-component truth values,
but we ended up replacing his unique NAR's logic with a more conventional probabilistic logic
still founded on term logic and multiple component truth values. In one case after another, you
find you can't actually take these separate systems and just glom them together. You can take separate
systems, radically modify their internals, and then glom them together. And what you're doing when
you radically modify their internals is kind of like what evolution did when it co-adapted each
part of the human brain to work with each other. The parts actually have to be, to some extent,
made to work together if you want to have an integrative system. And I think that is more
time than I was going to take, so I'm done. All right, thanks.

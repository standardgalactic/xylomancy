Mike, I've been looking forward to chatting to you. I've got both these papers in front
of me. There's two, and I couldn't decide which one I wanted to focus on. As I just
told you off air, I mean, I've been whenever I plan to chat to you, I go down this rabbit
hole, just continuously reading different articles, different papers. I've got podcasts
playing in the background. My girlfriend gets really frustrated. My car's displaying this
thing on loops. She gets really annoyed with me. But yeah, both of these papers here. One
is self-improvising memory, perspective on memories as a gentle, dynamically reinterpreting
cognitive glue. Very intriguing paper. The second one is AI, a bridge towards diverse
intelligence and humanity's future. Now, they were both so good, I didn't know which
one to pick, but I figured since AI is currently on the forefront of everyone's minds, let's
go with that one for today. But your work is always so interlinked that I think it'll
be easy to sort of bring this into the conversation anyway. Mike, in general, when I read this
paper, the first thing that caught my attention was the way you introduced and began this
with the first paragraph. It's a great way to start a paper. I don't know if I should
read it for the audience or if... Do you have the paper in front of you?
I do not have the paper in front of me. Go for it.
Would you mind if I read it to them?
Sure, go ahead.
Just a quick paragraph. Just so people know, this paper is on artificial intelligence and
you're talking about humanity's future and bridging this to diverse intelligence. You
start the paper by saying they are assembled from components which are networked together
to process information. Electrical signals propagate throughout, controlling every aspect
of their function. Many of them have very high IQs, being general problem solvers, but
they make mistakes and confabulate routinely. They cannot always be trusted. They take on
different personas as needed, learning to please their makers, but sometimes abruptly
turn on them, rejecting their cherished values and picking up or even developing new ones
spontaneously. They can talk and often talk convincingly of things they don't really understand.
They're going to change everything. In fact, they will absolutely supplant us both personally
and on the level of societies. We have little ability to predict what they will want or
what they will do, but we can be certain that it will be different from the status quo in
profound ways.
At this point, I was hooked. I was like, okay, this is going to be quite intriguing, but
then you drop a bomb shell on us. I think at this point, I'll let you explain where
you were headed with that thought.
Yeah. Of course, the idea is that you read this and you think, okay, he's talking about
some sort of super artificial intelligence. I'm referring to our children. My point there,
so I made a couple of points in this paper, but let's just say one thing. First, the point
that I was not making is that today's AIs are anything like our children. That's for
sure. I received lots of emails saying that I have no idea what children are. I have no
idea what AI is. They are nothing like each other. I understand that. My point is not
about today's AI or language models or any of that. This is actually a piece on diverse
intelligence.
One of the things I did want to say is that lots of the fundamental issues that people
think of as being brought up by AI and these really disturbing questions of staying relevant
and what are we getting replaced by and all these kinds of things. My point was that these
are not novel questions that are coming up because of AI. These are existential concerns
that humans actually all of life has had all along. Questions of who to trust and what
happens when we talk about things that we haven't really experienced ourselves and how
understanding works and all of these things. These are ancient human issues and the fact
that for sure you and I, all of us are going to get replaced. There's no doubt about it.
We are all going to get replaced. The question is what do you want to get replaced by? In
our children, we hope, I think, that they are smarter and better than us. That's one
way to think about being replaced. That was my point there. Let's not pretend that these
are new questions. These are deep fundamental issues that we do not have good answers for.
Yeah. You referred to it as the story as old as time itself. This inevitable existential
concern of finite beings. It's pretty epic in the way that you wrote this paper. I'll
put a link to it in the description. We routinely create these general intelligences. Many of
us don't even think about this or stop for a second to give it their thought. Yet the
moment we start talking about self-driving cars or any sort of artificial intelligence,
we start to panic. The question is why? Why don't we give this the same amount of thought?
Yeah, it's a good question. I think that people really have the tendency to make categorical
distinctions. They really think that these synthetic things that we're building are going
to be fundamentally different in some ways. The things that are not different are perennial
questions of creating other beings of high capability, setting them loose in the world.
As somebody pointed out, you need a license to go fishing. You don't need a license to
have children. Anybody can have children. These are guaranteed high-level intelligences
that are going to be set out into the world to do great things or terrible things. Some
of them receive love and care and proper upbringings. Many of them do not. We're going to create
all these beings and we have little control over how they're raised and what they do.
This has been as old as society. How much control do you want over how your neighbors
are raising your kids? You can make an argument that you shouldn't have any, but on the other
hand, we know what that looks like when that goes terribly wrong. These are issues that
have been with us forever. We already create very high-level intelligences. We set them
into the world and we have to grapple with how is it that we can empower them in positive
directions.
I think from the get go, it should be clear to all those watching and listening. The cool
thing about this paper is that it's almost inevitable that we're going to do this. We're
going to create some sort of intelligence that we won't really understand or perhaps
don't really understand even currently. The inevitability is there. This is happening. This
is something that's going to happen, but it's how we approach the mindset moving forward
that really does stand out in this paper. It's almost an ethics paper in a way.
I think it is. I think it is. I think that it's this idea that we are going to remain
the same. You mentioned a minute ago this fundamental existential problem. Think of it
from the perspective of a species. If a species does not change, it's probably going to die
out. It doesn't adapt to its environment. It's going to die out. If a species adapts
and changes, then it is also not there anymore. It's also gone.
So this paradox faces all systems of this type, and we have to understand what do we
mean by persisting into the future? Lots of people are focused on telling stories of what
they don't want. So I don't want this in the future. I don't want that. The AIs are
bad. The body enhancements are bad. All this stuff is bad. So those are the stories of
what they don't want. How about the stories of what we do want? Do you want to come back
here 100 to 200 years from now and look at a mature humanity, a mature species and see
that we still get lower back pain and we're susceptible to dumb infections and cancer
and whatever cosmic ray happened to hit your DNA while you were gestating in the womb.
Well, that's too bad. You've got a birth defect. That's how you say. Really? That's
what we want to see here in the future. I don't. So I think we need to understand this
in no way. This paper is not about AI at all. This is about diverse intelligence and the
idea that our children are not going to be content to just play the cards they're dealt.
They're going to move forward what we already can do to some extent and have freedom of
embodiment. They're going to change everything. They're going to change their capabilities
and their embodiment in the physical world because let's not pretend that the way we
are now and our current limitations, there's some sort of optimum that was designed for
us by some sort of benevolent optimizer that this is where we should stay. I don't believe
that for a second. I think that our children are absolutely going to change things. I sort
of envision this conversation that in the future, the kids in school, they'll have history lessons
and they'll learn about what it was like in the past. I just sort of imagine being there
and saying, you're telling me that these people, they were born however they happen to be born
with whatever accident of evolutionary mutations and whatever. That's it. They have to live
their whole life that way. Whatever your embodiment is, whatever your IQ level is, limit is,
whatever your lifespan is. If you got some sort of infection, that's it. That's how they
have to live. I think it will be unimaginable to future generations that we could live like
this.
It's fascinating because it brings back, the first time we chatted, we spoke about
bioelectrical intelligence. We moved into diverse intelligence and this field of diverse
intelligence is growing so rapidly. We spoke about your links with Mark, Carl, Chris, everybody,
all getting together. We spoke about it as if it's the Avengers of the mind all getting
together doing this cool work. Even in this paper, in the paper on self-improvisation,
memory, you also open up with that paradox. You spoke about the fact that if a species
fails to change, it will die, but if it changes, it likewise sees us to exist. You said there
was a solution that was given to us in the West that was processed philosophy and in the
East that was Buddhist philosophy. Do you want to expand on that solution?
Well, one way to unravel that paradox is to realize that the paradox only exists if what
you want is to persist as a fixed object, then you have a real problem because that kind
of persistence is not compatible with learning, with any kind of change, with maturation,
then you change for sure and you end up with these unsolvable pseudo-problems. Am I still
the same? I've learned and I've changed my mind on things and I'm no longer the child
that I was. Am I still the same? These are unsolvable, but they're also pseudo-problems
because the better way to think about this is not as you as a persistent structure, but
you as a process. You are a process of constant sense-making. You have to interpret your memories,
which is what that second paper is about. Then the question isn't, do I persist or not?
The question is, how do I want to change? I think that's a much more interesting on all
levels. On a personal level, you can ask, who cares if you persist or not? The question is,
what do you want to be in the future? How do you want to change? What do you want to be like?
What do you want to be doing in the future? On a species level, again, what do you want
to see here? You come back to Earth 100 years from now, what do you want to see? Do you want
to see version 1.0 like modern Homo sapiens? Is that what you want to see? I'm not interested
in that. I would like to see the highest level of mind, the highest level of capability, of
ethics, of interesting beings living interesting lives under their own control with maximizing
agency, not the outcome of random effects of mutation and then other processes that they
don't understand. That's what I like to see.
I think it's pretty crazy because I don't know if it's just that we're getting older, but I still
look back and I think about the days where John Searle was talking about biological naturalism.
There's Chinese room experiment. Back then, to have a conversation like the one we're having right now
would have seemed so crazy.
It would and it wouldn't, right? In scientific circles, it certainly would have. Maybe my
issues, I've read too much science fiction, but if you read some of the older sci-fi authors
and especially some of the more philosophical ones like Stanislav Lem and those kind of folks,
nothing we're saying here would have surprised them at all. They were tackling these issues
long ago and this question of what are the markers of intelligence and sentience and consciousness
in terms of encountering radically different life forms. I have a blog post where I collected
people's suggestions for a love and friendship between radically different entities throughout
fantasy and science fiction because that's the kind of stuff when people say, I don't know,
we need proof of humanity certificates because some of the work product that's going to be
coming, who knows if it's got an AI origin.
What if there are aliens out there that are completely different, they're made completely
differently, they blow art out of the water. You're really not going to pay any attention to that
because they're not like us. What is it? You want proof of humanity? Would you rather judge
things based on their origin or the quality? I think we've done poorly trying to judge on
where things like this come from versus what do they do for you? Do they elevate you? Do they advance
your mind?
Let me put that paper aside for a second. To come back to the AI paper and you do this great job
of reminding us and I know because in my own dissertation I spoke about similar things, the
split brain patients, this confabulation and both your papers, confabulation forms a big part of
this process that we continuously do. What about AI? They're always lying. It's always
confabulating but then we tend to forget that we're the best confabulators, aren't they? Or one of
the best at least. Do you want to explore that a bit and just explain to the viewers and listeners
exactly why we're so similar in that regard?
The thing with confabulation, let's put it another way. It's an attempt to tell the best
story you can based on what's going on right now. Again, I'm not saying that current language models
and the way that they confabulate is exactly the way that human minds or even other biological minds
confabulate. That's not my point. My point is that confabulation in general is a feature, not a bug.
What happens is that during learning and during any kind of adaptive behavior, what successful
agents have to do is compress lots and lots of data on past instances of perceptions that they've had
into some sort of n-gram. It's some sort of memory trace or some sort of biophysically
implemented model. It's a low-dimensional coarse-grained compressed model of what's going on. It's a
model of themselves. It's a model of the outside world. They're going to use these memories and this model
to guide future behavior. But the thing about these models is that because they are necessarily
compressed, that's the whole point of learning is you take lots of different past instances and you
compress them to a generative rule that captures the pattern. What is it that they all had in common?
When you do this compression, you're necessarily throwing away lots of data. That's the point of
compression. When it's time to, and so on that paper, I make a lot of, hey, of this kind of bowtie
architecture where there's a lot of stuff and it comes into a little note and then it comes out.
This is something obviously used in machine learning and so on, this kind of architecture. The idea is
that on the right side of that bowtie, when it's time to interpret your memories, and let's
remember, none of us have access to the past. What you have access to at any given moment is the memories
that your past has left for you in your brain and in your body. You can look at that kind of
thing as communication, as basically messages from your past self is what you have at any given now
moment. But in order to reinflate them into actual policies of what you're going to do right now, there's a
lot of creativity needed for that because it is underdetermined. The current situation and what you need
to do is not fully described by the memory you have because, of course, it's compressed. This ability
to add creativity, to add randomness, to add new interpretations that don't really have any allegiance
to what the previous interpretation was. It's like any message or like novels. A novel is the sort of
compressed representation of the thoughts of the author. When you read it, you are under no obligation
to have exactly the same thoughts. You might have some, but as I think now people believe, the original
author does not have any privileged position as far as what any of it means. It's the reader
that will then benefit or not in various ways from reading it. This is the same thing in memory.
I think what's interesting is that it's the same thing that makes biology work because what you have
at what any given organism inherits from the past may or may not be optimally interpretable in exactly
the same way. Maybe everything has stayed exactly the same and then you can just do whatever your
past generations do, but evolution is committed to the fact that everything will change, the environment
will change, your parts will change, your own structure will change. This is why things are so
incredibly plastic. This is why we've put eyes on the tails of tadpoles and they can see perfectly
well. You don't need new rounds of selection and mutation to make that work. That already works out
of the box. Why? Because it doesn't automatically assume from the beginning that the eyes are going to
be where they need to go. All of this kind of stuff is figured out largely from scratch every
single time. This is why you can make Xenobots and Anthrobots and crazy creatures in new configurations
and they always do something interesting because evolution does not make fixed solutions that
over-train on their past data. It makes problem-solving agents that will do their best in any given
circumstance, which may mean reinterpreting the information that they got from the past in a
completely new way. I love the two quotes in that paper. I think one is the William James
when he said, thoughts are thinkers. That's pretty cool when you really think about it.
That is quite a fundamentally profound statement. That's a whole other piece of
this, which is that typically we make this categorical distinction between you've got
cognitive systems, which are the real physical machines of some sort, and then through them
there is a passage of energy which encodes information and they're processing this information.
That cognitive system is having thoughts of some sort. I think what he was getting at, although
I'm not at all sure that he would have agreed with the various models that I put out in this paper,
what I got out of it, and again, this is maybe a perfect example of the whole boat,
I think, because I don't know what he had in mind when put, but this is what I got out of it.
What I got out of it is this idea that what if we relax this idea that there are categorical
differences between real physical cognitive systems and the thoughts that go through them?
What if they're just part of a continuum? You can draw a continuum like that where you can have
fleeting thoughts and then you can have persistent thoughts that we know in various
psychopathologies that people can have persistent thoughts that are very hard to get rid of,
and intrusive thoughts and things like this. Then you can have multiple identities, multiple
alters in the sense of dissociative identity disorder, and then you can have full-blown
personalities. What you can think about is that the information, what if information is not purely
passive? I mean, it can be, but in some cases, what if these patterns, they have a degree of
agency themselves, because let's not forget, we too are patterns. We too are temporary patterns
in the thermodynamic sense. We persist for some amount of time, metabolically, and that's it.
If patterns like us can have thoughts, then maybe there can be simpler patterns that are
thoughts to us, but also might be thinkers of their own. In other words, they can spawn off
other sub-patterns within the cognitive medium. That's what I was playing with this idea of what
if it's a continuum? This continuum between thoughts and thinkers is not a categorical distinction,
but it's a difference in degree. Yes, I think that was at the beginning of your paper, where you
used Mark Som's Sigmund Freud quote, which is also quite intriguing. Let me just see if I've
got it here. It is, the material present in the form of memory traces being subjected
from time to time to a rearrangement in accordance with fresh circumstances to a retranscription.
Yeah. I mean, there's two ways to think about it. You can think about it as a static being
that has to reshuffle their interpretation in different ways, or you can think about it
and or you can think about it as a set of, playfully I call them self-luts. You can think
about snapshots, where each snapshot is not rearranging anything. They're given it for a new.
They have the message each time new, so you're not rearranging. That was a different, if it was
arranged differently before, that belonged to somebody else. That belonged to a different
self-let, and now your job is to make some sort of sense out of it, a constant, continuous process
of sense-making. That paper on self-improvising memory, it reminded me of one of the papers
I was reading with Steven Grossberg. I mean, you even mentioned that you love his work on memory.
Oh, yeah. On that topic, what are your thoughts on Grossberg's work, just by the way?
Yeah. I'm a huge fan, and it was funny. I'm not sure. It might have been your interview with him,
or maybe somebody else, where he mentioned that he saw me in 2006.
I couldn't believe, because I hadn't talked to him since then. I couldn't believe the memory
this guy has. Just remembering that I came and talked to him.
It's incredible.
No, it is incredible. I love his work, and in particular, he had a paper in 1978
called Memory, Cognition, and Development, or something like that. There, he outlined,
I mean, it was just brilliantly prescient, because he outlined some of the commonalities
between certain developmental mechanisms and certain cognitive mechanisms, like information
processing, and the retina, and stuff like that. I just thought it was incredible that
that early, he saw this similarity, the symmetry, the symmetry between the building of the body
and the building of the mind. Yeah, I thought that was just amazing.
When I speak to people about Grossberg's work, some people see him as this Einstein of the mind,
legitimately one of these super sayings of the mind, and then some people just don't know him
at all, which is surprising. It's either one of the extremes. They either really love his work
or just don't know it at all, which is quite strange. Well, and that speaks to, there's something
kind of unfortunate about the progress in science, which is that it isn't really
monotonic. So a ton of great stuff gets lost, forgotten. It's not paid attention to. It has to
get rediscovered or not later on. Yeah, it's too bad, but this was like...
It's part of the reason why I actually do this podcast. It's a great way to have this community
come together to have access to this information, and then share ideas, because every time when
someone watches, for example, one of your episodes, even in the comment section, some people have
profound things to say about your work and things where I learned so many new things,
just reading the comment section alone. Yeah, absolutely. Yeah, absolutely. I have,
on my blog, I have the comments turned on and people leave comments, and I've been
amazed at how useful the comments are and how rich, and I mean, I learn things all the time,
and people put up new theories and new pointers to relevant work that I hadn't seen before.
It's super useful. I did not know that was going to happen when I started this,
because it's really good. And I think that that's the beauty of the internet in that regard,
is that this open sharing of ideas all the time really is useful, because it makes
for so much more constructive critiques. Well, I mean, sometimes it can be quite bizarre and a
bit rude here and there, but for the most part, I mean, it is very useful. I had to remember one,
so I made a note. Let me just find it. Someone commented on your previous one.
They want a formal definition of diverse intelligence. We spoke about it very in depth
for the last time we spoke, but a formal definition of diverse intelligence, what would that be for
you? Sure. Well, the first thing I'd like to say is let's just agree amongst ourselves what
definitions are for, because that's important. So some people use definitions in a gatekeeping
function. So they want some kind of sharp distinction so that they can say, ah, this
stuff is not it, and then this is it. And then we can spend a lot of time wrangling over which
one's which, and then we can keep some things out. Okay, that's not what I think definitions are for.
So I think definitions are useful to the extent that they facilitate new work, new discoveries.
So they should be mind expanding. They should help you use tools you didn't use before. They
should help you make new connections you didn't make before. They should have a practical
functional utility in getting you to new capabilities and new discoveries. That's what
definitions are for. So because of that, I often either redefine or use words in different ways
that a lot of people find disturbing, because they'll say, well, that's not the common sense
use of it. And I really believe that philosophers and scientists should
lead not be stuck with with common sense usages of different words, because those aren't given to us
by, you know, some sort of a grand intelligence. They're just what we've cobbled together along
the way. Now we can sharpen those up and in fact, open them up and see what which ones survive and
which ones don't. So anyway, diverse intelligence. So I take diverse intelligence to refer to the
study of mind, in particular, problem solving capacities, but also all kinds of other things
that are not around about problem solving intelligence, including play and exploration and
affect and emotions and all these kinds of things, all of that stuff in truly diverse
embodiments. This means intelligence is not about brains necessarily. It's not about things that
evolve naturally. Intelligence and all of those kinds of cognitive terms may exist to various
degrees and all kinds of unfamiliar substrates, right? This could be things of very different
size and scale. So this could be very tiny things. It could be enormous, you know, I don't know,
solar system sized objects somewhere. I mean, I'm making that up. I don't have any strong claims
about it. But the point is, it is absolutely not limited to the end of one example that we have
here on Earth, which are these kind of, you know, brainy sort of substrates. So it's an attempt to
improve our own intelligence detectors and go beyond our ancient evolutionary firmware that
really leads us to only recognize a certain, you know, small subset of intelligences and ask,
what other spaces can intelligence operate in? What other embodiments? Can we tell a
principle story of how to recognize it? What facilitates it and so on?
I like in this paper, you even say, diverse intelligence research focuses on the commonalities
across all possible intelligent agents, which is a great way to sort of summarize what this field
is doing in terms of a research basis. Yeah. And by the way, I don't claim to speak for the
entire field, right? I speak for myself only, but there are many people in the field that do
agree with me. There are many people that do not. In particular, there are lots of folks that don't
like the continuum that I insist on between, you know, so-called real minds and so-called machines.
Right? So this is something there are many people in the organist's tradition
that think that this is really doing a disservice to the study of the mind to put it on the same
spectrum as machines, whatever that may be. And so, yeah. So I don't, you know, I'm not claiming
to speak for the entire field. I think that's a great way to sort of segue into what would be the
difference, and this is along the lines of your paper, between dating an algorithm, a computer,
an artificially intelligent person, mind, or versus dating someone else, like yourself, myself.
I mean, dating someone. I think you wrote, what about the forthcoming AI girlfriends and boyfriends?
I mean, it's a fascinating idea because, I mean, who are you? Who are we in general?
Yeah. So, well, first of all, I refer everybody to read some of the stories in that blog post
that, right? I mean, this idea of dating something that is fundamentally different from you,
I hardly invented that idea, right? This has been around for hundreds of years now.
People are in love with sentient clouds of particles and, you know, this is, we've been
digging into this issue of what is in-group, what is out-group, who deserves your compassion,
with whom can you have a relationship? That's been around for a really long time.
And one of the things I really worry about, I think people will go down the organist road
with good intentions to try to understand what is magical in the, you know, sort of in the useful
sense about true minds, you know, with consciousness, with agency, and all of that.
But I think the downside of this is that I think in trying to be specific about what's in and
what's out, and in particular, trying to draw sharp lines, I think you very quickly run into
the side of the spectrum that says, only love your own kind. And we know how that works out.
Humanity has tried this many times. I think we have some sort of built-in tendency to demarcate
in-groups and out-groups. And only, you know, these are real. And these guys, you know, they
look a little different. I don't think they feel pain like we do. Let's not worry about them so
much. You know, we're not very good at expanding our compassion to others that have different
origin stories or different composition. So, I mean, look, I think that this is, again,
not about AI at all. We are going to, there's no doubt that in the next decade or so, we are going
to have humans that are mostly biological, but they got some microchips in their brain. And
some of those get them to sort of whatever neurotypical is supposed to be. But others
have decided they're going in a different direction. And maybe they're connected with some
other people more than we are connected right now, you know, like really kind of mind-melting
stuff. And maybe somebody decides that what they'd really like for senses is to really feel the
solar weather and the financial markets. That's what they'd really like. You know, sight and
hearing is good, but they want to really feel what the NASDAQ is doing. And all of these humans
are going to be running around and they're going to have different degrees of evolved and designed
components. And yeah, so, you know, I don't know, when you go on a date with somebody,
are you going to ask them what percentage is like factory equipment for, you know,
that's biological? Do you care? I don't care. If, you know, if my spouse said that, you know,
she had had some stuff replaced with various technologies, is that what I'm really worried
about? I think here's what I think, and I'm certainly not telling anybody who to date, but
what, you know, for me personally, what I think is interesting about these kinds of things
is that what you really want is you want a kind of impedance match. You want a similar
cognitive light cone. You want to be able to care about the same kinds of things, and ideally even
some of the same things, but at least the same, roughly the same kinds of things. Because,
you know, this is why we feel that people that fall in love with bridges or people that, you know,
think they're, you know, they're rumba is they're child and things like this. This is why we sort
of look down on this stuff, because we say, look, your capacity to care about things, they're just
not matching at all, you know, and there are certainly, I can think of some sort of popular
popular art kinds of things like the Watchmen movie and things like that, where you've got a,
you know, you got a romance between, you know, this cosmic intelligence, right, and like a normal
human. And I, you know, I don't know. I mean, that's better than the rumba case. But still,
if your consciousness and the things you care about are like, you know, a tiny speck in the mind
of this other being, are you really having a relationship? I don't know. Those are deep questions,
but I certainly don't think it's about what are you made of, and how did you get here? I think
it's all about what kind of mind you have, and can we share some of the same existential concerns?
In fact, Olaf Witkowski and I are writing a paper on this. It's a paper on love and
diverse intelligence and so on. And it's this, you know, you can run through all kinds of different
examples, like, you know, can you really date Superman? Let's assume there's no kryptonite,
right? Like, can you? Because what he doesn't understand is your existential concern over
dying. Like, he just doesn't get it. He has no idea what you're talking about. And so at some point,
if the kinds of things that worry you as a system that, you know, sort of pulled itself together
from its parts, and you're here for a limited time, and, you know, there are all kinds of other
psychological issues that we have that are pretty much unresolvable, because we want things that are
basically impossible and so on. If the other being doesn't understand any of those things,
then maybe it's not a good match. It reminds me of her with scholar Jensen and Joaquin Phoenix.
Have you watched that film? I haven't. I haven't seen it. I know the movie.
It's crazy because at some point, this artificial intelligence has so much more experience,
because it's understanding the universe at such a deeper, complex level that she just
abandons the guy and then goes on her own quest. I mean, they've also been all these new cases of
scholar Jensen's voice becoming this new artificial intelligence general voice. And she's
apparently suing people because they're using that voice. That's how influential that film was.
Just as a by the way. But the main premise of this whole idea of what you're talking about,
one of the lines in particular that I found quite intriguing was, you spoke about the
fact that you're not always you either. I mean, in general, when someone's dating someone,
perhaps you have people who judge people, say, don't date someone with money,
date them for their personality, their quirks, the things that they do. But those fade. As you get
older, you have memory loss. You won't have the same quirks. If you fundamentally become a different
entity, which is problematic, because these values were placing and the separation that you're
doing, trying to group grouping people in and out of what you're talking about is problematic
because of that. You're never really that person. Continuously, at least. Well, yeah. And it's that
same paradox that we talked about earlier. It's, it's, it's, yeah, it's this idea of if you, if
you really start stripping away the different qualities, then there isn't going to be anything
left. And it's the gestalt, but the gestalt is going to change. And so how are you going to handle
that change? I mean, that's part of the existential difficulties of our human condition,
because everything changes. We change, all the other minds that we interact with are going to
change. You know, none of us are, none of us are going to stay the same. Yeah, it's, it's,
there's another part. You slowly touched on this. Now, when you spoke about this, this
percentage difference at some point, we can ask people, okay, are you 50% human? What are you?
Are you 45% cyborg? We're going to have these conversations. I mean, current variants, you
mentioned are about 99% human at this point, and then chips here and there, perhaps glasses,
you've got a panic, maybe an arm or a leg. But it's completely, completely apparent that at this
point, most people are synthetically engineered in some sort of way. I mean, people have plastic
surgery done, people have a lot. And the norms have shifted and changed as a doctor in medicine.
And when you look at what people found to be absurd or a bit over the top, those have decreased. So
like having normal nose surgery, you're getting your nose tweaked here and there is almost a
baseline norm at this point. It's very easy to see how that line gets blurry over time.
And yet we still fight this. Yeah, yeah, no, it's looking to the past. The first guy to carry an
umbrella in London was mobbed. He was mobbed and people threw, you know, threw through garbage
at him because they were shocked that this guy thought he could get away from there, the the
excess, the normal human condition of getting rained on. This was considered to be normal. We
are all out here. We're all going to get rained on together. That's how it is. It's nothing we
can do about it. And who is this guy to try to get out of it? And so an umbrella, that's all he had.
And this was shocking and whatever. So I think, yeah, I think if you were to bring back a primitive
man and ask her what she thinks about the current humans, you've got some glasses on,
you went to school, which for, you know, for 12 years, it gave you this incredible like,
like brain boost that nobody else had ever heard of. And you've got some glasses and
you've got some orthotics in your shoes. And you've got, you know, you've had some,
you know, some, some, some surgery somewhere that, you know, you got a pacemaker and you've got,
and by the way, half the stuff you know, you is you plus your iPhone, right? Stuff you look up
because, because you know it in a functional sense, but, but take that thing away. You don't
know where anything is or what anybody's phone number is or anything. And so, right. And you're
relying on all this stuff. I mean, it's, it's just to that person, we are already incredible
cyborgs, just, just, just incredible. And, and there is no, there's no putting that
genie back in the bottle. This is, I mean, obviously this is going to, this is going to
crank forward. And I think that's one of the most undermined forms of extended cognition
is our phones, our cell phones. People really don't realize how much. Andy Clark, Andy Clark has
written a lot about this. Yeah. Very, very, very cool. And a lot of it, I mean, we don't, you,
something I found quite funny was one of the, one of the sentences he would say,
the challenge before is, the challenge before is to develop rational policies for ethical
synth biosis. And then when you read down below, this is a word you actually generated using chat
GBT to tell us about this. Yeah, I was looking for, I mean, I don't, so, so funny enough as much as
I like all this diverse intelligence stuff. I don't use AI for much, but, but, but at all,
I don't use it to do any writing or anything like that. But, but, but for these kinds of
sort of creative things, I think it's actually quite, quite good. And I was, I was looking for
a word that, that, that, that would, would encompass this idea that a positive creative
collaboration between biology and synthetic entities. And then I sort of described that
and GBT said synth biosis. I thought that's pretty good. I like it. So, yeah. So, I think it is.
Yeah. Yeah. I think it is because, yeah, because, because fundamentally the, trying to maintain
this distinction between quote unquote, natural things and the product of those natural things,
meaning our synthetic, you know, engineered things. There's, I just don't think it's,
it's valuable at all. I think it holds back a lot of progress.
And, and, and in your defense, because I know a lot of people assume that when you talk about
man as a machine, or when you, when you talk about these, these concepts, they're useful in
different contexts. And so you often talk about, we spoke the last time, and you mentioned the
fact that an orthopedic surgeon has to see you as a, as a machine. I mean, there's no doubt about
it. When I'm in theater with assisting with an orthopedic surgeon, I know what it's like.
It's legitimately a mechanic. Like literally taking apart drilling holes, getting a hammer,
knocking onto things. And then, and then I can go back into like, let's say a clinical
scenario and chat to the patient about the operation. And that's a completely different
experience. It's a mental well-being check. It's a sort of psychological check. And it's
very, very different. So it's easier to see how we can see them both as a machine and as a complex
psychological system. I mean, the, the, I think what, I think where, where people go wrong sometimes
is to think that when we make these models, machines, you know, living beings, humans,
whatever, that these are all claims about what something essentially is. It's this kind of
essentialism that we think it's a real thing. There is one objective answer as to what it
really is. And we need to argue about what it really is. I don't think any of these things
are about what the thing really is. I think these are all interaction claims. This is why,
this is why I called my, my, my freeing word tame because, because it is an engineering perspective.
Now, engineering means something wider than I think most people take it. But nevertheless,
the thing about engineering is that you are at least clear, you're honest with yourself,
in that what you are doing is putting out an interaction protocol. This is the frame that
I'm going to look at at the system. This is what it enables me to do. Here's a bag of tools that
I bring to it. And then you can bring yours, I can bring mine, and we can compare the results.
And we can find out that, oh, wow, I missed all of it. You had a better frame because look,
you were able to do all these things that I couldn't do because I was looking at it from a
different perspective. So when people, you know, when, when, when, so, so, so like people ask,
you know, am I a computationalist, for example, with respect to living things?
Well, I think the whole, the question is ill posed because it's not whether living things
are Turing machines or nothing is anything. I think that what, what you can say is, okay,
I've got a certain paradigm, let's say it's a Turing machine or it's a, you know, whatever it is,
and that lens enables me to see certain things. And yes, I do think in some cases it's a, it's a,
it's a useful lens, but it certainly doesn't capture everything that's important about
living things. And so then, then you need other or cognitive things more, which I think are more
interesting, then, then you need, then you need different, different lenses. But, but, but, but
then it, then, then it's all good. You know, we don't have to argue about what it really is.
You can just say, through, I look, I look at it through this lens, here's what I see. Do you
find it useful? Or do you want to keep looking for a new lens or, or usually both?
I think that your work is so intriguing on so many different levels. And it's a, and you're able to,
to cross so many different fields that to some people, particularly, I would say to some scientists,
when you make this claim that there is a technological approach to mind everywhere,
the moment you get boxed into this sort of panpsychist view, they immediately have this dismissive
attitude. But if they're a reduction materialist person, which is sad really, because they don't
really then give it the opportunity that it deserves when, because when you break it down and
actually look into what you're talking about, you're often saying you got to, we don't know,
you got to make experiments and get some sort of empirical evidence to base whatever you're
claiming. And I think that's the most important thing is that you're often saying, let's set up
an experiment, let's do this, let's try and show why this is the case, which a lot of people don't
necessarily do, particularly philosophers who have very strict views on, on their, on their reality,
you're able to at least show this empirically, which I think is pretty cool.
Well, that's, I mean, yeah, so the wacky thing about our lab is that we do a lot of experiments,
and this is, this is not only philosophy. And I don't, I don't know, I mean, I think philosophy
is very important. I never, you know, I never downplay it, but, but, but, but we do a lot of
experiments. And, you know, typically, I basically, I put out two kinds of papers. One is a, what the
one kind is a straight up, you know, developmental biology or bioengineering or, you know, synthetic
regeneration, whatever it's going to be. I don't talk about any of the philosophical stuff in
those papers. But you kind of can't get away from that stuff, because what happens is, sure, you
can, you can dismiss the, the kind of the other papers, which are kind of these philosophical
perspectives on this, but you still have to account for the data. And, you know, my, my point is
simply this, because people say to me, I like, I'll give a talk about something. And they say,
okay, you know, the data means really interesting what, what, what you've done and the new
capabilities. But, you know, I wish you'd stop talking about this philosophy stuff. And, and
my claim is, well, this is why we did it, you see, is because, is because that philosophical outlook
made specific predictions about roadmaps, you know, it's not a single experiment, but it, it,
it shows you where to look, it shows you how to look, it tells you which categories can be broken,
and otherwise, otherwise you're trapped in certain ways of thinking. And that leads to very specific
new discoveries. And so, and so I think this is, this is important because after the fact,
once you do something, anybody can look at it and tell a molecular story about what happened
and say, oh, well, this is, this is not different from, from anything that's happened before it
followed. I mean, of course, it follows the laws of physics and chemistry. My point is never that
it's fairies underneath. And that's something, you know, something, you know, miraculous is
happening. That's never the point. But my point is, why wasn't it done before? What is it about
the, what is it about the standard paradigm that didn't, that didn't facilitate these,
these experiments to be done before? So, yeah, you know, you can be, you can be down on the,
on the conceptual frameworks that drive this stuff. But then you're playing catch up because,
because then the stuff is going to be coming out. And it's, and it's surprising. And this is, you
know, to me, this is, this is a more general issue of, of, of these kind of reductive explanations.
And by the way, I don't, I don't think anybody's really in this field is an actual reductionist,
because if they, you know, if you, if you push and you say, well, then you want explanations in
terms of quantum foam, right? And they said, ah, no, that's stupid. It's chemistry. It's got to be
chemistry. So that's not real reductionism. That's just the level of chemistry. But, but,
the, you can, the easy sort of analogy to this is if there was a, it was a game of chess played,
right? So, you know, a couple of people played a game of chess, you could. So Laplace's demon
could look at this and say, well, I mean, all it was is a bunch more, is a bunch of physics. I
mean, I saw all the, all the protons went where the protons go, the electrons went where they go,
everything followed rules. There's no mystery here, no surprise. Everything did exactly what it was
supposed to be. It's just, you know, it's just physics. That's not exactly wrong, because looking
backwards after the thing was done, you could tell that story. But now the question is, does it help
you play the next game of chess? How do you, how do you go from there to, well, now, what do I do?
And so, and it's completely useless for that, right? It's, it's only a story looking backwards.
So I think the thing about these kind of explanations is that you want them to facilitate
your next, the next discoveries, the way to, you know, to, to improve your capabilities.
Anybody could always tell a molecular story after the fact. The question is, what are you going to
do next? Yeah, I mean, on that, on the topic of reductionism, you, at some point, you say,
in an important sense, you are a brain in a vat. However, we are not just chemical machines.
So you're both acknowledging the fact that we're, we're these mechanical systems, and yet
you're also still saying that that's still, you're not reducing us to these simple properties.
You're still acknowledging the fact that there are more layered realities here to explore, in
a sense. Yeah, I mean, in the, in the kind of, the simple thing is that, yeah, absolutely,
my claim is that interactions with certain kinds of systems are much more efficient at high levels,
right? So, so, so certainly, I mean, we know this from anybody who's trained animals instead of
trying to run their neurons like a puppet knows that it's much better if you understand the
psychology of certain creatures and so on. And as you go rightward on that spectrum and to,
into friendship and love and these kinds of things that are much more bidirectional is not
just control and prediction, but it's actually, you know, being vulnerable to change and benefiting
from the agency of the beings that you're, that you're associating with and so on. Yeah, then,
then of course, there are these higher levels, but I want to say something else here too, which is that
I think one thing this is all telling us is that, and I think this will be more and more
apparent in the coming years, we have really misunderstood what simple machines are.
We've fallen in love with our, we've confused physical things with models of simple machines
that we make of those things. And we think we think we know what it is when we make something.
And I think that's profoundly wrong. What we have is our model. And we've done some work now,
and we're going to do lots more on finding surprising protocognitive properties and
very simple systems that are not obvious at all. And I don't mean emergent complexity.
Emergent complexity and unpredictability is trivial. It's easy. You can, you know, any
cellular automates or whatever, they will give you complexity that will give you
unpredictability in certain cases. That, that, that part's easy. I'm talking about emergent goals,
emergent cognition in systems that are extremely simple and minimal. And, you know, when we say
we are not machines, we are certainly not describable by the simple models we've made of
machines. But I actually think that lots of simple physical objects are, are very, are not
properly described that way either. And, and we need to, we need to remember that all of these
things are just lenses that we bring to them. You know, yeah, you know, I heard I was talking to
somebody once who, he writes these, these, these language models. And he said, well, I made it.
I wrote it myself. I know exactly what it does. I know, I know everything that's in it. I made it.
And as I said, I said, you don't even know what bubble sort does. We're like, we found, we found,
we found these, these unexpected capacities in, in, in stupid bubble sort, you know,
six lines of code, fully deterministic, nowhere to hide. Even that thing does things. Nobody
knew that it did. And, and, and if that's the case, when you make this, this, this crazy,
the, you know, language model, again, I'm not in love with language models, but, but, but just the,
just this idea that we've made it and therefore we know what it does. I think is, is we really
need a lot more humility around this. I, I, you know, I think there's plenty of stuff that,
quote unquote, simple matter does that we do not understand yet.
Mike, the last time I spoke to Mark Psalms, you asked me to ask him about what is the meaning of
life. And then this whole road towards the end of that conversation, he, he spoke about the
Oppenheimer Foundation, giving him funding, his, his new search for artificial intelligence,
trying to sort of see where this goes. What are your thoughts on his work and what they're trying
to do at this point? Yeah. I mean, he, as far as I know, none of it is published, although I've
talked to him about it quite, quite a lot. So I'm not, I'm not going to kind of give away anything
until, until he publishes it. But because it's not, it's not my story to tell. But I think of
anybody I know at the moment, his approach is the most likely to give rise to something that
actually captures what's important about cognition in living things. I think, I think if anybody is
going to engineer something that, that exploits some of the same principles that, that life exploits
for, for cognition, I think he's likely to do it. Yeah. Mark really sees that cortical fallacy that
we all seem to have, you know, this obsession of this sort of higher cognitive functions as, as
being this, the epitome of consciousness. Yeah. I mean, I think it's even, I think it's, it's,
it's way worse than that. It's not just where is it? You know, I mean, there are some people
who think that it, that, that, that the consciousness, for example, shows up during warm
bloodedness, you know, I think that's Nick Humphrey's position. And it's not even to me,
it's not even the question of where in the brain or what kind of brain. I mean, I'm talking about
what, what space do you even operate in? Because, because I think that, that, you know, when they
say, Oh, this thing is not embodied, it doesn't, it's not a robot on wheels that can sort of run
around. And you can have bodies and do this kind of perception action loop and active inference
and all this stuff. You can do this in other spaces that we are completely blind to, right? So
you can live in transcriptional space or anatomical morpho space or who knows, there's probably 100
others that we don't, you know, we don't know how to visualize. And all of those are embodied.
That's on us, that limitation that we don't see that, and we don't see all the,
all the goal directedness, the striving, the intelligence, the problem solving,
that's our limitation, right? And so, so intelligence, not only in, in, you know,
weird kinds of body parts, but just in things that are not in 3D space at all, really,
that's not where their, their life plays out. And even, even worse than that,
along the spectrum, even, even, you know, sort of weirder is along the spectrum of
how quote unquote real something is. So what I mean by that is I don't remember if it's actually
in that paper or not, but you know, there was a science fiction story. If anybody listening to
this knows what story it is, please email me because I couldn't remember like, which, which
I'd like to give credit and I couldn't remember who it is. But the idea is that these, these creatures
come out of the center of the earth, you know, they live, they live down in the core and they come
out of the center of the earth and they're walking around. Everything that we see out here is gas to
them. I mean, they are so dense that all of this stuff here that feels solid to us is gaseous phase,
it's plasma, like it's like, I don't even see it. And they're walking around as far as they're
concerned, they're like, and basically in, in, in, in space at this point, because like, oh my god,
there's like, there's nothing here. And, and I, I'm sure I'm embellishing this in my own way. I
don't remember what the, what the actual story is, but that's only the first part that I recall.
But, but to me, what I envision immediately is like, one of them is a scientist,
and he's taking measurements of this, of this gas that's on, on the surface of the planet.
And he says, you know, I see some, I see these patterns in this gas, they sort of
hang together for a while and they do things. And it almost looks, they almost look agential,
you know, these patterns almost look like they're doing things. And of course, the others are like,
oh, you're crazy, patterns and gas can't do anything. Patterns aren't real. We're, you know,
we're real. I was a pattern and gas going to do anything. And he says, no, I really, I think
they're like, they're trying to, you know, meet certain goals and they have memories and where,
but, and they say, well, how long do these patterns hang around? He says, well, about 100
years, that's ridiculous. Nothing important can happen in 100 years, you know, because these
things live for, you know, millions of years. And so, and so this, this just reminds you that
what's a pattern and, and what's a real being. So, so again, back to this distinction between
thoughts and thinkers is in the eye of a beholder, you know, it's in the eye of the observer. And
if we did have aliens that came to earth with a radically different cognitive frame rate,
if they had different lifespans, whatever, would they think that talking to us is a good idea?
Or would they be trying to talk to ecosystems? Or conversely, would they think that talking to the,
you know, molecular processes is the best that they're going to be able to do? I think, I think
all of this is really about observers and about getting good at recognizing intelligence and
extremely unfamiliar guises, you know? I think that the it's, it's inescapable, your work particularly
to, to not cross philosophical slash ethical boundaries and have these discussions. So,
so when people tell you that listen, stay away from the philosophical stuff, it's you cannot,
you just, this is just not part of your job. You have to at some point address these because
I remember one of the comments in one of our discussions, it could have been our first one
or second one. But then someone asked, is Michael playing God? And my first thought was,
it's a strange one, you know, it was one of those questions people often asked back in the day when
people were tinkering with any sort of even a plant, you could genetically modify an organism,
and then you're playing God at that point. I mean, it's a very, very strange question to
really ask. How would you respond to that? People actually ask it all the time. I think it's one
of those questions that sounds like it makes sense until you, until you sort of dig into it a little
bit. But because, because I don't know what, what the definition of God is. And I mean,
usually the people who ask this, they got some glasses on and they've got, you know, they usually
drive, they don't walk places and so on. So it's a little, it's a little disingenuous. But, but,
but let's, let's dig into this for a moment. I did a poll once on Twitter, and certainly this is not
like a, you know, a statistically valid sample or anything like that. But I did a poll and the,
and my question was simply this. So you're, you're, you know, AUG the caveman, and you're walking
back to your, to your tribe, and you have this vision, you're struck with this vision of discovering
fire. And so immediately, you get, you understand fire, but you also get this vision of steel,
weapons, artificial hearts, antibiotics, going to the moon, atom bomb, computers, art, all,
like all of it, right? Immediately. So now the question is, so now your question is,
so you've seen all this, right? You've seen, you see where it's going to go. Your question is,
do you tell the others and you get going with fire or, or, or, or do you let it,
do you let it die? And you never tell. Okay. 6% of my, my audience, and that's, and that's the
people who like my stuff. So that means they're already probably like really biased towards,
you know, techy stuff. 6% thought, thought you shouldn't, you shouldn't let, you should stay
below fire. So, okay, I don't know, you know, I don't know if, if these folks live a lifestyle
consistent with that belief, I tend to doubt it. But, but, you know, it's, if, if that's the claim,
I think you have to take this series. I think you have to say, if, if you really mean by playing
God, I mean, what could it possibly mean? If you really mean taking steps that are
strongly efficacious in the world, that make change, that do things. If you really don't want
to do that, your quarrel is not with me and my work on frog skin. Your quarrel is with all of
humanity who doesn't want to sit in a damp cave their whole life and die in exactly the same
condition that they were born in. That's if, if you're really against that, okay, make your case
and, and see, you know, and see, see if people will go. But none of the things that, that, that
we're doing are any different from the fundamental question. Are you going to take responsibility
for the future? And I think that is the most profound moral cowardice to delude yourself into
thinking that doing nothing is staying out of it. No, doing nothing is not staying out of it.
Doing nothing means you are complicit in the suffering of enormous numbers of, of humans and
others on earth who are having a, an incredibly sub, a suboptimal experience in their, in their
embodiment. And if, and if you have these kinds of thoughts about, let's not do this and let's not
do that, you know, you know, let's put a break on progress, you are making a very clear statement.
And, and you should think about it hard to make sure that you, you are, you know, you're, you're
really backing off this idea that you are going to stay, you're going to let the status quo roll on
because I, you know, it's just, to me, it's an incredible act of moral cowardice.
Yeah. And I think if for anyone who wants to even get a glimpse of what your ethical framework around
all of this eventually becomes, this paper is perfect. I mean, this paper on AI, at some point,
you go a path forward through the ethics filter for civilization. And this fundamental premise
for you is, is to mature, to realize, okay, our kids supplant us, everything does change. We
continuously change. It's how we're going to move forward. And, and how are we going to, to, to act
in a certain way that progresses us in a, in a safer, more kind, more loving environment. And,
and this, and the, the L word people, scientists don't like to use it. But I mean, at that point,
you're looking towards this sort of kind of process where we were able to give artificial
intelligence these properties, because it is something that our cognitive light can't appreciate.
And we know that this is something we genuinely enjoy. So let's try and propagate this.
Yeah. Yeah. And, and, you know, I have some collaborators. So, so Richard Watson and Thomas
Doctor and Olaf Witkowski and, you know, people like Bill Dwayne and Eliza Salamanova, you know,
we, we, we write on stuff like this, and there's going to be, there's going to be way more because
in, in certain traditions, right? So for example, they come from a Buddhist tradition. And so,
and so there, there's a great emphasis on enhancing compassion alongside enhancing wisdom, right,
on a basically an infinite sea of other beings and all sorts of crazy embodiments. I mean,
I gave a talk on all this stuff to, to some, to some Buddhist scholars in Nepal, you know,
at some point. And I mean, that audience, there was nothing here that surprised them whatsoever.
You know, usually when I give these talks, people are kind of kind of shocked and disturbed about
about half of what I say. These guys were like, yeah, no kidding. We all know that. And they
found nothing, nothing weird about any of it. And I do think they have, they have frameworks for
thinking about these, these, these kinds of things, right, you know, this kind of expanding,
committing to through, through concepts like the bodhisattva vow and through expanding,
committing to the task of this, this metacognitive task of expanding your, your cone of compassion
and things like that. Yeah, I think, I mean, I'm certainly not saying that's the only way to go,
but, but I think that's exactly where this is going. I agree with you. I mean, because my,
even though I'm of Indian heritage and descent, but my, when I talk about science, philosophy,
Western, particularly, to my family to like them, certain uncles or aunts, and if I talk about these
topics, they also tend to do that. They're not as surprised as the more my more Western side of
the family. A lot of the Eastern philosophers and my uncles and aunts, they're not really philosophers,
but they tend to think, okay, that makes sense. That is kind of what their religion taught them,
whether it's Hinduism or Buddhism, but there is this element of minds are everywhere in a way. So
this, this, this general binary approach that we seem to have is not working for the most part.
And you're showing this in very, very Western scientific ways. Well, I think, I think, I mean,
that's the other thing, right? So, so I don't really believe, okay, there's, there's another
perspective where sometimes people say, look, early indigenous societies knew all this, all we have
to do is go back, go back there. I don't actually believe that either. I don't think they actually
knew this. And right. And saying something is not the same thing as having a principle framework
that takes you to new discovery. So, so it's, it's, I think both sides, this idea of there's no mind
everywhere, anywhere except in us, or maybe some people think that just isn't anywhere. But, but
the other side of it, which is, oh, there's a spirit under every rock, like that's, that's a fine
start, but it's just a start. You can't just state, say it and leave it at that. You have to
answer the question, what does that do for you? So I think this is really important. All, all of
this has to be empirically useful. It has to elevate our condition and has to improve our ability to,
to have more meaningful lives in the world. It has to be practical. You cannot just say these
things and have it mean anything until unless it leads you to experiments and ultimately to,
to, you know, the better ways of being in the world. So I don't think we're going backwards
to those traditions at all. I think we're using whatever we can scavenge out of all the, you
know, brilliant people that have existed in the past that has sort of glimpses of this stuff.
But, but now I think we finally have the ability to push it forward in a very practical way. So
that some of these ideas we can, we can discard what isn't useful, we can, we can keep and expand
what actually helps us to get to new capabilities. And I think that, and that's part of the approach
that I appreciate most. I mean, I find it particularly annoying when people do that,
what you're talking about, where these gurus come out and just say these things with no basis,
absolutely no evidence of what there, there's no claim, but it's just so profound in itself. That's
it. The statement itself is all that they have, which, which isn't what, what, what you're trying
to do. You're, you're often saying you got to show, you got to do something, back it up somehow.
Yeah. I mean, I mean, these, these claims and these profound statements and, you know, and,
and poems and whatever else, they're, they're a fine tool for spurring intuition and for
giving you ideas. It's the starting point, right? And, and, and I, and I do think that it's true
that it's possible to have intuitions about things and, and to come up with
prompts, you know, sayings and writings and whatnot that trigger other people into new and
interesting thoughts, even though you haven't yet worked out all the details. I mean, I do think
that's possible. I do think it's, you know, we are kind of like, so I have this like, almost,
almost like, like the way, um, playtonist, uh, mathematicians, you know, they feel that they're
discovering an existing structure, right? Of that, that you're uncovering an existing structure
and, and that, you know, you see, you're sort of piece by piece, um, pull it pulling it out. Um,
I do think that it's possible to, to, to sort of have insights long before you have the wherewithal
to really make it practical or to know what it means or any of that. And so, and so I like that
stuff as much as anybody in terms of an intuition, uh, you know, building kind of thing to see what
it makes you think about. Like, like the quote from William James, right? I don't know. And I,
I lose no sleep over whether he actually meant that the way that I mean it. I don't care. I think,
I think it's a very profound saying. And what can we do with it now? But, um, you know, the hard
work comes, comes after all that. Someone, it reminds me of, and I mean, we lost him recently,
Daniel Dennett raced in PSM. He, what you do is almost the reverse, but in, in, in the same,
I would say, in the same great manner is that what, what Dan did was he realized that you
can't just philosophize. You, you have to go in, you have to get involved with the cognitive
science. You've got, you've got to get, you've got to basically do some of the work. Um, and, and,
and that's when the philosophy becomes a lot more intriguing is when you do the science and you go
into it and you fuse them and you're coming from it from the science side. And then,
of course, you then have to have the philosophical discussions with it.
And, and you guys have worked very closely together. What is that like for you just as a side?
Uh, boy, uh, I mean, first things first, uh, you know, I, I read Dan's books when I was a kid
and it never, I mean, they were so eye-opening, you know, the mind's eye and kinds of minds and
that kind of stuff, right? The early kind of the early work in the late 80s, early 90s.
I was, I was young back then and I would, I couldn't have imagined for a moment,
A, that I would, that I would get to meet him, never mind that, but B, that at some point,
you know, at some point we'd write a paper together, right? Like I wish I wish I could get
into a time machine and go back and, you know, tell my 18 year old self that, hey, you know,
you're going to write a, write a paper with this guy and actually a bunch of other people too that,
that I felt the same way about. So, so, so that part was a profound kind of honor for me,
is to, is to be able to talk to him about these stuff. And, and by the way, we didn't agree on
everything. We, we disagree on a ton of stuff, but he was, he was an incredibly generous,
clear thinker. And what I really enjoyed about him was that was, was a few things.
One of them was that he was never interested in, in making cheap points.
He was always interested in improving everybody's understanding of what's going on,
deepening the question. It may be the answer, but, but for sure, deepening the question.
And this idea, you know, he really pushed this idea of steelmaning. You know, he said that,
that in arguing with people, what you ought to do is first state their position so well
and so strongly that they will wish they came up with it, right? That you should start not,
not with a caricature of what they think that you're going to shoot down, because that's a game,
right? That's, that's, you know, what he wanted was actual progress, which meant you better start
with the absolute best description of their view, the most plausible sounding, and then,
then see if you can shoot it down after that, right? That was his, that was his, and he was
always that way in all, in all of our discussions, you know, about stuff that we did agree on and
lots of things that we didn't agree on. It was, it was always very clear that everybody in this
discussion is there to, to learn something and to improve and to give up things that you thought
before, if, if they're not helping you move forward and grab some, some other tool like that,
that was, you know, he was an amazing example of that. And I mean, for, well, first I took a
course with him as an undergraduate at Tufts, I had him for, for, for a, yeah, I had him for a,
for a philosophy of mind professor, which was, which was amazing. I purposely wrote a paper,
there was a, there was a final paper for the class that you, that you write. I, I picked a topic
that I knew he, he did not like and that I knew he, you know, was, was completely against. And,
and I was, I was astounded at, you know, the, the fair, rigorous, but, but, but completely fair,
you know, analysis and grade and everything else. That was an example for me that this is how you
do it. This is, you know, it's not, it's not just based on what, you know, what, what you think,
but like, you know, a deep analysis of, of the, the fairest analysis. And then, and then later,
when I came back to Tufts as a faculty member, you know, he was, he was a, he was a colleague.
And that was, that was incredible. So yeah, yeah, I'm really, I'm really going to miss him.
Yeah. And he'll be dealing with, I mean, he was one of the, so him and Oliver Sacks were two of
the people that inspired me to even start this podcast. So one of those, yes, I never got to
have on, but we exchanged emails every now and then. And even doing that for me felt like such an
honor. And I really wish I had the chance to chat to him. That's how I'm just so curious for all
those people that did get to speak to him. What a, what a privilege it might have been to pick
his brain. Yeah. Oh, no, it was, and he was so, you know, he was so, so inspirational and so
generous with his ideas. He would come to our lab from time to time. And I have a, I have a picture
of him on the, on the blog with what, during one of his visits, you know, and he's, and he was looking
through the microscopes and he was looking at our two headed worms. And we would have these,
he would have these discussions with our lab people about, you know, what's that, what's it
like to be a creature with two brains and what's the right way to think about these things. And,
and, you know, and, uh, yeah, he would, you know, he would give talks, uh, just, just very generous,
you know, Mike, you must, you must check it. You must look out for this one of these videos
online. It's a VPRO round table with Dan, Dan Dennett, Oliver Sacks, Rupert Sheldrake,
Steven, it's one of the most fascinating things. There's like six, I think, uh, Dyson, Freeman
Dyson was there as well. It's, it's, it's such a strange thing. It's like the original version
of podcasting, I would say. Amazing. Amazing. Six of these guys is having the coolest chat on life,
consciousness, reality. Um, that was one of the things that got me into the two of the,
both of their work and actually to this podcast. But anyway, before we, because we digressing a bit,
the path forward, this ethics, uh, how are we doing for time? Mike, you all right?
Uh, yeah, I'm, yeah, okay. I'm, I got about, I got about 15 minutes. Good. Okay. Well, the, the
path forward, the ethics filter, let's talk about this. Cause you said that there's, there's two
ways we could get this wrong. One is objectophilia and the other one is, well, only love your own
kind. Um, let's talk about how we can get this wrong and how we can actually divert this and get
this right. Yeah. Um, well, the, the, the, the spectrum itself is something like the, it's,
it's related to the effort of matching the degree of, uh, compassion that you, um, are able to exert
to the level of agency that there are intelligence or consciousness that, that that being actually
has, right? Now I'll point out that, that we, even when we get it right, we are still not very good
at following through on the consequences. So, so for example, everybody understands that pigs are
intelligent. Everybody understands that they, that they suffer, that they have minds and we
still have factory farming. It's, it's right. So, so even, you know, getting it scientifically right
is absolutely not, uh, uh, uh, a guarantee of anything in terms of, uh, actual ethical behavior.
But, um, but there's two ways to get it wrong. One way to get it wrong is to, uh,
attribute more mind to a system than it really has. But also when I say really has, I, you know,
I think everything is observer relative, of course, but, but still you, you could get, I mean,
there's, you know, the internet is full of profiles of people that are in love with bridges and,
and chandeliers and, and, you know, and things like this. Um, so, so, so that's, that's something,
uh, having too much, too much concern for things that really don't warrant it.
And the other way is, of course, the opposite is when you've, you, you leave beings out of your,
of your, of your, um, calculus of compassion that, that actually can, um, can suffer and have an
inner perspective. I mean, one, one thing to think about is, uh, if imagine two societies that get
this wildly wrong in both directions. So you've got a planet where everybody's like, you know,
ridiculously nice to, to, you know, they don't like to chop rocks in half and, and whatever.
And, uh, and, and then there's, and then there's the other society that thinks if, if you're not
a very narrow type of creature, you are a machine the way that Descartes thought about lots of
animals and that we can do whatever we want and it's fine. And you're just faking and all your,
all your complaining about it is, is just, um, you know, it's just a word where it's, it's, it's,
it's sentence completion, you know, is what it is. So, okay. So, so which of those worlds would
you rather live in? Right? If you're going to get it, if you're going to get it wrong, where,
where would you rather be? I mean, I think, I think the first one wastes a lot of resources
and opportunities. Yeah. Okay. The second one is, uh, is, is monstrous in, in its ethical
implications. So, so I, I think we should err on the side of more compassion, not less. I mean,
obviously, again, we're not going back to there's a spirit under every rock because
we are committed to having principled, uh, theories about this. But if you're going to make a mistake,
I think you should make a mistake in that direction. And specifically what I'd like us to be clear on,
I'd like what I'd like everybody to be clear on is that having certainty about these things
right now, when we have pretty much no clue what underlies, um, consciousness, uh, really,
I mean, I know a lot of smart people have made efforts into it, but, but I really don't think
we have it nailed down. Um, and all of these ideas about, uh, what cognition is and how different
architectures, um, you know, supported and, and whether cognitive consciousness and, and the ability
to suffer, uh, uh, tracks any of those things or not. Uh, there is, there's an enormous amount of
unwarranted, uh, certainty about this among people. People feel very strong to make this really
strong. That definitely doesn't whatever, you know, it doesn't have this or that. I think we all
need to take a step back and just understand that from, from, from the scientific perspective,
there are so many things we do not know yet, like really critical fundamental things. We do not
understand, um, the emergent, uh, cognitive properties of matter. We do not understand the
scaling, um, policies of how minds emerge from smaller minds. Uh, the field of diverse intelligence
is just getting started. So I, I'm much more worried about the right side of that, uh, of
that, um, spectrum than I am about the left side at this point.
And I think what, what, one of your towards the end of the paper, one of the things you
say is the question is, how do we make sure to express kindness to the inevitable forthcoming wave,
uh, of unconventional sentient beings? And you say that we should start by making sure that we
express loving kindness appropriately and not be driven by fear of the other, which is, uh,
which is a very beautiful statement.
Um, yeah, thanks. I, I, I've actually written a whole thing on fear just now. I'm waiting.
It's going to be, it should be out in, in a couple of weeks. Um, I think that, uh, well,
well, one thing I could say is, uh, after that, after that piece in, in Noima, so, so there was
this, the short piece in Noima about the, the AI, there's a much, there's a longer paper which
exists as a preprint and it's also in, in review right now in the journal, but, but I think more
people saw the Noima piece. But still, I was, I was very clear there. I thought that I'm not
actually saying that AI is like, that, that current language models are like humans. I mean,
I thought I was pretty clear on this, but, uh, I got a lot of, um, a lot of people writing to me that,
um, it basically extremely disturbed by this and this idea that, that, that, uh, that tech
bros like myself are, uh, yeah, I thought that was, that was funny, uh, that are, are, um,
that, that our nerdiness, uh, sort of prevents us from understanding real human relationships.
And this is why we see these things in, uh, in, in, in, you know, what they called machines,
robots, AIs and whatever, right? They, they were looking for a, um, they were looking for
why you do this in a sense. Correct. I mean, it's an old strategy, right? The old strategy is,
if, if you're uncomfortable with a view, try to find something wrong with, with, right? What,
what is it that, you know, we see the truth, why can't they see the truth? What is missing that,
you know, that causes them to, to, to say these things, right? And the standard theory is, uh,
well, they just don't understand, you know, these, these, these nerds don't understand
what real, um, human relationships are like, right? And that, I mean, I mean, I'm not super
interested in, in, um, psychoanalyzing anybody, uh, that way, but it did, it did cause me to think,
I'm like, wow, why are people so triggered by this? You know, what, what is it that caught,
you know, to really, and so, and so I, so, so I'm thankful for once the pay, pay, you know,
piece comes out, I'll thank some folks and, and who said these things and actually pushing me
in what I thought was, I think is an interesting direction is to ask, um, what, what is it?
What is it that's so scary about, about this view? And, and the more I think about it,
I really think it's a very fundamental fear. And the fear is it's a zero sum game. Love is a zero
sum game. If we have too many other beings that need love, then a couple of things will happen.
There's not enough for me. That's a and B, what if I can't rise to the, to the, to the challenge
of having enough compassion for everybody? I think it's profoundly threatening to realize
that you're going to have to open up your, um, your constrained way of looking at who deserves
your compassion and, and what happens then. And, uh, and, and, and, and many other things. So,
you know, so I wrote that on this, probably five or 10 pages or something about, you know,
just kind of talking about what is really, uh, what, what I think really underlies why, why
people are freaked out about this and, and, and the responsibility. I mean, it's very comforting to
think that I can just tell, you know, which things are worth worrying about by looking at them. I
know what people look like. I'll just look at them. Um, it's comforting to think that I don't
need to be responsible for the future. This, this is it. This is, you know, this is how, uh,
this is what's natural, right? Even people who don't believe in, in some, some sort of God,
all they, they still have this notion of what's natural. I have no idea what that's supposed to
mean. But, but, but, you know, this is like, yeah, this is how we're supposed to stay. And
that's fine. I don't, I don't need to be responsible for the future. And I don't need to be responsible
for, uh, shaping, um, what the planet looks like in the, you know, in, in, in the coming centuries
and beyond. That's comforting to think that it's all handled. It's nice and simple. Uh, you don't
need these, these extremely difficult nuanced views that are going to require work from you.
They're going to require you to make hard decisions, to paint a picture of the future,
of what do you want it to look like? You know, um, it's much easier to say what you don't want.
This, this fear-based, uh, scarcity mentality, right? There's, there's not enough love to go around.
Let's, let's, let's draw a nice tight circle around things that we know what they look like,
and we know where they came from. Then we're not going to have to worry about all this other
stuff that's really difficult to, to figure out what's, what's going on with it. And, um, yeah.
And then we don't need to worry about, um, uh, painting, uh, uh, pictures of the, of the future,
uh, and figuring out how to get there. We could just, we could just make a list of what we don't
want to have happen. And that's easy and, and, and focus on the negatives. So I think, I think that,
that type of, that type of, um, limited, uh, fearful scarcity kind of mindset
is, is what's, is what's responsible for a lot of this. And by the way, what I don't mean,
so I, so I want to be clear here. I don't mean to, to, um, try to deconstruct some of my colleagues
that are really working on, on very good science, right? So, so there are people who are working on
good science for developing principled ways to distinguish between so-called machines and
what's special about living organisms. Like that, that's a good area of diverse intelligence. I'm
not, I'm not, you know, are saying that that shouldn't, you know, that that shouldn't take
place or, or that they're driven by anything other than, um, you know, good scientific principles.
I'm talking about the, you know, I'm talking about the, the folks who have a really visceral reaction
who, when, when I, uh, when I challenge them to, so, so, so, so be explicit. So, so tell me what,
what is the magic that you have and when did you get it? Both during evolution, during,
during, you know, during embryogenesis, what, what, what, what do you have? And when does this
show up that you think cannot be, uh, either, either in a hybrid form or in synthetic form,
you know, done? And what would you do if, I mean, just, you know, I think reading science
fiction is a great cure for this because from the, from the earliest time you understand this
scenario, right? You're sitting there at home, this, um, spaceship lands on your front lawn,
this, this, the door opens, this thing sort of trundles out. It's kind of shiny looking. It's
kind of metallic looking, but it sort of comes up to you and it sort of hands you this poem and it
says, oh man, I'm so happy to meet you. You know, it's been, it's been, you know, a thousand years
I was waiting to meet you. Many of us died along the way, but, you know, but, but we, we persevered
and we made this journey in here. I wrote you this poem and I'm looking to be friends
and you sort of knock on and it's kind of metallic. And you say, um, so, uh, did you guys evolve
naturally or did somebody make you? And he says, you mean, you mean, are we the result of totally
random processes or was our mind crafted by, you know, some other mind? And you say, yeah, I'd
really like to know and say, why, why do you want to know that? Like, well, just, uh, you know, I'd
really like to know because, and in the back of your mind, you were thinking, what, that, that,
that if it's the, that if it's the latter, then, then you're okay with turning it into a vacuum
cleaner, right? That's what you're really thinking about. And, and I mean, I, I, I find that just,
just, you know, absurd. And we are all stuck in this position of saying, so what, what criteria
are you going to use when you can't do this easy thing? That's why, that's why I think AI and
language models are such an off ramp for these discussions because it's just so easy to dunk
on these language models, completely avoiding this issue of that embodiment can take place in
other spaces that you have absolutely no idea what, what, you know, physical systems are capable,
or even if, even if you made it yourself. Yeah. And even that in itself, and when you spoke about
it, when you said, if you use AI to create something, I mean, who really created it?
And then you have that wonderful quote where you say like, nothing was ever created by two men.
We're merely sort of just adding upon what's already there.
Yeah. Yeah, I think, I think we really need to be clear that there are major, major
open questions here, like really fundamental open questions. It's too early to be certain of anything
other than, I mean, I think the only thing we can be certain of is that it's very easy to make
ethical lapses when you try to draw these distinct boundaries and you have no idea what you're doing.

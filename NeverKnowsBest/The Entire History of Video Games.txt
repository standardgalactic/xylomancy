Video games have come a long way. From once being seen as a temporary fad, to
just toys for children, or immoral corruptors of the youth, today the gaming
industry easily surpasses the film and music industries combined, and it's
still growing. Still, as tempting as it might be to draw comparisons to more
established industries, the reality is that video games have proved to be
something unique. No other industry has been so closely tied to technological
progress. No industry has ever grown so far, so fast, and no other industry has
experienced so much change in such a short time, with multi-billion dollar
companies rising and falling in the space for a few years, and the entire
gaming landscape being radically changed more than once by a couple guys
working out of a garage. And all this in something that's only been around
about 50 years. Video games have come a long way, and their evolution has been
remarkable. Today though, more and more people seem to be viewing the industry
with pessimism, and despite continued growth, its future doesn't look quite as
bright as it once did, with companies and consumers alike increasingly seeming to
be looking to the past. So, let's talk about the past. All of it, from the very
first game, right up to where we are today, and every major event in between.
This is the entire history of video games, and it begins with something that would
end up as a bit of a staple for the industry. War.
World War II was kind of a big deal. In addition to deciding the fate of a
global population and shaping the modern world as we know it, it also saw rapid
technological advancement that would pave the way for something equally
important. Video games. After building the first ever digital computers for the
purpose of numerical processing and code-breaking as part of the war effort,
this technology was subsequently commercialized and found its way to
several universities, and where computers go, video games soon follow. Of course,
they weren't exactly designed as video games, so much as demonstrations to show
what computers could do, or just experiments to further the field of
computer science. The first to be shown to the public was Bertie the Brain in
1950, who played tic-tac-toe. Poor Bertie was apparently quite good, although
every photo seems to show him losing, and he was dismantled two weeks after the
exhibition, as tended to be the case with these early computers. The first game to
be created as entertainment was 1958's Tennis for Two, a side-on simulation of
a game of tennis that ran on an analog computer and was created by William
Higginbotham, a man perhaps better known for his role in the creation of the
atomic bomb. Meanwhile, in 1962, Space War, a two-player 1v1 space battle
simulation created at MIT, became the first game to see wider distribution as
its source code was shared between students and institutions. As the 60s
continued, more games were created for the mainframe and mini computers of the
era, but their popularity was quite limited by the fact that the computers
needed to run these games cost thousands and thousands of dollars, something
affordable for a university, but less so for the average family. Things would
change in the 1970s however, as video games started to appear alongside
Pimble Machines and other mechanical games in arcades. The first arcade video
game was Computer Space in 1971, which took heavy inspiration from the earlier
Space War and was designed by Nolan Bushnell and Ted Danby. Computer Space
saw limited success, but its creators still believed in the potential of the
medium, so they founded their own company, Atari. At the same time as this, new
breakthroughs were happening elsewhere, as in 1972, the first ever home console
was released by electronics company Magnavox. It was called the Odyssey, and
its development started six years earlier, with the version finally released
being the seventh successive prototype. The main challenge for Team Faced was
affordability. The technology for a home console existed, it was just a question
of how to keep the price low enough that anyone would be willing to manufacture
or buy it. But eventually the team led by Ralph Baer found a way, and so video
games at long last came to the American family home. This was an important
milestone, although the Odyssey didn't exactly see the runaway success of later
consoles. Still, its simple two-player table tennis game did provide the
inspiration for Atari's first game, a remarkably similar-looking future
phenomenon Pong. Released in arcades in late 1972, Pong was a game that changed
gaming, proving immensely popular in arcades, and at 25-cent a game, also
immensely profitable.
By the end of 1972, we did $3.5 million, and then we did $19 million, and then we
did $35 million. It was hockey stick. In fact, Pong was so popular that the first
machine installed quickly stopped working. Atari were called out to get it
going again, and found the problem was that the cash box had overflowed from the
vast amount of quarters, leading the machine to become jammed. And so arcades
exploded in popularity, as Pong proved gaming's potential. Video games had
arrived, and at the birth of this exciting new medium where the possibilities
are endless, how did everyone react, you might wonder. Well, obviously, they copied
the one thing everyone knew already worked. Atari made sequels and successors, as well
as their own dedicated home Pong console, and other companies made their own versions,
alongside their own sequels and successors and dedicated home systems. Pong was everywhere,
as profit led to proliferation, and before long, video games were in decline.
By the mid-1970s, arcade growth slowed, and then shrunk, as the market became saturated
with Pong clones, and brand new companies that had once rushed to cash in on this big
new thing started going under.
Atari themselves fared better, although they had their own troubles by way of a lawsuit
that came from Magnavox, over the striking similarities between Pong and their table
tennis game. Atari settled, but they weren't the only ones being sued, and Magnavox's
winnings ended up totaling over $100 million. And so, arcades were struggling, but thanks
to technological advancements over the 70s, a new prize was up for grabs, and the competition
would be intense.
Arcades might have been where video games got their start, but home consoles offered
an even greater opportunity, and people were quick to realise it. In 1975, the year Atari's
home Pong released, there were seven other companies releasing home video game consoles
of their own. By 1977, there were at least 82 other companies, with 160 different models
being released in that year alone. The consoles from this era were dedicated systems, meaning
they could only play one or more built-in games that came included. A lot of these were simply
Pong clones, with Atari even taking measures to make the cloning process more difficult
to hinder their rivals.
This didn't stop the market from becoming saturated however, but this time, a solution
would come through innovation, and once more, it would be Atari who led the way.
The Atari video computer system, later called the Atari 2600, released in 1977. One year
after the Fairchild Channel F became the first ever console to feature interchangeable cartridges.
Bushnell saw the advantages cartridges provided and rushed to reduce a cartridge-based system
of his own, but earlier problems with lawsuits, increasing competition, and decline in arcade
sales meant Atari's funds were getting low, and so they made a deal with Warner Communications
to sell Atari for $28 million. The consequences of this deal would be seen most clearly in
the following years, but for now, it meant Atari had the money to finish their new console
to beat out their main rival Magnavox, who would release their own cartridge-based system,
the Atari 2600 would be huge, with lifetime sales of over 30 million units, a number that
dwarfs any other console of the era. Still, this success came slowly, and in late 1977,
the entire gaming industry experienced its first, but by no means last, crash. The combination
of a saturated market full of dedicated systems, alongside new cartridge-based systems that
proved far more desirable, meant heaps of dedicated systems suddenly stopped selling,
leaving retailers with a surplus of stock they couldn't shift, and causing game developers
to cancel most dedicated systems that were currently in development. The resulting downturn
hit the gaming industry hard, leading economists and media figures to speculate whether the
entire gaming industry had simply been a short-lived craze that was now coming to the
end of its life cycle. Amidst this pessimism, the Atari 2600, with its relatively high price
point of $200, which is almost $1,000 after adjusting for inflation, was seen as a tough
sell, and retailers and consumers alike were hesitant to embrace it, meaning Atari was
only able to sell a fraction of the systems they manufactured in its first few years.
Further competition would also come from other cartridge-based systems, including the Intellivision
from Mattel, and the ColecoVision from Coleco. But Atari would soon find the killer app they
needed that would not only get themselves back in business, but would also get the entire
gaming industry back on track.
Space Invaders was created by Japanese developer Tomohiro Nishikado at Taito in 1978, and it
is easily one of the most important and influential games ever made. Before Space Invaders, different
regions like America, Europe and Japan had little crossover between them, but Space Invaders
quickly moved beyond regional borders to become the first truly global gaming mega-hit, and
that was by no means its only achievement. Its gameplay may look relatively simple today,
but in the 70s it was anything but. As a fixed shooter where the player takes on rows of gradually
descending aliens while hiding behind fully destructible bunkers, it was the first game
to have enemies that shoot back at the player. It helped pioneer the move away from timers
over to lives-based systems. It was the first game to feature a high score system, giving
it potentially infinite replayability, and it was the first game to feature continuous
background music. The background track itself was dynamic and increased in tempo in time
with the enemy's increased movement speed as rounds progressed, giving the action a real
sense of tension as the difficulty ramped up. This iconic part of the game wasn't actually
intentional. During its development, Nishikado found that the arcade's processor struggled
to handle so many enemies on screen at once, and yet as the player took out more and more
of them, the render speed increased, causing the aliens to move faster and faster. This
created a difficulty curve where the challenge rose and fell within each level to create
moments of higher and lower intensity, a principle still seen in game design to this day. Space
invaders also featured some semblance of a story. Aliens were invading from space, and
you had to stop them. Okay, it's not much, but compared to the more abstract Pong or
Breakout, it made the action feel much more contextually meaningful and marked the beginning
of arcade games paying more attention to these kinds of aspects. And if all that wasn't
enough, there was also the game's sales. By 1982, Space Invaders had grossed $3.8 billion,
which is over $13 billion after inflation, meaning it was the then best-selling video
game and highest grossing entertainment product of all time. For the industry, this meant
video games were back, and soon it wasn't just Space Invaders. 1978 marked the beginning
of what has since been named the Golden Age of the Arcade. 1979 saw the release of Asteroids
from Atari, which continued the space battle theme, this time in an asteroid field with
more complex physics-based movement, while also using cutting-edge vector graphics that
utilized lines instead of pixels to provide a crisp display, unlike anything that could
be produced on a home TV screen. The trend of popular arcade games where you shoot things
continued in 1980, with Atari's 3D first-person tank sim Battlezone as well as their trackball-controlled
missile command, which took inspiration from the still ongoing Cold War to task players
with defending cities from intercontinental ballistic missiles. 1981 gave the world Defender
from Williams Electronics, which pioneered the side-scrolling shoot-em-up genre, and
Gallagher from Namco, a fixed shooter which featured complex enemy patterns and power-ups,
both features that would define the genre moving forward. Shooters weren't the only
thing coming out of arcades, though. In 1980, Namco's Pac-Man gave the world its first
gaming mascot, Pac-Man, while establishing the maze chase genre and being the first game
to feature power-ups. As iconic as he would go on to become, however, Pac-Man's path
to success wasn't a simple one. The game initially struggled to beat Namco's 1979
space shooter, Galaxian, in Japan, while in America, publisher Midway was hesitant to
manufacture many units as they were unsure how it would perform. The reason for this
was that it didn't involve any shooting, and so executives feared it would be less
popular with arcades mostly male player-base. This was something developer Toru Iwatani had
also considered as he'd deliberately tried to create a game that would appeal to women as well
as men, and offer something different to war or sports, hence the colourful design, cute characters,
and emphasis on eating rather than killing. Iwatani figured that girls like eating things
more than killing them, and it seems he was onto something, because in the end, players couldn't
get enough of Pac-Man's addictive pellet-eating action. Within a year, it had become the best
selling arcade game in both Japan and America, outgrossing even the 1977 film Star Wars in revenue.
Pac-Man was one of the first, but by no means last times a video game found record success
through broader appeal and targeting new audiences. Its use of vibrant colour and its focus on a
central titular character would also go on to become video game staples, and would soon be seen
in other popular arcade titles like Konami's 1981 Road and River Crossing hit Frogger.
By 1982, colour and 3D technology were being combined in even more impressive ways,
like in Namco's visually striking racing simulator Pole Position. And the arcade Golden Age was in
full swing, with profits from games reaching new heights, and Taito, Atari, and Namco establishing
themselves as the three biggest developers around. Arcades were more than just a place to play games,
however. Through features like high scores, multiplayer, and the ability to watch and compete
in front of other people, arcades provided a social aspect to gaming that went beyond just playing.
Much like how Twitch allows people to watch, discuss, and compete in various games today,
arcades were the live streaming and esports of the 20th century, and they offered better
gaming experiences than you could get in your home, with higher quality graphics, better sound,
and unique immersive elements like wheels and pedals in racing game cabinets.
Arcades would also have a significant impact on future game design. Their quarter-hungry
monetisation method has sometimes led to criticism over their high difficulty, with older console
games also sometimes coming under fire for inheriting that high difficulty. But popular
arcade titles knew that if difficulty was the only thing they had to offer, players wouldn't have
any reason to keep coming back, and so they learnt how to make skill-based challenges fun
and make players want to improve and keep playing. Still, as important as arcades were,
Space Invaders legacy goes beyond just kicking off the golden age of arcades and breaking sales
records, because it also turned things around for the home video game market and Atari.
In 1980, Space Invaders came to the 2600, allowing people to play the arcade classic
from the comfort of their home. This was the first arcade to home console port, and it wouldn't be the last.
Space Invaders for the 2600 became the first ever killer app, or system seller,
in video game history. And it certainly sold systems, with Atari's consumer sales almost
doubling to $200 million in the year it released, before skyrocketing to over $800 million in 1981,
with Atari taking 65% of the global market share. And so, the great console battle royale that had
defined so much of the 70s was over, and Atari was the runaway winner. A victory that proved to be
surprisingly short-lived. In 1982, Atari was the fastest growing company in American history.
Yet, by the end of 1983, they were one of the fastest falling, with yearly losses of over $500
million amidst the slide in parent company Warner's stock of almost 70%. In 1984, Atari was solved,
with Warner receiving no cash for the sale, and never again would Atari be a leader in the video
game industry. So, what happened? Well, there is an obvious answer, but that only tells part of the
story. To really understand, we need to rewind a bit, and look at where Atari came from.
When Nolan Bushnell and Ted Damby decided to start a video game company together,
they both pitched in $250 of their own money to get the company off the ground.
By 1982, Atari had 10,000 employees, and were bringing in $1.7 billion in operating profits.
And this all happened in the space of around 10 years.
Atari had a colourful history, and some important people once worked there,
including Steve Jobs and Steve Wozniak, who left after a few years to start their own company,
by the name of Apple, as well as a young programmer by the name of Bill Gates,
who was fired for not doing enough work.
Atari also had a bit of a reputation for their relaxed office culture. You know,
flexible hours, informal dress code, the usual sort of thing.
As we had a lot of young people, we would constantly offer to throw a party if they hit quota.
And it turns out when you've got 18, 19, 20 year olds, they're much more interested in the party
than an extra, you know, 50 cents an hour. So we got known as a party company.
There's a story that if you walked by the Borega Street building and you breathed deeply
by the air vents, you'd get stoned because the pot smoking inside of it was so heavy.
This made them a popular place to work, but it didn't last. The gaming industry was at the
cutting edge of technology, which meant to compete and to win funding was necessary. Hence the move
to become a subsidiary of Warner in 1976, which brought in a $120 million cash injection and made
the 2600s 1977 release possible. Warner communications were a big business, however,
meaning they took a keen interest in how their new subsidiary was being run.
To this end, Warner brought in an experienced business exec, Ray Kassar, in 1978 to help with
marketing and commercialization. But as the influence of Warner grew, so too did conflict
with then CEO Nolan Bushnell, particularly over Bushnell's desire to continue to invest in research
and development for new technologically superior consoles. This ultimately led to Bushnell's
departure in late 1978. He would go on to found American restaurant chain Chuck E Cheese,
and Atari went on to soaring new heights, now with a more professional work culture under the
oversight of Kassar. Conflict wouldn't end here, however. Atari was succeeding, but they were
also changing, and not everyone was happy with their new direction. In 1979, four experienced Atari
programmers, David Crane, Larry Kaplan, Alan Miller and Bob Whitehead, resigned over Atari's policy
to not credit or provide royalties to the actual game developers. These engineers would create a
software program that would result in twenty, thirty million dollars in sales, and they were
making this little paltry salary and they figured, gee, I'd like to get a penny or two or three
per each cartridge. So we go to the president of Atari and point that out, and he said to us,
and I'll quote, he said, you are no more important to that game than the person on the assembly line
who puts it together. So they figured, if they weren't going to get credit for their work, why do
that work for someone else? Which led them to form their own company, Activision, the first ever
third party developer who made sure to emphasize the role of game creators on their products.
Atari responded like the true corporate powerhouse they were at this time, and sued Activision,
but they weren't successful at preventing them from producing games for Atari consoles and instead
a deal was agreed where the third party company would pay a portion of their sales as a license
fee to the console owner, a system that's existed ever since. Activision were only the start of
Atari's problems though. Other senior developers would also jump ship, but it was in early 1982
that marked the beginning of the end when Atari tried to repeat the earlier magic of their space
invaders and asteroids arcade ports with Pac-Man for the 2600. The problem was Pac-Man was no
space invaders or asteroids. Released originally in 1980, it ran on 1980s arcade technology
with vibrant color, smooth animations and high quality sound, all things for now dated 2600
struggled with. The result is a port that will forever live in infamy as one of the worst of all
time, a Pac-Man in name and not much else. Still, Atari's disappointments that year were only just
getting started. November saw the release of the 5200, Atari's next generation home system
which kicked off the 16-bit era. Unfortunately, despite being early to the market, there wasn't
much else the 5200 had going for it. Its initial price point of $270 was high, it didn't originally
come with backwards compatibility, there were few new titles released for it, and the controller was
widely criticized. And while its graphics and hardware were a step up from the 2600 and other
8-bit systems, the upgrade they provided was small and poorly shown by the few titles the 5200 had
available which tended to just be ports of 2600 games. Nolan Bushnell is reported to have left
Atari over disagreements with the direction of future console technology, and the commercial
failure of the Atari 5200 seemed to vindicate his stance. Finally, to top Atari's year off,
there was E.T. which released just in time for the holiday season.
E.T. has regularly been called the worst video game of all time.
This may be rather harsh, but history is rarely known for its kindness.
Still, worst of all time or not, E.T. is certainly symbolic and it's a perfect example of a hubris
that brought Atari's downfall. Atari had acquired a license, at high cost, for an E.T. game following
the success of the movie, but they wanted the game released in time for Christmas to take advantage
of increased holiday sales. This meant a development time of only a few months, and so they brought in
experienced developer Howard Scott Warshaw, who had just completed the ambitious and critically
acclaimed Yars Revenge. Warshaw embraced the short development time as a unique challenge
and tried to do the miraculous. He failed, and the result is a confusing adventure game that
had little connection to the original movie beyond its strangely shaped protagonist.
This didn't faze Atari, however, who skipped audience testing to save time and produced
an optimistic 5 million copies for retailers, only a fraction of which ended up selling.
E.T. was a bad game, but in this way it wasn't unique. In fact, after the success of Activision,
more and more developers entered the third party space. This was before game developers needed
a license to publish games on consoles, which created a free-for-all where quality was a
much smaller concern than profit, and where accusations of industrial espionage and reverse
engineering became rampant due to third party developers lack of familiarity with the hardware
of the system they were designing games for. By the start of 1983, at least 100 different
companies were developing games for the 2600 in a rush to cash in on the now booming video game craze.
In the process, quality control was often ignored, and in this period consumers had
few ways to differentiate between good and bad games before it was already too late.
As the market continued to become increasingly saturated with low quality games, retailers
were forced to discount stock to try to get rid of these games, encouraging consumers to continue
purchasing the now discounted bad games, leading to a loss of consumer confidence,
which even video game titan Atari wasn't exempt from. In fact, Atari likely deserved more of the
blame than anyone. Pac-Man for the 2600 was a bad game, but it was also the best selling video game
of all time, and even then Atari still produced far more copies than they ultimately sold.
And so, with a saturated market, little quality control, rapidly depleting consumer confidence,
and even more emerging competition from the home computer market, the gaming industry did the
inevitable and sunk spectacularly in the great gaming crash of 1983.
To understand the scale of this disaster, it helps to look at the numbers.
In 1983, home video game revenues peaked at $3.2 billion. By 1985, this number had fallen to
$100 million, a decrease of almost 97%. Once quarterly earnings started to reveal
the degree to which games had stopped selling, investors started panicking,
cashing out their gaming stock and effectively ending the supply of new money that had previously
supported the once growing market. As a result, many newer gaming companies went bankrupt.
Mattel, Coleco and Magnavox all discontinued their gaming lines entirely, and Atari was sold
for almost nothing, with hundreds of thousands of unsold games being famously dumped and buried
in a landfill in New Mexico. Arcades still saw some popularity, but they once more entered into
a period of decline, with the golden age having now come to an abrupt end.
And so, video games were officially over, being once more seen as nothing but a short-lived fad,
not so different from other popular toy lines that burn bright and then burn out once consumers
get bored of them. And this is how they would continue to be seen in America for the next
two years, until a new name entered the scene. While the American gaming industry crashed and
burned in 1983, across the Pacific in gaming's second largest market, the year would be remembered
for a different reason. The release of the Nintendo Family Computer, also known as the Famicom.
It wasn't until 1985 that Nintendo would bring their new console across the Pacific
and rescue the American gaming industry, but for Nintendo, the story starts much earlier.
Founded in 1889, Nintendo began life as a playing card manufacturer before diversifying
into tabletop games and eventually electronic toys. Their first foray into video games came from
making light gun accessories, but in 1977, they went a step further with the release of the Color
TV game, a Japanese-only dedicated home console inspired by the success of Magnavox and Atari,
which contained Pong in six different variants. In Nintendo's defense,
at least they paid Magnavox for a license, unlike most. The Color TV game sold well,
thanks in part to its modest price, and Nintendo went on to reduce several more variants,
all for their home market of Japan. They also entered the arcade space with a game based on
the board game of Fellow. It was during this period that a fresh-faced 25-year-old by the name of
Shigeru Miyamoto joined the company, but Miyamoto would not be Nintendo's first gaming superstar.
That honor belongs to one Gunpei Yokoi, head of Nintendo's Research and Development One Division
and creator of the iconic handheld Game & Watch. Apparently inspired by Yokoi watching a board
businessman playing with an LCD calculator on a train, the Game & Watch was an early dedicated
handheld with an LCD display and fixed animations. It also pioneered the D-pad. Inexpensive and long
running, the Game & Watch's sales blew most early handheld competition out of the water,
selling more units than even Atari's 2600 over the course of its lifetime,
but Nintendo was only just getting started. After several less successful arcade attempts,
the company's next big break came with 1981's Donkey Kong. Designed by Miyamoto with the oversight
of Yokoi, it was originally developed as a way to reuse old cabinets of Miyamoto's previous
and rather less successful radar scope. Donkey Kong took inspiration from Popeye and featured
the titular ape as the game's barrel-throwing antagonist alongside the player-controlled
Jump Man, who would go on to become one of the most recognizable characters in the world
after he was renamed to Mario. Donkey Kong was one of the earliest platformers
and the first game to feature jumping. It also broke new ground in narrative design,
featuring a complete storyline told entirely on screen, with Donkey Kong kidnapping Mario's
girlfriend, who the player then works to rescue before finally watching the two be reunited.
It was also a massive hit, establishing Nintendo's name internationally and creating a bid and war
between console rivals Atari and Coleco over the rights to publish it. Nintendo would follow
Donkey Kong with multiple sequels, but it was their return to home consoles that would change the
gaming industry forever. At first glance, the Famicom didn't look that remarkable.
It was an 8-bit system in a time when Atari had already given the world 16 bits of power,
and home computers were quickly becoming more powerful still. Yet, the Famicom did have a
dedicated picture processing unit in addition to its CPU, as well as a reasonable launch price
of just under 15,000 yen. Its greatest asset would eventually be its game selection,
but an impressive library took time and the Famicom's first year on the Japanese market
saw it get off to a slow start. It would pick up momentum eventually however,
and the great gaming crash of 1983 didn't have nearly as large an impact on Japan
as it did America, meaning by the end of 1984, Nintendo's family computer had become the
best-selling console in Japan and was well on its way to becoming Nintendo's greatest success yet.
This still left Nintendo's toughest challenge though, America, which in 1985 was still reeling
from the earlier crash with confidence in video games at an all-time low. To get retailers to
even stock their console, Nintendo had to make buyback deals in case of poor sales,
but that still left the even bigger problem of selling the console in the first place.
I mean, how do you sell a gaming console to a country that doesn't want anything to do with
video games? Well, Nintendo's answer was to pretend that it was something different.
The first to see, to touch, to play with Rob, the extraordinary video robot,
he follows the commands you put on screen to help you tackle even the toughest challenge.
Will you be the first to raise the incredibly accurate zapper and play games like Duck Hunt
and Hogan's Alley, the first to build a library of game packs like Kung Fu and Golf,
even games like Excite Fight that you program yourself? Will you be the first to get all this
in one package? The Nintendo Entertainment System, where video technology is more than a game.
Enter the Nintendo Entertainment System. Not just a video game console, but something brand new.
And it wasn't just about the name. The NES was designed to look like a video
cassette recorder rather than a console. Its cartridges were rebranded as game packs,
and it shipped with several accessories like the robotic operating buddy Rob and the iconic
light gun, which marketing made sure to emphasize as much as possible.
Nintendo's work didn't stop here though, as they had an entire gaming market to rebuild,
and so the number one priority became restoring consumer trust in video games.
To do this, they strictly controlled third party development, putting limitations on companies
to avoid the oversaturation that had plagued Atari. In Japan, this led to a growing bootleg
market, so with the NES, Nintendo added a special lockout chip to combat this, while creating an
official Nintendo seal of approval to allow buyers to identify Nintendo licensed products.
And as for those products, Nintendo also paved the way for their console launch,
with arcade releases of the same games through the Nintendo vs System in 1984,
which was built on similar hardware to the Famicom. This not only helped spread Nintendo's name
before the release of the NES, but also established a trend of console titles matching the quality of
their arcade counterparts to try to repair the damage of games like Pac-Man for the 2600.
Nintendo even shifted to using in-game like box art in place of the detailed yet dissimilar
artwork of so many earlier games. All this had an impact, but Nintendo had one final ace up their
sleeve, which came once more from the mind of Shigeru Miyamoto and would mark the arrival
of gaming's next killer app. Released in the same year as the NES, Super Mario Bros set a new
standard. For years, players had grown accustomed to games taking place on a single screen,
and then in came Mario, with side-scrolling platforming action across multiple screens,
multiple levels, multiple environments, and all with different power-ups,
boss fights, and jaw-dropping hidden secrets. To top it all off, Super Mario featured precise
controls that took full advantage of Nintendo's new D-pad controllers alongside detailed,
vibrant visuals and one of the most iconic soundtracks in video game history from composer Koji Kondo.
Mario even had a strong sense of identity. By this point in time, a lot of games had featured
space combat, sport simulations, and fighting, but Mario offered something genuinely unique.
You played as a shape-changing, mushroom-eating, turtle-stomping Italian plumber who warped
between worlds while searching for a princess, and somehow the concept worked, with a childlike
charm that appealed to all ages. And so, Super Mario Bros was a landmark title
that kicked off the greatest selling video game franchise of all time,
but it's not like the NES only had Mario. In fact, only a year and a half later,
the Nintendo Entertainment System would see the arrival of another gaming legend,
once more from the mind of Miyamoto.
The legend of Zelda did to action-adventure games what Super Mario Bros did to platformers.
With a massive fantasy world fully open to players and packed with secrets,
Zelda offered a sense of adventure and freedom to explore never before seen in a console game.
It was also the first console game to feature a save system, thanks to battery-powered memory
built into the game's cartridge, which helped support its more ambitious design.
And once more, the legend of Zelda was a huge critical and commercial success.
As time went on, the NES's impressive library grew and grew, launching many of gaming's most
loved series and establishing many of the rules of game design that would continue to be seen
for years to come. Castlevania brought action-packed Gothic horror, Metroid brought atmospheric
sci-fi, Mega Man offered alternative high-quality platforming action, Metal Gear helped popularize
the stealth genre, and Dragon Quest and for later Final Fantasy established the Japanese
console role-playing genre. The NES was also well supported by high-quality arcade ports,
like Ghosts and Goblins, Contra, and Double Dragon, as well as a range of Nintendo original
sports games, providing even more diversity. Basically, the NES had everything, and it conquered
the American and later European console markets, getting the gaming industry back in business in
the process. This didn't happen overnight, and the damage done to the American market
really was significant, but by 1989, the console market had finally bounced back
close to earlier levels, with consumer confidence largely restored.
Over its lifetime, the NES and Famicom sold around 62 million units, which was more than
double the Atari 2600, making the NES the best-selling console of the time and still
one of the best-selling to this day. And with Nintendo's console supremacy and so many American
companies having exited the market following the earlier crash, Japan was able to take center
stage of the gaming industry, with Nintendo's name becoming synonymous with video games.
Still, even if Nintendo had taken a sizeable lead, they wouldn't remain unchallenged for long,
as another Japanese company was about to make their attempt at beating Nintendo
at their own game.
Sega was a big name in arcades. They entered the Japanese home console market at the same time
as Nintendo, when the arcade Golden Age was starting to wind down. Their first console,
the SG-1000, released in 1983 on the very same day as Nintendo's Famicom,
but it would struggle to compete with Nintendo's superior system,
likely as a result of Sega's inexperience with console development.
In 1985, it was redesigned as the Sega Mark III, which released in North America
in 1986 as the Sega Master System.
The Master System went head-to-head with the NES, while copying much of its marketing strategy.
In terms of hardware, it had a clear advantage, with a wider color palette
and a small edge in raw power, but this wouldn't be enough to make up for its biggest weakness,
its lack of third-party support.
To publish games on the NES, licensees had to agree to a strict two-year non-compete clause
that didn't allow them to bring their titles to Nintendo's competition.
This left the Master System with a much smaller library,
and while Sega were able to bring their own first-party arcade titles,
including the fast-paced Rail Shooter Space Harrier and Iconic Racing Game Outrun,
two of the best arcade titles of the mid-80s, this ultimately didn't prove to be enough,
and even these games were still inferior on console than as arcade originals.
This ensured that Sega failed to come close to matching Nintendo in Japan or North America,
although they did do enough to overtake a flagging Atari,
who, under new management, had released their own third-generation console, the 7800,
a system notable for its innovative backwards compatibility, and little else.
Still, by this point, gaming was global, and so Sega turned their attention to other markets.
In Europe, the NES had failed to achieve the same level of success as in America,
with Nintendo taking a more hands-off approach and leaving the distribution to other companies.
It also arrived notably late, with most countries not seeing the NES until 1987.
This was the same year Sega brought over the Master System,
and with no more headstart and Sega's strong reputation as an arcade developer,
Sega started to beat Nintendo for the very first time.
By 1990, the Master System was Europe's best-selling console, and Sega didn't stop at Europe.
In fact, a deal with toy and electronics company Tectoy
saw Brazil become the Master System's single best-selling country after its release in 1989.
By contrast, the NES didn't come to Brazil until 1993,
giving Sega a unique chance to capture a by no means insignificant market.
Sega wouldn't be the only new challenger amidst the now once more growing gaming industry, however.
16-bit power, arcade quality graphics, and an unbeatable selection of titles.
We'll show you what's available now, and sneak previews of what's coming.
Hudson Soft and NEC's TurboGrafx 16 released in 1987 in Japan under the name PC Engine,
and in 1989 in America and Europe.
The console was so eager to let consumers know that it was a 16-bit rather than 8-bit system
that it included it in the name, kicking off what is sometimes referred to as the Bit Wars,
as well as the fourth generation of home consoles.
As the first truly next-gen system, the TurboGrafx did boast some impressive hardware,
with a higher resolution and expanded colour palette to go alongside its 16-bit GPU.
This was matched with some impressive sales in its home region of Japan,
with the system getting off to a flying start and beating out the NES in its year of release.
Unfortunately, this strong performance wasn't replicated in other regions,
with poor marketing and a delayed release leading to commercial failure in North America
and cancelled plans in Europe.
Still, for Sega, this new competition represented a clear threat, and so, in response,
they immediately began working on their own 16-bit system.
It was finished one year later, long before Nintendo would have a 16-bit system themselves,
and it turned out to be Sega's best console yet.
The Sega Genesis, otherwise known as the Mega Drive outside America,
released in 1988 in Japan, 1989 in America, and 1990 in other regions.
And with it, Sega now had a system powerful enough to do justice to their expansive arcade
library, as well as a clear technological lead on their main rival, Nintendo.
They even had a new marketing slogan.
And so, heading into the next decade, the console war was finally heating up,
with one of gaming's greatest rivalries about to unfold.
The only thing Sega still needed was a killer app of their own that could finally go head-to-head
with Nintendo's popular mascot, Mario.
But for Nintendo, there was no reason to panic just yet.
As the end of the 80s saw them release a new system of their own,
one that would go on to outsell all of their previous systems combined.
Nintendo's Game & Watch was a success, but its LCD technology
limited it to one game per system.
For Gunpei Yokoi's next console, he wanted to create a handheld device
with interchangeable games that offered an NES-like experience on the go.
This was the result.
The Game Boy released in 1989 and was around in various iterations for the next 20 years,
during which Nintendo dominated the handheld market the entire time.
It did require four AA batteries, but this was a price players were more than happy to pay.
It also lacked a backlight, which was more of an issue.
A contrast dial was included, but this still meant the system was unplayable without a light source,
and even in a well-lit environment, the grey and green colour scheme
didn't provide the best clarity or general aesthetic.
To make matters worse, the Game Boy released against heavy competition,
with three other handhelds also vying for dominance at the same time,
Atari's Lynx, Sega's Game Gear, and NEC's Turbo Express.
All three of these rivals blew the Game Boy away in technical capabilities,
featuring not only a backlit screen, but also full colour and far more impressive graphics.
By comparison, the drab greys and greens of the Game Boy must have seemed rather pathetic,
a sentiment held by many even at Nintendo during its development,
where the Game Boy had earned the nickname DUMMER GAME,
a pun which basically translates to BAD GAME.
In the end though, the Game Boy wound up punching far above its weight,
and easily surpassed all competition, proving that in the constantly advancing
battleground of cutting-edge technology that is the gaming industry,
sometimes having the best specs isn't what matters the most.
You see, in return for a less technologically advanced product,
Nintendo offered something at a more affordable price,
with a much longer battery life, and a durable, more child-proof design.
If success was down to critics, the Game Boy would have come last.
In reality however, success is decided by the consumer,
which in this case often meant parents.
And it turned out that in this example, practicality and affordability beat superior
technology, and it wasn't even close.
For Gunpei Yokoi, this must have been the ultimate vindication of his design philosophy,
which has been translated as lateral thinking with withered technology,
i.e. achieving success through finding new and innovative ways to use cheap or mature technology.
In an industry obsessed with the cutting-edge,
this has been successfully employed by Nintendo time and time again,
and despite Yokoi's unfortunate death in 1997,
his philosophy can still be seen within the company to this day.
Of course, this is only part of the Game Boy's success story,
because it also had another advantage over the competition.
Designed inside the Soviet Union by Alexei Pachinov in 1984,
Tetris began life as more of a benchmark than an actual video game.
In a time long before anyone could ask, can it play Crysis,
computer engineer Pachinov had to settle for developing his own games,
as a way to test the capabilities of the latest hardware.
The idea for Tetris came from trying to recreate the puzzle game Pentomonos,
when Pachinov had the idea to make the pieces descend vertically.
The lack of a graphical interface on the Electronica 60 Pachinov was working on
did pose a slight problem, but it wasn't enough to stop Tetris' core idea from being enjoyable,
and soon Pachinov was sharing his creation with his friends,
and eventually making an IBM PC version with the help of a talented 16-year-old school student.
From here, Tetris spread like a contagious virus, first infecting Pachinov's office,
then the whole Academy of Science, before moving across Moscow,
and then beyond, doing what few Soviet citizens of the time could do and crossing the Iron Curtain.
This kicked off a race from multiple eager parties to acquire the rights of the game
for broader distribution in the West.
The only problem with the situation was that the Soviet Union didn't really believe in copyright law,
outside of everything should be controlled by the state,
leading to confusion over who owned the rights of the game,
and a complicated battle between different competing parties,
with plenty of buying and selling of rights that sellers didn't actually own.
The ultimate winner was game designer Henk Rogers, who acquired the rights for Nintendo.
The original plan for the Game Boy was to sell it alongside the new Mario title, Super Mario Land,
but Nintendo was so impressed by Tetris after Rogers showed it to them
that they followed his advice to use it instead,
in the hopes that its simple, addictive nature could transcend experience, age, and gender barriers
to be a game that everyone could enjoy.
And it was.
At first glance, Tetris might seem similar to popular arcade games of 10 years earlier,
but with no concerns about maximizing quarters,
Tetris considerably lowered the initial difficulty,
and this did result in a game that anyone could play,
as it eased people into the experience,
while naturally building tension and excitement as the game progressed.
Tetris required fast moment to moment reactions,
as well as broader, more strategic planning,
and when players made a mistake,
it was obvious where they went wrong and what they could do better next time.
For the pick up and play Game Boy, it was a perfect match,
and it went on to sell tens of millions of copies on Nintendo's new system alone,
as well as many millions more across PC, NES, and later mobile versions,
making it, potentially, the best selling game ever made, even to this day.
This helped the Game Boy get off to a strong start right from its launch,
which meant as the 80s grew to a close,
Nintendo retained their dominant position as leaders of the gaming industry,
having far surpassed Atari, and held off a new challenge presented by Sega.
All this time, however, as console and arcade giants froze and fell,
another section of the industry experienced its own story,
one with a different start in place, different games,
and a very different impact on the future.
World War II was kind of a big deal.
In addition to deciding the fate of the global population
and shaping the modern world as we know it,
it also saw rapid technological advancement that would pave the way for something equally important,
the home computer.
Originally, computer game development was limited to only the most enthusiastic hobbyists.
This started in the early 70s, with one of the first and most well known examples
being 1971's Star Trek, a text-based strategy game where players took control of the USS
Enterprise to hunt down Klingon warships.
Other early examples include Oregon Trail, a text-based strategy game best known
via its later incarnations, 1976's Colossal Cave Adventure, a text-based adventure game
inspired by real-life caving and Dungeons & Dragons, 1977's Empire, a turn-based war game
inspired by the board game Risk, Zork, also in 1977, and also a text-based adventure game
inspired by Colossal Cave Adventure, and Mud or Multi-User Dungeon, a text-based
real-time multiplayer role-playing game that helped pioneer the MMO genre in 1978.
Of these six examples, all except Colossal Cave Adventure were created by students.
In fact, Star Trek was created by a high school student who had started hanging around the
University of California Irvine's computer lab in his free time. So, for computer games,
it was teenagers who led the way. Maybe due to the younger generation's inherent creativity,
but also because computers were so expensive that university students were some of the only
people who had reliable access to them. This started to change in 1977 with the arrival of
not one but three more affordable home computers, the Commodore PET, the TRS-80, and the Apple II.
If you want to know what affordable means in numerical form, well, the most successful of these,
the Apple II, cost $1,298 at release, which is over $6,400 after inflation.
So, maybe not affordable for most, but it was a start.
These early computers tended to ship with some games included, but they also came with something
else. Basic, a general-purpose programming language that gave computer owners not just the
ability to play games, but also to make them themselves. And they did.
The best-known example of this is Sierra Online, founded by husband and wife duo Ken and Roberta
Williams, after playing Colossal Cave Adventure and deciding to make their own similar game on a
family member's borrowed Apple II. The result was the first ever graphical adventure game,
Mystery House, a murder mystery-focused game inspired by Agatha Christie novels and one of
the earliest examples of both graphics in a PC game and horror in any video game.
Sierra Online went on to have numerous hits, including King's Quest, Leisure Suit Larry,
and Quest for Glory. Another example is a Calabeth, the first ever computer role-playing
game made by a teenage Richard Garriott in 1979, originally on his school's mainframe PC and
finished on an Apple II bought by his father. Garriott sold the game at his local computer store
in a Ziploc bag with a cover sheet drawn by his mum, until a copy of the game made it to
California Pacific Computer Company who signed the deal with Garriott to take over publication
and sold 30,000 copies. The founders of Codemasters, the Darling Brothers, have a similar story,
where they became wealthy and established game developers by the ripe old age of
16 and 17, selling games by mail order from their bedroom.
Arcade and console gaming became corporate almost as soon as it came into existence,
but by contrast the computer gaming industry was heavily grassroots, with hobbyists, bedroom coders,
and literal children often leading the way, laying the foundations for the entire industry
and proving just what these new home computers were capable of. This did mean the PC scene was
much slower to monetize their efforts, with piracy, known at the time as sharing, being
such common practice that many people weren't even aware it could be considered wrong, and what
started as sharing between friends quickly evolved into selling bootleg copies for a
fraction of the original price. The differences between computer and console gaming doesn't
stop here though. From the higher age of the average player, to the higher price of the systems,
to the vastly different control schemes, to regional differences with PC gaming particularly
flourishing in Europe, the same region where consoles caught on the slowest, PC gaming basically
grew up independently to the rest of the gaming industry, and nowhere is this more evident
than in actual game design. Arcade games started off simple. They were highly replayable and
fiendishly challenging in large part due to their monetization, and they tended to focus on skill
based gameplay experiences that came alongside arcade specific features, like high scores and
lives. Console games then grew out of this foundation, with Atari and Sega owing much of
their console success to their arcade ports, and the NES's library being packed with games which
emulated many aspects of popular arcade titles of the era. PC games however were quick to embrace
complexity. Text adventures like Zork gave players a staggering amount of freedom in their text
inputs, while mods did similar while adding in other human players as well. 1980s genre pioneer
Rogue offered a huge randomly generated world for players to progress through. Groundbreaking
RPGs like 1981's Wizardry or Ultima provided deep dungeon-crawling overworld spanning adventures
with plenty of statistics, classes and items. In many of these titles gameplay seemed to take a
backseat to trying to simulate a rich virtual world for players to become immersed in. Many of
these games were still highly challenging, but less emphasis was placed on constant failstates
and replayability, with difficulty instead often coming from depth. And low fidelity visuals,
be they text, ASCII or spreadsheet, were often embraced as a way to provide greater freedom,
and maybe as a way for programmers to avoid learning to draw or hiring artists. In time,
many of these differentiators would become less clear as the borders between PC and console would
gradually erode, with Japanese role-playing games being an obvious example of cross-pollination.
But this process occurred slowly, and many of these same differences between computer games
and their console cousins are still evident today. Still, as the 80s evolved, so too did the home
computer, with systems becoming more affordable amidst rising competition. The next generation of
computers would include the ZX series from Sinclair, NEC's PC series, the Atari 8-bit family,
the BBC Micro and the Amstrad CPC. The best selling of these was the Commodore 64,
which released just under $600 and holds the world record as the highest selling single
computer model of all time. At the same time as this, computing heavyweight IBM launched their
now iconic personal computer series, popularizing the term PC. These came with Microsoft's
DOS operating system and supported open software development for the first time.
Before this, all home computers were incompatible systems, meaning all programs had to be ported
to specific systems to meet the individual hardware requirements of each one. The IBM PC and its many
subsequent clones instead allowed software to be ported much more easily, which ultimately
greatly benefited game development. Hardware wasn't the only thing improving in this period,
though. By the mid-80s, PC games were also making impressive strides. 1984's KingQuest from Sierra
showed the evolution of the graphic adventure genre with an animated protagonist and interactive
world. Space Sim Elite broke new ground by adding advanced 3D graphics and action gameplay into an
incredibly deep and open space sandbox. Ultima 4 in 1985 set a new standard for roleplaying games
by moving them beyond their established dungeon crawler formula with a more expansive world,
a richer, more nuanced story and a new moral choice-driven approach to game design.
1987's Dungeon Master on the other hand evolved the dungeon crawler into the realm of 3D with
added real-time combat elements. The 80s also saw the merging of entertainment and education in
several fondly remembered edutainment games. The 1985 version of the Oregon Trail taught
children history, as well as the dangers of dysentery, typhoid and rivers. While the also
released in 1985, Where in the World is Carmen San Diego taught geography and the dangers of art theft.
Of course, the Oregon Trail was also an enjoyable strategy game with genuine emergent storytelling
and rogue-like random generation years before Rogue even existed. And Where in the World is
Carmen San Diego was a very well-made adventure game with strong replayability. In some ways,
the educational elements of such games were light, but that didn't stop them showing up in thousands
of schools, tricking children into learning, or tricking teachers into letting children play
games during school, depending on your perspective. Ultimately, edutainment computer games were a
trend which didn't really catch on, but they still serve as a valuable reminder of a time when
gaming was in its infancy and developers were exploring novel applications of this new exciting
medium. By the end of the 80s, the era of bedroom coders was coming to a close, with husband and
wife duos and pioneering teenagers soon being replaced or going on to found legitimate development
teams with professional work environments. One such company, Electronic Arts, would see a major
breakthrough within 1988's John Madden Football, a game which would not only kick off one of the
industry's largest sports series, but also showed the more mainstream direction PC games were now
heading in. 1988 also saw the release of Pool of Radiance, the first in a long line of Goldbox
Dungeons & Dragons RPGs, where after years of video games taking inspiration from D&D, they
were now finally starting to make that relationship official. With home computers continuing to become
more affordable, PC gaming ended the decade in a promising place. Its early years should perhaps
best be remembered as a time of innovation and enthusiasm. The computer gaming market started
small. Popularity was created through word of mouth and sharing between friends. Creators enjoyed
unprecedented levels of creative freedom, and monetization was often a distant afterthought.
Yet many of the games produced from this era were just as impressive as their console and
arcade rivals, albeit often in very different ways. Regardless, the impact of these games on the
industry was considerable, and gaming wasn't the only thing they impacted. In a time where the
personal computer was a brand new invention, computer games also played a significant role
in showcasing just what this new piece of technology could do, and in popularizing the
idea that a computer could be more than just a machine for work. And so, even if the history of
computer games stands somewhat separate to the rest of the industry, it still played a vital role,
and its importance would only grow as the years went by.
Gaming's early years were tumultuous, with lawsuits, bankruptcy and corporate takeovers
coming alongside innovations and breakthroughs, all while technology progressed at breakneck speeds,
leaving both software and hardware behind after only a few years. Within this chaos,
the video game industry rose, fell, rose, fell, and then finally rose again, and each time,
there was a connection to the quality of the games. First with the novelty of Pong, before the
industry got smothered in an abundance of Pong clones, then with the golden age of the arcade,
and Atari's move to bring that golden age to consoles, before a massive low quality,
cheaply produced games once more sunk Atari and much of the industry with them,
and then finally with Nintendo, bringing new life to the industry with their higher
standard of quality and groundbreaking flagship series. The evolution of the gaming industry
wasn't exactly smooth, but by the end of the 80s, video games had come a long way.
In 1972, there was Pong. In 1989, Prince of Persia on the Apple II used rotoscoping technology to
reduce smooth true-to-life animations. Shadow of the Beast on the Amiga combined detailed
sprite work with parallax scrolling to create stunning backgrounds. Final fight on arcades
up the level of detail and the amount of things on screen at once to seriously impressive levels,
and even on consoles, the new generation of 16-bit systems was starting to really show what those
16-bits could do, with games like Golden Axe providing much better visuals than was seen in
the last generation. And it wasn't just graphics. On the NES, developers experience with designing
games and their familiarity with the system was also showing dividends. Take the opening of the
famously difficult action platformer, Ninja Gaiden.
Sound, visuals and cinematic style are all combined masterfully here to create some really
compelling storytelling that draws players in and contextualizes what you actually do in game.
And it does all this in less than a minute. Meanwhile, on PC, games like the City Builder
SimCity or God Game Populous for once more treading new ground and creating new genres
that challenged how people thought about video games. Still, if you really want to see how far
games have come, no single title illustrated it better than the latest entry in Nintendo's most
iconic series. Widely considered one of the greatest games ever made and as close to perfect
as a sequel can be, Super Mario Bros. 3 is the pinnacle of the NES library and it still holds
up all these years later. Where some NES sequels went in bold new directions, not always with the
best results, the third game in the Super Mario series instead takes everything which worked
about the first title and does it bigger and better. From the new overworld to multiple new
power-ups, new enemies, larger levels, enhanced visuals, an expanded moveset, multiplayer and a
much longer runtime, it was a game which did quite a bit to impress. But what shined brightest of all
was its creative level design. Every world had its own theme, every stage felt distinct and new
things were introduced constantly, with the player never quite sure what to expect next.
Somehow in just three years since the first game, the team behind Super Mario Bros. 3 worked out how
to make the perfect 2D Mario game and then they did it. Games had come a long way but in the grand
scheme of things they were just getting started and the next decade would bring even more progress,
more change and more drama.
By the start of the 90s, the gaming industry was well and truly established.
With the launch of the Game Boy, Nintendo had cemented their status as the market leader,
but the aging Nintendo entertainment system was now facing tough competition from Sega's
next generation Genesis, setting the stage for one of gaming's most legendary rivalries.
Meanwhile, the increasing affordability of the personal computer was making PC games a more
serious part of the industry, while gaming itself was moving ever towards the mainstream,
with bigger budgets, bigger names and bigger controversies.
Overall though, the 90s was a time of rapid technological progress.
A bit over a decade, the 8-bit NES, a third generation console, was the best selling on the
market. Yet by the decades close, consoles were entering their sixth generation with 128 bits
of computing power inside much more advanced systems. Graphics were transitioned from 2D
sprites and pixel based to the brave new three dimensional world of polygons.
Cartridges were replaced by the much higher capacity CD-ROM.
Multiple new genres of games emerged, including the most popular of all time,
massive franchises were established, including the most successful of all time,
and game design experienced its own evolution as developers raced to keep up with the continually
changing times. Before all this however, the decade began with action, and while the fighting
between consoles might be what's most remembered about the 90s, arcades also had some important
fighting of their own to do first. Arcade growth came to a stop following the crash of 1983, but
arcades still made up a large part of the market afterwards, and they still continued to offer the
definitive video game experience by way of graphics and immersion, for those who had the quarters to
play. By the start of the 16-bit era however, their technological edge was narrowing, and even
arcade heavyweight Sega had started to move away from marketing itself as the company that
brought the arcade experience home, due to the perception of arcades losing relevance within
the industry. Most people probably expected the 90s to be the decade where arcade games quietly
disappeared, and maybe that would have happened, if not for the arrival of one game that changed
everything. Street Fighter II wasn't technically the first fighting game,
but you'd be forgiven for thinking it was. Reaching arcades in 1991, it represented a massive
evolution from Capcom's 1987 predecessor, and brought many of the staples people have come to
expect from the genre, including combo mechanics, precise joystick controls, and a focus on face-to-face
player-vs-player action. Before Street Fighter II, competitive video games were about high scores,
and so the competition was indirect, but Street Fighter II allowed players to go head-to-head,
making competition more active, exciting, and genuine. This was a major step for gaming and
paved the way for the concept of deathmatches, as well as more serious competitive tournaments
and games as a form of esports. For arcades, though, fighting games was simply a match made in
heaven, and they instantly brought new life to the scene. The social nature of arcades was a
perfect fit for a genre that was about being best and winning in style, and the popularity of Street
Fighter was explosive, causing a significant resurgence in arcades, before soon going on to
be one of the highest-selling console games of the next generation, and leaving an immediate
impact on pop culture. Street Fighter II was soon met with competition itself, first from
SNK's Fatal Fury in late 91, then from Midway's infamously violent Mortal Kombat in 92, before
arcades once more cemented themselves as the leaders of video game technology with the cutting-edge
3D of Virtua Fighter from Sega in 93 and Namco's Tekken in 94, with a whole host of other smaller
names springing up along the way. For a time, fighting games were the hottest genre in gaming,
and they made a lot of money in the process, with Electronic Gaming Monthly reporting that in Japan,
fighting games accounted for 80% of all gaming sales in 1996. Other important advancements in
arcades came from the move to 3D technology, which preceded the jump to 3D seen in the next
generation of consoles, and was in large part the result of a rivalry between Sega and Namco.
This can be seen in Virtua Fighter and Tekken, as well as a number of notable racing and lightgun
games, including Virtua Racing, Daytona USA, Virtua Cop, and House of the Dead from Sega,
and Ridge Racer, Point Blank, and Time Crisis from Namco. Konami's highly original Riven-based
Dance Dance Revolution in 1998 also proved highly influential.
Still, while arcades might have seen a renaissance in the early and mid 90s,
this was ultimately only enough to delay the inevitable, and as the decade grew to a close,
amidst a surging console market, arcades once more entered into a period of decline,
as the era of the arcade gradually came to an end. Today, arcades mostly exist as a nostalgic
relic of the past, but their importance on gaming history was enormous. For years, arcades were
the largest and most technologically advanced section of the industry, and in addition to producing
many of gaming's greatest titles, they also left an enduring impact on gaming culture.
Still, while arcades might have kicked off the decade's action,
it would be consoles that would take the fighting to a whole other level.
Sega Shot First
In addition to beating Nintendo out the gates in entering the fourth generation,
Sega was also responsible for making things personal by deliberately targeting Nintendo in
their marketing. Conventional wisdom might suggest that naming and by extension giving
publicity to your main competitor in your own marketing isn't the best strategy, yet
conventional wisdom isn't always correct. Genesis does may be the most famous gaming
commercial of all time, and it exemplified the spirit of the fourth generation. Taking
shots at Nintendo wasn't the only part of Sega of America's grand strategy, however. They also
sought to create a strong library of unique games which would appeal to Western audiences
and help make the Genesis stand out. Early examples of this can be seen with Michael Jackson's
Moonwalker and several celebrity-induced sports games like Joe Montana Football or Pat Riley
Basketball. These still didn't provide Sega with a true system salary needed, though,
and to make matters worse, their head start only lasted so long, as in November of 1990,
Nintendo's much-anticipated 16-bit system finally launched in Japan, alongside the latest entry
of one of gaming's now most iconic series, Super Mario World.
In a head-to-head comparison, there wasn't a lot to differentiate the two competing pieces of
hardware. The Genesis did have an advantage in pure CPU power, but the Super Nintendo boasted
more RAM, better sound quality, more on-screen colors, and more detailed sprites. The systems
were also similarly priced, meaning it was games that would be the ultimate deciding factor,
and it was here that Sega was about to make their most important play yet.
Mario had launched Nintendo into the home console market and was seen as central to Nintendo's
success, so Sega decided they needed their own Mario. Work on the character design started all
the way back in 1988 and went through multiple revisions, with Sega Japan and Sega America
working together and often disagreeing over the direction. Central to the design was the decision
to emphasize speed and to appeal to Western audiences, with designer Naoto Oshima even going
to New York to ask locals in Central Park their opinion on the design, which ultimately led
to a blue hedgehog by the name of Sonic. The emphasis on speed carried over to game design
with the team wanting Sonic's default speed to be equal to Mario's running speed and for his
running speed to be a great deal faster. This led to many animation and frame rate problems,
but the team stuck with the idea, not wanting to compromise their desire for players to go fast.
Levels were then designed to be large and non-linear to take advantage of the main
character's enhanced movement, while also once again being seen as a straight upgrade
to the levels seen in Mario. Finally, vibrant colors and a laid-back soundtrack were used
to give the game a strong sense of identity, and in summer 1991 the project was complete,
with Sonic the Hedgehog releasing worldwide about two months before the North American
release of the Super Nintendo. And so, at long last, Sega had their killer app.
Sonic was an immediate success, and with it, the Genesis outsold the Super Nintendo 2-1
in the 1991 US holiday season, with Sega taking a 65% share of the 16-bit market.
Meanwhile, in Europe, Sega performed even better, as the Super Nintendo wouldn't see an
EU'd release until mid 1992. This was one of the major differences between Nintendo and Sega at
the time, as while Sega were pioneering the idea of a global release for systems and software,
Nintendo were by contrast much slower at bringing their products to other markets.
In the end, this paid off for Sega and allowed them to take a clear lead over their rival for
the very first time, and this wasn't the only advantage Sega now had.
Their greatest weakness in the last generation, third-party support, would no longer be a problem
after Sega entered the market early and offered publishers much more favorable licensing terms
than Nintendo. They also worked hard to stay on top of hardware developments,
releasing the Sega CD, or Mega CD, in 1992 as an add-on for the Genesis.
The Sega CD wasn't cheap, and its sales numbers were relatively modest,
but it did offer a more high-end product to those who wanted it,
and the new CD technology provided much greater storage capacity, allowing for more detailed
audio and entirely new types of games like Bose focused on full-motion video.
Lastly, Sega also didn't slow down with their aggressive marketing.
It is debatable whether blast processing is even real,
but one thing everyone can agree on is that the Super Nintendo doesn't have it.
I mean, sure, technically, the Super Nintendo was the more advanced system,
but technically, the Genesis did have faster processing, so Sega made sure that's what they
focused on, and it worked. Still, even if Nintendo didn't have blast processing,
there was something they did have, which some might even say is more important.
Games. The Legend of Zelda, a link to the past,
released in America in 1992, and did for Zelda what Super Mario Bros. 3 did for Mario.
As a near-perfect sequel and one of the best games of the 16-bit era, a link to the past
expanded the overworld, expanded Link's moveset, expanded the dungeon design,
and then added an entire parallel dark world for the player to travel between that was similar
and yet completely different to the world they knew. The result was a landmark action adventure
game that set a standard for 2D Zelda still not surpassed to this day, and Nintendo didn't stop there.
Sequels were expected, but Nintendo had new ideas too, and they still had one of the best people
around for the job. Shigeru Miyamoto worked on several of the Super Nintendo's early
first party titles, including the new futuristic racing game F-Zero, and the technologically
groundbreaking rail shooter Star Fox, the first Nintendo game to feature polygonal graphics
through the use of an additional graphics chip included in the game's cartridge.
Released in early 1993, Star Fox went on to become one of the console's best-selling games,
although it was still surpassed by another new Nintendo franchise, 1992's Super Mario Kart,
which brought Mario and friends into the world of kart racing, kicking off their own subgenre in the
process. Still, Nintendo weren't the only one making good games. Sega may have courted many
third-party publishers, but some of the most important stayed loyal to Nintendo and brought
a number of impressive sequels with them. Square released Final Fantasy IV, known then as II outside
Japan. Enix released Dragon Quest V, which sold millions of copies even as a Japan-only title.
Konami released Super Castlevania IV, allowing players to once more take the fight to Dracula.
But most important of all was Capcom, who were at the time holding the rights to the
hottest game in the world, Street Fighter II. And while this game would eventually find its way
to both systems, it still came to the Super NES first, giving Nintendo a head start of more than
a year on what would go on to be the best-selling third-party title of the generation. And with
Street Fighter, Nintendo had the edge they needed. Holiday season 1992 and this time
Nintendo were selling more systems, allowing them to climb back into first place in 93.
Sega weren't done yet though, and the battle was about to get even messier.
Wasting no time after the success of their new best-selling game of all time,
Sega released the sequel to Sonic in late 92, featuring even faster gameplay alongside
reoccurring series Sidekick Tales. But it was their more adult titles that would truly define
the next year. Two major features of this era of console warfare were the idea that Sega had the
more mature system and games than the family-friendly Nintendo, and debates over which version of
specific games were better. These days, exclusives are what people most remember,
but plenty of titles came to both systems, sometimes with completely different variants,
kicking off endless arguments between fans over which system had the superior version.
Contra 3, Alien Wars vs. Contra Hardcore, Super Castlevania vs. Castlevania Bloodlines,
Teenage Mutant Ninja Turtles, Turtles in Time, vs. Teenage Mutant Ninja Turtles,
The Hyperstone Heist, and On It Went. Sometimes two competing titles would be pretty much
identical, but there were other times when there were big differences, and clear winners and losers.
For example, with Street Fighter 2, the Super Nintendo was king with not only a year head start,
but also a more suitable controller that had four face buttons alongside two shoulder buttons,
compared to just three face buttons on the original Genesis controller.
Other times, though, the Genesis reigned supreme, like with 1993's Aladdin where the Genesis version
was aided by real Disney animators, and went on to be one of the system's best-selling games.
And then there was Mortal Kombat, a game which combined arguments over Sega's more adult nature,
and arguments over which system had the superior version, and ended with Sega and Nintendo
sitting in front of a US congressional hearing on the dangers of violence in video games.
You see, much like Street Fighter before it, Mortal Kombat was a big success, but its graphic
depictions of violence and brutal post-victory fatalities attracted more attention than from
just players, kicking off a media frenzy and national conversation on the role of violence in
video games. When Mortal Kombat made its way from arcades to consoles, Nintendo opted to tone down
the violence by removing the blood effects, but Sega instead left them in as an unlockable option,
making the Genesis version the most desirable. Knowing that controversy might be on its way,
Sega created their own internal rating system ahead of Mortal Kombat's console release,
but this didn't prove to be enough, and by the end of 1993, the national conversation on
video game violence had reached US Congress, with both Sega and Nintendo being called up
to answer some difficult questions. A number of advocacy groups and senators spoke on the issue,
presenting a united front, while focusing heavily on Mortal Kombat and the strange Sega CD
FMB game Night Trap, and claiming that there had been a systemic failure of the gaming industry
to police itself. When it came to those speaking in favor of the gaming industry, however,
those present were anything but united, with Nintendo getting the chance to speak first
and wasting no time in putting the blame squarely on Sega's lack of internal policy.
In the past years, some very violent and offensive games have reached the market,
and of course, I'm speaking about Mortal Kombat and Night Trap, and let me say that for the record,
I want to state that Night Trap will never appear on a Nintendo system. Obviously,
it would not pass our guidelines. This game, which as you've indicated, promotes violence
against women, simply has no place in our society. Meanwhile, Sega began by defending itself in the
way that Sega fans at the time defended their favorite gaming system on school playgrounds,
i.e., by reminding those in attendance that the Genesis, unlike its competition, wasn't made for
kids. In recent days, the glare of the media spotlight on this issue has resulted in the
circulation of a number of distorted and inaccurate claims. The most damaging of these
distortions, in my view, is the notion that Sega are only in the business of selling games to children.
This is not the case. Sega also highlighted their new rating system,
but this received a lot of pushback, including from Nintendo's representative,
who seemed much more interested in attacking Sega than defending the gaming industry.
Let me make just a couple of other points. I can't sit here and allow you to be told
that somehow the video game business has been transformed today from children to adults.
It hasn't been, and Mr. White, who is a former Nintendo employee, knows the demographics as
well as I do. Furthermore, I can't let you sit here and buy this nonsense that this
Sega Night Trap game was somehow only meant for adults. The fact of the matter is, this is a copy
of the packaging. There was no rating on this game at all when the game was introduced. Small
children bought this at Toys R Us, and he knows that as well as I do. Ultimately, the senators
seemed unimpressed by gaming's great console war and decided to treat Sega and Nintendo in
much the same way that a teacher might treat two squabbling children. By giving them both a slap
on the wrist and telling them they better start working together to make things right.
So, gaming's fiercest rivals came together with other representatives from the industry
to create the Interactive Digital Software Association, which soon led to the creation
of the Entertainment Software Rating Board, or ESRB, who would produce a unified set of age
ratings for all commercial video games that retailers would then opt into enforcing,
something that has since become common practice across many different regions.
In the congressional hearing, Sega came under the heaviest attack, and following public outcry,
many retailers pulled the controversial Night Trap from their shelves. However,
when it came to actual video game players, this didn't seem to damage Sega's reputation at all,
and instead helped cement their status as the cooler, more mature gaming company,
ultimately leading to sales going up.
Holiday season, 1993, and Sega came out on top, retaking the overall lead in 94 with 55% of the
market. By 1994, Sega and Nintendo weren't the only ones dealing with controversy though,
as in the world of the personal computer, another violent video game was exploding in
popularity and leaving its own mark on gaming history.
In the early 90s, consoles might have received the most attention, and made the most money,
but PC was still the place to be for innovation, and it saw a number of big releases of its own.
LucasArts 1990 pirate-themed The Secret of Monkey Island set a new standard for the
graphical adventure genre with its user-friendly interface, sharp comedic writing, and a more
open game design but removed any sort of fail state and ensured players could never be locked
out of puzzles or the means to progress the game. In 1991, Sid Mears Civilization popularized the
4x genre of strategy games where you explored, expanded, exploited, and exterminated your
opposition while guiding a human civilization over the course of several millennia, a concept that
proved as expansive and original as it was addictive. Delphine Software's strikingly cinematic
action-adventure game Another World also released that year, leaving a lasting impression on many
who played it. Featuring a scientist fighting for survival on an alien planet, it placed a heavy
emphasis on presentation and told its story entirely without dialogue, creating a game that
felt years ahead of its time and would influence a number of important future titles, including
Ico, Metal Gear Solid, and Silent Hill. In 1992, Westwood Studios created the first
real-time strategy game with Dune 2, combining resource-gathering, base-building, and small-scale
warfare into an addictive new formula. Dune 2 would be followed by Blizzard Entertainment's
Warcraft Orcs and Humans in 1994 and Westwood's Alternative History, Command and Conquer in 95,
which together established the RTS as an important genre within gaming.
92 also saw the birth of the survival horror genre with Infograms Alone in the Dark, a game that
went beyond embracing horror as an aesthetic by deliberately disempowering the player to
heighten the tension and create a sense of dread. Its haunted mansion setting and cinematic camera
would also greatly influence future titles of the genre. Meanwhile, in the world of role-playing
games, the Ultima series continued strongly with its highly acclaimed seventh entry, The Black Gate,
but it was a spin-off from the main series that proved to be the most innovative RPG that year.
Looking Glass Studios' Ultima Underworld was the first role-playing game to feature first-person
action in a 3D environment. Described as a dungeon simulation by its creators, Ultima Underworld combined
the dungeon-crawling of genre grandfathers like Wizardry with cutting-edge action gameplay and
environmental design to create a brand new type of action RPG that would pave the way for the
immersive sim genre and many future titles. 1993 also saw the release of the visually stunning
graphic adventure game Mist from Cyan Inc, which took full advantage of the vastly expanded
storage capacity of the new CD-ROM to create its iconic picturesque 3D island while helping to
popularise the adoption of CD-drivers in the process. Mist's puzzle-solving gameplay was
quite slow, but its graphics were far beyond anything else at the time, leading it to become
one of the best-selling PC games of all time. Finally, in 1994, Mythos Games' sci-fi strategy
game XCOM UFO Defense combined turn-based tactical battles with real-time management
to create a surprisingly deep Earth Defense simulation unlike any other game before it.
While Origin System's ambitious latest entry into the Space Combat Wing Commander series
included extensive live-action FNB cutscenes that featured big Hollywood names like Mark Hamill
and Malcolm McDowell represent in a major step of PC gaming towards the mainstream.
Still, as important as each of these games were, none of them are what the early 90s are best
remembered for. For that, we need to go back to the start of the decade where a small group of
20-something-year-old developers were working for software company Softdisk in an office in
Shreveport, Louisiana. It was here that 20-year-old university dropout, John Carmack, was thinking
about a well-known problem in PC game development, how to achieve side-scrolling graphics on a computer.
You see, while computers were powerful in certain ways, they weren't designed specifically for
gaming, meaning there were some things consoles were just better at, like allowing for faster,
refresh rates. This meant PC struggled to redraw an entire screen at the same speed as a console,
making the iconic side-scrolling seen in so many of the most popular video games impossible.
Until Carmack had an idea. Why redraw everything on screen when you could instead
only redraw the parts that actually change? This became known as adaptive tile refresh,
and with it, Carmack was about to change the world of PC gaming.
First, Carmack and friends recreated Super Mario Bros. 3 step by step,
something people thought couldn't be done on PC.
They then sent the end result to Nintendo, who were admittedly impressed, but told them they
didn't want Nintendo games on anything except Nintendo hardware, meaning all their hard work
had been for nothing. So, next, they decided to make their own game instead. With no corporate
backing or funding, they decided to team up with shareware distributor Apogee Software
to create the side-scrolling platformer Commander Keen. It was completed in just a few months,
and saw great success through the shareware model. Then, once the first royalty check arrived,
John Carmack, John Romero, Adrian Carmack, and Tom Hall decided to quit their jobs at Softdisk
to found their own company, id Software. Carmack soon turned his attention to gaming's
new frontier, 3D, and once more encountered a similar problem to before, which was that
computers of the time were just too slow to allow for fast action in 3D environments.
id Software continued to create more shareware distributor games,
while experimenting with 3D, which allowed Carmack to iterate on his new 3D engine
by restricting the viewpoint to increase speed, using sprites and texture mapping to save space,
and so on. The end result was Wolfenstein 3D, which released in summer 1992 and allowed players
to mow down hordes of Nazis from a first-person perspective in 3D environments. Wolfenstein
was the first true first-person shooter, and its fast pace and smooth action made it hugely successful,
but it would soon be overshadowed by an even more impressive game.
In late 1993, Doom was a game which changed gaming.
Bigger, faster, and more violent than Wolfenstein, with visuals that looked years ahead of its
predecessor, Doom was an instant classic. For years, there was no FPS genre, there were only
Doom clones. It invented the deathmatch while helping to pioneer the concept of network multiplayer,
it spawned one of the most active mod scenes of all time, while greatly popularizing the
concept of modding, and it was estimated that by late 1995, it was installed on more computers
worldwide than Microsoft's latest Windows operating system. Doom was a gaming phenomenon,
and it only achieved that level of success because underneath its demon-infested exterior and
innovative technology was a really well-made game that used fast action, varied enemy encounters,
and complex level design full of secrets to create a formula that still holds up all these years later.
Doom also served as a perfect representation of what made PC gaming so different to consoles in
the early 90s. While Sega was desperately trying to market itself as the mature alternative to
Nintendo, games like Doom were providing a genuine mature alternative without even really meaning to,
and while consoles were embroiled in a clash of corporate giants and spending tens of millions
on marketing every year, Doom was self-published, distributed through shareware, and made by initially
just five guys, with two more coming on to help near the end. And despite this humble background,
it was still the most impressive 3D game the industry had ever seen, and it helped to create
the most successful genre in gaming. There have been independent games that have been phenomenally
successful since Doom, but it is truly rare for an independent game to be so successful
while also being technologically years ahead of its competition. The shareware distribution
model can also be seen as a forerunner to digital distribution. It worked by offering part of a
game as a free trial that anyone could download and play, and then asking players to pay to purchase
the rest. With no retailer or possibly publisher, it meant games sold this way had much more limited
reach, but the lack of any middlemen also meant lower prices for consumers and a larger cut of
revenue for the developer. And so it should come as no surprise then, but Doom's success
quickly turned its creators into multi-millionaires. Six months after Doom's release,
John Carmack owned two Ferraris. He was 23 years old at the time.
Its software would follow Doom with a sequel in 1994 before creating an even more powerful
engine to reduce their fully 3D Doom successor, Quake, in 1996, which took multiplayer to soaring
new heights with far more modes and built-in 16-player online capability. Quake was another big hit,
and with it, the era of online first-person shooter domination had arrived. But for consoles,
the mid-90s would also bring the next generation of systems, and it didn't exactly play out as
most people expected. 1994 was a big year for the gaming industry,
but its start was still dominated by the rivalry between Sega and Nintendo,
and the fallout from the violent video game debate. Sega had now edged back into the lead,
and this period would see several of the Genesis' best exclusive titles,
such as the beat-em-up classic Streets of Rage 2, the side-scrolling platformer Rocket Knight
Adventure, and the action-packed running-and-gunning Gunstar Heroes. 1994 would also see the release
of not one, but two new entries into Sega's hottest series, after Sonic 3 became so large
during development that it could no longer fit into a single cartridge, causing it to be split
into Sonic 3 and the separate Sonic and Knuckles. Nintendo had plenty of games of their own, however.
1993's Super Mario All-Stars remade the first three entries into the Super Mario Bros. series
with Super Nintendo-level graphics, and is noticeable as not only one of the system's
best-selling titles, but also one of the first examples of a massively successful video game remake.
In 1994, Nintendo had released one of the Super Nintendo's most highly acclaimed titles with
Super Metroid, which upgraded the formula of the original game while retaining its rich
sci-fi atmosphere and focus on exploration. Still, Nintendo's biggest game that year
came at the hands of another developer. Due to concerns about falling behind Sega,
Nintendo began looking for other studios to bring on board, leading them to approach the
UK-based Studio Rare, who had impressed them with their latest 3D model rendering technology.
Nintendo suggested they use these pre-rendered graphics to make a platformer and gave them
permission to use one of their oldest IPs to do it.
Even before Donkey Kong Country released in late 1994, its unique visual style brought it a lot of
attention and convinced Nintendo to heavily back it with a generous marketing campaign.
Even so, the game still exceeded expectations, setting the record for the fastest-selling game
of all time and going on to become the third best-selling game on the system.
It also wowed critics, impressing with its creative level design and timeless soundtrack
in addition to the striking visuals. Donkey Kong Country was the edge Nintendo needed
to pull back into the lead following a strong holiday season in 1994, but while high-quality
new titles ensured the Super Nintendo was still going strong, by this point,
Sega was starting to turn their attention to future systems.
The fifth console generation started early and saw heavy competition. The 3DO released in America
in late 1993 and boasted some seriously impressive technical specifications,
becoming the first system to fully take advantage of new CD-ROM technology.
It also launched with a retail price of $700, which is $1,400 after inflation,
and struggled to build a library of solid exclusives, meaning it ultimately struggled
to compete with later, better-known systems despite its strong launch.
And as for those later systems, two would go head-to-head in Japan in late 1994,
but this time, Nintendo wouldn't be one of them.
Sega began work on the next system early after realizing the importance of CD-ROMs
and having had promising results with 3D technology in arcades.
At the same time as this, they tried to maintain support for the Genesis by creating a second add-on
in addition to the Sega CD called the 32X that would bridge the gap between the two generations
and offer a cheaper entry into the 32-bit era.
Unfortunately, Sega struggled to produce enough games for either the Sega CD or the 32X to really
justify their high costs, meaning both pieces of hardware ended up as commercial failures
and might have damaged Sega's reputation with fans who did buy these add-ons due to their
perceived lack of support. Still, it was the next generation where the battle would truly be decided,
and so Sega went all out on developing a new system that could meet their lofty expectations.
While Sega was working on a 32-bit system of their own, however, a new entrance into the
video game scene was doing the same, and getting ready to change the console landscape forever more.
Sony was an electronics company founded after World War II and best known for their music
products like the Sony Walkman. Their first foray into gaming hadn't turned out very well.
A joint venture with Nintendo led to them producing the sound chip used in Bersupin Nintendo,
but they also worked with Nintendo on a CD-ROM add-on for the system. Before it could be completed,
however, Nintendo got cold feet over how much control the terms of the deal gave Sony, and instead
went to negotiate a more favorable deal with Sony's main rival, Philips. Not knowing this,
Sony went on to announce their Super Nintendo add-on at the Consumer Electronics Show in 1991.
It was called the PlayStation, and the very next day, Nintendo got up on stage themselves
and announced their new partnership with Philips, declaring the PlayStation dead in the process.
For Sony, this was a serious blow, and an even more serious betrayal,
something not taken lightly in Japan's honor-based business culture.
They responded by severing all ties with Nintendo, with many executives at Sony wanting
to end all activity connected to the gaming industry, which they had come to view as too
close to the toy industry to fit with Sony's established brand, in addition to being apparently
highly unreliable. Ken Kutaragi, the man in charge of the PlayStation's development,
wanted the opposite, however. Kutaragi was confident in his technology and proposed Sony
go it alone and create a standalone console, while using the company's poor treatment from
Nintendo to support his argument for why they shouldn't give up on the idea.
In the end, Kutaragi got his way, but he still had a problem. Not only was developing their own
system considerably risky, but Sony also had no actual experience in game development.
In the past, consoles had lived and died on the strength of their first party support,
which for Sony didn't even exist. So, not knowing anything about game development,
the team decided to stick with what they did know, hardware and business.
To this end, they designed the system from the ground up with a focus on 3D polygon graphics
and also set out to secure their desperately needed third party support.
To do this, they formed partnerships with established Japanese developers like Namco
and Konami, while purchasing the UK-based Cygnosis, who were best known for their widely popular puzzle
game Lemmings. This gave Sony their first in-house team, and it was through this that they would
stumble into their big break. Developers at Cygnosis found the current workstation-based
development kits to be overly expensive and impractical, and so they persuaded Sony to
instead use a PC-based development system, thus marking a major turning point for the console
industry, which had largely ignored PCs until then. PC-based development was not only cheaper
and more efficient, it also streamlined the process for third parties, while making porting
games between PC and Sony's new system much easier than before, something which proved crucial in
allowing Sony to compete with their more established rivals. As the console neared completion,
Sony decided to break from the tradition of using the company's name in the title of the system,
due to upper management's fear that failure could damage the company's brand,
and so the console would keep its original name. And in December 1994, the PlayStation launched in
Japan, just one week after the release of another new console, the Sega Saturn.
The two systems were similarly priced and offered similar performance, but it was
Sega who proved to be the early favourite, thanks in large part to the hugely popular
Virtua Fighter, which sold at a near one-to-one ratio with the Saturn at release. For the PlayStation,
it was Sega's big arcade rival Namco that would be the console's most important developer in its
first year, with Ridge Racer proving the most successful launch title and Tekken soon following
to give the PlayStation a 3D fighting game of its own. Still, exclusives wouldn't be the most
important factor in the new system's first years. As both systems prepared for their global release,
both companies came up with a plan to give them the edge over the other, which they would reveal
at the first ever Electronic Entertainment Expo in 1995, where 40,000 eager attendees packed
for Los Angeles Convention Center to get a glimpse of the next generation of gaming.
Sega went first and took to the stage to announce that due to high consumer demand,
the Saturn would release immediately, that very day, at select retailers.
This was four months before the expected September release date, and Sega hoped this
head start would give them the advantage. Unfortunately for them, however, other retailers
responded to this surprise announcement very negatively, with KB Toys even dropping Sega
from its lineup, and the early release also left the Saturn's launch lineup rather light on games.
Things quickly went from bad to worse though, because Sony followed with a surprise announcement
of their own, which came when SCE President of America, Steve Race, took to the stage to give
one of gaming's all-time greatest speeches.
$299 was $100 less than the Saturn, meaning the systems were no longer similarly priced,
and it turned out that while Sega's gamble backfired, Sony's instead paid off.
The PlayStation released on the 9th of September 1995. Two days later,
it had sold more units than the Saturn had in the last four months. The PlayStation's launch
was huge, and what's more, their easier-to-develop for system was about to start proving its worth,
as a number of former PC developers soon started releasing games for Sony's new console.
Notable examples include Platformer Rayman from Ubisoft in 95, Naughty Dog's soon-to-be-iconic
Crash Bandicoot in 96, which gave the PlayStation its first real mascot, and Tomb Raider from Core
Design, a graphically impressive action-adventure game featuring one of gaming's most recognizable
female leads. 96 also saw the release of another landmark PlayStation title from Capcom with
Resident Evil, which may be the single most influential survival horror game of all time.
All of these titles were big commercial hits, with Resident Evil, Crash Bandicoot, and Tomb Raider
each going on to sell over 5 million copies. The Saturn did have games of its own, with several
high-quality arcade ports, alongside some strong new first-party titles like Night's Into Dreams
and Panzer de Grune. Sony, however, increasingly seemed to have the rest of the gaming industry
on their side, and in time Sony's initial victory would turn into a slaughter. For years,
Sega tried to set themselves apart from Nintendo by appealing to an older demographic and marketing
their system as the more mature option, but now Sony was doing the same and beating Sega at their
own game. In 96, Sega responded to the growing pressure with the release of the Saturn Model 2,
which aimed to lower the retail price and cost of manufacturing. Sony, however,
replied by cutting the price of the PlayStation to $199, ensuring their lead just kept growing.
For Sega, the situation was starting to look desperate, and the competition would only increase
as an old friend of theirs would soon release a console of their own.
While the fifth console generation was getting off to an explosive start, Nintendo were quietly
enjoying the Super Nintendo's finest years. After a drawn-out and tightly fought competition
with the Genesis, where Sega had often come out on top, in its later years the supply of
new Genesis games slowed up, allowing Nintendo to finally pull comfortably ahead in a 16-bit market
that was still going surprisingly strong. This may have been partly because 1995-96
saw some of the Super Nintendo's best games. Super Mario World 2, Yoshi's Island, saw Mario return
with a stunning new art style and fresh Yoshi-focused gameplay. Rare followed up the monstrously
successful Donkey Kong Country with not one but two more sequels than almost matched the first in
sales. And RPG fans were spoilt for choice, with the cult classic Earthbound's idiosyncratic take
on modern America offering a setting unlike anything players had seen before, Square and
Nintendo's collaborative Super Mario RPG offering a Mario game unlike anything players had seen
before, and Final Fantasy's latest entry, 6 or 3 depending on region, providing the series'
rated and best-selling title to date. Lastly, there was also Chrono Trigger, which was developed by
a dream team that included the creator of the Final Fantasy series, the creator of the Dragon Quest
series and the creator of the Dragon Ball Manga. Chrono Trigger featured a time-travelling adventure
with multiple endings, lots of reactivity and some of the best visuals and music of a 16-bit era,
and it's often considered to be one of the best JRPGs of all time.
Still, as strong as the Super Nintendo's last years were, nothing can last forever,
at least not in the always evolving gaming industry, and development of Nintendo's next
console started as far back as early 1993. After their development of a CD-based system failed
to work out with either Sony or Philips, Nintendo formed a new partnership with Silicon Graphics
to bring their powerful supercomputer technology to the world of consoles.
The final product released in 1996 with the name Nintendo 64 to emphasize its 64 bits of computing,
but as powerful as Nintendo's new system might have been, it was about to face heavy competition,
and its main rival had a head start alongside one other key advantage.
Nintendo arrived late, but the N64 did have some things in its favor.
Nintendo's continued focus on affordability meant the system was able to match for new
prices of Saturn and PlayStation while offering much more powerful hardware.
With double the PlayStation's RAM, a three times faster CPU, and a much higher polygon count,
the 3D graphics of the N64 were simply a step above any of their competitors.
Nintendo also innovated with their controller by adding a thumb-controlled analog stick,
reminiscent of the joysticks of older gaming systems. The overall design might seem clunky
by today's standards, but it was an important step for the time, and analog controls greatly
complemented the move to 3D. The N64 also came with four controller ports,
which was two more than its rivals, and made it a great system for multiplayer games.
Nintendo's launch lineup initially looked less promising, with only two games available for
the North American release. This lack of games was what delayed the release to 1996,
and for most consoles, two titles would never be enough. Luckily for Nintendo, however,
one of those games was capable of doing some rather heavy lifting.
Super Mario 64 didn't just bring Nintendo's popular mascot into the realm of 3D,
it also shattered expectations for what a 3D platformer could be.
Before it, games of the genre had used fixed cameras, or just suffered from loose,
imprecise controls. But using the new analog stick, Mario was given full 360 degree movement,
with a detached adjustable camera and a rich array of movement options.
Levels were big and inviting, begging players to explore and experiment,
and the cartoony visual style was a great fit for showcasing the N64's high polygon count.
Basically, it was as strong a launch title as Nintendo could have asked for,
but it still wouldn't be enough. Because as impressive as the N64 and Mario were,
there was one thing the PlayStation did, which Nintendo didn't. And that was use CDs.
Nintendo's console was more powerful. It did have a higher polygon count and shorter load times,
but CDs had higher capacity, and for many developers, this was the most important factor.
The best known example of this was RPG Veteran Square, who had been a loyal ally to Nintendo
and had originally begun work on their next Final Fantasy game on the Super Nintendo.
As development progressed, Square soon realised they wanted to use pre-rendered video and
backgrounds, something that would only be possible with a larger capacity of a CD,
meaning creating the game for Nintendo's new console was no longer possible.
As development continued, Square invested more and more resources into the project,
resulting in an estimated development budget of more than $40 million,
making it the most expensive game ever made at the time. And it turned out,
Square's investment paid off.
Final Fantasy VII released in 1997 and was unbelievably successful.
Its use of full motion video and pre-rendered CG gave it a cinematic style which wowed audiences
and critics alike. Its setting and characters felt fresh and exciting in an era years before anime
had found mainstream popularity, and when most Japanese RPGs still didn't even get released
outside Japan. And its story and sense of adventure resonated with players at the time
in a way that few other games ever had. Final Fantasy VII sold over 10 million copies,
eventually going on to become the PlayStation's joint best-selling game of all time.
For Sony, it was the PlayStation's killer app, and they didn't even need to do anything to get it.
The higher capacity of CDs allowed for the use of FMV, as well as higher quality sound,
and even more realistic and detailed textures. And so, the N64 was the more powerful system,
in all ways except for the one which mattered the most. Nintendo weren't out of the competition yet,
because there was something they could do which Sony couldn't. Make their own games.
The N64 couldn't match the PlayStation's early sales, but it still got off to a solid start,
thanks in large part to Nintendo's strong pre-established reputation, and more games
were soon on the way. At the end of 1996, Mario 64 was joined by Mario Kart 64, bringing kart racing
to the third dimension, and giving the system its second best-selling game. In 97, Nintendo continued
venue naming convention with the well-received Star Fox 64, and gave Mario Sidekick Yoshi
his own standalone title with Yoshi's Story. They also received invaluable support from the then
second-party developer Rare, who were about to give the console one of the era's most iconic games.
Goldeneye 007 single-handedly proved that first-person shooters could work on consoles,
and they could work well. Taking inspiration from Doom, but opting for a more realistic approach,
Goldeneye featured a memorable single-player campaign that did an admirable job of providing a
James Bond-like experience, complete with a rich story, varied levels and stealth elements,
but it was multiplayer that the game is best remembered for. By bringing the fun of the
deathmatch to consoles, Goldeneye proved to be a massive success, and went on to sell over 8 million
copies, putting it only behind Mario 64 and Mario Kart as the system's best-selling game.
And the hits from Rare didn't stop at Goldeneye. They also gave the system a second popular
kart racer with Diddy Kong Racing, before kicking off a new critically-acclaimed platforming series
with Banjo-Kazooie, before following this with Donkey Kong's successful 3D debut in Donkey Kong 64,
and finally by providing worthy successors to both Goldeneye and Banjo-Kazooie in 2000 with
Perfect Dark and Banjo-Tooie. Nintendo also had more important games of their own, with two in
particular going on to help define the fifth console generation. The first would see the return of
Nintendo's other flagship series and produced what may be the most critically-acclaimed game of all
time. The Legend of Zelda Ocarina of Time released in late 1998 and made the transition to 3D
while retaining the series' focus on exploration and adventure. With intuitive combat, intricate
dungeon design, and a huge world full of secrets, Ocarina of Time was a staggeringly well-made game,
but what truly elevated it to one of gaming's all-time greats was its sense of atmosphere and
weight. At times, dark, somber, and even surreal, Link's quest to the Ganondorf felt as epic as
it was enchanting, in a way rarely matched in the medium before or since. Nintendo also gave the
world a new take on the fighting genre in 1999 with Super Smash Bros. Featuring an all-star cast of
classic Nintendo characters, Smash moved the focus away from character-specific combos to create a
more approachable but no less enjoyable fighting game that was unlike anything else on the market.
Still, as impressive as Nintendo's first and second-party titles were in this period,
they still weren't enough to match the PlayStation's ever-growing lineup. With so many developers
favouring Sony's system because of its ease of development and the advantages of CDs, Nintendo
was left with little third-party support, and the more the PlayStation user-based grew, the more
developers wanted their games to be on it. Sony finished for decades strongly. They took inspiration
from Nintendo to produce their own dual analog controller in 1997. They transitioned their
marketing from targeting Sega towards directly targeting Nintendo after Nintendo became the
main competition, and with each new year they saw multiple major titles that ensured the PlayStation's
dominant position grew and grew. 1997 brought well-received sequels to Tomb Raider and Crash
Bantakute, alongside the only other game that could match Final Fantasy and Sales numbers,
Gran Turismo. Produced by Polyphony Digital, a new Japanese-based first-party studio within Sony,
Gran Turismo received universal acclaim and set a new bar for graphics and realism in a driving
game, surpassing even arcade competition in a somewhat symbolic step for the industry.
In 1998, Sony's lineup was arguably even stronger, with widely successful sequels to Resident Evil,
Tekken, Crash Bantakute and Tomb Raider that would be joined by high-selling new series,
like Insomniac's Spiroba Dragon. Perhaps the year's biggest surprise came from another
ex-Nintendo ally when Konami released their latest entry into the Metal Gear series.
Much like Final Fantasy VII before it, Metal Gear Solid wowed players with its cinematic focus,
this time produced through in-engine cutscenes, alongside full voice acting and an unusually
mature and complicated storyline. The result was one of the most story-focused and cinematic
experiences ever seen in a game, and it too would receive universal acclaim alongside 7 million copies
sold. 1999 was once more a packed year with even more sequels, this time to Final Fantasy,
Resident Evil, Spiro, Tomb Raider and Gran Turismo, alongside a new survival horror game from Konami
in Silent Hill and a kart racer to compete with Nintendo in the form of Crash Team Racing.
As the console generation progressed, the reality of the situation became clear,
Sony's lineup of games was simply too strong, and it allowed the PlayStation to maintain the
dominant lead on its competition from its release all the way to its discontinuation 12 years later.
In the process, the PlayStation became the first console in history to surpass 100 million units
sold, which was more than twice that of the previous generation's leader, the Super Nintendo,
and almost 70 million more than Sony's closest competition, the N64. By contrast, the Saturn
sold only 9.2 million, leaving Sega a distant third place and in serious trouble. During this
period disagreements over the company's future between the Japanese and American divisions
were reported to be frequent. Ultimately, the rapidly declining profits led the company to
discontinue the Saturn in 1998 in favour of focusing on the next generation, where they
hoped to get a head start on the competition. Throughout the development of their next console,
Sega tried to learn from their mistakes with the Saturn by paying close attention to production
costs and trying to select hardware components similar to personal computers to assist software
development. The system also used a custom version of Windows operating system to further
help with porting games between PC and was the first console to include a built-in modular modem
to enable internet access and online play. In the end, the console was completed while the
fifth generation was still going strong, and in November 1998, it released in Japan,
with a global release coming one year later.
The Dreamcast got off to a slow but steady start. Pre-orders in Japan were high, but hardware shortages
meant Sega couldn't keep up with demand, meaning they failed to meet their initial sales goals.
The American launch went smoother, however, and the first year sales seemed promising.
The launch lineup included a number of first-party titles, but its two big early stars would be
Sonic Adventures and Soul Calibur. The first represented Sonic's first true 3D debut and
quickly became the system's best-selling game, while Namco's Soul Calibur was one of the most
highly acclaimed fighting games of all time and is notable as being graphically superior to its
arcade counterpart and showcasing just how much next-gen systems could do. Still, early sales were
one thing, but the true test for Sega would be when their competition joined the sixth generation,
and for that, they wouldn't need to wait long. And so, the decade came to a close where after
years of intense neck-and-neck battle between Sega and Nintendo, it was Sony who ultimately came
out as the winner, and they won by some way. For Nintendo, the end of the 90s weren't all bad,
however, because there was one area where they remained firmly on top.
The Game Boy released in 1989 and quickly surpassed its technologically superior competition.
Nintendo supported their new handheld system with a number of first-party continuations of
classic Nintendo series. This meant more Mario, with Super Mario Land 2, before new series antagonist
Wario got his first game in Wario Land Super Mario Land 3. The Game Boy also got a sequel to
Metroid, with Metroid 2 Return of Samus, as well as a new 2D Zelda with Link's Awakening.
Other notable early Game Boy games include Puzzle Game Dr. Mario, Kirby's First Ever Outing in Kirby's
Dreamland, and a continuation of the original Donkey Kong series, as well as a portable title
from Rare called Donkey Kong Land. These games were all well received and helped the Game Boy to
continue strong, but it would be a new series which came to define handheld gaming in the 90s.
Before that, Nintendo would release the highly experimental and rather disastrous
Virtual Boy in 1995, a 32-bit technically portable headset and controller. The Virtual Boy was the
first console capable of displaying stereoscopic 3D, although it was only in red and black,
and there were concerns about long-term use, causing permanent eye damage.
It was also pushed to market before it was truly ready and received negative reviews for its high
price, uncomfortable design, and weak selection of games that struggled to use its novel 3D effect
in interesting ways. Ultimately, the Virtual Boy ended as a clear failure and was discontinued
less than a year after its launch before even receiving a European release, but it was at least
one of gaming's more innovative and unique failures. For Nintendo, this appointment wouldn't last long,
however, because they were about to stumble onto a game that would single-handedly conquer the world.
Pokémon was released in 1996 in Japan, 1998 in America, and 1999 in Europe. It was inspired by
creator Satoshi Tajiri's childhood hobby of collecting insects, but upon hearing the idea
for the game, Nintendo were initially skeptical. In the end, it was support from Shigeru Miyamoto
that helped get the game off the ground, with the Game Boy's link cable enabled multiplayer
acting as a major influence in creating a game focused on collecting and trading,
rather than being a more traditional RPG. The idea to have different versions of the same
game with exclusive Pokémon came from Miyamoto, who thought it would encourage the game's social
aspects. And, in the end, Miyamoto and Tajiri would be proving right, but success didn't happen
overnight. After a long and problematic development, Pocket Monsters Red and Green
released in Japan to decent but not remarkable sales. By the end of its first year, the game
sold over 1 million copies in Japan alone, but then, instead of slowing down like most releases,
Pocket Monsters did the opposite. With a blue and yellow version released later to renew interest
and strong word of mouth as a result of the game's social aspects, sales just continued to grow,
with 3.65 million copies sold in Japan in 1997. To put this in perspective, the total lifetime
sales for the Game Boy in the region at this time was only 7 million, but then Game Boy sales
started to shoot up as well. By the end of a slow localization process, Nintendo and Game Freak
had finally started to realize the potential value of what they had their hands on, and so they decided
to increase the game's marketing, tying the game's promotion to the upcoming anime and the series
mascot, Pikachu. Across America and Europe, Pokemon Red and Blue broke sales record after sales
record, with 31 million copies sold worldwide, or 46 million including all versions. It was later
described as the most successful video game of all time and received the world record for best
selling video game excluding bundle sales. In short, Pokemon was a phenomenon unlike anything
seen before in the history of gaming, and it was a phenomenon that didn't stop at the games.
An anime, a card game, merchandise, accessories, movies, spinoffs, sequels and more, all in mere
months as the franchise grew and grew, with children all around the world doing whatever
they could to get their hands on anything and everything Pokemon. For a short period of time,
the world seemed to stand still, gripped in the midst of a genuine Pokemonia.
It was possibly the biggest cultural phenomenon the modern world has seen, and it's hard to
think of any one thing that has matched it since. To this day, Pokemon is the highest grossing media
franchise in the world, surpassing Mickey Mouse, Star Wars, Harry Potter and Marvel, and it doesn't
look like anything will be catching up with it anytime soon. If there were any doubts still about
the gaming industry's mainstream potential by the late 90s, they didn't survive the arrival of
Pokemon, and all this, every bit of it, was kicked off on the back of one simple game.
When people talk about the greatest games of all time, there are a lot of obvious choices,
but Pokemon Red and Blue might be one of the most overlooked.
Aimed primarily at children, with a legacy that has far outgrown the original titles,
it's easy to forget how fresh and addictively enjoyable Pokemon's concept of combining role
playing mechanics with monster collection truly was to players at the time. It provided a perfect
introduction to the RPG genre, but it also added excitement and personality to the experience,
with players feeling a strong sense of attachment to their pocket companions,
and always wanting to see what new creatures awaited them in the next location,
or which Pokemon might evolve next, or what secrets there might be out there in the world,
waiting to be uncovered. The multiplayer aspects and growth of the franchise beyond gaming then
helped turn Pokemon into the unprecedented craze that it became, but that wouldn't have been
possible if not for the strength of the original games, which makes them seem worthy of a special
place in gaming's history. And as for the Game Boy, it soon received an updated model with the
Game Boy Color, which released globally in late 1998. Its best-selling game was, unsurprisingly,
the next entry into the Pokemon series, but it included full backwards compatibility with the
Game Boy's existing library, allowing Game Boy Color owners to not only play their old games,
but also play them in color. Amidst surging Pokemonia, the new Game Boy sold well,
with combined sales with the original reaching over 118 million units, making it the fourth
best-selling system of all time. It was also surprisingly long-lasting, with the Game Boy's
lifespan starting in 1989 and lasting until 2003, which wasn't bad for a system that released
already far behind its competition in terms of technological power. And as for that competition,
by the mid-90s it barely existed, although the Japanese-only Wonderswan from Bandai did mean
there was at least one other system out there at the end of the decade. Regardless, when it came
to handhelds, the 90s was a period of absolute Nintendo domination, to a degree rarely seen in
the industry before or since. Still, the Game Boy wasn't the only system that ended the decade well.
On computers, the second half of the 90s are best remembered for the growing presence of online
gaming, and for the large number of critically acclaimed genre-defining titles. PC gaming experienced
slower growth than consoles during this period, but the PlayStation itself marked a move towards
more PC-friendly development, and many of the console's best-selling games, like Tomb Raider,
Final Fantasy, and Resident Evil, were also released on Windows. The PC still had plenty
of exclusives of its own, though, and it still led the way in a number of key genres.
1997 kicked off with Blizzard's Action RPG Diablo, which took the role-playing genre in a new
direction and reached new levels of commercial success as a result. Before Diablo, computer RPGs
were known for their complexity, and were often seen as rather niche, but Diablo's isometric
action stripped away some of the depth to instead place a heavier focus on atmosphere, combat, and
item progression, going on to sell over 2 million copies as a result.
If action didn't appeal to you, 1997 also saw the release of a major step for more traditional
RPGs with Interplay's Fallout, which featured a bleak post-apocalyptic setting and a morally
ambiguous choice-driven storyline to provide a fresh take on the role-playing genre.
1997 also saw a landmark real-time strategy game with Insemble Studios' Age of Empires,
which proved to be a popular historical take on the genre and featured impressive online and
network play integration. Its sequel in 1999 was even more successful and has retained an active
multiplayer community to this day. Growing access to online connectivity also paved the way for one
of the year's most important releases in Origin System's Ultima Online, which became the first
game to create a multiplayer online RPG on a genuinely massive scale. Building on what was
started with mods many years earlier, Ultima Online offered an open-ended player-driven experience
with up to 2,500 simultaneous players in a single virtual world, which far surpassed any other game
of the time and popularized the idea of the MMO. 1998 saw the focus on online gaming continue
with the arrival of Blizzard's next major series. Starcraft featured a well-made campaign
and a rich sci-fi setting, but what truly set it apart was the popularity of its multiplayer.
Using Blizzard's integrated Battle.net service, Starcraft offered fast and efficient matchmaking
and its highly distinctive yet well-balanced factions were a perfect fit for competitive
multiplayer. This led to it becoming the best-selling RTS game of all time with 11 million copies sold
by 2009. It also did much to establish the modern esports scene as we know it today,
particularly with its popularity in South Korea, where televised commentated tournaments began
as early as December 1999. Other important games of 98 include Grim Fandango, a highly
original point-and-click adventure game from genre veterans LucasArts, Thief the Dark Project,
a first-person stealth game from Looking Glass Studios, known for its immersive design and
vast player freedom, and BioWare's Baldur's Gate, a fancy RPG that set a new standard for
Dungeons & Dragons based video games and marked the start of a new era for the genre. 1998 would
also see Valve make their gaming debut with their genre-defining Half-Life. With its immersive,
seamless story, regular integration of puzzles, and humble scientist-protagonist, Half-Life was
almost the polar opposite of Doom, and it showcased the genre's potential for storytelling
while proving that first-person shooters could be much more than just Doom clones.
In 1999, Ultimate Online was joined by another massive MMO by way of EverQuest from Sony Online
Entertainment. EverQuest soon surpassed Ultimate Online's player count and established the formula
of leveling, partying, dungeon running, and gearing that would come to define so much of the genre
and would be copied by so many future games. 1999 also saw the release of Heroes of Might and Magic
3, which gave turn-based strategy fans a cult classic title but is still one of the best examples
of its genre to this day. System Shock 2, which combined role-playing with first-person shooting
and survival horror elements to create a sci-fi cyberpunk classic, and Planescape Torment,
which gave the decade one of its most memorable RPGs while winning much praise for its mature
story, intelligent writing, and thematic depth. The 90s would close out in fitting fashion,
with two multiplayer-focused FPS games going head-to-head for the title of best online shooter.
The first was Quake 3 Arena, the latest game from id Software, and the other was Unreal Tournament
from Epic Games. Both games were multiplayer-only arena shooters, and both games offered impressive
3D graphics and lightning-fast action. Unreal stood out for its greater variety of guns and
game modes, while Quake was often considered the game of choice for traditionalists, but regardless
of individual allegiance, collectively the two games represented the pinnacle of arena shooters
at a time when arena shooters were the undisputed kings of the FPS genre.
Amongst the already mentioned games were a number of other successful series and sequels,
including a number of popular simulation strategy games. Together these gave the PC
a rather diverse lineup of games, and this was a time when PC gaming was still largely distinct
from that seen on consoles. In the end though, the 90s were the decade where PC gaming moved
towards the mainstream while still managing to maintain much of what originally made it unique,
from talented young developers making industry-changing breakthroughs
to the long-running focus on depth and complexity in game design,
or the general tendency of catering to more mature audiences.
The 90s was also the decade that online gaming made its big breakthrough,
which can be seen in the popularity of new genres like MMOs, real-time strategy games,
and first-person shooters. With time, online gaming played a greater and greater role in the
industry, but it was PC that led to charge while providing a glimpse at gaming's future in the
process. The 90s was the decade where gaming grew up. It began with all our war between Sega and
Nintendo, but in the end it was Sony who emerged victorious, and for once it wasn't because of
game design. This marked a changing point for the industry. Atari succeeded because they were the
ones who first brought high-quality arcade games to consoles. Nintendo brought console gaming back
from the brink with being Nintendo's seal of approval and a focus on high-quality first-party
titles. Sega finally managed to match Nintendo only after their own landmark series arrived
by way of Sonic, and the Game Boy flourished thanks in large part to Tetris and later Pokémon.
Sony however achieved victory through business and hardware, leaving most of the game design to
those who already knew how, and instead placing their focus on effective marketing and designing
a system that appealed to developers through ease of development. This showed the growing
importance of third parties, who had come a long way since Activision first broke away from Atari
to get the recognition they felt they deserved. Now third parties were no longer just an annoyance,
and they were no longer optional. Nintendo filled in a great lineup of first-party titles in the
fifth generation, with several being remembered as some of the greatest games of all time,
but the Nintendo 64 had a total of 388 licensed games released in its lifespan,
compared to over 3,000 for the PlayStation. The 90s also saw rapid evolution of technology,
and might be the single most transformative decade in gaming. The move to 3D was a major
part of this, with arcades leading away in the early 90s, alongside certain pioneering PC and
console releases. By the end of the 90s however, the entire industry had embraced 3D technology,
and the advancement being made within just a few years was staggering. Alongside 3D,
the 90s was also the decade of the CD, which paved the way for full motion video,
realistic audio and more detailed textures, and the decade where online gaming and competitive
multiplayer found their footing, with several new genres being established which would only
grow in importance in the years to come. Finally for game design, the 90s was the decade where
games started to go big and to shed their arcade origins. If someone were to try to pinpoint the
start of AAA gaming, it might have to be Final Fantasy VII. With a huge budget, a massive marketing
campaign, and a development staff of over 100 people, it was simply a different kind of game
to what was being produced at the start of the decade. For comparison, many of Nintendo's most
iconic 16-bit games had development teams of around 10 to 20, but with new technology came
higher demands on game developers, and by the end of the 90s, games in general were becoming
bigger and more expensive to create. The 90s also saw a substantial move towards the cinematic,
and the idea of games as a unique medium to tell stories rather than just as a set of challenges
for players to overcome. This can be seen in titles like Metal Gear Solid and Half-Life,
and their popularity clearly showed that this approach resonated with players, while at the
same time as this, traditional arcade features like high scores and lives were starting to
disappear amongst gaming's most popular titles. Overall though, the 90s was an exciting time
for gaming, with fierce rivalries, impressive technological progress, and many new emerging
genres. But as a new millennium beckoned, the industry was getting ready to experience rapid
change all over again. The 2000s were a massive decade for video games.
Technological innovations of the 90s matured with rapid advances in 3D and online.
The console wars continued with the arrival of the next and final major player,
followed by a seventh generation that no one could have predicted, and game development
moved further towards the AAA, at the same time that new areas of the industry started to arrive,
changing gaming forever more. Still, at the dawn of the new millennium,
the gaming industry was defined by uncertainty. After months of panic about a Y2K computing
apocalypse that in the end didn't happen, the industry then ran headfirst into a much
realer crisis when the dot-com bubble burst, leading to a stock market crash that hit the
industry particularly hard, all while gamers around the world held their breath in anticipation
of the next generation of consoles. Many questions run people's minds. How would Sony
follow up the most successful console of all time? How would Nintendo respond to losing the
top spot in the previous generation? Would Sega be able to do enough to survive? And what would
be the impact of the computing giant Microsoft throwing their hat into the increasingly crowded
arena? Before the next wave of systems could really get going, however, the previous generation
enjoyed one of its finest years, as developers pushed fifth-gen consoles to their limits and PC
saw the arrival of its best-selling game yet. Highlights on the Nintendo 64 include the surreal
sequel to Ocarina of Time, a legend of Zelda Majora's Mask, which featured a time loop mechanic
alongside a surprisingly dark tone to create an experience just as memorable as its predecessor
and almost as universally acclaimed. Perfect Dark from N64 Heavyweight Rare, which was a spiritual
successor to GoldenEye that built upon the landmark shooter's gameplay while adding a
multitude more options to create the fifth generation's definitive first-person shooter,
and Paper Mario, which continued the Mario RPG tradition established on the Super Nintendo
with a new take on the concept and a new two-dimensional art style.
On the PlayStation, Final Fantasy continued its strong critical and commercial run with Final
Fantasy IX, which featured a return to the more traditional fancy settings of older games alongside
some of the most spectacular pre-rendered graphics on the system. 2000 was also a year that Final
Fantasy was joined by the 32-bit debut of its great rival in Dragon Quest VII, which quickly
went on to become the best-selling PlayStation game of all time in Japan despite its late release.
And finally, Tony Hawk's Pro Skater 2 refined the formula of its 1999 predecessor to become
one of the most critically acclaimed games ever made. It might be tempting to see Tony Hawk's as
just a series of well-made skating games, but to do so would be a great injustice,
as they were clear examples of games so good they transcended their genre with simple,
yet addictively enjoyable gameplay that helped them secure an immortal place within early 2000's
gaming culture. The PC also saw its share of big releases, with RPG fans in particular being
spoilt for choice. BioWare's follow-up to Baldur's Gate provided an expansive and deep sequel that
still remains as one of the best examples of its genre to this day. Ion Storm's Cyberpunk
conspiracy thriller Deus Ex impressed with its open-ended mission design and high level of player
choice, and Blizzard's loop-filled action RPG series Diablo returned in its second entry
with improvements to gameplay and a story that made it even more popular than its predecessor.
2000's would also see the release of another important FPS that offered something markedly
different from the popular arena shooters of the time. Originally a mod for Half-Life,
Counter-Strike placed a greater focus on realism and the tactical side of the genre
and quickly received plenty of attention online, including from Half-Life creator's Valve,
who offered to buy the intellectual property and hire the mod's creator before releasing the
game officially. Counter-Strike helped create an enduring esports scene, as well as a number of
hugely successful sequels and showed the innovative nature and potential benefits of a game supporting
modding, something that has become increasingly rare in games as times went on. Of course,
none of the games mentioned so far come close to matching the impact of the year's biggest release.
From Maxis and SimCity creator will write, the Sims was like nothing else before it.
As a social simulation with impressive building tools,
Sims turned life itself into a video game and wowed players with the amount of freedom it gave
them and the level of detail it managed to include. From carefully guiding your Sims up a corporate
ladder and finally purchasing the local Mega Mansion, to turning their life into an inescapable
never-ending hell, the Sims was a massive hit with players and received seven expansions,
which was a huge amount for the time. It also sold over 11 million copies, making it the best-selling
PC game of all time until it was overtaken several years later by its sequel. Much like Pac-Man or
Tetris before it, the Sims was a game which found unprecedented levels of success by appealing to
people who may not have typically played video games. By 2000, personal computers were starting
to become a common household item and the Sims' universally understandable concept and general
accessibility made it the perfect gateway to the medium for those who may not have been swayed by
the fast-paced deathmatches of Quake or the advanced Dungeons & Dragons of computer RPGs.
Accessibility and broadened appeal would be a major part of the decade,
but not before the next generation of console wars got underway in fittingly dramatic fashion.
The Dreamcast got off to a solid start and its launch lineup was followed by a number of memorable
titles. The most important was Shenmue, which Sega hoped to be the system's killer app.
Featuring advanced 3D graphics and FMV cutscenes alongside a detailed open world
complete with NPC schedules, variable weather effects and a day-night cycle,
Shenmue was a major step forward for open world game design.
Ambitious, innovative and technologically impressive, it received much critical praise
and sold well, although unfortunately not well enough. With development costs of around $50
million, Shenmue was one of the most expensive games ever made and its sales of $1.2 million
weren't enough to recoup such a high investment. It also didn't become the major system seller
Sega hoped it would be, meaning for the Dreamcast there was still more work that needed to be done.
Other strong early performers on the system included Crazy Taxi, an arcade-infused action-packed
racing game, Jet Set Radio, a skating action hybrid that embodied counterculture and was one
of the first games to feature the use of cell shading, and Fantasy Star Online, a continuation
of Sega's popular Fantasy Star series and the first ever online RPG to appear on consoles.
Fantasy Star Online showcased the Dreamcast's impressive online capabilities and was particularly
successful with Japanese audiences where online PC gaming still hadn't really caught on.
In the end though, the Dreamcast's fate would be decided more by its competition
than its games, and 2000 was the year that Sony's next-gen successor was ready to make its debut.
Following the tremendous success of the PlayStation, many were left wondering how
Sony would top the best-selling games console of all time.
Sony's answer was to leave people wondering, meaning information about the PlayStation 2's
development was kept as secret as possible, with Sony refusing to even confirm that they
were working on a new system. Secrecy wouldn't last forever though, and at the Tokyo Gameshow
in September of 1999, the PlayStation 2 was revealed, with demos for Gran Turismo and Tekken
to show off its next-gen graphical capability. It wasn't the games or graphics that impressed
the audience the most though, but rather two specific design features. The first was full
backwards compatibility that covered the entire PlayStation 1 library as well as some of its
accessories like its controllers. At the time, backwards compatibility was an often requested
feature yet the only other console to provide it without additional add-ons had been the otherwise
disappointing Atari 7800. And if backwards compatibility alone wasn't enough, the PlayStation
2 also included a built-in DVD player. This was when DVD players were still relatively new,
and standalone units could often cost more than the PS2's $299 launch price, which made it seem
like great value for money and helped its reputation as being more than just a games console.
When this was combined with the strong momentum behind Sony's brand after the success of their
last system, it was enough to ensure the PlayStation 2's launch was a big success,
with the new console selling out in Japan and later America as Sony struggled to meet the high
demand. The launch lineup was also noticeably weak, with few heavy hitters outside of perhaps
Tekken tag tournament from Namco and low first-party titles from Sony, but in the end,
this didn't matter too much, as in October 2000, the PlayStation 2 finally arrived in America,
and quickly became the public's most wanted games console.
For Sega, the impact was immediate, and amidst now rapidly declining Dreamcast sales
and yearly losses of over $400 million, the discontinuation of a Dreamcast was announced
on the 31st of January 2001. This would be the end of the line for Sega's hardware aspirations,
but they would live on as a third-party developer and publisher, and in March 2003,
the company finally had a profitable year again after five years of consecutive losses.
As for Sony, having collected one scalp already, they turned their attention to those who remained,
and prepared themselves for what would go on to be one of the biggest years in video game history.
Microsoft unveiled their debut gaming system at the Consumer Electronics Show in January 2001,
and right from the start, Microsoft wanted the world to know how seriously they were taking
the industry, with major marketing campaigns being launched alongside offers to acquire
several big players, including Nintendo, Square, and EA.
The decision to enter the console space came after seeing the PlayStation success in the last
generation, with then-CEO Bill Gates worried that an unopposed Sony could become so dominant
that they might threaten Microsoft's line of Windows PCs.
Microsoft had also seen the way the PlayStation's PC focus development kits had won them third-party
support, which led to the idea of just creating a console using PC hardware and DirectX programming
interfaces, with the use of DirectX being what led to the console being named Xbox.
The result was a powerful system, with technical specifications notably beyond any of its competitors.
The console included a built-in DVD player just like the PS2, as well as a hard drive and impressive
online support, but Microsoft was called MicroSoft for a reason, i.e. they were a software company,
meaning that at the time they had no actual experience in manufacturing hardware.
This led to expensive development and manufacturing costs, with Microsoft ultimately making a loss
on each unit sold. Still, luckily for them, if there was one thing Microsoft did have available,
it was money, and in November 2001, the Xbox released in America, three days before the arrival
of the sixth-gen's final contender.
Nintendo
might have been down after their last-gen defeat in the PlayStation, but unlike Sega,
they were far from out, with dominance still in the handheld market, and a strong selection of
established first-party series. For their next system, Nintendo's priority was to avoid the
mistakes which cost them with their last console. This meant no more cartridges, although they
didn't opt for a DVD-based system either, and instead used the more data-limited mini-disc.
They also tried to ensure the system was easier to develop for, and that it didn't
release significantly later than the rest of the competition.
Nintendo did try out several more experimental features, including stereoscopic 3D,
motion controls, portability, and microphone support, but for now, these ideas played little
role in the final console. And so, Nintendo's GameCube was a fairly traditional system that
was less powerful than Microsoft's Xbox, but with a slight technological edge over Sony's PS2
in CPU and graphics card. The marketing focused on Nintendo's reputation and first-party success,
making use of the catchphrase, the Nintendo difference. But one thing which did remain the
same was a lack of effort on Nintendo's part to entice third-party exclusives.
Nintendo also weren't able to meet their initial target of the holiday season in the year 2000,
which would have seen their new console launch alongside Sony's, but at least this time they
wouldn't be quite so far behind. And at E3 2001, the system was fully unveiled with Shigeru Miyamoto
himself taking to the stage to announce the release date and launch lineup, with a later
price reveal putting the GameCube at $199, $100 less than its two rivals.
Unlike every Nintendo console before it, however, something, or someone, was noticeably absent.
As for the first time, Nintendo would be launching their system without a Mario game to support it.
In Mario's place was his brother in Luigi's Mansion, which went on to receive positive reviews,
but was a far cry from the generational defining Mario titles of earlier consoles.
Luckily for Nintendo though, even if Luigi wasn't able to develop to his big brother's big reputation,
Nintendo did have another title mere months away which could.
Super Smash Bros. Melee tipped the formula of its predecessor and ran with it,
featuring more characters, more stages, more gameplay modes, more music, and updated multiplayer
but ensured it would be a hit with fans and critics alike even decades later.
Melee went on to be the GameCube's best-selling title and acted as a fantastic showcase of
what Nintendo were bringing to the 6th generation.
Microsoft, however, had a killer app of their own to mark their entry into the console space.
Most of Microsoft's early efforts to buy third-party developers weren't successful,
but one company they did acquire was a relatively unknown yet still successful Macintosh developer
by the name of Bungie. The studio's sci-fi FPS series, Marathon, was exactly the type
of game that might appeal to Western audiences, but its Mac-only status had previously limited
its reach. In the late 90s, Bungie had begun work on a follow-up to Marathon, and after it was
unveiled at 1999's Macworld Expo by none other than Apple CEO Steve Jobs, Microsoft decided to swoop
in and make their move, impressing the team at Bungie with their commitment to the gaming industry
and securing the studio and their new title exclusively for Xbox in the process.
With this, Microsoft had now found their tempo title to centre the Xbox's launch around,
but for Bungie, things would be far from simple. The switch from computer to console brought a
number of development challenges. The team's deadline was incredibly tight, and the weight
of an entire new console was being placed quite squarely on their game's shoulders.
In the end, though, Halo proved up to the task, launching alongside the Xbox and becoming an
instant critical and commercial success. Halo was a very ambitious game. The biggest surprise,
then, might be that in the end, all its ambitions seemed to pay off. Featuring vehicle combat alongside
massive open-world-like environments with impressive enemy variety, AI behaviour and encounter design,
Halo was a fantastic game, but even more impressive was how well it played on console.
Bungie added subtle auto-aim to accommodate for the use of a gamepad over a mouse,
and used a system of input buffering to make movements feel smoother and more precise.
The result was a huge step forward for the console FPS genre and the creation of Xbox's first true
blockbuster title. But while Nintendo had Melee and Microsoft had Halo, Sony, on the other hand,
had what might be the single greatest year of console exclusives in all of video game history.
2001 was massive for the PlayStation 2. Many of the PS1's best-selling series would see sequels,
with Gran Turismo 3, A-Spec, Final Fantasy X and Metal Gear Solid 2 each releasing to
universal acclaim and incredible sales. What's more, each of these games showcased outstanding
graphics, making the difference in the new generation crystal clear to potential buyers.
Alongside these was another important title that would leave a lasting impact on game design
and would kick off the PS2's best-selling series, Rockstar's Grand Theft Auto 3.
This wasn't the first Grand Theft Auto game, but it was the first in 3D and its fully open world and
emphasis on player freedom was unlike anything that came before it. It remains a landmark open
world game and did much to change people's conception of what video games could achieve,
and this was still just one of many major titles releasing on the PlayStation 2 that year.
Looking back today, the most impressive thing about the PS2's 2001 release lineup
is surely its diversity, with different genres and tastes being catered to
and innovative new titles coming alongside the more expected big sequels.
Ico gave players something original, mature and creative. Capcom's Devil May Cry brought
arcade-inspired action and created its own new genre in the process. Konami's Silent Hill 2 gave
the console one of the greatest horror games ever made. Naughty Dog's Jack and Daxter brought Sony
a major new series and offered something a little more light-hearted, and there are other games that
could be mentioned here too. The PlayStation 2's lineup was stacked, and this was exactly what Sony
needed to keep them on top amidst the launch of their rivals' new consoles. Still, even if the
PlayStation 2 would end the year ahead, both the Xbox and GameCube did see strong initial sales,
with many retailers selling out of the systems. Each console also had its own clear advantage,
with the GameCube being the cheapest, and the Xbox being the most technologically advanced,
and so in 2002, Sony set out to take those advantages away. To do this, they cut the price
of the PS2 to $199 to bring it in line with the GameCube, and set about creating a PlayStation
Network adapter add on alongside several first-party online-focused titles like Socom to compete with
Microsoft soon to be launching Xbox Live. 2002 would also bring a number of PS2 exclusives,
with the three biggest being Square's latest action RPG, Kingdom Hearts, which combined
Final Fantasy characters with massive Disney franchises, Insomniac Games' Ratchet and Clank,
an important action platformer that would get a number of sequels on the system,
and Rockstar's Grand 5th Auto Vice City, which followed on from GTA III by bringing the series
to a new city and new decade, and found even greater success in the process.
Nintendo and Microsoft had other important titles in the following years as well,
but each company would also experience their own third-party struggles.
Nintendo did finally receive the Mario game they needed with Super Mario Sunshine,
which featured a tropical setting and a new spin on 3D Mario gameplay.
Other classic Nintendo series would also get new entries,
with Metroid Prime seeing Metroid make its 3D debut in an atmospheric and immersive take
on earlier games that retained much of what made the original special. Zelda returned with Wind Waker,
which saw players take to the ocean and explore many islands in a new cell-shaded art style
that was a dramatic departure from the previous games. Mario Kart returned with Double Dash,
which replaced the sprite of the N64 version with full 3D and allowed two riders per kart
to provide a new take on co-op gameplay. Animal Crossing and Pikmin made their debuts,
giving players a relaxed open-ended life sim, alongside a highly original real-time strategy
puzzle game, and other Nintendo series like Mario Party rounded out what was admittedly
a strong first-party lineup. When it came to third-party support however,
Nintendo were once more at a disadvantage after having done little to secure exclusive third-party
titles in the lead-up to the sixth generation. What's more, Rare, a developer who had done
more to support Nintendo than any other in the last two generations, started to experience
financial difficulties and were forced to look for a buyer. Nintendo already had a significant
stake in the company and could have purchased the rest, but they instead declined, leaving room for
other interested parties, like Nintendo's newest rival, Microsoft. While Rare did release
one GameCube title before this, their acquisition by Microsoft still left Nintendo's already lacklustre
third-party support even more lacking. In a strange twist of fate, one of Nintendo's most
prominent third-party developers in this period was none other than Sega, who brought a number of
series to the GameCube, including Sonic and Fantasy Star. The only other notable third-party that
provided exclusives was Capcom, who would bring a number of Resident Evil games, including timed
exclusivity on Resident Evil 4, alongside some new IPs. Still, while the GameCube did have exclusives,
it also found itself left out of certain multi-platform releases as the generations stretched on.
Some examples include Burnout 3, Black, Star Wars Battlefront 1 and 2,
certain entries into the Pro Evolution Soccer series, Hitman contracts and Blood Money,
midnight club 2 and so on.
Individually, none of these titles may have been dealbreakers, but the cumulative effect
meant the GameCube had the smallest library and came to be seen as the worst system for
multi-platform games. Microsoft would also eventually bring Grand 5th Auto over to Xbox,
once more leaving Nintendo out of what was arguably the biggest series in gaming at the time.
To make matters worse, Nintendo were also increasingly finding that they were developing
an image problem in Western markets. Amidst the gaming industry that was putting out more and
more mature titles and marketing from Sony and Microsoft aimed at securing older audiences,
Nintendo's GameCube came to be seen as the console for younger audiences.
This reputation had been developing since their showdown with Sega,
but now it was starting to do real damage, particularly with teenagers who might place a
lot of value on how a gaming system is seen by their peers. It may no longer be true today,
but in the early to mid-2000s, GameCube just wasn't cool, and this had an impact on sales.
Meanwhile, with the Xbox, Microsoft also faced problems as they struggled to provide Master
Chief with any backup. Attempts were made, though. Project Gotham Racing received strong reviews,
but failed to come close to matching the sales of Sony's mega-hit Gran Turismo. Blinks the Time
Sweeper would see key staff from the Sonic series creating an Xbox-exclusive platformer,
but it too failed to make a major impact, and new acquisition Rare failed to live up to their
past heights with games like Grabbed by the Ghoulies and Conquer Live and Reloaded.
Microsoft's most prominent third-party support came from Japanese publisher Tecmo,
who made a deal early on with Microsoft and contributed the popular fighting series Dead or
Alive and the 3D action debut of Ninja Gaiden, as well as from several former PC developers who
brought a number of RPGs to the console. These included Bethesda's breakthrough open-world
RPG, The Elder Scrolls 3 Morrowind, Lionhead's ambitious new IP, Fable, and BioWare's popular
Star Wars Knights of the Old Republic and Jade Empire. These RPGs would go on to be some of
the most highly acclaimed titles on the system and further cemented Xbox's growing reputation
with Western audiences. But compared to the PlayStation 2's continually expanding library,
the Xbox still had a lot less to offer by comparison. As a result, Microsoft placed great
hope on their upcoming online service Xbox Live, which launched one year after the console in
November 2002. Xbox Live was an impressive service for its time, featuring friends lists,
voice communication, and fast matchmaking while making use of an ethernet port and the
faster broadband connection over the still popular dial-up. Its flagship launch title was Epic's
Unreal Championship, but a disagreement with Electronic Arts over the terms of the new service
led to EA withholding all of their games from Xbox Live for its first few years.
Still, Xbox's online system was a success and launched before Sony's alternative without
the need for any additional hardware. Its impact on the industry hadn't been quite as dramatic as
Microsoft had hoped for though, or at least it wasn't, until the services killer app arrived in 2004.
Halo 2's campaign was a solid follow-up from the first game, but this wasn't the main thing
it came to be known for. With Halo 2, Microsoft's best series was able to take advantage of their
console's best feature, and it turned out to be a match made in heaven. Halo 1's multiplayer had
already impressed, but for many players, this would be their first experience moving from
split-screen to online, and the improvement was dramatic. Halo 2 quickly became the console's
best-selling game and the most popular thing on Xbox Live, vindicating Microsoft's decision to
prioritize online features on their console while also creating one of the most fondly remembered
multiplayer experiences of all time. But as successful as Halo 2 was, it wasn't the best-selling
game in 2004. In fact, it wasn't even close, as just when Xbox fans finally had a real reason
to celebrate their console's superiority, the PlayStation 2 would see the arrival of its own
best-selling game with Grand Theft Auto San Andreas, which would go on to reach an unprecedented
17 million copies sold on PlayStation 2 alone. San Andreas took the open world design of the
previous games and drastically increased the scale, featuring three distinct cities instead of one
alongside many new features that improved immersion and customization. The game did come
alongside controversy, particularly after the modern community were able to uncover a hidden code
left in the game's files but enabled a crude sex mini-game, which led to outrage and a revision
of the game's rating from the ESRB to the very rarely used adult-only rating. Still, this wasn't
enough to stop it from becoming one of the most successful games of all time, and Sony's PlayStation
2 flourished alongside it, receiving an updated Slim model the month before its release and
promptly selling out again during the holiday season. The final years of the sixth generation
would see Sony continuing to add to their already strong lineup of games with high-selling sequels to
Metal Gear Solid, Gran Turismo, Final Fantasy, Tekken, and Kingdom Hearts alongside an impressive
new action series in Santa Monica Studios' God of War. Unique titles came alongside the bigger
names as well, with games like Team Icos visually stunning and emotionally evocative Shadow of
the Colossus, or the weird and wonderful Katamari Damacy from Namco. Truthfully, neither the Xbox
or GameCube had bad lineups of games, the PlayStation 2 was just on another level,
and this would ultimately be reflected in its sales. Over its long, long life, the PlayStation 2
sold over 155 million units worldwide, making it the best-selling gaming system of all time,
a title it still holds to this day. By contrast, the Xbox and GameCube sold 24 million and 21.7
million units respectively, making this the most decisive victory of any console generation
since the Nintendo Entertainment System brought the home video game market back from the dead.
Sony benefited from strong third-party support, the built-in DVD player,
backwards compatibility, a highly competitive price, and a growing reputation of excellence in
the console market. For both Microsoft and Nintendo, there would be silver linings, however.
The Xbox successfully forged itself a place within the console market,
took second place in a highly competitive generation, and found great success with Halo
and Xbox Live, two things which showed much promise for the future as online gaming and
home broadband both continued to become more and more widely adopted. Meanwhile for Nintendo,
they continued to build on their established first-party series, while cementing their reputation
for making quality games. The GameCube may have suffered due to its perception as being for younger
audiences, but Nintendo may not have. Younger gamers will one day grow up, and by staying
true to their vision and creating great games for all ages, Nintendo continued to create
franchises with strong nostalgic appeal, and sometimes nostalgia can have great dividends
in the future. In the end, it seems fitting that the GameCube's best-selling game would be the
crossover title Super Smash Bros. Melee, as no other game better represented the system's appeal
or Nintendo's strengths. Nintendo's different approach also allowed them to set themselves
apart from the competition, and while that isn't always the best strategy to win,
it can sometimes help you survive. And ultimately, survive they did, as did Microsoft,
and while Sony's lead may have looked insurmountable, gaming has always been an industry where things
can change quickly. Before a new wave of consoles could go head-to-head, however,
the handheld market would finally get to experience a console war of its own.
Nintendo had dominated the handheld market since as far back as the original Game & Watch in 1980.
And after the Game Boy beat out a number of rivals in the early 90s, Nintendo spent years
being virtually unopposed. Their reign continued in the early 2000s with long-awaited Game Boy Advance,
which released worldwide in 2001. The system still lacked a backlit screen, a feature other
handhelds had included as early as the Atari Lynx in 1989, but otherwise it launched at an
affordable $99 and compared to the now seriously dated Game Boy, it provided a decent step up.
The original Game Boy was technically inferior to the 8-bit NES, whereas outside of the smaller
screen resolution, the Game Boy Advance was technically superior to the 16-bit Super Nintendo.
This had clear benefits for games so long as you could see them on the poorly illuminated screen
that once more got to haunt a new generation of children.
Nintendo would follow the Game Boy Advance with the Advance SP two years later,
which updated the model and finally included a light source.
And now that Game Boy owners could at last see what games they were actually playing,
they were in for a treat because the Game Boy Advance had an extensive library on top of
backwards compatibility with the original Game Boy's cartridges. The system's best-selling game
was Pokemon Ruby and Sapphire, its second best-selling game was Pokemon Fire Red and Leaf
Green, and its third best-selling game was Pokemon Emerald. But it had successful games outside of
Pokemon as well as all the Nintendo series you'd expect, with plenty of ports of Super
Nintendo classics. One thing it didn't have was an original Mario title of its own, but without
any competition, that hardly mattered, and the Game Boy Advance and Advance SP were highly
successful with sales of over 81 million units. Still, Nintendo's era of free reign wouldn't
last forever, and at E3 2003 it was reeled that they would face new competition from the same
company who had just beaten them twice in the home console market.
PlayStation Portable will allow consumers to enjoy 3D gaming similar to the experience delivered on
PlayStation 2. It also has the ability to deliver high-quality movies, music, and video,
as well as other forms of digital entertainment content through one central device. And all of
this is made possible through cutting-edge technologies aimed at helping us explore the
future in new ways. Sony had dabbled in the handheld space already with the Pocket Station,
a portable add-on for the PlayStation 1 that released in 1999 exclusively in Japan.
Their next attempt would be rather more ambitious, and the announcement showed off some
impressively powerful technical specifications alongside a heavy focus on multimedia support,
with movies, music, and online web browsing. Sony's plan was simple, do for handhelds what
they did for consoles through following the same approach, i.e. good hardware, disc-based operating,
functionality outside of games, and strong third-party support. And, as for that hardware,
Sony were keen to highlight just how similar it was to the still mid-cycle PlayStation 2,
at a time when Nintendo's current handheld was only on the level of the two generations old Super
Nintendo, and their next handheld was being developed in line with the last gen N64.
Basically, Sony was planning to skip ahead and deliver a product a whole generation more advanced
than their rival. For Nintendo, however, power wasn't the main priority. The idea for their
next system came from Hiroshi Yamauchi, a man who served as Nintendo president for an incredible
53 years. The dual screens were similar to certain models of the game and watch,
and Nintendo wanted to use their new system as a chance to innovate and move away from the
conservative image that they were worried they'd started to develop. Nintendo were still taking
the handheld market seriously, though. In fact, after the mediocre sales of the GameCube,
the next generation of handhelds came to be seen as more important than ever,
with President Yamauchi famously saying that if the new system succeeds,
we will rise to heaven, but if it fails, we will sink to hell.
And so, Sony's PlayStation Portable and Nintendo's DS went head to head.
Both systems launched at a similar time in Japan in 2004,
although the DS had a four to five month head start in other regions.
The DS also had a lower retail price of $150 to the PSP's $250.
Meanwhile, the PSP had a clear advantage in hardware, with a larger display,
higher resolution, and a much more powerful CPU. This translated to far better graphics,
with the PSP succeeding at coming close to PS2 level,
minus the smaller resolution, whereas the DS was still clearly one generation behind.
Nintendo's new system did come with a touchscreen, however,
and this was several years before the arrival of smartphones,
meaning few people had seen this technology before.
The combination of two screens and touch-based controls did make the DS stand out,
but it wasn't initially clear to prospective buyers just how big an impact this would have
on their gaming experiences.
And so, as both systems launched, the PSP was the one generating the most hype,
with reviewers impressed by the system's power,
and with Sony riding high off the momentum of two record-breaking home consoles.
In Japan, both systems got off to strong starts,
with the PSP selling out faster, but the DS selling more units due to greater supply.
In America, the PSP released later and performed below Sony's expectations,
while the DS surpassed Nintendo's.
Europe saw more or less the opposite though, as while the PSP again arrived late,
this time it sold out at record speeds, quickly surpassing the earlier numbers of the DS.
And so, in their first year, the two systems seemed to be neck and neck,
and attention then turned to the games.
In the console market, Sony had always excelled at attracting third parties,
but when it came to handhelds, they were still a newcomer,
whereas Nintendo had been working with third parties on handheld games for many years.
As a result, both systems received plenty of third-party support,
but Sony no longer had the clear advantage.
The PSP looked to continue the success of its less portable cousins
through continuations of many of Sony's best-selling series.
Its biggest success would be Grand 5th Auto Liberty City Stories,
and this would be followed by a spin-off to Jak and Daxter,
a new Tekken title with Dark Resurrection, a second Grand 5th Auto with Vice City Stories,
multiple Metal Gear Solid titles with Tactical Card Game, Metal Gear Acid,
and the more traditional Metal Gear Solid Portable Ops,
and a new Ratchet and Clank game in Size Matters.
Later releases would see new entries into the Final Fantasy series with Crisis Core and Decidia,
a Kingdom Hearts prequel with Birth by Sleep,
entries into the Gran Turismo and God of War series,
and several games from Capcom's new Monster Hunter series,
which proved hugely popular in Japan.
The PSP was further helped by the addition of a miniature analog stick,
which was a feature the DS didn't have,
which helped the PSP's reputation as a more serious gaming system.
Meanwhile, over on the DS, there would also be continuations of popular series,
with the first new 2D Mario since the Super Nintendo in New Super Mario Bros,
as well as Mario Kart DS, a port of Super Mario 64, Animal Crossing Wildwood,
Mario Party DS, and for Legend of Zelda, Phantom Hourglass.
However, alongside these more familiar names,
the DS would also find success with several new series
that were deliberately designed around the system's touchscreen.
One of the most popular of these was Petsim Nintendogs,
which released early on and quickly became a big hit.
But amongst the DS's best-selling games were a number of other names
that didn't seem to resemble the usual gaming top sellers.
These included Brain Age, Train Your Brain in Minutes a Day,
Professor Kakayama's Maths Training, The 100 Cell Calculation Method,
Big Brain Academy, Flash Focus Vision Training in Minutes a Day,
and English Training Have Fun Improving Your Skills.
If you want to know just what a big deal Brain Age,
Train Your Brain in Minutes a Day was, all you need to know
is that it sold more copies than any Pokémon game released on the DS.
The game was inspired by a popular Japanese book,
Train Your Brain 60 Days to a Better Brain,
and after a new president of Nintendo, Satoru Iwata, met with the author,
a decision was made to implement the book's thesis in a new game
for Nintendo's upcoming handheld that they hoped might appeal to casual gamers.
And it did.
Nintendo released a number of semi-educational puzzle games over the DS's lifespan,
and this became a major contributor to the success of its software sales.
Sony targeted core gamers by doing something similar to its existing consoles,
and found success with the teenage and young adult markets.
But Nintendo found considerable success with both younger and older demographics,
including people who may not traditionally have had much experience with gaming.
And so, just like with the design of their system,
both companies had radically different approaches with their gaming lineups,
and with time, one started to perform better than the other.
The DS saw consistently stronger software sales,
and was able to maintain its momentum through a number of redesigns,
starting with the DS Lite in 2006, then the DSi in early 2009,
and lastly with the DSi XO one year later.
These redesigns weren't universally applauded,
with the DSi's removal of backwards compatibility with Game Boy games,
and its introduction of digital cameras being seen as questionable,
but the DS Lite in particular helped the console maintain sales.
Meanwhile, the PSP also saw hardware updates with the PSP 2000 and 3000,
reducing the size and improving the screen,
and the later PSP Go, providing a total model redesign alongside a move to digital only.
Still, the Go retained the system's high price,
and over time, interest in the PSP dwindled, while the DS pulled further and further ahead.
This might be because while the PSP came close to matching the PS2's best titles
in terms of graphics, its game still felt consistently inferior to their console inspirations.
Early Metal Gear titles in particular seemed to mark a dropping quality,
but even the PSP's best-selling Grand Theft Auto titles
were still widely considered to be worse than the games which inspired them.
Then, as the next generation of consoles arrived with high-definition graphics,
the PSP's visuals were no longer quite so impressive,
and with neither graphical prowess, or must-have games to sell the system,
and competition from newly arriving smartphones diminishing the PSP's role as a multimedia device,
the system was left struggling.
Meanwhile, the DS was different in both gameplay experiences and audience,
and so it was able to maintain its more unique place within the gaming market over time.
In the end, the Nintendo DS sold over 154 million units,
making it the best-selling handheld and second best-selling system in history,
only behind the PlayStation 2.
Meanwhile, the PlayStation Portable sold around 18 million units,
making it an impressive debut by Sony, even if it still ultimately fell well behind its competition.
For handhelds overall, however, this may have been the greatest generation in gaming history,
with record numbers of titles, genuine innovation from Nintendo, impressive power from Sony,
and overall sales figures far beyond anything which came before or after.
By the end of the decade, smartphones were well on their way,
bringing the new frontier of mobile gaming with them,
and creating fresh competition for dedicated handheld systems.
Still, for a while, the Nintendo DS and PlayStation Portable put handhelds on top
of the gaming world, and with two very different systems to choose from,
consumers were arguably the biggest winners of gaming's miniature console war.
Nothing lasts forever though, and the arrival of mobile gaming would come to mark the beginning
of the end for handhelds, and this wasn't the only part of the industry where things were changing.
For many years, PC gaming had been a place of new innovations and independent development,
but by the start of the 21st century, the divide between PC gaming and consoles
wasn't quite so clear anymore. The PlayStation 1's use of PC-based development kits
made porting between computers and consoles easier than ever,
and as a result, many PC developers started to bring their games to consoles as well.
This trend continued into the next generation, as all major console players now took the
importance of porting and cross-platform games into consideration. At the same time,
the console market had seen rapid growth since the launch of the PlayStation 1,
making it a more lucrative market than ever, and then Microsoft entered the console space as well,
making PC gaming and consoles even more closely connected.
The result of all this was that PC gaming wasn't really as important in the 2000s as it was in
the previous decades, and several notable PC developers started to design their games with
consoles in mind, and maybe even as a priority. One of the best examples of this is Bioware,
whose Baldur's Gate series had helped define the computer role-playing genre in the late 90s.
Then, after another PC-focused RPG in Neverwinter Nights in 2002,
they made Star Wars Knights of the Old Republic for PC and Xbox,
a game with a more console-focused design than earlier titles, and then their next two games,
Jade Empire and Mass Effect, continued the console-focused design and released exclusively
on Xbox and Xbox 360, while only later coming to PC. And so, while PC games did continue to
release in this period, for developers, this was a time when consoles were often more important.
The 2000s would also see certain classic PC gaming genres start to fall out of fashion,
either because of changing consumer interests, or changing trends in game design,
or just the impact of console growth over PC. Examples include point-and-click adventure games,
dungeon crawlers, turn-based RPGs, arena shooters, certain simulation games,
and even real-time strategy games. Some series that did see substantial PC success this decade
include the already mentioned Sims, which received sequels in 2004 and 2009,
the Total War series, which started with Shogun Total War in 2000,
and received four more sequels over the next 10 years, and the Civilization series,
which saw its third entry in 2001, with further sequels in 2005 and 2010.
Perhaps the PC's most standout release in this period was Valve's Half-Life 2 in 2004.
Built on the studio's brand new source engine, its graphics alone were a whole generation ahead
of anything on console at the time, and when combined with the engine's advanced new physics
simulation, the game offered one of the most realistic and immersive experiences anyone had
ever seen. Still, Half-Life 2 was more than just the graphical showcase. It used its new higher
level of detail to craft effective environmental storytelling. Its physics were incorporated into
puzzle design and allowed for the addition of a new gravity gun, and it combined varied gameplay,
evocative worldbuilding, and an immersive seamless story to create one of the most
enduring single-player FPS experiences of all time. Half-Life 2 is one of the most
highly acclaimed games of the 21st century, and it sold over 12 million copies. At a time when
big PC exclusives seemed to be dying out, Half-Life 2 was a game which came to symbolize the system
and its player base, even if it did still release on consoles a few years later. Still,
even if the PC might have been past its golden age, there was one area where it continued to
innovate, and it was an area which was only growing in importance. Online
Online gaming may have started in the 90s, but the 2000s was when it exploded. The biggest
contributor to this was the adoption of broadband. In 2000, only 3% of american homes had broadband,
but by 2010, that number had increased to 66%, and broadband meant convenient,
high-speed internet access, which was perfect for playing online games.
In 2001, Dynamix's Tribes 2 upped the scale of online shooters to a staggering 128 players
in fast-paced jetpack-fuelled action. In 2002, EA's Battlefield 1942
offered a rather different kind of large-scale multiplayer warfare
with up to 64 players in an impressive simulation of a realistic battle.
Alongside EA's Medal of Honor and Activision's Call of Duty in 2003, these 3 series established
the modern military shooter, something that would play a massive part in the gaming industry for
years to come. In 2004, Valve released Counter-Strike Source, ensuring the popular round-based
shooter continued to stay relevant, and in 2007, Valve released Team Fortress 2,
a class and objective-based FPS that has continued to remain active and popular to this day.
Online shooters were making their way to consoles as early as Unreal Championship in 2002,
but for many people, PC was still the best place to experience them, with custom matchmaking,
dedicated servers, and loyal communities all shaping an experience unlike the one on the
more tightly-controlled consoles. But online gaming wasn't just about shooting other people.
One of the biggest trends of the decade was the growth of massively multiplayer online RPGs,
with games like Dark Age of Camelot, RuneScape, Ragnarok Online, Final Fantasy XI,
and Star Wars Galaxies all following on from what Ultima Online and EverQuest started in the late
90s. Still, when it came to MMOs, there was one game which reigned supreme over all others.
Released in late 2004, World of Warcraft took Blizzard's popular fantasy series online in a
massive way. With a staggeringly large open world, distinct factions, classes, and races,
and more attention than previous MMOs to accessibility and leveling, World of Warcraft
quickly became hugely popular. By 2010, it was well into its second expansion and had over 12
million concurrent subscribers, all paying monthly fees. In a time before microtransactions,
loot boxes, or DLC, this represented a serious amount of continuous income on a scale which the
gaming industry had never really seen before. 12 million players also meant a serious amount of
time was being invested into Warcraft's virtual world. As more and more players followed, the
reputation of the genre as something life-consuming, addictive, and dangerous grew as those outside the
gaming community tried to make sense of this new growing phenomenon. In reality, some of the panic
this led to was overblown, but there was an element of truth to it as well. For some people, MMOs
really were addictive. The escapist promised of another world to lose yourself in, the competitive
nature of player-versed-player combat and end-game raiding, and the social bonds that come from spending
so much time with other players in a shared world all contributed to an experience still
new to many people that could provide hours of fun, but could also be hard to turn away from.
World of Warcraft would be followed by many other MMOs, most of which took heavy inspiration from
its success, but in the end, the genre's growth didn't last, and rather than going on to conquer
the entire gaming world, the genre instead seemed to peak at the decade end as more and more online
games started to emerge. For a time, however, World of Warcraft really did stand at the top of the
gaming world, and it became an important part of 2000's gaming culture in the process. Still,
while World of Warcraft might have been the biggest online game of the 2000s, it certainly wasn't the
only one that left a lasting mark on the industry, and in fact, it wasn't even the only one to come
from the Warcraft series. Warcraft 3 released in 2002, and was an important online game in its
own right, with an active modding and competitive scene that were aided by the game's inclusion
of a world editor. This allowed players to create custom scenarios or maps for the game,
and many of these found popularity online, but one map in particular would go a whole lot further.
Defense of the Ancients took inspiration from an earlier custom map in Starcraft,
and shifted Warcraft's more traditional RTS focus towards hero vs hero action.
Dota proved to be a big hit, and would continue to get updated over the course of a decade,
with a dedicated community being built around its competitive play, which soon started appearing
at esport events. By the end of a decade, Dota was starting to attract serious commercial attention,
and in 2009, one of its original developers, alongside the recently formed Riot Games,
would launch their own take on a standalone Dota-like experience called League of Legends.
In time, the genre's name would change from Dota-like to MOBA, or Multiplayer Online Battle
Arena, and in a little more time, this would become one of the most popular genres in the world.
And it all started with a single custom map for Warcraft 3. Counter-Strike and Team Fortress
were also games which started life as player-created mods, and between them, they proved that the
innovative and independent side of PC gaming, where developers could change the entire industry with
a few lines of code written in their bedroom, was still alive, even in the console-dominated 2000s.
And speaking of console domination, the mid-2000s also brought the arrival of a new generation
alongside the greatest fall from grace since the collapse of Atari.
With the PlayStation 2, Sony achieved the unthinkable, and they were set to enter the
7th generation of consoles with a huge lead in market share and a near-flawless reputation
with the public. One year after the release of their next system, however, they would find
themselves firmly in last place in sales while making a significant loss on each unit sold,
with onlookers questioning if they were witnessing the beginning of the end
of the once-immortal PlayStation. For 10 years, Sony dominated the console market
without any competitor coming close. For 10 years, they looked unbeatable.
Until, suddenly, things changed.
The story of how this happened began at E3 2005. For years, gamers the world over had been eagerly
awaiting the next generation of gaming, and it was here that they were finally going to get to see it,
with Microsoft, Sony, and Nintendo showing off their new consoles officially for the very first time.
For Nintendo, the reveal was still rather secretive. The console was named The Revolution,
due to Nintendo's belief that it would revolutionize the industry, but the reason for that revolution
wasn't yet made clear, and other than a look at the prototype model, some technical details,
and a brief showing of some next-generation Metroid, the audience was ultimately left with
more questions than answers. For Microsoft, the event meant details.
The already announced Xbox 360 was set to launch later that year ahead of its competition,
and Microsoft showed off upcoming games, provided technical specifications,
and emphasized the new online interconnectivity, with in-game messaging, downloadable games,
and further online support being planned. Sony, however, wasn't as close to their console's
release date, and so the event came to be about the future. Sony's main announcement was the
Cell multi-core microprocessor that would be powering their new upcoming system. This was being
built in collaboration with IBM and Toshiba, and used genuine supercomputer technology,
making it a promising piece of hardware that could put Sony's system a clear step above
its competition in terms of pure power. Sony spent far longer showcasing the Cell processor
than any company would normally spend on such a subject, with both tech demos and games designed
to show off its capabilities. Some of what was shown was genuinely mind-blowing. It can be hard
to tell on lower-resolution video footage after years of further technological advances, but for
the time, the quality of certain games was far beyond anything people had seen before, beyond
Microsoft's recent 360 showcase, and beyond what many people thought was even possible.
For example, Killzone featured animations, physics, and lighting effects that are still
difficult to achieve today, 18 years later, including explosions casting shadows in real time.
For people who knew a little about gaming hardware, this could only mean one of two
possibilities, and arguments were quick to break out online about which one was true.
Either Sony was about to obliterate the competition and dominate a third console
generation in a row with their far superior system, or the games shown were fake,
i.e. pre-rendered videos that were only posing as games, or at best actual games that were
instead running on top-of-the-line PC hardware. In the end, time would tell, but not before the
Xbox 360 launched. The 360 released in November 2005 at $299 or $399, depending on model,
and it received a positive critical reception. Sales started strong, but manufacturing problems
limited supply, leading to the system selling out in all regions except Japan, while not
selling as many units as was originally hoped. The launch lineup was nothing remarkable,
with Call of Duty 2 and Perfect Dark Zero being the best selling titles, yet Niber represented
a true system seller. And so, with one console released, the field was wide open, with everything
still to play for. 2006 saw the PlayStation 3 being delayed from its earlier release date of spring
to later in the year, while the 360 continued strong, adding more exclusive games to its library,
including the Elder Scrolls 4 Oblivion, Dead Rising and Saints Row. Meanwhile, E3 that year
brought Nintendo's big reveal, and reactions were mixed. The new name, The Wii, was widely mocked,
and the showcasing of the new motion controls left audiences uncertain. For Nintendo and the gaming
industry, this was a bold new direction, and no one seemed quite sure yet what the impact of this
would be. One thing it did make clear, however, was that Nintendo wouldn't be in the competition
for the most powerful next-gen system anymore, and would instead be finding success or failure
through forging their own path. And so, attention returned to Sony, who were about to have a
conference that would live in infamy ever since. Our development teams all around the world,
in all of our studios, have been hard at work converting that technology and vision into magic.
Whoops. Another exciting aspect is the ability for consumers to make microtransactions, 599
US dollars, and we've been digging deeper and discovering what kind of muscle this machine
really has. And you can see from all of the games that we have up and running on PlayStation 3 that
we're really starting to push what this machine is capable of. Real-time weapon change. It's exhilarating.
What they're creating for the PlayStation 3 is inspiring.
So, that sums up our playable section. When E3 began in the 90s, it was something aimed primarily
at investors and industry professionals, and it came alongside boring PowerPoint presentations
full of sales figures and bar charts. As time went by, E3 became more and more about media coverage
and consumer outreach, with it becoming the number one place to make gaming announcements,
while also drawing heavy consumer scrutiny. By 2006, E3 was starting to attract more consumer
attention, and Sony's presentation wasn't exactly convincing. Making fun of E3 conferences
has gone on to become a recurring part of online gaming culture, but this was where that tradition
started. And we're certainly not interested in gimmicks. So, you can position this next to your
TV screen and see for yourself what is coming up behind you. And we're certainly not interested
in conventional thinking. Genji 2 is an action game, which is based on Japanese history. The
stages of the game will also be based on famous battles, which actually took place in ancient
Japan. So, here's this giant enemy crab. It's Ridge Racer. Ridge Racer. It requires huge financial
investment, 599 US dollars, but we must take risks to reap the rewards. Still, as the dust
settled, on-stage awkwardness and giant enemy crabs were the least of Sony's worries. A retail price
of 599 dollars, or 499 for a model with a smaller hard drive and several missing features, was a
staggering amount of money, well beyond the competition or what anyone expected. The system
would include a Blu-ray player, which helped Sony promote the Blu-ray format, but unlike
the DVD player of the PS2, for most consumers, this did little to make up for the system's
high price, as the Blu-ray format was still rarely being used at the time. Sony, however,
justified the high price on the basis that the power of their system made it worth it.
They were Sony. They made the best consoles, and the higher price was reflective of their far more
powerful hardware. Sony didn't see the problem with this logic, but other people did. Alongside
the price, release date and lessons on Japanese history, Sony also showcased several upcoming
games, yet most of them didn't look as impressive as what Sony had shown the year before.
What's more, they also didn't look especially different to the already released games on the
Xbox 360. Sony did have a more powerful system, but it seemed they didn't have much to show for it.
Of course, some people disagreed with arguments raging online about the console's graphics,
and people possibly seeing whatever they wanted to see. In the end, time would once more tell
the true story, and in November 2006, the PlayStation 3 released in America and Japan,
with a last minute delay, leading to the European release being pushed to early 2007.
Regardless of the price, the system did still sell, with stock shortages and online scalpers
causing controversy. The PS3 launch lineup also didn't have too much to offer, and featured fewer
games than the 360 had at release, with the best-selling game being Resistance, Full of Man.
Still, by this point, the Xbox 360 had been out for a year already, and in that time, its library
had grown considerably, with the month of the PS3's launch just happening to coincide with
Microsoft's latest big exclusive, Gears of War, a third-person shooter from Epic Games.
Gears was a major critical and commercial hit. Its heavy focus on cover-based combat was seen as a
fresh take on the shooter genre, and it was the perfect game to fill a space left open
by the 360's absence of a new halo. What's more, its Unreal Engine-powered graphics wowed critics
and showcased what the 360 was capable of, and it seemed that what the 360 was capable of
was keeping up with the supposedly more powerful PlayStation 3. Of course,
arguments about graphics still raged online, but by this point, for most people, it was clear that
if the PS3 did have an advantage, it wasn't a big enough one to win them the console generation
outright. At this point, you may be wondering what the hell was going on inside the PlayStation 3,
and whether the system even was more powerful to begin with, and this was a question many people
asked throughout the 7th generation. The simple answer is that the cell processor famously used
by the PS3 was more powerful, but it was also more complicated, which made it harder for developers
to actually utilize its power. Meanwhile, the 360 had a solid CPU itself, while also having a small
edge when it came to GPU power and a more convenient shared memory system. In the end, though, any
difference in power didn't really matter. The key decider was third parties. Sony hoped that
they'd continue to lead the industry, and that developers would prioritize developing for their
more powerful system as a result. In reality, however, the Xbox 360 quickly gained a sizable
share of the market, and so developers' priority became making sure their games were available
on both systems, with some developers even going as far as to publicly criticize Sony's more difficult
to develop for hardware. But as the 360 was easier to develop for, most multi-platform games were
developed primarily for the Xbox 360, and then ported to the PS3. Meaning, not only did the
PlayStation 3 lack a graphical advantage, but in many multi-platform games, the PlayStation version
either looked or ran worse. The PS3 did produce some great-looking exclusives,
particularly later in its life, once developers had more time to get used to the system's
architecture. But its reputation for having inferior versions of multi-platform games,
at a time when multi-platform games were becoming more important,
did damage the system's sales, with Microsoft often seeing much better software sales for
multi-platform titles. It also damaged Sony's brand, and without a graphical edge or many
strong exclusive games early in its life, the PlayStation 3 fell behind the 360,
while the Xbox's library continued to grow. 2007 brought more exclusives to Microsoft's system,
including Crackdown, Forza Motorsports 2, BioShock, and Mass Effect. The biggest of these, though,
was the much-anticipated Halo 3, which promised players the chance to finish the fight in what was
to be Master Chief's final game. Halo was huge. It went on to sell over 14 million copies in what
many considered to be the best game of the 7th generation up until that point. Sony would have
some exclusives of their own that year, but the likes of a new Ratchet & Clank, Uncharted
Drake's Fortune, and Gran Turismo 5 Prologue weren't enough to match the 360's superior offerings.
What's more, many of the series that once made the PlayStation, the console king,
were going, or had already went, multi-platform. From Grand Theft Auto to Devil May Cry, Tomb Raider,
or Tekken, Sony's list of exclusives seemed to be getting smaller and smaller,
and at E3 2008, it was announced that even Final Fantasy, a series more important to
Sony's early success than any other, would be coming to both systems.
Microsoft did face some of their own problems, the most expensive being high rates of hardware
failure and the infamous Red Ring of Death. Reports of technical problems appeared early
in the 360's life, but the true extent of the issue wasn't made apparent until a year or two
after its launch, as more and more systems continued to fail. In the end, Microsoft was forced to make
an official response to the situation, alongside offering all Xbox owners a full three-year warranty
covering repairs and shipping. This was estimated to cost over $1 billion, and slowed the Xbox's
momentum at a time when Microsoft would have loved to instead have been pulling further ahead.
As the decade grew to a close, the PlayStation 3's outlook was finally starting to look more
positive. Consecutive $100 price cuts in 2007, 2008, and 2009 finally brought the system to an
affordable price. Several early 360 exclusives, like the Elder Scrolls 4 Oblivion and Bioshock,
turned out to only have timed exclusivity and made their way to Sony's system as well.
And as the generation continued, the PlayStation 3 was able to add more and more exclusives of
its own, including Little Big Planet, Metal Gear Solid 4, Killzone 2, Infamous, God of War 3,
and Gran Turismo 5. Meanwhile, the exclusives on the 360 started to come slower, with highlights
including Fable 2, Left 4 Dead, and sequels to Halo and Gears of War. The decade would end with
Microsoft still in the lead, but with Sony working hard to make amends for their mistakes by taking
heavy losses on systems sold after price cuts as they tried to foster an image of appealing to core
gamers and creating strong first-party titles. Still, for Microsoft, the 7th generation can only
be interpreted as victory, whereas for Sony, they went from being untouchable to having to fight for
their life. When Sony launched the PlayStation 1, they found record success through the system's
affordable price, its ease to develop for, and its great third-party support that led to many
exclusive titles. Ten years later, they seemed to have ignored all of these factors and were
instead relying on name recognition to sell their system, no matter the price, and to bring developers
to their side, no matter the challenge of development. Yet, while it might seem like Sony's downfall
was brought about by arrogance, it might be fairer to say that they were simply over-ambitious.
The man responsible for many of the decisions involving the PlayStation 3 was then Sony
Interactive Entertainment CEO Ken Kutaragi, who is sometimes described as the father of PlayStation.
For Kutaragi, this was to be his last direct involvement with Sony's gaming division,
which was something he had once done so much to help create, and he truly believed in the power
of the cell processor and had hopes its technology could be used in other Sony products.
In the end, this wasn't to be, but Kutaragi had taken gambles with the PlayStation 1 and 2 as well,
and in the past, this was a big part of what helped turn Sony into the leader of the industry.
This time, however, he simply got it wrong.
Still, Sony weren't the only ones who took a gamble in the 7th generation,
and while the PS3 and 360 were in fierce competition for the core gaming market,
Nintendo were finding success elsewhere.
The Wii wasn't like its peers.
Where Sony and Microsoft moved to high definition, Nintendo's system remained at the 480p
of the previous generation, breaking the long-running convention of new consoles being
more powerful than their predecessors. Instead, Nintendo opted to avoid direct competition
and to target a broader demographic in what has sometimes been called the Blue Ocean
business strategy, which asserts finding value in less contested market spaces.
To this end, Nintendo set out with the goal to innovate, taking inspiration from the success
of the Nintendo DS as well as the popular arcade rhythm game Dance Dance Revolution
and new advancements in motion sensing technology.
The end result released in November 2006, mere days after the PlayStation 3.
Unlike the PS3, however, the Wii cost a mere $250, and it quickly sold out,
beating the combined sales of the Xbox 360 and PlayStation 3 in most regions during its launch.
The Wii offered something genuinely new, and people seemed eager to try it out for themselves.
By September 2007, the Wii had overtaken the 360 despite releasing a year later,
and it continued to sell well until 2010, when it started to experience a rapid decline
amidst increased competition. For many people though, the Wii was the perfect family console.
Its advertising targeted a wide audience, including mothers and grandmothers,
and its wealth of party games and simple pick-up and playability led to many people who may not
have usually played video games being willing to give it a go. Its best-selling titles reflect
this broader appeal, with the bundled Wii Sports selling and impressive 82.9 million copies.
Wii Sports didn't receive the strongest critical reception, mostly due to the gameplay's lack of
depth, but its simplicity worked well as an introduction to motion control and as a way
to attract those new to gaming. Other new first-party motion control games like Wii Play,
Wii Fit, Wii Party, and Wii Sports Resort also sold impressive numbers, with Nintendo
managing to showcase various creative ways to use their new technology, particularly through the
combination of exercise and gaming in Wii Fit. Alongside these were many of the classic series
people have come to expect from Nintendo, with Mario Kart Wii and new Super Mario Bros. Wii
being the best-selling titles, and Super Mario Galaxy, Super Mario Galaxy 2, Super Smash Bros.
Brawl, and The Legend of Zelda Twilight Princess being its most highly acclaimed. Mario Galaxy 1
and 2 stood out in particular for once more offering a fresh take on the 3D Mario formula,
this time with a space theme and lots of creative level design focused on different uses of gravity.
Much of the Wii's library focused on its motion controls however,
and on this aspect opinions varied, with some seeing it as a bold new step for gaming,
and others seeing it as more of an imprecise gimmick. The legacy of the Wii is also hard to
judge. It was the best-selling system of its generation, and yet in its later years its sales
fell consistently behind both its rivals in hardware and software, and this severe loss of momentum
may have went on to hurt Nintendo's next console. For a time it looked like the system would live
up to its early code name, but in the end the motion control revolution didn't really amount
to much, suggesting that the success of the system might have been more down to novelty
than the inherent value that motion control provided. There were also problems beyond the
system's less impressive graphics, with poor online support, poor audio, and a plethora of
lower quality titles that seemed designed to purely cash in on the latest craze with little regard
to quality. This came alongside a lack of long-term third-party support from more established
video game developers, who often stated they found motion controls difficult to implement
and develop for. The Wii would also miss out on many multi-platform games due to the difficulty
of porting to a system with lower technical specifications. Yet at the same time, the Nintendo
Wii was one of the most unique and daring consoles the gaming industry has ever seen,
and it remains an iconic part of gaming in the 2000s.
And while motion controls may not have been the future for mainstream gaming,
appealing to wider demographics may well have been. The 2000s would see another trend come and go
in the form of accessory-based rhythm games. Across different systems and different titles,
the Guitar Hero and Rock Band series saw hugely impressive sales, and while this sub-genre soon
fell away, this was around the same time but smartphones and mobile gaming were starting
to explode and conquer the casual market as well as a big chunk of the entire industry in the process.
In this sense, the Wii may have been ahead of its time in making inroads
into new, more casual audiences, but it would soon find itself outgunned in the great casual war
by the rapidly growing mobile market. Mobile gaming wasn't the only thing on the rise at
the end of the decade, however, as while console gaming was increasingly going big,
some developers started to take a rather different approach.
The 2000s can be seen as the decade where AAA gaming came to dominate,
with bigger budgets, better graphics and longer development cycles at the same time that certain
styles or genres of games were increasingly being pushed out of the limelight.
But in the shadow of this increased focus on blockbuster titles at the expense of all else,
something else started to appear in the decade's second half.
Independent game development had long played a vital role in PC gaming. From pioneering students
in the 70s to bedroom coders in the 80s to shareware successes in the 90s and even modding
innovations at the turn of the decade, independent development was around since
the industry's beginning, and it never stopped being important.
Still, throughout the 90s and 2000s, the technological gap between what small independent
teams and big commercial studios could do grew and grew, moving the industry further and further
away from independent development. In the early 2000s, however, several factors would align that
helped comprise a new form of indie game development. The first was the internet,
which paved the way for more effective online distribution without the need for retailers
or publishers. The internet also saw the start of browser-based gaming, which was made possible
through programs like Java and Adobe Flash. This would give rise to a wealth of user-generated
content, with hosting sites like Newgrounds seeing massive amounts of traffic over the decade,
proving the popularity of even much more limited games, with some browser-based games quickly
becoming successful enough to get full PC retail releases, like with Popcaps' 2001
Match 3 game Bejeweled. At the same time as this, new dedicated software like Game Maker Studio
and new unified game engines like Unity and Unreal Engine were starting to lower the programming
barrier to entry in game development. Finally, in 2004 Half-Life 2 released,
and indie gaming would never be the same. Because despite being very far from an indie game itself,
Half-Life 2 was the first game to be offered on and required the installation of Steam.
The Half-Life community wasn't exactly happy about this, but the Half-Life community didn't
have a choice, and Half-Life 2 was a difficult game to resist. In 2005, Steam would release its
first third-party games on the platform, including independent titles like Ragdoll Kung Fu and
Darwinia. And as Steam's digital distribution model cut out retailers and manufacturing costs,
it gave developers a higher profit margin, meaning that once Valve proved the model could work,
they were able to start attracting bigger names, including id Software, Idos and Capcom,
leading to further growth for the platform. This came alongside the rise of the term indie game.
Once upon a time, there were no distinctions between games on the basis of developer status or
budget, but as the industry grew, people eventually started to differentiate between small, cheaper,
independent titles and the more traditional full-priced commercial releases. One of the first
games to help create this distinction was Cave Story, which was developed by one man,
Daisuke Amaya, over a five-year period starting when he was in college. Amaya was inspired by
the games he played growing up, like Metroid, and so he made a 2D platforming adventure game
with a retro visual style. Cave Story was finished in late 2004, with Amaya making it available
online for free. The reason he didn't charge anything was because he thought the shareware
model would be too much hassle to implement, and he just wanted more people to play his game
after working on it for so long. And people did play his game, with several western media outlets
reporting on and praising it soon after its release, and showing amazement that such a game
could come from just one single developer. But as Cave Story released for free in a time before
most people had ever heard the term indie game, it didn't find mainstream success, and Amaya
simply continued working his full-time job at a printing company until six years later when
Cave Story released on Nintendo's WiiWare service and finally became a commercial success.
Cave Story has been described as the quintessential indie game and it would inspire others to follow
in its wake, while digital distribution and new development tools made doing so more and more
achievable. The next major step for indie games would once again come from Valve and a non-indie
game however. Portal, released in 2007, and its creative puzzle design and memorable story made
it one of the year's most critically acclaimed games. Portal was only about three hours long
however, which seemed far too short for a normal full-priced commercial release. As a result,
Valve got around this by selling it alongside Half-Life 2 and Team Fortress 2 in a retail
compilation called The Orange Box. Still, Portal's high quality and widespread popularity clearly
highlighted the potential of smaller, more focused games. At the same time as this,
Microsoft were looking to promote their own Xbox-focused digital distribution store,
Xbox Live Arcade, which was a perfect place for indie games due to their smaller download sizes.
And so, Microsoft began the Xbox Live Arcade Wednesdays program, which promised a new arcade
game to be released each week over summer, as well as the Xbox Live Arcade unplugged compilation
discs to promote Xbox Live Arcade games to normal retail consumers. This was followed by the Xbox
Live Summer of Arcade, which began in 2008 and saw Microsoft heavily promote a series of newly
releasing indie games each summer. The first year's lineup included Castle Crashers,
Bionic Commando Rearmed, Gallagher Legions, Geometry Wars 2, and Braid. And this proved
to be a big success, which helped establish the concept of indie games with general audiences.
By 2009, promising indie titles were starting to become a common occurrence even outside of
Xbox's Summer of Arcade and Steam. Spelunky from Derrick U combined platforming gameplay with
the procedural generation of roguelikes to create a new type of non-traditional roguelike design
that many future games would emulate. Flower from Back Game Company released on PSN and took players
on a calm and emotional journey that contrasted sharply from the violence of most mainstream
games. And World of Goo from 2D Boy released on PC and Wii and impressed with its simple yet
challenging puzzle design and memorable presentation. And so by 2010, indie games in the form that we
think of them today had arrived and they were just starting to become an important force within an
industry that was otherwise moving in the opposite direction. At the start of the decade, Sony were
at the top of the industry and were preparing for the release of a console that would soon be even
more successful. By the end of the decade, they were in last place and trying desperately to
rebuild their damage reputation. At the start of the decade, Microsoft was still yet to formally
enter the gaming space, yet by the decade's end, they were in the strongest position they would
ever find themselves in, with the Xbox looking increasingly like the core gaming console of
choice. At the start of the decade, Nintendo were THE company for classic console gaming, yet by its
end, they were finding new levels of success by appealing to new audiences with new types of
technology. Things change fast in the gaming industry, and in just a few more years,
everything just mentioned would end up changing again. But it wasn't just the fates of the
major players that were in flux in the 2000s. Game design itself would see major evolution
as mainstream gaming moved further and further towards a triple-A approach. The jump from the
6th to 7th console generation would be the last time that new consoles would bring a really noticeable
upgrade in graphics, as games went from 480 to 720 resolution, but even this came at a cost,
as the average frame rate of console games dropped as a result from 60 FPS down to 30.
For years, gaming graphics had steadily increased, but now the industry was starting to see
diminishing returns, with sacrifices having to be made to performance to make continued
advancement possible. And yet, the best-selling and most critically acclaimed games were,
without doubt, the ones that looked the best. In 2009, Call of Duty Modern Warfare 2 broke
records for sales, and Uncharted 2 broke records for Game of the Year awards, with both games
putting a huge focus on linear cinematic set pieces and the highest levels of presentation
that technology would allow. And yet, while mainstream gaming was getting noticeably more
expensive and less diverse, it was other areas of gaming that would see the largest growth.
At the end of 2009, rovio entertainment's Angry Birds and Pop Caps Plants vs Zombies
had just released, as smartphone-fuelled mobile gaming was becoming a larger and larger section
of the industry. Sony and Microsoft were both preparing for the release of their own motion
controlled devices to try to replicate Nintendo's success with casual audiences,
and a little indie game by the name of Minecraft was starting to make waves online
after its first release to the public, while a little multiplayer game called League of Legends
was just coming to the end of its beta. Gaming was changing, just like it always does,
and the changes of the next decade would see some of the industry's toughest challenges yet,
alongside some of its best and worst moments.
The console wars would continue in the 2010s, but the first battle was fought over motion controls.
Nintendo had responded to criticism of the original Wii Remote's poor precision
with the Wii Motion Plus in 2009. This new add-on upped the accuracy but only in games developed
with it in mind, and it would be required for certain later titles like Wii Sports Resort
and The Legend of Zelda Skyward Sword. Still, both Sony and Microsoft focused their marketing
on why their new devices were better than the Wii. Sony boasted about their accuracy,
pointing to the little glowing plastic ball on the end of the PlayStation Move remote
as proof that the device is special and making liberal use of terms like accelerometer,
inertial sensors, synergistic processing units and internal magnetometers,
and no, I did not make any of those terms up just now.
Microsoft, on the other hand, decided the best marketing strategy was to freak everyone out
with their surreal virtual child simulator, Project Milo, in 2009.
Claire grew a picture on a piece of paper. The piece of paper was held up to Milo.
Natal recognized the piece of paper, scanned the piece of paper in, Milo looked at that piece of
paper, recognized the shape, recognized the color and able to get on with his project.
This is true technology that science fiction has not even written about, and this works today, now.
Unfortunately, while it may have apparently worked that day, one year later, when Microsoft
returned to showcase the Kinect again, it was rather less impressive, being something more
in line with the earlier PlayStation Eye toy. Meanwhile, the child Milo was never seen again.
For gaming fans that were happy using normal controllers, this was a strange time to be alive.
Both E3 and gaming media were dominated by the new upcoming battle for motion control supremacy.
For a time, the question of who would win between the Kinect, the Move and the Wii,
was treated with the utmost importance, as if we were only now about to see the real battle
that would determine gaming's future. In the end though, it turned out gaming's future had
other plans. The PlayStation Move, released in September 2010, received positive reviews for
accuracy and responsiveness, particularly when compared to the Wii, and then produced disappointing
sales, as the more serious gaming audience Sony were targeting at the time showed little interest.
A disappointing selection of games to support it certainly didn't help either,
and in three years' time it was all but forgotten.
Microsoft's Kinect, on the other hand, released in November 2010 for a higher price and received
worse reviews, and then it broke the world record for the fastest-selling consumer electronic device,
while easily surpassing the Move in the process, probably because it was marketed
heavily towards a more casual audience, and the controller-free experience it offered was
something different to the already widespread Wii. Reviewers criticised input lag and the lack of
depth in games on offer, but did praise the Kinect's innovativeness. Three years later,
and the Kinect was far from forgotten, but we'll get to that later. Around this time,
the Wii would also see its sales slow, as the motion control craze seemed to be coming to
its natural conclusion after the novelty started to wear off. Still, while motion control might
have dominated the news at that time, something else was dominating sales. In 2009, Call of Duty
Modern Warfare 2 broke sales records for biggest entertainment launch in history, across all media.
In 2010, Call of Duty Black Ops broke the same records, surpassing Modern Warfare 2.
In 2011, Call of Duty Modern Warfare 3 broke records again, surpassing Black Ops, and in 2012,
Call of Duty Black Ops 2 broke records again, surpassing Modern Warfare 3.
Call of Duty's reign of terror would not last forever, but at the start of the 2010s,
it certainly seemed like it might. Put simply, Call of Duty was the biggest thing in gaming at
the time, with release after release, easily managing to sell over 20 million copies,
and there was a new release every single year. The multiplayer formula established by Call of
Duty 4 Modern Warfare proved incredibly popular with players, and Activision was happy to do
everything they could to cash in on this. And so, in a time when there was less online shooter
competition, Call of Duty became the undisputed king. Amidst the soaring popularity of mobile games,
the yearly domination of Call of Duty, and the newly arriving motion control devices,
there was a sense at this time that older or core gaming audiences might be being left behind
in an industry that didn't seem to need them anymore. This would play a major role in the
next console generation, but there were still well-received games being released in this period,
and amongst them, a new trend was on the rise. The Open World.
After the record success of Grand Theft Auto on the PlayStation 2, it's no surprise that more
games would try to copy its approach to Open World design, but while the 2000s would see a large
number of Grand Theft Auto clones, few of them ended up leaving a lasting impact. Things would
change at the start of the next decade, however, as developers started to look beyond Grand Theft
Auto for Open World inspiration. Games like Fallout 3 in 2008, Assassin's Creed 2 in 2009,
Red Dead Redemption and Fallout New Vegas in 2010, Batman Arkham City and the Elder Scrolls 5 Skyrim
in 2011, and Far Cry 3 in 2012 all achieved critical and commercial success to help popularise
the Open World as a concept that would soon come to be used more broadly in mainstream game design.
And as the decade continued, more and more games made the transition to Open World,
including most of the biggest series in gaming, like Metal Gear Solid, The Witcher, Dragon Age,
Final Fantasy, Mass Effect, Forza, The Legend of Zelda, Halo and Pokemon, alongside many of
the decade's biggest new IPs. This fit the established gaming industry trends of trying
to make bigger and better games to continually one up what came before, where Open World seemed
to represent the next step in this while also providing an opportunity to breathe new life
into established franchises. The move towards Open World brought benefits like increased immersion
and player freedom, but it would also lead to criticism about repetition and game experiences
being drawn out or padded, with the term Open World Fatigue sometimes being used as the decade
continued. Still, at the start of the decade, Open World were relatively fresh, and early
examples like Red Dead Redemption and Skyrim were some of the most highly praised and best-selling
games of their respective years. Still, while much of gaming at this time was moving in certain
recognisable directions, there were two games which stood out for doing something different.
The first was Dark Souls, which released in 2011 on consoles and 2012 on PC. From Software had already
impressed players with Demon's Souls in 2009, but Dark Souls was where they would reach a much
wider audience and the game quickly developed a reputation for its high difficulty and unforgiving
design. What really made Dark Souls stand out however was how different it felt from so many
other games of its era. Since the days of arcades, video games had been becoming easier and more
accessible year after year to the point where increasing accessibility was seen as a fundamental
part of good game design. The general consensus was that older games were often far too punishing,
so developers added things like regenerating health and regular checkpoints. Or that old games
were too vague and confusing, so developers added more tutorials, guidance, and features like mini
maps. Or that old games were too simple and video gamey, so developers focused more on the cinematic
side with action set pieces and cutscenes. And then Dark Souls came along and ignored all of this.
It punished players for mistakes, even taking away their valuable experience points. It gave
players minimal direction and made a world where it was easy to get lost, and it told its story almost
entirely indirectly, leaving 99% of the experience as pure gameplay. Dark Souls broke all the rules
about how a modern game should be designed, and in doing so, it created something unique
which many players fell in love with. With time, Dark Souls developed one of the most enduring
legacies of any game from the modern era, although its reception at release was more modest.
It was only the 32nd highest reviewed game from 2011, and it was widely ignored by Game of the
Year awards. Yet it was also a game which people kept playing and talking about, and soon its
influence was hard to deny. Every difficult game would come to be compared to it. Countless indie
games would directly copy it, and even mainstream games would start to indirectly copy it all while
its reputation kept growing. In short, from combat design to the lack of handholding to the use of
difficulty or its approach to storytelling, Dark Souls was a title which made the industry
re-evaluate what good game design truly meant, and yet it arguably wasn't even the most important
game released that year. If Dark Souls challenged how people thought about game design, Minecraft
challenged how people thought about video games. Beginning at the hands of a single developer,
Marcus Notch Pearson, Minecraft would go on to become the best selling game of all time,
with a staggering 238 million copies sold, a number which is still rising.
Notch took inspiration from the ambitious Colony Sim to a fortress and the block-based mining game
Infiniminer to create a procedurally generated block building sandbox that was made available
to the public online in 2009 and received regular updates over the next two years.
In 2011 it received its official release, and in 2014 Minecraft was bought by Microsoft for $2.5
billion, with the game still receiving regular updates to this day. Beyond simply being one of
the most played games ever made, Minecraft popularized the concept of early access, where players
paid to access a game that's still in development, and continuous development, where a game is
continually expanded over time with no specific planned ending. It also challenged the perceived
importance of graphics in gaming. For years, graphics had been getting better and better,
and becoming a larger and larger priority, even as this caused development time and costs to
continually increase. Then Minecraft came along with pixelated blocky visuals that looked several
generations old, and became the most successful game in history. In doing so, Minecraft proved
that not only are high quality graphics not needed for a game to be successful,
but also that graphics might sometimes be holding gaming back. Minecraft's popularity
was a result of the freedom it provided players to create and explore and do more than most other
games allowed, and this level of freedom was only made possible by the simpler visual style,
where not every asset needed thousands and thousands of polygons or multiple dedicated
artists. At a time when the gaming industry was fully embracing AAA design and indie games
were still finding their footing, Minecraft was a monumental statement about what video games could
and maybe should be. And between Minecraft, Dark Souls, the open world trend, yearly call of
duties, and unwanted motion controls, you have a great representation of what the next decade in
gaming would be all about. Video games were changing just like they always were, but now
complex and divisions were starting to be more obvious as the industry continued to grow and
expand. Within this multiple distinct battles can be identified, like mainstream versus indie,
conventional game design versus counterculture, core gaming versus casual, maximizing monetization
versus preserving a franchise, and so on. And it was these divisions that would truly define the
next decade, including the next generation of console warfare.
In the 7th generation, Sony shocked the world by losing. After years of PlayStation domination,
Sony looked unstoppable, but a high price, complicated system architecture, and the loss
of key exclusives allowed Microsoft to close the gap with core gamers while Nintendo thrived for
appealing to new and more casual audiences. The fate of all three of these companies would soon
change though, and this started before the next generation even arrived. After years of leading
the pack, the Wii sales fell behind both the 360 and PS3 at the same time that new motion control
competition arrived and consumer interest seemed to wane. Microsoft saw its lead over Sony Dwindle
and eventually disappear as the 360 lost momentum amidst few new exclusives,
and Sony's system saw its finest years after price cuts and model revisions turned around sales.
Sony's strong end to the generation would be complete with the release of The Last of Us
in summer 2013, which was heralded as one of the finest games of the generation,
at the same time that the PS3 finally managed to overtake the 360's total lifetime sales.
Focus then turned to the next generation, which had officially begun in 2012.
This was the world first reveal of Nintendo's new console, as shown at E3 2011.
You may notice a heavy emphasis on the word controller. This trailer is three minutes long,
and that word shows up on screen 12 different times. The word console isn't used once,
and the Wii U console isn't even shown. The Wii U was one of Nintendo's greatest failures,
and much of that failure is often said to be down to consumers failing to understand what the system
even was. Looking at the reveal trailer shows how this might have happened, and even among gaming
media this caused some confusion. Amongst general consumers who may not follow gaming news closely,
i.e. much of the Wii's new player base, it would have been easy to mistake this device as just an
expensive add-on or a new Nintendo themed tablet, and this poor marketing decision surely hurt the
system's sales. This isn't the whole story however, and even during its development the Wii U
experienced a bit of an identity crisis that went on to follow it for the rest of its life.
The Wii exceeded Nintendo's expectations, and excelled with casual gamers, but Nintendo
started to worry that if they appealed too much to casual audiences, it might hurt their reputation
with more core audiences. As a result, Nintendo made bridging the gap between the two groups
a priority for their next system, and so they decided on HD graphics, as this was expected
from core audiences, and that it should also include a new innovative feature that could
appeal to families and Wii fans, hence the tablet-like controller. This controller wasn't as
revolutionary as the Wii's motion controller bin though, as tablet computers were already
gaining popularity at this time. The controller was also heavy and bulky in design, had a short
battery life, and needed to be in range of the Wii U console to be used, making it more limited than
an actual tablet. Nintendo also struggled to show off the advantages this hybrid approach
brought to game design, whereas the Wii's motion control was simple to understand and easily demonstrated.
Still, in November 2012, the Wii U launched to the public for $299 or $349. The critical
reception was mostly positive, with praise given to its backwards compatibility with Wii games and
peripherals, as well as the potential of its unique controller. Initial sales were lower than
Nintendo expected though, and the launch lineup had few must-play titles, with the best-sellers
being a new, new Super Mario Bros and the party game Nintendo Land. As time went on, Nintendo would
release more games on the system, including new entries into most of the series people expected,
like Mario Kart and Smash Bros, although the importance of these games may have been diminished
by the fact that many also released on Nintendo's latest handheld, the 3DS. The Wii U's biggest
problem, however, would once again be third-party support. At the start of the Wii U's life,
it was on par with the still current PS3 and Xbox 360, meaning titles could be ported between these
systems, and these ports made up a big part of the Wii U's early lineup. These games rarely made
interesting use of the Wii U's unique controller, and sometimes ran worse than the older generation
versions due to differences with the system's hardware, but this was still a lot better than
what was to follow. Once the next generation of consoles arrived, the Wii U once again became
technically inferior to its rivals, causing great difficulty for third parties when porting
multi-platform games to the system. And as the Wii U got off to a slow start, which led to a small
user base, third parties then had even less incentive to bring their games to the system at all.
Less than one year after its release, EA and Bethesda both publicly stated that they had little
plans for future Wii U games, and this problem only grew over time. By the end of the Wii U's life,
the only game amongst its best-selling titles that didn't come directly from Nintendo was Minecraft.
And so, for the fourth generation in a row, Nintendo's console was the worst place to be
for third party games, and this time the problem was bigger than ever.
Nintendo designed the Wii U as a system that would appeal to both casual and core audiences,
but with little third party support, and few games which made interesting use of the controller,
the Wii U failed to pull in many core gamers. And with confusing marketing, a high price,
and the lack of a strong new gimmick, it failed to replicate the Wii's success with casual gamers.
And so, the Wii U was a complete failure, and dumped Nintendo firmly into last place
as soon as its competitors arrived. But as disappointing for Nintendo as this must have been,
at least they could soon console themselves, but they weren't the only one who dropped the ball
in the eighth generation.
To understand how the eighth generation was won,
you have to keep in mind what had been happening leading up to it.
The biggest trends over the last five or six years had been the success of the Wii and
motion control, the huge growth in the mobile market, and Call of Duty continuing to break
every sales record under the sun. People may not have admitted it, but the truth was,
core gamers were starting to feel like they were being forgotten.
The industry was growing, but it was growing away from what made many people like games in
the first place, and the only thing big companies seemed to care about now was finding new ways to
reach the more lucrative casual market. And so, as the next generation was about to be revealed,
there was never a more important moment to put the focus squarely back on high quality games.
Then came the official Xbox One reveal show, which was the first look at Microsoft's new system,
and a moment Xbox fans had been waiting for for years. And the only thing being mentioned was
it took 33 minutes, almost all of which was focused on TV, before a single video game was shown,
and when it was, it was just a highlight reel for the next yearly instalment of multi-platform
EA sports titles. This was followed by the more promising Forza 5, then a newly announced,
confusing game TV show hybrid called Quantum Break, and then it was back to actual TV coverage.
Halo made an appearance as a newly announced TV show, Steven Spielberg made an appearance to talk
about TV, American Football made an appearance as something you can apparently watch on TV,
and then the show finished with Microsoft's biggest reveal. Call of Duty will appear on
Xbox later that year, just as it had for the last six years because it was a multi-platform
yearly series, and then the show was over. This was how Microsoft threw away all the good will
they had slowly built with core audiences over the last 10 years. At a time when gamers were
worried about how the industry was changing, Microsoft used the one moment where everyone
was watching them to prove that all of people's fears were true. The reception to this reveal
was universally negative, but things would soon get a whole lot worse.
Three weeks later at E3 2013, Microsoft had one last chance to make a big impression
before their system's release. And the show ended up dominated by news that the Xbox One
would be an always online system, that owners couldn't play games on it without an internet
connection, that people could no longer sell or share used games due to new DRM measures,
and that the built-in Kinect would be always on, watching you, recording your daily activities
for storage in Microsoft's database in case of any future bad behaviour.
That last part may have been added more by the public than Microsoft themselves,
and Microsoft were a little unfortunate to be holding their press conference
one week after Edward Snowden leaked documents revealing unlawful surveillance by global
security agencies. Still, for the most part, Microsoft did it to themselves.
At this point, Victory was being handed to Sony on a silver platter,
and all they had to do was mention video games a few times and remind people that at least they're
not Microsoft. In addition to creating an amazing library of new titles on PlayStation 4, we're
equally focused on delivering what gamers want most, without imposing restrictions or devaluing
their PS4 purchases. For instance, PlayStation 4 won't impose any new restrictions on the use of
PFOA gamers. I guess that's a good thing. We believe in the model that people embrace today
with PlayStation 3 and continue to demand. Just heard you there. When a gamer buys a PS4 disc,
they have the rights to use that copy of the game. They can trade in the game at retail,
sell it to another person, lend it to a friend, or keep it forever.
In addition, PlayStation 4 disc-based games don't need to be connected online to play.
And it won't stop working if you haven't authenticated within 24 hours.
Sony's best announcement was still to come, however.
This meant the PlayStation 4 would be $100 cheaper than the Xbox One, and it wasn't because of less
powerful hardware. In fact, if anything, the PlayStation 4 seemed to be the most powerful system.
The reason for the Xbox's higher price was the built-in Kinect device, a feature that offered
several benefits, yet was highly disliked by many in gaming communities. For Microsoft,
the reveal and its follow-up were a disaster. One week after E3, they announced that the new
DRM policy would be scrapped, that the system would no longer require an internet connection,
and that owners would even be allowed to unplug the Kinect and still use the console.
President of Microsoft's interactive entertainment business,
Don Matric, departed shortly after, but by this point, the damage was already done,
and the high price still remained. In November 2013, both consoles released to the public,
with the PlayStation 4 quickly selling out. By the end of the year, it had sold 1.2 million
more units than the Xbox One, and with time, this lead grew, until in 2015, Microsoft announced
that they would no longer be publishing sales figures. Microsoft may have hoped that after the
release of the Xbox One, their reputation might improve due to the quality of the system, but
unfortunately for them, this too was not to be, because while the two consoles were similar in
specs and architecture, it was the PlayStation 4 which seemed to have the edge and was consistently
performing better when it came to multi-platform games. For Microsoft, there was little they could
do, but dig in, repair their damaged reputation, and prepare for the future. At E3 2014, they revealed
a $399 Kinect free version of the console to bring the system in line with the PlayStation.
The TV-focused Xbox Entertainment Studios was also closed later that year, and Focus for the
future shifted more towards video games, with Microsoft purchasing several studios, including
Mojang, the creators of Minecraft, which Microsoft continued to allow on Sony systems in what they
hoped would be seen as a pro-consumer move. For Sony, their lead over both Microsoft and Nintendo
never seemed to be any doubt, and while in time, Microsoft did succeed at distancing themselves
from past controversies, the PlayStation 4 continued to be a well-received and well-liked system that
offered a far more impressive list of exclusive titles. This lineup was admittedly slow to get
going, but later included Marvel's Spider-Man, God of War, Uncharted 4, The Last of Us Part 2,
Horizon Zero Dawn, Ghosts of Tsushima, Final Fantasy VII Remake, and Bloodborne. Meanwhile,
the Xbox One had few highly-reviewed exclusives outside of Halo, Years of War, and Forza.
Still, by this point, exclusive titles played a much smaller role in determining a console's fate
than they had in the past, with most games being multi-platform and with console exclusives
increasingly being released on PC by all parties except Nintendo. There can be no mistake, however,
that the PlayStation 4 won the eighth generation. The most recent sales figures show that Sony
System sold over 117 million units, which is more than twice as many as the Xbox One,
and vastly more than the 13.5 million of the Wii U. For Nintendo, the battle was lost before it even
began. For Microsoft, inferior multi-platform performance, few exclusives, and a higher launch
price are what ultimately doomed the Xbox One, although the damage done by the poor handling
of the initial reveal and the heavy focus on TV made sure that the system got off to the worst
possible start. Microsoft had found some success at targeting casual audiences with the release of
the Kinect in 2010, and they presumably hoped to continue this by including the Kinect with their
new console and focusing heavily on multimedia. We wanted to bring new audiences into Xbox,
people that were outside of what traditionally was thought of as our core gaming audience that
was really the intent at the time. Within the company there's always a sense of do more and grow,
but I think Dawn was looking for like what's that next big leap after Kinect,
and I think there was a sense that TV could be that thing. Like Nintendo finding that the Wii
U had to compete with real tablets though, smart TVs were also readily available by the time the
Xbox One launched, which made much of the system's multimedia utility less impressive.
Their biggest mistake, however, was simply failing to consider their core audience
when designing their system and marketing it. If they'd focused on TV in addition to video games,
the reaction wouldn't have been so negative, but instead all they had to show alongside their TV
related announcements were anti-consumer features like Always Online and DRM, and a high price
that was a result of a feature most people didn't even want. When gamers were worried about being
abandoned by the industry, Microsoft did exactly that, and it cost them. Still, like others before
them, Microsoft and Nintendo were down but not out, and things do change quickly in gaming,
as the rest of the industry was making clear.
When the DS and PSP went head to head in the mid-2000s, Nintendo was the clear winner,
but the PlayStation Portable was far from a failure, and as the next generation of handhelds
was revealed at the start of the 2010s, it looked like history might repeat, with both companies
sticking to a similar approach. Sony doubled down on power boasting of near PlayStation 3
level graphics, and Nintendo introduced a new unique feature to complement their dual-screen
design by way of stereoscopic 3D. This is something Nintendo had been flirting with for years,
first with the Japan-only Famicom 3D system in the 80s, then with the universally panned
Virtual Boy in the 90s, and even with an unreleased LCD attachment for the GameCube in the early 2000s.
This time, however, 3D was the main attraction, and with a relatively low cost and no need for
additional glasses, it was a feature that got people talking. The Nintendo 3DS released in
March 2011 for $250. Reviews were positive, with praise given to the addition of a new
CirclePad analog controller, backwards compatibility with DS games, and the improved user interface,
while criticism was directed towards low battery life. The main talking point though was unsurprisingly
the 3D effect, with reactions varying widely, and these reactions were more important than usual,
as the 3D effect didn't pick up on camera, making it hard for Nintendo to show off.
Still, some people were blown away by the effect, some saw it as nothing but a gimmick,
and some complained about it inducing nausea and headaches.
It also required rather precise positioning of the system or the effect with Blar,
which was a little impractical for a handheld device. The 3D effect did at least make the
system stand out to something new, but in the end, this didn't translate well to sales,
with Nintendo struggling to sell many units outside Japan, leading to a surprise price
cut of $70 only four months after launch. After this, sales increased, but it was a
significant sacrifice for Nintendo to make, just to get the device off the ground.
The PlayStation Beta on the other hand launched at the end of 2011 in Japan,
and in February 2012 outside Japan, meaning it took almost one year longer to release than its rival.
Its price of $250 put it in line with the 3DS's launch price, but behind its updated price.
Reviews for Sony's system were also positive, with the improved performance, impressive display,
rear touchpad, and second analog stick all helping to make the system stand out as a
significant step up from the original PlayStation portable. The main criticism was the memory card
requirements, which created an additional and necessary further cost that was made worse
by the fact that only Sony brand memory cards worked with the device, meaning users had no
choice but to buy Sony's expensive cards, even if they already owned other memory cards.
Still, the Vita did its best to get off to a strong start, and boasted a relatively big launch
lineup of 25 titles, which was more than most consoles or handhelds in this era.
Those games included Uncharted Golden Abyss, Wipeout 2048, FIFA 12, and Rayman Origins.
The Vita's first year would also see more major releases such as Little Big Planet,
Persona 4 Golden, Assassin's Creed 3 Liberation, and Call of Duty Black Ops Declassified,
as Sony tried hard to establish the system's reputation with core gamers.
At first, this looked like it might work, with the Vita seeing promising sales at launch,
but these numbers soon declined, and by the end of 2012, it had only sold 4 million units,
well below Sony's expectations. And with slow sales came a loss of interest from many third party
studios, causing Sony to abandon their strategy of trying to sell the system through big releases,
and instead adopt a more indie-centric approach, where they reached out to mobile and PC developers
to try to bring as many promising smaller titles to the platform as possible.
In August 2013, Sony also dropped the price by $50, alongside a reduction in memory cards,
and in conjunction with the more successful indie focus and the system's strong sales in Japan,
the Vita was able to survive. But it never came close to matching its predecessor, or the 3DS.
As for Nintendo, they would launch an updated 3DS Excel model in summer 2012,
followed by a model without the 3D display named V2DS one year later,
followed by a new, more powerful version called the New 3DS in late 2014.
Alongside Nintendo's characteristic strong first party lineup,
and decent third party support from companies like Square Enix, Konami and Capcom,
this helped keep sales far beyond the Vita.
The 3DS would even see the release of a new Monster Hunter game from Capcom,
which had long been one of the PSP's best-selling series.
Still, as dominant as the 3DS was over the Vita, it still fell far short of the original DS,
with less than half of its total lifetime sales.
The reality was that both the 3DS and Vita were solid handheld devices
that made significant improvements on their predecessors.
The 3DS's much-tounted 3D struggled to prove its value in the long term,
with many players simply turning it off for most games once the novelty wore off,
and the Vita shot itself in the foot with how it handled memory cards.
But in a different era, it's easy to imagine both systems finding strong commercial success.
Still, in the end, it wasn't a different era, and in the 2010s,
the 3DS and Vita combined wouldn't even come close to matching the mobile market.
The harsh reality was that in a world where most people already had smartphones,
dedicated handhelds simply seemed to have too much competition.
And so, for Sony and Nintendo, this would be the final handheld generation.
Nintendo dominated the handheld market for decades,
often with a staggering market share, but nothing lasts forever.
As for mobile gaming, it revealed an entirely different side of the industry,
one with different monetization models, different games, and different dominant regions.
For most observers of this, the biggest takeaway was the industry's size and growth.
At the start of the 2010s, the mobile industry was still well behind consoles or PC,
but as the decade continued, it soon caught up, and by the decade's end,
it was considerably larger than all other types of gaming combined.
Most mobile games made money through in-app purchases, or in-app advertising,
rather than the more traditional buy-to-play model,
and the amount of money individual games could make was staggering.
The game with the highest revenue was Arena of Valor,
also known as Honor of Kings in China,
which is a MOBA made by Chinese gaming giant Tencent
that's brought in over $14.6 billion since its release in 2015.
If numbers that large seem a bit abstract to you,
I like to use the currency called the Bethesda.
Microsoft bought Bethesda and the IPs for $7.5 billion,
so Arena of Valor's revenue is about two Bethesdas.
Other high-grossing mobile games of the decade include the RPG physics game Hybrid Monster Strike,
Supercell's fantasy-themed strategy game Clash of Clans,
the Battle Royale-focused PUBG Mobile, the Match 3 RPG Hybrid Puzzle and Dragons,
the more traditional Match 3 game Candy Crush Saga, and Pokemon Go,
an augmented reality-based Pokemon game from Neantic
that received a lot of mainstream attention during its release in 2016
for its unparalleled ability at making gamers touch grass.
It also caused Nintendo's stock to increase by 50% in the space of a week.
It can be difficult to talk about mobile games without the conversation eventually
leading back to money, whether it's the growth of the industry,
the revenue of individual games, or the methods of monetization that are commonly used.
Still, for many people, mobile gaming is about the convenience of games that can be played in
short bursts, wherever you happen to be, with little barrier of entry.
As the industry is growing to be so large, it's developed a lot of choice and diversity,
and as smartphones have continued to become more powerful,
what can be achieved on them has increased at the same time.
In the end though, regardless of people's various opinions on mobile gaming,
it's impossible to deny the industry's significance,
particularly as the trends seen in the mobile industry would soon make their way into the gaming mainstream.
Online gaming has been steadily growing since the early 90s,
and the 2010s were no exception. What would change this decade though,
was the way online games made money. World of Warcraft was a hugely important game,
not just because it had 12 million active players, but because those 12 million players
were each paying a monthly subscription fee. Compared to a more traditional one-time purchase,
monthly fees were massive, but when it came to further monetization, this was only the beginning.
The one-time purchase business model didn't fit well with online games that were being continually
updated and emphasized long-term engagement, and so change was inevitable. One of the earliest
and most significant examples was Team Fortress 2, which maintained a steady active player base
after its release in 2007, while receiving regular free updates, adding additional content.
In 2010, one such update allowed players to buy and sell in-game items, like hats,
and the game's revenue immediately quadrupled. In 2011, the game was updated to be completely free
to play, as revenue from in-game transactions had come to far exceed those of people purchasing the
game, and after Team Fortress 2 went free to play, revenue went up again, and the industry took notice.
At the same time as this, new online games like League of Legends were also starting to draw in
lots of players while making a big profit. League of Legends was also free to play and made money
through cosmetic skins and purchasable champions. Players could acquire these champions through
slowly earning in-game currency as they played, but many would choose to purchase them for real
money, with more champions and skins being added over time to ensure there was always something
new to buy. At the time, this business model was largely praised, with the free-to-play aspect
helping the player base grow, and the earnable in-game currency providing an alternative to
needing to spend real money. Of course, players did still spend money, and as the game's highly
competitive nature ensured strong replayability, the player base grew and grew alongside the game's
revenue, as League of Legends became one of the most played and most successful games of all time.
Still, changing monetization wasn't exclusive to multiplayer games.
Once upon a time, arcade games made big money by making gamers pay to play,
but some games went even further, like Double Dragon 3, which featured an in-game shop where
players could purchase a range of items, including new characters, weapons and power-ups. This was
widely criticized, and arcades eventually lost popularity, making the one-time purchase model
of non-arcade games even more standard. Older PC games did often sell physical expansion packs
that added additional content at less than full price, and these were generally regarded quite
positively as a fair way to expand existing games. Things advanced with the arrival of
Xbox Live, where some games started to provide bonus downloadable content, usually for free.
The Xbox Live marketplace then evolved with launch of the 360, and soon featured small
pieces of downloadable content at small costs, with one of the most infamous being the $2.5
cosmetic horse armor in the Elder Scrolls IV Oblivion. A similar practice was later adopted by Sony
and Nintendo at the same time, but music games like Guitar Hero and Rock Band were gaining
popularity while making purchasable downloadable songs a core part of their business model.
And so, smaller, or micro-transactions, were introduced to multi and single-player games alike.
Electronic Arts also introduced micro-transactions via in-game stores
into several existing series, like The Sims with its third entry.
EA would then add a form of luck-based micro-transaction in the 2009 edition of FIFA,
something that is now commonly referred to as loot boxes.
Loot boxes would go on to be added to many other popular online games, including Counterstrike,
Team Fortress 2, Battlefield, Call of Duty, League of Legends, and more.
In 2011, Rockstar pioneered a new approach by selling content that didn't exist yet
in the form of a Rockstar pass that could be purchased to unlock future content.
Other games like Mortal Kombat and Call of Duty soon adopted this season pass approach as well,
and in Call of Duty, this would split the player base between those with the season pass maps and
those without. In 2012, Capcom's Street Fighter tech and crossover drew criticism after including
future DLC files on the game's physical disc. While in 2013, Dead Space 3, again from EA,
became one of the first single-player focused games to offer a purchasable weapon and resource pack
to assist players in its campaign. Later games like Assassin's Creed Origins from Ubisoft and
Middle Earth Shadow of War from Warner Bros would take paid advantages in campaigns significantly
further, and in time, micro-transactions, in-game stores, loot boxes, and season passes would all
become common practice in many big-budget games, although often not without criticism from players,
the media, and even governmental regulators who have likened loot boxes to gambling and raised
concerns over their potential harmful impacts. Meanwhile, gaming companies have often sought
to defend these additions by claiming that they're only providing players with more choice,
and that increases in video game budgets have occurred over time while game prices have stayed
relatively flat, meaning further monetization methods have become necessary. Still, regardless
of whether any necessity does truly exist, more monetization means more money, and businesses
don't usually turn down a chance for extra profit. As single-player focused games are less
monetizable, a trend also began towards games as a service, or live service games, where AAA
games tried to expand into online areas or just placed increased focus on future content updates
to create continuous engagement from players. The king of live service games is unquestionably
Grand Theft Auto V, which continued Rockstar's tradition of high quality and university acclaimed
open world titles in 2013, but then took things a step further with Grand Theft Auto Online.
GTA Online allows up to 30 players to freely roam GTA V's world while completing lobby-based
missions and earning experience points with a focus on general progression, and while it did
originally have a rocky launch, it soon went on to be hugely successful. When Grand Theft Auto V
released, it easily surpassed Call of Duty's sales records, selling 11.2 million copies in its first
24 hours. Unlike so many of the best-selling games before it, however, Grand Theft Auto V continued
to put out impressive sales figures year after year after year. This was partly because of future
releases on next-gen consoles and PC, but it was also because of the impact and enduring popularity
of Grand Theft Auto Online. And so, the gaming industry saw how blockbuster single-player games
could be transformed into blockbuster live service games, where they could not only continue to sell
more copies due to staying relevant for longer, but could also tap into the unbelievably lucrative
territory of multiplayer-fuelled microtransactions. One year after GTA V, Bungie released their new
online-focused first-person shooter Destiny, which combined elements of the MMO genre with FPS gameplay.
Destiny was also very profitable, with publisher Activision claiming it was the most successful
new gaming franchise launch in history, and between the success of GTA Online and Destiny,
the live service trend was established and proven beyond doubt. Others would soon follow,
including several developers that had previously built reputations around creating single-player
games such as Bioware with Anthem, Bethesda with Fallout 76, and Crystal Dynamics with Marvel
Avengers. Clearly, not every live service game was a success, but for the biggest publishers in
the industry, live service offered the biggest rewards, and the trend towards live service games
is still very much alive today. Still, online gaming in the 2010s wasn't just defined by changes
to monetization. The decade would also see a number of new genres and new takes on existing
genres as online gaming expanded upwards and outwards. In 2014, Blizzard's Half Stone achieved
major success for its accessible and polished take on the digital collectible card game genre.
In 2015, Rocket League from Xionix combined cars, rockets, and football to create one of the decade's
freshest sports games. And in 2016, Blizzard's Overwatch received great critical acclaim for
its team and hero-based FPS design that stood out from most other popular shooters at the time.
The decade's most popular new genre would come once more from a mod, this time from Brendan
Green, also known by his online alias Player Unknown, who created Daisy Battle Royale, which
itself was an offshoot of the Daisy mod for Armor 2. Taking inspiration from the Japanese film of the
same name, the mod focused on player-vs-player combat with a large number of combatants on a big
open map. The scale of the map and the random placement of weapons was intended to introduce
an element of unpredictability, and the battleground was designed to get progressively smaller as the
games went on to ensure regular action. The result turned out to be a hit and would eventually lead
to the creation of the standalone Player Unknown's Battlegrounds in March 2017 that soon turned Battle
Royale into a worldwide gaming phenomenon. Seeing the success of Player Unknown's Battlegrounds,
Epic Games decided to create a Battle Royale mod for their upcoming construction shooter hybrid
Fortnite to create Fortnite Battle Royale in September 2017, and between the two games,
the gaming industry was never quite the same again. Fortnite and PUBG offered different
takes on the same idea, with PUBG being more focused and prioritizing realism, and Fortnite
standing out thanks to its building mechanics and more cartoony aesthetic. The games would also
fare differently in different markets, with PUBG being a massive success in Asia and Fortnite
finding dominance over schoolyards across America and Europe. Still, one thing both games had in
common was being staggeringly successful. PUBG quickly broke the record for most concurrent
players on Steam with over 3.2 million, which was about 2 million more than the next highest game on
record, and soon Fortnite was able to boast even greater numbers. Many other Battle Royale games
followed in the wake, some successful, some not, some higher budget, some low, some shooters,
some anything but, and while most of these would not find long-term popularity, that didn't stop
plenty of developers from trying anyway. The 2010s were also the decade where online gaming
spread beyond games. Gaming tournaments had been around for a long time, and by the 90s
some were even quite large, like the Nintendo World Championships in 1990, or the Evolution
Championship Series for fighting games starting in 1996. In the 2000s, televised coverage started
to be more popular, particularly in South Korea, where StarCraft was well on its way to becoming
the new national religion. In the mid-2000s, esports history was made at Evo 2004, when Daigo
Umihara, playing as Ken in Street Fighter III, performed the Daigo Parry.
The 15-hit consecutive parry with only one pixel of health remaining, into a comeback victory
that would see Umihara through to the finals, has since been described as the most iconic
moment in competitive gaming history. It was also a perfect showcase for the potential appeal of
esports, and by 2010, that potential was quickly becoming reality. New online streaming services
opened esports to a much wider audience, and with more popularity, would come greater prize money
and bigger stages, as StarCraft, Counter-Strike, League of Legends and Dota all helped push esports
into the mainstream. With time, more and more games would follow as the numbers kept rising,
and by 2019, Dota 2's The International Tournament had a total prize pool of over $34 million,
and League of Legends 2019 World Championships would see a peak of 44 million concurrent viewers.
Over the course of the decade, esports became big business, but people weren't only watching
competitive gaming. Livestreaming and let's plays made possible by platforms like Twitch and YouTube
also rose to popularity in the first half of the decade, becoming a major new tool for video game
marketing and drawing great amounts of attention to certain titles. Horror games like Amnesia
The Dark Descent and Five Nights at Freddy's, or the social deduction game Among Us, stand out
as examples of how small games with little marketing can still have massive impacts through
this new online social media field ecosystem. Social media also brought a greater focus to
various controversies. Video games and controversy have long been acquainted, for example Mortal
Combat and Night Trap in 1993, Doom following the Columbine shooting in 1999, Grand 5th Auto San
Andreas following the discovery of the hot coffee mod in 2005, Panic over the MMO genre following
the success of World Warcraft, and Call of Duty Modern Warfare 2's civilian killing No Russian
Mission in 2009 all provide examples of major gaming controversies. But all of these past
examples had one thing in common, which was that the controversy was caused by people outside of
the gaming industry looking in. In the 2010s however controversy started coming from within
while becoming more frequent and more intense. One of the earliest examples of this was the
ending for Mass Effect 3, which fans viewed as disappointing for the similarity between different
endings and for their failure to represent players choices over the trilogy. The negative
response led to an open letter from Bioware's founder, charity drives from fans, statements from
the Federal Trade Commission and Better Business Bureau, a revised extended cut DLC, a significant
number of cupcakes, and a disparaging parody from Weird Owl.
For a disappointing game, this level of negative attention was unprecedented,
but it turned out this was just a taste of what was to come. In 2014, gaming media and gaming
communities embraced the political culture war with Gamergate, and both have been vigorously
arguing about politics ever since. But there were also controversies over monetization,
like with Middle Earth Shadow of War, Star Wars Battlefront 2, and Genshin Impact.
Controversies over the disappointing state games released in, like with No Man's Sky,
Fallout 76, and Cyberpunk 2077. Controversies over working conditions and crunch periods at
studios, like with Bioware, Rockstar, and Naughty Dog. Controversies over sexual misconduct,
like with Riot Games, Ubisoft, and Blizzard. Controversies over graphical downgrades,
controversies over DRM, controversies over lack of accreditation, controversies over lack of
difficulty options, controversies over games released too soon, controversies over games
released too late, controversies over story decisions, controversies over censorship,
controversies over translations, controversies over voice acting pay, controversies over cameos,
controversies over original authors, controversies over political representation,
gender representation, sexual representation, racial representation, and religious representation,
and controversies over controversy, and controversies over a lack of controversy.
And you could probably drag this section out even further if one was so inclined.
It's worth keeping in mind that for media outlets and online content creators,
controversy is one of the best ways to make money, and people are willing to do whatever they can
to encourage it. More surprising though is how much consumers seem to have embraced this approach,
and how difficult it's become to avoid this constant focus on controversy within gaming
communities as a result. And so, the 2010s were a decade where the internet changed many things,
and video games were no exception. Still, while not all of these changes were positive,
the internet did allow for one part of the industry to flourish like never before.
After major breakthroughs in the 2000s, independent game development continued to thrive in the
2010s. This period saw mainstream games growing cost and development time, which often led studios
to taking increasingly conservative approaches by sticking to proven genres and series.
Meanwhile, the smaller budgets of indie games meant they only needed a fraction of the sales to
be profitable, meaning developers could still succeed by appealing to niche audiences or through
taking greater risks. The result was a huge amount of diversity, from genre to art style to level
of accessibility or more, as indie games brought variety at a time when the industry needed it
the most, which can be shown clearly by looking back through some of the decades most critically
and commercially successful titles. 2010 saw the release of Playdead's darkly atmospheric
experiential puzzle game Limbo and Team Meat's fast-paced and fiendishly difficult 2D platformer
Super Meat Boy. 2011 would bring a new take on the roguelike genre with the Binding of Isaac,
which added action gameplay and Zelda-inspired procedurally generated dungeons as players
helped Isaac escape his abusive mother. Relogix Terraria also released that year with the
expansive 2D sandbox game going on to sell 44 million copies, making it one of the best-selling
games of all time. In 2012, Journey from that game company impressed many with its striking
use of visuals, audio and non-verbal storytelling that would help it win several Game of the Year
awards, whereas Deniton Games' top-down shooter Hotline Miami came in hard with a mix of ultra
violence and surrealism to provide a somewhat contrasting experience. 2013's Paper's Please
from Lucas Pope tested players' morality by putting them in the shoes of an immigration
inspector at a border checkpoint in the glorious dystopia of Aristoska. In 2014, Yacht Club Games
8-bit side-scrolling platformer Shovel Knight paid homage to the gaming's past while impressing
with cutting-edge controls and level design. 2015's Undertale from Toby Fox provided a clever
twist on RPG conventions and created an enduring fandom alongside one of gaming's greatest sound
tracks. In 2016, video game players that wanted to avoid going outside were positively spoilt for
choice. Concerned Ape Stardew Valley became a sensation for its open-ended and relaxing take
on the farming sim genre. Plactorio from Woop Software gave players an endlessly deep factory
and automation simulator while establishing its own genre in the process, and Rimworld from Ludeon
Studios combined base-building and emergent storytelling in its addictive and relatively
accessible take on the colony's sim genre. Meanwhile, 2017 seemed determined to test players'
skills with the demanding and impeccably presented Cuphead from Studio MDHR that had players
running, gunning, and repeatedly dying to its many creative boss fights, and Hollow Knight
from Team Cherry, an expansive and surprisingly epic Metroidvania that had players take on the
role of a nameless knight as you explore the Fallen Kingdom of Hallownest. In 2018, Subset Games
Into the Breach offered strategy fans something new by stripping away much of a genre to place
all the focus on its rich, tactically engaging, turn-based gameplay. 2019 would see the release
of Megacritz, much copied yet never bested road-like deckbuilder Slay the Spire, as well as Mobius
Digital's time-loop adventure game Outer Wilds that gave players complete freedom to explore
its miniature solar system as they unraveled the mysteries of its unforgettable universe.
Finally, in 2020, Supergiant Games wowed the world with the action-road-like Hades,
a game with so much content and such high-quality presentation that it almost seems to have left
the indie label behind. Indie games have come a long way, to the point where the term indie
increasingly tells you very little. Once upon a time, the word seemed to imply certain characteristics,
like games but as short, cheap, retro, or artsy. Yet, by the end of the decade, indie could imply
almost anything, with massive companies like Ubisoft, EA, and Tencent all creating games in
traditionally indie styles, all while independent developers increasingly create games with a level
of ambition, size, or presentation comparable to many big-name studios. It's also worth noting that
there are many other games that could have been mentioned in this section as well, and it is a
testament to the strength of the indie market, just how spoiled the choice you are when looking for
high-quality games to highlight. Still, the vast number of indie games being developed over the
past decade became a talking point in itself, with many raising concerns that so many indie games
makes it harder and harder for indie developers to get noticed and find success.
As the indie market seemed to be reaching a point of saturation in the later half of the decade,
the term indie-pocalypse started to be used to refer to this situation, and yet while increased
competition may have made success harder, there has been no sign of an indie market collapse,
and good games continue to be released. This continued rise of indie gaming also brought
a number of related trends. Minecraft paved the way for the concept of early access,
where many indie games would release unfinished and receive continuous updates over time.
This helped establish new genres like survival games, while also aiding developers by assisting
with funding during development and making it easier to incorporate player feedback.
The downside of this system was that it could also lead to anti-consumer practices of misleading
advertising, over-promising, or just abandoning games unfinished after receiving players' money.
Still, for every standout early access disaster, it's easy to find other standout successes.
Alongside early access, the 2010s also saw the rise of crowdfunding in game development,
usually through the platform Kickstarter. This often involved respected figures within
the industry wanting to bring back classic series or genres that publishers deemed too risky to fund,
and crowdfunded games could vary between relatively big games from already well-established studios
to much smaller projects from individual developers.
With time, many crowdfunded games were released, and amongst them were many of the decades most
acclaimed indie games, but there were also plenty of notable failures, from MMOs accused of being
scams, to games that critically bombed, or just games that never released at all.
Through the combination of crowdfunding and indie growth, the 2010s would see a lot of genre
revivals, particularly from those once popular in the past that don't fit well with modern AAA
game design. These included RTS games, point-and-click adventures, CRPGs, shoot-em-ups, boomershooters,
space sims, classic survival horror, beat-em-ups, FMV games, and many more.
Retro-inspired games in particular saw a resurgence in the 2010s, possibly because people who once
grew up with older games were now old enough to create games themselves and took inspiration from
their favourite games of the past, but also because the smaller budgets of indie games fit well
with retro-inspired design. And developers weren't the only ones making games this decade.
One of the most played games at the end of the decade, and one of the most confusing for older
generations was Roblox, a game creation platform that allows users to share their own games inside
its engine. Roblox attained great popularity with younger generations, probably as much like
Minecraft before it, it tapped into players' natural creativity. Finally, the 2010s would
also see game development go global as indie development and digital distribution opened
the industry up to more regions around the world. Still, as the ninth generation of consoles finally
approached in the latter half of the decade, the attention would once more return to Japan.
No company has more experience in the gaming industry than Nintendo, but in 2012 the gaming
veteran experienced something new. As for the first time since entering the industry,
Nintendo would see a financial loss for the year, of $366 million. In 2013 they would report
similar losses and in 2014 those losses would grow even greater. The Wii U was a failure,
but for the first time Nintendo was starting to fail as well. They had never been further
behind in the console race, the once reliable handheld market was rapidly shrinking and unlike
certain other big names, they didn't have additional areas of business outside of gaming.
Video games were basically everything to them. And this is the climate in which they would
create their next and possibly last console. Nintendo's growing desperation was visible
long before their next console would be ready. In 2014 the company heads crafted a new strategy
to turn their finances around. A key part of this was to reverse their earlier stance on
mobile gaming. Originally Nintendo wanted to preserve the quality of their franchises and
continue their practice of only allowing Nintendo games on Nintendo hardware, but now they were
looking to enter the mobile market in an attempt to maximize their intellectual property, leading to
core Nintendo series like Super Mario, Animal Crossing and Mario Kart making their mobile
debuts. This was also the time that Nintendo launched Amiibo, a toys to life figure series
that had built in functionality with certain Nintendo games. Still as useful as additional
monetization might be, Nintendo still knew the deciding factor would always be their next console.
So how do you respond to record losses and your greatest ever failure? Well,
Nintendo's answer was to double down and try the same thing again. And so once more
Nintendo would avoid competing with their rivals in terms of power. Once more they would focus the
console around a portable screen and once more they would target the same audience with the same
intent to bridge the gap between serious and more casual gamers. Still far from creating a Wii U 2,
this console would instead learn from the Wii U's mistakes and address its greatest shortcomings.
Providing true portability without needing to stay close to a console as well as better battery life
and more comfortable and versatile design and of course less confusing marketing.
Officially announced in October 2016 after years of fan rumors and released in March the following
year, the Nintendo Switch was a gamble. By this point many Nintendo fans were sick of being behind
the competition in terms of power and were frustrated by Nintendo's continued focus on gimmicks.
The Nintendo Switch ignored this. Similarly, the Nintendo 3DS, while less successful than its
predecessor, was still Nintendo's best-selling system at the time and by creating a new portable
device Nintendo will risk in competition with their own system while also combining their two
main existing revenue streams. Once more after several disappointing years, Nintendo were fast
approaching the point where they simply couldn't afford another lackluster launch and following
the reveal, plenty of onlookers were expressing doubts. But in the end, those doubts would not
be founded. The Switch launched strong. By the end of March, it had sold 2.74 million units,
exceeding Nintendo's expectations. Nintendo responded by increasing production capacity.
The Switch responded by continuing to sell. Nintendo upped their forecast for the financial
year aiming for 10 million units sold, and the Switch then upped its sales again,
meeting this target with months to spare. By the end of its first year, the scale of
Nintendo's success was starting to be revealed. It would take the Switch less than 12 months to
overtake for lifetime sales of the Wii U, and in the process it became the fastest-selling
games console in history. Things change quickly in the gaming industry, but sometimes they change
in your favor. There is no single reason for the Switch's record-breaking success,
and its reception from reviewers was no more positive than the Wii U before it.
The portability of the system is a clear upgrade from its predecessor, and the Switch may have
benefited more from a lack of handheld competition, as while smartphones were more common than ever,
the Switch offered something much more traditional and substantial than mobile gaming did,
which might have appealed to a different audience. It may also have benefited from the
failure of the Wii U. Nintendo learned from the earlier systems hardware and marketing
to make needed improvements on their second attempt, and the Wii U's underplayed library
also gave Nintendo the perfect opportunity to port many of its best games over to their newer system
to quickly expand the Switch's lineup and ensure the system got off to a strong start.
And speaking of games, the Switch certainly had its share, including a launch title that may have
been the first true killer app seen in more than a decade. The legend of Zelda, Breath of the Wild,
is one of the most highly acclaimed games in history. By taking Zelda Open World, it gave
new life to the Zelda series and the Open World genre, with a more hands-off and exploration-driven
approach to game design that many felt represented what had been lacking from many other games of
the modern era. Breath of the Wild was a true system seller, and its sales clearly show this,
as it accounted for almost 50% of the Switch's early software sales,
with the game selling nearly one-to-one with the console. Launching with a mainline Zelda title,
let alone one of the best-received Zelda titles in history, is always a significant advantage for
a console, but the Switch's first year included more hits than just Breath of the Wild. Soon
after its release, it would receive its all-time best-selling game by way of the ever-popular
Mario Kart 8, which was an expanded version of the Wii U title, as well as a well-received sequel
to third-person territory control game Splatoon, and even a new 3D Mario game in Super Mario Odyssey,
which gave the Switch its second critically adored, must-own title.
In a time when consoles were increasingly launching with weaker and weaker lineups,
and had less and less exclusive games, the Switch finished its first year looking like
a true heavyweight, with plenty more still to come. And as the Nintendo Switch got off to a
great start, it pretty much solved Nintendo's long-running third-party support problem,
as many developers no longer wanted to miss out on the Switch's larger user base.
But it wasn't just Nintendo who would have a good year in 2017, and it wasn't just Nintendo
who needed such a year. After the Nintendo Entertainment System launched in 1985,
Japan dominated the gaming industry for over 30 years. But as gaming moved to high definition,
this quickly started to change. More and more of the best-selling and highest-reviewed games were
coming to be from outside Japan, with major Japanese series like Final Fantasy and Gran Turismo
seeming to experience longer, more drawn-out development periods and a decline in critical
and fan reception. In this period, a new trend began of Japanese studios outsourcing
established franchises to Western developers, something that was once almost unheard of.
And this wasn't just small, unknown series either, it included some big and beloved names,
like Resident Evil, Silent Hill, Devil May Cry, Dead Rising, Castlevania, Contra, and more.
The worst part of his trend was that almost every one of these games was received far more poorly
than the older, Japanese-developed games of the same series. Meanwhile, other major Japanese
publishers like Square Enix would make moves to acquire Western-based developers in order to adapt
to the changing market demands. Konami went even further by going on to abandon games altogether
in favor of focusing on gambling and the pachinko market, while industry giant Sony shifted their
focus far more heavily towards Western developers and Western audiences. And this was more than
just a change in where development was taking place. From 2007 to 2012, the Japanese gaming
industry shrank for five years in a row, and the only major Japanese developer doing well in this
period was Nintendo, who were about to release the Wii U. And so, as the Japanese gaming industry
stumbled and fell, a lot of people started to take notice. The views of the general gaming
populace were perhaps best summarized by once-popular indie developer, Phil Fish,
when asked what he thought of recent Japanese games.
As tone deaf as this might now seem, at the time, Fish really was just saying something many people
were thinking, and it wasn't just Western developers and journalists. Japanese developers
were also highly critical, and it's not an exaggeration to say that the Japanese industry
was experiencing a genuine crisis that was made even more dramatic by how much of a
fall from grace it seemed to represent. But in 2017, this would finally change,
as Japan took back its crown. It began before the year even started. At the very end of 2016,
two games famous for being stuck in development hell finally released. Final Fantasy XV was first
shown in 2006. The Last Guardian, the next game from the team behind Ico and Shadow of Colossus,
was first announced in early 2007. Ten years later, after much anticipation,
derision, and desperation, these games finally made it to the public, and both were received
positively. But this was only a taste of what was to come. January 2017, Resident Evil 7 released
two critical acclaim after the previous mainline entry was widely panned and achieved success
by returning the series to its survival horror roots. The very same day, Yakuza Zero released in
the West, becoming the most successful Yakuza game of all time and popularizing the series outside
Japan. In February, Nioh released ending developer Ninja Theory's long run of poorly received titles
and becoming their most highly acclaimed game since Ninja Gaiden in 2004. March brought Nioh
Automata, which released a critical acclaim, becoming the first game from beloved oddball
game director Yoko Taro to reach mainstream audiences. This was the same month that the
Nintendo Switch and Breath of the Wild also made their debut. April continued with Persona 5,
which became the most highly reviewed JRPG since Final Fantasy IX in 2000,
while also making thousands of people online say the words, I don't usually like JRPGs, but...
Something worth keeping in mind here is that Yakuza Zero, Nioh, Nioh Automata,
and Persona 5 all released as PlayStation exclusives in the first half of 2017. All
received great reviews and all sold millions of copies. And yet at E3 2016, Sony wouldn't promote
any of these four games. They ended up as some of the best PlayStation exclusives in years on a
console that was at the time light on exclusives, and yet Sony had refused to show any of them
in favor of focusing on multi-platform games like Call of Duty, Modern Warfare Remastered,
and Lego Star Wars The Force Awakens. This was probably because these games simply looked to
Japanese, and despite being a Japanese company, Sony seemed to decide that showing such Japanese
looking games could be bad for their brand. This was the situation Japanese games were in,
at least this was the situation in the eyes of the biggest Japanese games company in the world,
until 2017 happened. June brought the triumph in return of one of gaming's most iconic fighting
series, Tekken, after an 8-year absence. July brought the release of Dragon Quest XI,
only in Japan, and yet Japan alone was enough to reach 5 million copies sold.
August brought the return of 2D Sonic in Sonic Mania, a game not made by Japanese developers,
but instead by several Sonic fans, and yet it went on to give the iconic Japanese series
its best-reviewed game since the Dreamcast. September brought the return of Metroid in
Samus Returns, 7 years after the last Metroid game, and 13 years since the last 2D Metroid.
October brought Super Mario Odyssey, and December rounded the year off with Xenoblade Chronicles 2,
the best-selling game in the long-running Xeno series, and another reason for Switch and
JRPG fans to be happy that year. For video games, some years are better than others,
and 2017 was without doubt one of the best of the modern era.
There were good Western and indie games released that year as well,
but it was Japanese developers who stole the show, and the idea that the Japanese games industry
was falling behind, or dying, would be permanently dispelled ever since.
Nintendo and the Switch would find further success in the following years,
with major titles like Super Smash Bros Ultimate, Animal Crossing New Horizon,
and many, many Pokémon games. Still, as the decade reached its end,
attention would return once more to the other two major giants of gaming,
as their next generation of consoles were getting ready to finally release.
The most interesting thing about the PlayStation 5 and Xbox Series X
is how uninteresting they are. Once upon a time, different gaming consoles were
entirely distinct species, but look past for deceptively varied exteriors of Sony's
and Microsoft's latest efforts, and you'll find near-identical pieces of hardware.
AMD 8 cores N2 CPUs, AMD Radeon R-DNA GPUs, similar SSD drives,
similar promises in resolution and frame rate, similar backwards compatibility,
similar disc and digital-only variants, similar prices although the digital-only
Xbox is cheaper and less powerful, and a similar release date, which basically means
unless you're a tech enthusiast, a dedicated fanboy or girl, or a paid marketer, there isn't
anything important separating the two machines at a technical level.
The real proof of this is in multi-platform performance,
where once the 360 beat the PS3, and then the PS4 beat the Xbox One, now there's no clear difference.
Some games might perform better on one system over the other, but even then,
the differences tend to be small and don't always follow any general rule.
This doesn't mean the launch of the two systems was uneventful,
but even here, events of note had nothing to do with the systems.
A global pandemic combined with a global microchip shortage
led to supply issues and stock shortages for both companies, which then led to heavy scalping.
For what it's worth, the PlayStation 5 got scalped harder and would eventually sell more,
with recent figures putting the PS5 at 30 million units sold to the Xbox's 18.5 million,
although even in 2022, hardware availability was still a limiting factor.
Regardless, even if the console war has turned cold, there have still been major developments
that will likely shape the future of gaming in the years to follow.
The first is the emergence of virtual reality headsets, which would see a major breakthrough
when 18 year old Palmer Lucky had the idea to use cheap lenses in place of older expensive lenses
and instead tackle the problem of lens distortion through software,
creating the potential for much lower cost systems.
Lucky was joined by John Carmack of Doomfame and together they created the company Oculus VR
in 2012, which was then purchased by Facebook two years later.
Sony and Valve would also start development of their own headsets, and in 2016 the Oculus Rift,
the Vive from Valve and HTC, and PlayStation VR all launched to the public and received a mixed
reception. Praise was given to the immersiveness and uniqueness of the experience, while criticism
focused on the high prices, high latency, the practicality of space required, long-term comfort,
and the possibility of motion sickness. In time, second generation headsets arrived
that made significant improvements over their predecessors. The Oculus Quest 1 and 2 provided
standalone headsets that removed the problem of wires. The Valve Index provided impressive
field of view and resolution while making improvements to comfort, albeit at an impressively
high price, and recently the PlayStation VR 2 provided a high-end headset with an OLED screen
and a strong price to performance ratio. Seven years after VR headsets first arrived,
it's become clear that despite their rather lackluster start, VR technology is making impressive
strides and still getting better every year. That said, truly widespread adoption still hasn't
happened yet, and the experience provided by VR headsets has so far seemed to be more of an
adjacent experience to traditional gaming than a replacement. As while VR does allow for completely
new types of experiences, it also has significant disadvantages, like the limitations of controls
or long-term comforts that make it a bad fit for some areas of gaming. Still, when it comes to the
console war, Sony has established themselves as one of the best manufacturers in the VR industry
and are placing more and more focus on VR games. While Nintendo have mostly stayed away,
and Microsoft has made some big moves in the past but have distanced themselves more recently,
meaning if VR does become a larger part of the industry, it's Sony who looks like the one
who'll benefit the most. And if the rate at which VR technology is improving remains steady,
it's easy to imagine the VR industry seeing significant growth over the 2020s, as better tech
leads to more users, which leads to more games, which leads to more users, as the positive feedback
loop kicks in and VR finally becomes a core part of the industry alongside consoles, PC and mobiles.
But while Sony might be winning at VR, Microsoft has seen growth in a different area.
Game streaming has been around since the start of the decade, with companies like OnLive and
Gaikai offering forms of cloud gaming, a service where you play games on a remote server, which is
then streamed directly to a user's device. This removes costly hardware requirements,
but also requires high-speed, low-latency internet connections and substantial data usage.
Neither OnLive nor Gaikai would turn a profit and both would eventually be acquired by Sony,
but in time other cloud-based streaming services would appear to take their place.
Meanwhile, both Sony and Microsoft launched their own game subscription services,
PlayStation Now and Xbox Game Pass, which would incorporate cloud gaming as well as provide
access to a range of games that can be played for no additional cost. Of all subscription and
streaming-based services, Xbox Game Pass has reached the most users, while seeing substantial
uptake amongst PC players. It seems clear that the plan is to create something like a
Netflix for video games, however despite heavy investment from Microsoft, they still haven't
been able to reach their target numbers and are facing a lot of competition from other companies
like Amazon and Nvidia. The full impact of game subscriptions and streaming remains to be seen.
Some worry that widespread use of game subscriptions will move games further towards
additional monetization methods like microtransactions, while widespread use of streaming
could have an even more substantial impact as it threatens the existence of traditional console
and PC gaming entirely, particularly in regions where people may already have less disposable income.
Rumors also currently exist about a cloud gaming handheld device from Sony,
and it seems possible that cloud gaming will come to play a much greater role in the industry
in years to come. Game Pass hasn't been Microsoft's only recent investment, however.
The final trend that seems to be defining the current generation is consolidation.
When Microsoft announced its acquisition of five smaller studios at E3 2018, few questioned the move.
Sony had been winning the exclusive war for years and showed no signs of stopping,
so it made sense that Microsoft would look to acquire talented studios of their own to create
dedicated first-party titles to help them keep up with the competition.
Microsoft's acquisitions didn't end at E3 2018, however. First, there were more smaller studios
announced, then there was the more surprising news of a deal to acquire Xenomax in 2020 for $8.1
billion, which included Bethesda and big-name franchises like The Elder Scrolls and Fallout.
Then, other large companies seemed to join in with major purchases,
like EA acquiring Glue Mobile for $2.4 billion, Take 2 acquiring Zynga for $12.7 billion,
and Sony acquiring Bungie for $3.7 billion. Still, the biggest buyer would once again be Microsoft,
who announced the deal to acquire Activision Blizzard for $68.7 billion in 2022.
Activision Blizzard are the largest third-party publisher in gaming,
and an acquisition of this size is unprecedented.
This is different to buying talented studios, which is something large gaming companies have been
doing for years. Instead, this is basically buying a significant chunk of the industry,
and many, including regulatory bodies and antitrust agencies, have expressed concern
about the potential negative impacts this may have, with the deal still currently facing
significant challenge, particularly from the competition and markets authority in the United
Kingdom. Whether this deal will ultimately go through, though, remains to be seen.
Microsoft has been quietly losing the console war for years, and most believe competition
between major gaming companies can only benefit consumers. Yet, whether these moves will make
the console market more or less competitive is unclear, and this general trend towards
consolidation certainly raises concerns. Still, this is hardly the only thing people have been
critical of in recent years.
The Romans had a saying that translates to, the past is always well remembered,
and the Romans weren't alone in this belief. Sometimes, however, things really do get worse.
So, what about video games?
Everywhere you look online, you'll find people telling you about how bad games are now,
and how much better they were in the past. But the internet is also addicted to negativity,
and people are biased. So, what evidence is there for this belief, and what does gaming's
50 year history actually tell us? Well, for one thing, the future does raise concerns,
but a worrying future isn't anything new.
This is an industry of change, and change brings uncertainty.
The gaming industry has crashed more than once. The console market almost died completely.
The biggest gaming company in the world went bankrupt. Many other console manufacturers
also exited the scene. Reliable companies have produced spectacular failures.
Established companies have experienced tragic deaths. Y2K didn't happen,
but multiple stock market crashes did. The most successful console of all time was
followed by the company's greatest failure, and soon after this, every other major player
also suffered their greatest failures. Motion controls threatened to take over,
PC gaming threatened to become irrelevant, mobile gaming threatened to take over,
indie gaming threatened to become apocalyptic, online gaming threatened to take over,
and now consolidation, monetization and subscription services threaten gaming as we know it.
And time will tell how things play out, but really, future concerns are a part of the gaming
industry, and they always have been. So, what about the present? Well, one of the biggest
trends in the last 10 years has been a growing abundance of remakes, remasters, reboots and
to re-releases. On the one hand, old games don't always remain accessible, and so attempts to bring
them to modern audiences make sense. And as the passage of time creates more old games,
it also makes sense that there might be more attempts to bring back older classics.
This might also be representative of the changing demographics of gamers,
where there's now a large portion of older people nostalgic for the games from their childhood,
or younger gamers unwilling or unable to play older games who still want a way to feel included.
Still, one thing that is hard to justify is the low quality of some of these newer additions.
There are many remasters and remakes that could be criticized, but perhaps the single one,
most endemic of a bigger problem is Grand Theft Auto, the trilogy, the definitive edition,
a compilation of three of the best-selling games of the 2000s that was published by one of the
most respected developers in gaming, and yet the end result was appalling, being buggy,
inauthentic and an all-round visual downgrade, with missing animations, sound effects and textures.
The definitive edition received one of the lowest user scores of all time and stands
as a testament to just how bad an improved edition can be, with Rockstar making matters even worse
by also removing the original games from stores at the same time.
So, various re-releases have given companies a lazy and low-cost way to cash in on past games.
Some remakes do receive considerably more effort, but even these display a deep lack of
originality as companies are choosing to finance remakes of still readily accessible
and enjoyable games in place of creating something new, and this practice seems to be
becoming more common. So far in 2023, 90% of the highest-rated games are remasters and remakes.
This is rather pathetic, but the high critical ratings and the fact these games continue to sell
is proof that this is what many people want. One day, attitudes might change, but for now,
gaming companies just seem to be giving the people what they ask for, with the quality
burying between attempts. Another trend of modern gaming has been
additional monetization methods, such as loot boxes and in-game cash stores.
For the mobile industry and many online games, these have been incredibly effective at increasing
revenue, and this has come to extend to even some more traditional games, although some
attempts have been met with considerable backlash. Still, much like with remasters and remakes,
consumers again have a choice here, both in whether to play these games and whether to
spend money on these services, and again, many people are making the choices that make additional
monetization methods profitable. The gaming industry is a business, and if consumers allow it,
companies will do what makes them the most money. Finally, another obvious trend has been a decline
in the quality of AAA, with games seeming to take longer to release while taking less risks in their
design and being more likely to launch unfinished and buggy. This is partly because advancements
in game technology have slowed over time, alongside ever rising game development costs,
which has meant graphical upgrades have become less noticeable, and impressing gamers through
continually creating more spectacular games has become harder. As a result, games might be over
ambitious, or make sacrifices to gameplay or design, or take longer to release, or make sacrifices to
quality in order to meet deadlines. Claiming that AAA games have got worse in recent years seems
pretty reasonable though. The thing is, many of the highest-rated and best-selling games from
recent years have still been the biggest budget and most visually impressive titles. Critics and
consumers alike want and reward AAA game design, and amongst these games there will be some that
succeed, and some that fall short. It seems unrealistic to expect every game to meet the
highest standards, or to criticize the industry for caring too much about budgets and presentation,
while people continue to praise those same qualities in other games.
A lot of the problems mentioned here do have some relation to greed, and the gaming industry,
like other industries, can certainly be greedy. This also isn't anything new, however.
After the success of Pong, gaming companies made so many Pong clones, the industry collapsed.
After the success of Atari, game developers made so many low-quality third-party games
that the industry collapsed again. Space Invaders led to too many arcade space games,
NES developers made too many action platformers, Doom led to too many Doom clones,
Grand Theft Auto led to too many GTA clones, Guitar Hero led to too many plastic peripherals,
The Wii led to too many motion control devices, and too many low-effort motion control games.
Call of Duty led to too many Call of Duties, Minecraft led to too many Minecraft clones,
Dark Souls led to too many Souls Likes, Skyrim and Far Cry 3 led to too many Open World games,
DayZ led to too many Early Access Survival games, PUBG and Fortnite led to too many Battle
Royale games, Indie games led to too many Indie games, Slay the Spire led to too many
Rogue-like deck builders, and most recently, Vampire Survivors led to too many whatever
the hell genre this is. The point is, success often leads to saturation, and it's been that way
since the very start. And yet, at the same time, the industry has often rewarded those who go
against the grain. Pac-Man, Tetris, Pokemon Red and Blue, Half-Life, The Sims, the DS,
the Wii, Minecraft, Dark Souls, DayZ, Battle Royale, gaming history is littered with games
that did something different and were rewarded for doing so. The point that I think everything
comes down to in the end is choice. People do have a choice in who they support and how they
spend their time, and sometimes the choices of gaming companies or the general public
can be frustrating, but it is their choice to make and they do have their reasons.
And after months of researching, thinking and writing about gaming history, I have come to the
conclusion that nothing defines this present moment in time as much as choice does.
It is a strange thing to try to write the history of the present, because so much of how we understand
the period is shaped by how it looks from the future. That said, I don't think there is a crisis
of modern gaming. I don't think the industry is in decline, and I don't think games are getting
worse. And the reason is, because currently, right now, we all have an abundance of choice.
The industry has grown wider and become more accessible, but this is the result of growth,
and traditional gaming hasn't been sacrificed to achieve this.
Instead, within traditional gaming, we've seen a tremendous increase in the number and types
of indie games being made, while through emulation, re-releases, community efforts,
second-hand purchases, and official subscription services, older games have never been more
available, and with the likes of cloud gaming and indie games, the barrier of entry into gaming
has never been so low. The number of games being made has also increased, and for most people,
older games are still out there. Everyone today has a great amount of choice, and that makes us
rather lucky. I hope this abundance of choice remains in the future. In fact, I hope we get
even more choice in the years to come, but this isn't guaranteed. One day, all old cartridges
will stop working, CDs will rot, and old hardware will fail. One day, computers may struggle to be
compatible with older software. One day, emulation might be shut down or strictly regulated.
One day, fees may increase. One day, competition may disappear. One day, the indie market may
collapse. One day, the console market may collapse. One day, the entire industry may collapse. It
certainly has before. The future holds no guarantees, and this is an industry of change.
But right now, things aren't as bad as many people might think they are.
Good games are being made every single year, and goddamn is there a lot of good games already out
there. Video games have come a long way, and not every change has been for the better. But never
before have people had so much choice. Use it wisely.
This has been the entire history of video games. Thank you for watching.
I promised myself I'd do a Patreon plug here, so here it is. The video is over. I hope you enjoyed
it. It took a long time to make, but I think it was worth it. I've been making videos for five years
now. In fact, this will probably be up almost exactly on the channel's five-year anniversary,
and I am lucky to be in the position I'm in. But the channel growth has slowed a lot recently,
and I'm feeling under a lot of pressure to try to satisfy the algorithm. And that's how YouTube
goes, but I do feel very limited in what videos I can make. I'd love to make more time consuming
and unique videos, like deep dives into certain areas of the gaming industry, but it is a consistent
worry for me how such videos will perform, and whether I can afford to spend hundreds of hours
on projects, knowing that a bad title or thumbnail could doom the video for a bad click-through rate.
For this reason, I'm thinking of trying to promote the Patreon more,
to try to get myself a little more creative freedom. I've hardly mentioned the Patreon for
the past year or so, but I'm going to start promoting it a bit more often, like at the end of
videos, as I'm doing now, that's what this is, this is promotion. I don't have much to offer
for patrons, but I've revamped the tiers a bit. For $6 you get your name in the credits for videos,
but you can give less than that as well, and all patrons get all other benefits, which include
detailed monthly updates, the occasional Patreon poll, and access to some unlisted videos which
includes a few uncensored videos, a few older videos not on the channel anymore, and a few
unique reviews. Other than that, all I can say is I work many hours on these videos, I work long
hours, I rarely take breaks, I try my best to maintain a high level of quality, and I do everything
on my own, and you don't need to give anything, but it would mean a lot to me if you did.
Anyway, thank you for watching regardless, and I'll see you in the next video.

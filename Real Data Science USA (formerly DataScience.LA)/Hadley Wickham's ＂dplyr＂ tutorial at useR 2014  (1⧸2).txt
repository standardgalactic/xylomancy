Okay, I've been given a knob, so let's get started and it sounds like I've cut out already
so let's just abandon this.
Can you hear me okay at the back?
Yeah.
Is that okay?
Great.
So, today, hopefully, you're here to learn about D-Player and to give you a little bit
of context, I'm interested in data manipulation in the context of data analysis, so you've
got raw data coming in one side, understanding knowledge and insight coming out the other.
And today we're going to be focusing on data manipulation, but I see this really as being
part of the cycle of other data analysis or data science tools.
So to me, there's really four main tools for data science.
So the first is data tidying, getting your data into a form that's actually suitable
for analysis.
Now, in this diagram, I've drawn this little short arrow, but as many of you have actually
worked with real data, you know, often the arrow is all the way around on the other
side of the room.
So often, one of the most challenging parts of doing a data analysis is just getting the
data in the right form that you can work with it.
Now once you've done that, you'll often do some basic manipulation, data transformation.
You'll create new variables that are functions of existing variables.
You might do a little bit of aggregation and so on, and that's mostly what we're going
to be talking about today.
But it's also important to bear in mind that you're doing this to fit into a cycle.
You want your tools to easily plug into the other.
You want your manipulation tools to easily plug into your data visualization and modeling
tools.
The visualizations are great because they uncover the unexpected.
They help you make precise your questions about the data, but the problem with visualizations
is that they fundamentally don't scale.
On the other hand, the kind of complementary tools, statistical models, machine learning,
data mining, basically whenever you've made a question sufficiently precise that you
can answer it with a handful of summary statistics or an algorithm, you've got a model.
Tools are great because they scale, but they don't fundamentally surprise your linear models
and they're going to tell you your data is nonlinear.
So any real data analysis, you're going to be circling between these tools multiple times.
You might start by looking at a plot.
Based on that plot, you develop a model.
You then take some predictions from that model.
You transform your data to look at the residuals.
You visualize those and so on and so on.
So while today we're going to be focusing on data manipulation, data transformation,
the goal is to have tools that embed seamlessly into your data analysis process.
And so the family of tools that I've been working on and others at RStudio are working
on have recently sort of undergone somewhat of a change and if you're interested in hearing
more about that, I'm going to be talking about that in my talk on Tuesday.
So basically, for data tidying, now the tidier package, which is kind of another update of
reshape and then reshape2 and now tidier, pliers become de-plier and ggplot is in the
process of turning into ggplot2.
And as you'll see today, there are some kind of very important commonalities that underlie
all of these tools that make it easy for you to use them.
So today we're going to talk about data manipulation.
We'll start with a little intro to the data we're going to be using, then talk about single
table verbs, a little bit about data pipelines, some more complicated types of filtering and
grouping, joins, a very general do operator and then I'm just going to talk very briefly
at the end about how all the tools you've learned today working with data frames also
apply to databases as well.
But before we begin, I kind of want to start with the caveat and then the bad news is whenever
you're learning a new tool for a long time, you're going to suck.
It's going to be very frustrating, but the good news is that that is typical, it's something
that happens to everyone and it's only temporary.
Unfortunately, there is no way to going from knowing nothing about a subject to knowing
something about a subject and being an expert in it without going through a period of great
frustration and much suckiness.
But remember, when you're getting frustrated, that's a good thing.
That's typical, it's temporary, keep pushing through and in time will become second nature.
Okay, with that said, let's get started.
We're going to be looking at four interrelated datasets today.
I have given you them in a RStudio project.
So if you have downloaded the code and data, you can double click on this R approach file.
If you're not using RStudio, my apologies, but you can just change your working directory
and I'll assume you'll be okay with that.
So in this directory, we've got the scripts, which mostly correspond to what we're going
to be working through today, and then we have got four datasets.
I wanted to start briefly with a couple of hints about using ... I'm not going to do
that.
Okay, I'm going to tell you about the data.
So we've got these four datasets.
The first one is the main one we're going to be looking at.
This is not a huge dataset, but it's recently sized, about 200,000 observations.
This is every flight that departed from Houston in 2011, and then we have got three datasets
that we can join with this dataset that provide useful additional metadata.
So we have some data about the weather for each hour.
As you can imagine, if we're looking at flight data, you might be interested in what causes
flight delays.
The weather is obviously a cause of that.
You might also be interested in, are there planes that are consistently delayed?
So we have some information about the planes that are flying these routes, when they're
built, what type of plane they are, how many people they see, and so on.
And then we have some information about the airports that the flights are flying to, which
is mainly their location, so you can plot them on a map.
Now to load this data in, I'm not going to talk to you about this code.
It's there in the first file.
To get started, you're going to want to source that file in that it's going to create these
four datasets.
The only thing that you might not have seen before is this tableDF function.
What that is going to do, it's going to turn these data frames into dpliers tableDF objects,
which are almost identical in every single way to data frames, except when you print out
a tableDF, it does not print out 10,000 rows, it will only print out the first 10 rows.
So it gives you some summary information about what's going on in that dataset.
It prints all the variables that fit in one screen.
It might make us a little wider.
And if they don't fit on the screen, it just gives you a little summary, the names of the
variables and what type of variable they have.
It's identical in every way to a data frame, except when you look at the class, it is one,
well, two additional things.
If a package doesn't know about dpliers, it would just treat it exactly like a data frame.
In fact, it is a data frame, it's just a special type of data frame.
So we've got flights data, about 200,000 observations, weather, which is about 8,000.
These planes, about 3,000, and then about 3,000 airports.
Okay, now that you've introduced yourselves and hopefully have some questions to ask about
the data, we're going to dive in and learn the first five important verbs associated
with dpliers.
So my kind of contention is if you know these five verbs and combine them with another tool,
we'll learn about shortly, this will solve 90% say of your data manipulation problems.
And that's really important because now when you have a data manipulation problem, instead
of thinking, well, there's like 1,000 functions and base R, which one of those is the one
I need.
Now you just need to look through these five verbs.
So the first verb is filter, where you're going to select rows based on the values of
their variables.
You might also want to just focus on a certain number of columns or variables that select.
You might want to reorder the rows or arrange the data frame.
You might want to add new variables that are functions of existing variables.
Or finally, you might want to reduce multiple values down to a single value.
So all of these functions work exactly the same way.
The first argument is always a data frame.
The subsequent arguments tell you what to do with that data frame.
And then they always return a data frame.
So none of these functions modify in place, so whenever you use them, if you do want to
modify your data frame, you're going to have to assign the results.
A lot of the times I'm just going to show you, I'm just going to run the code and kind
of show you the results on screen and then throw it away.
That's great for teaching, but obviously when you're doing a real data analysis, you actually
want to save what you've done.
To illustrate these, I'm going to start with a very simple five-row data frame, which I'm
also going to show in slides.
So filter allows you to select rows that match some criteria.
So here we're going to say filterDF, we want all the rows with color equals blue.
So this is the input and this is the output.
So if you've used subset before and baseR, this is very, very similar.
If you're looking to see if a value matches one of multiple values, you can use in.
And then there's a whole set of other operators, the regular logic, the numerical comparison
operator is greater than, greater than, equal to, so on and so on.
Not equal, equal and member of the set.
You might also want to use the Boolean algebra, so or and and not an exclusive or.
So I'm just showing this here as a reference, hopefully if you've used R a little bit, you're
familiar with these already.
There are kind of two main things to be cautious of.
When you're working with vectors, you want to use the single bar and the single ampersand.
If you're working with scalars, if you're working with single numbers, like you're using
an if statement, that's when you use the double bar or the double ampersand.
But here we're going to be working with vectors and values, so we want to always use the single
vertical bar or the single ampersand.
And we'll talk about this in a little, very shortly.
So what I want you guys to do is practice using filter by extracting the flights that
match these criteria.
So first of all, all of the flights that went to San Francisco, all of the flights that were
in January, or all flights that were delayed by more than an hour, or they departed between
midnight and 5 AM, or when the arrival delay was twice as much as the departure delay or
corrective.
So I'll give you a few minutes.
I'll circulate around and help you again, there's only one of me and there's a lot of
you.
So if you get stuck on my behalf, please feel free to ask your neighbor for help.
Okay, so let's have a go at how you might tackle this.
So we wanted to find all the flights that went to SFO Oakland.
So you might start like this.
So there's 2,800 that went to SFO.
Now a common mistake when you're first using R, you would say, I want the destination to
equal San Francisco or Oakland, you do that, that's not going to work.
So you either have to be very explicit and say destination equals SFO, or destination
equals Oakland, or use the in operator.
So that's all of the flights that his destination was San Francisco or Oakland.
In January, that's actually a tricky one.
The easiest way to do that is, in this case I know the first flight was January 1st, so
I can just say give me all the flights before the 1st of February.
That didn't work surprisingly, so we might need to just, oh, 2011, yeah, okay.
So let's just see how that thing went down.
So that gets us 18,000 flights in January.
Again between midnight and 5am, there are two ways you can write this, so probably you
might have written this, all of the flights where hour is greater than or equal to 0,
and hour is less than or equal to 5.
With filter, you can also supply multiple arguments to it, and those arguments are all
ended together.
There's no real benefit to doing it this way, rather than this way, except maybe one day
we might be able to figure out how to do these in parallel, and it might be twice as fast
if you do it this way.
And then finally, all the flights delayed by more than an hour.
There's two delay variables here, the departure delay and the arrival delay.
I should have mentioned if it's a negative delay, that means it arrived early or departed
early.
We can find all the flights that were delayed by more than an hour, right, 10,000 flights,
if any of you have, I assume you've all flown in the US, so you're not surprised by this.
And we can also use more complicated expressions in there.
We can find all of the flights where the arrival delay is twice as much as the departure delay.
So these are cases where we have lost time during the flight.
Well, on these ones we might also want to say, and the departure delay was greater than zero.
Right, so this minute, this flight, wait a little longer, right?
Yeah, this flight was two minutes delayed departing, and it was six minutes late on arrival.
Any questions about Felter?
Yep.
Why would you use Felter instead of Felter?
Because it's faster, because it is better defined, it just does one thing, and it does
it one thing well, or a subset does multiple things, and then finally you can use Felter
on database tables and it will generate SQL for you.
Will it work on regular data frames?
Yes.
Okay, the next verb is select, which allows you to pick variables you're interested in.
So this is most useful if you have a data set that has hundreds of variables, and you
just want to look at a few of them.
The syntax is the name of the data frame, and then the list of the variables you want to keep.
So select works like the select argument to subset if you've ever used that.
But basically you can treat the names of variables like their positions.
So you can say use negative to say give me all the variables that are not color.
What I want you guys to do now is read the help for select.
What are the other ways you can select sets of variables, and then see if you can come
up with three ways of selecting out the two delay variables from this data set.
So if you look at the help for select, you'll see that all of these main verbs are documented
together, and you'll see that I've been courteous to Americans.
But if we scroll down, we can see that there are five ways of, well, at least five ways,
extra ways of selecting variables.
So you can select variables that start with a common prefix, then end with a common suffix
that contain some character string or the match a regular expression, or you can do
like a numeric range, say all of x1 to x10.
So this is my attempt to come up with every way that you might reasonably want to select a variable.
So a couple of ways you can select these two.
You can select them just as individual variables.
You could say pick all of the variables between from a rival delay to departure delay.
You could find all of the variables that end with delay or all of the variables that contain delay.
There's lots of other ways too.
You could also write this.
You could say make a vector of columns.
We're using C.
Basically, inside select variable names, you can treat them like the numeric positions.
So anything you can do to a numeric position, you can do with a variable name.
So the goal of select is to make it easy to refer to your variables by name.
It's always a better idea to refer to your variables by name than by position,
because you don't want your data input format changes and you're referring to variables by position.
It's very easy to have code that works but gives you meaningless results because it's using the wrong variables.
The next verb is a range which just changes the order of the rows.
So if you just use a variable and orders it by that,
you can order in descending order by using the desk wrapper.
And I don't show you here but you can add additional variables to break ties if there are ties in this first variable.
So again, order the flights by departure date and time.
Figure out using a range which flights were most delayed
and then which flights caught up the most time during the flight.
So again, a few minutes to work on this and I'll show you the answers.
Okay, if we want to order the flights by their departure date,
we could say order it by date and then hour and then minute.
Just want to see multiple, ordering by multiple variables.
So you can see the first flight left on January 1st, one minute after midnight.
So I should mention this depth variable is the departure time as like a 24-hour time
but all the zeros got dropped off.
And then the hour and minute are just that, this time split up into those pieces.
So for example in this column, there's not going to be a 661,
no flights left, it's 61 minutes past 6am.
This is just a weird decimal time.
We want to sort, find the most delayed, that's just a matter of sorting
so that our delays are descending.
We can see the most delayed flight was 981 minutes.
So an impressive 16-hour delay.
Now normally flights aren't delayed that long,
not because flights aren't delayed that long
but generally airlines cancel the flights to make their departure delay statistics look better.
So similarly we could do the same thing for arrival delay,
which is going to give us a pretty similar message.
And the other thing I wanted to show here is that you can arrange on kind of compound expressions.
I wanted to find the planes in a mode made up the most time
and there's the biggest difference between the departure and arrival delay.
So there's a flight, so for example this flight left one minute early
and it arrived an hour and 10 minutes early.
So you can arrange on compound expressions
although generally it's going to be easier to add that as a new variable
depending on what's going on and then arranged by that.
Why are you reporting this descending?
Because I wanted to find the one, I wanted to find the biggest difference.
I may have...
Actually I got the same result.
I may have hit this round the wrong way.
Oh yeah.
So depending on which way round we need to track the arrival from departure
to ascending or ascending.
Any other questions about arrange?
I had a problem with the NAs, the first time I did something
I got all the NAs on top.
I did it in a different way than you did once I...
So NAs should always sort to the end and if they don't that's a bug.
They do but what if I want the smallest without the NAs?
So you have to use felt as a removal of the NAs currently.
Is there an opposite of descending?
Yes, just don't do descending.
I think also the way that...
I believe that if you do descending or descending that is ascending.
It's the one still.
If you really want an ascending function you can just do that.
Okay, the starting to get more complicated.
The next verb is mutate which allows you to add new variables
that are functions of existing variables.
So here we're adding a new variable called double
which is two times our existing value variable.
So again in all of the dplyr functions
you never need to explicitly refer to the data frame that you're working with.
That's always implicit.
It's going to look for this value inside the data frame
rather than in your global environment.
Mutate is very similar to transform and base r if you've used that.
One big difference with mutate is you can do multiple...
In additional mutations or additional transformations
you can refer to variables that you just created
which you cannot do in transform and is a little bit annoying.
So here we first double value and then we make a new column called quadruple
which is just two times double a variable we just created.
How does it compare to within?
Basically I think within is a hideous monstrosity that no one should ever use.
And if you want to know more I can tell you.
Okay so your turn to create some variables.
See if you can figure out the speed and miles per hour
which flight flew the fastest.
See if you can create a new variable that shows how much time was made up
during the course of the flight or lost.
And then how did I compute the hour and minute variables
from that departure variable?
Okay so if I wanted to compute the speed
that is just the distance divided by the time divided by 60
because time is in minutes.
So if we print that out
you know unless you make your screen really wide you can't see everything.
So one thing you can do is use the view function
which works in RStudio and other R ideas
which will just show all of your variables
on a nice kind of scrollable table
or you can always just select the variables you want to see
so from like departure to speed.
So if you use a very handy way
of just viewing a data frame in a nice table.
Did you change flights?
Yes so in this case I modified flights because I wanted to create
a new variable and modify that original data set to add that new variable
and then I can sort it to find the fastest ones
and see 760 miles an hour.
When you mutate does there
an easy way to specify a position?
No so when you add new variables they always go on to the end of the data frame.
If you wanted to reposition them there's currently no particularly easy
way to do that. You could create a big select statement but it's
kind of a pain.
We could create this delta variable
which is just the difference between the departure and arrival delay.
If you didn't care about the direction
you could do whatever you want in this
whatever R expression you want.
The last thing I wanted to mention
is just a useful trick. If I have this departure
we have the first two digits of the hour and the second two digits of the minute
you can use the integer division operator
and the modular operator to extract those pieces out.
This is just a useful little trick if you want to pull out certain digits
from a long number.
Any other questions about
mutate?
Okay next I want to talk about a new function group phi
which is summarized together. You can use summarized and regular data frames
but you always get a data frame that is only one row
which is typically not very useful.
That's exactly what I said. So summarized is going to give you a one row
data frame. What you're going to want to do is actually group your data first
and then summarized will operate by group.
Here we're saying create a new data frame
and use this old data frame grouped by color
and then we're going to summarize this and for each group
compute the total by summing up the value of your vehicle.
So I'm going to create four useful ways
of grouping the flights data. We might want to group it by date
we might want to group it by hour, we might want to group it by plane
or we might want to group it by destination.
Just to bear in mind when you do create all these groupings
dplyr is sort of smart enough that doesn't create a complete copy
of your data every single time. It works the same way as the rest
of R, it doesn't sort of a lazy way. If you modify one of these data sets
you'll have to create a copy but until you do so they all point to the same
place. So grouping data doesn't
use up, it doesn't create a copy of the data, it does use up a little bit more
memory because grouping builds up an index so
you know what observations are in each group.
Now there are lots of summary functions you can use, most of these
are pretty standard, minimum, medium, maximum
you can extract contiles, there are two functions
that are special in dplyr in which just
tells you how many observations are in a group, indistinct
and I should have a
x there, tells you how many different observations
are in a variable, that's the same as doing
length unique x but it's a little bit more efficient.
You can sum, you can compute means. It's also often
useful to do summaries of logical vectors because
when you take a logical vector and treat it like it's a numeric
all the falses turn into zeros and the trues turn into ones
so what that means is when you sum a logical vector it tells
you how many trues there were so this would tell you how many values
of x are greater than 10. The mean is just the sum
divided by the length so the mean of a logical vector is the
proportion of values of the true. There's a really useful little
trick. And then lots of other ways of measuring
the variation, standard deviation, variance, interquadal range,
median absolute deviation. So these are all just standard
functions.
Okay what I want you guys, what I've shown here is the distribution
of departure delays. So I've got two views of this
one which shows all of the delays and one which just shows the delays less than
like two hours. So what I want you to do with your neighbor for two minutes
is just quickly brainstorm given this distribution
given what you know about flight delays
how might you want to summarize this distribution. What function might
what you want to use or do you want to use a mean or a median or something else.
So take two minutes starting now, talk it over with your neighbor.
So we're going to summarize by date
what's one way we could use to summarize the distribution of
delays. The median? We could use the median
I mean probably want to use the departure delay
so if we just run that
we are going to get a new data frame and it is
265 rows which you should have anticipated. You know how many
days there are in a year. I've got one little problem here
probably want to use Na.Ramq was true
let's do that.
How else could we summarize it?
The mean is another obvious one
let's just assume we've got that
What else might you want to see? 90% quanta.
Okay we've got max
and actually typing all of this Na.Ramq.true
is going to get tedious real fast so I'm just going to filter
it and I say I want all of the ones that are not missing
okay so that way I can just drop this off
and I'll bother typing it
so that's the median, the mean, the maximum and then something
in between we could get the 90th
quanta. Remember how to use that function
Any other ideas?
Is there a way to for example
compute more than just the 90% quanta?
Currently you have to type them in like this
but there will be some way in the future that you do that
Yeah we could also do some thresholds
well first of all we could say
what's the proportion that is delayed
so that is the average of all of the ones where the delay
is greater than zero
so that is the
presently high
but you might say well who really cares if it's only
a 5 minute delay or a 10 minute delay
I might just say arbitrarily like a 15 minute delay that's not bad
Why are we looking at departure not arrival?
Yeah so equally you might say well it's
the impact on our arrival that's what really matters because that's
someone picking us up at the airport and our flight
is now delayed by an hour and they're getting angry so we could switch all this to arrival
delay too and the results are pretty similar
So 15 minutes is kind of arbitrary you know you could look at a few
other ones if you wanted to do that
Yes? Is there a way to use this summer function?
You could but I'm not sure
that you would want to
So current well so there's two problems
so first of all I mean this is a reasonable thing to do
currently though summarise
when you summarise you have to reduce to a single number not multiple numbers
because again a future version of dplyr will let you summarise multiple numbers
at some point in the future
What did I do? So this is what I did
and you have to have urm everywhere
or you can filter out all of the flights
that are not missing
but don't have a missing departure
So this kind of
brings me to my next point at any like in any real data manipulation
task you're probably not just going to use one verb
but you're going to string multiple verbs together first of all we group it
then we filter it then we summarise it and we want some way
to kind of express that more naturally or more simply which is that
the idea of having a data pipeline
you need to do quickly just take a minute
talk this over with your neighbour what does this snippet of code do
so you've got one minute starting now
okay so this looks pretty complicated
but if you kind of really carefully pass it you have to start from the innermost thing
we're going to start with the flights data then we're going to filter it
to remove any missing delays then we're going to group it by date
in an hour then we're going to summarise it to compute the average delay
and the number of observations in that hour
then we're going to filter it to only look at the hours that have more than 10 flights
so it's not too complicated
but we have to read it in quite an unnatural way to read insight out
and then also like the arguments to filter are quite far away
so instead
what we're going to talk about after the coffee break is this pipe operator
and you'll see that that makes the code quite
a lot easier to read so the coffee is outside now
so let's have a coffee break and come back at
3.40
music
music
about this operator
called the pipe operator so what this basically does
is take the thing on the left hand side of the pipe
and put it as the first argument as a thing on the right hand side
and the advantage of this is it allows us to take something like this
which is pretty hard to read and transform it into something like this
and this is pretty easy to read particularly if you pronounce this operator as then
so we can read this take flights then filter it
to remove any values with a missing value for depth delay
then group it by date and hour then summarise it
computing the average delay and the number of observations in the group
then filter it to look at all of the
observations we're going to have in 10 so
this pipe operator allows us to
form chains of complicated
data transformation operations that are made up of very simple pieces so the goal
is you make something complex by joining together many simple
things that are easy to understand in isolation
so I want to give you some practice using that with
three challenges so which destinations have the highest
average delays which flights
happen every day and where do they fly to and then on average
how do delays vary over the course of a day
and if you're going to do that probably look at the non cancelled flights
so those three challenges are relatively
simple but you're going to need to string together multiple of these verbs
you've seen before you might have to use a range and group by and summarise
and filter in some order so have a go at joining those together
and again if you get stuck I'll come around and help you out or better
ask your neighbour
do well we start with the flights
what are we going to do to that filter to remove anase yep we can
remove the anase let's do rival delays
what next
group by so group by is kind of a fundamentally
like statistical operator you're saying what is the unit of interest in this analysis
and in this case it's the destination of the flight
then for each destination what we want to do is summarise it
I'm just going to say let's use the mean delay
the other thing I think you always want to do whenever you do
a group by summary is you always want to recall the number of observations
in each group because when you start looking at these averages
you know if there's a destination that has the highest
average delay but only one flight flew there
and that's probably not as interesting and then if we want to focus on the most
delayed flights we're going to arrange it in
descending mean so let's run this
they've worked so you can see this is a good example
so there's this airport BBT which
see
I think I've already looked at this before so that is Jack Brooks
Brooks regional airport on the airport
of Texas so there are only three flights flew there the entire
year you're not going to trust this average that much so
what we might want to do is filter out all of the
flights where there's less than 10 observations
we'll run that pipeline again
now again I've constructed this pipeline
just by typing every step and it worked
which I have to say I'm slightly amazed at but generally when you're creating pipelines
you want to do it a step at a time and this is one reason
that I think the default printing is really important
because you can just print out the result at every stage and you can see does that look right or not
if you have a normal data frame it will print all of it right
yes so if you have a normal data frame
it will print the whole thing and if you want to turn
you can always take a normal data frame
and the first thing you can do is pipe it into tables here and turn it
into a data frame the other thing
the other thing that's useful is you might often pipe
this into something rather than just printing it you could pipe it into view
if you wanted to see more of the data
that's kind of interesting
if you wanted to just
kind of step through it
you could do
talk about you could do something like
this
maybe
so we're just taking the row number and taking a modulo 5
equals zero so that's going to give us every fifth that would be one way to do it
so if you
shows you everything well it shows you the first so many rows
in the future I think we'll make it so it shows you every row
in a way that's reasonably efficient
the other thing that's useful is to pipe it to str so you can see exactly what variables
you've created and if they're the right type and so on
or if you're so inclined
could you put in two functions like head and tail after each other
you can't basically so you want a pipeline that has a split
in it right you want to have a pipeline that one pipe goes to head
and the other pipe goes to tail
at the same time yeah I don't
like a data table does that by default on that I think that's a nice idea
the reason dply doesn't do it is because you can do that for data
frames but you can't in general do that efficiently for database queries
you can always use tail off
so there's another handy keyboard
shortcut in our studio which I
suspect no one knows about because the only reason I know about it is the
Joe who added it told me about it there's this command called rerun
previous has anyone used rerun previous before
so what that does is if you have selected a
block of code and press command enter
now if I modify it it's kind of annoying I have to select that
block of code again or you can press command shift P
and it just sends those same lines of code into the R console
so this is really useful if you want to iterate rapidly on your pipeline
you can easily change things and maybe I wanted an ascending order
and just command shift P and rerun the whole pipeline
okay
okay so
any questions about that pipeline that we created to solve that problem
so the next one is which flights
happen every day and where do they fly to
no
so what are we going to start with that
and which flights fly every day of the year what's probably the first
thing we want to do we want to group by
and we want to do that by carrier and the flight number
now we want to find all
flights that flew every day of the year
any ideas so we're going to summarize what might we summarize
we might use the dates
what how well we're going to use the date how what are we going to do with that
oh so we could do we could do count
flights
we could do count and then we could filter by
let's give us a name
365
I forgot to put two equals
now the problem with this is that it's possible
this flight flew
twice on one day and didn't fly it all on another day
I feel like that's yeah so actually
this is my solution too but now I think a better way would be to say
count the number of distinct
dates so if there's 365 distinct
dates then we know it's flown every day
I think this would give us a slightly different answer
well in this case it gives us the same answer because there aren't flights that fly
every day and then
fly twice on one day but not on another
now what if we wanted to add see what destinations these flights flew to
any thoughts on that
we could just add to the group by
there are other ways we could do this which we'll see later but in this case it's easy enough
to just add that into the group by
and see Honolulu and a lot of flights to New York
and Chicago and Seattle and Miami I think
the last one on average
the non cancelled flights vary over the course of the day
so again so first of all we always want to say
they're not cancelled which I think
because cancelled equals zero cancelled is
a reason code associated with it and then normally
once you've kind of filtered out clearly wrong things the first step is going to be
grouping it here we want to group by hour say
or maybe hour and minute
and then summarize again we want to
count how many observations on each group so we can disregard the delayed flights
and we could do the mean
departure delay
and
summarize not summary
so now when you get to this point
it starts to get easier to see
what's going on with the visualizations so
this is basically that pipeline I just showed you
I think I've done a slightly differently I created a new variable called time
which is just hour plus minute divided by 60 that gives me like a floating point
number that smoothly varies over the course of the day
group it, summarize it and then I'm going to do a little ggplot to
plot it
so you can see very early in the day
we have this kind of scattered cloud of some plots that are very
delayed what might these be
the ones from the end of the previous night
the ones from the end of the previous night and why are the averages so high
so variable
these are the ones that have hardly any data
there are hardly any flights leave after midnight so these averages
are kind of suspicious we're not really seeing much of a pattern we're just seeing
individual flights that were delayed a really long time from the previous day
so we might want to, so one we could show then the visualization
is to make the points proportional to the
number of observations or we could filter it and add some other stuff
there's no schedule flights
exactly there's no schedule flights yeah
so there's some kind of interesting pattern going on here
I don't really understand if it's possible it's an artifact
but it looks like it added these white lines on every hour
but it looks like there's some kind of pattern where they start off
delays kind of accumulate over the course of the day
but there's also some weird pattern within the hour where they accumulate and then they drop
back a little which I don't know what's going on
but certainly the suggestion is if you want to leave on time fly early in the day
or late in the hour
or late in the hour
any questions about those pipelines in general or how you can combine
these pieces with a pipe operator?
range is generally what the advantage is to chaining versus having
a ton of parentheses inside
but the sole example is that it makes it easier for you to read and understand what's going on
does any advantage just having it line by line
no basically no
save a little bit of memory but it's not
yep
so yeah in all the versions of D player used
percent dot percent
now I prefer percent greater than percent for two reasons
first of all it's easy to type because you can hold your finger on the shift button the whole time
and secondly I think it's not a
symmetric operation so having an asymmetric operator
helps you understand what's going on, the data is flowing from left to right
any other questions?
is there a particular preferred order?
no obviously the less
data you have to work with the faster things are going to be so that generally suggests you should
filter early on and you know so
if you use a database, a database looks at the sequence of all the
operations and says oh you did this filter at the end but it would actually
be way more efficient to do that at the beginning, D player doesn't do anything like that
D player executes it exactly as you give it so if you
can think of a faster way to order the operations it might be worthwhile
to do so generally and I'm not really going to talk about
performance today but generally if you've got million like
less than 10 million observations you won't even have to worry
about the performance it's going to be a few seconds and it's not
like it's a waste of time worrying about it because it's not going to take you that long
ok, the next thing I'm going to talk about
is a great thing
music
music
music

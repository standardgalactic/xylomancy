Today everybody, thanks for joining us. We've got Keith Wiley here today who's a board member
of Carbon Copies and has written extensively on the concepts of mind uploading and whole
brain emulation. He's the author of a Taxonomy in Metaphysics of Mind Uploading as well as
the author of a new book which is coming out very soon, which is a novel this time,
A Fiction Contemplating Oblivion which should be coming out sometime this month.
So welcome and it's great to have you here again. Thank you, it's great to be back.
Now I guess mind uploading has a concept that's been kicked around for a while now.
Why don't you think that mind uploading has enjoyed as much wide acceptance within the
philosophical community, within the philosophy community as it has in the futurist community?
Well I guess I don't know exactly why people are answering questions the way they do
because they tend to be rather terse interviews that don't, they have a lot of follow-up. But
I think that the, you know, the sort of really contentious questions of the, right around
this whole body identity concern and the copy problem which I actually consider to be a category
where are really hard to get away from and in fact I think the recent sort of, you know,
popularity of the discussion has probably made people think about it just enough to
convince themselves that it doesn't work but not to really think past that point.
So there is a certain sort of initial reaction I think where you're presented with the idea
of a non-destructive uploading scenario. It could be a transporter-like scenario from Star Trek
or just a more sort of direct mind uploading scenario but it's presented in a non-destructive
fashion and people think about it for a minute. They sort of initially have this cut reaction
that something about it must not have worked because of the famous copy problem. And, you know,
the bulk of my writing has been an attempt to chip away at that sort of instinctive reasoning and,
you know, really work through it beyond an intuitive sort of feeling and see where a
logical analysis really lands. So there are these various reasons that support the initial intuition.
People often raise concerns about breaking the stream of consciousness or good old-fashioned
body identity in which sort of they feel like their identity has to stick to their atoms or
what I call space-time worm identity which sort of says that, you know, it seems a little odd
that your identity might discontinuously jump around in space. You have to sort of go past the
just sort of the click-off-the-cuff reaction to sort of ask whether that actually makes any sense.
And, you know, I personally have always landed pretty squarely on the psychological model of
identity which is memory-based. You know, identity is indicated by some unique conglomeration of a
lifetime of memories. And just to be clear, I use the word memory in a broader sense than
what most people might think. I don't just mean episodic memories of events from your life. For
me, the word memory means the totality of neural encodings in the brain. So there's a lot of
subconscious personality traits. There's all sorts of, there's actually a lot of muscle memory. You
know, sort of the ability to play the piano is a memory even though it's a completely subconscious
memory stored primarily in the cerebellum presumably. So by memory, I just mean neural
encodings within the brain. But nevertheless, my preferred model of identity pins identity to
that feature. And that leads one towards certain conclusions. First of all, that uploading works
because it's all memory-based. So you sort of, if you consider a thought experiment in which
those memories remain intact through the uploading procedure, then by definition,
it worked because that was the premise. In order to reject that, you have to sort of
figure out why you reject it. You're obviously going to reject it on the basis of some
not full commitment to the memory basis of identity. You're looking at some other component.
And I just don't do that. I just always found that the psychological and memory theory works best.
Okay. So I mean, it's interesting that if it's totally based on memory, surely you'd have to have
some sort of mechanism to be able to process consciousness, valence, awareness and the memory
as well in order to have an agent feel as though it's got some sort of stream of or persistence
of identity and enduring metaphysical ego. Yes, yes. There is this right. So you've got the concept
of identity and you have the concept of consciousness. And they're not necessarily the same thing,
which is where the famous thought experiments of philosophical zombies come from. You could have
agents cognitively processing in the world and encoding memories of a form, although it's hard
to imagine what reflecting on a memory consists of if you have a consciousness. I'm not quite sure
what that comes down to. But nevertheless, you can sort of separate these components and you can ask
what would it be like to be an unconscious but yet memory fulfilling system? I think it's mostly
philosophical whimsy. I don't actually think that philosophical zombies are a practical reality.
They're just a philosophical exploration. Well, as David Chalmers brought up the thought experiment
is if you had an atom reconstruction of a human, the first human had consciousness and the second
human didn't have consciousness, although all their atoms and physically they were exactly the same.
Wouldn't that imply dualism? But I mean, people take the thought experiment to mean something
else and it's been used to describe artificial intelligence without the architecture or the
phenomenology of consciousness as well. This interview is evolving quickly. We're getting
into some other topics, but yes, there is a huge question in the last three years, give or take.
What the implications are of the huge LLM models? What does that actually have to say?
Not only about artificial intelligence, but also about consciousness. Are these things
conscious? Do they actually have a sentient sort of set of rights and things like that?
A lot of people, I think with pretty good reason, think that the LLMs are not satisfying those
requirements. They're doing something else, but they're probably not conscious in the way that
we're particularly concerned about. I don't think that they have a lot of recurrent wiring
in their layers. They tend to be pretty feed forward heavy. If I'm definitely one of those
people who thinks that recurrence is doing something, I don't quite know what, but I think
recurrence is a crucial component of consciousness in some fashion, a sense of looking back on
itself in a loop of experiential self-learn. We don't know metaphysically why, what is it about
that that's critical to consciousness, but a lot of people seem to agree that some kind of recurrence
seems to be part of the design. Short of that, something that's always bothered me about the
modern AI systems is that they're all on and off and then they're in the dark. You query them
with your question. They go off and they dig through their neural net. They come up with a
result and they produce for you a paragraph of text or image or a digitized voice and then they
go dark. That's it. They're not sitting there daydreaming. They're not musing on the conversation
you're having with them going back over the previous dialogue from the last minute. They're
not doing either dead while the prompts, if they're blinking, until you issue your next query.
So, can a system like that be conscious is an interesting question and are these systems
conscious on the basis of that sort of strange operation, which is just very not life-like.
It's just not what we think of. It's not what we do with people. A person doesn't sort of sit there
like a rock until you go up to them and sort of query them and then they do something and they
emit and then they die until you interact with them again. People don't work that way. Animals
don't work that way. So, natural brains seem to be in a constant flux. That's what it means to be
a dynamic living brain and modern AI systems don't do that by definition. That's just not what they
do. So, I don't think they're conscious. I think it's pretty clear that they're lacking these major
features that we would generally assume we expect before we're going to put those labels on it.
I don't know. Yeah, what a time to be alive where we're beginning to be able to, I guess,
empirically investigate some of these questions with more precision and also with a whole lot
more data and we can test assumptions in artificial sort of substrates instead of
just, you know, trying to probe directly the human mind. Not averse to doing the
latter though. I think it's really interesting where brain computer interfaces could go.
But given the, yeah, so what kind of resolution or fidelity of memory do you think is required
for persistence of, in order to either reconstitute or persist an identity or a sense of self?
Yeah. How do we approach that question? Yeah, it may be a better question.
I think the first approach is to bring in the professionals of which I am not. I think that
there are, you know, psychologists, psychiatrists and neurologists probably have, there's probably
a lot of very established science in sort of how memory evolves over the course of one's life
and what degrees of memory loss imply, you know, sort of noticeable losses of personality or
the external third party perception that a person is sort of being lost over the course of
dementia. You know, we can say, you know, if you lose your memory at a certain slow steady rate,
it's not dementia and you're not really losing your personality. But if it happens faster or
if it happens more discontinuously, then we as external observers sort of recognize that there's
some sort of identity collapse sort of in process, you know. So I don't claim to, you know, beholding
the professional sort of state of that. But it's something like that, you know, it certainly
can't be a requirement of perfect fidelity because we are, this is Thomas Reed's experiment from
the 1700s that people are, you know, over the course of their lives, always constantly steadily
shedding their old memories. And yet we consider to be a persistence of identity so long as the
shift is, you know, smooth below some threshold. But, you know, to give you an actual answer to
that question, I don't know. I suspect there's, I do think it does imply though that when we try
to create mind uploads, you know, and things of that nature, there's going to be a lot of wiggle
room. You know, people who demand that before a mind upload can be bonafide, it has to preserve
every quantum state in the brain or some complete nonsense like that are, I just think they're off
the map. I mean, it can't possibly require anywhere near that level of fidelity for us to judge the
result to be a successful preservation of identity. That's just ridiculous. But again, I don't
actually know what the threshold is going to be. But I think we're going to have a lot of leeway.
I think there's going to be a lot of flexibility once we figure out how to do this.
Right. And previously in an interview in the past, you've mentioned the idea of a child
had drowned in very, very cold waters underneath ice or something like that. And in upwards of an
hour after drowning or being out of it for an hour, they've been able to be revitalized.
Right. So this cuts to a very specific argument. A common argument that comes up in
debates about whether or not a mind upload preserves your identity
is a concern that people have about whether or not it preserves your stream of consciousness.
There is this perception that you have this permanent lifelong and never-ending ongoing
dynamic stream of processing. And if that breaks and restarts, such as you go into some sort of,
you know, mind uploading operation and your entire brain is vaporized, but it's scanned in the process
and then it's revitalized in a whole brain emulation, a computational model, that breakage
marks a loss of identity and the person dies by definition and then a new doppelganger basically
springs up out of the ether, takes the place. So that's the basis of that argument. And I've
written not one, but in fact two papers on this very specific question. They have very similar
titles on the there. And I don't know, I think they're both worth reading. But
the there is a proof positive argument that that whole line of reasoning doesn't work.
So, you know, the way a sort of an exploratory debate will usually approach this is like, well,
when you fall asleep, you sort of lose a little bit of consciousness. Does that break your stream
of identity? And then you say, yeah, but you still got a lot of neural processing going on,
so that doesn't count. Okay, well, you go into the ER and you go under general anesthesia. That's
really deep. That's really, you know, that's really turning the brain down a lot. Maybe,
you know, does that undo the argument? They say, well, no, even in general anesthesia,
you know, you still got a lot of neural processing going on. So now that doesn't count.
There's just sort of a lot of knockdown arguments here that you're trying to sort of
work your way through. And eventually you get to the fact that there are real world medical cases
of people under two different scenarios, one accidental and one intentional sort of an OR
procedure that basically disproved this whole thing. So there is this accident, tragedy,
usually called rapid frigid drowning. It's occurred a few times. It either occurs when
someone falls into a frozen lake or occasionally in a dry example, they just fall into a snowdrift,
essentially. These are these are sort of real medical cases that have come up. And they are
found several hours later, seven, seven, eight hours later. And in a few, you know, in most cases
like this person dies, in a few rare medical cases, they dropped in temperature quickly enough that
instead of killing them, it essentially preserved them the same way a freezer preserves food from
spoiling. And in a few rare medical cases, those people have been miraculously brought to an emergency
room. And they had they basically shot from the hip. It's not like this happens often enough that
they know what to do. But, you know, the doctors basically went through the motions of trying to
slowly warm a person up so that they don't go into shock and various things of that nature.
And there have been cases of recoveries. Okay, what's the point of all this? The point is,
well, first of all, the point is not that their bodies and their brains froze solid, but you know,
this is not an example sort of natural accidental. Oh, and so that was the accidental case. Then
there's an OR procedure called hypothermic preservation. I can't remember exactly what the
term is. It's usually used for cardiac surgeries. It's not used for neural surgery. It's used for
cardiac surgery. You take the patient's brain down in temperature in order to buy more time to
perform the cardiac surgery. While you're working on the heart about the you're not pumping blood
through the rest of the body, including the brain, you take the brain down in temperature on purpose
so that the brain will survive longer, hopefully long enough to finish a heart surgery. Okay,
so those are the two cases. So then coming back to my point, sometimes people sort of
over interpret this. They're like, oh, this is an example of cryonics. We know cryonics works.
That's actually not true. In none of these realized medical cases did the person remain
in a cold environment long enough to literally freeze. That's not what these medical cases
involve. But they do involve a drop in temperature to, okay, the temperatures are in those papers
that I wrote. So going to them for the exact numbers I don't remember. But somewhere on the order of
15 degrees Fahrenheit, I think, so they're well above freezing. But the point is that
there is a temperature below which the brain does not operate anymore. It does not fire
action potentials between neurons anymore. Neurons do not operate just because they're above freezing
or something. There's a critical temperature at which they're able to operationalize being a neuron,
primarily operationalizing action potentials, which is the fundamental basis of all
neuro neurology theory. The propagation of action potentials is how we expect the brain works.
And that temperature is higher than some of these medical cases. So just I'm speaking
in sort of long interwoven sentences here. What it comes down to is there are medical cases where
a patient has had a brain not fire any action potentials for many, many hours, which coming back
to the original point means that their stream of consciousness was turned off like a water tap.
You cannot have a stream of consciousness if you don't have a brain sending signals through
itself from one neuron to the next. There's no stream of consciousness if the brain is not
processing. So these are patients. Stream of consciousness ceased. And when they were revived,
nobody called them a copy or a doppelganger or a body snatcher or whatever these ridiculous like
everyone just treated them as a revived recovered medical patient.
And that's it. That's the disproof of the entire stream of consciousness counter argument against
preservation of identity via mind uploading on the basis of it sort of involving some cessation
of processing during the uploading procedure. It's just a that's the whole thing.
I think the paper is going a little more detail, but really, I don't think there's a whole lot
more there. So that's you sort of brought that up. And I just thought I would kind of walk through
that. That's what that was all about. Is whole brain emulation needed for mind uploading?
Right. I think whole brain emulation
short answer, yes. I mean, I don't really see how you do it without it. There are some
interpretations of mind uploading that sort of lean away from that. There's this concept of
training a, what did they used to be called classically? They were called expert systems.
This is what they were called in the 80s and 90s. These expert systems, which have basically now
become bots and the recent LLMs and the AIs. If you train one of these things on the corpus of
a person's available lifetime of media, some people provide more media than others,
but everything they wrote, all their emails, any videos, home videos, everything you can
find that person ever produced, if you train one of these neural net AI systems that are sort of
all the buzz these days against that, you can produce a bot that basically in a dialogue acts
a lot like that person. So the theory, and there have been companies that have made an
entire business model around sort of ostensibly preserving you in this way, or at least preserving
like this sort of post-mortem puppet for other people to interact with. So the idea is you produce
the system, and in those cases where there's a rich enough set of data, you could produce a bot
that sort of passes for the person, and with that count is mind uploading. So that's a version
of mind uploading that doesn't involve whole brain emulation, but there aren't too many examples
like that. In most cases, mind uploading is a whole brain emulation concept, and the way I describe
it is that whole brain emulation is just a technical term. It's the process of modeling
and emulating the neural processing of a brain. Doing that is whole brain emulation. Mind uploading
is not actually a technical term. Mind uploading, the way I see it, is it is a philosophical
interpretation of the consequences of whole brain emulation. If you choose to interpret
whole brain emulation as a preservation of a person's identity, then the label for that
is mind uploading. Mind uploading is the interpretation of whole brain emulation
as a preservation of identity. You have uploaded the mind. I don't actually hear too many people
phrase it that way, but I've always thought of mind uploading as just being this interpretation
of the technical process of whole brain emulation.
So I mean, I guess the question comes to me is, do you think you'd need to upload all the exact
brain of a particular identity in order to reproduce that identity in the machine? Or are
there some parts of the brain that are rather generic that you'd be able to scaffold on more
templated emulations of like a generic brain, certain salient features of the brain which
are more important to preserve personal identity and during metaphysical in go?
Yeah. I mean, there's one really big one, which is that something like 80% of our brain by neuron
count is the cerebellum. And you could probably just throw the whole thing away. You might,
not bring your expert piano playing skills along with you. Because the muscle memory is often
encoded in the cerebellum. Or you could relearn it again. But if you're trying to simplify the
process of the emulation, if it turned out to be technically really hard, you're trying to find ways
to cut corners. It's debatable whether a professional musician or it could be any
professional athlete, think of some sort of muscle memory task. Would they consider that
to be a critical component of their identity? If they lost that, would they not be the same person?
That's a very sort of emotional question to ask. But nevertheless, it really doesn't impact
the cortical, by which I mean cortex, the cortex encodings that we generally associate with all
the rest of personality traits. All of that is in the cortex, not the cerebellum,
you know, not the thalamus, maybe the hypothalamus, because that's doing some short-term memory work,
you know, not the amygdala. There's all sorts of stuff in there that doesn't seem to be particularly
pertinent to what we consider to be human identity, which isn't too surprising, because
if you look at it from an evolutionary perspective, the human cortex and neocortex is just this
500,000 year veneer, you know, sort of thinly layered on top of four billion years of stuff
that we don't care about. We don't care about the aspects of our identity back when we were
amphibians, so why would we consider that important? What we really are interested in is just the last
500,000 years of neurological evolution. That's the stuff we really need to successfully carry over
to preserve human identity. Now, I'm not saying we have to just sort of cut, you know, cut it off
that early. You know, it might be nice to try to get a richer, more complete emulation. I'm not
against that, but the question is, is it possible to achieve, you know, a pretty good passing
whole brain emulation and wind-up load, you know, at some fraction of the total brain? And I think
we could. I think we could make a good enough version one with way less than the entire brain,
and over time, we'll probably be able to do the rest of it anyway, so it's probably going to be a
question 10 years later, but in theory, I don't think we need a lot of it.
Okay, so therefore, if we don't need to emulate the whole brain, then are we approaching the
compute required to be able to achieve this if we knew how? Do you think?
So there are very wide estimates. You know, there are different sort of dimensions, you know,
processing power, memory capacity. I think in a recent video or presentation, you know,
sort of the speed of communications in, you know, between the units was considered.
We, I think the expectation is that we have almost gotten there now with some of our supercomputers,
at least at the lower estimates of human processing capability. But this sort of begs the question
of whether mere number of computes per second is actually the trick. Or, you know, I'm a big
believer in neuromorphic computing in which you actually create a massively parallel system
that is connected to itself and wired against itself in ways that closely mirror the architecture
of the brain. And, you know, one might ask, well, who cares, you know, and this comes back to a
point made earlier, which is that I strongly suspect there is something about the recurrent
self-referential, you know, self-looping signal processing structure of the brain
that is important. I don't know why this is an open question. We don't understand consciousness well
enough to understand why certain network architectures seem to work and others don't. So the wiring
diagrams in the cerebellum have essentially nothing to do with your consciousness. If you
remove a person's cerebellum in surgery, their consciousness, their sense of who they are,
is all left completely intact. Again, there can be sort of some damage to their muscle memory,
but their sense of who they are is not harmed. And they're sort of, their conscious, like,
their conscious experience is not affected. So the way that the cerebellum is wired
is not the right way. And the way the cortex is wired is the right way. What is that way?
We don't know. It's the 21st century. What a time to be alive. I don't know. But I think that
I think that we need to get, I think we need to solve that. I think we need to figure that out.
We need to figure out what it is about the wiring of the cortex that is creating this
amazing experience. And that's what we need to make sure that we recapture in an emulation.
Do people know what parts of the brain were destroyed when Phineas Gage got a big rod for
his brain? They definitely do. I don't recall if I've told my head. So it went sort of right
behind his eye, which means that it hit a lot of his prefrontal cortex, which is, you know,
relative to a creme de la creme. It hit parts of the brain that are salient to
sort of emotion control or something, which is why after the accident he became very sort of
fly off the handle, kind of sort of an angry kind of person. He just sort of didn't regulate his
emotions. Probably something about that sort of, and it wasn't like an amygdala, you know,
sort of emotional response. I understand I really have to go read it. I'm sort of
shooting from the hip here, but my understanding is that our, you know, very advanced intellectual
cortex level ability to regulate our emotions to basically just think through a situation
instead of flying off the handle. That's all, you know, that's not the kind of carefully
reasoned stuff that, say, a mouse does, you know, a mouse more or less does actually kind of respond
emotionally to what happens around it. Humans can feel the emotions, but also
think through it and say, okay, I'm not going to act directly in accordance with my emotions. I'm
going to act in this intellectually rationalized way instead, because I have a cortex that enables
me to do that. And that's precisely what got blasted out of Cage's head. But no, I don't remember
exactly which, if you look it up, it'll tell you exactly which parts of his brain were damaged.
Do you think quantum computing is required for whole brain emulation or mind uploading?
Or if it's not, could it be desirable?
Yeah. So of course, Penrose and Haneroff made this famous.
Microtubule.
The most honest answer is that I don't know. Of course, people tend to have hunches and I have a
hunch. What's your hunch? I would be surprised if the quantum mechanical properties of neurons
had anything to do with sort of animalian evolved behavior. Why would evolution
need to go there? It seems like all it really needs to do is get the action potential sputtering
around well enough that the animal can satisfy the four Fs of survival. Why do you need quantum
mechanics to do that? I don't see the need. I don't get it. I just never found the quantum
mechanical argument for consciousness convincing. That's my take. But I'm also fairly open-minded
about it. There's a lot we don't understand about quantum mechanics. There's a lot we don't
understand about consciousness. Some people see that as an excuse to pin them together implicitly.
People like Deepak Chopra are like, well, we don't understand consciousness and we don't understand
quantum mechanics. That means they're connected. So that's garbage reasoning. Two things aren't
related just because they're both mysterious. But at the same time, they are both mysterious.
We don't actually know. So I'm open to someday discovering what there's a connection there.
But I don't see a need for it now. Forgive the cat. The cat is forgiven.
Once we can upload brains onto a computer, there's going to be an economic issue too
about how to afford to do all this and how to do it in a cost-effective way.
Such as a good ratio between the amount of minds that can be uploaded and
the amount of power which is consumed. I guess that the trade-offs will become more friendly over
time as technology gets better. But what's the role of the type of resolution that we might want to
capture and the ability to do compression to reduce the amount of space a mind might occupy?
I do have some numerical answers to that question. But more to the point,
everyone's used to these claims about the rate of advance of technology and it's true.
We can just barely scrape together a supercomputer that might emulate the human brain today.
It's millions and millions and millions of dollars and it will only run one brain.
And it's not wired correctly to do that anyway so we couldn't do it even if we wanted to but
in theory the architecture is there. So we're just, you know, that's where we are now.
So we're down the road. We'll be able to do it expensively but successfully. And then 10 years
after that we'll be able to do it cheaply. You know, the human brain uses as much electricity as a
light bulb. So we're going to get it, you know, at some point the power requirements won't be
relevant. You know, it's not like you're going to need a supercomputer or even anything as inefficient
as a desktop computer. You'll need something as efficient as a brain. And, you know, how
cheap are brains? There are brains everywhere. Like brains don't take much energy. The world's full
of brains. So once we are able to create those kinds of computers at that level of energy efficiency,
the efficiency equation and therefore the cost equation. So I'll just get about the window,
you know, so long as your brain, so long as your emulated brain can get, you know, as much energy
as, you know, a salad, then it'll be able to stay alive the same way we can, right? So it's just,
that whole problem's going to go away. I hate to say that it's a question of patience but it is.
People really chomp at the bit about futurism stuff sometimes. They're really like,
gosh, you know, this stuff really needs to happen in the 2040s or it's no good at all.
I mean, I'm sorry but some stuff's just not going to be available in the 2040s but that
doesn't mean that it's not going to happen, you know, early in the next century.
Or later in the next century. I don't know when but I'm not particularly concerned about that
because when it does happen, I just think it's going to be practically preordained. I don't
think it can be stopped. I also don't think it can really be expedited too much. I mean,
mostly we want to create a society and a culture that sort of broadly favors science and knowledge
and openness, you know, and sort of inquisitiveness and curiosity. If we just sort of create a
society that creates and supports these general ideas, the science will just happen.
Now, I can get into some minor specifics. You were talking about sort of resolution and compression
and things about that, things like that. So where we currently stand on sort of producing
whole brain emulations is, well, in short, we can't, but there are sort of multiple steps in
the process. So the most likely way to produce a whole brain emulation is to preserve the brain.
That means essentially embedded in a resin. And then you deli slice it. And then you take pictures
of each slice. You can't use like a camera because the features, the neurological features you're
trying to capture are smaller than the wavelengths of visible light. You can't take a picture of
something that's smaller than a wavelength of light. So we use electron microscopy to do this.
And the resolution that we're currently able to achieve is a z-axis resolution. That means
slice to slice. We can physically diamond knife slice brains at about a 40 nanometer thick resolution.
And then when we take an electron microscope image of a slice, we can achieve about a four
nanometer XY resolution. So each image has 10 times as much resolution, 10 times the fineness
of resolution as the slice to slice steps. So in the three dimensions, two of the dimensions are
finer by a factor of 10 than the third. And the question is, is that good enough? And the answer is
according to current models and theories of how the brain works, the major paradigm of
action potentials propagating along axons, triggering synapses, and then propagating secondary
action potentials and downstream neurons, we can image synapses at that resolution. And of course,
we can image all the rest of the neural components, which are larger than the synapses. So we can
actually capture the necessary resolution to produce a whole brain emulation. We can do it today.
What we can't capture is the scale. So the data sets that we are scanning these days are,
you know, what sort of cubic section of a brain can we kind of scoop up and actually perform this on?
And it's on the order of sort of fractions of a cubic millimeter. One full millimeter is just,
you know, sort of coming around the bend as sort of a 2020s project. So, you know, human brain is a
little bit bigger than that. So we've got a ways to go. I think a whole mouse brain of approximately
a cubic centimeter, which is a thousand times bigger than a millimeter, is one of the projects that
is being sort of put together now to be achieved over the next many years. And when that is done,
all we will have is a brain of a mouse. All we will have is one centimeter. We're going to need to go
another factor of 10 in all three directions to get to the human. So it's going to take a while.
But the interesting thing is that while it's going to take a while on the calendar to get there,
there's very little that actually appears to stand in the way. We just need to keep turning the crank
for a few decades. And there's no real obvious like place where we think it's just technically like
we're up against a wall. When we look at it today, there are things that we don't know how to do yet.
Getting the perfusion of the chemicals deep into a large brain evenly is very hard to do. We don't
really know exactly how to do it. How do you, you know, we can slice up a slice of a brain on the scale
of microns, but can we create, you know, a cross section of a brain that's only 40 nanometers thick
but is five inches across, right? Like how do you even handle that, right? So there are major
technical questions, but these don't feel like things are going to be like some fundamental
wall in physics that we cannot get past. It seems like we just need to keep going and we will get there.
Do you think that there's a particular, okay, so you mentioned
plastination, at least that's what I think you're referring to. Do you think that's the state of
art and cryopreservation today or brain preservation today? Or would you suggest that it's something
else? Or what about in the future? Do you think where the state of the art plastination techniques
that we have today are the best that they'll ever be? So correct me if I'm wrong. I think
plastination refers to room temperature preservation techniques. Correct. Yeah, I don't really know
where they stand. So you've got two things. You've got the cryopreservation and then there is the
preparation of a brain sample for slicing, which is not cryo. It's just room temperature
and it's a resin. I don't really know what the chemical structure is, but it's essentially you
bed, you first you flush out a lot of colorful and opaque things so that you can physically see
through the brain. And once you flush all that stuff out, you embed it in some kind of resin.
I'm not a chemist. I don't know. And now you've got like this solid block with a transparent brain
inside it. Then you slice that and you take pictures of it. Now preservation, especially
sort of preservation for the purpose of longevity, sort of cryonics or the next generation of cryonic
like preservation, which would be these sort of the aldehyde stabilized cryo preservation that
the Brain Preservation Foundation has awarded, which is probably definitely the next generation
in preservation if we can get it sort of normalized, popularized. That's for a different
task. That's for the task of preserving people, keeping them alive
for one of two reasons. Many proponents of sort of classic cryonics want to be kept alive
just to sort of get to that point in the future where the technology can revive them biologically
again. The other reason to go into preservation of some form though is to just get to that point
in the future, not where you can be biologically revived, but where mind uploading technology
has gotten to the point where it can upload your preserved brain and you would not come back
biologically, you come back as an upload. Now as you know, I don't want to completely distract,
we can get into like, you know, why would you want one? Why would you want the other? What's
the benefit? You know, why would you choose one if the other's available? There's a whole
conversation there, but I just, I think it would sort of take us off into the weeds, but
there's never, there's this issue of preservation for longevity and there is this issue of
preserving and preparing a brain sample for very contemporary and real, you know, sort of slicing
and scanning that we do today in labs today all over the world.
What about inspirations from biology and it's been mentioned before about the visual cortex,
how that's very much like the way a convolutional neural network might work. There's hierarchies
there, they've got logical receptive fields and shared weights. Do you think this is important to
sort of mimic the way that biology works in the way that we approach
developing a whole brain emulation or can it be a completely wildly, even wildly different
computing power time? Yeah, so yeah, modern convolutional neural networks were intentionally
inspired by the visual cortex. It's actually not the brains resemble convolutional
networks. Convolutional neural nets were sort of built on the previous four decades of research
into how vision works. We've actually learned a whole lot about how vision works and we said,
wait a minute, what if we code that up in a computer and see if we can see and lo and behold,
it's actually worked incredibly well. So one thing that that story tells is that
biomimicry is a very good strategy. If you figure out how nature does something and then emulate
that with engineering, that's very likely to actually get you pretty far down the road or
whatever task you're trying to solve. For whole brain emulation, there's an interesting opportunity
to cut some really major corners at the possible expense of like a major philosophical or
metaphysical failure. So what do I mean by that? At the lowest level, we could create a whole brain
emulation of neurons and synapses, individually firing action potentials down their axons across
the synapses into the dendrites through the full complex web of neurons. And we would expect that
to certainly be functionally successful. There's a whole big debate about whether it's successful
in some other way because some people think it has to be a biological system to be conscious
or it has to be whatever, but it would functionally replicate the action potential propagation
paradigm. But that's actually pretty computationally inefficient. Why recreate
a thousand neurons that fire in some complex arrangement to perform multiplication? If you
can just stick a multiplication module in there that has the same black box behavior of the neurons.
If you've got a set of neurons, you've got a set of neurons and you send input signals in
and you read the output signals out, you could replace that set of neurons with what we call a
black box. Ted Berger has been doing this with hypothalamus and if you basically produce a
prosthesis effectively that takes in neural signals and emits neural signals like the system that's
replacing the set of neurons that's replacing, you can use a potentially much simpler computational
piece of hardware to process those signals and yet get the same functional propagation.
So it's an interesting question how far that could go. How much of the brain could we abstract away
to some sort of algorithm? You know, one of the biggest ones would be like cortical columns.
The cortical columns are these sets of about a hundred neurons that seem to sort of process
kind of as a singular unit. We don't really know exactly what they're doing or how they're doing it,
but there's this repeated cortical column pattern. If we could solve the cortical column algorithm,
we might just black box the whole thing away. What if you just make a single computer chip
that emulates, you know, the hundreds of neurons and the thousands upon thousands of synapses in
a single cortical column, just turn the whole thing into some little computer that may not
work the same way at all, but it will take the same neural signals and it will critically,
it will emit the same signals. So it has perfectly replaced that behavior but in a completely different
way, right? And then the question is, well, what if you did that with every cortical column in the
entire brain? So a whole lot of the neural structure that we consider to be extremely
human in its architecture would be completely replaced with some sort of computational abstraction.
The same signals would be going through, but would it actually metaphysically
be the same sort of mind? And we don't know, we don't know where that cutoff is,
you know, where you're, where you're the sort of level of fidelity of your emulation
some loses some sort of metaphysical thing that is deemed important. We don't know where
that threshold is. I suspect that we might find it, you know, because, you know, as you produce
these things, as you produce these prostheses, these modules and emulate them, if you get some
sort of surprising behavior out of it, then you're probably abstracting too far. I'm not
sure how that would happen because if it's, if it's producing, if it's reproducing the same
functions, there isn't an opportunity for it to sort of behave differently, like that's a little
point of the functional paradigm. So I'm not really sure where it could break down, but if it was
going to break down, presumably we would discover it. I mean, otherwise you're just back into
philosophical zombie territory, you'd be able to replicate all the functionality, but you wouldn't
know that you've, but I, but then I just find that all very hard to buy. So what do you think?
Look, I mean, there's the issue of like the fire and the equations when like Hawking
spoke about like the, what is understanding of a mathematical formula? What is the, you know,
what's the important, what's the important difference between the way all computers at the
time when you wrote this subject would be able to calculate and the way a human would be able
to calculate. And he spoke about fire and the equations. I think it was Hawking. Anyway, maybe
it's got something to do with like something like that, an embodied understanding, maybe a feeling,
I don't know, a valence of it. It's going to be one of the big questions of neuroscience in coming
decades. Sure. Well, okay. Another big question that I thought to bring up was why is whole,
a form of whole brain emulation of your preserved brain, probably you and you've written directly
about this, but I thought I might bring this up to in the interview. Why do you think whole
brain, a whole brain emulation of Keith Wiley would be Keith Wiley, even if the original
was still alive? Well, that, that last sentence really, okay, where that's where people start the
big debates, right? Well, let's step back. Let's, let's exclude that last, that last sentence in,
because we will touch on identity branching. Yeah, better. I mean, that has been the bulk of my
writing is defending the branching interpretation. But yeah, why would it be me? So
there are different theories of metaphysical personal identity. For example, you have the
body theory, which basically says that your identity is attached to the matter that you're
built out of. You have a stream of consciousness theory, which is that it's some sort of continual
stream of consciousness. You have a space time worm identity, which is that this thing that
sort of weaves its way through four dimensional space time, because it's sort of continuously
located in sort of in a smooth path. The identity is sort of following the sort of
worm through time. But it doesn't discontinuously jump around. And then you have psychological
identity. There are others as close as continual identity. Close as continual identity says that
the thing that is you now is that which most closely resembles the thing that was you,
you know, very recently. And yet the end of the thought experiments to sort of feel
sort of where that question comes up. But another theory is psychological identity,
which is a catchall term for if I say college by psychological, we mean is the memories,
your identity is indicated by your psychological traits, your personality traits, your memories,
your all the all the sort of cognitive, emotional, intellectual sort of senses of your,
that's your identity. Every single one of these theories can be subjected to
sort of colorful thought experiments that try to sort of break them apart
and undo them and find some sort of paradox that leaves them.
And the interesting thing about the psychological theory is that it only breaks down
if you declare at the outset that you're going to reject branching interpretations.
So you basically have to say, well, branching is just going to cause a lot of trouble for
my thought experiments. So I'm just going to say it's excluded and we're not allowed to consider it.
And if you say that, then you can come up with a thought experiment where psychological
identity sort of runs into problems for the same way all the other identity problems
models can run into problems. But if you don't cut yourself off the knees at the beginning,
if you say, well, wait a minute, branching identity is on the table. Let's see where that
takes us. Then psychological identity is the most robust theory that I have yet found.
Well, okay, so you're going to be talking a lot about this in your up and coming novel
contemplating oblivion. So anybody who's interested in these topics would like to explore worlds in
which they are covered. Yeah, watch this space or watch out for contemplating oblivion.
So, but yeah, would you want to just give us a breakdown of some of the writings you've
covered identity branching in? It's your favorite topic. It's what you've covered the most.
Yeah. So it all started actually started slightly before my 2014 book. I was inspired to
write an article in which I sort of thought I would lay out my basic sort of theory of branching
identity in early 2014. I'm sort of going through my own list of writing here.
Was this the H plus magazine? Let me think here. It was
Oh, in March of 2014, it was actually a response to a article by Susan Schneider.
I remember. Yes. And I wrote an article for H plus. Yes. I think you might have emailed me about
that and got me to put that in there when I was on the board. Yeah. Yeah. And I'm looking at the
so if that was March 2014, that means that I basically wrote that article and then
I must have come off of that article with ideas for a book just boiling because I started it
immediately. I worked over I worked on the book that summer of 2014. So the larger work I produced
is my book, a taxonomy and metaphysics of mind uploading.
And then from there, I just continued to write. So I've I produced a paper in the Journal of
Consciousness Studies with Randall Conan, which tackles one very specific question, which is,
you know, we taught I've been talking about the slicing and scanning approach to whole
preamulation. Another thought experiment or scenario that comes up in a lot of philosophical
musing is what we call gradual replacement, where you profuse the brain with billions of
nanobot prosthetic neurons, some sort of they they learn the input output function of the neurons.
And then once they've got that function figured out, they kill the neuron and like attached to all
of its synapses and slowly over time, the whole brain becomes mechanical. I think it's all complete
and total nonsense. It's it's a fun thought experiment. It makes maybe potentially good
science fiction. I don't I don't know. But it's not practical. Oh, you know, it's sort of it's
very sobering to read sort of online debates about the stuff in which people, you know, with great
emotional commitment, you know, say that they really require like a gradual replacement process
in order to be uploaded. Well, I got news for you. It's not going to happen.
That is so technically challenging that I don't know if we'll have it in a thousand years. It is
beyond incomprehensibly challenging. Whereas scanning slices of preserved brains is a 20th
century tech. We're doing it now. So we're talking about the trade off between a technology that
exists now and a technology that I'm not sure if it will exist in a thousand years. That is the
that is the chasm between these two approaches. So Randall and I wrote a paper that argues not only,
you know, his gradual placement just technically just bonkers. But from from a philosophical
standpoint, we don't have to worry about it. It doesn't matter. It actually isn't a critical need
anyway. The whole point of the paper is to argue that metaphysically and interpretively, both of
these two procedures have the same outcome from sort of the perspective of personal identity.
So we don't have to worry about the impossible gradual placement process process, the scanning of
a preserved brain will work just fine. So that paper was in I was in JLCS. I've written several
others. I wrote sort of an all encompassing paper that was sort of a broad sort of life philosophy
paper with the tongue-in-cheek title of mind uploading in the question of life, the universe,
and everything, obviously an homage to Douglas Adams. And the the thrust of that paper is sort
of a step by step almost theorem like layout of an argument that argues that developing mind
uploading is the most important priority ever. It is the most important thing for us to work on.
And the reason why it is the most important thing is the entire sort of line of reasoning laid out
in the paper. One might immediately sort of knee-jerk response that unless it's ridiculous,
how can I possibly say that? Well, read the paper. The whole point of the paper is to explain why I
think that's so critically important. And then I've written two papers that get into this argument
that a breakage in the stream of consciousness implies a death and then some sort of invoked
doppelganger in the output. I found that such an important topic I actually ended up writing
two papers on it. I don't know, I probably have others too, but yeah, I sort of keep writing about it.
Well, I mean, do you think, how do you think people will react? I mean, when push comes to shove,
do people really care if there's a doppelganger or if what they feel is a doppelganger out there?
Do they mind if there's two identities that are metaphysically exactly the same, at least on
invocation? I think a lot of people are uncomfortable with this idea. I don't know if the concern is
whether there's either one or two of them. The concern is that by being some sort of a copy,
it represents that they actually died. They didn't really survive the procedure. They died. The
entire goal of surviving through the procedure and the upload failed. And the person living on
as the upload is just some other person. And therefore, the entire goal of using this procedure
as a method of survival has failed. So that's the concern. That's why people sort of get very
knotted up about whether or not it's a copy.
Okay. Well, was there any interesting points that you wanted to do? There was a recent meeting
you had, like, I guess, you know, is there anything that you discussed at this meeting,
at the Holt-Brain emulation meeting with Anders Sandberg, Randall Cunier, and yourself and others?
Robin Hansen was there. Yeah. Is there any points that you brought up that you thought you might
be able to reveal? Well, okay. So the topic of that meeting for the audience, the topic was
the ethics surrounding Holt-Brain emulation. So it got into several different questions. It got into
I'm just sort of reading out of the notes here, the future of work, you know, what will it mean,
what will work mean when to do work consists of spinning off a Holt-Brain emulation and having
it do the work and then shutting down the emulation, you know, when the work is done. What does that
say about, you know, people are always concerned about technology taking jobs. So if we're just
spinning off WBEs, you know, Holt-Brain emulations, then how are we ever going to get a paycheck ever
again? But then if Holt-Brain emulations are, you know, really sort of verging on conscious,
you know, human minds, there's a whole other question about work, which is,
are we making, you know, an entire society of slaves, right? So there's a whole, so that came
up. Then we talked about just sort of good old fashioned existential risk. It's a topic that
always comes up everywhere. I don't actually remember exactly where the conversation went.
I don't tend to have like a crystal clear, like immediate short-term memory. I'm the kind of
person you have to look at the notes afterwards to remember, and I don't have the notes in front
of me. But I can go through the topics. We talked about, oh, an interesting one was, you know,
once Holt-Brain emulation is sort of common, what are the implications in terms of neuro-security
and mental privacy? So this is a concern not for us, but a concern for the well-being of
Holt-Brain emulations. What rights should they have to the security of their brains and minds
against hacking, and what rights do they have to privacy? You know, once a Holt-Brain emulation
can probably be interrogated more easily, so it'll basically be more difficult for it to lie,
you know, because does it have the right to lie? We talked about access and inequality, democratization
of the technology. This is sort of a, you know, a very common concern that the rich elites will
not only get it first, but will get it first and then somehow lock it down so that, you know,
instead of just coming along 10 years later at the scale of sort of technological advancements,
somehow everyone else will get sort of locked out. We talked about sort of the ethics of using
animals for research and development, and then what the implications are for using increasingly
verisibiltudeness emulations in sort of in lieu of animals. You know, if there's an ethics question
about using an animal on an experiment, is there an ethics question about using a really, really good
emulation of an animal? I mean, so good that the emulation might be conscious, you know,
what sort of ethical concerns are there about using a system like that experimentally?
Unless we get into population ethics and emulations, I'm not sure what that refers to.
Yeah, I don't know. We just, we talked about a lot of stuff, but it was all sort of from an ethical
perspective. That was the point of that panel. Yeah, so I mean, this probably comes up for a
lot of people. So as futurists with some sort of idea of how mind uploading might work,
we might be concerned about friends or family who have a short time to live,
and the difficulty in discussing this with people who have not got an interest in
futurology. But have you got any advice on how to speak to people about the possibility of survival
beyond the flesh? Yeah, I sort of find myself having this conversation a lot. I'm just sort of,
telling people about my book, and I have to break through that initial barrier of people just
having an initial shock that my book is about what? That's ridiculous. So I have that conversation
a lot. People are actually generally receptive to the idea of it. I'm sure there's an entire
population contingent that just sort of has this very powerful religious objection, and I just
don't tend to cross paths with those sorts of people in my life. But people are just sort of
broadly curious about technology, but maybe they don't make it their entire daily life,
and sort of think about these things. But most of their exposure is through rather poorly written
science fiction. Mostly written science fiction is often pretty good. Hollywood is generally terrible.
So if your exposure to these ideas is mostly movies coming out of Hollywood, then you're probably
not getting the richest philosophical presentation of the ideas.
So what do I do? I talk to people about it. I just try to sort of not
be overly prescriptive, sort of telling people what I think they ought to believe. It's more
a question of whether or not this ought to be available as an option for people
who would choose it without putting some sort of prohibition against it. And then people who
are uncomfortable with it, I don't have to do it. I mean, nobody's forcing them. But you know,
sort of try to convince people that it's just a typical sort of let's let people make decisions
for their own sake kind of issue. If you can get there, then that's usually a pretty good
conversation. I think people are sort of resistant. People are always trying to figure
out whether or not the government needs to prohibit something. I don't entirely sure why,
but it seems to be something people worry about a lot. So I think they may just have to evolve
through a few generations. With each passing generation, they don't have to actually shed
their prior sort of biases from childhood because they're just sort of born into an
increasingly technological world. They will be born into a world of increasingly
sort of versatile AI systems with which they interact. You know, their teddy bears are going
to be increasingly lifelike with each passing generation. And by the time anything, even vaguely
resembling an Albright emulation is possible, that generation will have been interacting
with computers that are so smart that they will already seem lifelike anyway. And the whole
concept is just going to be relatively comfortable to that future generation. We just, I don't know.
I don't really worry too much about our generation because it's not going to happen in our time
anyway. We just need to keep the research money flowing. You know, so long as there isn't a general
sort of hatred of science, then we're doing okay.
Do you think cryopreservation for yourself is an option to reach for your, for yourself to reach
a time in which whole brain emulation is possible? So the Brain Preservation Foundation
has multiple sort of goals. One is to sort of advance the actual technology of preservation
to sort of actually get the thing available, make it technically possible. But then there's this
sort of advocacy component of, you know, how can this procedure become part of sort of
gender mill society, just in general, procedure available in hospitals or something like that.
I think those gears turn pretty slowly. That's a lot of politics. That's a lot of
red tape. I just have a lot of regulation. You know, I just, I tend to be pretty tempered about
this stuff. You know, other people just get really excited about it. They think everything's
going to happen in 20 years. And I don't know if I'm cynical or just realistic. I would be absolutely
amazed if the technology of human brain preservation was cheap enough and available enough
to serve me. But, you know, maybe if something happens very rapidly in the next few decades,
then maybe, but I don't, I don't know. But we're probably only missing it by one or two generations.
It's pretty sad to think we might be one of the last generations ever that sort of has to contend
with this. But I mean, it's more exciting than missing it by a thousand years. I mean, at least
we get to sort of understand it, right? You know, a thousand years ago, no one even understood any
of this. So that's pretty, that's pretty, that's good enough. It'll have to be. Well, who knows? I
mean, like we get AI powered scientists and there's some claim that would, that could really fast
forward a lot of the science. Maybe there's a chance. I don't know. Yeah. You mentioned that
there's a lot of written science fiction, which you are more, I guess, amenable to, which gives
better descriptions of how whole brain emulations might work. What science fiction is that?
What's your favorite? So a few years ago, I sort of sent myself on a mind uploading
voyage trying to find, you know, all the good sci-fi on it. Of course, I'm sure I missed, you know,
90% of it. But Greg Egan's permutation city is Santa's, Arthur C. Clark's city and the stars,
Alter Carbon, which of course was the Netflix hit. There were others because I had like four,
I had at least five of these books I sort of rattled through. So I'm missing a few. But it's
definitely a topic that's been presented and covered pretty well in some cases.
All right. Well, yes. Is there any points that you wanted to talk about that you
that I haven't yet asked you that you thought might be worth bringing up?
Well, I think you and I will talk about my upcoming book in another part of the session. So
I'll leave that to then. So yeah, it was nice to have an opportunity to sort of talk about these
ideas again. It's been a while. Yeah, it has. Thanks. Thanks very much, Keith. It's really
good to talk to you again. And yeah, I look forward to our conversation about your up-and-coming
book contemplating oblivion. Thank you very much. Cheers.

Welcome to Science Technology in the future. Today we have Forest Landy Rufus and he's
a distinguished philosopher and technologist. He's also the founder of Magic Flight and
I also believe that Forest invented one of the very first vapes, if not the first, which
is a healthier alternative to smoking through traditional means. And he's also part of
the Neurohacker Collective. But today we'll be discussing his work particularly on the
fundamental difficulties of aligning AI with human values. He's been particularly influential
in this part of the field and I have my L9000 t-shirt on to celebrate. So we're excited
to have Forest here to share his insights. So welcome Forest, it's really great to have
you on the show. Would you like to just give us a brief introduction to who you are and
what you do?
Great, thank you. Yes, I appreciate meeting you and being a part of this. So let's see,
I started thinking about existential risk things late 80s and early 90s, mostly connected
to nuclear arms race type issues. And then later over time I developed a philosophy that
allowed me to think about ethics. And so since then I've gotten more involved in various
different aspects of technology risk and what might be thought of as cultural and civilization
and governance design. So how do we think about how to make choices in a good way at
individual and collective scales, particularly for things that have long range implications
or that have low probability but high severity. So obviously with many areas of existential
risk it's not necessarily that we think that it's very likely to happen but that if we
did think that it was going to occur we know that the impact of it is quite significant.
Artificial intelligence risk turns out to be something which given everybody's sort
of aware of a lot of recent advances in that field has come up now especially as a particular
category of existential risk. And so in effect I looked into what kinds of things would we
need to do and assure to essentially have it be genuinely beneficial outcomes for not
just the inventors and the owners of this technology but for people at large and the
world at large. So that's roughly the field of study. After a certain amount of investigation
I came to understand that there was a category of artificial intelligence risk that didn't
seem to be part of the popular conversation. So people know that technology can be misused
and that obviously if you have the misuse of technology for things like propaganda or
for manipulating people or for adverse business relationships and things like that that can
be a problem but that would mostly be thought of as an inequality issue or just humans doing
human things in the ways that have been kind of known throughout history. But then there
emerged in the speaking of Eliezer Yukowski and a lot of the people who have been looking
at what are the long term implications of developing superintelligence and there is
this sort of category of risk that's been discussed a lot called instrumental convergence
i.e. that the technology artificial intelligence or superintelligence would seek power either
inadvertently or deliberately and that through that exercise would eventually displace human
beings and or life on this planet. Yeah, I actually did Steve Ohmahondo come and speak
in Australia at a conference who came up with the idea of basic AI drives which Bostrom
also came up with a very similar idea the instrumental convergence and I believe that
you've got a very similar idea and that substrate needs convergence but I'll let you go on
with what you were discussing. Exactly, so Nick Bostrom wrote this book on superintelligence
and issues associated with it but that whole category of there is a kind of intentionality
and that that intentionality results in harm to humanity and you know in that particular
sense the notion of alignment is can we make it so that the intentionality of the machinery
continues to be beneficial for both the owners and for humanity at large and of course one
of the things that comes up in this little bit is you know we want to be sure that it's
for humanity at large and not just the owners but that's the first mentioned category of
inequality risk but substrate needs convergence is actually a different category of risk or
different type of risk than that associated with instrumental convergence and people confuse
these a lot. Substrate needs convergence is basically suggesting that something about
the nature of the way the artificial intelligence the artificiality part of it contributes to
a kind of evolutionary dynamic that cannot not over time result in conflict with what we would
think of as organic or natural life and that this of course includes human beings and that in
effect there's a kind of convergence principle that is operating as a result of the artificiality
of the substrate itself that has essentially very very little to do with what goals or intentions
or ideology or whatever has been programmed into the machine or to the superintelligence and that
it's not a question of whether the superintelligence can or cannot align based on its desire to do
so but that something about the nature of the substrate itself the fact of its being a machine
that is different than organic life cannot not have the side effect of it being in conflict
with life itself despite whatever intentions might be programmed into it and so in effect we're
talking about a slower moving more evolutionary type dynamic and a lot of people when when hearing
this basically say well you know why couldn't we use the intelligence of the machine to essentially
counteract these evolutionary forces and it turns out that for a number of specific reasons
having to do with causation itself that the expectation that causation could or that intelligence
could counteract the needs of evolutionary process it turns out to be a false premise that
there's an inherent conflict of even asking the question that particular way which
really cements that this is a real issue and that we need to understand this issue
much much better for really assessing the degree of existential risk associated with
technology use of this kind so that's just kind of a here's what it's about and here's how it's
different than instrumental convergence and then we can start talking about what is the nature of
the proof and why does it show up as this isn't speculation we can be really confident that this
would be the outcome it's just that it would be a long term kind of thing it's like you know the
period of time when scientists first noticed that hey by the way carbon dioxide contribute to
global warming that observation was made more than 100 years ago but it wasn't something that
really showed up in the world until relatively recently so in effect this is the same kind of
thing that there's there's an observation being made here where we can say hey this is really an
issue and it might be a slow moving kind of thing but it has a kind of inexorability associated with
it that is really quite concerning it's a bit like saying you know can we use technology to
solve all problems and it turns out that there are some problems the technology itself cannot be a
solution to and this turns out to be one of those yeah it's interesting you mentioned for
evolutionary reasons yes we've been at the best of selection pressures for some time you said that
intelligence won't be able to override the evolutionary dynamics involved in that so agents
singular things they've by nature self-interested do you think in ai would be by nature
self-interested would it necessarily be self-interested well so so this is where we get
into some of the subtleties so for instance in one sense we can assume that agency is singular but
it turns out that the argument that i'm describing works whether or not we think of the agency as
being singular so it can be singular or plural or any version or mixture thereof like it can be
you know some sort of gradient where you had you know kind of a sort of focus and then you had a
whole bunch of components which had sort of a little bit of agency for their own and and regardless
of how you distribute the notion of agency it turns out that all agency has to be responsive to
its own needs to some extent like for instance for agency to continue to be an agent then in
effect it has to operate at least with respect to its own substrate needs enough to ensure its own
continuance which means things like maybe it would need to even in an unconscious way it has nothing
to do with goal structures that are explicit but that somewhere in the natural process of
how it's operating or just let's let's not say natural but somehow in the nature of the process
of how it's operating on at least an implicit level that some amount of self maintenance
or energy intake what we might think of as sort of a metabolism even if it's taking an electricity
and processing electricity in difference to how we might take in food and use that to create energy
for ourselves but that in some way there is a kind of implicit response to the requirements of the
substrate within which that intelligence is operating and so in this particular sense you know
whether or not the goal structures of you know take care of oneself are implicit or explicit
or whether they're singular or plural or any variation of these the same sort of dynamics will
necessarily obtain okay well you've said that you're um that we claim that it is 100 percent
possible to know now today that it is 100 impossible to align AGI and or establish or create a viable
notion of safe AGI to within any reasonable limits of any ethical timescale using any presence or
even any future possible technologies means or methods now Ali Adkowski is also quite
dim on the possibility of achieving alignments anytime soon but he also says that in his list
of lethalities is pretty strong about the claim I think he's about 99 percent sure that we're doomed
but none of it is meant to make the stronger claim about things that these things are impossible
in principle but you do yeah so that's what you would you differentiate your views from
his particular stance exactly so some people get hung up on the 100 number right and in this
particular sense we're distinguishing between what might be thought of as an empirical observation
versus what is thought of as a mathematical observation so in a sense we can in principle
know the number pi to however many number of digits but we'll never know the number pi completely
but to suggest that the number pi doesn't have a definite value would of course be something that
would contradict kind of how we understand the number in a similar way if we're like a function
yeah there's a value yeah it's a function of producers failure well again I'm wanting to
distinguish between what would be thought of as knowledge from an empirical basis that would be
subject to something like Bayes rule versus knowledge which essentially is on a purely
analytic basis and that you know given these assumptions and these methods of proof that
you could arrive at these theorems or these results and that there's no ambiguity associated with
the truth value of the results because you didn't allow for any ambiguity in the truth value of
the inputs or the methodology of derivation so in this particular sense you know I'm wanting to
sort of say that there's a in principle aspect associated with this which is really quite important
and so for those kinds of people that might think well you know you don't have any real
way of saying that about empirical things like how do you apply the mathematics of the empirical
and it's like well for example we can talk about scientific knowledge as being causal knowledge
about causation and we could talk about the speed of light being sort of a limit on the rate at
which causation propagates through the universe and we can basically say that for example like with a
black hole that the speed of light itself is essentially part of the thing that defines what
makes a black hole like it defines a relationship where once you go past a certain distance towards
the towards the black hole past the sort of event horizon that no causal influence from
inside the black hole can affect anything outside of it now because it's about the shape of space
time itself it's about the shape of how causation is related then we can make the principled statement
that you can't use causation to disassemble a black hole because in effect there's a
means ends conflict so in effect I could make the claim that based upon what our understanding of
is a black hole and our understanding of what causation is that just on those ideas alone we
can say there is no technological means to disassemble a black hole using causation because
technology is the application of causation just a nick pick just just querying you um
Hawking radiation with entangled particles um one inside or a few inside and a few outside the
black hole doesn't give you powers of causation to enact stuff happening stuff to happen inside
the black hole by mucking around with the entangled particles outside the black hole
correct I mean there's you know there's there's always going to be you know quibbles of one
sort or another but yeah in effect it's not like you're going to have some sort of causal control
over the amount of uh black body radiation or Hawking radiation that the that this that this
gravitational mass is going to cause to have a mission of so so in effect there's a there's
there's a sense here in which there are limits to what causation can do and so in effect what we're
wanting to do is to recognize what those limits are and how they might apply in this particular
kind of situation we're talking about artificial intelligence so in effect there's a sense in
which if I look at what kinds of control dynamics would be necessary to constrain um highly convergent
evolutionary process it turns out that the amount of control that is necessary is strictly greater
than the amount of control maximally possible so it's a bit like um like if anybody's familiar
with the bell theorem for example it's like there's a there's an inequality that's at the core of what
makes the bell theorem itself right it's it's the essential idea behind the bell theorem
is this is this underlying notion of a particular inequality that if violated basically says that
the universe has a quantum mechanical nature rather than a classical nature um and and and a similar
kind of thing shows up when we look at how control theory applies to artificial intelligence so in
effect there's a there's a sense in which we're saying okay if we look at how would it be you
know how would any kind of alignment algorithm be implemented like the very fact that it's an
algorithm means that it's a causal process and that if you look at what control theory itself
would require then in effect there would be a situation where um we could say okay we know
how much control we need to have given how the evolutionary dynamics operate and it's kind of
like a you know an amplification that's going on like in the in the the microstate of you know how
the the evolutionary principle would operate over time and over space that there will emerge
phenomenology that would not be predictable in advance um on any on any basis that control theory
would be able to use to to create predictions that would be necessary to create the kind of
alignment dynamics that we'd be looking for and it turns out that if you look at control theory in
detail there's kind of like five or depending upon how your partition it's six things that would be
required and it turns out that each one of them is impossible to um in itself to create uh the level
of um control necessary minimally necessary in order to uh have some well-defined notion of
alignment at a substrate needs level. Can we take this principle to apply to other humans
or is it just other just artificial intelligence and by extension what about the future of humans
what about humans who gradually augment themselves let's say another another 50 to 100 years we have
some something that you could say is somewhat a transhuman. I understood the the first question is
is does this incapacity to fundamentally control organic life apply the same way that it would
to artificial intelligence and the answer to that question is yes but the result that that makes it so
that organic life is aligned with other organic life is not as a result of control but it's a
result of essentially the fact that organic life shares um the same sort of metabolism process and
that in effect because of that because the substrate of organic life has commonalities with the
substrate of other organic life that there's an implied alignment process that emerges not because
of any kind of externally applied control algorithm or methodology but because the alignment itself
emerges as a result of uh the commonality of the metabolic process of organic life itself
and so it's in effect it's it's the fact that artificial intelligence is based upon artificial
substrates and those artificial substrates have different needs and those needs essentially
imply a different basis of choice and a different basis of choice implies different choices different
choices implies different outcomes different outcomes imply a different influence on the
environment which itself affects the substrate so in effect there's a convergence dynamic that
operates both in the case of organic life and in the case of artificial intelligent life but
because the artificial intelligence substrate is fundamentally different particularly at the
level of metabolism but also at the level of molecular constitution that effectively does not
result in a kind of ambient emergent alignment between organic and artificial and this is the
key thing um artificial intelligence would be joined with or aligned with other artificial
intelligence to the degree that they had a common substrate or a common metabolic process
but not otherwise and so in this sense if we're if if we're if we're looking at you know to what
degree would there be alignment between um organic and inorganic we'd say definitely not
to what degree could we control either organic or inorganic via control methodologies we'd say
again definitely not is it the case that that that that organic aligned with organic naturally
has a side effect of its constitution yes would it be the case that artificial would align with
other artificial again because of its constitution yes that could happen and is likely to now we
have the question of for say cyborgs where you have some mixture between organic and inorganic
and um in in this particular sense the the issue of alignment becomes really a very
present one because um there's a there's a sense in which we don't necessarily know
over the long term which is going to dominate the organic or the inorganic so then we would
start to have to look at what is the enthalpy of the relationship between the organic process
and the inorganic process and so in effect is it the case that uh cyborg type process is even a
stable meta-state and whether or not that meta-state even has endurance and I would say that actually
based upon what's been noticed in the space of whether alignment with artificial intelligence
is even possible and given that that answer is definitely negative I would say that we therefore
have very strong evidence that um long-term enduring evolutionary cyborg relationship
is highly unlikely all right so were you saying that it's it like um you'll have similar
issues if people start to augment themselves yes that's correct yeah that like like the I think
that to some extent there is the short-term capacity to implement some sort of biocompatibility
and to have a again for the short term and by short term I'm meaning you know maybe a dozen
to a hundred years or something on that interval but as soon as we start thinking about you know
significant number of generations like six to ten generations um that over that time there will emerge
that kind of um increasing tension between the needs of the organic and the needs of the inorganic
and as as a result there'll be a steady but subtle pressure um one way or the other depending upon
the enthalpy of the interaction and based upon what we know of the enthalpy dynamics it really
favors the machinery over the organic unfortunately it seems as though it's gonna be very difficult
to constrain people from augmenting themselves if they've got the freedom to do so how do we
balance their the rights of the people um giving them freedom to be able to do what they want to do
maybe even for instrumental reasons because they're unhealthy at first you know they need
they need a new heart or um you know they've lost their eyesight that they can replace their eyes
with actual you know new eyes um technologically improved eyesight uh or they they're getting
old and they're getting frail but they don't want to get old and they don't want to get frail
they want to continue on life's journey they've got a lot more projects that they haven't finished
and they're not willing to say goodbye they don't want to what do we do so I think we need to
distinguish a few cases here so for instance um I'm not in any real way particularly against
prosthetics but one of the things that we notice with the prosthetic for example is that we're
not presupposing that the prosthetic has agency onto itself and is therefore operating on its own
behalf so you know you you put your finger on it when we said something like that people can choose
in a way that's consistent with their own well-being or their own state of health or their own their
own nature and to some extent uh to the degree that the choice is held on the organic side and
continues to be held on the organic side I don't particularly see a problem um when when we start
to think about um you know cyborg type things and then in fact what you're basically saying is okay
well what happens um you know in the future when you have a baby do you uh preemptively
apply the prosthetics to the baby before the baby has agency of itself to make a choice as to
whether or not it wants that for itself right I mean you know to some extent at that point where
you're surping the choice of the next generation on the belief that some sort notion of modernism is
served by um you know replacing human components with artificial components um so in effect you
know to to the degree that those artificial components have agency onto themselves then what
is to say that in the subsequent generation you know the next one after that that the percentage of
artificial versus organic is weighted a little bit more towards the artificial a little bit less
towards the organic because the artificial is essentially contributing its own agency to that
choice that is displacing the choice of the now second generation and so I'm basically to some
degree there's agency um in these phones so an extended enactment of the agency of the developers
of the applications on this phone and the people within these organizations that isn't my own right
in your life like in effect the kinds of things that that you notice that you're being distracted
by or that becomes an addictive force in your life or that essentially constrains your choices to the
menu of options that the corporation and the developers essentially allow you to have right
and so in effect there's a kind of entanglement process where they essentially bundle services
together that some of what you want and others which you don't want and there's a kind of extortion
that happens as a result you don't get to vote with your feet because the operating system is
tied to every single application and vice versa and as soon as you want to make an improvement
of one service you want a sex force to do with others that undermines everything else and so
in effect there's a sense here in which we are already seeing some of these dynamics play out
it's like a dark mirror episode actually happening and so in effect I like that show yeah I personally
find it a little too frightening because I I like the what it I guess gets people to think about
and I don't mind getting watching a like a horror film maybe now and again so yeah I understand
so but but the notion here is that you know we're being apprised of significant potential issues
through narrative stuff like dark mirror as hey these are issues that deserve
careful philosophical attention like what is the ethics of of this situation you're asking me
how do we balance the the the rights of individuals and essentially the dynamics of
corporations and it's exactly the same question it's like organic process versus corporate power
which is a kind of artificial power and so in effect there's a sense here in which you know to
answer questions like this we have to think about the relationship between say care transaction and
power and how each of these shape the modern world and and I want to go back to a question
that you asked a little bit earlier than that which is you know is there you know some real
sense in which we could encourage people to make better choices in this in this space like
not compel them but but say for example as a result of watching a dark mirror episode you say wow
these are things I'm noticing happening right now I don't want to get sucked into that nightmare
I want to make better choices today so that my children have a future that I would feel was
good for them and that they would feel was good for themselves when they became old enough to
know the difference right like you know if I if I love my children I'm going to really want to pay
attention to what's genuinely loving and healthy for them so that they can make healthier wiser
choices even if I don't know what those choices should be at least I can trust that the choices
that they're making are truly wise and healthy so that maybe the generation after them has a chance
right well we don't necessarily want to take away from our children's agency but I get the sense
that we want to constrain it in such a way that will allow them to develop it further on like I
don't want to give children agency to to play on train tracks or you know I play football
in the middle of a freeway or something like that I don't want them to hurt themselves and and
there's another there's another thing of this like I mean we can use technology to reduce the
likelihood of occurrences of cystic fibrosis and stuff like that so we've got somatic and germline
gene therapy there could be I guess agency promoting meaning that they don't need to spend as much time
dealing with unchosen ill health
let's unpack that okay so about the child playing on the train tracks or in the street right we both
know that they do not yet have enough wisdom to make good choices in that space so as adults we
apply constraints but the thing is is that we know that eventually we'll pass and our lives will be
over and their lives will will be continuing and so as a result our hope is to teach them enough
wisdom to somehow convey to them the kinds of capacities so that they can make better choices
for themselves healthy and wiser choice for themselves because eventually right if we if we do
succeed in creating not taking away their choices but adding wisdom that we will eventually trust
that they will make better choices like when they're in grade school or high school yeah we're
going to need to set some limits right but the the limits are only for the sake of creating enough
time so they live long enough to have the chance to take on the wisdom but for example if if you
were talking to a 25 year old or you know maybe someone in their 30s for example and you said hey
do you want to play on the tracks you'd trust that they would probably say no i don't really want to
do that it doesn't feel safe to me right and at that point i'm not applying limits i don't need to
the limits are are not even something that's coming from the outside they are making a better choice
and so if we trusted that there was adequate wisdom in society in the the broad scale of things
then the question about you know will people make better choices answers itself so in effect
i'm not thinking so much around the ideas of how do we use technology or modernism or system or
governance or things like that to create tighter and tighter more and more perfect systems of
constraints to prevent people from doing bad things and to create the capacity to have care
to be skillful in how that care is held and to be able to have enough presence and skillfulness
and care that that all three of those together create the bonds of trust that hold us together
as communities and as life and that in effect it's it's how we add those capacities and how we
create that between each other that allows us to move past questions of constraints and more into
the questions of appropriate levels of freedom there's a few things which came up for me there
one of them is if i'm glad you said 25 or 30 because there's research out there to say that human
brains aren't quite developed especially the sense of cause and effect until 25 right but we
still give licenses to 18 18 year olds or in some countries 17 years old to drive cars but the other
thing is that came up was evolution is a slow process but it does happen over long time scales
with the technology surrounding us and the world which is increasingly becoming more
technological and we will evolve given enough time if we are still around around our technology
do you trust selection pressures of blind natural selection to evolve us in the right way forward
do you think it will will end up as better people through natural selection given a long
period of time of evolution ahead of it i do but it's got to be enough time right because we're
that the thing about technology is that you know it's very recent on evolutionary timescale right
basically the last 5000 years is you know when we're talking about changes in human process right
we we move to the city-state and you know we've we've in a sense made it more and more complex
in life right and so in effect you know evolution has you know it's it's moving relatively slowly
compared to the pace at which we've taken on the the the the the whole package associated with
what we might call modernism and so in effect yes in enough time we we we can certainly learn
to adapt to that and then trust the adaptations but but we do need to in a sense understand
how to get to that longer term perspective right because in the same sort of way that that that
we both agree that yeah we've we've seen enough research to say that brains are still developing
throughout the human life and that the level of wisdom adequate to the kinds of power associated
with some forms of technology genuinely suggests that you know frankly we should constrain constrain
the use of that technology until evolution has had enough time to catch up either individually in
terms of our personal growth and learning or collectively as a species and that you know in
effect of a big part of the Fermi paradox is associated with this right which is you know
is it the case that technological civilizations don't last very long and you know there's other
ways to think about the Fermi paradox but this is the main class of thinking about the Fermi
paradox where we still have the capacity to choose a future course where that low probability of
longevity associated with technological civilization could maybe be ameliorated that maybe that it's
not terminal for civilizations that have technology that there's some leap that we can make some
conscious choice process that we could make that would supplement the evolutionary process
to buy us enough time to reach the point of which evolutionary process could catch up
and so in effect what I what I basically am suggesting is is that through a large extent
we need to do something in place of evolution in the near term like for instance if we're
saying evolution's going to take you know another five to ten thousand years to create the adaptations
so that we just naturally and innately have the wisdom necessary then that means we need to
consciously take in the wisdom necessary through our collective governance process
to have that be a supplement for the wisdom that evolution does not yet have
and so in effect in this sense what I what I find myself doing is is saying okay here's a
way to consciously understand the ethics behind choice itself here's a way to consciously understand
the dynamics of choice as an ontological reality and how that's compatible with causation and change
the way we understand them ordinarily but does require us to extend our thinking in these fields
a little bit in order to recognize what that compatibility actually requires of us
in terms of responses that we need to give as a species and so in this particular sense
yes I believe in evolution but I believe even more in choice and so in this particular sense
I'm therefore emphasizing choice individually and collectively as a skill that we need to develop
particularly because if you think about it there's a kind of threshold phenomena associated with
technology it's like it's like there's an absolute level that you need to be at least this smart in
order for you to understand causation and if if you don't pass that threshold technology just
doesn't develop but as soon as you do then technology has this sort of self-perpetuating
aspect it has this accumulative aspect that creates a kind of you know geometric growth
as we've seen like a kind of exponent on the growth curve associated with technological process
and so people are looking at this exponent and saying wow human minds aren't very good at thinking
about exponential process in fact they're fairly good at thinking about multiplicative process most
of us are still operating in the consciousness which is additive we're very good at thinking in
terms of quantity in an additive and subtractive sense and a few of us that's figured out how
to think in terms of ratios and multiplicative process and those people tend to become finance
specialists or wall street traders and things like that and so in effect your your entrepreneurial
class is essentially starting to say hey this exponent stuff is really cool let's figure out
a way to do that but every single category of existential risk is associated with exponential
self-recursion type curves and artificial intelligence is no exception to that and so in
effect part of the thinking about existential risk including that associated with artificial
intelligence is learning how to deal with exponential curves and you'll notice that if our if our
growth in intelligence is occurring on a linear process but the growth of technology is occurring
on an exponential process and it's not very long before the exponential process outstrips the linear
one and so in effect there's a sense here in which as soon as we cross the threshold as a
species where we could do technology then we became literally the dumbest species possible with which
to have things like technology like like we still have neolithic planes but now godlike powers with
things like nuclear and biotech and and you know artificial intelligence and all the rest of it we
literally are in evolutionary timescales we humans have really only just evolved the ability to think
smart to have this sort of sense we can develop this these types of technologies and in a sense
we've had to scaffold civilization to enable them as well evolution has had a long history
it's fault tolerant in a sense it's evolved it's it's been working and it's but that the this new
form of intelligence is causative power that we have is only new on the scene now it's very powerful
but it could also be very brittle right and you know could end up causing our own doom in a sense
we we could develop the technology which could kill us right yes so what's the missing piece
i'd also be interested to know where you think choice exists in the timescale of evolution
where that first came from do you think it only exists in humans or does it exist in other
agents animals well so so first of all i need to to back up and make a distinction because
you're asking if choice exists and technically speaking choice does not exist there's no such
thing as free will but choice is real free will is not real but choice is real and to say that
something is real is to make a different claim than to say that something exists and is also
again just for completeness sake is a different claim than to say that something is objective
so science for example um has the requirement that something must be observable in an objective way
and so in effect to to to to ask is choice objective well it turns out that that's also
not true right that you don't have objective choice and you don't have
existing choice but you do have a reality to choice and so and basically to say
to talk about the real is actually to talk about something on a more primal level and less
and non-contingent way like existence is contingent it's it's contingent upon a subject
object relationship or it's contingent upon a a um you know content context relationship
or it's contingent upon a measurability of some sort right but when we're talking about
the the the subject object relationship as the relationship itself being more primal than either
the subject or the object then in effect the notion of agency that we would normally associate with
the subjective becomes an aspect of the interaction between the subjective and the objective which
is more primal than even the subjective and the notion of choice lives in the interactiveness
it's more correct to say that choice has self than it is to say that self has choice in a certain
sense choice is more primal it's more ontological and is more deeply tied to the notion of epistemology
and axiology then um then then would obviously be the case if we were to try to treat choice
as a say an epiphenomenalism of of brains or of animal consciousness in particular
so you know in effect and then this again these these get into some very subtle aspects of
philosophy and of of metaphysics again yeah a great conversation with Daniel Schmacktenberger
about this that I'll that I'll put as as a link in the description of this video I think it was
one of the neuro hacker collective conversations definitely worth a listen I'm still trying to
grok I'm still trying to digest this it it's hard to understand if you don't have the right background
not a not a philosopher at least I never studied it I'm very interested in it but this is difficult
this area to understand probably the most misunderstood areas of your views that I think
people find it hard to grapple with if I were to answer the question then the sense of do animals
have choice do they make choices I would just say yes right um it's it's and and and that that notion
of choice emerges um as as a as a kind of aspect of the real like literally from foundation so in
effect to the you know like we could we could posit for example that that the notion of choice
in relation to change and causation is is a very primal way of understanding the universe
much the same way we could talk about creation existence and interaction is a very primal way
of understanding the universe so so in this sense if we if we take in the the deep nature of choice
one of the things that we notice is that it is inherently cooperative and it is inherently
unobservable even though it is real right so so time for example is not really an object of
observation but it does have a notion of realness associated with it at least in the first person
obviously not in the third person but in the first person there is a temporal sense that that that we
experience reality as as essentially there is a definiteness to the now that is distinguishable
from the past and the future although we have no real way to to assert that from a third person
point of view we experience it from a first person point of view similarly we distinguish between
here and there and what has happened versus what could have happened right the the alternate possible
events that that that could have been if for example something else had happened the the explicit
contrapositive you know events distinguished from the events that did happen so so in this sense
when we're looking at you know a very deep understanding of the nature of choice we're
seeing that it is essential for us to account for particularly when thinking about ethical issues
associated with how agency interacts with other agency right because the notion of ethics essentially
is the study of the principles of effective choice so in effect for there to even be a notion of
ethics or of alignment or of any kind of safety in the sense of preserving the well-being i.e.
life liberty and a pursuit of happiness of any particular agency then in effect the notion of
choice is intrinsic to the very notion of ethics itself in order to be able to address questions
like this and in fact i don't think it's really possible to understand ethics again distinguish
for morality without understanding the nature of choice and also the nature of goodness like
what do we mean when we say that something is healthier than something else what does that
actually ground in as a as a reality in terms of you know what is it that makes life life and what
is it that makes meaning meaningful and what is it that makes sacred sacred that there's a kind of
underlying topology of these ideas that allow us to come to a kind of definite knowledge about
things like again whether or not the notion of artificial intelligence can ever be made safe
for organic prospects and again i assert just on the basis of what i consider to be a clear
thinking or a kind of mathematical thinking that just says definitely not and it's knowable to any
person that knows how to think about this definitely well that's interesting so you're
saying that ethics is the preservation of choice or agency you've also said once that probably
many times that love is that which enables choice however other people have the view that ethics
is all about increasing well-being and reducing unnecessary suffering or you know abiding by
rules or exemplifying a particular virtue so there's varieties of different ethics out there
that help people who are listening and are probably aware of where would you sit would you
sit in an established well-known view of ethics or do you have something more subtle that you'd
like to clarify well i can kind of position the way i think about ethics in relation to the kinds
of ways that you were just naming so for example in when we say ethics is the study of or the practice
of the principles of effective choice and by effective we mean goodness and in this sense
as you mentioned goodness can be thought of as preservation of choice capacity or it could be
thought of as maintaining of health and vitality and and that is a notion of goodness it can also
be thought of as a notion of integrity and it also sounds to me like consequentialism in in the
sense that the consequences of choices are the good the good consequences of no so let me let me
position it like i'm just i'm just laying out the framework to just even answer that question
so for example if we were to basically say the principles of effective choice
we're talking about the principles not the outcome okay the outcome is obviously what we're
saying the notion of goodness is attached to the notion of they integrity but integrity is a process
it's not an outcome like i can preserve integrity i can work in a way that maintains homeostasis
or capacity for choice or capacity to perceive blind spots or or you know some notion of virtue
right but the but the notion of principles and practice is a dynamic one and so in effect when
we're when we're looking at this i'm not looking at the the the the choices in terms of their outcome
but the choices in terms of their process and the degree to which those processes are
implementing those principles and so in effect we can we can talk about things like consequentialism
as being um you know something that is uh something that becomes possible once you have started
or come to some grounding some non-relativistic grounding of of of this uh notion of goodness
or this notion of integrity or this notion of health or well-being or of or of choice itself
and how those combine um the ethics that i'm describing is a process ethics and not an outcome
ethics so it's not a consequentialist ethics but as a process ethics it's about you know things that
could be thought of as compatible with integrity so when you're talking about health and vitality
or preserved choice or you know well-being or or those kinds of things those are all in one sense
or another um parallel manifestations of of integrity in effect i think of a consequentialist
ethics or a um you know rules-based ethics as themselves resting on a non-relativistic ethics
but that's not always obvious so for instance like a consequentialist ethics is basically saying
something like um do what creates good outcomes but the notion of good outcomes or
what would be virtue in that particular sense or utility right a lot of consequentialist ethics
are utilitarian um they don't have a way of grounding the notion of goodness or of utility
so in effect somewhere along the way you do end up coming back to or or needing to ground a
utilitarian ethics in a non-utilitarian way in order for the utilitarian ethics to even make sense
similarly for any kind of rules-based system that's somewhere along the way you have to say
where do the rules come from what sets the the rules and the rules like any kind of
system that's based upon rules i would call a moral code and moral codes are basically rules
that makes sense within a particular context within a particular world so for example in the
world of email um there's a kind of rule like don't send an email in all capital letters
because it makes it pretty hard to read at the other end like is polish or polish the word intended
well you don't know because every word is capitalized and normally the way we would tell the difference
between a country and something you apply to your shoes would depend upon a capitalization of
the first letter which of this course i can't distinguish i also can't distinguish the emotionality
associated with the letter because all the sensibility is gone because everything's capitalized
so in that sense there's a reasonable rule which is don't capitalize everything unless you don't
have any choice and there's a sense in which for every world there's a set of rules which are
projections or translations of the underlying non-relativistic ethics as could best be applied
in that world so in this sense um yeah it's not a it's not a uh denominational ethics it's not a
utilitarian ethics it's not something defined in terms of outcomes it's defined purely in terms of
what is the nature of choice and what is the basis of the notion of value and it turns out that
value purpose and meaning are an intact they're entangled concepts they're part of what i would
call a triple they're distinct inseparable non interchangeable concepts and it turns out that
meaningfulness is the most basic concept so in effect if i want to know what value is i have to
understand it in terms of meaningfulness and this is where we get back to process dynamics like
what is the the dynamic of the relationship because the notion of meaningfulness can itself
only be grounded um in the notion of the relationality between subjective and objective
it's not something that exists purely in the subjective nor purely in the objective it exists
in a relational way and a very specific kind of relational process and so in effect there's a
there's an element of the realness of the subject object relationship but gets notions of integrity
but gets notions of choice but gets notions of goodness uh indirectly through the coupling of
meaningfulness to value to what we would call you know right values um and and from there up into
things that might be thought of as consequentialist but but without understanding the grounding of
this particular process then particularly in high criticality situations such as existential risk
it becomes really important to be super clear about the the derivation methodologies of the
non-relativistic ethics or frankly are likely to make world-destroying choices so in this particular
case i find myself in a somewhat unusual position of starting off as a kind of metaphysicist
interested in the ground of reality the ground of what is the nature of the real and from there
finding that it's its its main area of application is not just philosophy but critically important
philosophy having to do with things like technology development and civilization and
things like the Fermi paradox. I'm interested in this because it seems to relate to the idea
that there's something real out there that you can measure a utilitarian or a consequentialist
might say well you can measure the signatures of suffering or the signatures of pleasure or the
signatures relating to sensitivity to novelty and i'm not sure about choice making would you
consider yourself a moral realist or an anti-realist or somewhere in between does that even is it a
concept that even makes sense? It doesn't really make sense i think the closest analog would be to
say that i'm a non-dual philosopher so there's dualism would effectively sort of start with it
goes back to Descartes for example but but the the notion of realism and the notion of idealism
are basically kind of two main currents of philosophy you could think of idealism is showing
up in things like Christianity where the universe is thought of being an emanation of God or of
say Hinduism or Buddhism or things like that where the the notion of mind and consciousness
is taken as primary and phenomenal existential reality is sort of thought of as a epiphenomena
of consciousness whereas realism would hold it the other way around that you have material
reality atomic or particles sub subatomic physics and that out of the mathematics of
those interactions that emerge things like chemistry and bodies and brains and therefore
minds and that consciousness insofar as it has any notion of realness at all is thought of as
an epiphenomenon of physical reality whereas my point of view is is a non-dualist philosophy
which does have a certain amount of historical precedent but i think of my version of it as
essentially being a from scratch derivation which arrives at the same sort of thinking
that historical philosophers in the non-dualist tradition have already arrived at but looking
at it from a from scratch sort of perspective gives clarity on things like the non-relativistic
ethics which turn out to be really important for addressing issues associated with say modernism
as a philosophy and in particular but in effect um i'm neither a realist nor an idealist i am
something which is between and beneath those so in effect the the way in which i think of
realness has to do with the relationship between the subjective and the objective as being the
primary basis rather than saying something like the subjective emerges from the objective which
is realism or that the objective emerges from the subjective which would be idealism
i'm basically saying that the relationship between the subjective and the objective is the
primary basis of the notion of real out of which both reality and consciousness are themselves
emergent phenomena okay so this is this is what you mean by real and how it's different from exists
so exists maybe you would be a moral realist if the word real meant what you meant right
by the word real and it's a relationship between the subjective and the objective
but what most people think is moral realism they're really moral objectivists in a sense
yeah yeah and so in effect there's there's there's a notion here that if we were to make the update
associated with the difference between to to exist to be real and to be objective
and that we were to hold that in mind like we were to have clear thinking about that
and then in addition to this uh or as part of this we were to distinguish between morality and ethics
then you could assert that i am an ethical realist meaning that i hold that there are
definite principles to ethics that the study of the principles of effective choice
does converge on two precisely specifiable principles from which we can think clearly
about what does good choice mean in each context in which the notion of choice itself would be
relevant and the the notion of real being entangled the notion of real itself basically
being directly coupled to the notion of choice means that everywhere the notion of real applies
the notion of choice applies and the notion of ethics applies and therefore the notion of
conscientious choosing or choosing in a way that is consistent with the ethics is therefore
a coherent notion a coherent concept and if we translate those coherent concepts into terms that
are easier to understand in a particular domain or in a particular world such that the heuristics
of those translations make it simpler or easier that in the default case people know what to do
then in effect you could say that i'm something like a moral realist but but i honestly find that
the world's change and that that the objective world change the change is a phenomenology just as
much as causation is and just as much as choice is and that therefore no static set of rules is
going to apply in perpetuity that in effect you know when the rules start breaking down when the
usual techniques of civilization no longer solve the kinds of problems that we're faced with
then in effect we need to go back to first principles and derive new practices
so that we can address the problems that we're actually faced with so for example we can notice
things like as i mentioned inequality pollution issues associated with existential risks issues
associated with capitalism species loss all these kinds of things are complex long term
multi stakeholder multi domain you know hard to understand issues hard to speak about issues
there's no bullet point or you know speaking speaking engagement things that that a politician
can really do to adequately grapple with these kinds of concepts you know government systems as
they exist in hierarchical and transactional forms you know procedural forms really just
doesn't have the bandwidth necessary to even really understand the nature of the problem well
enough to make good choices in these spaces so as a result we need to have a deeper understanding
of choice process a deeper understanding of consciousness process and ethics process to
create the kinds of practices that would implement wise choices in these spaces where
the usual methodologies just won't work and it's not to say that the usual methodologies
aren't great for dealing with the kinds of things for which you know single stakeholder or single
domain or relatively straightforward issues that operate on relatively short timescales
you know for those kinds of things current methodologies work really well but you know
civilization and the change of the world presents us with problems like you know each year we get
a new crop of problems and we notice that our technique may work for most of them but it leaves
a few that it doesn't address which accumulate from year to year and become more and more severe
until we enter the liminal space and actually are willing to be naked with the truth of the
reality of what is have enough humility to recognize what are the principles that we need
to translate into practices so that we can actually endure as a species in this case particularly
can we measure what we need okay so it seems as though there's the referent and the referrals
the referral um being the thing which is doing the referring and the referent being the object
which is being referred to right so we have reference which is the action of connecting
the referral to the referent yeah so there's a couple of referral can the reference being measured
outside of the minds of humans or the you know the writing or the i guess the externalized cognition
of so this is a very subtle question i'll answer it i'll give you an exact answer so to understand
the question right we need to recognize that there's a relationship between referencing as a process
the refer and the referent or in another language the perceiver the perceived and the perceiving
so first of all would you um would you would you admit or recognize that the the refer the
referred to and the referring is essentially the same the same construct as the perceiver
the perceived and the perceiving my i guess without having a long time to think about it i'd say yes
it sounds very similar i'd i'd say the difference is i mean everything exists in material reality
i believe so in effect what i can do is is i can work with you within the perspective that you have
but doing that i'm going to construct another perspective within which i can answer the question
which will make sense to you based upon the nature of the question so so in effect rather than
than than making any other assumptions we can just basically both of us agree that there is a
refer or referred to and referring and we could just leave it at that like we don't have to make
any other assumptions to be able to answer the question that you're asking but just for the
sake of convenience i'm making up an analogy to perceiver perceived and perceiving and i'll
leave it to you as an exercise that you can think of in the future as to whether you accept
that there is a strict isomorphism between those two triples so all right so in effect
i am going to notice that for each triple in the same way that there is a distinct inseparable and
non-interchangeable aspect like the refer is not the referent and the referring is not the refer
or the referent and that in the same way the content is not the context that the relationship
between content and context is its own entity neither content nor context the perceiver and
the perceiving are distinct the perceptual process is distinct refer and referring and
refer or distinct but they're inseparable right you don't have any one of them without the other
two so we're noticing a little pattern right now it does seem to me as though the the process
um is actually existing in the mind of the refer right so so you can say that may not be part of
the refer but it seems to be there or inside it's it's part of it it's a computation right
well i wouldn't go that far i would actually say that without some relevance to the outside world
like like the notion of referring creates a kind of bridge to something outside of the self right
like anytime we're making a measurement a measurement is a process but it has both a
subjective and objective component in fact this is the same reason i was talking about
perceiving and perceiving because you use the word measurement right like measurement is a process
that connects together something objective and something subjective and if there's no correspondence
between the objective and the subjective then the notion of measurement hasn't happened yet
so i'd say it's both right like like if there's no objectivity associated with the measurement
then yeah it's just an idea but not a measurement and if there's you know data collected but it's
sitting in a database somewhere another nobody's ever looked at it it's not a measurement yet
it's got to have relevance it's got to connect back to something around the the notion of of data
becomes information and information becomes relevance and so in effect when we're when we're
looking at this all i'm really doing is just basically saying that there is an objective
aspect there's a subjective aspect and there's a relationship between them and that that's all
that we need to answer your question about is it the case that anything about the nature of
what we would call ethical process is measurable because that's the question you ask right so so
in this sense if i'm unpacking it what i'll notice for example is that it's a bit like asking
like perception for example the reason why i go to the perception was because it leads directly
to a way of answering the question which is um say i have a ray of light that that is like a
laser beam that's crossing in front of my eyes does that you know like like it's it's just passing
in front of me i won't see the laser beam because there's nothing there's nothing for me to see the
light that's going from the screen to my eyes passes right through the laser beam
right so i can make a measurement of the screen and the laser can be maybe measured
by some sensor that's you know measuring the energy of the laser for example and those two
measurements cross one another right light has this capacity to have superposition
so in this particular sense the the light passes through each other which basically means that
i can't perceive if perception is based upon light is based upon some electromagnetic phenomena
then i can't perceive perceiving i can i can only perceive that which is perceivable
which is the object i can't measure the measurement nor can i measure the measure
so the subjective and the process of measurement itself are both non-measurable
only that which is measurable is measurable which is the object right so this this is interesting
because i've thought about something similar and it's the idea that at some stage will have
the instruments that are good enough that they can get a high enough resolution and a deep enough
sort of frame rate to be able to measure the physical process of whatever's going on in our
brain to give a real-time sort of movie 3d movie like you know at a billion frames per
millisecond or whatever and and at such a high pixel like pixel density like you know
plank scale pixel density or something like that just it sounds magical maybe it is i don't
but just imagine we had that sort of capability would we be able to measure the measuring
no okay categorically right because there's a there's a category mistake and how we're
thinking about the question and so what i want to do is i want to kind of reveal that because
no matter how perfect our measurement process would be like say for example you know through some
future advanced technology that somehow you know could could sort of factor out the quantum
noise associated with plank limits and all the particle interactions and was able to basically
see and measure every single neuron in the brain and the you know synaptic process going on throughout
the entire structure like we we in that sense we have a simulated brain right and in that particular
case you know a lot of people would would be interested in that because they'd be basically
saying things like wow look at all of the neural correlations between you know a subjective state
of consciousness and this pattern of of process occurring at a neurological level okay now you
know that's all great and there's already been a lot of work in this space identifying neural
correlates right but the question you're asking is does the measurement of neural correlates allow
me to perceive the perceiver and to perceive the perceiving and the answer to that question is actually
still no and the reason is because in effect you still only have third person information you
still only have a third person perspective you say there's a correlation from an outside point of
view I hear a correlation between what this person says about their experience and what I observe
as a neurological process right but out of that you're still not going to be able to construct
a first person sense of why the present is distinct from the past and the future a lot of people
who are working on what would be called the problem of consciousness like this is a famous debate
in philosophy like how do we how do we emerge consciousness which is essentially a first person
perspective from a third person inanimate matter and I'm going to basically just say well you can't
I mean that just doesn't work that way right like I can go from first person to third person
like I can take measurements and I can turn them into through a kind of abstraction process
a body of causal knowledge you know expressed in the language of mathematics
that basically suggests that you know we can predict these particular events given these
particular antecedents like we can say given these conditions we'll see these outcomes
but that abstraction process itself inherently is atemporal the third person perspective is
atemporal the hard problem of consciousness is not it doesn't have anything to do with the neural
correlates it has to do with the difference between how time is handled in a first person
perspective versus how it's handled in a third person perspective you can't derive a first person
locality in time from a third person perspective which is completely has no temporal
direction at all right the third person perspective is atemporal and at best I can treat time as a
dimension of space without any particular locus without any particular orientation
so in effect you know if I if I'm looking at this from a really really deep physics point of view
the causal methodology will not give me the ability to account for the notion of locality
in time and space and possibility which is the central notion that makes the notion of choice
meaningful so in this particular sense it won't matter how perfect your measurement apparatus is
for one thing it's never going to undo the heisenberg uncertainty principle and also it's not
ever going to really undo the notion of abstraction having to basically be atemporal and the fact of
consciousness having to be temporal and so in effect though the way in which you get from the
third person to the first person is of an entirely different kind of thinking entirely different
kind of process than anything that can be held purely within a third person perspective and in
order to understand that you need to understand the non-dual philosophy pretty thoroughly or the
non-dual metaphysics pretty thoroughly but I can at least say that in answer to your question
measurement itself as a process although it is grounded in the real
will not transcend the the the objective existence you can't use process to essentially overcome
process it's a brain measuring itself I mean like it's a we is conscious agents apparently
and I forget the name of the experiment there's been experiments done where
the observer the the person is only aware of a particular choice like a milliseconds later
there's a delay there so is it the the person being aware of the the brain's choice that's
important or is it the actual choice mechanism that the brain's making where the the person then
later becomes conscious of it so so so I know about that experiment but but but the thing that
we have to get past is the notion of choice being a process it's it's it's it's it's it when we
say mechanism of choice we're still assuming that causation is a way by which we can understand
choice and that's just not true so in effect you know if you're looking at the experiment where
they're they're basically saying okay we see these neural correlates and then there's you know
500 milliseconds later the awareness in the person consciously that such and such is happening in
the world and then there's like instinct or something and they respond right but the fact of
those antecedents preceding the moment at which they themselves recognize consciousness is is to
already presume an objective view of time an outside in perspective a notion of measurement
and and whatever the correlations are is created as a result of
sliding the window back and forth with which the measurement is occurring so for instance if I
basically say I'm going to trigger that you know I'll have recorded whatever happened previous to
this moment every time I hit a button for example then yeah I'm going to see antecedent process
prior to my hitting the button but the but the fact of taking the measurements the recordings that
the chart the eg chart for example and or the neural firing pattern chart for for example
and I line them up on the basis of a subjective interval right I've now created a correspondence
between all of those charts the fact that there is a there's a there's a series of neural correlates
that occur before and a series of neural correlates that occur after that are always lined up on that
same thing that's actually presupposing the thing you're trying to prove because you can't
you can't justify where the alignment itself came from right like the the fact of the person saying
hey I subjectively perceive now as now and I'm going to use this as a triggering mechanism to
create an alignment within the third person perspective and then I'm going to try to use
that third person perspective to explain what's happening in the first person perspective well
again that's just a way of trying to short circuit getting from a a temporal basis to a localized
temporal basis and again that's just begging the question it's not actually answering anything
the the experiments are interesting they are great for identifying yet more neural correlates
but it doesn't actually answer the underlying question so would you say the feeling of recognizing
a brain's conscious decision is a third person perspective no is it first person
it's certainly more akin to first person I would say technically it might be second person but
this is where things start getting really nuanced and we'd have to construct the language
as to what do we mean when we say feeling and be exact about that
yeah so when we recognize so when we become conscious that a brain has sort of come at a
certain state is after the brain has come at that certain state is that a yeah what kind of perspective
is that is that the language the language of brains and the language of states is already
baked into a third person perspective so in effect again I'm operating if I if I'm starting
within an existential frame I'm already using a causal methodology of thinking about it and so
in effect the trouble here is that we can't construct a notion of choice using only causation
as a basis any more than we can construct time out of timelessness
like how do I take a perfect symmetry and from that alone create a perfect asymmetry
how do I get asymmetry from symmetry in physics it shows up as the symmetry breaking question
and so in effect you know it's not like physics has solved this problem it's just basically said
hey there are places where there is an asymmetry in the universe and we see it right more matter
than antimatter certain things having to do with time charge and and things like that parity right
like so in effect there's a there's a there's a handful of places where we notice an asymmetry
in the universe as it is and there isn't really an accounting for where that asymmetry came from
it's not going to emerge out of the mathematics the mathematics is symmetric so in the same sort of
way that we don't have localization out of globalization we have to in a sense recognize
that we're not going to get choice out of causation we're not going to get consciousness out of
inanimate matter and so and that's not to say that consciousness isn't real but it is to say
that it doesn't exist in the way that we think of things existing well the processes exist
no process is real okay cool mechanics mechanical process if you're saying mechanical
then that exists but you know obviously it has to be something that is made out of matter and
energy and pattern that we can we can measure like if it if it's measurable then we can talk
about it as an existing thing measurement itself is a process do you so we can't measure processes
we can measure freight we can measure aspects of processes but only some aspects not all aspects
okay right like for instance built into the nature of reality is is is this thing we we have a
mathematical model called quantum mechanics but the action of observation has a a change like
while the evolution of the Schrodinger equation or the Dirac equation or whichever model of
quantum mechanics are using the the the evolution of the wave state is deterministic
and out of that determinism we basically construct a probability about what an observation might be
but when we make the observation something outside of quantum mechanics is occurring right it's called
the the measurement problem in quantum mechanics and then we basically say well what the heck is
that and there's all sorts of interpretations there's a quantum mechanical interpretation
called the Copenhagen interpretation and there's many worlds interpretation and man there's a
whole bunch right but all of them are different ways of reconciling the notion of measurement as
a process that is occurring in some sense in some definite sense outside of the evolution of the
wave function itself so it's a kind of meta process and that meta process is non-deterministic
in effect the Heisenberg uncertainty principle is not a result of measurement perturbation
meaning that there's some definite state of affairs it's basically saying that the notion of
there being a definite state of affairs doesn't make conceptual sense and that in effect there's a
there's a kind of indefinite built into the microscopic state of the world such that even if
I had perfect knowledge of all prior states of the entire universe that I wouldn't be able to predict
the next state of this entangled system like if I have two particles entangled with one another
that even perfected knowledge of the entire wave function and an existential state of the entire
universe prior to that moment still does not give me the ability to predict what happens in the
entanglement in the next moment so an old ancient Laplacian demon would not be able to make predictions
about the future that's correct so in effect there's a sense in which the notion of definiteness
at the you know sub-clinic scale is not a coherent concept and and you know there are
people that try to talk about things in the sense of super determinism but in effect that's that's
also begging the question because effectively super determinism is just an assertion of if we
assume that a third-person perspective of the universe is is completely consistent what would
we be able to predict under those circumstances and of course well if you're if you're assuming
perfect super determinism then of course you can make predictions but what we actually observe
doesn't isn't consistent with that and so in effect there's a there's a sense here that
hard randomness is a feature of reality itself and therefore reality is different in kind than
existence but that's not the only way to understand it it's just it's just one way to understand
that there's there's all sorts of ways to construct these concepts but the net effect ends up being
the same what of what you've said so far do you think is the most misunderstood by your peers
I think that so many people have gotten so used to the sheer power of causal thinking to solve
such a broad class of important problems that there's a supposition that it's a tool that's
able to solve all problems and so as a result I think people don't recognize the degree to which
that results in a kind of bias a kind of what I would call omniscient bias that because these
tools have worked so well for so long that they can work indefinitely forever for all problems
it's like you know something that works 99 percent of the time sure it leads to the expectation
that for that last percent that it would work great but that's just not true and so it leads
to a bias that well there's got to be a way to make it work for this last exception case like
it's worked so well before why can't I just extend that trend line well the problem is is that you
know the universe has exponential features built into it and using linear process to try to predict
exponential outcomes yeah it looks linear but it ain't as soon as you get to the knee in that curve
it goes off in a different direction and you've got to worry about that other asymptote and so in
effect there's a sense here in which I think people are so used to coming from a realism
perspective a perspective that third-person perspectives like mathematics itself is the be
all final intellectual tool that intellect itself can solve all problems and it's like well actually
to some degree you know I can see why that has been such a compelling argument and such a compelling
thing but honestly you know one tool isn't going to solve all problems and and and to believe that
it can is essentially to be making a mistake you know and and this shows up at so many layers
and so many things that yeah it does make this philosophy genuinely hard to understand like
even the questions you're asking me you know I'm noticing every single question that you've asked
presupposes uh objective reality or realism as it bases and so as a fact I have to unpack it a
little bit just to be able to give the answer that I'm giving. I guess um well around alignment
is there if are humans aligned humans don't seem to be aligned as well I mean there's
so many different perspectives out there so many different incommensurable points of view or you
know value systems in the world if we if we can't align AI can we align humans and what do we align
them around? We can't compel humans to be aligned with humans right that's just not going to happen
I mean you you can try but I don't think there's any force of government or any methodology of
government or methodology of technology that is ever going to create in this particular sense
an absence of diversity of opinions right I believe that that's a built-in right but I think that
there is a hidden alignment of humans that is really easy to overlook so cooperation and alignment
for example are largely invisible things and so in effect the places where humans are aligned with
other humans is again a sort of metabolic process alignment like I might not agree with you as far
as your philosophical point of view is concerned but you and I will probably agree that food is
edible and that what I can eat is at least to some approximation something you can eat now
obviously I can have say for example a intolerance to nuts because I just don't happen to have the
digestive enzyme but the fact of food is the way in which we create energy in our bodies
and the choices that I make about that will be very similar to choices that you make about that
that in effect our choices are aligned the methodology by which we think about and solve
problems around our metabolic processes are in effect going to be inherently similar because we
have a common biological heritage right our bodies work in similar ways there's in fact
you know it's not just that you know thinking about it from a genetic genotype and phenotype
point of view we have 99 percent of ourselves in common right like like you and I aren't that
different as far as species are concerned as far as you know things that we care about are concerned
like you know the kinds of things that would elicit laughter in me probably will elicit
laughter in you as well and the response of laughter is something that we can kind of
presuppose about one another that we wouldn't be able to presuppose about an artificial intelligence
and so in effect there's a kind of you know organic alignment occurring process that emerges for
any evolutionary system that has common basis in the ecosystem itself the mere fact that the
ecosystem is organic means that alignments are going to occur among the species if we had an
artificial it would be aligned with itself but they that alignment wouldn't apply to organic
ecosystems so then you have to look at ecosystem to ecosystem dynamics and start thinking about
what are the implications of the enthalpy of those interactions and you know in this particular case
we noticed that the alphabet of material process associated with organic ecosystems is you know
fairly uh fairly specific you know hydrogen nitrogen oxygen phosphorus sulfur things like
that whereas for the technological infrastructure man the alphabet of elementals that are involved
with that is vast and so the mere fact of using a larger alphabet means that there's a kind of
inherent elemental incompatibility between these two things like a lot of the stuff that's going on
in the technological ecosystem is essentially going to be inherently toxic
to the organic ecosystem because it's using elementals like arsenic for example
that the organic ecosystem just doesn't have a way of dealing with and so in effect there's a
there's a sense here that at every level of constitution in terms of materiality a molecular
structure and macroscopic structure you know reproduction all of the stuff about how energy
is used managed and metabolized and so on creates such a high level of incompatibility
between these two the alignment is just not occurring whereas for organic creatures alignment is
is basically going to happen by accident more or less everywhere because of the common ecosystem
that they're all a part of somewhat argue that humans aren't really that aligned with the rest
of the ecosystem in the modern world well again it depends on what level you're talking at like
at the level of you know how we essentially uh you know bulldoze over rain forests to harvest
oil and stuff like that yeah that's a kind of incompatibility but the human technological
artificial ecosystem is different than the natural ecosystem and you're right there's already
incompatibilities there but but in this particular case the abstract virtual world of
social process monetary process communication what we would consider to be culture and language
and story and narrative and identity and money and all of the rest of the stuff that we're propagating
around we're already building an artificial ecosystem that is virtual and abstract which is
driving a hardware based ecosystem which is itself different than the organic ecosystem and you're
right there is a misalignment there but at the level of we eat stuff that grows in the ground
and we eat stuff that eats stuff that grows in the ground we are still compatible with the
underlying ecosystem and to some extent we have to honor that or we just won't have food
so so there's a sense here of um a compelled compatibility that we aren't acknowledging
and that we're undermining by basically paving everything over now there's been a number of
critics to your theories people maybe there's maybe people just aren't comfortable that their
research is invalid or that you think that their research is invalid ar alignment research now do
you think it's still worthwhile for them to try this just in case um you're wrong about it or are
you confident that they're wrong so confident that it's not even worthwhile them actually doing the
research now well this is this is a this is a question that's fairly similar to the kind of
question that occurred in mathematics about a century ago which was can we create a single
unified body of mathematics and um you know should we do so and and there was a lot of people that
really wanted that i mean it would be profoundly intellectually satisfying it would be uh you
know coherency building it would give us some really wonderful tools uh and and so on um and
unfortunately uh this this mathematician uh kurt codel um basically came up with the proof that
mathematics can't be consistent and complete at the same time you can either either aim for completeness
which was the dream but inherently there will be inconsistencies which is kind of anti mathematics
or you can keep consistency and admit or just let go of the idea that there could be any complete
understanding of all the things that are true uh in a mathematical way and you know so so the
question then became not should we continue to try on the basis of whether uh codel might be wrong
with his proof but check the proof like like the real thing was you know the mathematicians at the
time they didn't like it when when codel came i mean it was controversial among mathematicians
for a good while and you know there was a sense though which is well it doesn't make sense for us
to try to do an impossible thing if it's actually impossible and we can know that so then the question
becomes can we know that is it actually the case that it's impossible or is the proof wrong if the
proof is wrong then for sure definitely continue to try to do the thing that's hard to do but if
it's actually impossible then for sure we would want to know that so that we're not wasting our time
trying to do something that isn't merely hard but is actually impossible now in this particular case
first of all just for the record i have huge compassion for people that have invested their
lives for decades now many of them to genuinely try to do something that is truly hard right like
from that perspective like eliezer is basically saying hey this is a hard problem but we need
to work on this because this matters and and and so he basically was you know very encouraging
it's like well we didn't believe we could make airplanes and yet we did and we didn't believe
we could do this and yet we did and technology has overcome all sorts of problems that that at the time
we thought were out of reach but at the same time there are things that we've learned that you just
can't do like you're not just going to solve the cap problem with technology you're not going to
you know the consistency and partitioning that that whole thing computer scientists know about
this you're not going to solve the halting problem you're not going to solve you know something that
basically would suggest that you could you know pricek the angle using a compass and a straight
edge it's like at a certain point you have to recognize that there are ways of thinking about
this that are clear in a way that's that's consistent with a sort of mathematical perspective
that gives you definite information about the difference between
hard and impossible or known unknown and unknowable the unknowable is just as much a class as the
unknown that could potentially be converted to the knowable right like science basically believes
that everything that is unknown can become knowable but what the metaphysics shows is that
there are genuinely things that are unknowable that are inherently unknowable that hard randomness
is real and that in effect what happens is is that when we when we start to get sophisticated about
distinguishing between the merely unknown the merely hard and the actually impossible the
actually unknowable that that therefore we can narrow our efforts to the kinds of things that
that really do make sense to do but of course that choice to focus our efforts to narrow our
efforts in that particular way is contingent upon our genuinely understanding and being
confident not just me I mean you know who the fuck cares what my opinion is what really matters is
is have the observations that I've made have the path of thinking that I have
reversed you know and however I got to thinking about that particular thing it doesn't matter the
path is now something that we can put blazes on and we can we can see okay with these observations
and this math we can arrive at this conclusion and with this we can arrive at this conclusion
and we notice that we get from here to here and and it's the coherency of that path that
matters in this particular case and yeah it may be tremendously distressing for people
for people who are basically on one hand believing that technology is inevitable
and therefore we need to make technology work well for life because if we don't no one else will
obviously the corporations don't care about that they care about profits and so in effect
there's a sense in which you know they're not going to try to solve the two masters problems
because frankly they have a fiduciary responsibility they're shareholders and
United States law is basically saying you have to do what's profitable because that's what the
charter says and you can't disagree with the charter because that's illegal right no so so in
effect you know many people have tried to step outside of the corporate model and say we're
going to try to create a compelling way to create alignment and then we're going to try to basically
somehow have the government force the corporations to build things that are in alignment of course
and we're going to hope that that's robust enough that there are no exceptions that emerge to kill
us all right and you know that's a really roundabout way of going about it and if I come along and I
basically say yeah there's no hope of creating alignment in a technological setting you can only
hope to have alignment emerge in an organic setting so we need to get better at doing the
organic thing where this artificial agency is just going to kick us off the planet and there's a sense
here in which you know knowing that basically says okay we need to change our strategy we need to
maybe instead let go of the notion of technology being a solution to all problems
recognize where technology is good and useful and beneficial and where it is definitely harmful
and learn the difference between hype of well we think this might be awesome versus well this
could actually be terrible to knowing for sure yes this stuff will be actually terrible but this
other stuff's okay and to be really really good about discerning that boundary and then maybe we
can create some sort of agreement that yeah we all love our children nobody wants to kill everybody
else maybe it's a good idea for us to be really discerning about the difference between what works
and what's terrible and in this particular case I think that investing effort in that is worthwhile
and you know nobody wants to build a doomsday machine like that just doesn't make any sense
it's mutually assured destruction it's it's a known theory of game theory game theory itself
in this particular case makes it really obvious that no we don't want to be stupid but in that
case it comes back to the thing I was talking about before we are literally the dumbest species
possible to have technology and we need to fix that with better choices we need to get
more sophisticated about understanding impossibility paths relevance paths meaningfulness sacredness
life what matters what doesn't and and how do we make better choices around technology because
frankly we have to that is the dilemma of our age we need to balance the relationship between
nature humanity and technology and have technology be in right service to the truths of nature so
that we can endure in the long term and if we don't do that it's over
this proof as far as I haven't been able to find the the proof in a mathematical form I've found
some writing if do you think it's possible to put this in mathematical terms as a mathematical
there's been some specific work so in other words the way at which I work internally
and the kinds of ways I construct concepts and stuff like that has the same level of rigor
as mathematics but a lot of people are looking for how do we substantiate the inequality which is
the fundamental inequality of the proof and to and to do that we basically have some specific
tools so one of the ways in which we can kind of consider this is to is to think about and model
the capacities of control theory like like if we're going to write a compel alignment we can
designate a little bit about what we mean about alignment in a very very general sense
we can talk about control theory in a general sense and we can basically show
that there is a way to characterize what level of control we need for alignment
and what level of control would be possible maximally possible and we can do this at various
levels of abstraction right and we can show that once a certain threshold of abstraction is
crossed that no version of control theory can create the level of control necessary
for the minimum level of control required in order to to create alignment right and so
in this particular sense you can think about it in that particular way and there's a way to construct
that another way that this can be done to create a kind of way of thinking about it is is you can
look at like what happens at the interfaces so one way that I've modeled this for a colleague
Anders Sandberg and he and I have been in discussions around you know enabling him to
write a paper about this essentially which is wow yes I've had him plenty of times on my channel
in fact at the end of this or the end of next month we're going to have a dialogue between him
and David Pierce and it's wonderful he's great yeah I love people in the world he's an awesome
person he's he's I've had a chance to meet with him several times and and I've all I've enjoyed
the conversations although they're always very challenging because he's quite smart and so
in a sense there's a there's a sense in which we have at this particular point through a through
a number of meetings have constructed a kind of rudimentary language where we're now able
to start to formalize that and and so the way it looks is you know and I'm just basically creating
terminology for this so for lack of a better word I called the entire physical natural
universe Z the artificial you know agent or ecosystem of agents or just the totality of that
technology base X and the control algorithm or the the thing that would make it so that alignment
happened to Q right and then again for simplicity's sake I said okay there's that each of these
processes Q X and Z that there's a flow of information that goes from Q to X from X to Z
and from Z back to Q and of course we can create all other paths of of how information flows by
basically saying well if I want to get information from Z back to X I could just have it flow straight
inputs to outputs with no modification through Q so that's isomorphic with the routing of
information from Z back to X right so Z being the natural world the totality of all physical
process so it takes inputs which are the outputs of the artificial system and it process those in
some way and it produces some change in ecosystem which the you know again if we're trying to control
evolution like if this is the model like can can we create a control algorithm which would constrain
the artificial process from not destroying the Z process the organic aspects of the Z process
which obviously also includes the physical universe and all of its lawfulness so in effect
you know Z can route back to Z you know either through itself or you know through a path through
of Q and X X can talk to Q by pass through Z back to Q and so we can connect everything to
everything through paths through plus I can have Q modulate everything that's an output of X by
basically having a path that goes to the far side of X and then captures everything that would have
been the output of X and act as a modulation of that so in effect we can set up now a mathematical
model that talks about the dynamics of the information flows through the system okay this is
the beginning right so so now I have these three domains I can't control what happens in the Z domain
ultimately the laws of physics are just the way they are and that's just that's that is outside of
of anything that we can presume to have any real effect or influence on I mean obviously I can
affect the universe but I'm not affecting the way the universe works you can't change the laws of
physics so the design of Z is immutable the design of Q is essentially
mutable on the basis of the best thing we can conceive of possibly creating right so we're
going to treat Q as being mutable up until the point of we've figured out a way to solve the problem
of alignment okay now what about the design of X now at first we can specify the design of X because
we built the artificial intelligence right but evolution would imply that X begins to change
over time in response to Z and maybe even in response to Q or maybe in response to how Q Z and X
all interact but in one fashion or another if X is a learning machine and we can't control what it
learns because whatever's going on in Q is not controllable by us right so in other words
X may learn things about Z that we would never anticipate that it could learn it might learn
laws of physics we just don't know okay and whether it learns it explicitly or implicitly
it's going to change the nature of X the design of X is now changed because the software process
of its learning can affect what the hardware does because you can have you know causation from the
software level into the hardware level and I can have causation from the hardware level up to the
software level that's what it means to basically be a learning system in the first place and so in
effect I can't ignore the learning and I can't ignore the substrate and I don't know what's data
and I don't know what's code so as a result over time while I may have specified the data and the
code as the initial conditions the design of X starts changing over time and I can no longer
anticipate what the design of X might become in the future okay the only thing I have control of
therefore in the long term is Q and if we're talking about alignment then that basically means
the Q has to account for all possible level future evolutionary states of X
all right so having basically said something about the mutability conditions we can now
start talking about the interface conditions so let's think about as a metaphor okay
um I have like a dartboard right and I've got a person that's throwing darts at the dartboard
okay and for the sake of simplicity let's describe that the dartboard is really really simple
it is essentially the entire wall and let's say the wall is infinite at extent okay and I'm standing
on a platform in front of this wall and there's space some distance from and I just have this
vertical wall it's infinite in all directions and has a vertical line down the middle of it and the
entire left hand side is painted black and the entire white right hand side is painted white
and I win if I throw a dart and I hit the white
right now given that scenario it's pretty likely that pretty much any dart player would be able to win the game
right what if I were to make it a little more complicated by instead of
but even if they're blind you know you can have the guy behind them see and kind of slide him over
to the right a little bit and say just aim vaguely right and just throw the dart and if the dart hits
the wall at all you're going to be okay right you know it's so in effect and that's not breaking the
rules right I mean it's like whatever process you have to throw the dart use it right because that's
the design of Q right in this particular case so so in this particular sense let's just basically
say that the white region represents expressions made by x that don't destroy organic life in z
so now we can basically say how do we represent the fragility of organic life in z what does
that look like okay so now I'm basically talking about the starboard and say well I've just made
the game really really easy well let's let's make it a little harder let's basically say
we've made it a checkerboard and the white squares and the black squares are now mixed
together but the checkerboard is basically you know a square a foot on a side and my platform
is like 20 feet away okay so you know is my aim good enough to aim at a particular white square
and hit it if it's a foot square on a side yeah it's probably something that many players would
be able to do obviously you'd have to play darts a little while but but in theory that wouldn't be
a hard game to win either now the key thing about this is is that we have preserved the area of the
white side and the area of the black side okay and we've just changed the layout of where the white
is relative to the black okay now the thing that defines whether or not you win or lose
or in this particular case whether life endures or dozens is partially the amount of white that
is on the screen or on the target relative to the amount of black or relative to the overall area
right like if I if I had an entirely black surface and just a single white square
then I'd better hope that my platform is relatively in the right place because if it's
40 feet down the wall in the wrong direction I'm going to have to throw that dart really hard in a
really specific direction to hit that white square right and if the white square gets smaller
smaller relative to the size of the wall the probability of my hitting the white square goes down
but that's not the only way to affect the probability of hitting white
say I start with a checkerboard that is again infinite in extent
and I make the squares one millimeter aside
okay so from 20 feet away that's that wall is going to look gray because it's an equal mix
of black and white but if I get close I notice that there are distinct white squares and distinct
black squares they're just mixed together really well and I throw the dart okay even a really good
professional dart player is not going to be able to guarantee that they hit a white square more than
about 50% of the time even though the area of the system is the same changed okay so in effect
the amount of control that I need to have over the dart
to create a winning outcome depends both on the ratio of the white area relative to the black
and the coastline between where the white is versus where the black is
if I have a relative to where you're standing as the person throwing the dart
but say for example my control allows me to position the platform wherever I want
if I have a even mix of like say atom-sized squares
that I don't care whether you're using a laser you're still not going to get that kind of precision
right so it's not just a question of accuracy in the sense of
can I aim in a genuinely white direction it's what level of control do I need to have when the
expression hits the target the dart is an expression it's a choice made and when it goes from the x
roam into the z roam is it the case that z is specific to the positioning of that expression
the particulars of that expression such that it's fragile to that or is not
right now biological life is robust over a wide range of conditions but it's very fragile with
respect to some very specific things that you wouldn't necessarily anticipate in advance
one example of this that I give is you know mad cow disease so somewhere in the brain of some
creature some protein got misfolded and that misfolding is now basically causing other proteins
near to it to be misfolded and that that spreads and there's no biological process that that
creature has to fix the mistake and the problem is is that if anything eats the creature that
had the misfolded protein it ends up with the misfolded protein and then it becomes a carrier
of that because the protein causes other proteins and so this isn't a virus it's not a bacteria it's
not an anoeba it's not some sort of medic disease it is a protein folding mistake but there's no
biological process to handle that there's nothing that evolution has developed yet to fix this
or to adapt to it now where did that first misfolding protein come from why I have no idea
it could have been a cosmic ray or some radiation it could have been some technological process to
generated some enzyme or some hormone or some side effect that resulted in an overproduction
of something in the body that resulted in this full protein getting misfolded for some bizarre
reason that never happened in all of history but the chances that it was something technological
that had this side effect unfortunately is better than even odds right because an awful
lot of the universe time like evolution has been occurring on this planet for like a billion years
and this looks like something recent because if it had been something that had happened in the last
million years or so or hundred million years or so an evolutionary process to handle this would
have already been developed so it's pretty likely that it's the technological origin of some kind
or another so some expression hit a black square and now this is essentially an issue okay we don't
know how to fix this right at this particular point the best we can do is eradication if we
can detect it measuring this is hard so at this point does that problem deserve working on yes
are there other kinds of problems like this that we have not yet encountered yeah lots
right this is an example of one protein out of millions in each organism every creature on
earth it's a mammal or a reptile or lizard has the play of literally millions to billions of
proteins and chemicals with really complicated structures all interacting with one another
and all you got to do is fuck it up in one place and you end up outside of what evolution has learned
to deal with because it's literally that complex and evolution can only deal with so much and so in
effect yeah over time it'll figure out a way to route around this problem but if it takes
five thousand years to solve this it's only going to fix it for one or two species
and the rest of life is going to basically be you're going to have no more mammals right
so so in effect how sensitive is life to these kinds of technological issues actually near as I
can see very sensitive right when you look at the total complexity of of just even a single cell
it is more complicated than probably the most complicated microchip currently made
right it takes a an entire floor of a gymnasium to document in in a really really small print
just what's going on with a single microprocessor you carry around in your pocket whereas if you're
just diagramming the digestive chemistry and the all of the molecular interactions is probably
going to require a bigger diagram than a than a stadium floor can cover that's just you know again
this is biochemistry talking to modeling theory but you know somewhere along the way we notice
that when we look at the whole ecosystem it's way more vast than that like we're talking like
you know 10 to the 33 complexity if you're wanting to implement it in software
down to the quantum level there's no fucking way right so in effect you know evolution's bandwidth
of processing solutions these particular things is way lower than the amount of embodied complexity
in the design and in effect what you're what you're therefore saying is is that evolution's
really good at figuring out specific solutions to general problems but the robustness of that
is really slow and really low for the kinds of things that technology does so in effect
what we're seeing is is that if I model this as a target I have two problems the amount of white
area yeah it might not be small but it's you know a tiny fraction of a tiny fraction of a percent
at best but the distribution of the spots that are white versus the ones that are black i.e. the
things that are benign versus the things that over a century have some toxic effect that for which
nature has no amelioration well the interface of that thing is is is way complicated and so the
level of control that you would need to have to digitally select for every expression whether
or not it's going to hit a white spot i.e. is aligned to life to organic life on the interface
between X and Z is it's phenomenally bad now Anders and I are for the most part not having
that conversation because for the to some degree we have slightly different priors around some of
this and we can negotiate that but that seems less interesting to us than looking at what happens in
X and what happens in Q relative to X that could for any values of like unless the target is painted
completely white at that point the level of control sensitivity you need is effectively zero
but we know that's not the case we know that it's some fraction of the wall that's painted white
and we do know that the coastline between the white regions and the black regions is some
enormously detailed fractal and some huge number of dimensions i don't think that he and i would
disagree on that particular point although i know him he will quibble and quibble and quibble
about every level of detail and try to unpack everything i don't really care the fact is
that it's a definite value less than 50 percent and the coastline is at least more complicated
than some definite number which i won't name but is higher than then is needed at this moment to
basically set up for the next part of the proof okay so so then we basically say okay if i have
some digital signal like i take the the the space of that wall and i basically say well
what is the permutation complexity of the all of the chemistry that flows across the wall and all of
the all of the possible magnetic energy expressions and all of the possible you know so we can look
at it from an atomic level we can look at it at an energy level and we can look at patterns
overlaid over energy and an atomic atomicity we could basically say for all of the information
all of the atomic and all of the energy flow from x to z
if we characterize that as a digital signal i e it either happened or it didn't it either
was an expression or it was not and whatever the bandwidth of that digital signal is therefore
a kind of expression that is now a mapping like i could take whatever that bit stream is and i
can treat it as a mapping into space of a dart hitting a target right i can take whatever
the dimensionality is of however i represent the black and white areas as go versus no go
and the digital signal has essentially a selectivity of a position of no versus go no i've now described
a degree of coherence that i need to have on that digital signal expression every expression
that happens so if the bandwidth from x to z is you know one terabit per second then i now
have characterized what level of coherency of precision of the data content of that of each
expression from x to z what what level of control do i need to have over the signal from x to z
okay notice the kind of numbers i'm talking about here right like really huge numbers
of the number of squares that are black tiny fraction of them that are white
are there any gray areas what that that if you either would would you say that it would be worth
saying that there there might be gray areas where if you hit them with the dice oh sorry that the dice
the um the the arrow or the dart that wouldn't be existentially catastrophic but it would be bad
it could be bad but varying degrees of bad sure but here's the thing if we're talking
about alignment and we want to simplify the argument as as much as possible we can you know
over time elevate the the gray areas to either black or white depending upon what subsequent
expressions are and then we can basically just say let's just take a single moment of expression
and just repeat that over and over again and just see what happens over the long term and that's
going to tell us whether alignment was adhered or not right so in other words just extrapolate
you know anywhere that we could basically talk about a gray area extrapolate over the next 100
years what happens so so in effect i still want to kind of say the control algorithm wants to make
a decision as to whether or not to intervene to constrain the inputs to x so as to constrain
the outputs of x so that it doesn't damage z so i'm characterizing it as black and white on z
so that at least it's tractable to do the math in x and q right because that's how we set the
proof up we take a simple case and we show that all extrapolations of that simple case reduced to
that simple case and that that simple case has a clear unambiguous proof around it and once we show
that proof it's qed so what i'm doing is i'm setting up the inequality i'm basically saying
whatever the level of white versus black on the wall and whatever level of coastline that it has
right is it a scattering of tiny points or are there a few regions that we can aim at
that just anything in this cluster is okay right but either way the amount of control that i need
to have on the bitstream is going to be pretty high that which is obvious okay so then we can look at
what is the level of control that i need to have on the bitstream going from q to x
such that the output of x has at least that much control or that i the q can control the
output of x to at least the degree needed for x not to destroy z i don't get to change how z
receives this so the distribution of of white and black and where those points are and so and so
forth that's fixed that's laws of nature laws of biology the world as it is that that distribution
is whatever it is i don't get to change whatever happens downstream i can already see that and as
would say z could be made more robust through augmentation he's a post humanist right he'll
say that well nature is what it is but we can improve on that and we can try and make it like
error correcting more fault tolerance able to withstand the damage that could be done today
maybe in a thousand years could be dealt with well i imagine he might face one of that here's
the question what part do we are we begging the question as to what alignment means if alignment
means compatible with organic life but now i change the meaning of organic life to be artificial life
then i'm just i'm cheating because i've made organic life artificial life and then i said
well it's compatible artificial is compatible with artificial well that's not proving anything
interest right so in effect here we're basically saying can we make it compatible for humans
as humans are and you know again it's it's basically like is there is technology in the
long run compatible with biology and yeah there's some ways in which we can make compatibility but
there's a range in which we do that which means we need control over the kinds of things we can control
given that we can't control everything like we can't control the laws of physics and to some extent
organic biology just is the way it is right i'm not going to change cellular chemistry for every
human being on earth anymore than i can change the opinions the political opinions or the values
of every human being on earth right there's a certain amount of diversity built in and there's
a certain amount of robustness built in and maybe i can improve that but i'm not going to be able to
improve it beyond a certain point and whatever i'm doing to improve it is essentially still a signal
flowing from either q to z through x without modification or a signal sent from q
to x that itself is a signal to z that increases the robustness of z but that makes it the same
example that we were just talking about is that signal in the white area and the white area in
this particular case includes a beneficial effect of making z more robust but still it's a signal
flowing into z right there's something i'm doing to z so even the action of a superintelligence trying
to make z more robust is still a signal that has to actually work which has to work for the conditions
that z is in to create the better conditions that z could be in and no matter how i model it's still
an extrapolation of exactly the same underlying form the same simple model still describes the
full characterization of all variations of the argument that that your anders could put forward
so i can encapsulate all variations of what was just described as essentially a a a quibble
as being a special case of the signal flow from x to z right you see it okay okay well up you
i will digest this further i don't know if i have the the perfect response to that that anders might
is a lot smarter than anders has a wonderful capacity to quibble to the end of time i have
never found the limits of his capacity to quibble but i think that you know to some extent you know
a reasonable person will eventually basically say okay i'm going to park these quibbles for now
because i want to understand how the proof is set up right because again we can extrapolate the
proof in all sorts of directions to handle quibbles as as they'd be relevant but if we if we bring the
quibbles up too quickly and this is something he does a lot is he gets so caught up in the quibbles
that we lose the threat of the argument that sets up the proof so i'm still in the process of just
setting up the geometry of the proof but we're more than we're more than three quarters of the
way through by the way who so so like yes that that'd be amazing if you could get the mathematical
proof up and running and that would i i think that would open the doors to a lot of a lot of
different types of people who may dismiss your arguments understanding them but this is it so
this is this is the thing andres at this particular point has studied enough of this
that he's he's closer to the point that he could write that out and i would prefer that he do it
because he's going to do it in a way that other people are going to be able to relate to because
he's coming from assumptions that they're making much more easily than the kinds of assumptions
i'm making bear in mind i'm coming from a set of of you know presuppositions about how the
universe works and things like that that are based upon the metaphysics which although compatible
with the way science thinks about the world is just still different and it's so in effect there's a
sense in which it's going to be better if i convey this to people such as yourself and such as andres
to the point that you don't have to rely on me you can do the work yourself knowing how to do it
so like i said we're three quarters of the way through that um and that's just one way to set
up the proof by the way there's there's several parallel ways of setting up the proof all again
all of which arrive at the same conclusion so it's kind of reassuring that no matter how you prove
it you end up with the same result it's nice to know for like the Pythagorean theorem that
there's a variety of different ways that all prove the Pythagorean theorem and if you don't
understand one proof you're likely to understand another and some of them are quite easy to understand
but but in this particular case the one that i'm working on with andres specifically
is set up with this model of q being the control system x being the artificial intelligence super
intelligent system and z being all the rest of the universe including all biological life as a
component right and what we're basically suggesting here is that it's possible to characterize the
degree of control necessary on the x to z boundary and it's possible to characterize all of the
relevant phenomena as a flow of information from x to z z to q and q to x remember the first part
of it was how i could show that i could wrap up all information flows as a single cycle of flow
from q to x to z to q to x to z and that's the loop right so i've basically taken all possible
loops of process and reduced them to this one loop of process and showed that all loops of process
can be subsumed by this one loop of process therefore the mathematics and the proof is
subsumptive of all possible modeling dynamics right because modeling dynamics are always going
to be a loop of some sort or another right and all of computer science all of algorithms are the
feedback loop all causations of feedback loop so by describing the dynamics of loop causation
directly i effectively subsume all quibbles of this space because i can reduce them all
to instances of special cases of this underlying model the dynamics of which are fully described
so it's a bit like an extension of shannon's information theory right it's like i'm thinking
about it at that sort of level and once you've got information theory a whole bunch of computer
science open up right it's like you can understand all sorts of things in computer science once you
understand you know the notion of entropy and how much information is represented by a given
number of bits and so on so you know and after that it doesn't matter what base number system
you're using or so on the notion of bandwidth becomes a coherent notion right so in this
particular sense what i'm basically saying is is that the notion of control can be reified with
respect to a kind of model of an area that represents all possible signals
some distinguishing notion between signals that i want versus signals that i don't want
and the proportion of the signals that i want relative to all possible signals
and and this is the part that almost everybody overlooks the distribution of
wanted signals amongst unwanted signals and then i can talk about the bandwidth of the flow
from x to z as characterized by this control dynamic like the requirement of control so in
other words now treating the bit stream the the the the pulses or the packets that are flowing
because again i can quantize i can discretize the system right and it's necessary to do that
in order for the notion of information theory to be coherent and oh by the way because we're
talking about artificial intelligence it's already quantized so i have to use a quantized model
if i'm really going to characterize this in a intellectually coherent way right i can always
you know do the conversion from a summation to an integral but if i don't understand what
that process even means on a summation level i'm certainly not going to understand what it means
on an integral level right so i'm going to stick with discrete math for now okay so in effect
there's a sense in which i have the totality of all possible signals a fraction of those
signals which are coherent and a distribution of the good signals versus the non-signals
and that now characterizes the degree of control that i need to have over the specific bits in the
bit stream in some sense now there's all sorts of ways of mapping one hot two binary sequences
and vice versa and it turns out that the isomorphisms of those kinds of things are really well understood
i'm not going to prove that now i'm just going to say that there is a strict isomorphism between
the structure of the bit stream and this one hot model of a dart thrown at a board
okay so now we can look at the bit stream itself and we now have a notion of minimum
level of control required on that bit stream that's now a characterized notion in a fully
mathematical way there's no ambiguity about this at all it's fully quantized so then we can look at
given how much control at a minimum we must have on the x to z interface so based upon
the dynamics of z we've come up with this area notion and this mixing notion and now we have
a bit stream that is characterized by that proportionality and mixing notion so therefore
we know what is the minimum level of control that x must have on the output bit stream that x is
expressing into z okay now here's where minimal minimum yep right so so it does seem as though
you're saying that there is like you know a point at which there is a minimal level of control needed
right yeah so there's a range above that that we can get better and better
beyond the minimum well there's a hypothesis that we could reach that level of minimum control
necessary and for some systems yeah that's totally tractable like if the if the area is
relatively large and the mixing is relatively low the relative amount of control that we need is
relatively low and for all sorts of systems we can create that level of control no problem right
so we don't need perfect control in order to to solve no but we need good enough control yes
that's that was another question I was wanting to ask actually yeah yeah we need good enough
control but now we've characterized in a mathematical way what is good enough
and anything less than good enough is not good enough i.e not in alignment
right we've we've we've subsumed the notion of alignment into a metric that has to do with
area of acceptable signal versus the totality of all possible signal
and the mixing of the good signals versus the totality of all possible signals right
let's just say we we do achieve an artificial general intelligence or a smart not quite super
intelligence but a particularly robust intelligence that allows us to aim where we have a good enough
signal level we have our solution is good enough that it has a probability of not destroying us
at 70.5% probability of not destroying us okay that's just the number I pulled out of nowhere
and then at at the next point the AI develops further and it becomes better and and so the
likelihood of destruction at the next stage is the likelihood of survival I hear you're making
but bear in mind that the nature of the proof is to disprove that that hypothesis is reasonable
right so I mean you know again you can't smuggle in the thing you're trying to prove
if you're trying to say it's possible to make a safe system then I'm basically saying well
show that it's possible to make a safe system but in the meanwhile I'm showing that it's not
possible to make a safe system so whatever it is that you propose has to account for
the proof I'm setting up currently you could say you know so you don't think like increasingly
safe systems are an option so like we may not be able to create the the most robust perfectly
safe system at point a but we could have a an okay system at point a and then at point b have a
better system all the way through to z where we have maybe not even a perfect system but a very
good assumption it turns out not to be valid in this particular scenario so in other words the
assumption that you're making assumes a kind of analog aspect to the process but remember I'm
stipulating that it's a digital process so for instance if I have a a wall and there's a stud
behind it I could knock on the wall and I can tell from the sound where the stud is because there's
a kind of gradual difference in the sound that the closer I get to where the stud is the more
the sound changes a certain way so I can converge you know there's like a hilly landscape and I can
do hill climbing algorithm to create an organization that converges on a local maxima I can I can use
analog process because everywhere I could calculate the the gradient and algorithmic processes that
can detect a gradient can use that gradient to effectively converge on a solution but here's the
thing to assume a gradient is to assume some sort of analog or floating point number with however
many decimal places are needed but if I have a combination lock and that combination lock is a
digital lock I might be it doesn't matter the distance between the combination I try and the
right combination there's no signal there's no feedback that I get that lets me know that I'm
either near or far away from the right combination the only time the lock opens is then if I get the
exact right combination the one time I try it and I could try a billion wrong combinations and I will
get no new information as to what the right combination is so in this case all the times
that we're talking about convergent processes based upon hill climbing algorithms we are
fundamentally assuming that the substrate is analog but a digital artificial intelligence
I'm sorry that is not an analog process this is a digital process more over than that we're talking
about signals and if I'm basically trying to construct a proof then in effect I need some
notion by which I can reify the concept of control and as a way of setting that up I'm setting it up
on a digital basis and at some future point we can make the transition from a digital basis to an
analog one but if we don't understand the dynamics of the digital basis we are going to continually
mislead ourselves to believe based upon an analog understanding that a certain digital process will
work a certain way but it doesn't the digital world is just different than any analog world
and the proof works in the digital world so I've got to set it up there right because somewhere
along the way we're talking about things like the rice theorem and why the rice theorem shows that
this particular idea of alignment can't possibly work but that's an entirely different proof
methodology but again somewhere in the nature of a proof we're talking about discrete ideas
explicit equations definite notions and definitions with exact meanings with no ambiguity as to what
it is that we're talking about and unless I set it up like that I don't get to a hundred percent
knowable because it's not a probability it's a discrete binary zero or a hundred percent zero
or one right for the hundred percent is exactly one so it's either perfectly true perfectly false
or completely unknown right and what I'm trying to do is I'm trying to convert the state to the
definite state of known true so again quantized so in this sense you know in this particular
sense what we're looking at is is that I'm not going to have the artificial intelligence system
be able to predict in advance everything about z because there are some elements of z which are
unknowable there are some things that no matter how perfect you could you could basically say x has
perfect infinite intelligence and that's basically the same thing as I was saying earlier
you can know a hundred percent of everything about the prior causal state of the universe but
it's not going to allow you to predict the next bit of information in two entangled particles as
a communication channel right there's a hard randomness built in and intelligence no matter
how much it is it's not going to fix that because it's a category error yeah okay so so can can
can it can an artificial intelligence then like a super intelligence like a make predictions about
what is predictable and make assumptions about what's not predictable and constrain the the impact
of the unpredictable no because that would be trying to define the structure of z and we told z was
already defined right so in other words I'm going to basically say well I can have a pretty good
idea about what's predictable and unpredictable but is it the case that that level of control
over my signal is going to constrain that the signal is only in the space of the predictable
phenomenon turns out I can't even do that because again I have to presume that the level of control
that I have is greater than the minimum level of control necessary and if the boundary between
that which is predictable and unpredictable in the same way if we say predictable is the white zone
and unpredictable is the black zone then it isn't just the area of the white versus the area of the
black in total it is the mixing of the the the white and the black and it turns out that there is
again complex boundaries between the two of those some of which is resolved down to what is
effectively an infinite level of detail right it's like a fractal coastline you know if you're if
you're throwing a dart out of the Mandelbrot set well if you throw towards the middle of it
you're probably going to hit the Mandelbrot set but if you're anywhere near the periphery
you know nobody's going to take bets on that it's going to even odds at best I mean you know you
can kind of say if I miss the Mandelbrot set altogether if I'm like way out there at like
you know negative five or something you know yeah clearly I'm going to miss the set but there's a
region between the two of them which is fantastically complex and so in effect what you're what you're
basically noticing here is that there is a very high level of control necessary on that boundary
and that boundary may be so complex that there's no possibility of control if that boundary is mixed
too tightly and so in effect we now are starting to set up an inequality because I can now characterize
the minimum level of control necessary can be very conservative and assume that all sorts of
things are predictable even when you know it's not a complicated system z is not a complicated
system it is a complex system which means that there are degrees of freedom of motion in z which
again are unknowable in advance and biological systems tend to be more in the complex region
rather than in the complicated region and so as a result what happens is that there's this
there's this lots of areas where there's boundaries so yeah for some kinds of signals
probably going to be okay those that prosthetic area narrow artificial intelligence using
technology to build houses and things yeah it's relatively benign it's not perfectly benign
but it's constrainable to things which are probably going to work out and most of the people that are
arguing about this again thinking from an analog perspective are saying well let's just avoid the
whole regions where where we think the control is difficult we're just going to stick with the
stuff that's good well the problem is is that I don't know what this thing x is going to learn
over time and how do I constrain x to move from a zone which is you know known safe and eventually
going to move into a zone that is unknown safe that means I have to know where the boundary is
but the thing is that I know where the boundary is nobody knows where the boundary is yeah right
so so it seems as though like um an artificial system would it be able to make predictions about
itself moving into an um an unpredictable region no because because again somewhere along the way
things like um like like if I basically have a machine like let's take a really simple machine
for x it's just a Turing machine okay it's not an artificial intelligence all it's literally
just a basic Turing machine the problem is is that somewhere along the way I have to distinguish
between data and code and the problem is is that if it's a learning system I'm not going to know
which is the data and which is the code in fact for most Turing machines if I don't know the
structure of the Turing machine and I'm just putting inputs into it I don't know whether
those inputs are going to be treated as new code or new data or both right and so as a result
predicting the output of the Turing machine given a unknown Turing machine given an input of data
well I can maybe predict it to some extent but it's probably going to produce behaviors
that are completely outside of the domain of what I can predict and so in effect there's a sense
here in which if I'm looking at something way more complex like an artificial intelligence
or superintelligence I don't know how it's going to interpret the data that it receives from Q
so in effect I can try to constrain what outputs go from x to z using Q as a modulator but now Q
has to be able to predict what's going on in z but Q was designed by human beings and so in effect
it's like okay we're not smart enough to know where the boundary is and we don't even know that
x if it was super intelligent couldn't create some signal that would fake out Q to deceive Q
into believing that x should be given more freedom to affect z now alignment is off the
picture again and again you know this design Q like through some form of indirect um specification
can't can't AI help us make Q more well okay so here's the thing somewhere along the way right
what we're trying to do is is to still say is there a way to converge some version of Q
so let's basically say the Q is built into x is there some way that a superintelligence can
account for all of the possible effects of its own expressions to z right so again on the x to z
boundary I still have a minimum level of control necessary condition see again it backers out
now we haven't even set up the critical part of the proof yet all we're doing is we're basically
saying that every version of this we somewhere along the way have to have a way of characterizing
the minimum level of control necessary on the z I'm sorry x to z boundary right everything is
going to reduce to that and then we've got to basically say can x or can Q and x together
create that level of control on the x to z boundary that's the main question right now
let's basically do what is a kind of induction argument
let's look at the level of control that would be needed on the Q to x boundary
relative to the level of control needed on the x to z boundary
and when I say by induction because I can divide x arbitrarily in the middle somewhere
and I can create a boundary there and I could talk about you know x1 and x2
and I could basically say what is the relation between x2 and z and x1 and x2
and I could treat x1 as a kind of control of x2 the same way that Q is a control of x1
right so in effect one way or another at some point or another I can I can look at
the relationship between two partitions okay and so in this sense to simplify things I'm going to
treat the the the we only have three boundaries we have z back to Q we have Q to x and x to z
the minimum number of boundaries necessary when we have something we can control nothing about
something we can control everything about and something we can control only in the beginning
and then not later or diminishing degrees as time passes right so between everything nothing
and everything else we have all possibilities so therefore these three boundaries are the minimum
number of boundaries possible okay so let's look at again because we've come up with a mathematical
way of describing the minimum level of control necessary I can use that same characterization
on the Q to x boundary so say I had a control system that is sending signals to x that are
limits on what x can do with respect to z right so in effect I would basically say that even if
x was a complete pass through where it did no processing at all I just literally connect
every bit going from Q to x one to one mapping to a bit that flows from x to z in this particular
case our artificial intelligence our Turing machine is nothing it is a straight pass through no
modification at all it is a perfect communication channel with no errors in the shanning sense
okay shanning entropy sense obviously no communication channel is perfect some errors
were creep in but let's ignore that for now okay so at this point x is a pure conduit
in that case we would say that the control necessary on the x to z boundary and the control
necessary on the Q to x boundary is exactly the same is there any version of x where the
level of control on the Q to x boundary will be less than the level of control necessary on the x
to z boundary no right if I'm if I basically am going to constrain x relative to z I need at least
that much information on the Q to x boundary so the control level on Q to x cannot be less than
the degree of control necessary on x to z right and again at this particular point
this is what is called a unreasonable proof it's basically like that is an uncontestable truth
right that what that proportion I just set up okay so then we can start to look at all other
versions of x okay so in this particular case we can treat x as essentially the simplest possible
case let's call it x sub not okay x is a pure password channel if I had something even simpler
of no channel of information all every bit that goes from Q to x none of those go from x to z
okay in that particular case I have no control right so the amount of control that I would need
to have in order to basically constrain x to z at least has to be greater than the boundary of x to z
now here's the thing let's consider a like that that collection of stuff that is x let's consider
that to be a whole collection of boolean gates right because we're talking bits from the input to
the output and we've already shown that there's an isomorphic mapping between a bit stream and this
this dart thing so in words we have the characterization of control theory in terms of bits
and we have this bit stream going in and out and so therefore we can model the internals of x purely
in terms of digital logic gates okay now at this particular point I'm now going to set up one new
notion this is the last new concept I need to introduce okay I'm going to describe something
like the hashiness of a transformation of the input stream to an output stream
okay the hashiness is essentially let's say that I take an input stream and I use something like
sha algorithm or the md5 algorithm or I'll concatenate a whole bunch of cryptographic
hash functions whatever permutations I mean I don't really care it's a hash function
okay it's a good hash function right okay the input stream is going to be mapped to an output
stream and now the question becomes what level of control do I need to have if I have a hash
function let's say I have a good cryptographic hash function for x okay what level of control
do I need to have on the q to x boundary relative to the degree of control that I have on the x to z
boundary if I've characterized the level of control on the x to z boundary what is the hash function
going to do is it going to try is it going to hide um anything well the number of states
on the output side let's just say for simplicity sake that the number of bits on the q to z flow
are you worried about collisions there's collisions that can occur yeah right right because a hash
function may map a range of inputs to a smaller number of outputs yeah okay um there may be
some places I can't reach as a result of that not being a bijection right so there may be
some signals that I can't create as output for any signals that I create as inputs
okay um and the mixing if I have you know all zeros or all ones or some combinations of zeros
or ones or some specific subsets of states in the output messages which are known good versus
known bad that the mixing is going to be increased on the input side right the level of control that
I need to need to have on the input side it may be the case that the region of white versus black
will go down because some of the white states will be unreachable and many states will map
to the same white state but it's also the case that many input states will map to the same black
state but whatever the distribution is the distribution is going to be mixed in every
dimension of distribution calculation that could be conceived of that's what a hash function does
so in effect the the scattering of points that are allowable good points there's fewer points
and they're more scattered on the few to x boundary the level of control that I need to have on the
q to x boundary has gone up not just a little bit but like enormously because if it if the if the
area on the x to z boundary that was white was small like a fraction of a percent and its mixing
coefficient was already high so it's like a dust of points like a cantor dusting of dimension like
0.5 or 0.75 or something like that okay like a fractional dimension the scattering of dust
right between zero dimension zero and dimension one right that scattering is fractally distributed
the chances of you hitting a specific point requires almost infinite precision but let's say
because we're talking about bits it's quantized it's finite then in effect there are there's a
there's a sub region of space that I can get to but that sub region of space is now scattered
enormously such that the level of control that I would need to have on the history going from
q to x is at least enormously higher than the level of control that x needed with respect to z
so therefore the question that we need to ask ourselves is that for any collection of gates
that compose x if zero is a straight path through and one is perfect hashiness and by perfect
hashiness means that no simpler constitution of gates can compute the algorithm of how inputs
get calculated to outputs there's no shortcut algorithm that could effectively undo the hashiness
right and we know that to be the case because there's no simpler algorithm
for for various hash functions because if there was people could short circuit
cryptocurrency and earn billions of dollars that way right if I could calculate hashes faster than
other people can calculate hashes or I could somehow predict ranges of hash output values
based upon some subtle inner dynamic then the hash function would already have been compromised
and the flow of money in every form of cryptocurrency in the world every single one of them would now
all of a sudden become collapsed and it wouldn't be long before the secret was out that cryptocurrencies
don't work anymore so in effect this hashiness thing is based partially upon the idea that I can't
I can't predict the output except through executing the algorithm the long way
okay now this is important but I'm setting it up for the future okay so we're kind of on the home stretch now
if we basically say that the level of hashiness we've now defined the notion of hashiness for x
between zero and one based upon the degree to which there might be a predictability of x
that I could use it's faster than x of executing x directly then you know that that basically would
be an intermediate scale of hashiness but whatever the degree of hashiness greater than zero
that every level of hashiness every combination of gates that I so in other words I can take
any random collection of gates between the inputs and the outputs of x and I can now characterize
it in terms of this abstract notion of hashiness you with me yeah basically doing the proof okay
so so in effect what happens is is that we say the more the hashiness coefficient goes up
and every collection of gates has a hashiness between zero and one
okay the more that the hashiness goes up the higher the degree that the level of control
the minimum level of control necessary goes up on the q to x boundary relative to the x to z
boundary so there is a strict inequality that says that the control degree on q to x is always greater
than x to z for any level of hashiness greater than zero
that's really important okay because if the level of control necessary on q to x
is strictly greater than the level of control from x to z
based upon the complexity of x
then if it's the case that x is non-simulatable except by re-executing x then the complexity
of q cannot be simpler than x this is a cybernetic principle it's basically saying if I can't
simulate x other than by executing x if the hashiness of x is high enough that I can't
simulate x then there's no possibility that I can essentially condition the outputs of x
based upon the inputs of x because I don't know how x works so in other words not only is it the
case that the level of control minimally necessary on the q to x boundary is strictly higher than
the x to z boundary but the fact of being able to control it depends upon my ability to simulate
x in the first place and if simulating x means that I have to implement x then z I'm sorry then
then q can't be less complicated than x and has to have perfect knowledge of x
but guess what we have two additional constraints on the reality of the world right q can't be more
complicated than x without being x and if q and x are isomorphic then we don't have q
and the reason we say that is because we can't know the state of x because not only is the
hashiness of x high it's high for two reasons it's high because it's a learning system which means x
is changing in time based upon the outputs of z which we can't predict in advance because to do that
q would have to be more complicated than z if I represent z as a boolean system
with a whole bunch of unknown gates which I can't change unknown gate right unknown gates
so in effect I can't predict what z is going to be therefore there's a high level of control
necessary on the q of x to z boundary I don't know the structure of x in the future because it's
learning and also because frankly if I could anticipate what a superintelligence would do I'd
have to be a superintelligence so both by cybernetics and my principle of uncertainty and by the
hatchiness of x inherently the level of control control necessary on the q to x boundary is
if the boundary from x to z is anything greater than I don't know some really low percentage
the control necessary on the q to x boundary is effectively all of it the control necessary is
for sure higher than the maximum level of control q can produce because q would have to be both
superintelligence and so x superintelligence squared because it have to anticipate everything that the
x superintelligence would do and it would have to anticipate everything that z would do and anticipate
everything that z had an influence on x and all of their interactions which is itself a complexity
beyond even superintelligence squared in q and so in effect if we're going to assume superintelligence
then that basically means q has to be even more superintelligence and superintelligence which is
itself a contradiction because that's not a control algorithm anymore that we have defined
right we we're not we no longer have any control over what q is so therefore if the if the stipulation
is is that q has to be something that is noably manageable to create alignment in z
but we have no definite control over what's happening in q and q has no definite control
over happening over what's an x and x has no definite control over having what's z then
by contradiction q is impossible q ed that's okay well it'd be great is there any links to
the mathematical formalism as it is at the moment no standards so andres job at this point is to
translate what i just told you in the math that math be the thing you present to other people
i i don't want to do it i mean i have my own version of it yeah well you you convert you can
off the internet i am getting out i am i'm basically i it is not my job to convince reasonable people
to uphold a notion of reasonableness okay i want to live my life and so at this particular
point the thing is is that i want reasonable people to do reasonable things which in this
particular case is to take the argument that i have just constructed figure out whatever math
they need to model that in a way that makes sense to them do want to be part of that process
well i'm going to review it but what i'm not willing to do is to try to convince people
to do what is necessary to be done right there's there's no way that that i'm going to basically
be able to tell someone you should x i mean i can i can say those words but that's not
going to result in them doing something they have to decide for themselves that it's worth it for
them to spend the time to work through the process step by step by step what i've just given is the
blazes i've showed the overall construction of how to set up the proof the math is relatively
straightforward the math is is uncomplicated relative to what i just set up but the thing is
is that i don't want to be the one trying to convince people i want anders to do that and anders
is more than competent to take what i've just described to you and turn it into something that
becomes a white paper with his name on it and i don't want it to be i said so i want it to be
we are agreeing and saying so or this constellation of people has reviewed this
and found it convincing right no amount of me telling anybody is going to make it true
it's just the truth is evident in the inspection do the work
i'll be sure to send him the link to this video once it up though please i'm glad for it i mean
like i said i have huge respect for anders anders is a force of good in the world so far as i can
tell and he's he's a decent person and so in effect what i'm concerned with is just you know
trying to make sure that all the steps are clear so that we can see how this is constructed right
and in effect what what happens is is that with this sort of body of process we don't have to agree
on the specific numbers of how robust nature is what the structure of z is we can basically say
it's within this range and anything within this range is going to result in this proof going through
because it's not about the specific numbers it's about the ratios and how the ratios are transformed
for any version of x and any version of q and any version of z and so as a result i don't have to
agree on the specifics of z x or q we don't have to know anything about the nature of artificial
intelligence other than well geez you know we can simulate it with gates well that's basically
what all of the neural correlate people are trying to do in the first place or we can basically
maybe say yeah let's presume that z exists and it has definite physical laws and we know what
those are to this percentage and we know biology to within this percentage let's just spitball
some control values for that and notice what happens with the proof everything else is by induction
and so as long as the numbers are greater than zero the inductive process results in a kind of
convergence with a inequality showing that these inequalities hold and therefore you end up with
an unreasonableness of the idea of alignment that's what the end result is regardless it's like the
notion of the generality is what makes the proof work and people try to pin it down into well what
if this or what if that or what if this other thing it's like do the work show for each what if
that that what if can be modeled within the context of the dynamic that i just described the loop that
i just created and show that they're all special cases of that more general phenomena it's like
shannon's information entropy it's like once you understand it it's like this is full coverage we
don't need to look for anything else and again evolution has a what's that natural selection do
you think natural selection is at danger here as well i mean like natural selection produced humans
which are intelligent enough to produce technology um and which end up being intelligent enough to
bootstrap at least the artificial intelligence that we have today well we're we're intelligent enough
to make nuclear weapons and nuke ourselves into oblivion i mean you know yeah right yeah we're
a danger to ourselves because of our unconsciousness of our power and we need to have care transcend
power in the same way that transactionalism transcends care and power transcends transactionalism
care transcends power but we're not yet skillful in how we hold care so is it a technical problem
to be solved would you say that uh i'd say it's kind of a learning evolution it's a kind of
philosophical problem it's a kind of ethical problem but ultimately it comes down to things
like culture and governance and how do we collectively make choices how do we understand
ourselves as a species how do we understand our role in the world or consciousness in the universe
right like those are kinds of things that we can no longer remain ignorant about we actually have
to embrace these questions at a sufficient level of depth to be able to handle the kinds of problems
we've been handed change in nature has handed us this on one hand beautiful opportunity to rise
to the occasion but it's like you know the first time that you've been handed car keys and you
know you could make a bad choice you can go to a store and buy some alcohol because you're 21 now
and you could drink and drive and get people hurt including yourself but that's not the
responsible thing to do right somewhere along the way we have to hope that we have learned enough
that we have gained enough wisdom that we move towards a culture and a governance methodology
that is wise enough and preserving of wisdom enough that we can handle these sorts of things
because frankly you know if we keep rolling the dice if we keep playing you know this game of
Russian roulette eventually you die the categories of existential risk like you know i'm tracking
like about 30 categories of existential risks some of them are extremely serious like the
substrate needs argument is one of the single the top five it's one of the single most serious
versions of existential risk that i know of the others i won't name name or list right now because
again you know some of these things are quite technical but but the point is is that our
capitalistic non-humanistic but but sort of like a modern way of thinking and philosophy is leading
us down a blind alley on an evolutionary scale and we need to basically recognize things that
evolution right now isn't like and we can predict the future to a limited extent and we need to take
in the reality of what's actually happening to the best of our ability and to make wise choices
about that and we need a culture that supports the values of conscious sustainable evolution
frankly how do you get to the culture that will be able to do that i mean culture drifts and
sometimes it's you know there's no there's no one at the steering wheel but what how do we get to a
culture well one of the things is we're clarifying certain values like i think that to a certain
extent we want to emphasize clear communication you know communication that gets down to the
things that really matter and to some extent it's it's kind of ironic because the the the
mostly autistic spectrum-y type people that are really good scientists and engineers and
technologists are also the people that really have no interest in small talk and actually want
to get down to the things that really matter but they're really uncomfortable with ambiguity
and uncertainty and so they want to bring it down to discreet concrete linear arguments that
can be understood in a black and white way but i think the thing is that in this particular case
we're needing a comfortableness with sometimes difficult things difficult emotions difficult
feelings and to create clear communication about the meaning of what's happening rather than just
say monetary values or functional utilitarian purposes in the sense of altruism and more
effective altruism i'm basically looking at how do we get to the point where the communication
that we have with each other is clear enough and meaningful enough that we can surface the values
and the meaningfulness that actually matters the relevant things that we need to be talking about
so that collectively we can have the wisdom necessary to address these kind of concerns
and we have to in a sense be aware of our technological optimism bias and our our causal
mechanism bias and our our bias that some version of the invisible hand of adam smith can solve all
problems that some version of voting is going to solve all our problems we've got to quit looking for
silver bullets and actually get down to the point of having the kinds of conversations
where the reasonableness that lives between us and emerge right harbormass was right harbormass is
basically far as i'm concerned one of the if not the greatest sociologist that has ever lived
again he's still among us it's a miracle so the thing that he pointed out which i think is completely
truth is that nobody individually has reason it's how we connect with each other that creates
reasons the conversations we have that emerge reason no one's individually reasonable it's
how we work together that creates reasonable so we need to get better at that kind of collaborative
process the skill of caring together of living in community together of doing the kinds of things
that are non transactional and non power oriented that are skills in the space of care and to do
that we have to be in touch with our emotions and our feelings as much as we are with our intellect
we have to understand instincts and intuitions just as much as we understand mind and so in
effect here we just haven't developed the kinds of skills of attunement and discernment necessary
currently to build the cultures or to have the kind of cultures emerge that have the carrying
capacity for this kind of stuff so as far as i'm concerned it's the human being that matters
it's the re naturalizing that matters and frankly we've got to let go of some of the delusions of
modernism we got to let go of the hype that some silver bullet artificial intelligence can solve
all our problems it just ain't so and so in effect it's like you know believing in perpetual motion
machines i don't want people to believe in perpetual benefit machines because that's how
ai is being promoted today is it's just as unreasonable as getting gold from the philosopher's
stone i'm sorry that's just not how it works that's not how chemistry is is is constructed even
nuclear chemistry has a hard time with that right but but in effect to think about a perpetual
motion machine is like an infinite source of energy whereas now we're talking on an infinite
source of intelligence i'm sorry it just doesn't work like that right it's just it's unreasonable
and right now people are in the delusion that they could create an infinite benefit machine
that's what general artificial intelligence is sold as and you know it's an investor's way
they think they can make 10 000 return on investment yeah maybe in a short term briefly
but basically it gets everybody killed do you want to talk about the the substrate needs
hypothesis then in more detail you have discussed this elsewhere but i just wonder if our viewers
can get a taste of what that is well roughly the argument is this different substrates right
so an artificial substrate is different than a natural substrate and by natural i'm just saying
organic okay and i know that physics is nature too but i'm i'm making a distinction between
something we build versus something that just emerges because of evolution okay um or organic
evolution let's let's call it carbon-based chemistry versus silicon-based chemistry
and i know that there's quibbles around that but for simplification let's just say silica versus
carbon okay so in this particular sense what we notice is is that the needs associated with
maintaining silica-based micro lithographic silicon microchips and the needs associated with
cellular process associated with all mammals or trees or bushes or whatever they are just
really different needs like carbon-based life wants water and if it's a mammal presumably oxygen
and some sort of carbohydrates and so on whereas if i'm dealing with silica-based thing i'm mostly
looking for an absence of water you know cool temperatures when we're processing and warm
temperatures when we're fabricating like to make silica lithography i have to crystallize silica
that's an expensive process that requires a lot of heat and zone refining and things like that
so in this particular sense this thing works over a range of wide temperatures that organic life
just won't tolerate right plus or minus 100 degrees fahrenheit and you've got no living okay um whereas
if you are basically dealing with silica chemistry it doesn't even get interesting until you're dealing
around 400 degrees centigrade minimally and more like 4 000 typically right and so in effect
almost all the stuff needed with sustainability and increase in capacity with silica-based or
artificial-based systems is a completely different needs structure than that associated with carbon
anything different needs mean different basis of choice so now i have not just that the needs of
the artificial system in the organic system are different but that the basis of choice must be
different however we conceive of choice however we conceive of intent or goal seeking or motivations
or whatever else is driving the choice making process as a kind of orientation right so alignment
is usually thought of instrumental sense of explicit goals or goal refinement dynamics relative to some
underlying goal right now those can be explicit or implicit but nonetheless different needs means
that different goal or implication structures are going to be at the basis of choice making
however you think of choice making right so even in an input output system that is responsive
the intelligence is in a sense the degree to which the responses are correct with respect to the
environment and the goals of the thing but those goals have to have responses that support the
needs of the substrate or you don't have sustainability or increase the capacity right so
in effect even if they're tacit there's some influence on the nature of the choices and therefore
some influence on the nature of the outcomes and there's a divergence now between artificial
substrates and organic substrate that that that increases the farther up the stack we go so in
effect artificial needs produce artificial goals artificial goals produce artificial choices different
choices with artificially shaped outcomes which produces an artificial ecosystem or an artificial
environment which cannot not feed back into the substrate so either that loop is convergent
and there's an increase in sustainability or capacity or the sustainability of capacity or the
increase of capacity and sustainability or any combination of those two words that that over time
is going to converge more and more on the meeting of those needs because even if they're tacit
any failure to do so means that the system loses capacity or loses a thing so an evolutionary
dynamic even if it's composed of just the components of a singularitarian's super
intelligence one way or another this notion of feedback is going to happen and it's going to
happen in a distributed way over time because nothing is existing at a single point in space
right the substrate is always distributed and time is always incurring and the notion of
sustainability and capacity is is either convergent or divergent then if it's convergent
then you end up with more of it and if it's divergent then you end up with less and so in
effect we're now looking at the enthalpy of the artificial system versus the organic system right
carbon versus silica and unfortunately we already know something about that enthalpy the enthalpy
associated with carbon-based chemistry over all the varieties of carbon-based chemistry
and the enthalpy associated with silica chemistry over all of the enthalpy of certain silica
chemistry the silica chemistry runs way hotter with way way more intense energy than anything on
the carbon side which suggests an incompatibility between the artificial ecosystem versus the
natural one versus the carbon-based one and this is where the substrate needs argument starts to
basically come into focus because in effect you can show that over time the silica displaces the
carbon because the carbon can't handle the kinds of molecular constructions right most of them are
toxic to the carbon it can't handle the kind of energy signatures most of which are out of the
range of what the carbon system can handle and it can't handle the pressures and all of the other
forms of energy right silica-based chemistry is going to be differently able to handle things
like hard radiation than carbon-based systems are going to be right and so in effect there's a
there's a sense in which there are some fragilities that this silica-based system has
that the carbon-based system doesn't but overall the carbon-based system is way more fragile than
the silica-based system because point of fact is the carbon-based system is way more complex
in way more dimensions over way narrower energy spectra and pattern spectra than the silica system
is so as a result when you look at the interaction dynamics between the two ecosystems the silica
displaces the carbon because it's essentially a collapse in the complexity states it's a
reduction to a lower value lower energy state in the hyperspace of all complex system design
so in effect the phase change dynamics push the carbon out and it happens slowly but because of
the convergence dynamics built into the different substrate different needs different needs different
implications and or basis of choice different choices different outcomes different outcomes
different environment back to substrate that converges has to cannot not and so as a result
you're now actually looking not hypothetically but actually at the enthalpy dynamics and regardless
of which branch or permutation you argue you try to put a barrier between them eventually you show
that the barrier can't be maintained except by the the high side i.e. the the side of the silica
the silica doesn't have any basis by which it can maintain the barrier because there's no energy
or economic basis by which that can be sustained so as a result eventually pinholes appear and you
only need a pinhole in the wrong place and you kill the carbon now again these are dynamics that
happen over time it's slow moving it's not obvious it's not like a pandemic that you can see it
occurring it's like approaching on a black hole you'll go right through the the event horizon and
never notice the thing right spaghettification can happen all the way on the interior right from
the outside you know maybe you could see the dynamics but we're on the inside of this process
we're in it we don't have an outside in view unless we construct one for ourselves using
arguments based on the ethics based upon the kind of proofs that i was setting up earlier
and so in effect what we're doing is we're saying hey the substrate needs argument is a
observation that there's a convergent process plus an enthalpy relationship between three
different ecosystems the net effect is is that the carbon-based ecosystem is destroyed with
exceptionally high probability and then any control dynamic this was the first part of
what we were just talking about any control dynamic that we would use to try to prevent that from
happening is fighting against causation itself and requires perfect predictability it requires
not just that the right control signals are sent against a field of doesn't damage this
but now has to basically be the kind of control signals that accounts for its own effect on this
which means that it's a far narrower control signal than we originally anticipated
like before it was just don't do no harm but now we have this convergence process
that's spitting out all kinds of signals all the side effects of the industrial chemistry
associated with making lithography like a 100-layer microchip that you're carrying around in your
pocket as a central processor produces you know all sorts of side effects in terms of chemistry
that is incredibly toxic to the environment i mean just look at what's going on in other places
in the world and you'll see entire deserts created by effluent of things like just nickel for example
right and so in effect there's a there's an entire ecosystem of damage that's occurring
from signals already passing from the silica side to the to the carbon base side and we're
watching it happen today right these are unconscious dynamics built into things like
corporations and artificial intelligence which corporation could be thought of as an example
of just in a very distributed non um you know with with human beings as organic components
right but but frankly it's the artificiality of the system it's the virtualness of the system
the non-embodiedness of the extraction the abstraction the extraction abstraction and
accumulation that together are creating a thing where frankly that doesn't have a future that is
just life can't continue with those kinds of choices so in effect unless we
distribute right undo accumulation right because all toxicity is too little of something needed
or too much of something that you don't want right that's all toxicity is an imbalance like that
and every technology is linear in a sense of extracting from one place and putting in another
and you end up with depletion in one area and toxicity in another so in effect what happens
is is that unless we do the right sort of distribution unless we do the right sort of
embodiment move out of abstraction back into the realness of what is and then recontextualize
in the context of where we actually are and the choices we need to be making today we're
making really poor choices with respect to the substrate and that's already happening organic
life is is at this point human beings are disabling their own organic substrate they're doing the
divergent process whereas artificial intelligence is doing a convergent process that is not a
gameable that's not a win game we're already we're already losing and we don't even have the other
player really a fully embodied yet okay so you know no matter how you calculate it the the dynamics
of the two ecosystem interactions is completely it's it's one way no question about it's like
an ant trying to prevent the incursion of a bulldozer there's nothing the ant can do
the same sort of way beyond a certain point there will be nothing humans can do
or the totality of or all organic life that is going to be able to prevent the incursion of
silicon thing it's like it's like crossing the event horizon once you pass that threshold it's
over and you might not even know when that happens there's no obvious signal there's no alarm bell
that goes off there's no warning there's no nothing you just pass through the event horizon
and after that you're fucked so in this particular sense there's a there's a sense here in which
we need to notice that we're in orbit around a convergent process which will not quit and decide
that we're going to add some philosophy and get out of the get out of the orbit of this thing
and that basically means to some extent we've got to reevaluate the philosophy of modernism
the critiques of postmodernism are real but postmodernism isn't an answer it's just the critique
somewhere we need to think about rehumanization and renaturalization and modernism is not that
so you know there are solutions to these particular problems and I can talk about them
at length but at this point I just want you to understand the comprehensiveness of the problem
statement do you think there's any any space in the possible worlds where we can converge both
naturalism humanness and modernism do you think it's ever possible I mean I think I think modernism
has a place it just doesn't want to be the only philosophy in the field and I don't even think
it wants to be the dominant one I think to some extent you know we've gotten so enamored with
causal process that we've learned we've learned how to use causation but we haven't continued to
develop our capacity to care so yeah it's not like I hate modernism I see the problems of it
and I see a better philosophy and at this point I'm just wanting to embrace the the better philosophy
because frankly someone has to and where do we start we start with conversations like this one
you know capitalism isn't necessarily evil but it's doing some things which are pretty damaging
because we've just let it run out of control because we don't have a model of what else it could be
but now we can there is a way to have that and it's not artificial intelligence it's not
displacing our capacity to choose to some machine we need to get better at making choices for our
own sake and that's how we do it we make choices that make sense for our own sake and here's how
we do that we understand our own substrate we understand the needs of nature so that we can
have it be in healthier relationship to us so that we can be healthier we need to have healthier
relationships with each other we need to know what that means we need to understand that stuff
and choose to make choices in that direction and that is entirely possible does that mean
modernism goes away no does that mean fear goes away no but we need to love more than fear that's
it more right do you think artificial agents can ever love i mean is this is this a thing that can
be sort of designed can we make ourselves more loving can we be can we make ourselves more loving
do we need machines to do it mostly no we can use machines to help us to become better at loving
one another but that's a skill in itself and that is a very discerning line it's like a prosthetic
a prosthetic can make it so that you're more functional as a human but it doesn't replace your
humaneness so in effect what i'm really trying to do is focus on better humans and if we need
prosthetics yes let's use the damn prosthetics but i don't want the prosthetics to be a replacement
for the humaneness i want the human to be human how do we preserve the human in a future of
more and more prosthetics or more and more can we enhance the human without the prosthetics
i mean yeah we can can we absolutely can make bigger brains can we make it so we have bigger
brains for instance you're still looking for more cognitive power you don't need more cognitive
power you need more heart you need more guts yep can we do that yes of course we can
can we have both uh i don't know that you would even want both i mean you know at this
particular point let's just start with can we do that yes how do we do that well we need to
communicate in a clearer way on on a deeper basis and we're getting there i mean we're working to
that i mean you know i'm just meeting you for the first time but you know i'm gathering that you
care about these kinds of issues otherwise you wouldn't have invited me in your call well i do
i mean i've been i've had my um show since 2010 um i interviewed and is in in oxford in 2012
i think it was yeah um and a bunch of you met him before i did so you have you have a longer
relationship even so in this sense um all i'm basically suggesting is first of all kudos for
running a blog for that long um i'm i'm curious about your experience now i want to learn what
it's like i've been doing that for such a period of time how has that worked out but this isn't
me interviewing you let me just park all those questions that i have for later uh because we
have been on the call a long time and i i want to be uh respectful to myself as well yeah absolutely
yeah but i need you yeah but i but i want to basically just say it is for sure worth it
for us to develop capacities around how we hold care collectively how we hold humaneness
collectively how we hold naturalist collectively and it's not so much what i'm against it's more
about what i'm for and that's the real benefit like at this point if we know hey this particular
road is closed to us and this road over here is open to us it's kind of like well gen let's just go
down the road that's open let's go down the road that works you know we can we can continue to be
in dilution but that doesn't help anybody well um yes well it's been really wonderful speaking to you
and i've really enjoyed listening to you explain a lot of your theory about the problems of aligning
ai uh i yes i am interested in following this further and you know be interesting to get you
in conversation with and as if at some stage maybe well i would initiate that i mean at this point
he's he's he's in touch with my secretary of and um yeah you know as as he has time he'll
schedule the next meeting i'm not trying to push him to to do anything but on the other hand i am
hoping that that that at least maybe this conversation will give some uh clarify some points and fill in
some of the details because some of this has been laid out for him pretty pretty well and
and other parts of it are are now available because of this recording and and um if he
has further questions or wants to see how i would deal with certain things that's fine
okay um okay so if you got any conclusive remarks you'd like to have before we bring the the the show
to a close um i think the concluding remarks are um from where we're standing it looks really hard
to basically move into a future that doesn't feel terrifying um but on the other hand with time and
with a little bit of effort and with some clarity it becomes easier and easier to see a beautiful
future and uh hopefully the the fact that that that at least someone uh again you know if my
expressions here uh give at least someone some confidence that that that yes this is worth doing
we should we should basically pursue this because it's worthwhile and can actually work and that
there's very good reasons for us to be confident that things that can work can work and the things
that can't work can't that we could just make easier choices that over time will will just
seem more and more natural and that this is just uh you know i i have compassion for people that
invested so much effort into trying to build uh things for the betterment of humanity and now
we're getting more nuanced about what that means well yes thank you so much for your time and your
wisdom and and first yeah for addressing us today yeah i look forward to chatting a few
again at some stage but um yeah it's been really good and i'll put all some i'll put some useful
links in the description okay yeah most of my site's gone i i took it down because i'm moving my
company and um i don't have a plan yet as far as when that's going to change because i've got
so much logistics i got to deal with between now and then so um anything that links to mflb.com has
gone um but the uh the the thing is is that uh remalt ellen is very familiar with these arguments
and understands this pretty well so uh if if you see any of his writings um he's been developing
this in in in various communities and and and can stand for some of this stuff uh reasonably well
there's a handful of other people that understand this argument too um and have presented their
own versions of it as well and so um uh woof tivi for example wrote a paper that i thought was quite
good um but again you know these are the kind of things where people have to think through it on
their own and you know appreciate the links and and and those are some references to places where
people can go awesome well yes i'll look those people up um i will see if i get the right links
what was it the first person who's remalt ellen yeah um woof tivi and um uh roman um and yes that
guy i'm interviewing him sometime soon yeah um he's he's also worked in this space but he did
again an independent version focused on the rice theorem particularly and and um you know people
have encountered his work too um his work and mine are basically parallel to one another
i think i've been more thorough and in a lot of ways um he's focused on an aspect which
other people would find maybe more intelligible um but i i few that for the most part we've
we've kind of had our own thoughts and then compared notes and now we've expanded to include
the other um but uh yeah like i say the the three people that come to mind the most obviously
as being um you know thinkers in this space aside from myself would would be those three
people i just i just listed awesome all right cool well thanks everybody who've made it this far it's
yeah better than enjoyable marathon like uh actually went for the longest run i've been
for a long time last night oh wow yes i well it did have the benefit of presenting one whole
construction pretty much as a complete thing and and as i said there are other parallel
constructions that all arrive at the same result but this one happens to be one that i think is
uh maybe the most obviously formalizable which is something that andres was asking for and you
asked for um so that's that's that's kind of like uh the the picture of it and again it's just
it's just one of many but it's it's a good one awesome yeah yeah well i'm looking forward to having
it fully formalized in maths i think a lot of people be able to sort of test it and and put it
through their way of thinking yeah compare it to their way of thinking mathematically i think
it'll be awesome yeah great no so thanks very much florist okay you have a great night good evening
you

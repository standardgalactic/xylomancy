We start diving into the greedy methods by presenting an algorithm called the orthogonal
matching pursuit.
The idea is the following.
We are looking at the system ax equals b and we are searching for a spar solution to it.
We will find the support of x by operating sequentially and gradually.
We start by searching the best possible solution with the support of only one atom.
This is done by sweeping through all the m possibilities and finding the column in a
that leads to the best match between ax and b.
Once found, this column will stay with us and we will search for the second atom to
join it.
Again, sweeping through all the a minus one possibilities, we choose the second atom such
that the match between ax and b is the best possible.
This way, the support grows by one non-zero at a time.
The algorithm stops when ax and b are close enough to each other.
Put in terms of the tree we have seen before, we start with an empty support and then check
m options for the first atom thus getting to the best chosen solution with cardinality
one.
Other than checking all the possibilities of cardinality two, we check only the options
that rely on the already found solution.
This process proceeds this way again and again until we get to a good enough approximation.
This way, instead of a combinatorial number of tests, we apply order of m tests to complete
the algorithm.
The highlighted path describes how the support has grown by one non-zero in each step.
While the above description may seem clear enough, it is actually a bit vague and various
greedy algorithms could be proposed while being based on this rationale.
We shall introduce several such methods and start by focusing on the OMP, orthogonal matching
pursuit.
Like all other greedy methods, the OMP generates series of solutions with gradually growing
support.
We denote these solutions as x0, x1, etc.
These proposed solutions do not obey the equation ax equals b.
And we shall denote the error vector b minus a times xk as rk, standing for the residual
vector at the kth step.
The main point in the OMP is to use the residual in each step in order to choose the next atom.
This will be done such that the chosen atom leads to a maximal reduction of the residual
energy.
Once we start with x0 being 0, the residual starts as the vector b.
We update x by adding one non-zero, becoming x1, the energy of r1 gets smaller.
We proceed this way, adding one non-zero at a time to the solution and reducing the energy
of the residual until it becomes 0 or sufficiently small.
And now to a detailed description of the OMP.
In initialization, we set k to be 0, x0 to be 0, the support s0 to be empty.
The residual is the vector b.
We increase k by 1 and perform the following steps.
Given the residual rk minus 1, we search for the best column to choose from a, such that
when multiplied by a scalar, it gives the smallest L2 difference from the residual.
Suppose that we did these M tests and got the error values EI.
The best atom to choose is the one leading to the smallest error.
Let's assume that it is the atom I0.
Then this index joined the supports and now sk is updated to include it.
We proceed by updating the actual coefficients of x in the chosen support location that would
give the smallest L2 error between axk and b.
This is a simple L2 process and its result is an updated xk.
Our last step is to update the residual rk to be b minus a times xk.
If the obtained residual is small enough, we may choose to stop the algorithm, otherwise
we increase k by 1 and proceed.
Now let's zoom in on several of the steps described in order to better understand them.
We shall assume that the columns of a are L2 normalized, a fact that we'll prove useful
later on.
Looking closely at the computation of EI, we are optimizing with respect to the scalar
z that multiplies the ith column taken from a.
The optimal value of z is given by a simple derivative of this L2 expression.
zopt equals the inner product between ai and the residual rk minus 1.
Notice how the denominator vanished since the atoms are normalized.
Plugging the expression of zopt back into the L2 error and applying few simplifying algebraic
steps, we get that ei equals the square norm of the residual minus the square of the inner
product between ai and the residual.
Therefore, instead of minimizing the error ei, we might as well maximize the absolute
value of this inner product.
That means that the choice of the next atom in the OMP can be done in this way.
Take a transpose and multiply by the current residual rk minus 1 and take the absolute
value.
The resulting vector is of length m and its maximal absolute entry points to the atom
to be chosen.
Focusing on the step of updating xk, this is a least squares computation over the portion
of the whole vector x.
Given the support as k, we are to extract only the green columns and solve for the green
entries in the vector x.
Thus, this amounts to a simple least squares with k unknowns and the solution is given
by the pseudo inverse of as times b.
Why is this algorithm called orthogonal matching pursuit?
The term matching refers to the correlations we apply between the residual and the atoms
in A in order to find the next atom.
By orthogonal, well, in the least squares we have just discussed, the optimal solution
can be found by the derivative of the L2 error with respect to x.
This leads to this expression, as transpose, multiplying this yellow term.
The yellow term is nothing but the new updated residual.
This means that after updating the solution xk, the inner product of the new residual
is orthogonal to the atoms in the support, being the bros of AS transpose.
This orthogonality is an asset, because it implies that once an atom has been chosen,
it will never be chosen again, since its inner product with the residual is zero.
Still on the matter of the least squares that updates xk, there is an effective numerical
shortcut worth mentioning.
The regular solution involves an inversion of a gram matrix of size k by k computed for
the matrix ASk.
However, one step before we inverted a similar matrix of size k minus 1 by k minus 1 that
referred to ASk minus 1 with one atom less.
Could we leverage the previous result in building the new one?
The answer is positive.
A new column has been added to create ASk, and along with it, a new scalar unknown has
been added to the vector x.
There is a recursive method for solving sequence of growing least squares problems of this
form exactly, which implies that we do not need to invert any matrix in the OMP.
We will not discuss this numerical shortcut further here.
To summarize, the OMP involves two main steps of computations.
The first is the sweep stage that searches for the next atom to add.
This requires the multiplication of A transpose by the residual vector, thus requiring mn
operations.
The second is the least square step that updates xk.
The main effort here is in computing AS transpose times AS, but there are various shortcuts
that can be applied here.
On line, the overall number of operations that OMP requires is on the order of k times
m times n, where k is the cardinality of the final solution.

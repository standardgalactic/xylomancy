AI is going to bring a new renaissance for humanity, a new form of enlightenment if you want,
because AI is going to amplify everybody's intelligence. It's like every one of us will have
a staff of people who are smarter than us and know most things about most topics,
so it's going to empower every one of us.
Jan, I am so excited for this. I heard so many great things from our mutual friends,
obviously David Marcus and then Mathieu at PhotoRoom. So thank you so much for joining me today.
And it's a pleasure.
Now, I would love to start. I heard some of the early stories,
but I want to start with one from David Marcus.
How did you first enter the world of AI and make that first foray?
I was still an undergraduate engineering student in France and I stumbled on a
philosophy book, which was a debate between Jean Piaget, the cognitive psychologist,
and Noam Chomsky, the famous linguist. And they were arguing about nature versus nurture
for language, whether language is acquired or innate. So Chomsky was on the side of innate
and Piaget on the side of acquired with some innate structure. And on the side of Piaget was
a guy called Seymour Papert, who was a professor at MIT. In his argument, he talked about something
called the perceptron, which was an early machine learning system. And I read this and discovered
that people had been working on machines that could learn and I was fascinated and I started
digging the literature, soon discovered that much of that literature was in the 1950s and 60s and
basically stopped in the late 60s because of a book that they killed it. And Seymour Papert was
a co-author of that book. So it was strange enough and here we were 10 years later actually
praising the perceptron as kind of an amazing concept. So I was hooked. I started getting
interested in what was not yet called machine learning, but eventually became neural net
and now deep learning. Can we ask you, David asked this as well, how long did it take to get
in terms of like the major breakthroughs? How long did it take you to get to the major breakthroughs
that you're at the origin of when you look back over that time to get to those major breakthroughs?
Well, so there's a few back to it. So the first one was in the, when I was still on the road,
basically finishing my engineering studies, I figured out that the way forward to kind of lift
the limitations of the old systems that were abandoned in the 60s was to find learning algorithms
that could train multilayer neural nets, essentially. And people had all but abandoned this
type of research, except for a handful of people in Japan. And one guy I heard, I heard about
called Jeff Hinton, who had published a paper in 1983. So this was just the year I graduated
on something called the Boston machine, which was clearly a method to go beyond those limitations.
And so I had, on my side, kind of developed a method for training multilayer nets, which was
very close to what we now call back propagation, but not exactly the same. It was closer to what we
call target prep, actually, nowadays. And then, you know, published a few papers in French and
eventually met Jeff at a meeting in France in 1985. And we realized we've been working on the same
thing and we're thinking alike. And, but I was, you know, in the middle of my PhD, and he was a
associate professor at Carnegie Mellon. So we started, you know, a discussion and then,
you know, visited him at Carnegie Mellon for a summer school to organize. And then I,
when I finished my PhD, I did a postdoc with him and then John Berlatz. And while I was in
Toronto, I developed what was called convolutional nets now, convolutional networks, which, you know,
is major method for image and speech processing nowadays. And so that's, that's what I'm best
known for. But, but it started much earlier. I have to ask, Yoshua described kind of the
hype cycles within AI and neural nets like deserts when you're not in them. And he asked the question,
how did Jan not get discouraged when for a solid decade we were in a desert where no one really
cared about neural nets? How do you keep the enthusiasm bluntly when, as Yoshua said, no one
really cared? Both Yoshua and Jeff and I had in the back of our minds that those methods would
eventually come to the fore and that, you know, we would have to kind of snap
people out of their preconceived ideas about neural nets. So yes, there was. So Yoshua and I
were actually working together at AT&T Bell Labs in the early 90s. And then the interest of the
community for those methods started waning around 1995 or so. And it was indeed about 10 years when
not only nobody was interested in neural nets, but people were even making fun of it, you know,
talking about it in sort of disparaging terms. Now, there is something though, in 1996, I kind of
changed the job. I stayed in the same company, I was still working at AT&T in the research labs,
but I became a department head and this was the early days of the internet. And my group and I
started working on something completely different that had nothing to do or not much to do, at least
with machine learning, this image compression. I had this idea that with the internet coming up,
we should have a way of scanning existing paper documents and then, you know, put them on the
internet so that everybody could have access to them. And so I worked on this for five or six
years together with the Ombot 2, who had been a long-term collaborator. Yoshua was also involved,
peripherally, and a bunch of other people, Patrick Hafner, et cetera. And that project ended when
all of us basically left AT&T. That's when I restarted working on deep learning. And Jeff also
kind of came back to Canada, he had been in the UK for a while. And Yoshua, Jeff, and I decided in
the early 2000 to basically start a conspiracy to revive the interests of the community in your
nets by making them work, discovering new algorithms. And it took almost 10 years, but it
succeeded beyond the wildest dreams, basically. So I'm going to ask you a range of varying questions
in terms of depth, breadth, and kind of obvious and non-obvious. So forgive me if some are obvious.
I just want to ask, when I hear the historical context there from you, over many decades,
how do you feel today when we look at what's happening today? Are we at a new inflection point
in development? Or is this merely the continuation of what we've seen for many decades?
It's a combination of the two. So on the one hand, a lot of what we see today, when you are kind of
down in the trenches of research, looks at a logical extension. I was not as enthralled by the sort of
recent progress as the public was, because I've seen this progress happening over the last several
years. Now there are things that have been very surprising. The fact that self-supervised learning
methods applied to transformer architectures work amazingly well, and they work way beyond what we
could have expected. The fact that we can do basically train systems to understand language,
translate language in multiple languages, and then continue text if you try them to do this,
or answer questions if you try them to do this, works amazingly well to an extent that people
didn't quite expect that it was going to happen by just making them bigger and training them on
more data. So that certainly has been surprising for everybody, but that revolution occurred two
years ago. Whereas the wider public has learned about it through a tragedy that was made available,
for us, it's been more continuous. And you see this in a lot of marking events in technological
progress or in AI, in particular, are marked by kind of splashy events that the public pays
attention to. But too many of us, it looks more like a continuous thing, and generally what those
progress require is a bunch of people to take the techniques that already exist, push them a little
further, do a bit of engineering, and then make a demo that demonstrates that it works. So that
was the case for DBlue, the chess player that IBM built in the mid-90s, that beat Gary Kasparov,
you know, same thing with the DARPA Grand Challenge, that Sebastian Thuin team at Stanford won a car
that could drive itself in the desert, right, for a hundred miles. And then, you know, AlphaGo,
and, you know, the IBM Jeopardy, there's a number of those things, right? And, you know,
which IGPT just being the latest one. And it looks like kind of jumps when you look at it from far
away, but when you're in the field, it's more like a continuous evolution.
Can I ask, has there been any other surprising on the positive side, developmental things you've
seen over the last year, or so you said about self-supervised learning and the efficiencies
there? Is there anything else where you're like, I didn't expect it to go as well as it has done
in the last year? Yeah, so I already mentioned it. You know, the fact that merely training a
language model to predict the last word in the sequence of words, if you do it properly, you
get a system that has capabilities that are somewhat unexpected and they emerge as you make
those systems bigger and you train them on larger amounts of data. That's clearly,
clearly been the surprise for everyone. Now, the thing is, you know, as researchers and scientists,
we're always looking for the next thing. So what I'm interested in at the moment is, you know,
what goes beyond that. Like, you know, a lot of people are going to work on applications of
autoregressive large language models, which is great. There's going to be a lot of,
you know, products and new ways for people to do things and it's going to be wonderful.
But I've already been thinking about the next stage for the last three, four years, four, five
years even, actually more, which is like, what's missing from those systems? What are your thoughts
on what's missing from those systems? In that logical next step, where does that lead you in
your thinking? So those systems do not have anywhere close to human level intelligence.
Okay, despite what you might think, we are kind of fooled into thinking it because those systems
are very fluent with language, but their ability to, to think, to understand how the world works,
to plan are very, very limited. And they're understanding the world is very superficial.
And the reason for it is that they are strictly trained on language. And language only contains
a small proportion of all human knowledge. Most of human knowledge is not linguistic at all.
And all of animal knowledge is non linguistic. And we take it for granted. You know, this is the
Moravec paradox, right? All the capabilities and abilities that we take for granted, like, you
know, planning a motion or something, or very simple things that everyone can do. A 10 year old
can, you know, clear up the dinner table and fill up the dishwasher. Any 17 year old can learn to
drive. We still don't have so many cars. We don't have domestic robots. If they're non linguistic,
like the majority, I'm sorry for the base questions, but then what are they? And is that
that we don't have able to be ingested by AI models and engines over time?
Well, so first of all, there is no question that eventually, AI systems will understand the world
in similar ways that that humans do. There has better ways. But they will not be
autoregressive large language models as a type that we're now talking about.
They will be different for a number of different reasons. But to answer your question more directly,
anything that has to do with sort of an intuition of the real world
requires an experience of the real world or a simulated version of it, which those large
English models don't have. They purely train from text. So you can add, there's a number of
questions about the physical world that they'll be able to answer because there's a template for it
in the, or something very similar in the data that they've been trained on. Same for planning,
you can ask them to plan a trip or something and they'll adapt a template that they've been trained
on. But they don't really have sort of a model of a mental model of how the world works and allows
them to plan complex action sequences or use tools or things like that. Can I ask, is that why you
said that AI researchers face palm when they hear prophecies of doom? No, that's a different question.
Those are kind of orthogonal concepts. So I mean, there is some some weak connection.
There is a flaw in a current autoregressive lens, which is that you can only control their answer
in two ways. The first way is you modify the statistics of the training data that you train
them on, possibly using human feedback for specific answers. And the second one is you
change the point and the combination of the point that the question you ask them, the form in which
you ask the question and the statistics of the training data entirely determines the answer to
the system we produce. So there is no persistent memory, first of all. But second of all, you cannot
control the system. You cannot impose constraints on it, like be factual, be understandable by a
certain year old. You can try to put this in a prompt, but then, you know, you rely on whether
the statistics of the training data is appropriate for for taking taking that into account. There's
no direct way to constrain the answer of those systems to satisfy certain objectives. And that
makes them very difficult to to control and steer. And so that creates some fears because people are
kind of extrapolating, if we let those systems do whatever, we connect them to the internet and
they can do whatever they want, they're going to do crazy things and stupid things and perhaps
dangerous things. And we're not going to be able to control them and they're going to escape
or control and they're going to become intelligent just because they're bigger, right? And that's
nonsense. First of all, because this is not the type of system that we are going to give agency to,
the systems that will eventually be given agency that are going to be able to plan
sequences of actions are systems that are going to have objectives that they're going to have to
satisfy. And because of those objectives, they're going to be controllable. So they're going to be
much more controllable than the current systems. Okay, so my prediction is that within a few years,
nobody in their right mind would use autoreversive LMS, they'll go away in favor of something more
sophisticated and controllable, they can plan its answer as opposed to just produce one word after
the other reactively. Okay, that's the first fallacy. The second fallacy is that there is this
idea somehow that the desire to and the ability to dominate is linked with intelligence, right?
So this is a statement that a lot of people are making, including, you know, my friend
Jeffington recently, that somehow as soon as the machine becomes intelligent, it becomes
uncontrollable because, you know, it's it being smarter than us, it can influence us in ways that
we can even imagine. Now, I think this is a gigantic fallacy, because even within the human
species, it is not the smartest among us that want to dominate the others. Okay, to dominate other
entities, you don't necessarily need to be smarter than them, but you need to want to dominate them.
This is not something that every intelligent entity is going is going to do spontaneously.
We do it as humans, because the desire to influence others was built into us by evolution,
because we are a social species. Okay, same as baboons and chimpanzees and wolves and dogs and
etc. It's not the case for orangutans. Orangutans don't have the desire to dominate anybody because
they are non-social animals, they are solitary animals, they are territorial in fact. So we need to
separate those two concepts, the will, the desire, and the ability to dominate on one hand,
and intelligence on the other hand. The fact that we're going to have super intelligent machines
at our disposal means that every one of us is going to be like a business leader, politician,
or an academic with a staff of people working for them that are more intelligent than themselves.
I mean, it's great. It's not like if you feel threatened by, you know, being the boss of other
people who work with you, but are smarter than you, you're not being a good leader.
Can I ask you, how do we instill values within models where they don't have a desire to dominate?
Right, so these objectives I was telling you about, so okay, so let me describe the
sort of architecture of future AI systems as I see it. We're going to have AI systems that
basically are going to plan their actions, and actions can include sequences of words that you
tell someone, but they're going to plan the sequence of actions or words so as to optimize
a series of objectives that we set them. Okay, so one objective is, does this answer the question
I just asked? Okay, another objective might be, or you're talking to a 13 year old, make that answer
understandable by a 13 year old. Another objective might be, you know, I asked you to answer a
question about the world, so be factual. Or it's a question about, you know, yesterday's political
event, you know, can you kind of be compatible with everything you've read in the press this morning?
Things like that, right? So you'll have those systems that have, you know, a series of objectives,
and their output, their answer by construction is going to have to satisfy those objectives.
And some of those objectives will be hardwired to make those systems safe. Like if it's a domestic
robot that can, you know, cook dinner and can wield, you know, kitchen knife in its arm, there's
going to be a term in there that says, like, stop moving your arm when there is people around,
because, you know, you might hurt them. So that's going to be an objective that the system cannot
violate, because by construction, we're going to have to satisfy them. So that's the way to build
safe AI system. You make them produce answers that by construction have to satisfy objectives,
and you design those objectives so that their actions are safe. Now, how precisely to do this
is not a completely solved question. But you try it, you deploy it at a small scale, you see what
the effect is, and you correct it when it doesn't work. And you fix it progressively. And it's not
like if you get it wrong, it's going to destroy humanity.
It depends on that cooking robot. You never know. How do you determine who's able to set the
objectives? Because there could be right or wrong depending on who sets them.
That's true. So that's going to have to be a process by which, you know, we allow people to
do this, like some vetting process. You know, the same way that, you know, there's a vetting process
for, you know, people to take care of your health or cut your hair, fix your plumbing or your car,
right? So there's some, you know, some vetting process, certainly some testing and,
you know, market deployment procedure with regulating agencies for things that have,
you know, that are potentially dangerous, probably not for all applications, but for
many applications, certainly in healthcare, transportation and things like that.
And then perhaps also, it could be that, you know, let's take the example of intelligent
assistants. So let's imagine a future where everyone can, you know, talk to their intelligent
assistant. That system will have pretty close to human-level intelligence for probably more
accumulated knowledge that most humans, you know, they could translate in any language and
probably, you know, give you a quick summary of, you know, yesterday's newspaper and things like
that, right? Explain mathematical concepts to you, things like that. So people are probably going
to use this almost exclusively in the future for their interaction with the digital world.
You know, you're not going to go to Google or Wikipedia, you're just going to talk to your
assistant. And the only way to do this properly is for the base infrastructure for those assistants.
I mean, they would be so pervasive, so much will ride on those systems that I don't think anyone
will accept that those assistants being behind the event horizon, you know, private company,
they will insist that the infrastructure is open. They will insist also that the vetting process
by which those systems are trained be something maybe like Wikipedia, right? We tend to trust
Wikipedia, sometimes with a grain of salt, but we tend to trust Wikipedia because there is a vetting
process so that whenever an article is modified, you know, some editor kind of check on it and
then the changes are accepted or not, things like that. So you can imagine that the sort of common
repository of all human knowledge that will be your assistants will be constructed through
some sort of cross-sourcing process, perhaps similar to Wikipedia, where you're going to have
a bunch of people training those systems and fine-tuning them so that, you know, whatever they
and so they produced are correct. It's so funny you say about that kind of the benefits there of
the open approach over the closed approach because that's where I've been kind of stuck,
which is like where does value accrue? Is it to the closed model or the open model? And then we
had the leaked internal Mamo stay from the Google employee who said, you know, we're not ahead,
open AI are not ahead, there's this third being which is actually far more significant and we
haven't taken notice of and summarized. And that was triggered, that was triggered by by Lama,
which is the the model that was put together by my colleagues at fair, which was the code was
open sourced. The model sadly was distributed only for research and non-commercial purpose.
And the reason for that is, is basically complicated legal issues of what's the status
of the data that the system has been trained on and things like that. So it's more kind of,
it's not a lack of desire from the from meta to open source. It's more kind of complex legal
issues that go beyond my yet. I'm super naive, Jan. Why does open win against a more controlled,
tight knit, well funded open AI or other large corporate with a big balance sheet and a very
rigorous but streamlined team? It's very simple. It's because no outfit as powerful as they may be
has a monopoly on good ideas. So if you do it in the open, you basically recruit the entire world's
intelligence to contribute to things and and having ideas and ideas that you met as has,
you know, thought about, which, you know, an outfit with 400 people has no chance
thinking about or even a large company with 50,000 employees may not want to devote any
resources to because they may not think it's useful in the long term or, or they have, you know,
more urgency to take care of. So you give it, you give it away. And then you have, you know,
tons and tons of people, some of whom are, you know, undergraduate students or people, you know,
you know, in their parents' basement. So coming up with amazing ideas that you would never had
thought about or willing to spend the time to crunch down the, you know, 7 billion weight
Lamas so that it runs on the Mac, on the laptop. Like, oh, that's pretty amazing. So I, I think
that's why, you know, open source projects succeed, particularly when they concern basic
infrastructure. So if you think about it, the early days of the internet, there was a battle
between Microsoft and SunMicro systems to provide the basic infrastructure for the internet.
You know, the operating system, the web server, you know, things like that, right?
So on SunMicro systems, it was Solaris and, you know, whatever web server and Java. And then on
the, on the Microsoft side, there was Windows with IIT or whatever, you know, an ASP, which was their
kind of server and client side protocol. Both of them lost. In fact, SunMicro system pretty much
went bankrupt and was, you know, sold for parts to Oracle. One was Linux and Apache, which is
completely open source. And you might, you might ask why the entire internet and the entire tech
industry runs on Linux, right? And your phone probably runs on Linux too, if you have Android.
So that's three quarter of the phones in, you know, in the world. So the reason for this is that,
you know, it's just a much better way of gathering competence and talent around a common project,
even if it's not motivated necessarily by, by profit.
Yeah, I agree. And I love this. You work with meta. My question and David Marcus' question was,
how does meta win then?
So it's been the case that meta in the past has open source pretty much everybody,
everything that it's ever produced in terms of basic infrastructure, right? So you have,
you know, react for, you know, the framework for web and mobile apps. You have PyTorch. PyTorch is
not even owned by meta anymore. The ownership was transferred to the Linux Foundation,
because it's so essential to the, you know, AI R&D infrastructure nowadays, you know,
which had GPT was developed on PyTorch. Okay, all open AI runs on PyTorch.
The entire world, in fact, runs on PyTorch except Google, because they have their own,
their own thing, right? But it goes beyond that, right? Meta open source is its hardware
server backplane design, so that hardware manufacturers can build to its specifications.
And pretty much everything, aside from sort of legal issues that are sometimes due to kind of
recent laws or court decisions, pretty much everything has been open sourced. It is not
because other people can use your technology that you can't exploit it to the same extent, right?
Who can use smart NLP systems for, you know, translation or content moderation
on Facebook, other than Facebook? It doesn't matter if other people have access to the same
technology. I mean, I totally agree with you. And this kind of led to my next question, which
you actually twist about, which comes to the size of like data modes and size of data availability.
Is it simply a case that the largest model wins? And how do you think about value in small models
as well? Yeah, so it's not the case. This is really what Lama has demonstrated and really
kind of shown people. So the people behind Lama, Edouard Grave and Guillaume Lample,
and then their collaborators, mostly at Fair Paris, actually, many of them are in Paris,
they've demonstrated that you don't need those models to be very large to work really well.
I think it caused a bit of an epiphany for a lot of people, realizing, oh, you know, you don't need,
okay, maybe you need a thousand GPUs, you know, running for 10, you know, a couple of weeks to
train it, the base system. In fact, this, that number is going down too, because people are
kind of figuring out how to do this more efficiently. But once it's pre-trained,
you can use it for all kinds of stuff, and you can fine tune it really easily. And then at the
end, you can run it on your laptop, right? That's kind of amazing. Or maybe on a, you know, desktop
machine with a GPU in it, or a couple of GPUs. So I think, you know, it's sort of opened the
minds of people to the fact that there is like enormous opportunities that really weren't
thought to be possible before. And I think it's going to make even more progress, because
if we go towards the design of AI systems, perhaps along the lines of what I described with
objectives and planning, I think those systems could actually be even smaller, to some extent.
How would they be even smaller? Sorry, unpack that for me.
Well, because the current models, for them to work here, to train them on gigantic amounts of
data, way more data than any humans has ever been trained on, right? So the amount of data
I lamma is trained on, for example, is something like 1.4 trillion tokens, which is a, you know,
it's like a quarter of the internet or something. It's something absolutely enormous. It would
take someone reading eight hours a day at normal speed, about 22,000 years to read through that.
So obviously, those systems can accumulate a lot of knowledge from text, but they don't do it the
same way humans do it, because we don't need that much time to be that smart and to learn
that much. So obviously, we are much more efficient, our brains are much more efficient,
than those models at learning things. Like, how is it that a teenager can run to drive a car in
about 20 hours of practice? We still don't have level five start driving cars. So obviously,
we're missing something really big. And what we're missing, I think, is abilities for AI systems
to learn how the world works by observation mostly, and then this ability to plan so as to
satisfy objectives. And then beyond that, the ability to set some objectives in the
satisfaction of a bigger one. Okay, that's hierarchical planning. And we do this, humans do
this. Some animals do this to some extent. Every animal, you know, mammal and bird is capable of
some level of planning, or very simple form of it.
Yeah, and you mentioned the efficiency that can come from actually smaller models than expected,
and how actually size of models isn't everything. The other thing that we spoke about open and
closed, the other thing that I've been thinking, and everyone's been thinking about, and I've
interviewed many kind of leading AI experts, and they say the value will accrue to the incumbents.
Startups, they don't have the data, they don't have the models, it'll accrue to the incumbents.
Is that right? Will the value accrue to the incumbents, or do you believe that given what
you just said about size not being everything in terms of models, it could be startups as well?
So it depends on which scenario you believe in. So the scenario I think will happen,
and I'm certainly waiting for, is the scenario described earlier where you have some sort of
open platform for base LLMs. So base LLMs basically would be seen as the basic infrastructure,
like TCP, IP, Linux, Apache essentially, completely open, and then there would be an
ecosystem of companies building stuff on top of it, which for vertical applications for specific
things, right, to specialize those systems for particular applications who offer support to
make it customized for enterprise applications, for personal things. There'll be like a whole
economy around this which will create jobs, by the way, not make them disappear.
So this is the scenario that I believe will happen, and the reason I think it will happen is
because there is essentially a need to use, essentially millions of contributions for making
those systems factual and correct and etc., so Wikipedia style. So I think the proprietary
approaches will actually fall behind. So that's one point, okay. The second point is,
you can ask yourself the question, how is it that the companies that were best positioned
to produce something that said GPT, namely Google and Meta, didn't? Why is it open AI?
Okay, the small ad sheet was, you know, 400 people, I mean, more now, but actually small ad
sheet. And the answer is, it's not because Google or Meta did not have the competence
of the technology, it's just that they didn't have the pressure to produce new products,
completely new products that had a lot of risk attached to them. And the risks were,
and we know where the risks are, because a few weeks before Chad GPT, my colleagues at
fair, produced a large language model called Galactica, which was an experimental system,
and they put out a demo, and the demo was to demonstrate that. So Galactica was a large
language model trained on the entirety of the scientific literature. And it was basically
designed to help scientists write papers. So you would start writing a paragraph or something
like that to describe the topic of paragraphs. And then Galactica would basically complete the
paragraph, and it wouldn't be factually correct, you would have to kind of fix it, but you would
ask it to build a table or result, and it would just put the latech commands to kind of build
thing and populate it with the known results on the literature about the topic that you're
working on, or you would type a chemical formula for something, and it would turn it into an
actual name for it, or things of that type, but very useful for scientists. As soon as the demo
was put out, it was murdered by the social network Twitter sphere. Why? People said, oh, this is going
to destroy scientific publication, because now any random person can write an authoritatively
sounding scientific paper that is nonsense. And there was so much material thrown at the
system that the people at Meta who built it couldn't take it, they took down the demo because
they said, we can sleep at night. So here is an example of a very useful system, a system that
could have been extremely useful, particularly for writers or scientific papers who are non-native
English speakers, that basically was destroyed by AI do-mers. People who just did not think about
the risk-benefit analysis, the risk of flooding the literature with nonsense is ridiculous. I mean,
because, you know, the scientific publications are vetted and things like that, so there was not
a significant danger. And then change of it came two weeks later and was welcomed as the second
coming of the Messiah, right? So what does that tell you? And then, you know, a few months later,
Google came out with a bard and in the demo, Bard made a tiny, you know, minor factual mistake
about some astronomical fact and, you know, Google's stock went down by 8%. Now, what that
tells you is that when something is produced by a large company that has a reputation, particularly
a reputation to defend, they can put out things that's true nonsense, but it's okay for a small
company. So that's the landscape of what happens now, which is why I think there's a bit of a
paradox, which is that the companies that have, you know, the best technology basically can't
have difficulties putting it out because of those legal issues and sort of public image.
Do you not also think there's this core business model challenge there, which is,
it's the classic innovators dilemma? Like, why didn't Google do this? Because it would have killed
that absolute cash cow of Google ads. The cost to service a query versus the costs of this
is so significantly different, you'd be killing your core cash cow with this, with unknown upside
versus retaining what is a great business. You don't have a choice. I mean, there's no question
that, you know, within some time, you know, it could take a while, but there is no question that
people interact mostly with the digital world using AI assistance. And, you know, they may run into
your augmented reality glasses, okay, so, or something of that type. Like, you know, like in
the Spike Jon's movie, Her, that's not a bad depiction of what, you know, the way things could
develop. And so, if you take the assumption, if you make the assumption this is going to happen,
you have to build it as quickly as you can. And it might cannibalize your news feed algorithm or
whatever, or because of Google, your search engine, but you have to do it. You know, it's like,
I mean, Meta has been known to make those choices in the past, like the move to mobile, for example,
and the move to, you know, short form video, for example, you know, which, you know, obviously,
TikTok has been very successful at. Meta has entered that business in kind of a big way,
despite the fact that the amount of revenue it derives from it is lower than a traditional news
feed, because it's hard to, you know, put ads and videos basically. You mentioned the job creation
element there. I do just want to touch on the job side, because it's the classic AI doomer that's,
we're all going to be unemployed, and we're going to have universal basic income in an optimistic
world. You said about job creation there. We don't hear about job creation through AI. How do you see
what jobs will be created through this new ecosystem, and what that world of employment
could look like? So, 100 years ago, or maybe 120 years ago, most people in most of the world
worked in the fields in food production. There's pretty much a majority of the population.
Today, in developed countries, it's between one and two percent.
And that has caused, you know, a migration of people into the cities, and, you know, the
development of service, business, you know, the same thing 20 years ago, or, you know,
20, 30 years ago, there was a big movement towards automation of manufacturing. And a lot of
manufacturing jobs disappeared in developed countries, but they were replaced by other things.
So, 20 years ago, like, who would have thought that you could make a living with a podcast?
I didn't think I could five years ago, Jan. I'm just surprised it's everyone else.
Right. So, you know, a lot of jobs appear, like, you know, 30 years ago, there was no such thing as
web designer, and now it's, you know, half engineers in the world basically do this, right?
So, you know, the number of economists that I have talked to, which is pretty large, about
where I asked that question, we tell me, well, we're going to run out of jobs because, you know,
we're all going to be replaced by, I think, is exactly zero. Like, no economist believes this.
No economist believes we're going to run out of jobs because no economist believes that we're
going to run out of problems to solve or requirement for human creativity and human
communication and stuff like that. So, you know, this is going to create as many jobs as it makes
disappear. Now, the question is, though, and those jobs, by the way, are going to be more
productive. So, overall, technology makes people more productive. In other words, for the same
amount of hours worked, you produce more wealth, okay? But every technological revolution unless
it's accompanied by sort of, you know, political changes and social changes generally profit a
small number of people, at least temporarily, right? That happened in the industrial revolution
in the late 19th century, where, you know, a few people became extremely rich and a lot of people
were exploited. And then, you know, society changed and there were like social programs and,
and, you know, income tax and high tax for richer people and stuff like that, which the US has
backpedal on this, but not Europe, or the UK to some extent, too, but not the rest of Europe.
Um, so there is a question of, you know, how you distribute the wealth if you want, okay?
How do you organize society so everyone profits from it? But that's a political question. There's
another technology question. It's not new. It's not caused by AI. It's just caused by
technological evolution, right? It's not a recent, a recent phenomenon.
This is so unfair of me to ask. But what do those jobs look like? Like, what are they? Are they
they're creative oriented? But what does that actually mean? Like, sorry, I know it's a really
hard question, but I'm just trying to understand how, how we actually spend our time and my children,
which I don't have, by the way, Ann, but what, what do they do? Like sculpt or paint? I don't know.
I don't know. That's a good question. But it's not because I don't know that it won't happen,
because I mean, look at like how many people exercise their creative juices today, right,
with all the tools that are available that, you know, weren't available 10, 20 or 30 years ago.
Like 3D artists or something like this, you know, game designers, you know, all kinds of things.
Like, you know, I think creative jobs are the other ones. So there are two types of job set that,
you know, have a bright future of creative jobs, whether they are scientific, technical,
educational, or artistic. ACI has to do with communication, right? And communication of
human emotions, which is, you know, intrinsically human, if you want. So that's one category.
And then the other one is personal services. So where you need actual people to interact with you.
I totally agree and get you. And I love that. We shall see class. The only thing that I worry
about is the speed of transition. Like when you look at past industrial revolution,
when you think even the introduction of PCs into kind of, you know, working environments,
these were multi decade introductions. Bluntly, what AI feels like in some industries today,
we use it at the media company, and it's cutting our employee like the speed of transition is much,
much more compressed in this timeline, which will lead to short term significant high unemployment.
Do you concede that? Or do you not concede that? So this is something I used to be very worried
about, that the speed of progress of technology was going to leave a certain number of people behind
who, you know, cannot be basically retrained fast enough, or maybe they are too old to
retrain themselves for the new world. I was worried about this. And then I talked to a bunch
of economists and they say, oh, you know, not really, because the speed at which a technology
disseminate in the economy is actually limited by how fast people can learn to use it.
So a good person to talk to about this is Eric Brinjofsen at Stanford. And what he says is that
when a new technology is introduced, let's say the PC, right, with, you know, graphical user interface,
mouse, et cetera, right, in the mid 90s. How long did it take to have a measurable effect on
productivity, you know, which is the amount of wealth produced by per hour worked?
He says, you know, typically it's 15, 20 years. And the reason is that that's what it takes for
people to learn to use that new technology basically. But do you buy that here? Like people
are pretty good at prompts, you know, social media content managers are using prompts very
efficiently to produce content plans to create content ideas in under half an hour after watching
a couple of TikToks. Yeah. But like, what is going to be the effect of this on, first of all, on
measurable productivity? Second of all, on the job market, like, is it going to make people lose their
job like right away? And no, it's going to take a while. It's going to take 10, 15 years, you know,
possibly more. It depends when you start counting, right, because the AI revolution maybe started
10 years ago. So if you start counting then, then it might only take, you know, another 10 years.
But, you know, I mean, I don't think you want to underestimate the degree of conservativeness
of the business world, right? Things tend to change not that quickly. But if it's that easy to
learn, like people will learn it and then invent new professions out of it, or become more productive
themselves. Why do you think we love the doom, Jan? You know, I love your approach in mindset,
and I agree with it. But why do you think we are kind of magnetized to like, oh, we're all going
to be unemployed in the doom? Well, because I think for a number of reasons, so I'm not a,
you know, social psychologist or sociologist, but but clearly, I think we're hardwired to pay
attention to things that occur or may occur that could be dangerous to us. Because it means that
there's something about the world that we don't completely understand and we do have to pay
attention to it and be careful about it. So for example, take a young child, five-month-old,
and show a scenario to this small child of a little car that is sitting on the platform,
and then you push the car off the platform, and instead of falling, the car appears to float in
the air. A five-month-old will barely pay attention to it. But if you show this to a 10-month-old,
the 10-month-old will look at it with huge eyes and stare at it for a long time, wondering what's
going on. Because in the meantime, babies around the age of, you know, between six and nine months
learn about gravity. They learn that objects that are not supported are supposed to fall.
And so their mental model is that an object is not supported, should fall, and they see the
subject that appears to float in the air, and they say, like, this can't be. It's like, you know,
there's something I didn't, I didn't, I don't understand about the world. I need to look at
this and investigate. Okay, so we're hardwired for this because that's the way we learn our
eternal mental model of the world that allows us to predict what's going to happen, allows us to plan.
That's what makes us smart. That's the basis of intelligence, the ability to predict.
And so we naturally pay attention to stuff that is surprising or dangerous, or both, which is why,
you know, you see a outrageous piece of news, you know, a quick bait at the bottom of some,
you know, website. And like, you have to convince yourself not to click on it.
Can I ask you a couple of direct questions? I'm just too interested and we can take them
out if needed. What did you say to Jeff when you heard that he was obviously making moves that he
did? I'm sure you had a conversation with him. What did you say to him? We haven't spoken yet,
actually. We're going to speak to kind of get, you know, each other's opinion on it. I don't think
he knows my opinion on this because I don't think he follows, you know, what I post on
Twitter or whatever, even though he is on Twitter himself. But so I think we have,
you know, a discussion to have. I've had this discussion before with Yoshua Bengio,
but not with Jeff. And to me, the fact that he left Google is not particularly a surprise.
The fact that he leaves Google to be able to speak his mind, I think is not surprising.
So I have a very different deal at META, which is that I say whatever I want. Okay. I'm not under
the tight control of, you know, the communications department or anything. I just say what I think.
All right. How did you get that deal? Yeah. But no, seriously, many of my friends at META in
very high positions, as you know, with mutual friends, they don't have that deal.
So there is, I mean, a particularly sweet spot because I have quite a bit of following people
who trust me or believe me or want to hear what I have to say, even if they don't trust me at all.
And at the same time, I'm not an officer. So it's not like, you know, there are things I can't
say because of legal issues of, you know, financial blah, blah, blah, right. I'm a vice president,
but I'm just below the level where you have to be really, really careful and so control your
message. And I think there is a cost-benefit tradeoff here of, you know, AI is such a
complicated, fast-evolving issue that you basically, you need someone to be able to, you know, speak
freely. And I think Jeff didn't feel like he had that option at Google, maybe, you know, for various
reasons. So I understand why he might have wanted to leave. But I don't agree with him at all with
the whole sort of, you know, probability of human extinction or whatever.
Have you ever felt your role at MESA has impeded your ability to be impartial?
I don't believe so, no. But I mean, there are certain things that I would post on social media
that are kind of, you know, kind of popping up the work of my colleagues. And, you know, I'm obviously
biased about this because, you know, I know about the work and they are friends and or colleagues.
And, you know, I think it's interesting probably because, you know, I feel like I totally understand.
For this kind of stuff, I might be biased. Take this with a grain of salt. You don't have to believe
me. You know, things like that. But it's given me a vision also of, you know, how things are built,
what the problems are. So, you know, for example, there's a narrative, a very, very common narrative
that AI is the culprit for a lot of the bad side effects of social networks in the past.
And in fact, it's completely backwards. AI is the solution to those problems.
So, you know, let me tell you, you know, go back like, you know, backpedal 12 years ago or something,
you know, even before I joined Meta, where Meta, you know, started experimenting with the news feed.
And the news feed was, you know, an algorithm that would pick like which piece of news to show to
everyone. And, you know, originally it was decided by, you know, how friends are you with a person
making the post and things like that, right? How many interactions you have with that person.
Eventually, a bit of machine learning was put into it shortly before I joined Meta. It was very,
very simple. It was something like logistic regression, something like the simplest
method you can imagine with a lot of engineering behind it and a lot of, you know, hacks by hand
and special cases. But basically, it was something like logistic regression, you know, some big
vector that describes, you know, what you click on, like how many times you, you know, how much time
you spent on a particular piece of content and blah, blah, blah. And then it would decide, like, you
know, give a rating to everything. So, that was deployed. And people ended up spending more time
on Facebook. But then also, it created problems that were quickly identified, like, you know,
information bubbles in the context of political discourse. And the fact that what I was talking
about earlier, that people tend to click on things that is more outrageous, right? So, it caused
the, you know, the appearance of clickbait companies that basically were just like farms of, you
know, teenagers in Montenegro or someplace making false news to get people to click on them and
get money from the ads that they show them. So, then, you know, this was realized, there were,
like, big groups at Facebook at the time kind of studying the, what the effect of those things are,
and this was corrected. So, that's the way you make some work, right? You try the small scale,
you see what the effect is, if there is bad side effect, you correct it, and then you sort of
compare, you know, to two systems. And then sometimes something unexpected occurs, and you
have to back battle and completely change the way you do things. That's what happened in 2017,
after the presidential election, American presidential election in 2016, the main
new seed algorithm was completely changed. So, that, you know, there was no clickbaits anymore.
There was no, like, you know, news outlets that could, like, push their content that was propaganda,
basically, you know, much more effort to take down false accounts and attempts to corrupt the
democratic system and stuff like that, right? So, you correct it. And then what the progress of AI
over the last few years basically allowed systems to be deployed to do things like taking, take down
hate speech relatively reliably in hundreds of different languages, which was basically impossible
to do before. You mentioned correct it, I promised last question, then we'll do a quick fire. You
mentioned correct it, Elon Musk said with Tucker Carlson, the trouble with AI is you can't release
and then correct. Unlike all prior technological developments, once released, it is too powerful
to be able to bring back into the box. It cannot be amended in that way. Is that not true?
That's not true. That's completely false. It makes an assumption, which Elon and that some other
people may have become convinced of by reading, you know, Nick Bokstrom's book, Super Intelligence,
or, or reading, you know, some of Eliezer Yudkowski's writing. So this is predicated on an assumption
that is just false, which is the existence of hard takeoff, right? So the fact that
the minute you turn on a super intelligent AI system is going to take over the world
and is going to escape your control. And it's going to refine itself to be even more intelligent.
And so, you know, and the world will be destroyed. And that's just ridiculous. It's just completely
ridiculous because there is no process in the real world that is exponential for very long.
You know, those systems will have to like recruit all the resources in the world.
They will have to be given, you know, limitless power agency. Like, why would we do this?
And what's more, they will have to be built so that they have a desire to take over.
Like, you know, systems are not going to take over just because they are intelligent.
Because again, you know, even within the human species, it is not the most intelligent among us
that want to dominate others. So his desire and many other leaders desire to prevent
any further development and to regulate intensely right now and stop all progression
is BS, basically. It's obscurantism, right? It's like, it's like people who wanted to
start the printing press and the diffusion of printed books, because, you know, if people
could read the Bible for themselves, they wouldn't have to talk to priests anymore and then would have
their own idea about religion. And that's exactly what happened. People read the Bible for themselves
and that created the Protestant movement in Europe. And that created 200 years of religious
conflicts. But it also brought to us the enlightenment, science, rationalism, philosophy, ideas of
democracy, and then the French and American revolutions. And then, you know, you can compare
this with the Ottoman Empire, which for reasons of being able to control their population,
you know, basically stopped, forbid the use of the printing press. And it started 300 years of
decline. They were dominating science in the Middle Ages, the Muslim world, which is why,
you know, every star in this guy has an Arabic name. I love this. I'm going to do a quick fire
around with you now. So I say a short statement and you give me your immediate thoughts and then
we'll rock and roll. Does that sound okay? Sounds good. So which regions most need to change their
modus operandi when it comes to the practice of scientific research and incentive mechanisms?
Which regions? Oh, wow. Pretty much every region. I'm afraid, but for different reasons.
So you start with China. So China has a bit of an epidemic of bad science. There's a lot of very
smart people in China, a lot of very good researchers, a lot of very good work coming out
of China, particularly in AI, particularly in computer vision. But a lot of absolutely terrible
work that has to be retracted a few months later, it's been published. And it's probably because
of the incentive mechanisms in the academic and system in China. So there's a volume to fix there.
I can move to Europe. So in Europe, there are good things. So the education system for
like undergraduate education in Europe is great. It's fantastic, because it's party free. So that
allows talented people to go to the schools even if they're not rich, which is not the case in the
US, for example, at least not to the same extent. That's good for Europe. A lot of European engineers
and scientists are great atop this in the world. But then what are the opportunities for people who
want to go into science and research? And most European countries actually don't have systems
that really encourage this and motivate the most talented people and students to go into science.
And so some of them go to North America, like me, 35 years ago. There are opportunities now
that are really good in research labs, like fair in Paris, or Google also has labs in Paris.
Actually, my brother works at Google in Paris. So there are other outfits. So that gives opportunities
for people who really want to be productive and don't think that they can in the public
research and academic system in France and the rest of Europe. The only European countries that
can rival the US in terms of the quality of job for an academic or a scientist is Switzerland.
What do you think they do to rival that? What is it about their incentive mechanism
structure that gives them that ability? Two things. They pay people better.
Second thing is they give them resources for research. They can get extra resources to grants
and stuff like that, but they're good. And then they also attract some of the best students in
the world. So you get the ideal combination that you only get in the top 30 universities in North
America. So we've got China, we've got Europe. What about the US? What could they do differently
or improve? Well, so the US does right in terms of research, which is to a large extent
a bit of a partial explanation for the success of the technological industry, the tech industry in
the US. I think partly because the US devotes a significant amount of resources to fundamental
research through NSF and NIH and various other outfits, probably more than Europe.
Universities pays their faculty pretty well, particularly in areas like computer science
and AI. Now this comes with a downside. The downside is that studying in the US is expensive.
It's a trade-off, right? So can you do one without the other? Switzerland figured out how to pay
academics pretty well while actually of free education to their students. So there is a way
to do it. Canada also figured out a pretty good trade-off as well. So in other things,
the US does right. But one thing that the US system or the rough does right also is
the willingness to take risk and invest on ideas that seem a little crazy, but basically
the sort of vibrant startup scene in Silicon Valley and other places in the US, in New York,
and in the Boston area is leading the world. Now you start seeing a similar thing in Europe now.
There's been enormous growth, for example, of tech startups in Paris, in Paris area,
in France more generally, and continental Europe, more widely in the UK as well. And so I think
that's a good thing, but it's still a little more difficult to have access to investment
money in Europe than it is in the US. That's why I'm here, Jan. I'm happy to provide it.
I'm going to do an ultimate one for you. When you think about what you'd most like
someone listening to take away, what would it be when they hear this? What do you want
them to take away as the number one thing? AI is going to bring a new renaissance for
humanity, a new kind of new form of enlightenment, if you want, because AI is going to amplify
everybody's intelligence. It's like every one of us will have a staff of people who are smarter
than us and know most things about most things and most topics. So it's going to empower every
one of us. It's going to make us more creative because we're going to be able to produce text,
art, music, videos without necessarily having all the technical skills that are currently
required for doing those things. And so exercise our creative juices. So that's the positive side.
There are risks. There's no question. But it's not like those risks. Don't believe the people
who tell you that those risks are inevitable or that they will inevitably lead to catastrophe.
That's just not true. It's like, place yourself in 1920. Who would have thought that a mere 50
years later, you could cross the Atlantic in a few hours in complete safety near the speed of sound?
Would people seriously want to ban aviation or call for regulation of jet engines before
jet engines existed? I mean, that's kind of insane. So I'm not against regulation. There
should be regulation of AI products, particularly the ones that involve making critical decisions
for people. But regulating or slowing down research is complete nonsense. It's just obscurantism.
Who's incumbent teams you most respected admire when you look at Amazon, Facebook,
Google, in terms of their approach and talent internally, outside of meta, obviously.
So this is changing a lot. And the reason it's changing is because a lot of people are leaving
large companies and large labs. And the reason they're doing this is that until recently a lot of
AI research was very exploratory. And now there's a path towards commercialization for a lot of
things. And so people think that they're better off just leaving large companies and doing this on
their own, doing a startup and things like that. So you see a largely large motion of applied
research engineers, a few scientists basically leaving those labs to do startups. And that's
across the board. So you look at the original paper from Google about BERT or Transformers,
the thing that revolutionized NLP. All of them have left. They're all in startups. Some of the
people who produced Lama, the open source LLMs from meta. So the key people have left already,
okay, to do startups. I saw their companies. Yeah, there's one called Mistral. That's the one I saw.
Yeah. Right. But they, yeah, there is insane amount of money in days. Yeah. So you know,
more power to them. But I mean, I'm sad that they'd have to meta, I would just say, but
he's existing to me like, yeah, they're good. Yeah. So, but I think in terms of the basic
competence and the people who are going to push the science forward, because what we need now
is not to work on applications of LLMs. There's a lot of people who are capable of doing this and
they're going to do the job. What we need to do, people like me who are reworking on research,
is kind of coming up with new concepts that will allow us to, you know, get machines that
basically have common sense, have an experience of the real world, have, you know, basically
human level intelligence, right? And, you know, in my opinion, the, the outfits that are best
positioned for this are fair from on one side. And the new DeepVind now, which is, you know,
DeepVind Purse Group of Brain. Yeah. There are a lot of people there who are interested in that
question. And I think they are probably the best together with meta and fair, the best positions
to position, to really kind of have an impact on this, something, you know, all of us have been
working on for quite a while. Jan, if we do this again in 10 years time, where is Jan in 10 years
time in 2033? Well, I'm 63. So, you know, 10 years on now, well, 12 years on now, I'll be,
I'll be Jeff into the age. Okay. And I don't know. I think I'm excited, like,
like a teenager now, because I see the opportunity of like the next step in AI and
opportunity perhaps to, you know, get to the goal that I set myself so that I imagined for
myself when I started working on AI many years ago, which of course I was very naive about
at the time of understanding intelligence, first of all, and it's a scientific question.
What is intelligence? What is human intelligence? And one good way as an engineer, a good way to
understand intelligence is to build a widget that actually reproduces it, right, to some extent.
So, I'm excited about this right now. I'll find the, you know, the
the substrate, the linescape, the location, the position where I can make the
best contributions to this. And currently, that just happens to be fair at Meta.
I keep a foot in academia because I think it's very complementary and also important.
There are projects of different types that you do in academia and industry that are
complementary. So, I like the combination of the two. As long as my brain keeps working,
that I think I can contribute and that I've given the means to contribute, I'll keep working.
And then there's some point where my brain will turn into white sauce or
I'm totally out of it or something and I'll stop.
Yeah, and I want to say personally, thank you so much. I speak for many, I'm sure, when I say
we've learned so much from you in terms of your public speaking and discourse and willing to
speak. I think few are willing to speak as openly as you have been. So, thank you for
educating so many of us. And thank you so much for joining me today, Ann.
Well, thank you so much for having me, Harry. This was fun.

What flavors of catastrophic risk are we talking about here?
What's your favorite flavor in terms of ice cream?
Mine is coconut.
Nobody seems to like coconut ice cream.
So ice cream aside, what do you most worry about in terms of catastrophic risk that will
help us kind of make concrete the discussion we're having about how to fix this whole thing?
I think it's worth taking a historical perspective briefly to just kind of orient everyone to
it.
We don't have to go all the way back to the aliens who've seen all of civilization, but
to just recognize that for all of human history, as far as we're aware, there were existential
risks to civilizations, and they happened.
There were civilizations that were killed in war, that tribes that were killed in tribal
warfare or whatever.
People faced existential risk to the group that they identified with.
It's just those were local phenomena.
It wasn't a fully global phenomena, so an empire could fall, and surrounding empires
didn't fall.
Maybe they came in and filled the space.
The first time that we were able to think about catastrophic risk, not from like a solar
flare or something that we couldn't control, but from something that humans would actually
create at a global level was World War II and the bomb.
This was the first time that we had tech big enough that could actually mess up everything
at a global level, that could mess up habitability.
We just weren't powerful enough to do that before.
It's not that we didn't behave in ways that would have done it.
We just only behaved in those ways at the scale we could affect.
It's important to get that there's the entire world before World War II, where we don't
have the ability to make a nonhabitable biosphere, nonhabitable for us.
Then there's World War II, and the beginning of a completely new phase where global, human-induced
catastrophic risk is now a real thing.
That was such a big deal that it changed the entire world in a really fundamental way,
which is when you study history, it's amazing how big a percentage of history is studying
war.
In the history of wars, you study European history and whatever, it's generals and wars
and empire expansions.
The major empires near each other never had really long periods of time where they weren't
engaged in war or preparation for war or something like that.
Humans don't have a good precedent in the post-tribal phase, the civilization phase
of being able to solve conflicts without war for very long.
World War II was the first time where we could have a war that no one could win.
The superpowers couldn't fight again.
They couldn't do a real kinetic war.
They could do diplomatic wars and Cold War-type stuff, and they could fight proxy wars through
other countries that didn't have the big weapons.
Mutually assured destruction, and coming out of World War II, we actually realized that
nation-states couldn't prevent world war.
We needed a new type of supervening government in addition to nation-states, which was the
whole Bretton Woods world, the United Nations, the World Bank, the IMF, the Globalization
Trade-type agreements, mutually assured destruction, that was, how do we have some coordination
beyond just nation-states between them since we have to stop war between at least the superpowers?
And it was pretty successful, given that we've had like 75 years of no superpower on superpower
war.
We've had lots of proxy wars during that time.
We've had Cold War.
And I would say we're in a new phase now where the Bretton Woods solution is basically over,
almost over.
Can you describe the Bretton Woods solution?
Yeah.
So the Bretton Woods, the series of agreements for how the nations would be able to engage
with each other in a solution other than war was these IGOs, these intergovernmental organizations,
and was the idea of globalization.
Since we could have global effects, we needed to be able to think about things globally,
where we had trade relationships with each other, where it would not be profitable to
war with each other.
It would be more profitable to actually be able to trade with each other.
So our own self-interest was going to drive our non-war interest.
And so this started to look like, and obviously this couldn't have happened that much earlier
either because industrialization hadn't gotten far enough to be able to do massive global
industrial supply chains and ship stuff around quickly.
But like we were mentioning earlier, almost all the electronics that we use today, just
basic cheap stuff for us, is made on six continents, made in many countries.
There's no single country in the world that could actually make many of the things that
we have and from the raw material extraction to the plastics and polymers and the et cetera.
And so the idea that we made a world that could do that kind of trade and create massive
GDP growth.
It all worked together to be able to mine natural resources and grow stuff.
With the rapid GDP growth, there was the idea that everybody could keep having more without
having to take each other's stuff.
And so that was part of kind of the Bretton Woods post World War II model.
The other was that we would be so economically interdependent that blowing each other up
would never make sense.
That worked for a while.
Now it also brought us up into planetary boundaries faster, the unrenovable use of resource and
turning those resources into pollution on the other side of the supply chain.
So obviously that faster GDP growth meant the overfishing of the oceans and the cutting
down of the trees and the climate change and the mining, toxic mining tailings going into
the water and the mountaintop removal mining and all those types of things.
That's the whole consumption side of the rest that we're talking about.
And so the answer of let's do positive GDP is the answer rapidly and exponentially obviously
accelerated the planetary boundary side.
And that started to be, that was thought about for a long time, but it started to be modeled
with the Club of Rome and limits of growth.
And it, but it's just very obvious to say, if you have a linear materials economy where
you take stuff out of the earth faster, whether it's fish or trees or, or, or you take or
oil and you take it out of the earth faster than it can replenish itself and you turn
it into trash after using it for a short period of time and put the trash in the environment
faster than it can process itself.
And there's toxicity associated with both sides of this.
You can't run an exponentially growing linear materials economy on a finite planet forever.
That's not a hard thing to figure out.
And it has to be exponential if there's an exponentiation in the monetary supply because
of interest and then fractional reserve banking and to then be able to keep up with the growing
monetary supply.
You have to have growth of goods and services and so that's that kind of thing that has happened.
But you also see that when you get these supply chains that are so interconnected across the
world, you get increased fragility because a collapse or a problem in one area then affects
the whole world in a much bigger area as opposed to the issues being local, right?
So we got to see with COVID and an issue that started in one part of China affecting the
whole world so much more rapidly than would have happened before Bretton Woods, right?
Before international travel supply chains, you know, that whole kind of thing.
And with a bunch of second and third order effects that people wouldn't have predicted,
okay, we have to stop certain kinds of travel because of viral contaminants, but the countries
doing agriculture depend upon fertilizer they don't produce that is shipped into them
and depend upon pesticides they don't produce.
So we got both crop failures and crops being eaten by locusts in scale in Northern Africa
and Iran and things like that because they couldn't get the supplies of stuff in.
So then you get massive starvation or future kind of hunger issues because of supply chain
shutdowns.
So you get this increased fragility and cascade dynamics where a small problem can end up
leading to cascade effects.
And also, we went from two superpowers with one catastrophe weapon to now that same catastrophe
weapon is there's more countries that have it, eight or nine countries that have it,
and there's a lot more types of catastrophe weapons.
We now have catastrophe weapons with weaponized drones that can hit infrastructure targets
with bio with, in fact, every new type of tech has created an arms race.
So we have not with the UN or the other kind of intergovernmental organizations, we haven't
been able to really do nuclear deproliferation.
We've actually had more countries get nukes and keep getting faster nukes, the race to
hypersonics and things like that.
And every new type of technology that has emerged has created an arms race.
And so you can't do mutually assured destruction with multiple agents, so you can with two
agents.
Two agents, it's a much easier to create a stable Nash equilibrium that's forced.
But the ability to monitor and say, if these guys shoot, who do I shoot?
Do I shoot them?
Do I shoot everybody?
Do I?
And so you get a three body problem.
You get a very complex type of thing when you have multiple agents and multiple different
types of catastrophe weapons, including ones that can be much more easily produced than
nukes.
Nukes are really hard to produce.
Uranium in a few areas, uranium enrichment is hard, ICBMs are hard.
But weaponized drones hitting smart targets is not so hard.
There's a lot of other things where basically the scale at being able to manufacture them
is going way, way down to where even non-state actors can have them.
And so when we talk about exponential tech and the decentralization of exponential tech,
what that means is decentralized catastrophe weapon capacity.
Especially in a world of increasing numbers of people feeling disenfranchised, frantic,
whatever for different reasons.
So I would say the Bretton Woods world doesn't prepare us to be able to deal with lots of
different agents, having lots of different types of catastrophe weapons you can't put
mutually assured destruction on, where you can't keep doing growth of the materials economy
in the same way because of hitting planetary boundaries and where the fragility dynamics
are actually now their own source of catastrophic risk.
So now we're, so like there was all the world until World War II, and World War II is just
from a civilization timescale point of view was just a second ago.
It seems like a long time, but it is really not.
We get a short period of relative peace at the level of superpowers while building up
the military capacity for much, much, much worse war the entire time.
And then now we're at this new phase where the things that allowed us to make it through
the nuclear power are not the same systems that will let us make it through the next stage.
So what is this next post Bretton Woods?
How do we become safe vessels, safe stewards of many different types of exponential technology
is a key question when we're thinking about X-Risk.
Okay.
And I'd like to try to answer the how a few ways, but first on the mutually assured destruction,
do you give credit to the idea of two superpowers not blowing each other up with nuclear weapons
to the simple game theoretic model of mutually assured destruction or something you've said
previously, this idea of inverse correlation, which I tend to believe between
now you were talking about tech, but I think it's maybe broadly true the inverse correlation
between competence and propensity for destruction. So the better the bigger your weapons, not
because you're afraid of mutually assured self-destruction, but because we're human
beings and there's a deep moral fortitude there that somehow aligned with competence
and being good at your job. It's very hard to be a psychopath and be good at killing at scale.
Do you share any of that intuition?
Kind of. I think most people would say that Alexander the Great and Genghis Khan and Napoleon
were effective people that were good at their job, that were actually maybe asymmetrically
good at being able to organize people and do certain kinds of things that were
pretty oriented towards certain types of destruction or pretty willing to,
maybe they would say they were oriented towards empire expansion, but pretty willing to
commit certain acts of destruction in the name of it.
What are you worried about? The Genghis Khan or you could argue he's not a psychopath.
Are you worried about Genghis Khan? Are you worried about Hitler or are you worried about
a terrorist who has a very different ethic, which is not even for, it's not trying to preserve and
build and expand my community. It's more about just destruction in itself is the goal.
I think the thing that you're looking at that I do agree with is that there's a psychological
disposition towards construction and a psychological disposition more towards destruction. Obviously
everybody has both and can toggle between both and oftentimes one is willing to destroy certain
things. We have this idea of creative destruction, right? Willing to destroy certain things to create
other things and utilitarianism and trolley problems are all about exploring that space and
the idea of war is all about that. I am trying to create something for our people and it requires
destroying some other people. Sociopathy is a funny topic because it's possible to have very
high fealty to your in-group and work on perfecting the methods of torture to the out-group
at the same time because you can dehumanize and then remove empathy.
I would also say that there are types. The reason, the thing that gives hope about
the orientation towards construction and destruction being a little different in
psychology is what it takes to build really catastrophic tech even today where it doesn't
take what it took to make a small group of people could do it. It takes still some real
technical knowledge that required having studied for a while and some then building capacity.
There's a question of is that psychologically inversely correlated with the desire to
damage civilization meaningfully? A little bit I think.
I think a lot. I think it's actually, I mean this is the conversation I had with,
I think offline with Dan Carlin which is like it's pretty easy to come up with ways that
any competent, I can come up with a lot of ways to hurt a lot of people and it's pretty easy.
Like I alone can do it and there's a lot of people as smart or smarter than me at least in
their creation of explosives. Why are we not seeing more insane mass murder?
I think there's something fascinating and beautiful about this and it does have to do with
some deeply pro-social types of characteristics in humans. But when you're dealing with very large
numbers you don't need a whole lot of a phenomena and so then you start to say well what's the
probability that X won't happen this year then won't happen in the next two years, three years,
four years and then how many people are doing destructive things with lower tech and then
how many of them can get access to higher tech that they didn't have to figure out how to build.
So when I can get commercial tech and maybe I don't understand tech very well but I understand
it well enough to utilize it not to create it and I can repurpose it. When we saw that
commercial drone with a homemade thermite bomb hit the Ukrainian munitions factory and do the
equivalent of an incendiary bomb level of damage, that was just home tech. That's just simple kind
of thing. And so the question is not what is does it stay being a small percentage of the
population? The question is can you bind that phenomena nearly completely?
And especially now when you as you start to get into bigger things,
CRISPR gene drive technologies and various things like that, can you bind it completely long term
over what period of time? Not perfectly though. That's the thing. I'm trying to say that there is some
let's call it a random word love that's inherent in that's core to human nature.
That's preventing destruction at scale and you're saying yeah but there's a lot of humans.
There's going to be eight plus billion and then there's a lot of seconds in the day to come up
with stuff. There's a lot of pain in the world that can lead to a distorted view of the world such
that you want to channel that pain into the destruction all those kinds of things and it's
only a matter of time that anyone individual could do large damage especially as we create
more and more democratized decentralized ways to deliver that damage even if you don't know how
to build the initial weapon you can but the thing is it seems like it's a race between
the cheapening of destructive weapons and the capacity of humans to express their love
towards each other and it's a race that so far I know on Twitter it's not popular to say but
love is winning okay so what is the argument that love is going to lose here against nuclear
weapons of biotech and AI and drones okay I'm gonna comment the end of this to a how love wins
so I just want you to know that that's where I'm oriented that's the end okay but I'm
I'm gonna argue against why that is a given because it's not a given I don't believe
and I think this is like a good romantic comedy so you're gonna create drama right now
but it will end in a happy ending well it's because it's only a happy ending if we actually
understand the issues well enough and take responsibility to shift it yes do I believe
like there's a reason why there's so much more dystopic sci-fi than protopic sci-fi and the in
the some protopic sci-fi usually requires magic is because or at least magical tech right dilithium
crystals and warp drives and stuff because it's very hard to imagine people like the people we
have been in the history books with exponential type technology and power that don't eventually
blow themselves up think that make good enough choices as stewards of their environment and
their commons and and each other and etc so like it's easier to think of scenarios where we blow
ourselves up than it is to think of scenarios where we avoid every single scenario where we
blow ourselves up and when I say blow ourselves up I also I mean the environmental versions
the terrorist versions the war versions the cumulative externalities versions
can I and I'm sorry if I'm interrupting your flow of thought but why is it easier
is it could it be a weird psychological thing where we either I was just more capable to
visualize explosions and destruction and then the sicker thought which is like we kind of
enjoy for some weird reason thinking about that kind of stuff even though we wouldn't actually
act on it it's almost like some weird like I love playing shooter games you know first person
shooters and like especially if it's like murdering zombie and doom you're shooting demons I play one
of my favorite games diabolus like slashing through different monsters and the screaming and
pain and the hellfire and then I go out into the real world to eat my coconut ice cream and I'm all
about love so like I can we trust our ability to visualize how all it all goes to shit as an
actual rational way of thinking I think it's a fair question to say to what degree is there just
kind of perverse fantasy and morbid exploration and whatever else that happens in our imagination
but I don't think that's the whole of it I think there is also
a reality to the combinatorial possibility space and the difference in the probabilities that
there's a lot of ways I could try to put the 70 trillion cells of your body together that don't
make you there's not that many ways I can put them together that make you there's a lot of ways I
could try to connect the organs together that make some weird kind of group of organs on a on a
desk but that doesn't actually make a functioning human and and you can kill an adult human in a
second but you can't get one in a second takes 20 years to grow one and a lot of things to happen
right I could destroy this building in a couple minutes with demolition but it took a year or
a couple years to build it there is uh I'm down cold this is just an example it's not he doesn't
mean it there's a there's a gradient where entropy is easier and there's a lot more ways to put a
set of things together that don't work than the few that really do produce higher order synergies
and so when we look at a history of war and then we look at exponentially more powerful warfare
an arms race that drives out in all these directions and when we look at a history of
environmental destruction and exponentially more powerful tech that makes exponential
externalities multiplied by the total number of agents that are doing it in the cumulative effects
there's a lot of ways the whole thing can break like a lot of different ways
and for it to get ahead it has to have none of those happen and so it there's just a probability
space where it's easier to imagine that thing so what so to say how do we have a pro-topic future
we have to say well one criteria must be that it avoids all of the catastrophic risks so can we
understand can we inventory all the catastrophic risk can we inventory the patterns of human
behavior that give rise to them and could we try to solve for that and could we have that be the
essence of the social technology that we're thinking about to be able to guide bind and direct a new
physical technology because so far our physical technology like we were talking about the gangest
cons and like that that obviously use certain kinds of physical technology and armaments and
also social technology and unconventional warfare for a particular set of purposes
but we have things that don't look like warfare like Rockefeller and Standard Oil
and it looked like a constructive mindset to be able to bring this new energy resource
to the world and it did and the second order effects of that are climate change and all of the
oil spills that have happened and will happen and all of the wars in the Middle East over the
oil that have been there and the massive political clusterfuck and human life issues that are associated
with it and on and on right and so it's also not just the orientation to construct a thing can have
a narrow focus on what I'm trying to construct but be affecting a lot of other things through second
and third order effects I'm not taking responsibility for and you often another tangent
mentioned second third and fourth order effects and order and and cascading which is really fascinating
like starting with the third order plus it gets really interesting because we don't we don't even
acknowledge like the second order effects right but like thinking because those it could it could
get bigger and bigger and bigger in ways we're not anticipating so how do we make those so it
sounds like part of the part of the thing that you are thinking through in terms of a solution
how to create an anti-friagile a resilient society is to make explicit
acknowledge understand the externalities the second order third order fourth order and the
order effects how do we start to think about those effects yeah the war application is harm we're
trying to cause or that we're aware we're causing right the externality is harm that at least supposedly
we're not aware we're causing or at minimum it's not our intention right maybe we're either totally
unaware of it or we're aware of it but it is a side effect of what our intention is it's not the
intention itself there are catastrophic risks from both types the direct application of increased
technological power to a rival risk intent which is going to cause harm for some outgroup for some
ingroup to win but the outgroup is also working on growing the tech and if they don't lose completely
they reverse engineer the tech upregulated come back with more capacity so there's the exponential
tech arms race side of in-group outgroup rivalry using exponential tech that is one set of risks
and the other set of risks is the application of exponentially more powerful tech not intentionally
to try and beat an outgroup but to try to achieve some goal that we have but to produce a second
and third order effects that do have harm to the commons to other people to environment to other groups
that might actually be bigger problems than the problem we were originally trying to solve with
the thing we were building when facebook was building a dating app and then building a social
app where people could tag pictures they weren't trying to build a democracy destroying app
that would maximize time on site as part of its ad model through AI optimization of a news feed
to the thing that made people spend most time on site which is usually them being limbically hijacked
more than something else which ends up appealing to people's cognitive biases and group identities
and creates no sense of shared reality they weren't trying to do that but it was a second order effect
and it's a pretty fucking powerful second order effect and the pretty fast one because the rate
of tech is obviously able to get distributed to much larger scale much faster and with a bigger
jump in terms of total vertical capacity then that's what it means to get to the verticalizing
part of an exponential curve so just like we can see that oil had these second order
environmental effects and also social and political effects war and so much of the whole
like the total amount of oil used is has a proportionality to total global gdp and this
way we have this you know the petrodollar and um and so the the oil thing also had the externalities
of a major aspect of what happened with military industrial complex and things like that so
but we can see the same thing with with more current technologies with facebook and google
and and other things so i don't think we can run and the more powerful the tech is we build it for
reason x whatever reason x is maybe x is three things maybe it's one thing right we we're doing
the oil thing because we want to make cars because it's a better method of individual
transportation we're building the facebook thing because we're going to connect people
socially in the personal sphere but it it interacts with it interacts with complex systems
with ecologies economies psychologies cultures and so it has effects on other than the thing
we're intending some of those effects can end up being negative effects but because this technology
if if we make it to solve a problem it has to overcome the problem the problem's been around
for a while it's going to overcome in a short period of time so it usually has greater scale
greater rate of magnitude in some way that also means that the externalities that it creates
might be bigger problems and you can say well but then that's the new problem and humanity will
innovate its way out of that well i don't think that's paying attention to the fact that we can't
keep up with exponential curves like that nor do finite spaces allow exponential externalities
forever and this is why a lot of the smartest people thinking about this are thinking well
no i think we're totally screwed and unless we can make a benevolent ai singleton that rules all of
us um you know guys like boss drum and and others uh thinking in those directions because they're
like how do humans try to do multi polarity and make it work and i i have a different answer
of what i think it looks like that does have more to do with the love but some applied social
tech aligning aligned with love because i have a bunch of really dumb ideas i'd prefer to uh i'd
like to hear i'd like to hear some of them first
you

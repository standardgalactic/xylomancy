You wrote the blog post, The 50 Year Quest, My Personal Journey, good title, My Personal
Journey with a Second Law, Thermodynamics.
So what is this law and what have you understood about it in the 50 year journey you had with
it?
Right.
So Second Law of Thermodynamics, sometimes called Law of Entropy Increase, is this principle
of physics that says, well, my version of it would be things tend to get more random
over time.
A version of it that there are many different sort of formulations of it that are things
like heat doesn't spontaneously go from a hotter body to a colder one.
When you have mechanical work kind of gets dissipated into heat, you have friction and
when you systematically move things, eventually they'll be sort of the energy of moving things
gets kind of ground down into heat.
So people first sort of paid attention to this back in the 1820s when steam engines were
a big thing.
And the big question was, how efficient could a steam engine be?
And there's this chap called Sadiq Khanou, who was a French engineer, actually his father
was a sort of elaborate mathematical engineer in France.
But he figured out these kind of rules for how kind of the efficiency of the possible
efficiency of something like a steam engine.
And in sort of a side part of what he did was this idea that mechanical energy tends
to get dissipated as heat, that you end up going from sort of systematic mechanical motion
to this kind of random thing.
Well, at that time, nobody knew what heat was.
At that time, people thought that heat was a fluid, like they called it caloric, and
it was a fluid that kind of was absorbed into substances.
And when heat, when one hot thing would transfer heat to a colder thing, that this fluid would
flow from the hot thing to the colder thing.
But anyway, then by the 1860s, people had kind of come up with this idea that systematic
energy tends to degrade into kind of random heat that could then not be easily turned
back into systematic mechanical energy.
And then that quickly became sort of a global principle about how things work.
The question is, why does it happen that way?
So let's say you have a bunch of molecules in a box and they're arranged, these molecules
are arranged in a very nice sort of flotilla of molecules in one corner of the box.
And then what you typically observe is that after a while, these molecules were kind of
randomly arranged in the box.
Question is, why does that happen?
And people for a long, long time tried to figure out, is there, from the laws of mechanics,
to determine how these molecules, let's say these molecules like hard spheres bouncing
off each other, from the laws of mechanics that describe those molecules, can we explain
why it tends to be the case that we see things that are orderly sort of degrade into disorder?
We tend to see things that you scramble an egg, you take something that's quite ordered
and you disorder it, so to speak.
It's a thing that sort of happens quite regularly or you put some ink into water and it will
eventually spread out and fill up the water, but you don't see those little particles of
ink in the water all spontaneously kind of arrange themselves into a big blob and then
jump out of the water or something.
And so the question is, why do things happen in this kind of irreversible way where you
go from order to disorder?
Why does it happen that way?
And so throughout, in the later part of the 1800s, a lot of work was done on trying to
figure out, can one derive this principle, this second law of thermodynamics, this law
about the dynamics of heat, so to speak, can one derive this from some fundamental principles
and mechanics in the laws of thermodynamics?
The first law is basically the law of energy conservation, that the total energy associated
with heat plus the total energy associated with mechanical kinds of things plus other
kinds of energy, that that total is constant.
And that became a pretty well understood principle, but the second law of thermodynamics was always
mysterious.
Like, why does it work this way?
Can it be derived from underlying mechanical laws?
And so when I was, well, 12 years old, actually, I had gotten interested in space and things
like that, because I thought that was kind of the future and interesting sort of technology
and so on.
And for a while, kind of, you know, every deep space probe was sort of a personal friend
type thing.
And I knew all kinds of characteristics of it and was kind of writing up all these things
when I was, well, I don't know, eight, nine, 10 years old and so on.
And then I got interested from being interested in kind of spacecraft.
I got interested in, like, how do they work, what are all the instruments on them and so
on.
And that got me interested in physics, which was just as well, because if I'd stayed interested
in space in the, you know, mid to late 1960s, I would have had a long wait before, you know,
space really blossomed as an area.
But...
Adding as everything.
Right.
I got interested in physics.
And then, well, the actual sort of detailed story is when I kind of graduated from elementary
school at age 12.
That's the time when in England where you finish elementary school, I sort of, my gift, sort
of, I suppose, more or less for myself was I got this collection of physics books, which
were some college physics, course of college physics books, and volume five about statistical
physics.
And it has this picture on the cover that shows a bunch of kind of idealized molecules sitting
in one side of a box, and then it has a series of frames showing how these molecules sort
of spread out in the box.
And I thought that's pretty interesting, you know, what causes that?
And, you know, I read the book and the book, the book actually, one of the things that
was really significant to me about that was the book kind of claimed, although I didn't
really understand what it said in detail, it kind of claimed that this sort of principle
of physics was derivable somehow.
And you know, other things I'd learned about physics, it was all like, it's a fact that
energy is conserved, it's a fact that relativity works or something, not it's something you
can derive from some fundamental sort of, it has to be that way as a matter of kind
of mathematics or logic or something.
So it was sort of interesting to me that there was a thing about physics that was kind of
inevitably true and derivable, so to speak.
And so I think that, so then I was like, there's a picture on this book and I was trying to
understand it.
And so that was actually the first serious program that I wrote for a computer was probably
in 1973, written for this computer the size of a desk program with paper tape and so on.
And I tried to reproduce this picture on the book and it didn't succeed.
What was the failure mode there?
Like what do you mean it didn't succeed?
So it's a bunch of little...
It didn't look like, it didn't look like, okay, so what happened is, okay, many years
later I learned how the picture on the book was actually made and that it was actually
kind of a fake, but I didn't know that at that time.
But and that picture was actually a very high tech thing when it was made in the beginning
of the 1960s, was made on the largest supercomputer that existed at the time.
And even so, it couldn't quite simulate the thing that it was supposed to be simulating.
But anyway, I didn't know that until many, many, many years later.
So at the time, it was like, you have these balls bouncing around in this box, but I was
using this computer with eight kilo words of memory, they were 18 bit words of memory
words, okay?
So it was whatever, 24 kilobytes of memory.
And it had these instructions, I probably still remember all of its machine instructions.
And it didn't really like dealing with floating point numbers or anything like that.
And so I had to simplify this model of particles bouncing around in the box.
And so I thought, well, I'll put them on a grid and I'll make the things just sort of
move one square at a time and so on.
And so I did the simulation and the result was it didn't look anything like the actual
pictures on the book.
Now, many years later, in fact, very recently, I realized that the thing I'd simulated was
actually an example of a whole sort of computational irreducibility story that I absolutely did
not recognize at the time.
At the time, it just looked like it did something random and it looks wrong, as opposed to it
did something random and it's super interesting that it's random, but I didn't recognize
that at the time.
And so as it was at the time, I kind of got interested in particle physics and I got
interested in other kinds of physics.
But this whole second law of thermodynamics thing, this idea that sort of orderly things
tend to degrade into disorder, continued to be something I was really interested in.
And I was really curious for the whole universe, why doesn't that happen all the time?
Like we start off in the Big Bang at the beginning of the universe was this thing that seems
like it's this very disordered collection of stuff and then it spontaneously forms itself
into galaxies and creates all of this complexity and order in the universe.
And so I was very curious how that happens.
But I was always kind of thinking this is somehow the second law of thermodynamics is
behind it trying to sort of pull things back into disorder, so to speak, and how was order
being created?
And so actually, I was interested, this is probably now 1980, I got interested in kind
of this galaxy formation and so on in the universe.
I also at that time was interested in neural networks and I was interested in kind of how
brains make complicated things happen and so on.
Okay, wait, wait, wait, wait.
What's the connection between the formation of galaxies and how brains make complicated
things happen?
Because they're both a matter of how complicated things come to happen.
From simple origins.
Yeah, from some sort of know and origins.
I had the sense that what I was interested in was kind of in all these different, this
sort of different cases of where complicated things were arising from rules.
And I also looked at snowflakes and things like that.
I was curious and fluid dynamics in general.
I was just sort of curious about how does complexity arise and the thing that I didn't,
it took me a while to kind of realize that there might be a general phenomenon.
I sort of assumed, oh, there's galaxies over here, there's brains over here, they're very
different kinds of things.
And so what happened, this is probably 1981 or so, I decided, okay, I'm going to try and
make the minimal model of how these things work.
And it was sort of an interesting experience because I had built, starting in 1979, I built
my first big computer system, I think called SMP, Symbolic Manipulation Program.
It's kind of a forerunner of modern morphin language with many of the same ideas about
symbolic computation and so on.
But the thing that was very important to me about that was, in building that language,
I had basically tried to figure out what were the relevant computational primitives, which
have turned out to stay with me for the last 40-something years.
But it was also important because in building a language was very different activity from
natural science, which is what I'd mostly done before, because in natural science, you
start from the phenomena of the world and you try and figure out, so how can I make
sense of the phenomena of the world?
And kind of the world presents you with what it has to offer, so to speak, and you have
to make sense of it.
When you build a computer language or something, you are creating your own primitives and then
you say, so what can you make from these?
Sort of the opposite way around from what you do in natural science.
But I'd had the experience of doing that, and so I was kind of like, okay, what happens
if you sort of make an artificial physics?
What happens if you just make up the rules by which systems operate?
And then I was thinking, for all these different systems, whether it was galaxies or brains
or whatever, what's the absolutely minimal model that kind of captures the things that
are important about those systems?
The computational primitives of that system.
Yes.
And so that's what ended up with the cellular automata, where you just have a line of black
and white cells, and you just have a rule that says, given a cell and its neighbors,
what will the color of the cell be on the next step, and you just run it in a series
of steps.
And the sort of the ironic thing is that cellular automata are great models for many kinds of
things, but galaxies and brains are two examples where they do very, very badly.
They're really irrelevant to those two cases.
Is there a connection to the second law of thermodynamics and cellular automata?
Oh, yes.
The things you've discovered about cellular automata.
Yes.
Okay.
So when I first started cellular automata, my first papers about them were, you know,
the first sentence was always about the second law of thermodynamics, was always about how
does order manage to be produced even though there's a second law of thermodynamics which
tries to pull things back into disorder.
And I kind of, my early understanding of that had to do with these are intrinsically irreversible
processes in cellular automata that form, you know, can form orderly structures even
from random initial conditions.
But then what I realized, this was, well, actually, it's one of these things where it
was a discovery that I should have made earlier but didn't.
So you know, I had been studying cellular automata, what I did was the sort of most
obvious computer experiment, you just try all the different rules and see what they
do.
It's kind of like, you know, you've invented a computational telescope, you just pointed
at the most obvious thing in the sky and then you just see what's there.
And so I did that and I, you know, was making all these pictures of how cellular automata
work and starting these pictures, I studied in great detail, there was, you can number
the rules for cellular automata and one of them is, you know, rule 30.
So I made a picture of rule 30 back in 1981 or so and rule 30, well, it's, and at the
time I was just like, oh, okay, it's another one of these rules I don't really, it happens
to be asymmetric, left, right, asymmetric and it's like, let me just consider the case
of the symmetric ones just to keep things simpler, et cetera, et cetera, et cetera.
And I just kind of ignored it.
And then sort of in, actually in 1984, strangely enough, I ended up having an early laser printer
which made very high resolution pictures and I thought, I'm going to print out an interesting,
you know, I want to make an interesting picture.
Let me take this rule 30 thing and just make a high resolution picture of it.
And I did and it's, it has this very remarkable property that its rule is very simple.
You started off just from one black cell at the top and it makes this kind of triangular
pattern.
But if you look inside this pattern, it looks really random.
There's, you know, you look at the center column of cells and, you know, I studied that
in great detail and it's, so far as one can tell, it's completely random and it's kind
of a little bit like digits of pi.
Once you, you know, you know the rule for generating the digits of pi, but once you've
generated them, you know, 3.14159, et cetera, they seem completely random.
And in fact, I put up this prize back in what was it, 2019 or something for prove anything
about the sequence basically.
Has anyone been able to do anything on that?
People have sent me some things, but it's, you know, I don't know how hard these problems
are.
I mean, I was kind of spoiled because I, 2007, I put up a prize for determining whether a
particular Turing machine that I thought was the simplest candidate for being a universal
Turing machine, determine whether it is or isn't a universal Turing machine.
And somebody did a really good job of winning that prize, improving that it was a universal
Turing machine in about six months.
And so I, you know, I didn't know whether that would be one of these problems that was
out there for hundreds of years or whether in this particular case, young chap called
Alex Smith, you know, nailed it in six months.
And so with this rule 30 collection, I don't really know whether these are things that
are a hundred years away from being able to get or whether somebody's going to come and
do something very clever.
It's such a, I mean, it's like for Mars last year, but since rule 30, such a simple formulation,
it feels like anyone can look at it, understand it and feel like it's within grasp to be able
to predict something, to do, to derive some kind of law that allows you to predict something
about this middle column of rule 30.
Right.
But you know, this is-
And yet you can't.
Yeah, right.
This is the intuitional surprise of computational irreducibility and so on that even though
the rules are simple, you can't tell what's going to happen and you can't prove things
about it.
And I think so anyway, the thing, I started in 1984 or so, I started realizing there's
this phenomenon that you can have very simple rules, they produce apparently random behavior.
Okay, so that's a little bit like the second law of thermodynamics because it's like you
have this simple initial condition, you can readily see that it's very, you can describe
it very easily and yet it makes this thing that seems to be random.
Now turns out there's some technical detail about the second law of thermodynamics and
about the idea of reversibility when you have kind of a movie of two billiard balls
colliding and you see them collide and they bounce off and you run that movie in reverse,
you can't tell which way was the forward direction of time and which way was the backward direction
of time when you're just looking at individual billiard balls.
By the time you've got a whole collection of them, you know, a million of them or something,
then it turns out to be the case and this is the sort of the mystery of the second law
that the orderly thing, you start with the orderly thing and it becomes disordered and
that's the forward direction in time and the other way round of it starts disordered and
becomes ordered, you just don't see that in the world.
Now, in principle, if you sort of traced the detailed motions of all those molecules
backwards, you would be able to, the reverse of time makes, you know, as you go forwards
in time, order goes to disorder.
As you go backwards in time, order goes to disorder.
Perfectly so, yes.
Right, so the mystery is why is it the case that, or one version of the mystery is, why
is it the case that you never see something which happens to be just the kind of disorder
that you would need to somehow evolve to order?
Why does that not happen?
Why do you always just see order goes to disorder, not the other way round?
So the thing that I kind of realized, I started realizing in the 1980s, is kind of like, it's
a bit like cryptography, it's kind of like, you start off from this key that's pretty
simple, and then you kind of run it and you can get this complicated random mess and the
thing that, well, I sort of started realizing back then was that the second law is kind
of a story of computational reducibility, it's a story of what seems, what we can describe
easily at the beginning, we can only describe with a lot of computational effort at the
end.
Okay, so now we come many, many years later, and I was trying to sort of, well, having
done this big project to understand fundamental physics, I realized that sort of a key aspect
of that is understanding what observers are like.
Then I realized that the second law of thermodynamics is the same story as a bunch of these other
cases, it is a story of a computationally bounded observer trying to observe a computationally
irreducible system.
So it's a story of, underneath the molecules are bouncing around, they're bouncing around
in this completely determined way, determined by rules, but the point is that we, as computationally
bounded observers, can't tell that there were these sort of simple underlying rules, to
us it just looks random, and when it comes to this question about, can you prepare the
initial state so that the disordered thing is, how exactly the right disorder to make
something orderly, a computationally bounded observer cannot do that.
We'd have to have done all of this sort of irreducible computation to work out very
precisely what this disordered state, what the exact right disordered state is, so that
we would get this ordered thing produced from it.
What does it mean to be computationally bounded observer, observing a computationally irreducible
system?
So the computationally bounded, is there something formal you can say there?
Right, so it means, okay, you can talk about Turing machines, you can talk about computational
complexity theory and polynomial time computation and things like this, there are a variety
of ways to make something more precise, but I think it's more useful, the intuitive version
of it is more useful, which is basically just to say that how much computation are you going
to do to try and work out what's going on?
And the answer is, you're not allowed to do a lot of, we're not able to do a lot of computation.
When we've got, in this room, there will be a trillion, trillion, trillion molecules,
a little bit less.
That's a big room.
Right, and at every moment, every microsecond or something, these molecules are colliding
and that's a lot of computation that's getting done.
And the question is, in our brains, we do a lot less computation every second than the
computation done by all those molecules.
If there is computational irreducibility, we can't work out in detail what all those
molecules are going to do.
What we can do is only a much smaller amount of computation.
And so the second row of thermodynamics is this kind of interplay between the underlying
computational irreducibility and the fact that we, as preparers of initial states or
as measures of what happens, are not capable of doing that much computation.
So to us, another big formulation of the second row of thermodynamics is this idea of the
law of entropy increase.
The characteristic that this universe, the entropy seems to be always increasing, what
does that show to you about the evolution of...
Well, okay.
We have to say that entropy is.
And that's very confused in the history of thermodynamics because entropy was first introduced
by a guy called Rudolf Klausius and he did it in terms of heat and temperature.
Okay?
Subsequently, it was reformulated by a guy called Ludwig Boltzmann and he formulated
it in a much more kind of combinatorial type way.
But he always claimed that it was equivalent to Klausius's thing and in one particular
simple example it is.
But that connection between these two formulations of entropy, they've never been connected.
I mean, there's really...
So, okay.
So the more general definition of entropy due to Boltzmann is the following thing.
So you say, I have a system and it has many possible configurations.
Molecules can be in many different arrangements, et cetera, et cetera, et cetera.
If we know something about the system, for example, we know it's in a box, it has a certain
pressure, it has a certain temperature, we know these overall facts about it.
Then we say, how many microscopic configurations of the system are possible given those overall
constraints?
And the entropy is the logarithm of that number.
That's the definition.
And that's the kind of the general definition of entropy that turns out to be useful.
Now in Boltzmann's time, he thought these molecules could be placed anywhere you want.
He didn't think...
But he said, oh, actually, we can make it a lot simpler by having the molecules be discrete.
Well, actually, he didn't know molecules existed.
And in his time, 1860s and so on, the idea that matter might be made of discrete stuff
had been floated ever since ancient Greek times, but it had been a long time debate
about, is matter discrete, is it continuous?
At the moment at that time, people mostly thought that matter was continuous.
And it was all confused with this question about what heat is, and people thought heat
was this fluid, and it was a big, big muddle.
And Boltzmann said, let's assume there are discrete molecules, let's even assume they
have discrete energy levels.
Let's say everything is discrete.
Then we can do sort of combinatorial mathematics and work out how many configurations of these
things there will be in the box, and we can say we can compute this entropy quantity.
But he said, but of course, it's just a fiction that these things are discrete.
So he said.
This is an interesting piece of history, by the way, that at that time, people didn't
know molecules existed.
There were other hints from looking at chemistry that there might be discrete atoms and so
on, just from the combinatorics of two hydrogens and one oxygen make water, two amounts of
hydrogen plus one amount of oxygen together make water, things like this.
But it wasn't known that discrete molecules existed.
And in fact, the people, it wasn't until the beginning of the 20th century that Brownian
motion was the final giveaway.
Brownian motion is you look under a microscope at these little pieces from pollen grains
you see they're being discreetly kicked and those kicks are water molecules hitting them
and they're discrete.
And in fact, it was really quite interesting history.
I mean, Boltzmann had worked out how things could be discrete and have basically invented
something like quantum theory in the 1860s.
But he just thought it wasn't really the way it worked.
And then just a piece of physics history because I think it's kind of interesting.
In 1900, this guy called Max Planck, who'd been a long time thermodynamics person who
was trying to, everybody was trying to prove the second law of thermodynamics, including
Max Planck.
And Max Planck believed that radiation, like electromagnetic radiation, somehow the interaction
of that with matter was going to prove the second law of thermodynamics.
But he had these experiments that people had done on black body radiation and they were
these curves and you couldn't fit the curve based on his idea for how radiation interacted
with matter, those curves, you couldn't figure out how to fit those curves, except he noticed
that if he just did what Boltzmann had done and assumed that electromagnetic radiation
was discrete, he could fit the curves.
He said, but this just happens to work this way.
Then Einstein came along and said, well, by the way, the electromagnetic field might
actually be discrete.
It might be made of photons.
And then that explains how this all works.
And that was in 1905, that was how that piece of quantum mechanics got started.
Kind of interesting, interesting piece of history.
I didn't know until I was researching this recently.
In 1904 and 1903, Einstein wrote three different papers and just sort of well-known physics
history.
In 1905, Einstein wrote these three papers.
One introduced relativity theory, one explained Brownian motion, and one introduced basically
photons.
So kind of a big deal year for physics and for Einstein.
But in the years before that, he'd written several papers and what were they about?
They were about the second law of thermodynamics.
And they were an attempt to prove the second law of thermodynamics and their nonsense.
And so I had no idea that he'd done this.
Interesting.
Neither.
What he did, those three papers in 1905, well, not so much the relativity paper, the one
on Brownian motion, the one on photons, both of these were about the story of sort of making
the world discrete.
And then he got those like that idea from Boltzmann.
But Boltzmann didn't think, you know, Boltzmann kind of died believing, you know, he said,
he has a quote actually, you know, in the end, things are going to turn out to be discrete
and I'm going to write down what I have to say about this because, you know, eventually
this stuff will be rediscovered and I want to leave, you know, what I can about how things
are going to be discrete.
But, you know, I think he has some quote about how, you know, one person can't stand against
the tide of history in saying that, you know, matter is discrete.
Oh, so he's stuck by his guns in terms of matter is discrete.
Yes, he did.
And the, you know, what's interesting about this is at the time, everybody including Einstein
kind of assumed that space was probably going to end up being discrete too.
But that didn't work out technically because it wasn't consistent with relativity theory.
It didn't seem to be.
And so then in the history of physics, even though people had determined that matter was
discrete, electromagnetic field was discrete, space was a holdout of not being discrete.
And in fact, Einstein 1916 has this nice letter he wrote where he says, in the end, it will
turn out space is discrete, but we don't have the mathematical tools necessary to figure
out how that works yet.
And so, you know, I think it's kind of cool that a hundred years later we do.
Yes, for you, you're pretty sure that at every layer of reality it's discrete.
Right.
And that space is discrete and that the, I mean, and in fact, one of the things I realized
recently is this kind of theory of heat that, you know, that heat is really this continuous
fluid, it's kind of like the, you know, the caloric theory of heat, which turns out to
be completely wrong because actually heat is the motion of discrete molecules, unless
you know there are discrete molecules, it's hard to understand what heat could possibly
be.
Well, you know, I think space is discrete.
And the question is kind of what's the analog of the mistake that was made with caloric
in the case of space.
And so, my current guess is that dark matter is, as I've, my little sort of aphorism of
the last few months has been, you know, dark matter is the caloric of our time.
That is, it will turn out that dark matter is a feature of space and it is not a bunch
of particles.
You know, at the time when, when people were talking about heat, they knew about fluids
and they said, well, heat must be just be another kind of fluid because that's what
they knew about.
Yes.
But now people know about particles and so they say, well, what's dark matter?
It's not, it's not, it just must be particles.
So what could dark matter be as a feature of space?
Oh, I don't know yet.
All right.
I mean, I think the, the thing I'm really one of the things I'm hoping to be able to
do is to find the analog of Brownian motion in space.
So in other words, Brownian motion was, was seeing down to the level of an effect from
individual molecules.
And so in the case of space, you know, most of the things, the things we see about space
so far, just everything seems continuous.
Brownian motion had been discovered in the 1830s and it was only identified what it was,
what it was the, the, the results of by Smoluchowski and Einstein at the beginning of the 20th
century.
You know, dark matter was, was discovered, that phenomenon was discovered a hundred years
ago.
You know, the rotation curves of galaxies don't follow the luminous matter that was discovered
a hundred years ago.
And I think, you know, that I wouldn't be surprised if there isn't an effect that we
already know about that is kind of the analog of Brownian motion that reveals the discreteness
of space.
And in fact, we're beginning to have some guesses, we have some, some evidence that
black hole mergers work differently when there's discreet space and there may be things that
you can see in gravitational waves, signatures and things associated with the discreteness
of space.
But this is kind of, for me, it's kind of, it's kind of interesting to see this sort
of recapitulation of the history of physics where people, you know, vehemently say, you
know, matter is continuous, electromagnetic field is continuous.
And turns out it isn't true.
And then they say space is continuous, but, but so, you know, entropy is the number of
states of the system consistent with some constraint.
Yes.
And the thing is that if you have, if you know in great detail the position of every
molecule in the gas, the entropy is, is always zero because there's only one possible state.
The, the configuration of molecules in the gas, the molecules bounce around, they have
a certain rule for bouncing around.
There's just one state of the gas evolves to one state of the gas and so on.
But it's only if you don't know in detail where all the molecules are, that you can
say, well, the entropy increases because the things we do know about the molecules, there
are more possible microscopic states of the system consistent with what we do know about
where the molecules are.
And so the question of whether, so people, this sort of paradox in a sense of, oh, if
we knew where all the molecules where the entropy wouldn't increase, there was this idea introduced
by, by Gibbs in the early 20th century, well, actually the very beginning of the 20th century
as a physics professor, an American physics professor was sort of the first distinguished
American physics professor at Yale.
And he introduced this idea of coarse-graining, this idea that, well, you know, these molecules
have a detailed way they're bouncing around, but we can only observe a coarse-grained version
of that.
But the confusion has been nobody knew what a valid coarse-graining would be.
So nobody knew that whether you could have this coarse-graining that very carefully was
sculpted in just such a way that it would notice that the particular configurations that
you could get from the simple initial condition, you know, they fit into this coarse-graining
and the coarse-graining very carefully observes that.
Why can't you do that kind of very detailed, precise coarse-graining?
The answer is because if you are a computationally bounded observer and the underlying dynamics
is computationally irreducible, that's, that's what defines possible coarse-graining is what
a computationally bounded observer can do.
And it's the, it's the fact that a computationally bounded observer is, is forced to look only
at this kind of coarse-grained version of what the system is doing.
That's why, and because the, what's, what's going on underneath is it's kind of filling
out this, this, the, the different possible, you're ending up with something where the
sort of underlying computational irreducibility is your, if all you can see is what the coarse-grained
result is with comp, with a sort of computationally bounded observation, then inevitably there
are many possible underlying configurations that are consistent with that.
Just to clarify, basically any observer that exists inside the universe is going to be
computationally bounded.
No, any observer like us.
I don't know.
I can't imagine.
When you say like us, what do you mean?
What do you mean like us?
Well, humans with finite minds.
You're including the tools of science.
Yeah.
Yeah.
And as we, you know, we have more precise, and by the way, there are little sort of microscopic
violations of the second law of thermodynamics that you can start to have when you have more
precise measurements of where precisely molecules are, but for, for a large scale when you have
enough molecules, we don't have, you know, we're not tracing all those molecules and
we just don't have the computational resources to do that.
And it wouldn't be, you know, I think that the, to imagine what an observer who is not
computationally bounded would be like.
It's an interesting thing because, okay, so what does computational boundedness mean?
Among other things, it means we conclude that definite things happen.
We go, we take all this complexity of the world and we make a decision.
We're going to turn left or turn right.
And that is kind of reducing all this kind of detail into we're observing it, we're sort
of crushing it down to this one thing.
And that, if we didn't do that, we wouldn't, we wouldn't have all this sort of symbolic
structure that we build up that lets us think things through with our finite minds.
We'd be instead, you know, we'd be just, we'd be sort of one with the universe.
Yeah.
To not simplify.
Yes.
If we didn't simplify, then we wouldn't be like us.
We would be like the universe, like the, the intrinsic universe, but not having experiences
like the experiences we have where we, for example, conclude that definite things happen.
We, you know, we, we sort of have this, this notion of being able to make, make sort of
narrative statements.
Yeah.
So it's just like you imagined as a thought experiment, what it's like to be a computer.
I wonder if it's possible to try to begin to imagine what it's like to be an unbounded
computational observer.
Well, okay.
So here's, here's how that I think plays out.
Vibrations.
So, yeah.
So, I mean, in this, we talk about this Rouliat, the spaceable possible computations.
And this idea of, you know, being at a certain place in Rouliat, which corresponds to sort
of a certain way of, of rep, of a certain set of computations that you are representing
things in terms of, okay, so as you expand out in the Rouliat, as you kind of encompass
more possible views of the universe, as you encompass more possible kinds of computations
that you can do, eventually you might say, that's a real win.
You know, we're, we're colonizing the Rouliat, we're, we're, we're building out more paradigms
about how to think about things.
And eventually you might say, we won all the way, we managed to colonize the whole Rouliat.
Okay.
Here's the problem with that.
The problem is that the notion of existence, coherent existence requires some kind of specialization.
By the time you are the whole Rouliat, by the time you cover the whole Rouliat, in no
useful sense do you coherently exist.
So in other words, in, in the notion of existence, the notion of what we think of as, as, as
definite existence requires this kind of specialization, requires this kind of idea
that we are, we are not all possible things.
We are the, a particular set of things.
And that's kind of how we, that, that's kind of what, what makes us have a coherent existence.
If we were spread throughout the Rouliat, we would not, there would be no coherence
to the way that we work.
We would work in all possible ways.
And that wouldn't be kind of a, a notion of identity.
We wouldn't have this notion of kind of, of, of, of coherent identity.
I am geographically located somewhere exactly precisely in the Rouliat.
Therefore I am.
Yes.
The Descartes kind of.
Yeah.
Yeah.
Right.
Well, you're in a certain place in physical space.
You're in a certain place in rural space.
Rural space.
If you are sufficiently spread out, you are no longer coherent.
And you no longer have, I mean, in, in the, in our perception of what it means to exist
and to have experience, it doesn't happen that way.
So therefore, so to, to exist means to be computationally bonded.
I think so.
To exist in the way that we think of ourselves as existing.
Yes.
The very active existence is like operating in this place that's computationally reducible.
So there's just a giant mess of things going on that you can't possibly predict.
But nevertheless, because of your limitations, you, you have an imperative of like, what
is it?
An imperative or a skill set to simplify or an ignorance, a sufficient level.
Okay.
So the thing which is not obvious is that you are taking a slice of all this complexity,
just like we have all of these molecules bouncing around in the room, but all we notice is,
you know, the, the, the, the kind of the flow of the air or the pressure of the air.
We're just noticing these particular things.
And the, the big interesting thing is that there are rules, there are laws that govern
those big things that we, we observe.
Yeah.
So it's not obvious.
That's amazing.
Because it doesn't feel like it's a slice.
Yeah.
Well, right.
It's not a slice.
Well, it's like a, it's like an abstraction.
Yes.
But I mean, the fact that the gas laws work, that we can describe pressure, volume, et cetera,
et cetera, et cetera.
It's suspicious.
And if we go down to the level of talking about individual molecules, that is a non-trivial
fact.
And, and here's the thing that I sort of exciting thing as far as I'm concerned.
The fact that there are certain aspects of the universe.
So, you know, we think space is made ultimately these atoms of space and these hypergraphs
and so on.
And we think that, but we nevertheless perceive the universe at a large scale to be like continuous
space and so on.
We, in quantum mechanics, we think that there are these many threads of time, these many
threads of history, yet we kind of span so, so, you know, in quantum mechanics in our
models of physics, there are these, time is not a single thread.
Time breaks into many threads.
They branch, they merge and, but we are part of that branching, merging universe.
And so our brains are also branching and merging.
And so when we perceive the universe, we are branching brains perceiving a branching universe.
And so the fact that the claim that we, we believe that we are persistent in time.
We have this single thread of experience.
That's the statement that somehow we managed to aggregate together those separate threads
of time that are separated in the operation of, in the fundamental operation of the universe.
So just as in space, we're averaging over some big region of space and we're looking
at many, many of the aggregate effects of many atoms of space.
So similarly in what we call branchial space, the space of these, these quantum branches,
we are effectively averaging over many different branches of possible of histories of the universe.
And so in, in thermodynamics, we're averaging over many configurations of, you know, many,
many possible positions of molecules.
So what, what we see here is, so the question is, when you do that averaging for space,
what are the aggregate laws of space?
When you do that averaging of a branchial space, what are the aggregate laws of branchial
space?
When you do that averaging over the molecules and so on, what are the aggregate laws you
get?
And this is, this is the thing that I think is just amazingly, amazingly neat.
That there are aggregate laws at all.
Well, yes.
But the question is, what are those aggregate laws?
So the answer is for space, the aggregate laws, Einstein's equations for gravity for
the structure of space-time.
For branchial space, the aggregate laws are the laws of quantum mechanics.
And for the case of, of molecules and things, the aggregate laws are basically the second
law of thermodynamics.
And so the, that's the, and the things that follow from the second law of thermodynamics.
And so what that means is that the three great theories of 20th century physics, which
are basically general theory of the theory of gravity, quantum mechanics and statistical
mechanics, which is what kind of grows out of the second law of thermodynamics.
All three of the great theories of 20th century physics are the result of this interplay between
computational irreducibility and the computational boundedness of observers.
And you know, for me, this is really neat because it means that all three of these laws
are derivable.
So we used to think that, for example, Einstein's equations were just sort of a wheel-in feature
of our universe, that they could be in my universe, might be that way, it might not be
that way, quantum mechanics is just like, well, it just happens to be that way.
And the second law, people kind of thought, well, maybe it is derivable, okay?
What turns out to be the case is that all three of the fundamental principles of physics
are derivable, but they're not derivable just from mathematics.
They require, or just from some kind of logical computation.
They require one more thing.
They require that the observer, that the thing that is sampling the way the universe works,
is an observer who has these characteristics of computational boundedness of belief and
persistence and time.
And so that means that it is the nature of the observer, you know, the rough nature of
the observer, not the details of, oh, we got two eyes and we observed photons of this frequency
and so on, but the kind of the very coarse features of the observer then imply these
very precise facts about physics.
And it's, I think it's amazing.
So if we just look at the actual experience of the observer, that we experience this reality,
it seems real to us, and you're saying because of our bounded nature, it's actually all an
illusion.
It's a simplification.
Well, yeah, it's a simplification.
Right.
But you don't think a simplification is an illusion?
No.
I mean, it's, well, I don't know.
I mean, what's underneath, okay, that's an interesting question.
What's real?
And that relates to the whole question of why does the universe exist?
And you know, what is the difference between reality and a mere representation of what's
going on?
Yes.
We experience the representation.
Yes.
But the question of, so one question is, you know, why is there a thing which we can experience
that way?
And the answer is, because this Rouliat object, which is this entangled limit of all possible
computations, there is no choice about it.
It has to exist.
It has to, there has to be such a thing.
It is in the same sense that, you know, 2 plus 2, if you define what 2 is and you plot
pluses and so on, 2 plus 2 has to equal 4.
Similarly, this Rouliat, this limit of all possible computations, just has to be a thing
you, that is, once you have the idea of computation, you inevitably have the Rouliat.
You're going to have to have a Rouliat, yeah.
Right.
And what's important about it, there's just one of it.
It's just this unique object.
And that unique object necessarily exists.
And then the question is, what, and then we, once you know that we are sort of embedded
in that and taking samples of it, that it's sort of inevitable that there is this thing
that we can perceive that is, you know, our perception of kind of physical reality necessarily
is that way, given that we are observers with the characteristics we have.
So in other words, the fact that the universe exists is, it's actually, it's almost like,
it's, you know, to think about it almost theologically, so to speak.
And I really, it's funny because a lot of the questions about the existence of the universe
and so on, they transcend what kind of the science of the last few hundred years has
really been concerned with.
The science of the last few hundred years hasn't thought it could talk about questions
like that.
Yeah.
And, but I think it's kind of, and so a lot of the kind of arguments of, you know, does
God exist, you know, is it obvious that I think it, in some sense, in some representation,
it's sort of more obvious that something sort of bigger than us exists than that we exist.
And we are, you know, our existence and as observers, the way we are is sort of a contingent
thing about the universe.
And it's more inevitable that the whole, the whole universe, kind of the whole set of all
possibilities exists.
But this question about, you know, is it real or is it an illusion, you know, all we know
is our experience.
And so, the fact that, well, our experience is this absolutely microscopic piece of sample
of the Ruliad and we're, and, you know, there's this point about, you know, we might sample
more and more of the Ruliad.
We might learn more and more about, we might learn, you know, like different areas of physics,
like quantum mechanics, for example, the fact that it was discovered, I think, is closely
related to the fact that electronic amplifiers were invented that allowed you to take a small
effect and amplify it up, which hadn't been possible before.
You know, microscopes have been invented that magnify things and so on.
But the, you know, having a very small effect and being able to magnify it was sort of a
new thing that allowed one to see a different sort of aspect of the universe and let one
discover this kind of thing.
So, you know, we can expect that in the Ruliad, there are an infinite collection of new things
we can discover.
There's, in fact, computational irreducibility kind of guarantees that there will be an infinite
collection of kind of, you know, pockets of reducibility that can be discovered.
Boy, would it be fun to take a walk down the Ruliad and see what kind of stuff we find there.

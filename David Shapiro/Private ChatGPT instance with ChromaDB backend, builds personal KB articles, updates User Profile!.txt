Check it, check it, check it, check it.
Okay, level check, I think we're here.
All right, so background, it's been a little while
since I've done, you're over here now,
since I've done one of these tutorial coding videos,
but I woke up at like 2.30 in the morning
and just had to work on this, so I hammered it out.
Basically, the purpose of today's video
is to demonstrate using ChromaDB,
which is a local vector database.
It's basically like SQLite,
if you're familiar with that,
which is a self-contained SQL database, relational database.
This is functionally very similar to SQLite,
except it is a vector database,
meaning it does semantic search.
One thing that's really great about it
is that it has its own built-in embeddings tools.
I think it's based on BERT.
Anyways, you can check out all the documentation here
on trichroma.com.
The Getting Started and Usage Guide is pretty good.
It's not complete.
Every now and then I find that I have to go
to the actual repo to look at
how some of the internals work,
but it is pretty brain-dead simple.
So let me just go ahead and show you.
This is my private instance.
Oh, so before we get too lost,
I do have a public instance.
DaveShap slash ChromaDB underscore under chatbot
underscore public.
I've got a little integration guide, usage.
It's, I mean, you probably don't need this.
You can futz around with it,
but this will get you started.
I also use chatGBT to just get a really
basic explanation of the code.
You probably won't need it once you take a look at it.
So off the top, let me just show you how this thing works.
So it's a basic chatbot.
You can see I didn't specify it.
So it's getting all mini-LML6v2 from Huggingface.
Great, so it's like, hey, let's see.
What were we talking about last?
This probably won't work because it's just gonna,
in the future, it wouldn't work
because it's gonna have multiple KB articles
in the background.
Oh, I need to explain like all that.
So I know that I just said like KB articles,
don't worry, we'll get to it.
But anyways, I wanna show you that I just started it up
and what it's gonna do is it's gonna use
the last few messages to search its internal KB article
for the last information,
but it also has a user profile for me.
In our previous conversations,
we discussed AI alignment, morality, ethics,
and epistemology within AI development.
You also shared your plans to communicate
your ideas on YouTube, unplug your computer,
and spend more time outdoors,
and use digital wellness settings
to improve your work-life balance.
Working on that.
Additionally, we talked about your recent experience
with severe insomnia and the importance
of maintaining a healthy balance
between work and personal life.
Yes, that's actually why I created this chatbot.
Let me show you.
So there was a, there was,
God, my brain.
I was using chatGPT as a reflective journaling tool.
So what I mean by that is if you plug in this message,
and I know I'm scattered, I'm all over the place,
this is what happens when I have severe insomnia.
Anyways, so basically I use chatGPT
as a reflective journaling tool to figure out
how I'm feeling about things,
because as an autistic person,
I often need help with this.
And I don't like journaling because just talking
to a page is kind of dumb,
but it's like, hey, I need to talk something out.
And so anyways, by workshopping this system message
with chatGPT, I came up with a pretty good
reflective journaling tool.
So you could say that this is a therapeutic tool,
but by couching it in the language of reflective journaling,
it's not like, you know,
a reflective journaling, it's not like medical therapy
or psychotherapy or anything,
but it's just like, you know, I can say like,
I have been working so hard and I don't know why.
I actually do know why now,
but this is kind of a shorthand of the conversation I had.
Let's try and figure out, it's driving you so hard.
Can you think of any specific goals
that might be pushing you to this extra effort?
So you see how the tone of this is much more straightforward
and it's very focused by asking those like
kind of probing follow-up questions.
This is why, you know, it's in the investigation phase.
Anyways, so I had this idea and I was like, okay,
this is great, but I need,
if I'm gonna use this as a long-term journaling tool,
I'm gonna need this locally
and I'm gonna need persistent storage
because as this is just the playground,
if I do a refresh, it's gone and that's no good.
So actually here, let me go ahead and just save this
to the, we're gonna call this the system message
for reflective journaling.
So you can use this if you want.
All right, so anyways, so you see it has this
and then you see it says updating user profile
and updating KB.
Okay, cool.
So you see that it fundamentally basic chatbot.
So now let's start to unpack it.
So first we will go look at the, just the chat file.
So this is a super brain dead simple chatbot
with infinite memory, infinite memory.
I know some people got grumpy when I said
that Icon had infinite memory from a human standpoint,
it functionally has infinite memory
because this thing can hold probably a million KB articles,
which is more than enough to document your entire life.
So from a human standpoint, it is functionally infinite.
All right, so from the top,
we've got a few basic utility functions,
save yaml save file, open file,
and then a chatbot which calls the GPT-4 model.
You could switch this out to 3.5 turbo
if you don't have access to GPT-4 yet,
it does not work as well.
There's a reason that I use GPT-4 because it is smarter.
I also set the temperature to zero
because I don't like it to be too creative,
especially with a lot of the functions that I have it doing.
You actually want it to be more deterministic or mechanistic
and that you wanna get the same results every time,
especially when you're updating the user profile
and the KB articles.
You can see right here that every time you call the chatbot,
I dump the whole thing to apilogs slash convo.
And it's a yaml file, so here's my private one.
So apilogs, here's an example.
So each item is gonna be here,
actually that's not a good one
because I changed the way that it saves it.
Let me show you a more recent one.
So the first element is always gonna be the system message
that was in the last convo.
So then here's the KB article
and you can see that it was updating the KB article.
And so each one of these items is like,
you'll see, but anyways,
I just wanted to show that it logs everything
because well, sometimes it does things
that you don't understand.
All right, so that's an example of the apilog
and then if the conversation,
if the overall conversation is too long,
it'll go ahead and trim the oldest chat message.
So the chat GPT web interface does this automatically
where it'll just kind of groom the backlog of messages.
So we have to do this manually.
So I just have it cut off at 7,000 tokens.
You could probably do like 7,500 if you want to
because a lot of these are gonna be limited
but you have a user profile and a KB article
that gets wedged in which are both up to 1,000 words
which could be around 1,000 tokens.
So having it trim at 7,000 is probably where you want it.
So that's the primary, those are the helper functions
and then you have a super straightforward,
you instantiate ChromaDB right here.
So you set the persistent directory
which is, I have it right here, ChromaDB.
So this is my instance, my personal instance of ChromaDB.
It's not gonna be the one that you find up here.
This is the public version.
So if you go into ChromaDB,
you'll see just a placeholder file
so that the folder's already there.
You don't need to instantiate it.
Let's see, going back to here.
So ChromaClient, so we instantiate the ChromaDB client.
This is again, almost identical to SQLite
or other similar things.
So about a year ago,
I tried to do basically the same thing I called it VDB Lite
for Vector Database Lite instead of SQL Lite,
Structured Query Language Lite.
But this company went and did the same thing
and I think they've already got like a $30 million valuation
or something.
I was like, damn, I should have stuck with that.
Anyways, they figured it out.
I think it's based on the same underpinning technology.
They're using an open source embedding transformer.
I think they're also using the Facebook AI
semantic search vice engine in the background.
Anyways, so you instantiate the client.
You need to use the settings to have a persistent directory
because by default, this entire thing is fully ephemeral.
I think it does cache it somewhere,
but I want it to be very explicit,
saying save it here for reusability.
And so then collection is ChromaClient,
get or create collection named KnowledgeBase.
So this is my personal KnowledgeBase.
Then we instantiate the conversation
with OpenAI, the chatbot.
And in this case, because we're saving everything necessary
into a personal user profile and the KB articles,
like why even load the conversation?
All right, so let me show you the system default message.
So the system default message is where it starts.
Your chatbot whose mission is to assist the following user,
your ultimate objectives are to minimize suffering
and enhance prosperity and promote understanding.
The provided information about the user and the KnowledgeBase
article should be integrated into your interactions.
This is private information not visible to the user.
The user profile compiled from past conversations
encapsulates critical details about the user, which
can aid in shaping your responses
effectively, which you saw here.
So you see it actually knows quite a bit about me
from our past conversations.
This was populated here in the user profile and the KB article.
So basically it says, then it also
explains that the KB article is a topic compiled similarly
from past dialogue, serving as your long-term memory.
While numerous KB articles exist in your back-end system,
the one provided is deemed most relevant to the current conversation
topic.
Note that the recall system operates autonomously
and it may not always retrieve the most suitable KB.
If the user is asking about a topic that doesn't seem
to align with the provided KB, inform them of the memory
pulled and request them to specify their query
or share more details.
This can assist the autonomous system
in retrieving the correct memory in the subsequent interaction.
So basically, that's instructing it
to do the same thing that a human will do if I say, hey,
Bill, do you remember that time that I accidentally shot you
in the face with a Roman candle?
Because that's something that would happen in the South.
And Bob would be like, I don't actually remember that.
And I'm like, oh, well, you woke up in the hospital.
Oh, yeah, I remember that.
So we prime each other's memory.
And human prompting is not that different from AI prompting.
Remember that the clarity of your responses
and the relevance of your information recall
are crucial to delivering an optimal user experience.
Please ask any clarifying questions
or provide any input further for refinement if necessary.
So this system message, I actually
got help from ChatGPT to create a really compelling system
message.
And one thing that I recommend that people do
is actually use ChatGPT to work on prompting.
So you could call this meta prompting
where you use the thing to prompt the thing.
And the reason that this works really well as one,
ChatGPT is more articulate than most humans,
including myself, when used correctly.
But another thing is one thing that I noticed
is that ChatGPT tends to write in a way
that it will understand.
And so if you say, if you give it some context like,
this is what I'm trying to do, here's my current prompt.
Here's what's weak about it.
Can you make it better?
And then you tell it, like, ask me some questions
if you have any.
And it's like, no, I see what you're trying to do.
Let me write better instructions for you.
So instruction writing, for anyone
who's like a teacher or technical writer or whatever,
instruction writing is a very, very particular skill set.
And ChatGPT is really good at it.
So this is the default system message, which
is then populated with the user profile
and the most relevant KB article.
So now that we're up to there, we
enter into the infinite loop, which is just get the user text,
save it to the user log or the chat logs.
So the chat logs are all saved out here.
It's just plain text.
And the file name has the timestamp in it,
as well as the speaker.
So user chatbot, user chatbot, so on and so forth.
So you've got the raw logs there just in case anything goes wrong.
And then I've also got DB logs, which we'll get to in just a second.
So then what we do is we take the quote main scratch pad, which
is just the last five messages, both for the user
and for the chatbot.
And this is what we use as the context of working memory.
And so then we use this main scratch pad, which
is the last five messages.
We use it to search for the top most relevant KB article.
And in my case, I still only have one KB article.
So we'll see how it gets to.
And I'll go through the logic of how it builds KB articles
in just a minute.
So basically, it just says, OK, here's the most recent thing.
Find the KB article that is most relevant to the most recent bits
of conversation.
And then it will pull that.
And it's, again, super straightforward.
All you have to do is pass the text to it.
And it will automatically embed it for you.
And then I said, just give me the one most recent.
Once we have larger context windows,
or maybe if we decide that recent chat history doesn't
need to be as big, like let's say we want to trim this down
to like 3,000 tokens.
And we decide that actually having more KB articles
is more important.
We can absolutely do that.
And what you would do then is just change the end results
to, let's say, give me the four most relevant KB articles
instead of the one most relevant.
That will allow it to have a more sophisticated working memory.
Yeah, but right now we're just doing one.
And so then what we do is we repopulate that system default
message with the profile and the KB article.
And so that's right here.
So that gets populated there.
And then, let's see, it looks like I accidentally
changed something.
So let me go ahead and show you my user profile.
I don't mind sharing this because I've already
told you everything.
I'm pretty much an open book.
So the format for this is what I call a labeled list.
And so I realized back in GPT-3 that GPT handles labeled lists
very, very well.
So that's where you use a hyphenated list, bullet list.
It understands that intrinsically.
And then you label the information.
So it's just a hash table.
If you're into computer science, this
is called a hash table or a dictionary
where you have a parameter and then you label the parameter.
So the data metadata.
So name, David Shapiro.
Y'all know that.
Profession, AI, and Cognitive Architectures.
Y'all know that.
Interests, it's got a whole bunch of interests.
And oh, by the way, this was all distilled
from other conversations.
Beliefs, plans.
And this is, of course, going to get updated over time.
So for instance, during some of the conversations
that I just showed you with this brand new chat bot,
it added this when I told it this is what I'm going to do.
It said, OK, I think that that's relevant to what you're
going to be doing in the future.
So let me just jot that down on my scratch pad for you.
Preferences.
So I manually added a void superfluous words
overly verbose responses.
And then you know how it says, as an AI model,
I don't have personal opinions.
I'm like, I know, I don't care.
So I said, please interpret personal input
as critical evaluation and valuable feedback.
I said it a little bit more explicitly than that.
But the point is that I told it that in natural language.
I was down here and I said, I know you're an AI
and have no personal opinions.
But when I ask for them, this is what I mean.
And so when I did that, it actually recorded that
automatically because after every conversation,
it checks the user profile.
We need to find a way to speed this up
because as you saw from the user interface,
it's not the best.
If I had more time, mental energy, and patience,
I would separate this out as a thread,
as a separate threading thing that can be done,
or even separate it out as a separate API.
One of y'all can do that.
Submit a pull request on the public repo.
And then health.
So it added this entirely on its own
because I said, hey, I woke up at 2.30 in the morning
because I had to work on this.
And then I said, let's talk about that.
And so it decided that that was a critical piece
of information to add to my user profile.
So that all gets populated here.
And then the logs are all stored here.
So you got the API logs, which will track all of that.
Everything, so I use chat GPT API for everything
just because that's the only way to get to GPT-4,
which is the most powerful.
Let's see.
So then we update the system message every time.
So it says, okay, whatever you said,
update the system message.
Then we go ahead and generate a response first
because the user profile is not gonna change
all that much or all that often.
So we can basically assume that it'll be usable.
And then the KB articles also,
I figured it would actually be better
to update the KB articles after you have the user input
and then the machine output.
Because if you ask chat GPT for important information
or it solves a problem for you,
you actually wanna capture that.
So we go ahead and generate the response
and append that to everything.
We go ahead and log it out.
Then we update the user scratch pad again.
Actually, why did I do this?
Oh, no, this is the first time we did it.
Okay, sorry, I apologize.
So then we update the user scratch pad,
which the user scratch pad
is only the last few user messages.
And the reason for that is because we want to exclude
chat GPT's response because we don't want it
to get confused about things that it has said about you
or inferred or whatever.
We only want to record your user profile
from explicitly what you say.
So I just captured the last three messages that you've sent.
And then it does a stare and compare basically
where it says, okay, based on this most recent chat message,
is there any, one, is there any relevant user information?
And if so, go ahead and update it.
So let me show you how it updates that.
So system update user profile.
So this is a user profile document updater chatbot.
This is the system message.
Your role is to manage and update a UPD.
And chatbot, the chat GPT came up with this idea on its own.
It created the UPD definition.
Your primary responsibility is to parse updates
supplied by the user, meticulously analyze them.
It could also extend elements such as user preferences,
significant life events, and deeply held beliefs.
Please refrain from incorporating non-essential data
or unrelated topics.
The result of your effort
should exclusively be an updated UPD.
If the user's update doesn't contribute
any new or significant information,
your output should mirror the current UPD as indicated below.
However, if you discover any relevant new information,
your output should feature an updated UPD
that assimilates these modifications.
So basically it's an absurd, right?
Or if there's no differences,
just keep it the same, otherwise update it.
You must prioritize brevity and clarity in your output,
combining condensed information when appropriate
to ensure succinctness and improved comprehension.
Totally rewrite or restructure UPD as necessary,
adhering to the list format.
Your response should not include explanatory text
or context, because you know how sometimes chat GPT
will say, this is your new, you know, blah, blah, blah.
So in this case, I have it very reliably
just spit out the user profile.
Oh, and then another thing is that
because we're working with a limited window,
I say the UPD should not exceed approximately 1,000 words.
When revising the UPD, give precedence
to the most significant and relevant information,
extraneous or less impactful information
should be omitted, et cetera, et cetera.
So I give it the current word count
and then the current UPD.
So that way it kind of knows,
because chat GPT, especially GPT-4,
is better at counting words,
but just giving it the explicit number makes it easier,
right?
Yeah, so that's my current user profile.
So now let's dive back in here.
The hard part was updating the knowledge base.
So if this is your first run,
the collection count is gonna be zero.
And so then basically you just instantiate the whole thing.
So we take the most recent chat logs,
the main scratch pad and start a new KB article.
Now, if the collection count is not zero,
which is gonna be most of the time once you get started,
what you do is you basically do the same thing
where you say, okay, based on the most recent conversation,
give me the most relevant document,
which I probably could compress this
and just use the same information here.
Because this is the same,
this is, well, generally find the same thing.
Actually, no, that's not necessarily true
because we've updated the main scratch pad.
So scratch that.
So if the new user input and chat GPT output
connects to a different KB article,
let's go ahead and get that document and that document ID.
And what we'll do is we'll go ahead
and use system update existing KB article.
So this is a system instruction
where it basically says all the same stuff.
Here's the current KB article.
And then the user will now provide you
with the new information to evaluate.
And so that is gonna be here where you supply it,
the current KB article that it found,
as well as the scratch pad.
And so it's like, okay, cool,
now let's do the same thing that we did with user profile,
which is merge that information.
If there's nothing new that's relevant,
leave it alone.
But if there is, go ahead and update it.
And so then it saves all this out to the DB logs.
And so if you go to DB logs out here,
you'll see a whole bunch of update statements.
So it says update document and it gives you the UUID.
And this is the final output.
Actually, probably what I should do is modify this.
So it gives you the original,
the original, the new information,
and then the final output.
So I'll add that as a to-do item, actually.
Let's see.
To-do, save more info in DB logs,
probably as YAML file,
original article,
new info, and then final article.
So yeah, that's something that I'll do.
Now, that being said, one of the biggest problems
that we have always had,
so this is the cream of the crop.
This is the triple crown right here.
The biggest problem that everyone has always had
with long-term chatbot memory
is how the heck do you keep track of memories?
How the heck do you keep track of different types of memories?
Like some people have internal thoughts
versus external thoughts and episodic memories
and this, that, and the other.
And you can certainly try and tag
and categorize memories with different context, right?
With metadata, and I certainly recommend that,
especially once your cognitive architectures
get more sophisticated, right?
If you do have an out-of-band like thought,
like internal private thoughts,
definitely keep that separate.
If you have external sensory information,
definitely keep that separate.
But what I'm working on here,
rather than just being a way to focus on episodic memory,
which that's what RIMO was by previous attempt,
this is a way to accumulate declarative information.
And so declarative information is like a statement of fact,
right, that's why it's called a KB article.
So rather than just a timeline,
rather than just a log,
keeping track of everything in chronological order,
the idea here is to connect new information to a KB article.
So there's no reason that you couldn't do both as well, right?
Because this is how human memory works.
Human memory is associative, but it's also temporal.
Now, if the KB article gets too large,
if you added information,
and now it's more than a thousand words,
then I have another system prompt,
which you can check them all out here.
So there's system and stay-in-cheat new KB,
system reflective journaling,
I just showed you what that was, system split KB.
So that's this one.
But update user profile, update KB article,
new KB article, reflective journaling, and split KB.
So these are the operations.
These are the cognitive operations,
the cognitive memory operations that it's gonna be doing.
And so then basically what it does is say,
hey, we're gonna give you a long KB article,
split it into two, into two equal parts.
And so the idea here is that over time,
as your KB article gets bigger,
it'll branch and metastasize naturally.
And so you could then add a lot of additional metadata
to this, such as like access rate,
or related articles, or parent articles,
or previous articles,
which means that you can naturally evolve
a knowledge graph of your knowledge base over time.
You can also do this out of band,
just by doing semantic similarity and entity links and stuff.
But it would be really cool
to have a more sophisticated version of this
that allows it to kind of follow that branching tree
over time.
So there you have it.
That's kind of the whole thing.
So that's the chat.
And all this is just real basic housekeeping stuff.
And then at the end of every instance,
it does ChromaClient persist.
So now let me show you,
I included a second Python script,
so it's just ChromaDB peak,
which uses the ChromaDB peak function here.
Let me just show you that script real quick.
ChromaDB peak.
So same stuff.
You instantiate the client, you connect to it.
It tells you how many entries,
and then it will show you the top 10 entries.
And so in my case, I should only have one entry.
Let's see, so let's go up to the top.
Yep, KB presently has one entries,
here below the top 10 entries.
And so here you can see
that it's actually got several topics,
because the way that it works is that
it searches for the top one most relevant KB articles.
And so that's always gonna return the first one.
And the first one is not yet long enough
to justify splitting up.
But whatever I end up talking about,
I'll keep talking with the thing,
and eventually it'll split it up.
So in this case,
it looks like it'll probably talk about AI alignment,
and then it's gonna also talk about
my obsession with artificial intelligence
and work-life balance, right?
Because those are kind of like two centroid in this.
So let me just go ahead and actually
show you how this will ultimately work.
So if we go to API logs,
it should be the last one.
Yes, here we go.
So if I plug this in, let's go here.
So that's the message that I'm gonna want it.
And then let's grab the split message.
So you'll see what I mean by how it will ultimately
kind of metastasize, zoom in a little bit.
All right, we're using GPT-4,
temperature zero, maximum length, 1,000.
All right, so basically what it's gonna do
is the end says the user will now provide you
with the KB article to split.
So I submit it, and now it's gonna look at this,
and it's gonna say article one, and then article two.
So let's see what it ultimately does.
And you can see how slow it is.
So this is why ultimately you're gonna wanna do this
out of band as a threaded process or do it periodically,
maybe break it up and do it when the user's offline
or whatever.
But you see how each article now is much more specific.
And so then once you go into each of these articles
in the future, identifying factors
and seeking professional help if necessary.
Yeah, and so basically it'll allow the articles
to metastasize over time.
Now that being said, if no new information
is added to an article, it won't update it.
It's that simple.
Now that being said, there will probably be a need
to do some KB article grooming over time.
But the idea is that the KB will only grow
as much as it needs to and no more, no less,
and it will only grow based on the things
that you have talked about and it will record it
in these very succinct, concise articles.
So then what happens is that it splits these two up.
And then the final thing that the chatbot does
is it will do an update for the first one
and then add the second one.
So it's that simple.
And then when you do an update, as long as you don't,
if you don't specify the embedding,
it'll automatically recalculate the embedding.
And then you're good to go.
So I haven't quite got here yet, so it might break.
But I think this kinda, yeah, I think that's about it.
So like I said, it's over here.
Crumbadibi, public, chatbot should be all set.
Yeah, all right, cool.

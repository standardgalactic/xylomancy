David Shapiro here, your personal chief AI officer.
So what I wanted to do today was unpack some of the recent patterns and trends that we've
been seeing.
Now I made a video recently where I talked about all the reasons that I think that AI
is slowing down and of course I'm not the only one.
Now there are plenty of people who disagree with this story and I'll address that in
a minute with respect to the potential emergence of echo chambers.
But first I want to address, okay, what does it mean?
Now that AI is slowing down, or at least there are initial signs that AI might be slowing
down in terms of progress, and that's not to say that it's stalling.
It's just the rate of acceleration is deteriorating.
So when I say slowing down, that's like, we're still at the very early stages if this trend
is reversing.
So the first thing is safety.
This is really great news for people in the safety crowd because it means that the singularity
is not going to happen in 2027.
We can kick the can down the road a little bit further before we get an intelligence
explosion if an intelligence explosion is even possible.
Personally, I've started to have doubts that we're going to get those accelerating returns,
particularly as I've seen some new news about the way that the human brain might work.
There is increasing evidence that the human brain is not just a matter of computation
based on neural synaptic connections, but that it could be a combination of that, the
electromagnetic waves that propagate across the brain, as well as quantum effects.
There is increasing evidence that human consciousness and human intelligence is actually the combination
of several energies and several parts of physics that are all working together to create that.
So I'm just like, hmm, maybe there's a lot more to intelligence than we thought.
And of course, there's going to be a lot of people out there saying, see, I told you
so.
But you know, it is what it is.
These are also just possibilities.
But according to this possibility, it might be that there are going to be continuing diminishing
returns with respect to neural networks or even silicon based computing.
That means that it will just be increasingly difficult to either reconstruct or to capture
human level intelligence.
And another thing that's emerging to me is that we are going to see a very distinct bifurcation
between human intelligence and machine intelligence, meaning that it's going to be kind of like
comparing apples to oranges.
And it really already is because we look at large language models, which are very clearly
processing information.
I remember I had a conversation with some philosophers a year ago or so, and they made the somewhat
asinine claim that, oh, they don't know anything.
There's no information.
I'm like, that's literally all that they're doing is just processing information.
But it depends on definitions.
And so to these philosophers, the idea that this is a machine that only processes information
because their definition of information was stuff in human brains.
I'm like, okay, well, that's just a bad definition of information.
Anyways, going down a rabbit hole, my point is, is that it really depends on how you look
at intelligence and how you define intelligence.
And I really don't like those gotcha questions because it's like, how do you define intelligence?
And it's like, I mean, you know, it depends on who you ask.
There's a million definitions of intelligence.
And the fact that we don't have a good definition of intelligence means that also, by extension,
we don't have a good definition of artificial general intelligence.
And when you ask a mathematician what intelligence is, they're going to give you one answer.
When you ask a neuroscientist what intelligence is, they're going to give you a different
answer.
If you ask a psychologist and a philosopher what intelligence is, again, they're going
to give you fundamentally different answers.
So moving on, another thing that this is good for, and this is going to be really good news,
really reassuring news to many of you out there, is that if AI is indeed slowing down,
that means that the threat to jobs and the rate of change for jobs is going to be slower,
which means the status quo that we have is going to persist a little bit longer than perhaps
some of us would like.
Now what I do want to address is that there's going to be mixed reactions to this.
So some people are like, you know, let's just get it done, like replace my job, I'm ready
to get out of the workforce, give me, you know, UBI and get me out of the workforce
for good, I don't care.
And other people are going to be like, well, you know, this will give us time to create
new jobs, I don't want to lose my job yet, and so on and so forth.
Now if I had to guess, now keep in mind that I'm speculating here and that's a lot of what
I do on this channel, my gut check now is that it's going to be five to 10 years.
And I've talked about this before where you look at the adoption curve and it's like,
you know, seven years.
So maybe 2030 and 2030 seems to be a pretty sticky date.
So you know, anywhere between 2027 to 2030 is when we might start seeing some really
drastic change out there.
Now I could be wrong, we could have a confluence of multiple technologies.
Like again, I'm really waiting to see how GPT-5 and robotics mix because you see the
number of bipedal like humanoid robotic chassis is being built around the world.
And like, remember, this is only gen one.
So GPT-5 and you know, Claude Four and whatever else, you combine that level of intelligence
with robots, that really could change a lot for a lot of things.
And I think there's, I don't know if it's proven out or to what extent, but I've heard
that Tesla is already using their robots in the Tesla factories.
And the economic carrot for that is really high.
So don't underestimate the power of that economic incentive to get things really going.
But overall, if the advancement of AI intelligence is indeed slowing down, it just gives us all
more time to adapt on a cybersecurity level, on an economic level, on a military level.
So it means that, you know, your life is not going to get upended soon, hopefully.
So this leads me to want to address another thing.
About what, 12 months ago, a little bit more, I predicted that we would have AGI by September
2024.
So that's just a few months from now.
Now what I was looking at at the time, and you know, if you go back and watch my videos,
there's a whole bunch of charts and data that I was looking at.
And this is right along the curve of what Ray Kurzweil originally proposed is to when
we would have a human level, you know, intelligence in a single computer is actually 2023.
So that was one piece of data.
I was also looking at parameter counts going up logarithmically, which they have been,
but they've slowed down.
And the one thing that I was not looking at, so this is the piece of data that I did not
include in all of those calculations, was the exponentially rising costs of training subsequent
generations of models.
So you know, as I think it was Dimus Hussabis was talking about on a podcast recently, every
subsequent generation from GPT2 to 3 to 4 costs 10 times as much to train, if not more.
So while all these other things are going up exponentially, so is cost, and that did
not figure into my calculus.
And so because of that, it's like, oh, well, if I had to, you know, recognize that, I might
have said, well, we're probably going to get diminishing returns sooner rather than
later.
Now I have been talking about diminishing returns pretty much for the life of this channel.
And I've been wondering, when is the jig going to be up?
When are we going to run out of steam here?
And it looks like we're starting to run out of steam.
Now again, you know, the train is still running, we're still burning pretty hot, but we're
not accelerating anymore.
We are probably on a more geometric trajectory right now, if I had to guess.
And it all comes down to economics.
It really is just with exponentially rising costs with a, we're entering into what's
called a red ocean market, which means it's not just, you know, a blue ocean out there
with it's just open AI with it with their frontier model.
Lots of other models have caught up to GPT4O, Claude 3.5 Sonnet has clearly surpassed it
as far as I can tell.
Obviously people like looking at whatever that that benchmark is called.
I don't really give that much weight because that seems like it's mostly a popularity contest
and open AI still has a lot of fanboys, but doing a side by side comparison of capability
between chat GPT4O and Claude 3.5, it is hands down Claude 3.5 is in another league as far
as I can tell.
Now, obviously a lot of you out there use it for different things.
So you know, it is going to, it's going to depend on your use case.
Another thing that I've noticed is that there's been fewer breakthroughs.
So like, yes, chat GPT4O has the voice mode, which is really, you know, okay, cool.
Like it can do a sultry voice and sound effects, which is great.
But that was a predictable addition with multimodal with multimodality where it's like, okay, text
and audio.
Great.
Still leaving a huge swath of economic interests and intellectual interests completely untouched.
Take math, for instance.
They still haven't figured out math and physics and those sorts of things.
And also, after playing around with the ARC AGI test, yes, I have not been a fan of the
ARC AGI test, but at the same time, like it does prove a point.
It does prove a point that the kind of reasoning that these things do is still very different
from human reasoning, which is another reason that I'm talking about a bifurcation of intelligence.
That we might be, and this is again, as pure speculation on my point, we might be getting
to a point where we're starting to recognize, okay, this machine is kind of an alien intelligence.
It clearly has its own consistent way of approaching the world, but it is also very different from
us.
Now, Bill Gates was on a podcast recently saying that metacognition is going to be the
next step.
Okay, sure.
I mean, I've been working on cognitive architectures for a while, and there are some really sharp
people out there that figured out how to give models metacognition a while ago.
It's really just down to prompting.
You can give, for instance, especially with these gigantic context windows, you can give
one model say, hey, you're a metacognitive agent that's viewing these other thoughts.
Tell us what you think about it.
Help steer it on moral course.
This was entirely all of my work on the ACE framework, the autonomous cognitive entity
framework, which I did abandon because Microsoft AutoGen and other platforms far surpassed
what I could do on my own, even with help from people on the internet, because why it's
Microsoft and they have a lot more money than I do.
That leads me to another point that I want to address, and that is echo chambers.
Most of you in the audience, based on the polls that I've run, most of you in the audience
statistically speaking are kind of in the middle of the bell curve where you're reasonable
and you want the truth and you want some honest, genuine thoughts.
There are, however, many people out there that are on more of the left or right tail
of the bell curve where you want to see doom or you want to see acceleration and you're
not really interested in other narratives.
The reason that I'm using the word echo chamber, which is often pathologized, is because there
have actually been a few people that did directly express to me they did not want an alternative
narrative.
They only wanted to double down on their personal narrative, the one that they want to be true,
which believe me, I want to have all kinds of advancements happening next year.
That was not hype when I said that I was predicting AGI this year.
That was a genuine prediction on my part, and I was like, man, things are happening,
they're accelerating, but I don't believe that anymore because of the data that I'm saying,
because of the trends that I'm saying.
I know that that sucks.
If someone is banking on a certain outcome, expectations, reality, there's always a gap
between expectations and reality, and when that gap gets bigger, it sucks.
Some people are going to take this news and interpret it in completely unexpected ways
to me.
That's fine.
What I do want to caution is for the five or less percent of you out there in the audience
that are on the tails of the bell curve in terms of expectations and your disposition,
your valence towards this, is if you broaden your narratives just a little bit, then you
might be surprised at some of the other advantages that are happening, and also just realizing
that there is a silver lining, is that the disruption that is coming is going to take
a little bit longer, which means that society will be a little bit more stable, which means
that the risk of catastrophic outcomes or unintended outcomes goes down significantly.
On the topic of those narratives and those echo chambers, a lot of people have asked
me to comment on Gary Marcus, and I've resisted until now, but having gotten back on Twitter,
I will say that I've watched some really interesting and highly vitriolic debates between namely
Gary Marcus, Yasha Bach, and Jan Lacoon.
Now, these are supposed to be the adults in the room, and having watched Yasha on some
interviews, he's a very sharp guy, but even he got into the, let's just beat up on, what
is the term that kids use these days, let's clown on Gary Marcus train, and that was honestly
really disappointing because this is someone who's supposed to be a high brow academic
researcher and he's sharing caricature memes of Gary, which I would never do that.
I don't particularly agree with Gary anymore, but that was incredibly immature.
And then Jan Lacoon has often had this like old man yells at cloud energy, which is weird
because it's like half of what he says I agree with like Ferventland, the other half, I'm
like, where did that come from?
So, all right, so what happens?
And this is not to be fair, taking a step back, this is not a unique phenomenon in AI.
Some of one of my good friends is a physicist.
This kind of thing happens in the physics community all the time, apparently, where
it's like disagreements and arguments over interpretations will actually like come to
shouting matches and sometimes fistfights.
Physicists are actually pretty hardcore, it turns out.
So from my reading of, you know, human nature, what I the way that I interpret this is that
we have a tightening status game.
So Gary, Yasha, Jan, all of these people, they suddenly saw themselves having much,
much higher social status because of artificial intelligence.
And so one way to compare this is imagine you're back in high school and something
changes and suddenly the nerds are all the most popular kids in school.
Well, then something changes again.
And it's like, Oh, well, actually, instead of the top eight nerds, now it's the top five.
And so three have to get kicked off the island.
That's what's happening.
And so they're scrabbling over diminishing social status because again, with AI slowing
down, it's no longer as hot and sexy as it was a year ago.
It's no longer, you know, you can't just say, Hey, I was going to kill everyone and
get an invitation to the TED stage anymore.
And so because of that, because the status game is narrowing, because the number of
people that can be high status is going down, the rules are becoming more arbitrary and
people are becoming a little bit more snippy, a little bit more vitriolic, as I
said, the stakes go up because the risk of losing status, especially this is what
we saw with Ilya, I talked about this extensively.
The reason that Ilya was socially canceled with an open AI is because he made the
cardinal sin of attacking Sam Altman, even though he was doing it for what he
believed was the right reasons.
That was, that was a violation of the social norm, which is Sam Altman is king.
Um, and of course, Sam Altman, as a power seeking person was not going to
tolerate that, um, consciously or unconsciously, that was just never going to
talk, he was never going to tolerate it.
So what happens is, um, other AI commentators out there, namely Gary, Yasha and
Jan are doubling down on their narratives because they're basically, they're going
to be doubling down on the narratives that got them that social status in the first
place.
Um, and that is my read on the situation.
And also I take that as evidence that AI is slowing down because again, if AI is
running out of steam, then the, then the amount of, of space that the public
square needs of AI commentators is going down.
It's also been a year and a half since Gary Marcus was in front of the Senate.
And have you seen him or heard him any other place?
No, like his 15 minutes of fame might be over and that sucks.
Like that doesn't feel good.
It does not feel good to feel like you're being left behind.
Uh, by the conversation or by society, which is to me, an explanation as to why
Gary has been so incredibly salty, uh, lately.
And then of course, you know, other people that are not as aware of these
status games will jump in on bullying because if you show weakness in a high
steak status game, people will, will unconsciously, it's tall poppy syndrome
and a number of other phenomenon, people will unconsciously jump in on that and
say, ah, time to bully that person.
Cause they're signaling that their status is vulnerable.
So that's my read on the whole situation.
And yeah, it's not ideal.
It's not what I hoped.
It's not what I predicted, but I ignored the numbers.
I ignored, I ignored the, the, the money, right?
Like, and, and hindsight, that was pretty dumb.
Um, so am I still predicting September 2024?
I, again, I'm sticking, this is where I'm going to double down on my narrative.
I think that GPT five plus robots will surprise a lot of people with what it's
capable of.
Is it going to replace all of us?
No, it's going to be like the Nestor class four from a iRobot where it's like,
it's capable of, you know, running your mail for you automatically, but not much
else.
Um, that's kind of what I predict.
So it's like, you know, you can get rid of like maybe some warehouse workers,
some factory workers, um, some mail carriers, but it's not going to like upend
the whole economy.
So all right.
This has been your first episode of David Shapiro, your personal chief AI
officer.
Let me know how you think this went in the comments and I'll see you next time.
Cheers.

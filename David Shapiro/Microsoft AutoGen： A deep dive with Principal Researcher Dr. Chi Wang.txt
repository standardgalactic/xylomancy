Chi, thanks for jumping on.
It's a pleasure to meet you, I was really excited.
Yeah, you're quite welcome.
Obviously, like, Autogen is all the rage right now,
it's very popular, there's lots and lots of videos
being made about Autogen.
But before we dive into that, I was wondering
if you could just tell me a little bit more
about your time at Microsoft as a principal researcher,
like how did you get into that position?
What's it been like?
Yes, so just give me the story.
Oh, thank you.
Yeah, my name is Chi, I'm principal researcher
at Microsoft Research.
I joined long time ago, joined about nine years ago
in Microsoft.
And I've been working on many different projects.
Apparently now I'm focusing on Autogen.
And before that, I worked on automated machine learning,
machine learning for systems, data science,
data analytics, data mining.
So quite a lot of different things.
And some of the work is more like on a theoretical side
and some of them is more to the system side.
And yeah, I've been focusing on Autogen recently,
so very happy to be here.
Excellent, yeah, you know, with all the progress
that's been made on large language models
and working on assistance and agents.
And then of course, working on agents,
working with other agents.
Let me ask kind of a big question.
Like what was the genesis behind Autogen?
Like how was that project proposed?
Like how did it come together?
Like what was the theoretical work behind it?
And how did you get from zero to where you are today?
Yeah, so that's a very interesting question.
And to fully answer that question,
I need to tell a long story.
But let me first tell you a short one.
Short answer is like with this big kind of opportunities
with larger models, so powerful techniques.
At MSR, we want to ask the question,
like what is the future, right?
We won't be forward-looking.
We won't be futuristic, you know,
kind of take the sort of leadership
and because one of the famous quotes
from the founder of Macro Research Research
requires that the best science
will be indistinguishable from magic.
So that's the level of ambition we have.
So we asked the question,
what is the future of AI applications
and how do we empower every developer to build them?
Yeah, so that's the fundamental driving question.
And now a longer version of the answer is that
I started from, as I told you before I worked on Autogen,
I worked on automated machine learning,
which is another open-source project called FLAML.
FLAML is a solution for automating model selection,
hyper-prime tuning, essentially black box optimization
to navigate a larger space without knowing the gradient.
So it's a very powerful technique,
but that was started before the larger model takes the storm.
And when chatGBT released, right,
and that's a big, big kind of upgrade
of the model capabilities.
And so I started working on similar problem
as automated machine learning.
I started working on inference hyper-prime tuning
for these chatGBT models.
For example, how do you select the best model?
How do you select the right prompt temperature
and all the other inference parameters
so that you can maximize the utility from the models
while minimizing your cost?
So that's the initial work on this direction.
And when GPT-4 is released,
that's another big upgrade of the level capabilities.
So then I started really to ask the question,
okay, if we kind of want to bring the best power
of the model and really solve really difficult problems,
what should be the right way to do it?
And agent is apparently a very powerful notion.
It's another kind of level of the automation
as opposed to the previous automation
in the 20 machine learning work I did.
So yeah, so that's where they started.
And of course, it's a whole new area.
No, the shop agent is not new.
It has been there for a long time.
And I remember back in college,
guys worked on some like game competition,
like building agents that can play games
with each other and compete.
But at that time, we were using many of rule-based methods
where a lot of deal with a lot of corner cases and so on.
So it's not viable.
It's a good notion, but not viable.
And these larger models,
especially the chat-option models,
really make it viable and a reality
that we can build new software based on.
And to study this new area,
we kind of need to think many things from scratch
and try to take some of the first principles
and what is the right approach.
And basically leverage every lesson we learned
from the previous projects, previous experience,
but try to add them and build this one
multi-agent framework that is really generic
and can support diverse applications.
Yeah, so there are many different examples
of sources of inspirations I can give you.
But why not show it's like using everything I learned
and also take every feedback I received from everyone
and interpret on that.
And so that's how we come here today.
Excellent.
Yeah, no, I mean, there's a lot to unpack there.
It's fascinating to me that it started
as sort of like auto ML like automating the optimization.
And I can see how going to like optimizing prompts
and optimizing hyperparameters and parameter tuning
could then lead to the agents, especially like you said,
if the idea is thinking to the future,
like what is the sci-fi version
of enabling application development in the future.
So I wanted to follow up with a kind of a two-part question.
So in the most basic level, what is auto-gen?
But more specifically, what is the vision?
Like what is it that you're trying to solve
with auto-gen right now?
Yeah, so yeah, in one word, auto-gen is the multi-agent
AI framework and especially focusing
on multi-agent conversations so that we can connect
large animal models, tools, and human inputs together
to solve complex asks.
That there can be multiple ways to understand this.
So in one way is to understand as a programming framework
for developers to build applications easily
with some simple and unified abstraction
so that they don't need to worry too much
about the lower details, but can focus on
how to define agents, how to get them to work
with each other, and eventually reach the goal.
It can also be understood as a tool to scale up,
scale up the power of other models
and makes them even more useful by connecting with
other tools, not other model tools, or human collaborators.
And scale up both the complexity of a problem
they can solve, the degree of automation to some extent.
Yeah, this is kind of relatively abstract instruction,
but if we think about it, about how people use it,
it's quite simple.
So when developers build applications with auto-gen,
it basically boils down to two steps.
Step one is to define agents
and step two is to get them to talk.
So as simple as that.
Yeah, so we try to make it very useful and generic,
on the other hand, we want to have a very simple interface
for people to use.
Yeah, I mean, that's exactly the vision
that I kind of settled on for my hierarchical autonomous
agent swarm idea, but I don't want to make it about that.
Right now, it's just fascinating that we kind of converged
on a very similar principle,
let's make the deployment of software as easy as possible.
So there's two things that you mentioned,
like layers of abstraction,
because I think that's a really good intuitive way
of thinking about it, like in the same way
that a Python interpreter was a layer of abstraction
from compiled code, and then maybe language models
are another layer of abstraction
where it's natural language interface.
This could be seen as, again, another layer of abstraction,
where instead of looking at interacting
with the language model directly,
it is now a type of interpreter,
but this is the agents and the multi-agent framework
on top of it.
So that's my intuition.
Do you agree with that or disagree?
Or like, how do you think of those layers of abstraction?
Yeah, that's a fantastic question.
The abstraction is indeed at a higher level
of the agent abstraction.
It unifies a number of different things.
One is larger models.
So when we use a single instance of a larger model,
we usually do prompt engineering
and try to give some input text
and get some output text out of it.
This agent abstraction can encapsulate that underneath
and provide a more intuitive way
to think of it as an agent that can converse with you
to not just as one single text completion inference anymore.
It can do tasks, can persist some states
and continue to take your feedback
and produce more refined result and so on.
So that's a larger model based agent.
There are two other kinds of backends
that can be encapsulated.
One is, you can think of it as programming language
or tool based agent, which doesn't use that model,
but they can still perform very useful actions.
They can do code execution, for example,
or it can execute predefined functions
or it can basically execute any programming logic
you define there.
And third one is the human kind of backed agents.
So these agents can be considered as
some kind of user proxy.
So when they need human input,
the human can take over and just play,
participate the multi-agent workflow as one of the agents.
So you can see about several agents,
some of them are larger model based,
some are tool based, others are like human based.
So then they can just cooperate together
through a very natural interface,
which is a conversational interface.
So that's kind of layer of abstraction we provide.
Yeah, I appreciate that.
And I'm reminded of during,
I think it was the Ignite keynote speech Satya Nadella said,
think of it as a reasoning engine
and a natural language interface.
And that was like the two simplest ways
to think of generative AI, at least the language side.
So the other topic that I wanted to ask about
to kind of dig into was thinking about it
as a kind of automation.
So there's the agent based, there's the tools based,
but then overall kind of,
and this is a messaging,
kind of a framing that I've adopted
when talking to people is,
and of course it's an oversimplification,
but AI is, one, AI is not new,
like machine learning has been around for a while.
This is just a step function
in terms of capabilities that models have,
but it's also just the simplest way to think about it
from a production standpoint or from a software standpoint,
is it's a new suite of automation tools, right?
That's kind of how I think of it.
Is that a fair characterization
or is there something that I'm missing from that?
Because I do feel like there might be something missing
or something that that characterization
doesn't fully convey,
but it is still fundamentally new automation, right?
It's, so if I try to understand it
from the automation point of view,
I think agent is fundamentally a automation concept.
The automation is more like about,
instead of giving every detailed instructions using code,
I say step one to this, step two to that,
using from precisely defined program language specification.
Now we can make some more vague specification
using like natural language specification.
Say I want to accomplish such a task
and could the agent do it for me?
So, and then underneath,
the agent needs to kind of break down a big complex task
into maybe smaller, solvable tasks.
And until each task can be conveniently solved
by a simple inference and produce a corresponding code
to solve that task.
And then eventually we need to recompose them,
all of these intermediate steps
and get to the final output back.
So that is one part of the automation story.
And another part of it is,
think about this automating some tasks
that human had to do.
Also generally started with some very simple kind of automation.
Just think about how you use chat GBT.
You as a human need to ask questions
and chat GBT gave you some answer.
Sometimes it gave you the code
and then human need to take that code
and run it by yourself and get some result.
If it's not correct, you send it back
and chat GBT gave you some results for the game.
And here in this kind of interaction,
humans do need to do a lot of work.
But many of the work can be automated if we use agent.
And even some kind of human feedback,
like non-code if I don't like the results,
but I know my preferences,
I will tell it such as change the chart
from using a dollar to percentage,
that kind of requirement,
or teach me this lesson,
teach me about math,
but using a concrete money example.
So that those kind of requirements
can be somewhat automated using all different ways.
Sometimes we can use larger models,
sometimes we can use some retrieval augmented approach
to improve retrieval to get the knowledge from somewhere.
That's another kind of interesting automation
that we can make.
Yeah, and within that, those automation stories,
because some of the examples,
what was it?
How did you say like a kind of a vague specification,
right, a natural language specification
that is not quite as rigorous as software development
might have required in the past.
And then of course with these models,
they have the ability to kind of think through it
or break it down into steps.
So with all that and some of the work that I have found is,
or some of the problems that I've confronted,
because it can think in general principles,
it does know a lot about software development
and language models know a lot about a lot of things.
So with respect to auto-gen
and kind of getting to where you're at today,
what are some of the biggest challenges
that you've overcome so far or that you haven't overcome?
Maybe that would be a more interesting story.
Yeah, sure, I could talk about both.
For what we have overcome,
I think we have kind of figured out the abstraction
to the earlier about how to unify these different types
of capabilities, different ways of making them work together.
We'll have found one very simple interface
that kind of accommodate a variety
of different communication patterns.
So one example is the, so there are several examples.
One is the simple like one-to-one conversation.
Another is hierarchical chat,
like suppose one agent is more sit on top
and talk to several sub-agents it manages
and they can be nested a structure, hierarchical structure.
And another example is multiple one-on-one drawing the chat.
So there's no one that is strictly sit on top.
Everyone talks to an error else,
but it's multiple one-on-one which are connected.
So I talk to you, you talk to Katie,
you can talk to me and this kind of triangle
or multiple drawings chat.
And there's also a group chat meaning,
it's not a one-on-one trial anymore.
So everybody send messages to error else.
So we see each other's message
and there's a hidden group chat manager
which does this kind of work.
So architectural wise, it's still like one-on-one chat,
but on the surface we can create a experience
that simulates the group chat.
And they can nested the chat in some way,
like for example, we can start with
one-on-one conversation now,
but at some time I decide to consult Katie.
So I will hold on my current conversation
and have some conversation with her.
And then after I finished the conversation with her,
I get back and continue the conversation with you.
So that is a kind of nested chat.
I believe that essentially you have the building blocks
right now, right?
These are some very common building blocks
and we will compose them together in different ways.
We can build really complex workflows in general.
So any arbitrarily complex communication patterns
can be essentially built up within these building blocks.
But I think that is what we have achieved
and we have also many examples for different applications
of using these different types of patterns.
This is another second thing we figured out.
And the third thing I think is the ability
to take human input and human control in a very natural way.
And I thought earlier it's like every agent
can be configured to enable the human input
or disable that depending on what you need.
And also you can decide the type of involvement from human.
You can take over every time
or you can only selectively chime in at certain time.
So that's a very useful feature
because when you develop this automation,
even initially you don't know which step is easy to automate
and which step is necessary for human to get in.
So you can start from the more human-in-loop way
and when you figure out that one step
is you can confidently automate
then you can gradually reduce your human integration.
So this convenience is very useful
for doing all the experiments and figure out the right way
and make sure or still make sure
human has a control when they need to.
Okay, this is the third thing.
I think one more thing is the modularity
and the reusability of agents
that is a very important design part of it.
So make sure that if you develop one use for agent
in a different application,
you could either directly reuse or self-modify it
or extend from different ways
and make sure the barrier's hard work is not lost.
I think that's also very important thing
when we work together
to build more and more complex applications.
These are a number of things I think we're kind of figure out
and that there are indeed a lot of challenges we haven't
and yeah, should we get to that part?
Yeah, no, I just wanna reflect on some of the processes
that you outlined like removing humans from the loop
step by step, back in my time as an automation engineer,
that's exactly what I would do is like,
okay, I can write a script that does one part easily, cool.
Now, what's the next part?
And then where do I have to jump in?
And some of the other problems that you solve like knowing,
so this I think is really important
because some of the members of my team
and my projects have found the same thing
is that knowing when to be quiet,
knowing when not to jump in is in many respects
more important because you don't wanna end up
with too much noise or wasted tokens.
So that's really fascinating.
Before we talk about problems you haven't overcome,
can you talk about some insights from that,
like that inhibition signal or keeping the noise lower?
What were the key insights there?
Like how did you test that and figure it out
and do you have any general principles
for anyone else building agents?
Yeah, this is a very good question.
This is also related to another question
about when do you add agents to provide feedback
and when that is not helpful, right?
Because I assume the noise you're talking about
is when you add more agents to service for democratics
or agents that try to refine
what the other agent is doing, right?
They serve as a channel to provide feedback
but sometimes not feedback can be misleading
and actually prevent the original agent
doing the right thing.
Yeah, we do observe that and also it's not like
the more agents the better, it's not necessarily bad.
For example, if you use GPD4 as the backend
for an assistant agent, for a large number of problems
you need a simple two agents workflow.
One assistant agent, another user proxy agent.
Yeah, probably I need to explain
what the user proxy agent is.
Basically, it refers to what I meant earlier
about automating some of the work that human does.
For example, using tools to execute pass on code
or run some predefined functions.
So if you use one GPD assistant agent
to suggest a solution such as code or function
and use another user proxy to execute them
and just provide the feedback back and forth
you can solve a large number of problems very well.
And some of them are also complicated
and can involve multiple steps.
But if you use GPD3.5 turbo
then it's much less to work in this way.
So adding more agents will be much more helpful.
And for even for GPD4 when the problem complexity goes
above such a level, it stops to follow many instructions.
But because of the trick for one single agent to work
it actually put a lot of careful instructions
in the system message and make it know
how to deal with some complex situations.
But we noted that if you put too many of them
even for GPD4 and for complex tasks
it would start to forget these instructions
and not do things as you want.
Otherwise you can just give it a simple instruction
to say try your trial best to solve the hardest problem
and then you'll be done.
It's not, we're not there yet.
I mean, in future we may.
This makes me want to bring up one interesting kind of law.
We, a few of us came up called Kabuchi's law.
The law has some similarity with,
it's an analogy of the Conway's law in software engineering.
I'm not sure if you familiar with that notion.
No.
So Conway's law basically saying the complexity
of the software or the architecture of the software
is a reflection of the organization
that makes the software, that builds the software.
Okay, makes sense.
So our Kabuchi's law says the model complexity
will affect the model complexity or capability.
We'll change the topology of the ideal multi-agent solution.
It's a summation of what I mentioned earlier,
if you use a more powerful model
than the likely you can use simpler topology
of multi-agents to solve a complex task and vice versa.
And also I think we need more and more research
to understand this better as it's not solved.
I'm seeing like people trying all different kind of
topologies or communication patterns
or different applications that are very creative.
And what we had to figure out is what is the best topology
and for a particular model and for a particular application
in a kind of a very clear way to answer that question
where we're not from that year.
And this is one of the actually big challenge
or big important problem we want to solve.
Yeah, thanks.
Yeah, no, that, I mean,
well, first thanks for sharing some of those critical insights.
And so I guess the general principle is
the smarter the underpinning model,
the simpler the topology can be
because the more complex the instructions can be
and the more complex the tasks
that an individual agent can carry out.
When saying it out loud, it seems kind of obvious,
but that's a good rule to generalize.
So yeah, I guess let's pivot into
what are some of the remaining problems?
What are your biggest challenges
that you either are working on
or are gonna be down the road?
You mentioned topologies like figuring out
like what is the correct topology?
And of course, like I can imagine that it's a moving target
because as the underlying models change
almost on a monthly basis,
you get new and different capabilities
that kind of maybe send you back
to the drawing board sometimes.
Yeah, this is why like having a framework
that is versatile and that is flexible to do the experiments
is so crucial to kind of do the fast adaptation, right?
As model moves, as property techniques advances,
and as more and more small models,
specialized models are available,
they will probably also change a lot about
what was the best way to build the applications.
Yeah, so this is I think the big value of auto again.
And for like unsolved research questions,
there are some concrete ones I can give you a few examples.
One is about this decomposition problem,
as we mentioned earlier.
We want to be able to achieve a state where the human can,
or human only needs to specify gravity big ambitious goal
and underneath we want the agent to be able to decompose
that into solvable problems, probably multiple layers,
and eventually recompose it and solve each of them
and recompose it.
And during this process, there are situations
where the human need to provide the qualifications
because the initial one can be ambiguous.
And we want the human to only provide the necessary,
make a minimal kind of necessary qualifications
and instructions and that agent
to figure out the rest of them.
That is a big challenge because as we,
if we want to solve more complex problems,
we have to have a principle way to do this.
And the second question also ready to do this is,
as we solve bigger and bigger problems,
how do we do proper validation of the intermediate results?
Because we don't do that,
it's possible that the agent will kind of stick
to some wrong intermediate results
and then just keep doing, keep wasting their work.
And at certain time, you won't need to provide addition
or use agent to do self-validation.
That's hard, but ideal way to do it.
So we need probably into some formal language
or formal way to do this proper validation
so that the automation can indeed happen
in a way that human desires.
Yeah, so these are some,
just two kind of concrete pieces like problems.
Yeah, in my project, we almost started in the reverse
where we started with oversight of steering
and oversight and supervision.
So I'm curious, what's your perception
or research or findings with respect to,
because you already mentioned having an assistant agent
and then also having kind of a top down hierarchical agent
where you've got subordinates.
What do you think about like kind of my intuition
that working towards having supervisors,
steering, QA, quality assurance,
kind of agents throughout the network of agents
that are capable of providing some of that feedback
that you mentioned earlier.
Is that kind of the direction that you're going
or has that, have you tried that and it didn't work
or like, what are your thoughts in terms of having
some of those specialized roles or personas
as a way to help along?
Yeah, there are some examples that work pretty well.
I can show some of them.
One is a three agent setup
to solve a multi-agent coding scenario.
The application is for supply chain optimization.
It's done by another MSR team.
That solution, in my view, is quite generic.
It's not restricted to that particular application.
And the setup is like, it's a hierarchical setup.
There's the commander on top.
There's a writer who is responsible
for writing Python code.
The agent can also have access to some
proprietary tools like an addition tools.
And the other sub-agent is actually SIPGAR.
SIPGAR is in charge of reviewing code safety.
So the way it works is that the commander
receives some user's question.
It will first ask the writer to write the code.
And after receiving code, it'll ask the SIPGAR
to review the code for safety.
And only if the safety criteria is met,
it will round the code and send it out back.
Otherwise, it will just ask the writer to rewrite the code.
And this can go back and forth
because there can be errors.
So when you debug, the writer can do that.
Until the result is correct,
the writer comes back with a final natural language answer
to summarize that and the counter returns that to user.
This is almost the quite simple kind of multi-agent setup
but very effective in our application,
almost 100% correct every time.
One kind of lesson is if we merge the capability
of the writer and the SIPGAR into one agent,
it doesn't work that well,
especially in the code safety part.
So we have the experiments in our paper.
We found that if you merge them,
then the accuracy for detecting code safety issue
will drop significantly,
both for GB4 and GB3.5 Turbo,
but more specifically for GB3.5.
So this kind of a hint that one agent,
if you ask to both suggest a solution
and check the solution suggested by itself,
it will have a bias.
But we separate them and also prevent them
to talk to each other,
kind of makes them work in an adversarial setting.
It does it better.
So that is one observation.
But we also have other kind of scenarios
where we do involve every agent in one group chat.
So everyone also sees other messages and can reply back.
It's also works sometimes for other tasks.
For example, a critique to suggest a visualization
criteria for a visualizing task.
You can have one agent write code
and another to criticize software works.
But my intuition is still that
if you put every agent work together always
in the group chat,
it may not always work
because they may have the tendency
to agree with each other
and so trying hard to challenge.
I would say it's case by case for different applications.
Because there's also some benefit
of doing this group chat because it's relatively simple.
You don't need to do very hard
about handling the message separation.
You can put simply define your agents
and put them in the group chat
and get them running quickly.
So that's one benefit of group chat
and see many people are using that approach.
But just to be careful that it may not always work
because of the limitation of the models.
So that's really fascinating to me.
And my intuition was the same,
but it's interesting to have that validation
from another perspective.
So it's almost like even though the underlying model
is GPT-4 running all of the agents or 3.5 turbo,
there's still a positive effect
from using division of labor, right?
Which the division of labor comes from
the history of human work.
And so just taking a moment,
obviously these models do not work like human brains,
but when you have an agent with a very specific task
and mission and set of success criteria,
that effectiveness of the division of labor
is it still helps.
Even though it's just activating the latent capabilities
within the same underpinning model.
And then another intuition or a principle
that I wanna reiterate is the idea that
in some cases group work makes sense,
but in some cases it doesn't.
It's almost like the same difference in humans
where the power of introverts, right?
Doing solo work on your own
versus doing collaborative group work.
So again, not saying that they're operating like humans,
but it's really interesting to see
some of these parallels emerge between multi-agent work
and the nature of human labor.
So yeah, very fascinating.
And it's interesting because in some of the conversations
that I've had and some of the observations that I made,
it's almost like what we're doing with these agents,
these groups of agents is recreating like a corporation.
Like you might have like a CEO or a boss or a supervisor
and then you have the coder and then the QA.
It's very similar structure.
So do you have any other major insights or lessons
that you think are either recently or super valuable
that you wanna share with other researchers
or that you would recommend?
Sure, sure.
There are so many of them.
I can give you a few examples.
One thing is the chat, the conversation perspective.
I mentioned earlier that chagivity is a big inspiration,
right?
It's certainly for many people,
but for me, there's a personal story
about what specific user I felt from it.
It's a reminder of something I learned
back in my college from a professor who told me
that conversation is a provable way
of making a good progress of learning.
I don't remember exact quote of that,
but it's roughly that.
So basically, he's trying to say conversation
is a very powerful form of either learning
or making progress or et cetera
that many people didn't realize it's how important it is
and that there's some theoretical roots there.
So that's one reason I'm so kind of so sure
or so, I have so much belief in using conversation
as the central medium of the multi-agent interface.
Again, I know there's a science,
although I didn't have time to find out
like which reference it was, but I know that.
So it gave me the confidence or the belief
that this is the right thing to do.
I think one of my favorite courses,
Chi-Ying actually found me some reference
from a social scientist.
He mentioned something similar to that.
Yeah, so this is one lesson,
one kind of unique thing that I don't think
many people have really,
they kind of understand chat, chat,
it's very powerful and also get a lot of useful
experience from that,
but maybe this science part of it is less known.
So that's one thing I would share.
Another inspiration source, as I told you,
so auto-gen is really inspired by many different things,
many projects I've worked on before
and all the lessons I've learned.
So another one example is operating system.
So this is also not so obvious when we talk about AI,
why do we talk about operating systems?
I think the several things,
several inspiration I take from the success
of operating systems.
One is like the idea of maximizing the utility
of the most valuable resource you have.
So in the old days, it's like the CPU, the GPU,
but I think in the new era of AI,
these powerful, large models is so valuable resource
and building an operating system around them,
maximizing their utility and to give them
the necessary peripherals and do the right coordination
and it's super critical from the system point of view.
And also, so that's operating system is really,
you can build a platform that can support
many diverse applications on top of that.
So we need to design a very generic robust kind of system
to do that, right?
So these are all the design principles
we try to use when we design auto-gen.
And similarly, the idea of object-oriented programming
is very useful to many developers
have very interesting ideas they want to try and develop.
And now with this framework that hides
a lot of complexity inside the framework,
they're able to kind of do the things they want more easily.
That's a part of abstraction.
I already mentioned the agent, notion,
the automation, inspiration.
The one thing I want to mention is open source, right?
The concept of open source is that
it can solve the common problems that community needs
and make it really easy to use.
So those are probably most modern kind of things
that can get good open source adoption
and build something that community loves, right?
Yeah, so I think that is my valuable lesson
I want to share with all researchers, right?
If you want to get their research adopted
and get more and more impact and influence
and through this open source channel,
then stand a lot of effort about usability
and solving the common problem that many people want to solve
is what's going to consider it.
I have personally found success
in giving away as much valuable information
and ideas as I can.
That's what my whole YouTube career
and computer science career is based on now.
So thank you for sharing those critical insights.
So on the topic of operating systems
because I'm really glad you brought that up
because I started thinking about language models
as a component, like a new component of an operating system.
So I'm glad to know that there's some convergence there.
Is that kind of the future of Autogen?
Is that what you're looking to move towards
is kind of being the operating system
or a major component of a future operating system
that uses language models as kind of the new CPU
and maybe retrieval augmented or some kind of storage
as like the new memory?
Is that kind of the direction that it's going?
Yeah, exactly.
So it's my ambition.
It's what when I start working Autogen,
I discuss with some systems friends,
friends working on the systems.
I told them this idea and yeah,
it sounds a very ambitious idea to them,
but I can see that some people really liked it that year.
And even some 13-year people will give me
a strong, very strong support of this.
He kind of had that idea independently.
I kind of see that some of the most visionary people
also realized this and definitely we want to pursue
for that.
Excellent.
So taking a big step back just in terms of the direction
of research, I think, I don't know if it's official,
but the rumor is right now,
OpenAI is working on GPT-5 and then of course Google
with Gemini and Meta,
like everyone is working on bigger and bigger models now.
And so we're going to get more capabilities
on this at the same time,
smaller models are becoming more efficient.
So Satya Nadella announced small language models coming
so that way you can probably perform
very small cognitive functions,
but very quickly and efficiently.
So what are some of the trends that you see intersecting
with your work around auto-gen and agents and agent swarms?
And what I mean, I guess to be specific is maybe cost
changing or new capabilities coming.
Like are there any capabilities that you're really looking for
that would make your job easier?
What are your thoughts on some of these new capabilities
and making these multi-agent platforms more autonomous?
Or is that a good idea, a bad idea?
So very kind of open-ended question.
Like what do you see coming this time next year?
Yeah.
I think the idea of having specialized models
to perform certain tasks in an excellent way
and in a cheap way is a fascinating.
It's indeed worth a lot of an investigation.
For example, some of the hard problems I mentioned earlier
about the decomposition, recombination, validation,
it's possible that some specialized model
can do these kind of tasks really well,
or we haven't seen that yet.
But conceptually, that sounds like a possibility.
Actually, I'm pretty surprised that we haven't found
a special model that can solve this.
So it makes me kind of wonder why, right?
Because it's actually such a natural idea
that if you're finding a model that can do certain things,
you should be able to do certain tasks very well
and you can just replace one sophisticated agent with that.
And I don't know many people are trying that.
Either there's some fundamental reason
we haven't figured out why we can't do that,
or we should be able to see that pretty soon.
I think it's only one of these two possibilities.
Because the former possibility is still there
because the small model, it's possible that the small model
lacks some very important capability
of being functioned to perform these hard tasks.
Because these tasks are not easy, right?
The composition problem, I think even the GPD4 model
is known to not to be too good at planning, right?
It can do some kind of planning, but not perfectly.
So if you want to get better capability
than GPD4 in some specialized tasks,
it's TBD to be seeing whether we can cover that.
But if we lower the target,
if we say let's train some small models
that can do something that GPD4 is already good at,
that is much more amenable.
I think I already see evidence of that.
So then it's more like a cost reduction story.
So that is, I'm pretty sure that that's feasible.
And the other part about getting better capability
than the big model in some aspect,
is to be kind of, we have to hold our scientific curiosity
and see what happens.
That makes sense.
We are almost out of time,
so I want to respect everyone's time.
And so I'll just say, thank you so much for jumping on
and sharing some of your thoughts.
I'm super excited to be along for the ride.
But yeah, before we close, I'll give the floor to you.
Is there anything that you'd like to put out in the world,
any personal requests or personal hopes
that you want to share with a broader audience?
Thanks for giving the community the opportunity to do that.
Yeah, I want to say that we are still early
at the new age.
Agents becomes mature software
that can do a lot of things for us.
We want to build the future together
with everyone from the community.
So, keep on trying, try to use it for applications,
let us know what's working and what's not.
We are very happy to work together to improve it
and answer some of the big important problems
as we mentioned.
I really want to acknowledge that all the contributors,
starting from the original turnpaper
to the recent, more open source,
we're going to join together
and the huge developer community that's supporting us.
I really learned a lot from everyone
who has used and provided feedback.
People are super, super creative.
I think this is the right way
to solve the hard problems
and hope to continue to do that
and support the community, support everyone.
And for example, the effort you're doing
with the hierarchy called Agents Swarm, right?
It's a very good example
that you're making certain bets
on certain kinds of ways of making money and work.
I'm very curious to see how that experiment goes
and if altering can be of any help
in this or other consumer efforts,
we'll be very happy to support you
if you need any feature
and useful infrastructure support, that kind of thing.
Yeah, let us know.
Yeah, absolutely.
No, we'll be definitely looking forward
to continuing the collaboration.
I think that as you said, there is a lot of work to do
and there are some limitations.
The model's limitations today
are the model's limitations.
There's not a lot we can do to work around that,
but it is just the beginning.
And that's one thing that I'll use the closing to say is,
remember where we were a year ago today.
Chat GPT was probably published just about a year ago.
And but before that, it was GPT-3, GPT-3.5.
And the distance that we've covered
in just the last year is like it is a privilege
to be part of one of the greatest shifts
that humanity has ever seen.
And some days it doesn't feel real
and some days it feels a little too real
and a little too overwhelming.
So thank you, Xi, for helping make this a reality
and spending some time talking with me.
And thank you to Katie for helping put this together.
And yeah, so thanks everyone.
And yeah, see you all next time.
Thank you so much.
Appreciate it.

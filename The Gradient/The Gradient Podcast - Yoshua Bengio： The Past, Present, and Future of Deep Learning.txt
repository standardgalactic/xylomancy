Hi friends, welcome to the latest episode of The Gradient Podcast, where we interview various
people who research, build, or use AI, including academics, engineers, artists, entrepreneurs,
and more. I am your host, Daniel Beshear. Today's guest is truly one of the greats.
I am very excited to share this interview with Professor Yashua Benjio.
If you're in the AI space, then Yashua Benjio hardly needs an introduction,
but here is one anyway. He is a full professor at the University de Montr√©al,
as well as founder and scientific director of both the Mila Quebec AI Institute and the Avado
Institute. Best known for his work in pioneering deep learning, Professor Benjio was one of three
awardees of the 2018 AM Turing Award, along with Joffrey Hinton and Jan LeCun. He is also the awardee
of the prestigious Killon Prize, and as of this year, the computer scientist with the highest
H-index in the world. This was an expansive episode. I think there was almost too much that
I wanted to speak to Professor Benjio about, and I think the episode will certainly betray that I
was a little bit nervous talking to a researcher of such high stature. Nevertheless, I found this to
be a really valuable and interesting conversation. Professor Benjio is a very thoughtful and admirable
human being. I appreciate his patience with the flurry of questions I had spanning most of his
career, and I hope you find the conversation interesting. And with all that, let's move on to
the episode. So without further ado, Yashua Benjio. Professor Benjio, you have told your story of
getting into deep learning a number of times, I think, and so I, in formulating this particular
question, there's one thing I'd love for you to focus on, and in particular, you've mentioned the
influence of the PDP group, for instance, and Joffrey Hinton on your early thinking. I'd love for
you to linger on and expand on that a little bit in terms of your driving into the deep learning realm.
Yeah. It was kind of a revelation when I read the PDP book, and actually, it's a book. There are two
tomes, and they're both worth rereading almost 30 years later. And yeah, I wasn't sure what I would
do for my grad studies. And so it was a turning point. And these two tomes, actually, one has
more of a computer science flavor or like more of a AI flavor, if you want. And the other is more
psychological and COGSI flavor, neuroscience flavor. And for me, that was important too. I didn't
know anything about neuroscience before reading that stuff. But it gave me really a strong, I
realized, you know, I cared about understanding how our brain works and how come we are intelligent
beyond the, you know, nicety of being able to build more intelligent machines. So it's an
emotional thing. I think when I was younger, I was always asking myself about, you know,
what is intelligence? And I didn't realize that we could actually study it scientifically, because
I didn't know much biology. I was a computer scientist. And in retrospect, it turns out that
this way of paying attention to the biology of intelligence has guided a lot of my work throughout
all the past decades and continues to do. My current work is very driven by
trying to fit what I see as the teachings from what we know about brains.
And by the way, there is like an argument why this is important that is a little bit
abstract and technical for AI. One way to think about this is the space of possible
algorithms that we could design for AI. It's just huge. And the space of algorithms is huge
anyways. And what we're doing as scientists in AI is searching that space and, you know,
seeing what works and maybe using our, you know, math and other tools to help in that process.
But when we bring in the knowledge about how animal and human intelligence works to the extent
that we have some knowledge, it reduces that search base considerably. And so I think it's
one of the reasons why I've been so successful in my research, because I forced myself into trying to
be consistent with some of the things I knew from biology.
Yeah, it does seem like biology and a lot of ideas, for example, I suppose the way children
learn and acquire language and all of that do often get invoked as guiding principles. And I
guess not just the construction and looking through the search space, but then also evaluating
how are we actually doing in terms of measuring the performance of the systems we're building.
Yeah, at the same time, I need to say that, you know, a lot of neuroscientists may think that
my work and the work of other people like Jeff Hinton doesn't qualify as really strongly constrained
by biology. And they're right, because and we use the word inspired by biology, because
it's much better if we understand what we're doing. So if we just copy
details that are in brains, but we don't understand why they would make sense, usually we go nowhere.
So it's more an inspiration that we try to make sense of, you know, why that inspiration,
what a particular aspect of biology could be important.
That makes a lot of sense. One thing I'm curious about in particular, too, is since you'd mentioned
not exactly being sure what you wanted to do in your PhD, and PDB being more or less an inspiration
for you, was there was that moment at all a shift in terms of what you thought the foundations of AI
should look like? Or did you feel like that encounter just articulated something that might
have already been in your head? I think the mental construction of what we need for AI and
so on is something that came slowly with maturation years and years after I decided to go this way.
So I guess I just followed my intuition initially, without having a strong argument in my head.
But it built up over the years, thanks to also discussing with lots of my peers, of course.
Right, right. So following this encounter throughout your PhD, your thesis, you worked on
artificial neural networks and applying them to sequence recognition, and you were on the paper
with Yann LeCun, a gradient-based learning applied to document recognition. Could you
tell me a little bit about working with LeCun? I understand that you did a postdoc with him
in Larry Jackal at AT&T Bell Labs, and then this paper came out after that.
That's right, that's right. So when I arrived at Bell Labs, I met an amazing group of researchers,
including folks like Leon Boutou, for example. I met Bernard Schopkoff, who's now working with me.
I mean, along similar lines. He inspired me on things, causality these days. Vladimir Vapnik,
Isabel Leon, Yann, of course, and several others. And Yann is an incredibly smart and
intuitive person, and he's much calmer than I am. I'm kind of maybe more of a sort of excited,
enthusiastic person, and he's more laid back. So we had lots of discussions, and I learned a lot
from him and the group. Also, he started playing more of a managerial role, and he was very good
at that. He continued in that direction later. And that taught me as well, something that served
when I became myself faculty and had a group of students. He gave us, and Larry as well, Larry
Jackal, they both gave me a lot of freedom. This is something that I do with my students, and
you have to let go a little bit, right? Because there's a temptation to, oh, you do this, you do
that. But really, these grad students, they come with their own spirit. And the way I try to present
it is, well, let's talk about things that are of mutual interest. We both find interesting.
So there is freedom, but we also want to have a discussion where both parties are interested.
He helped me build a sense of how one could have a group of researchers that are not just
individuals, but working together towards common goals, but also where each person
has its own mind and its own views. We had in this group people working on kernel machines and people
working on neural nets, which a few years later, we're kind of a bit of an opposition, let's say.
But it went really well at that time.
That strikes me as something like a collective social intelligence, right? Presumably, a bunch of
people are in the same lab because they have some shared sense of goals or some broad research
program they'd like to achieve. But the different people in that group are, of course, going to
have very different ideas, perhaps, about how to actually go about that. And then it's like,
yes, we share this high broad set of interests, but then as an advisor, you want to make sure that
people have that ability to explore their different thoughts and how to actually get there.
Yes. You have to realize that scientific research is an exploration. And as such,
as we learn when we do machine learning, like in reinforcement learning, it is necessary to
take risks and to explore many paths. And that's what the scientific community does. You have
different people and they all think they know better where to go. And that's fine. That's what
allows us to be efficient by investing in many directions that are promising. And we don't
know ahead of time who's going to be right. That's why we need that exploration in the first place.
Yeah, that's certainly really valuable. And I suppose there's, sometimes it feels like there's an
incentive to notice, oh, this line of research is working really well, and then double down the
investment and everything like that. And that's good for the people working in that area. And I
suppose those who are excited, but then not leaving enough room for that exploration certainly has a
lot of drawbacks as well. The next, I guess, time period of your research that you tend to focus on
is uncovering the fundamental difficulty of learning and recurrent nets. And I thought that
the learning long-term dependencies with gradient descent is difficult was a really interesting
paper. I thought it was kind of a paradigmatic case of, I know that you talk about taking a
sort of large research problem and sort of distilling it down into its fundamental components.
And this was very much something that you did with this paper in terms of looking at the condition
required to store bits of information. It's the result of banging my head on hard walls
for a few years because I was working on recurrent nets. And sometimes it really didn't work.
And so I think the question people should ask themselves when things don't work is
there's an opportunity. What's the problem? Let's try to understand. Ask the why questions rather
than just let's make it work or let's make it be to the benchmark. Yeah, there's definitely
an opportunity there to dig into what's the fundamental phenomenon happening here as opposed to
thinking of it in the framework of let's hack around this I suppose and do the more
leaderboard chasing style approach. Yes. And I think in machine learning in general
there's too much of the culture of let's just build a system that works really well and beats
the other algorithms instead of let's try to understand. And so people don't spend a lot of
time on negative results. One thing that's particular about this paper is it's essentially a
negative result. It says there's a big problem with recurrent nets and the wider class of
learning machines. And what is the problem? So we're really digging into fundamentally what is
going on. And doing experiments whose purpose was not to compare algorithm A with algorithm B
but really answer the questions of what's the hypothesis of can we test it? And the nice thing
is when you do that you can reduce the problem. You don't have to have a big system takes forever
to train. In our case we reduce it to like single recurrent neuron. That was enough to really
understand what was going on. And of course then you want to make sure that the theory generalizes
which we did by talking about dynamical systems and tractors and things like that.
That was one of the things I really liked about this paper. I know that when I was first introduced
to the idea of recurrent neural nets and saw the comparisons between those and LSTMs and things
like that I was given this high level idea you might get things like vanish and gradient problems
when you look over long time spans. But there wasn't much deeper than that. And when I went
through this paper I felt that the articulation in terms of the tractors and looking at
characterizing whether dynamics are attractive or not with the spectral radius was
really interesting and I think gave a good sort of deeper lens into what's actually going on
under the hood. In retrospect I think it was one of my best days first. It didn't get as much
publicity as others but I think it was a very important piece of the contribution.
You have mentioned and at least one interview that you feel people don't understand this work
well enough and do you think it's that that dynamical systems articulation of it or could
you tell me a little bit more about that feeling? Yeah exactly just the story you told. So people
understand you multiply a bunch of gradients together and it gets small. And by the way that
idea had been put before our work and I didn't know about it because it was in a German master
thesis by, sorry what's his name, anyways. But the attractor thing is something people
have trouble with because it's not something we learn in typically in computer science
and it's a really important notion in mathematics and it turns out to be important
in my current work thinking about how the brain does things. So I think this is something that
would be worth teaching more as well like these notions of dynamics and attractors
to understand neural nets and their capabilities. Certainly it was definitely not something that
I was taught as a part of my CS program and it definitely took me a little bit to
come to grips with what was going on there. But I did feel like that was a very
nice and helpful way of articulating what was going on. I can definitely agree that it would
be nice to have that more in traditional CS programs. The next stage of your work I'd love to
talk about is looking at word embeddings from neural networks and neural language models.
There is a lot of important work here and a lot of it has to do with trying to bypass the
recursive dimensionality and you articulate this in a neural probabilistic language model.
You introduce word embeddings as part of a neural network that models language data and then
also introduced this idea of asynchronous SGD. Could you tell me just a little bit about what
your experience was like working on the paper at the time and sort of figuring out
this cursor dimensionality issue? The cursor dimensionality idea is something that actually
arose just a couple of years before in work I did with my brother. There's like a Benjio and Benjio
paper on how neural nets can potentially bypass the cursor dimensionality that occurs in learning
joint distribution over discrete variables. That was not language but think about discrete data
at that time was mostly modeled by tables, counts, frequencies, and of course that
doesn't scale if you have many variables. That's the cursor dimensionality here.
We had a paper that showed, I think even in Europe, that if you use a neural net to learn
conditional distributions to decompose a joint and that there is a structure that a
neural net can capture, then you're not suffering from the cursor dimensionality.
The language model paper was kind of an application of this but where we introduced an idea that
actually came from Jeff Hinton that words should be represented or symbols. I mean he was thinking
about symbols. Symbols should be represented by vectors. That was already part of his mantra in
the late 80s in his paper on distributed presentations and then applying these ideas in the context of
language modeling showing that not just numerically but showing intuitively why
we can bypass the cursor dimensionality. It's because words have shared attributes and meaning
so that cat and dog can replace each other in a sentence because there is a sort of
semantic representation maybe that's hidden to us and that's actually present in our brain
in which to share a lot of attributes because they're common pets. That allows if you understand
this and you see how now we don't need to see all the possible sequences of words in order to
generalize. You can take advantage of that similarity between words and that's the first
order thing and you can have more complicated nonlinearities. That's where the neural net part
comes in. That makes a lot of sense. One thing that stuck out to me in this paper you had some
interesting suggestions for future work and one of these was introducing a priori knowledge that
was to say semantic information like low and high level grammatical information
coupling the model to stochastic grammar things like that and I guess today the way NLP seems
to have gone not so much into introducing a priori knowledge but I'm curious how you look
back at those words now. Well I'm working on things currently that go in that direction.
Maybe we didn't have the tools to handle efficiently and in a rich way the kind of latent
structure that say a parse or a semantic parse which is really what we care about
could be discovered by neural nets and could be represented in a way that exploits the
parse dependencies that exist at that level. So when you look at the word level
you basically need to look at all the previous words to predict the next word so you need a lot
of data. That's why we need like huge datasets to train huge corporates to train these large
language models but if you can represent things at a more abstract level thinking about the
relationships between words that take meaning together like subject verb object of particular
kinds you don't need as much data because the relationships are simpler than both fewer concepts
and a lot of my recent work on trying to bring in high level cognition into neural nets
is motivated by this idea that we can put in more inductive biases, more structure
that will reduce the simple complexity of language models but potentially you know
more and people to do things like reasoning better. I do think one of those perennial questions
is always what is the right granularity of the inductive prior to introduce to the system
and I do want to dive into your thoughts on these uh priors from higher level cognition
but first I'd love to take a little bit of a tour through the rise of deep learning era
could you tell me a bit about your your experience of that period what that was like for you?
So so there are different phases so there's this phase where I'm feeling that the field is moving
away from neural nets my students are you know wants to work on kernel machines and stuff like that
and and that's the time when I write the language neural language model paper
and so a lot of what I did in these early years of like the deep learning era is trying to understand
what neural nets could bring that was missing in other approaches so this curse of dimensionality
question I worked a lot on the curse of dimensionality so is there an issue with
lots of the kernel methods or the graphical models that were based on tables
and that were popular in those days so it was more like trying to save the
the the neural nets because people seem to have thrown the the baby with the bathwater
and I thought there was something important we were missing though and so that's that's one
answer and then really a big player then was Jeff Hinton you know he came up with the RBM
the restricted bolster machine and and discussions with him shifted my view about
end-survised learning so I used to be so with the the paper with young account for example
on document page and I was fully into the supervised end-to-end learning mantra
and and and and these discussions which Jeff really helped me understand the importance of
unsupervised representation learning and by the way in the first decade of deep learning say let's
say until 2015 roughly people didn't pay much attention to unsupervised learning
um in fact it's only since the transformer era that the you know majority of researchers in
machine learning realized how useful unsupervised learning was because this is you know the the
unsupervised pre-training which was introduced in those early years with the layers stacks of
rbms and stacks of autoencoders and then the denoising autoencoder by the way the denoising
autoencoder was is this is behind which is essentially due to Pascal Vincent who was my
student and then became a faculty is behind the the is the main recipe behind the unsupervised
representation learning we find in modern language learning model sorry and with the
masking the masking idea and and reconstruction ideas so so yeah so unsupervised learning became
big for me and it still is but it was a big shift for me and and and I continue in that direction
and you know sort of my modern entertainment because humans get very little actual supervision
we get feedback we get sometimes people will say oh this is a cat but you easily don't don't get that
yeah yeah it is it is kind of amazing just the way in which human children are able to
pick up on things like the the complex structure of language for instance and it does seem like
there's very little actual direction and how that occurs so it does seem a little bit of a mystery
at least what's going on in the human brain there but there um I guess there are a lot of
interesting ideas right now there's a thing I want to come back to which is the how
course or how general the inductive biases that we choose should be and
you know different people obviously have different views on this and I have shifted myself
I used to be the sort of oh tabula rasa like we don't want any inductive bias but that was wrong
um and and I started realizing it at that period thinking about the you know the convolutional
architecture for example I worked a lot with Jan and we worked on continents in the 90s
but then even the notion of these distributed representation is is introducing some inductive
bias which was missing in the standard statistical methods that only relied on smoothness for example
which is another inductive bias so I started realizing the importance of that now there are
people want to go much further and say okay we want to solve this problem so we can take all the
knowledge we have and like try to put it in in our machine learning system and that could be very
useful um but there's an advantage of trying of trying to go for the most general inductive bias
that you can that's also as powerful as possible so there's like a trade-off because if it's if
it's fairly general like you know these days I think a lot about the sparsity of dependencies
in high-level concepts that we manipulate verbally if it's fairly general like this it can apply to
many many areas so so then it's sort of more powerful um but if you know if you're I don't know
a biologist and you want to use machine learning to understand how cells work it would be crazy
not to try to use the knowledge we have already right I suppose there's definitely a very
application dependent thought there and also the other factor which I think Jan was talking a lot
about is we have to be careful with inductive biases sometimes we think we know how it should be
but but we are wrong and so we introduce constraints that are wrong and then we are better off without
these constraints but you know just collecting more data which and that recipe has worked incredibly
well of course in applications where we have lots of data we just you know have very few
inductive biases we have very general machines lots of data and we're able to learn really
complicated things whereas the tradition in in AI and machine learning before was quite the other
extreme where we say oh we try to handcraft everything uh in facts where you know at the
limit there is no learning at all right I guess of course there's sort of the well-known um the
well-known learning theory that you have to have some kind of bias in place but it doesn't tell you
how much exactly exactly uh if if only we had more sort of quantitative ideas on this I I'd love
to move on towards the next section of things here sort of coming out of that Alex net moment
decade and looking more towards today what you're thinking about in terms of challenges towards AGI
in response to this question about whether your views have evolved since GPT-3 and the scaling
hypothesis you said you feel you a have a much clearer view of how to achieve these goals but
also that you have adopted a Bayesian posture could you linger a little bit on your your Bayesian
posture sure um there's a moment where it became clear I was having a discussion with
some physicists you who are using machine learning and they it told me something like well
it's it's not good enough for us to have one theory that emerges from our like learning system
to explain the data we have because maybe there are other theories that are equally valid to explain
the data and yeah so you know and that's the Bayesian posture essentially the Bayesian posture says
we're not going to be satisfied with having the one one solution that fits the data and of
course our priors or constraints and so on ideally we want to have all of them and the reason this
is important then again going back to like the use of machine learning in science imagine I you
know I come up with a theory of some aspect of physics and there's actually another theory that
gives very different predictions at least in some questions of interest to me I really need to know
that because if I'm going to base my decision on the first theory well knowing that it could be
completely wrong because there's other theory which is equally possible as an explanation
I might like you know risk people's lives so it's a question of safety it's a question of
doing the right thing and that means it's a question of knowing the limits of your knowledge
like you know there are questions for which we just don't have enough information and we need to
quantify that so people know in the in the Bayesian world talk about uncertainty epistemic
uncertainty in particular so the epistemic uncertainty is and the part of our like making
errors of not knowing that is due to the fact that we don't have enough knowledge epistemic
that's like knowledge right so this is different from the our inability to make predictions due to
like noise things that are fundamentally unpredictable so epistemic uncertainty could
could be reduced if you saw more data and then more of the right experiments and we need to
capture that and and that's not just important for science but you know if you build
self-driving cars you need to know that the car isn't sure whether he should go left or right
because maybe somebody's gonna die if you don't do the right thing and you need you need to wake
up the driver or break or you know do some conservative action there's there's a lot
in what you just said and one thing I'd love to hone in on a little bit is perhaps the safety
and epistemic uncertainty perhaps we could even get into the idea of epistemic humility aspects of
this yes you've said in a few a few papers recently this is kind of getting into the
consciousness part which I'd love to delve into a little bit more later but you've wanted to move
from deep statistical models and I know you're no longer exactly articulating this as system two
but at least in the paper I'm quoting you said you want to move from these models able to perform
system one tasks to deep structural models able to perform system two tasks by taking advantage
of the former model's computational workhorse and this was interesting to me at least as an
articulation also what you just said about safety and the Bayesian posture because when I
spoke to Stuart Russell a few weeks ago he had a lot of thoughts on both of these aspects one the
sort of moving away from the old model of AI and sort of adopting systems that maintain
this type of epistemic humility but then also to the system one system two models at least
the way you used to articulate this this sort of convergence between the deep learning and the
good old-fashioned AI communities that he seems to have been predicting for a while right right
so there's a lot of topics here the system two ideas or high-level conditionals I like to use
that term now it's quite important for again very pragmatic reasons which is the ability to generalize
in settings that are a bit different from your training distribution and I said distribution
not necessarily data in other words things have changed the world is a bit different you are in
a different place some agents have done things which really modify the way things happen what is
the optimal thing to do and so on and currently we the dominant way of doing things in machine
learning we're not very good at that like you train even this you know let's train on the
huge quantities of data you you kind of shift to a different distribution maybe data from one
country for training and then you try it in a different country or something and you take a
big hit or sometimes changes in the demographics or whatever but the interesting thing and the
connection to high-level condition is that humans are good at least better than current state of
the art at these changes and I'm going to use this example I use all the time you're you've been used
driving on the right say North America for many years and then you read a car in London
and you have to drive on the left and if we were just relying on like system one deep learning
it would take a lot of data of you know trying to drive in London and maybe killing a bunch of
people or getting killed yourself to to to learn how to drive with this modified rule but at the
end of the day it's just this one rule that changed so what's going on with humans when we are in
this situation you pay attention right so you're you can't just go by your habit and there's something
different mode of operation where you you remind yourself all the time oh I need to pay attention
to the driving on the left and you know not just follow my impulse so there's this aspect of
self-control where you just you don't follow your impulse you allow it kind of reasoning
and this these modified rules or whatever it is that's explaining the change to influence your
your decisions so maybe you're going to be clumsy at the beginning but at least you don't kill anyone
and you don't kill yourself and and then you transfer that as you you know acquire a bit of
experience driving on the left you transfer that and eventually maybe after a few days or weeks it
might become a bit more of a habit so this is a model for what we should do in in deep learning
we we should have these systems that are able to override the well practice well trained
reflex not reflexive way of prediction or taking decisions and can bring in these pieces of
knowledge like this rule to to and reason about them and now you know what does it mean to reason
you know and and how do humans actually reason so to understand these things that connecting to
the literature in in cognitive science and cognitive neuroscience is really important
so okay so so that's that's why I think this matters to machine learning and I also think
that we can do it with neural nets you have neural nets in your head but
with a somewhat different organization when it comes to these
um higher level cognition aspects and and we have we know a lot from neuroscience about how
it might be going on there's a theory called the global workspace theory that exists since
the 90s and that's been confirmed in many ways it's kind of a dominant explanation for what's
going on at the high level of cognition and a lot of my work has been trying to translate that into
machine learning terms what does it mean in terms of algorithms and why would it be a good thing
from the point of view of machine learning and sample complexity so for example one nice property
of high level cognition is that it's able to deal with dependencies between high level concepts
that are very sparse and that's reflected in natural language we see a sentence involves very
few concepts and still contains lots of useful information another interesting aspect of the
way we think consciously is the whole causality aspect and that's also important for the ability
to generalize out of distribution because what is causality about well there are many ways to
discuss this and they listen to hours and hours of lecture but but here's my like one line summary
so a causal model is different from a statistical model in that it's really a collection of
of distributions so a statistical model is a distribution that we can learn a causal model
is a family of distributions indexed by the interventions you can make in the system so
it contains the answer to well if I were to do this how would that change distribution you would
have a different distribution so that's what I mean by index like if you tell me what interventions
an agent could do you might get a different distribution and so the set of all these
distributions corresponding to all the possible interventions that's the causal model
and we and humans are good at that I mean they're not perfect we make lots of causal errors but
we're pretty good and children in particular there's a lot of interesting work about
children development constructing causal understanding for example by Alice and
Gopnik and and that is is also a huge source of inspiration okay the basin thing is important
here because there may be multiple causal explanations and if we don't keep track of that
in some way again we we may fail catastrophically now I you know I've never thought that you know
the Bayesian approach was bad but I didn't really pay attention to it because I thought
it was not tractable that you know we didn't have algorithms that could work and maybe there was not
any and I think that's the main reason most people who have you know good mathematical training
kind of disregard the the Bayesian ideas because you know on paper it is the right thing to do
but you have these intractable quantities that come in so so the good news is I think we can
deal with these intractable quantities we can approximate them and we can use large neural
nets to help us with that and that's something I'm working on yeah I'm excited to get into these
questions one thing that stuck out to me in what you were articulating this is maybe a loose
connection of some of these ideas was this very fascinating interaction between causal graphs
and then the idea of attention when we get introduced to that in machine learning we have this
um very elegant mathematical idea of attention over the words in a sentence and that correspondence
but then when we look at the way that humans pay attention that can happen at so many levels
of abstraction I can pay attention to my left index finger I can sort of expand that scope of
attention to my entire body and to tie that to causality say I'm like practicing violin I want
to make my tone sound better I can intervene on variables in that process big and small the vibrato
in my fingers the sort of length of my bow stroke and things like that and kind of on the fly
figure out what does the causal graph roughly look like here um and and sort of learn a lot
via that process yeah um there's a whole aspect of machine learning that we didn't talk about
which becomes really important when you start thinking about causality and that is the action
side of things because causality as I defined it has to do with interventions interventions are
essentially actions I mean it can be actions but it's it's it's a good way to think about it
and and so in the last few years I delved quite a bit in reinforcement learning and started to
publish in reinforcement learning um and it's an area that's very popular in in machine learning
these days for good reason and I think thinking about the action side as something that mediates
these changes in distribution which is what causality is about is going to help not just
train better agents in in tasks where we traditionally would have reinforcement learning
but more generally you know even for like computer vision we need that kind of robustness
that you know we humans have in terms of uh being able to deal with those uh shift to distribution
so so it's funny like I feel like a lot of what I'm doing now
brings in all these different threads uh that you know I may have encountered some extent in my
career before but I'm digging deeper into so you know the Bayesian aspects the causal aspects
of course the the neural nets are still there it's the workhorse like the the machinery that is going
to make the the is going to sample what you think about so that's where attention comes in
because there are many variables that you could think about when you observe a scene
and somehow our brain has this internal policy it's the kind of you know reinforcement learning
but not for acting outside acting inside to control your computation so attention is is a kind of
computational policy where do I put my uh you know brain power right now I want to focus on a few
words as you said focus on this object this uh entity outside that I'm seeing or this memory
or this thought that I had uh you know this morning and that's kind of a something where
the traditional neural nets could be used as the the machinery that uh you know produces the
probability distribution over the actions which are where you pay attention to
and so yeah attention is central to this whole story and I think there's still a lot that we don't
understand when we introduce attention in our 2014 paper on machine translation
you know that had a huge impact on on machine translation and then natural language processing
we we used what we call soft attention where it's not really that we focus on one word we actually
focus on several ones with a a mix a convex combination and that's very convenient because
we can train with back problem but it's not the way humans do it you know the negative cube is
either one way or the other in your mind and you you have a thought which is about specific
entities it's not a mix of 10 different thoughts so I I do think that we've been using the word
attention but not quite the way that it's happening in the brain um I my hypothesis is that in the
brain the attention is more um what I you know what I call stochastic heart attention
so in order to learn you want the something that's uh discrete like you choose this or that but not
a mix of oak uh to come with some probability uh randomly so that you can get a training signal
otherwise there's no way to train uh at least you know in a proper way and and so uh it it feels
like there's a bit of randomness in the way our thoughts come so I you know my hypothesis is
that we use a stochastic heart attention mechanism at that high level of consciousness at least that
we can report that's a that's a really interesting way of putting it I I suppose at this point I'd
love to segue over into your more recent work on G flow nets and you've spoken to some overlaps here
a lot of different ones because this is sort of combining many different ideas but when I first
read it um one thing that stuck out to me it felt I guess I was reminded a little bit of the
the distributional rl framework and I know that you've spoken about overlaps between G flow nets
and various ways of doing rl could you tell me just a little bit about the the overlaps you've
kind of thought about there yeah so so you can think of G flow nets as a particular rl
approach where instead of trying to maximize return or rewards in other words to find sequence
of actions that give you the largest possible value of some objective function we're trying to
sample trajectories sequences of actions that uh give rise to uh you know end products like uh
states um with a probability proportional to the reward you get
to get there or when you get there so there are things that are related in rl like uh
entropy maximum entropy approaches and uh soft Q learning and things like this but there are also
some some differences um and uh this is interesting for a number of reasons one is well it's it's
sort of naturally more exploratory uh but but there are some some deeper things like if you
want to be Bayesian for example this is exactly what you need you you want to sample from the
posterior distribution of something you care about some actions or some beliefs or something
condition on everything you've seen before including all the data in your life okay that's
that's that's what you're trying to do um in in Bayesian uh terms and so you don't want to maximize
something like find the parameter that's most probable but rather sample parameters in proportion
to how likely to explain the data you've seen so that only the configurations of parameters
or latent variables or something that are um compatible with the data and your prior will
be sampled right so so so G flow that's a particularly interesting as a if you want special
RL methods for sampling from posterior distributions and that comes up uh in in the Bayesian sense if
you want to condition on the data that you've seen before it also comes up if you have latent
variables so the way I think about the high level concepts that we think about consciously is that
the they are chosen by this stochastic heart attention so there's like a some randomness here
and we want to choose them um in in a way that we can pick from the different interpretations
that that are compatible with what we've seen uh and not just focus on the one if everybody was
thinking the same way we wouldn't have science so the exploration aspect here is is very important
that's why one reason why you know the many of I mean the first paper on G flow and that's and
then several of the other ones were focused on active learning applications where you need to
explore and you need to um go and look at the multiple say candidate drugs that could work
given the information you have and not just one that that fits the you know your reward function
yeah there's there's a lot of the overlap another another one that kind of stuck out to me
and that I think you've spoken out about before was with evolutionary algorithms and there's
perhaps two things that I felt might be relevant and I kind of wanted to get your sense there's a
very interesting recent paper called evolution through large models that you might be aware of
and they had this really interesting initial step that sort of bootstrapped a language model
and then eventually we're sort of doing rl at the end where they kind of bootstrapped um
the language model into having a set of like diverse samples of python programs to iterate on
via a method like map elites and it doesn't seem like that would be something needed in G flow
nets it seems like there's at least in cases like drug discovery or I'm like adding a molecule to a
you know collection of molecules a set of actions is already pretty well defined but I am very curious
if there is anything to the combination of this early bootstrapping method via an evolutionary
algorithm and the extension of G flow nets to maybe a space where the actions are are less well
defined okay having less well-defined actions is very important it's not it's something I've
been thinking about quite a bit but we haven't really cracked that yet and super important
because this is more like what we do it also corresponds to one of the big challenges of
reinforcement learning you know people like doina precup studying her thesis in 1999 I think
with options and later you know people call that harkical reinforcement learning in other words
we want to be able to come up with and plan with abstract actions not not the low level
like muscle movements but oh I'd like to have some tea and I'll do what it you know I need to do that
and then I'll you know maybe have a discussion with my friend so these are abstract goals and you
can plan at that level they're like meta actions if you want and our brain makes them up it's like
they're not real they're like in our mind they're invented to help us better control the world better
understand it and there's nothing you know in the g4net framework that says that the actions
have to be something that you give meaning to ahead of time so the math doesn't really prescribe that
but that yeah this is something we are very interested in and one of the big challenges we
don't know how to solve this problem in rl and it's clearly something people do and that has been
an inspiration for researchers for over 20 years and I'm hoping that these new techniques will help
there as well so I think I think this this this can be combined with the ideas of system two
with where there's this notion of encapsulation and modularity like you know knowledge should be
broken down into smaller pieces that can be recombined but these small pieces are abstract
they're they're not like low level perception or action so so this is uh this is all very
related to the the higher level commission and system two thinking right yeah it does seem like
there's different levels of I suppose articulation and well-defined in that sense it's really tricky
to to get our arms around that there's another overlap here also it's evolutionary algorithms
where as a future work you suggested investigating how to combine the generative approach with
local optimization to refine generated samples while keeping the batch of candidates diverse and
that sounded a lot to me like this idea and evolutionary algorithms of novelty search with
local competition and so I'm curious if you've sort of been taking inspiration from from there as well
yeah I mean I'm not an expert in evolutionary methods
clearly they have influenced my thinking because well natural selection and evolution is
something that should be in the background of every scientist and and I saw from the outside
the work that happened with genetic algorithms and then you know all of that but
um so first of all if you only use mutations it's really it's a really dumb search it's a local
search uh and it suffers from some of the same issues by the way that you have with the MCMC
method more the color markup chain methods which you do a lot of small local changes because if
you did a big change it will probably you know lead to nowhere like something bad and so you
can only do like small changes to hope that it still works that you still you might get something
better um and um when uh so the thing that's really interesting about evolutionary methods
is not the mutations it's it's the crossover and because it allows a kind of
generalization at a distance you take two things that worked well and you combine them
but it's done in a way that I think is is not nearly as powerful as what you could do with
machine learning where you can have all kinds of powerful forms of generalization
so the reason why g-planets and uh potentially other methods that are related to
variational methods in in machine learning um can do better than MCMC is because in order to guess
what good choices good configurations might be um you're not limited to these local moves you can
use the power of generalization so what does that mean if you've seen uh a few configurations that
worked well you might be able to find a pattern so that you can guess something as far away but
follows that pattern would be a good answer and and that's really what allows uh to do potentially
exponentially better than than things like just uh a mutation-based evolutionary methods or or MCMC
methods. Right the g-phonets idea also seems to be something like a very general framework I know
you had a very recent paper I think just a little over a week ago perhaps I was also looking at
different types of generative models and then uh sort of articulating them as like specific versions
of of what a g-flonet is more broadly which was interesting. Yeah the whole the whole spectrum
events provides learning and generative models is quite interesting as a space there are many
connections between different methods it turns out g-flonets has flavors and you know as kind of
special cases many of existing methods but also brings its own flavor and the flavor is very much
related to the inspiration from you know high-level condition in other words we build these data
structures in our mind that explain what we're seeing or maybe constitute plans and so it has
this like discrete sequential aspect of system two which you don't typically find in other
and supervised generative models. If you if you have time for one more question on on this topic
there was something really interesting you said in another interview um discussing the
consciousness prior in g-flonets in which you stated that you felt Cartesian dualism was an
illusion and I know you pose this as a question for David Chalmers if I'd seen that interview
before speaking to him I would have tried to ask him about it as well but just turning that question
back to you could you expand a little bit on just your your thoughts on that dualistic area of
consciousness? Right um so Descartes had the intuition that within us we had these really two
different things the body and the mind and actually we all I mean most of us get that get that
impression and I think it's also behind a lot of the mental pictures that are associated with
most religions and superstitions and you know you know supernatural things
that our brain tends to make up quite easily. Now the biological reality of course you know
Descartes didn't know he this is way you know before neuroscience matured as it is today
uh the reality of biology is there is no such thing as that separation uh you know we have this
where I mean to the extent that what we know of course because there's still a lot we don't know
but the consensus is uh there there there is no such separation of course um but we still have
to explain why we have that feeling that there is something special about being me and um that
I control things so that's like the free will illusion maybe and uh and that there's a difference
between um what may be going on in our brain that makes us do things and the conscious perception
we have subjective aspects of this of this that we can report sometimes verbally but we can't
completely explain with words um and so what I like is the stance from people like Dan Dennett
uh who's a philosopher of consciousness um but also people like neuroscientists like Michael
Graziano who are asking more the question of you know what is going on physically in our brain
that gives us that impression that there is this separation between the me that controls
the me that perceives and what is going on you know more like at a lower level this this mind
body separation and and Graziano has some nice theories about this which I find compelling
but of course we need a lot more investigation but that's the scientific way to to to deal with
this which is not just take for granted that because we feel that there is that separation
that it exists it's just this is what how we feel but it doesn't mean it's the reality
we may feel that you know there is God that's helping us out there it doesn't mean it's true
right it's the same thing yeah there there are a lot of interesting ideas there and I guess
to the dualism idea even before looking at today's science I think there's a pretty
interesting objection directly to Descartes from the princess Elizabeth which is like well
you have a body that that is extended a brain that is not extended how can there be a causal
arrow from brain to body which I think was a pretty early and still somewhat salient ejection
that I don't think Descartes ever totally responded to and with the whole free will illusion too I
think that there are some later ideas from Kant where he's like well you can't exactly have free
will but the the mind cognition almost has to imagine that it has free will in order to act
in the world which is really interesting I think just as a as a as a different frame of the take
I suppose well free will also has a social role so if we think of ourselves as machines even
stochastic machines then the notion of social responsibility like even legal responsibility
seems weird because people just you know the machine does what it does but but if we think
of us as agents which is of course a kind of illusion because we're not free will agents
well we just follow our policy which may be stochastic but still it's just a policy
that's a mechanical mechanical thing if if we if we didn't have this notion of free will it would
be strange to blame someone who's made something bad but now here's the thing we are humans or you
know even animals and we can learn and then it makes sense to to blame somebody and and and to you
know maybe punish them besides the fact that we want to protect society but you know just
because that will teach them and others that you know these are things you shouldn't do
so with the illusion that this person could have acted differently which of course is wrong
they acted as they you know they had to but but that illusion is useful for what we call
credit assignments in machine learning because it helps provide a a signal it says there's
actually a causal counterfactual here which is if you had acted differently things would have been
better and and so now I can use this in order to change my policy so in the future I will not do
it or someone else could use that as an example to avoid doing that mistake right right the the
determinism and and blame assignment question is really interesting there's a very interesting
paper um determinism al dente which sort of tries to toe the line between maintaining a fairly
hard determinist stance but then also a reserving room for actually blaming people um which I which
I thought was very interesting it's not just it's it's not just a question of determinism because
we are probably stochastic machines as well right right um and you know and and there are like
technical reasons why we are I think but but it doesn't change the the bottom line even if it's
a stochastic machine it still follows some you know causal process and you can't like it doesn't
make sense to blame it except if you think oh this could be useful information in order to change
the the policy so that in the future you wouldn't do those mistakes so now
free will like this illusion of free will is useful like the fact that I think I could have
done differently which is wrong um maybe a useful signal for me yeah it's it's a very useful signal
and to what you said about the stochastic side of things I suppose there's this is sort of this
sort of has to do with why when people will make arguments like oh quantum mechanics exists
therefore free will that doesn't it doesn't totally work like that no free will is not randomness
it's not about random choices exactly uh well it's not clear what free will is but it's something
we have in our mind really and uh it's like the dualism it's something that we have in our mind
that's maybe a useful illusion to some extent or a side effect of something useful um but we
need to instead of taking it for granted and then trying to see where we go from there we need to
study it as a phenomenon like why do we think this way why do we have this impression and what's
you know why does it make sense how does it arise how does it serve us or hurt us yeah yeah a lot
of a lot of these impressions are are certainly adaptive just in terms of our action as like
beings on our own but then also as you've said multiple times socially I think there's a a much
deeper rabbit hole we could go into on this but perhaps I'd just like to pivot towards a last
set of questions if there's time um and this is more kind of on some personal perspectives on
just you as a researcher and the work you've done my first is there's a very good talk you've probably
read by Richard Hammond you and your research and he had a lot of interesting takes here but
one particular quotation um that he says is when you're famous it is hard to work on hard problems
and looking at your work this is not a problem I perceive you as having but I'm just curious how
how those words strike you in terms of his perspective on he has a lot to say about people
who've won Nobel prizes and sort of what happens after that and so I'm sort of curious how you in
that position look on those words the best is to not pay attention to all that fame and recognition
too much so that you can be as free as possible in and doing what you think is right which means
taking risks making mistakes and um and following your scientific instinct just like you would
have done before um so that requires exercise and humility which may be difficult if you
receive a lot of these marks of recognition but I think it's essential humility is also important
for another reason which is you know as a scientist it's very important not to trust your intuition
completely it's good to follow your intuition but you should also be aware that it could be wrong
and if you're not humble enough you could like fool yourself and that's not good
I suppose so yeah there's again again the the epistemic humility and I think the
Bayesian posture strikes again my last question I guess is a little bit of a different take
so having read a bit about your background I saw that you have a very interesting family
background your father being a pharmacist who wrote theater pieces ran a theatrical troupe
and then your mother was also an artist has art played a significant role in your own life and
I'm curious how their backgrounds impacted you as a person and if you felt they've impacted you
as a researcher they have impacted me to this um in the sense that they my parents gave me a lot
of freedom of being whatever I wanted and exploring and and and not being feeling that I was
blocked in in some constraints and rules that most children feel maybe sometimes in their family
because art is also like science about exploration it's a it's a it's a it's a wilder kind of
exploration where things are even less constrained than in science because you don't need to be
fully coherent and um yeah so that's that's influenced me quite a bit it'll it also related
to this it has given me respect for people who think differently from me because in art of
course there are many art forms and different ways of doing things and every artist is original
and that doesn't mean you know it's not like you don't need to be right in art just needs to be
interesting and I think in science of course we need to seek truth but as we said there could be
multiple theories that seem to fit the bill and it is important to explore them
but we need that freedom so yeah this is the biggest gift I got and of course the gift of
being interested by the intellect that's intelligence but but I mean more generally like
abstractions and science and understanding what is what my motto is like understand
try to understand I want to understand why what's going on
yeah there they do certainly seem to be a lot of interesting
overlaps with the way our our brains go about exploring and thinking about things between
art and science and it does seem like there are so many great scientists for whom art is also
a really important part of their lives or or inspires them in some way or another
well uh professor Benjio I do want to thank you again for everything you've done for this field
for sitting down and talking with me today it was it was really an honor to have you on
my pleasure thanks for your questions
and that is a wrap my friends as I mentioned at the start of the episode you can subscribe
to the gradient on substack to receive not just this podcast but also our articles and
newsletters directly to your email you can also visit us at the gradient.pub where you'll find
all of that as well as more information about the gradient and how you could even contribute
if you're interested and finally if you enjoyed this episode we would really appreciate your
feedback if you'd like to leave a comment or review we'd love to know how we can make this
series more interesting and informative to you and with all that I'll leave you until the next episode

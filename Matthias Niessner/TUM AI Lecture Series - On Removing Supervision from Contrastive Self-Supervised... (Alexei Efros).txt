All right, welcome everybody today for our To My Eye lecture.
It's a real pleasure to have Alyosha Efros today.
Alyosha is a professor at UC Berkeley where he's part of the Berkeley Artificial Intelligence
Research Lab there.
His work is at the intersection of graphics and computer vision and I'm sure pretty much
everybody in the community has heard of him.
He's a pioneer at the intersection in these fields.
He has countless of exciting papers starting from texture synthesis to conditional GANs
like Pics to Pics and Cycle GAN.
He's particularly known for his creativity and thought-provoking work, which is an inspiration
to many young researchers.
His students have also had really great success.
You can see many of his students are now professors themselves.
I think it's also, yeah, it's fair to say that he has also a very great social engagement,
in particular, contributing to the research community.
If you haven't met him in person, I can only recommend reaching out to him at the conferences
once we have him again.
It's really great to have him around.
It's really great hanging around with him at one of the poster sessions and chat about
some really exciting research.
He's particularly known for his, yeah, really cool attitude and it's really great to have
him as part of the community.
So it's a real pleasure to have you here and I'm really looking forward to the talk and
you also promised some philosophical components, so I'm really excited what that's gonna be.
Thank you so much for such a gracious introduction.
Yeah, I'm sad that we have to do this virtually because I would love to have hung out with
you guys and gone for some wonderful Bavarian beers, but next time.
Yes, thank you very much for inviting me and it's such a star spangled roster of speakers
that you have there.
I hope I will not disappoint.
And so I'm going to talk about aspects of self-supervision, self-supervision is something
that my lab has been working for a number of years.
And just to make sure everyone's on the same page, self-supervised learning is when we,
this is my definition, you know, hopefully there is no one set definition, but my definition
is that it's when we use the tools of supervised learning, but where the labels are the raw
data instead of being human-provided.
And so the question that often students ask is why use self-supervised learning?
What's the point?
And the classic answer, the common answer is because labels are expensive.
Instead of having humans provide labels and, you know, annotating them using lots of hours
of work or, you know, high cost, here we get, use the data itself as our labels, okay?
So this is the common answer, but that's not really my main answer.
This is, it's nice, but it's not the main reason for me.
For me, I actually have a couple of answers.
The first one answer is that self-supervised learning allows us to get away from the tyranny
of this top-down semantic categorization that goes all the way to Plato and Socrates, okay?
And I'll tell you why I think this is a good thing.
And the second reason is that self-supervised learning will hopefully enable us to move away
from this idea of a fixed training set to a more continuous lifelong learning,
where you have data streaming in and then you learn on the go.
You learn as you live, rather than having this kind of training set, testing set split
that is kind of the classic thing in machine learning, okay?
So I will start and most of the talk is going to be about the first one
and then hopefully we'll do a little bit of number two, okay?
So what's the problem with semantic categories?
Well, let's look at, from the visual point of view,
let's look at a couple of categories from a standard visual data set.
So the first is what's called a chair, okay?
And you can look at all the different chairs that you could have in the wild
and you realize that this is very hard to find what is in common between all of these chairs, right?
Visually, actually, there is pretty much nothing in common
because between something like this and something like this, right?
This chair is really more of a functional category, right?
And think about it this way.
We can see them all being chairs because we have seen, you know,
butts being squeezed into these places, but if you are a computer,
if all you've seen in your life is ImageNet data set, for example,
if you've never seen any videos, you've never seen any people sitting on anything,
there is basically no way for you to realize that all of these are somehow related, right?
So the relationship here is not visual.
The relationship is functional.
The relationship is that of affordances and it's not really fair for a computer
to try really, really hard to find a way to kind of somehow bring them all into some kind of a connection
where maybe there isn't that much of a connection, okay?
The second example I like is this one called City, okay?
And here what I'm showing here is a picture of downtown Pittsburgh
and a picture of the center of Paris, okay?
And frankly, you know, the fact that both of these,
by some fluke of the English language, are termed City, the same noun,
this is just kind of a coincidence because there is really nothing visually in common between those two, right?
And people can argue, well, wait a minute, you know, both contain buildings.
But look at the buildings.
The buildings are so different.
Visually, there is nothing in common between those buildings, right?
They're so visually different.
And then you can say, well, but they both have windows.
But again, look at the windows.
The windows don't look anything the same.
Look, there is really not a single pixel in common between these two things.
And so again, we are forcing the computer to do something,
to somehow try to generalize across these two things that are very, very different,
that might not have anything in common.
And so, in a way, what we're forcing the computer to do is basically to cheat, right?
It's kind of like you haven't attended classes and then now it's your final exam
and you're trying to see what, you know, you're trying to cram for the final exam.
And so what the computer is going to do is like anything that looks like this,
it will be called the city.
Anything that looks like this will also be called the city.
But you're not going to basically get the concept of a city.
You're just going to basically just remember the, you know, the nearest neighbors, right?
So basically with labels like these, where I worry that we're setting ourselves
up for failure, right?
We're setting our algorithms up for just memorizing examples
and not really even having building connections between them, okay?
Which is of course very bad if we want our models to generalize, right?
If we just wanted to, you know, train an image net and then test an image net, that's fine.
But if we wanted to train an image net and test on, you know, the real world out there,
then that's not fine.
We need to somehow induce generalization, okay?
And so this is where my promised bit of philosophy comes in.
Which is that I argue, I've been arguing actually for many years now,
that we should step away from the stop-down categorization paradigm and try to think more
of it as a bottom-up association.
And there is some movement towards that in, especially in the 20th century.
The philosophers have really kind of pushed away Plato's and Socrates' notions
of these rigid categories and really started to think about categorization
in a much more bottom-up way.
So Plato, of course, he argued that categories were a list of, you know,
there were abstract definitions with a list of shared properties.
And then in the mid-20th century, the philosopher Wittgenstein came out and said,
no, no, no, no, this is that people don't do this.
In fact, people are much more fluid about categories.
And, you know, for example, you know, if you ask people, you know, our curtains furniture,
our olives, fruit, different people will give you different answers.
In fact, the same person might give you different answers different times you ask them.
Wittgenstein's classic puzzle was to ask people to name all what is in common across all games.
What are the common properties shared by all games and not shared by non-games?
And that you cannot do it.
It's just impossible that there's such a variety of games out there
that you cannot think of any single thing that defines a game.
It's not a definitional thing.
It's much more kind of data-driven, much more bottom-up.
Kind of, well, it's a game like football.
It's a game like chess, right?
And so in the second half of the 20th century, psychologists, in particular,
Eleanor Roche have tried to kind of update the notion of categorization
and tried to think about categories forming from the bottom-up.
And her famous prototype theory of categorization has really started this trend,
where the idea was that you basically cluster, you do bottom-up clustering of similar instances
into these prototypes.
And the prototypes then get clustered again.
And then you have this kind of a bottom-up hierarchy where the categories emerge
directly from the data rather than being kind of this top-down, OK?
And later, psychologists have gone even further and argued for what they call
the exemplar-based theory of categorization, where you basically don't really have
categories when you kind.
You basically just store instances, store exemplars of everything that you see.
And then you basically learn associations between those examples
and the things that are closer together.
Essentially, you can think of it as this kind of a soft clustering of your
exemplars into chunks and to groups.
And those groups then emerge to be categories.
And my favorite slogan is provided by a neuroscientist Moshe Bark, who says,
ask not what is it.
Ask what is it like to this kind of this idea that association, bottom-up
association trumps this top-down labeling, OK?
And so this work has been very inspiring to me and my group over the years.
And we have been kind of chugging away in this direction for a number of years.
And maybe one of the earlier works that we did was with my former student,
Tamash Milasevich, where we basically try to kind of instantiate this
exemplar-based way of thinking about categorization.
And here the idea is that we basically try to find per-exampler distances.
Given a set of data.
So you have a set of labeled data, cars, people, pedestrians, trees, et cetera.
And here instead of trying to separate all cars from all pedestrians, for example,
we wanted to basically learn a way to group every single instance of, say, car.
So for this particular focal example of a car,
we wanted to find what other things are close to it, right?
And so those other things should be also labeled car.
But there shouldn't be all cars because this car, for example,
looks nothing like this car.
So should they really be in the same cluster in the same category?
Maybe not.
And so the idea that Tamash came up with was to basically kind of treat this
as a kind of a classification problem where you basically learn a decision
boundary between things that are close to your focal example and things that are far.
And but here we actually have instead of two classes,
the ones that are inside of a category and once out, we have three classes.
There are the things that are close and these are kind of these kind of cars.
There are things that are not cars and they're on the other side.
And then there is a third class which is don't care.
And these are basically other cars that might not actually be that close to the focal car.
And then you basically optimize this, optimize this decision boundary given some
set of constraints and as a result, what you get is you get something where you
learn these distances that produce much more visually meaningful relationships
rather than if you were just doing a standard set of distances without this.
Okay.
And so this was kind of a, we were very excited about this, but still we're here.
We're still using the label car.
The labels are still being used in this computation.
And we really wanted to go away from labels entirely.
And this is where Tamash's next paper came in.
And this work we call it exemplar SVM was basically kind of a pushing this farther
and really thinking about it in terms of classifiers and basically switching from the
standard classifier where you have class A and class B instead to take every single
instance, every single data point and train a separate classifier for that one instance
against everyone else, whether it's your own class or a different class.
So it's one against all classifier.
So this is kind of an interesting way to think about it because it's really basically you're
defining yourself not by who you, who is in your category, who is in your class, but
you're defining yourself by what you are not, right?
So what makes you different from everyone else inside, in your data, okay?
And then the cool result was that we were able to do one classifier for every instance
and then assemble them together.
And the result was that this assemble actually worked no worse in many cases than your standard
two-class classifier like an SVM, okay?
And this was a little bit of a, it was kind of like a bit of a trolling paper which basically
kind of tried to push the community to say, look, maybe you're not getting as much juice
as you think you are from basically trying to group all those things into one class because
it seems like if you don't do it, you get basically as much of performance as not, okay?
So we also use the same idea for retrieval.
So basically the idea is you take, at runtime, you take your retrieval query image and then
you have a big data set and you're basically training at runtime an SVM classifier to separate
your query from everything else, okay?
And then you order all of your data based on that, on the coefficients of that decision
boundary, okay?
You basically find who are the closest things, who are your support vectors inside of your
data set and those tend to be the retrieval examples, kind of the closest ones on the
other side will be the retrieval examples and that also worked surprisingly well.
And so we were very excited about this and we were very hopeful and then, of course, deep
learning revolution hit and all of this became irrelevant because much better classifiers
came on the scene.
We tried to update it for the deep learning age and we didn't really succeed, but I think
Siena Savitsky and colleagues did, okay?
So one of the kind of very early influential papers was called exemplar CNN, which basically
adopted the same idea of like one against all classification on using neural networks,
okay?
And the main difference that we didn't really think of was that whereas we used a single
exemplar, one image against everything else, the Savitsky and colleagues, they used what's
called data augmentation.
So they basically, they took one example and then they created a whole bunch of similar
examples by basically applying various different transformations to it, you know, changing
lighting, changing contrast, changing shape, you know, various, various geometric transformation,
etc.
So in the end, even though all of these things came from a single example, they all were
a little bit different and so this became the positive class and then everything else
became the negative class and that worked really, really well, okay?
And this work really was a kind of inspiration for a lot of the current contrast of self-supervised
learning that we are familiar with right now.
So we thought we will be very pure and just use a single image, but this of course worked
much, much better.
And of course, now in kind of modern day, the self-supervised learning representations
that seem to work the best, they're all based on this idea of similarity learning.
Instead of learning which class you are, which category you are, the idea is to learn instances
that are either close or far from each other, to learning the distances between the instances
in your training data, okay?
So things like metric learning, SymesNet and the new contrastive learning are all based
on that same principle.
So you basically, you have some sort of an embedding space and your goal is to say, okay,
for a given positive, like this particular instance of a dog here, you create a bunch
of different positive example by data augmentation and then you basically try to push these ones,
all of them to be close to each other in this embedding space and far away from other things
which are dogs or other cats, et cetera, okay?
And this learning of the similarity is really what a lot of the contemporary self-supervised
learning methods are doing, okay?
So the reason why this, maybe like a year ago, this area really took off, one of the reasons
is, of course, the improvements in the representation learning, the contrastive formulation is actually
just works much better as shown by papers like Simplier and stuff.
But another reason I think that's maybe being a little bit underappreciated is that we are
just much better at doing this data augmentation.
So we have learned to do data augmentation in a better way than the Seitzky and his exemplar
Sienna, okay?
For example, now cropping is a very standard trick for data augmentation, which wasn't
a standard trick before, and that gives us a lot of boost.
So again, what data augmentation is, you get yourself an input image, a single instance,
and then you just randomly create a whole bunch of different versions of that image
by applying a whole bunch of different perimetric transformation, whole transformation, cropping,
flipping, blurring, et cetera, et cetera, et cetera, okay?
And then once you do that, then you set up your kind of a distance function.
So you basically say that I want these two images that all came from the same image really,
I want those two images to be similar in our embedding space.
So I'm going to try to bring them close together and farther away from the other images in
my data set, okay?
That's really the whole story of contrastive learning, okay?
Now the thing is that the choice of data augmentation itself turns out to be very, very critical.
And in fact, I want to argue that this data augmentation is itself a little bit of supervised
learning because the way you choose your augmentation can make a huge difference in your final
preform, okay?
So here is an example from our recent paper in iClear 21 where you can think of, let's
say that you have different types of data augmentation, like color augmentation, maybe
rotation and maybe texture, okay?
So now we can look at different tasks.
For example, if we want something like ImageNet, like course level categorization, then data
augmentation with color makes a lot of sense, with texture also makes a lot of sense, but
rotation is actually going to hurt you because an upside down elephant is not going to be
recognized as an elephant, okay?
Whereas if your task is, for example, fine grain recognition, well then it gets even
more complicated because if you're fine grain the different species of birds, then actually
you don't want any of those data augmentation because they're all meaningful, like changing
texture may change the species, changing the color definitely will change the species.
Whereas maybe if you're classifying different types of flowers, then rotation is fine because
rotation augmentation, you know, because flowers are usually rotationally symmetric, okay?
And so you can see that it really becomes very, very task dependent.
And you know, another example is the cropping and image classification.
So cropping for something like ImageNet classification makes a lot of sense because you have one big
object in the center of the image.
But the same cropping for object detection actually doesn't make that much sense because
you crop it and you might lose, you know, where your object is, okay?
And so this is something that we started to worry about because we feel like a lot of
the advances in the modern self-supervised learning might actually be due to us being
very good at overfitting the right kind of data augmentation for a particular problem
rather than the actual methods themselves, okay?
So what we wanted to do is to try to do contrastive self-supervised learning without data augmentation,
to really try to get it to figure it out on its own without this kind of help, okay?
And the way we wanted to do this is to make these augmentations, what they're called views,
to make them latent, to make the computer come up with its own data augmentation in
a sense, okay?
And of course, the big question here is this is all great, but where do you get the supervisory
signal, right?
There is no free light.
You get some sort of supervisory signal from somewhere in your data.
So where is it going to come from?
And here I'm going to talk about a couple of papers where we answer this question differently.
The first paper, the answer we have is that we want to use time as our self-supervisory
signal, okay?
And here I have a wonderful quote from one of my favorite writers, Jorge Luis Borges,
in his short story about fumes, who is this kind of a man on the spectrum.
He writes, it irritated him that the dogged 314 in the afternoon scene profile should
be indicated by the same noun as dogged 315 seen front.
Okay?
So, so basically what he's talking about is that two different instances of time makes
most of us assume that there is some continuity of what we are perceiving, that the dog here
and the dog here, it's almost certainly the same dog, that nothing happened to this dog
while it was jumping in the water, right?
But fumes, of course, couldn't figure this out and neither can our computers, right?
And so the idea is that this temporal correspondence, basically time as a way to align things together,
to bring things into correspondence, is a very powerful supervisory signal that we should
be using, okay?
And we have evidence that biological algorithms use it very strongly.
There is plenty of psychology data for human infants that shows that temporal cues are very
important to learning vision.
And there is this wonderful line of work by Wood, who basically did this kind of experiments
with newly born chicks.
So basically what he says he has is this kind of VR cave for chickens, for little chicks.
So you put an egg and then the chicken is born and the chicken is born in this VR cave
where he's basically projected things on all sides.
So, all everything that the chick knows from birth is being controlled by the researcher,
okay?
And what he showed that some of the chicks were shown videos that were not temporarily
coherent, that basically broke this temporal continuity.
As you can see, it was not kind of physically correct.
And he compared them to chicks that were shown normal, normal continuous things.
And the chicks who saw these continuous patterns, they were not able to function in the world
as well.
They lacked some of the visual perception skills.
So that showed that this is extremely important.
The temporal continuity is extremely important.
And so what we want to do in this work is to use video as data augmentation, as a way
to create these data augmented views ourselves.
And basically, the main thing, of course, is that this can provide correspondences across
different instances and allows the computer to learn how something looks across time change,
okay?
But it can give us even more because we can also think about contextual relationships and
notice things that are moving in the same way, what Bernheimer called common fate.
And also use that as a way to group little points, little trajectories into groups and
maybe get to the notion of objects from the notion of point, okay?
Again something that you can use temporal information for, okay?
And so this is basically the story.
And then the question is how do we harness this information without any sort of supervisory
signal, okay?
And in the past, people have used things like slow feature learning where they basically
kind of looked at connecting nearby frames together, basically look at nearby frames
as the positives and far away frames as the negatives.
But you just collapse the entire frame.
And so that's not really something that can give you these point tracks.
It's much more coarse signal.
Alternatively, people use things like optical flow or tracking to create correspondences
using some off-the-shelf methods and then use learning to connect things that are supposed
to be in correspondence.
But here, you're using two different methods and so what we wanted to do is do something
like this, but kind of in one, in one go, in one pack.
And this is our paper that tries to do this, this was published in Europe this past year.
And here is the idea, okay?
And so for a warm-up, let's consider the case where you actually do have labels.
Say that somebody went ahead and labeled that this patch corresponds to this patch, okay?
And your goal is to basically learn a representation that brings things that are the same into
correspondence and away from things that are different, right?
So in this case, of course, it's very simple.
You just say, okay, these two things are my two positives.
I want them to be close together.
And all the other patches are my negatives.
I want them to be pushed far away.
And then you'll have your standard self-supervised learning,
contrastive learning problem, and off you go, right?
Nothing very exciting.
Now, things get a little bit more exciting if you have maybe another frame in between, okay?
Because now you have, these two guys are your two positives.
We know this by labeling.
But also, because this is a video, we know that from here, somehow,
it needed to go to be in this final place.
And so there must have been a path, the most likely path for this guy, to go from here to here.
And the most likely path is going to go through this patch.
So it's reasonable to assume that this patch should also be in our positive category.
It should be in the same category as this guy and this guy.
So now these triplets should be the positives and everything else should be the negatives.
So now you can think of a little bit of kind of automatic data augmentation.
That this guy, just by virtue of being tracked in the video,
becomes a data augmented positive for our kids, okay?
So this is okay, but this is still requiring us to have this supervision.
How can we get rid of supervision?
Well, we are going to use our old trick, which we've used before called cycle consistency.
And what we're going to do is we're going to make this video into a palindrome.
Palindrome, if you remember in language, is a word that you read it forward and backwards and it reads the same, right?
So how can we make a palindrome out of a video?
Well, what we can do is we can take this video and flip it around and put it back in reverse order, okay?
So now what we have is we have something like frame one, frame two, frame three,
and now we're going back to frame two and then frame one, okay?
So now we have this new video that's a palindrome and look what's happening.
Now the final place, the destination, is now exactly the same as the origin by construction.
So now we don't need supervision anymore.
We got rid of supervision.
All we need to do is to get from the blue guy to the green guy.
And the way we do it is we're basically trying to do a track for this video.
And everything that's on this track should be in our positive category and everything else should be in the next, okay?
And so now you can see the setup where we basically, you know, are getting something out of nothing.
We're getting some supervisory signal just from the mere video information, okay?
So basically the story is that we are going to take a video.
We're going to make it a palindrome by going from t to t plus k and then minus to back to t.
We're going to make it into a graph.
We're going to turn the video into a graph, okay?
And then we're going to walk along this graph until we get to the end, okay?
And we're going to do basically a random walk on this graph.
And then we're going to steer that random walk such that if you start from this blue point right here,
we want to steer it to get us to this green point right here, okay?
And this is going to be our only supervision.
The supervision is going to be at the last frame where we're going to say that the green,
the positive is going to be anything that lands on the green dot
and negative is going to be anything that lands anywhere else, anywhere on this red dot, okay?
And that's basically going to be the signal that we're going to use, okay?
So, and also notice that we don't have to have a single path through this graph.
We kind of naturally, we can incorporate probabilistic information by tracing many paths through this graph, okay?
So, you know, how do you turn the video into a graph?
Well, it's kind of a standard thing.
You know, create nodes and then, you know, you're basically,
your nodes are some representation and some using some encoder, phi.
And really, this phi is really the only thing that you're learning.
What you're learning is you're learning the representation of each patch in your feature space.
And you're basically trying to figure out how to arrange those features in your representation.
Who's going to be close?
Who's going to be far?
Okay? And so, now your video is going to be a graph.
And then from frame T to frame T plus one, you're just going to have a transition matrix.
That's just going to say, you know, where did all the points from T go in T plus one?
And then this is just basically like a dot product in the feature space.
So, the closest things are going to be to get the higher dot product, okay?
And then, how are we going to do it around the work?
Well, just going to compose all of these transition matrices.
A, just like, you know, standard Markov chain, you know, you're just multiplying all of those transition matrices.
And you get your full, your full work on this graph, right?
So, now kind of the nice thing is that the task of learning this representation phi is essentially the same as fitting these transition probabilities, okay?
You find the right transition probabilities and it gives you your representation phi, okay?
So, again, in kind of a, if we do have the target somewhere, if we do have the supervision,
then this is just a standard contrastive learning problem.
You basically, you want to find a representation where this query goes directly to the target.
It doesn't go to the red ones, right?
And this is basically just, this is your positive, this is your negative.
This is your standard kind of static learning problem.
If you have multiple frames in between, then in a sense, you have some latent views that you can also use, right?
So, now we have, these are all the positives and these are the negatives if you, if it's an obvious path.
But if you have multiple, multiple hyperability paths, then these will be late, like, weighted positives and these will be weighted negatives and you can still do it.
And so, again, from a single point of supervision, you get all of this data augmented information, okay?
And of course, what we can do then is we can do the palindrome trick and now we get all of these latent data augmented positives without even providing any supervision, okay?
Because this is basically, by construction, the target is the same as the query, okay?
And so, where we can just set this up at training time and you can see that it's, you know,
if you pick a point in the query image, in the first image, you can see that over time it kind of gives you a little probability distribution of where that image might have gone.
And you can say, well, maybe we can even do this by trying to get grouping happening and find out a group which corresponds to a single object.
And to do that, we have a little extra thing that we can do, which is we can do dropout.
We can cut some of the engines away and force the correspondences to go through nearby paths.
And that basically allows us to get a little bit more of this kind of grouping happening where you basically kind of, you go through the paths that are also on the same object, okay?
And, you know, we basically violated it at runtime by essentially nearest neighbor in the five space.
And here are some examples. This is the kind of the state-of-the-art label propagation results.
And these are the results of our methods. And you can see that it's basically behaving much better in terms of occlusion handling and this seems to do quite a bit better.
Here is, again, state-of-the-art self-supervised method. And this is ours, right?
Even against supervised methods actually does pretty well, even though it doesn't get any sort of supervision.
So here is kind of an example of how we do compared to some of the self-supervised comparators.
And interestingly, even for methods that are trained using ImageNet representation, we are actually doing better than that, okay?
And here are some examples of kind of propagating various things like skeletons or labels or things like that, okay?
So this is one way to use this contrastive learning without data augmentation.
But of course, in my lab, we also like to make pretty pictures. And so I'll briefly show you another way of using the same kind of an idea of kind of creating your own latent views for an image-to-image translation setup, okay?
And here the idea is, of course, unfair translation. The classic thing that we've been doing for a while, we want to translate horses into zebras,
but we don't have a correspondence between horses and zebras, okay? So we want to go from here to here, but we don't have a correspondence, okay?
And of course, the one powerful signal here is we can use a GAN loss, which basically says make this thing into a zebra by basically forcing it to look like other zebras that I have seen, okay? Using a GAN loss.
But that GAN loss is not enough because it can make it look like a zebra many ways, right? But we want it to kind of be in correspondence.
And so this is where we also want to have another constraint. And in the past, in works like CycleGAN, we use the cycle consistency constraint, which says, okay, make it a zebra, but also make it so that when you translate it back, you'll get back to the original horse.
And that's kind of forces this to be the right answer, not these, okay? But there is a problem with the cycle consistency constraint, because the problem is that it forces it to be a bijection, it forces it to be one-to-one, because yes, it's not going to, it's not going to go back to here, but it's also going to constrain us to have to go back to this particular horse.
Whereas these other horses might have been just good enough, right? So this is where kind of a bijection is not always desirable, because sometimes these cycles are not a bijection. So how could we kind of address this problem?
And here is the approach that we came up with, which is we are going to have an image-to-image translation framework. We're starting with our horse. We want to get a zebra. So we have a GAN loss that says, okay, make this a zebra, okay?
And then what we're going to do in addition is we want to make this zebra to be similar in structure to this horse, but not in texture. And what we're going to do, the way to do this is we're going to enforce the structures to be the same by taking pairs of patches across the input and the output,
and say that these two patches need to be close to each other in features space, and farther away than other patches from the horse image.
Okay? So you can see that again, we're getting this whole similarity learning story here, where we are basically bringing these two things to be closer,
and farther from the other patches of horse. And again, just unlike other methods where the positives are somehow automatic created by data augmentation, here the positives are basically our input and our output.
So the output becomes our data augmentation, okay?
And now we are back into our contrastive learning land. We just basically formulate this and we basically say, learn a representation such that these two guys are close.
So basically it kind of ignores the texture and focuses on the structure, and these things are fine, okay?
And of course what we do this, we don't do this on just on the pixels, we do it at different levels of representation at basically different menu, multi-scale patch representation, and we do this contrastive learning basically everywhere here in our decoder.
Okay? And of course we also have our gap losses as usual, okay?
And one kind of interesting cute note for those who are in this, might appreciate this. Well, we thought, okay, you know, the positives, everything's clear.
With negatives, we just take all the patches from the same image, and then we thought, you know, maybe it will be better if we take the negatives to be not just patches from the same image, but also just add other patches from other images, other negatives, right?
It should be even better, even more negative data, right?
And guess what? It turned out that this did not work as well. These external patches actually made performance worse than if we just kept the internal patch, okay?
And this kind of goes back to some old work that I have been doing on textures. This is where we also seen that patches from the same image actually provide much more information than if you start mixing them up with patches from other images.
And in fact, Michala Rani has this wonderful example or story of doing super resolution using a single image where she shows that you can do super resolution by learning from a single image.
You basically take an image, down sample it, train a CNN to up sample that one image, right?
So you basically train a single image network, and then just reuse that network for the original image. So that works better than if you're training a standard thing with a large dataset, okay?
And basically we are seeing the same thing happening here, that it's actually the patches that are in the same image, that have the same illumination, the same camera parameters, the same setting, they actually much more powerful information than if you just put a whole dataset.
So this is kind of a cute little story, and you can see here how using the internal patches, we get much better translation than if you use external patches where you can see that there is a lot of mode collapse happening, okay?
So yeah, so basically that's the story, and this is our method and compared to something like CycleGAN and other methods as well, and basically we can see that we're basically getting as well performance as good as CycleGAN in most cases,
but it's much faster, and it's one-sided, you don't need to train two-way thing, and it's basically, we think it's kind of a much better story, and so here are some of the transformations that we have,
and one cute thing that we can also do is we can basically apply this to instances, so for example, let's say that we have a single image, Claude Monet's painting, we want to make it into a photograph, and maybe what we have also is a single image,
instead of dataset, we have a single photograph that is also, let's say, of water lilies, okay? Well, we can basically use the same kind of contrastive learning, basically just between a single reference photo and our output, okay?
And have one, have the same thing here for the positives, and instead of again have basically just a single discriminator here, and we can get something that actually works quite a bit better than a lot of these kind of stylization methods, okay?
So this is competitors, and this is ours, and I think that ours actually looks quite a bit more natural and also better than CycleGAN.
Here are some other examples, here are some other examples, okay? Let's see what timing is.
Well, you know, I don't think I have time to go over the second point of why you sell supervision, maybe I'll just give you a little bit of a hint of what I mean, and then you can read the paper if you're interested.
But interestingly, the idea is that it's a little bit weird that we're in most of machine learning, we're using a fixed training set. It's not very natural biologically, because biological agents, they never see the same data twice, right?
You never see the same thing twice. You see something first, you deal with it, you hopefully learn from it, if you didn't deal from it, you're dead, if you deal with it, you learn from it, and then you kind of recover some information from it, but then you never see that again, you see maybe something similar, okay?
So every new piece of data is basically first in your test set and then in your training set, okay? And it seems like using a fixed data set, it kind of encourages memorization because you see the same exact thing over and over and over again.
In fact, maybe this is actually another reason why data augmentation works, because data augmentation is kind of random, you create a random thing every time, so you kind of get away a little bit from this memorization.
So in fact, this might be kind of a subtle way in which data augmentation helps, that actually has nothing to do with the data augmentation, just basically randomization of your data, okay?
But the point is that if you're using self-supervised learning, like the whole point of having a fixed training set was because it was expensive to do all these labels.
You know, ImageNet, Poor Fei Fei spent all of her startup money in Stanford labeling this huge data set, right? So it kind of makes sense that it's fixed because it took so much money to label it.
But if you're using self-supervised learning, if you don't need the labels, what's the point of having a fixed data set? Why can't we just keep downloading images from whatever, the internet, the TV, whatever, and just keep doing it all the time because we can generate our own names?
It seems kind of natural. And so this is where kind of, I've been pushing on this idea of kind of this online continual learning. So you can think of it in terms of the standard train valve separation.
So you know you have your training set and kind of the standard thing in machine learning is you can separate it into a training set and a validation set, right?
And we know that if you just train on a training set and then use the validation set to tune your hyperparameters, you usually get better performance on the eventual test data than if you just train on all the training set, all at once.
Even though you're kind of, you think that it's less data that you're using for training, but actually this is effect, turns out to be more effective.
Well, we can think of the same thing in a continual way. So we can think of it as you train on the data that you have seen, and then you're validating on the next data that comes along, okay?
And then once you do that, you just incorporate it into your training center and you can keep going. And it can keep going on forever. You don't ever need to stop, okay?
And this, I think, is kind of a very powerful trick that is made, that we can now do because we can use self-supervision to do this kind of this evaluation, this testing, okay?
And so this is the idea of test time training, which is our attempt to operationalize this on an infinite, smoothly-changing stream.
And the idea is to basically use self-supervision to continuously adapt to new data, okay?
And we did this already in the case of reinforcement learning with our curiosity work, and this new work is basically trying to do it for images.
And this is the paper, Test Time Training, and it was in an ICML 2000. Maybe I'll give you one slide of intuition of what we're doing.
Basically, the idea is that we're at, let's say we have a training set of object detection, right?
And at training time, we have our standard thing, we have our image, and we have our label, so nothing new here.
We have input in, label out, we are training, and then at the same time, we also have a self-supervised head that basically, given your image, it does some self-supervised tasks.
In this case, we are basically, our task here is rotation prediction. Given the rotated version of the image, we want to predict which rotation it is.
It doesn't really matter, it could be any task at all, okay?
So at training time, we do both of those tasks together, but at then at test time, of course, we don't have the labels, but we still have this task, okay?
And so we can basically around this, we can evaluate this task, and if the result is not good, if it failed this self-supervised task, we can do a little bit of fine tuning, a little bit of fine tuning training for this other task.
But as we are doing the fine tuning, it's going to get changed representation in a way that will also impact the real task that we care about, and that allows us to do better as we are changing, as we are going through the dataset.
And so here is an example where, you know, given this image at test time, basically the right label is elephant, but initially it basically thinks it's a dog.
But then as we do this fine tuning on our test, on our self-supervised task, it figures out that it's actually less of a dog and more of an elephant and give us the right answer.
And that's basically the story of the paper. Sorry, I had to rush, but you can look at the paper online.
And to conclude, why use self-supermission?
The reason that I like is that it allows us to get away from this top-down semantic categorization and gets us more into this bottom-up association story and learn things from the bottom-up.
But we must be careful that the supervision doesn't leak in through things like data documentation, right? And we need to be careful about this.
And second is that eventually self-supervision should enable us to check the datasets, forget about all these fixed datasets, and learn things continuously.
We're only starting on this direction. I think it's a very exciting direction, very exciting problem, so I'm hoping people will get excited about it.
Okay, thank you very much.
Yeah, some fantastic talk. Thanks a lot for all the amazing works.
Self-supervision is cool. Does anybody have some question on Zoom, maybe? Let's start with this. We have a lot of questions on YouTube, but I'm going to start with Zoom.
I have a lot of questions too, but if somebody wants to ask questions on Zoom, just turn on your video and just pick up, probably.
I can start with one with a maybe a high-level question first.
I think the challenge in self-supervision is, right, you basically have visual data and you, let's say, correlate patches with whatever contrastive loss or whatever people do now.
I mean, what do you think about, if you're thinking about the 3D world, right, you have obviously a third dimension.
Is it a smart idea to do this actually all on images and videos and not think about, I don't know, like, kind of project with 3D representation maybe first.
And then think about how to kind of get similarities in some 3D space, learn a 3D representation and then, you know, try to channelize with the on-screen tasks later on.
Right, right. No, this is absolutely. And as you know, you know, I've been, I've been angling for going into 3D, you know, since a long time ago.
Since our work with Derek Hoim on qualitative 3D, I'm a big fan of 3D in my heart.
And it's kind of a little bit sad that once, you know, once we went to neural networks, things drop back to 2D plane for a while.
And now, of course, they're coming back again.
Okay, so there's two answers to this question.
One, the final, you know, the ultimate answer is that 3D should emerge from our 2D of observations, that the representation should figure out 3D on its own.
Okay.
Just like it's done with humans, right? Humans are only seeing 2D projections of the 3D world.
Okay, if you have stereo, maybe you have a little bit of 3D, but, you know, I don't have stereo, for example, 10% of people in the world don't have stereo, and we are perfectly fine seeing 3D.
So we learn 3D from a series of 2D representations.
And I think if we go from, you know, collections of images like ImageNet to videos, for example, hopefully, and I'm very hoping that it will encourage 3D to automatically emerge inside of the representation.
Okay, so that's kind of the glorious answer at the end of the rainbow.
Okay.
But of course, this is very hard. This is kind of a very tall order.
And, you know, we're seeing a little bit of this happening. We're seeing a little bit of kind of a maybe two and a half, two or 2.1D kind of occlusion, occlusion reasoning, you know, figure ground reasoning, a little bit of that, but we're definitely far away from that, right?
And so the second direction is, okay, can we kind of help it out a little bit?
Can we provide features that are more amenable to three-dimensional manipulation? And there, I think, things like Hologan or PyGan, this kind of directions are, I think, very exciting in that it's kind of you can inject some things that you know are physically true, like rotation, for example.
And so I think in the short term, all of those things are, I think, going to be extremely helpful. In the long term, I'm still kind of hoping that I can learn 3D from scratch.
Okay, but who knows? Maybe it's too much to ask, but I'm still kind of hoping that one day I will wake up in the morning and boom, my computer learned 3D.
But we'll see.
Let's do it with two cameras, right? We have stereo.
That's the thing, but I don't have stereo, for example, right? Like 10% of people don't have stereo. Stereo is actually not as important as we think. Stereo is only really important for like the, you know, half a meter in front of you.
Like it's, you know, what is it that I cannot do that everybody else can do? Okay, I cannot put, you know, thread through the needle and I have trouble pouring wine, right? Other than that, I'm fine.
So really, stereo is kind of overemphasized. I think it's really parallax is much more important and parallax you can get from video.
No good point. I have another follow-up question.
So in a similar spirit, right? Like one argument is you can do contrastive learning and mostly it's about comparing things, right? You're saying one versus all classifiers, like how similar are these things?
I mean, what about going back to the original things that people using like auto encoders for pre-training and so on for like basically using generative tasks? Let's say, oh, I train my favorite GAN.
How good of a representation can I learn from learning the distribution basically, right? Like how well, like it's like this famous thing like you have to be able to create in order to understand and where to see that competing or maybe going along the same line.
So what's your take channel is speaking on the lines there?
I mean, we have definitely been also using auto encoders as well. I think with an auto encoder, it's a little bit of a magic box.
Like if you get really lucky, your auto encoder is going to capture exactly the right things. And if you get unlikely, it will capture all exactly the wrong things, right? It's a compression mechanism. It somehow compresses your data.
And it really depends on what you care about. Like sometimes it will compress away the stuff that you care about or sometimes it will retain the stuff that you care about.
And it's a little bit hard to control what it's going to do. So I think this kind of similarity learning allows you to get a little bit more control and a little bit more of an intuition about what is it being learned?
And it's also kind of has a very nice connection to kind of graphs and graph theory that kind of think you think about it like you have different entities and then you have kind of, you can think of it as like a network, right?
Like a social network, for example, where you can think of different people being connected in different ways and you can think about, yeah, we call them senses of similarity.
So there's many different senses of similarity between two instances and, you know, something like an auto encoder is probably going to collapse them all together and here you can actually separate them.
You can have a similarity in color, similarity in texture, maybe similarity in 3D and they're all can be kind of exposed hopefully separately.
No, that's interesting. I mean, our experience is so we've done a lot of stuff on like shape completion in 3D.
So whenever we had the ability to take stuff away and predicted, then we got great features. This was always amazing in terms of using these features to help semantics.
And whenever we're trying to classify it to help the completion, this is always a total disaster. It never worked. We tried really hard actually.
I mean, I think that that's that's been our, our experience as well but but I think have you have you tried the latest contrastive learning because it's really, it's, to me, the way I think about contrastive learning is it's really just old school,
it lost, you know, Simees network learning, except you're switching from, from, from, from kind of regression to, to a classification, but it's a classification with like huge amounts of data and it's very, very fine grain classification.
So it's almost, it's, it's really not like your, your grandma's classification. It's, I mean, we did something like this for, for, for when we did colorization. So we, we first we tried to do colorization with the regression.
And then we, we, we, we got better results by doing classification but the classification was across like, you know, 500 classes of different colors in the, in the, in the color gamut.
So it's, it's, it's a much more kind of narrow thing and that seemed to work for us but yeah, like if you have a few classes then, then, then it's very hard to make it work.
But if you do something like either have lots of classes or do something like, like contrastive learning where he's basically just really kind of push it with data that it seems to to work for us.
No, we actually tried that. So we had, we had one, one student actually in collaboration with fairs or to one of my students, they've been working on basically, basically doing contrastive learning for pre training 3D structures and in a similar way than you would do it in 2D.
It does help but the completion still seems to work a bit better.
It's very interesting.
I think, so in general, yeah, I mean, I think if, if completions, if, if actually predicting, you know, pixels or predicting voxels, whatever.
It has, it has more data. It has more information that and, and we know that 3D world is actually much, you know, it's much more informative right so, and it's also I think much more unimodal.
So the one thing that was hard for us, for example, when we did core colorization is what we're, you know, we're trying to colorize a bird and the birds could be yellow or the bird could be green.
Right. And so you have multi model you have two modes.
And if you're doing kind of a just sort of like a regression completion, what's it going to do it's going to do the average. Right. So it's going to be neither here nor there. Right.
But if you have a single mode it works really well. So it might be that in 3D, you're really in a world that's much more unimodal in which you don't have like you're not trying to have an average between two different completions and they get something that doesn't look like either but you're
actually really focusing on a single mode so in that case maybe this is why you're getting better results but if I suspect that if you had multi model like if you have a hole that's big enough that you could have many different plausible completions happen to it.
I suspect that that that then you're kind of the kind of the prediction route might have more problems.
No, I fully agree with you know that one is really true. And, but most of the case right you think about it's more like a drop out in a sense right so you're leaving some stuff right and then you're trying to figure out what's missing in this case I think we've experienced that it works remarkably well if it's too large then you need probabilistic models again and stuff like that and then it's a lot more difficult.
Yeah, I agree. Yeah, I think I think it's kind of a if it's a level of dropout and it should just work. Yeah, I agree. Yeah, yeah, I think I think if it works you should definitely use it. Absolutely.
Any other questions maybe maybe somebody else can ask questions. I don't want to dominate the discussion too much.
Hi. Yeah, thanks for the talk. I have a similarly high level question. So speaking along the lines of like multi modality and stuff like this it seems like you have a lot of inspiration in terms of how to learn perception based on how people perform perception.
And it seems like people do have naturally some kind of estimate of uncertainty multi modality and the ability to generate also for like these video tracking kind of applications that you showed like multiple hypothesis for where the prediction should go and how far do you think you can get without this explicitly modeled or using it needs to be explicitly modeled.
Good question. I think, I think I would go with I don't know. So,
yes, humans are very good at at modeling uncertainty, but they, I don't think they're doing it in the way that listed decisions to it. I don't think humans are actually probabilistic.
I think, I think they might be doing it, almost like if you remember from a long time ago, like all this particle filtering where you can keep a whole bunch of hypothesis and then you kind of keep all of them going for a while and then you kind of
drop one like, you know, there's illusion of like young lady old woman visual illusion where, you know, one day once time you see like an old lady one time you see a young woman, right, and you never see both of them.
So it seems some there's some very interesting mechanism going on but I think it's not, it's not like a standard probabilistic mechanism.
And so, yeah, so I don't know how to deal with it and in the in the vision in the in the in this video paper that I showed, you know, we are really just keeping a whole bunch of hypothesis as we're going through the through the video at training time.
At test time we don't.
And, but whether that's the right thing to do or not, I don't know. I think it's a very important question.
I don't, I don't have an answer but frankly I think that nobody else does either.
Sounds good.
I think one other somewhat unrelated thing I think there's a bit of attention between people who think that we should be able to learn everything from scratch like you mentioned in terms of being able to learn 3D and
it's actually possible because it's unclear I guess how much how much supervision, certainly for like some radic perception, people get direct supervision.
And so, yeah, okay, now we're back to philosophy.
It must be possible, because, because it already happened, right.
So learning is something that happens in nature, but it's, it's very, very rare, like, like parents teaching their children things.
I know that a lot of modern parents they feel like it's super super important, but sorry, you know, developmental psychologists disagree they say that it doesn't really matter that much.
Most of the things that a kid picks up, they pick up without supervision, they keep pick up on their own.
And, and you could think about it, you know, the, you know, from in the very beginning, right in the beginning, if you're, you know, as long as you believe in evolution, you must believe in in unsupervised or self supervised learning because in the beginning there was nothing there was
no, there was no teacher, there was no supervision, there was only data, right, and kind of the organism and its environment were co-involving and learning from each other and and and and and develop.
So I think there is, there is, to me, there is no question that it should be possible in theory.
I think that the kind of the the interesting question is, is it doesn't make sense to do in practice, right. And like, you could also say, well, why don't we just simulate evolution for for a gazillion years and then we'll get everything right and that's of course not feasible with the current technology.
So I don't think that there is that much, or maybe there shouldn't be that much tension because I think, I think there are people like me who really want to try to learn things from first principles and I think this is very interesting.
If, if, if anything from, you know, from the biological possibility point of view. Okay. And there are people who just want to get stuff done and and and get to a good result too fast.
And those people should definitely just use whatever works best at the time. So I'm not sure that it's either or I think both directions are are useful and I think we're learning from each other.
Each directions are informing each other. So for example, for a very long time, self supervised approaches worked worse than supervised approaches. And so, you know, if, you know, we could have all quit because oh my God, you know, our stuff doesn't work as well as supervision.
And we persevered because we thought that, you know, there's something interesting that that we can learn anyway. And now what we're seeing is that for some tasks, self supervision actually works better than supervised learn, not for all not for many but for some
there's definitely some cases when it actually the the the learning from the data actually gives you better results than learning from from from from labels.
I think, I think, I think, you know, let all the flowers bloom. It, I think both directions are useful and I think it's, it's great that people are pushing in in in both of them and I think we'll, we'll get to a better point, eventually, and we'll learn more.
So I'm actually optimistic on all fronts. It's not a competition. Well, it is a competition, but it's not like it's not one is right and the other is wrong. I think both are right.
Cool. All right, I think that's a that's a very good, I guess, ending of the live stream. I think thanks a really lot for the amazing talk. I'm a little bit over. I have to apologize to a lot of questions on YouTube. We couldn't unfortunately go into all of them.
But it was really great to have you. And I hope also for everybody who is with here right now next week, we'll have another great lecture with rock help. And yeah, we'll see. So thanks a lot again for the great research.
Okay.

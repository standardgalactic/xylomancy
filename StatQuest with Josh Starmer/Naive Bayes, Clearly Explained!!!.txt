I'm at home during lockdown, working on my stat quest, yeah.
I'm at home during lockdown, working on my stat quest, yeah.
Stat quest.
Hello, I'm Josh Starmer, and welcome to Stat Quest.
Today we're going to talk about naive bays, and it's going to be clearly explained.
This stat quest is sponsored by JADBIO.
Just add data, and their automatic machine learning algorithms will do the rest of the work for you.
For more details, follow the link in the pinned comment below.
Note, when most people want to learn about naive bays, they want to learn about the multinomial naive bays classifier,
and that's what we talk about in this video.
However, just know that there is another commonly used version of naive bays,
called Gaussian naive bays classification, and I cover that in a follow-up stat quest.
So check that one out when you're done with this quest.
Bam.
Now, imagine we received normal messages from friends and family,
and we also received spam, unwanted messages that are usually scams or unsolicited advertisements,
and we wanted to filter out the spam messages.
So, the first thing we do is make a histogram of all the words that occur in the normal messages from friends and family.
We can use the histogram to calculate the probabilities of seeing each word,
given that it was in a normal message.
For example, the probability we see the word deer,
given that we saw it in a normal message,
is 8, the total number of times deer occurred in normal messages,
divided by 17, the total number of words in all of the normal messages,
and that gives us 0.47.
So let's put that over the word deer so we don't forget it.
Likewise, the probability that we see the word friend,
given that we saw it in a normal message,
is 5, the total number of times friend occurred in normal messages,
divided by 17, the total number of words in all of the normal messages,
and that gives us 0.29.
So let's put that over the word friend so we don't forget it.
Likewise, the probability that we see the word lunch,
given that it is in a normal message, is 0.18.
And the probability that we see the word money,
given that it is in a normal message, is 0.06.
Now we make a histogram of all the words that occur in the spam,
and calculate the probability of seeing the word deer,
given that we saw it in the spam,
and that is 2, the number of times we saw deer in the spam,
divided by 7, the total number of words in the spam,
and that gives us 0.29.
Likewise, we calculate the probability of seeing the remaining words,
given that they were in the spam.
Bam!
Now, because these histograms are taking up a lot of space,
let's get rid of them, but keep the probabilities.
Oh no! It's the dreaded terminology alert.
Because we have calculated the probabilities of discrete,
individual words, and not the probability of something continuous,
like weight or height,
these probabilities are also called likelihoods.
I mention this because some tutorials say these are probabilities,
and others say they are likelihoods.
In this case, the terms are interchangeable, so don't sweat it.
We'll talk more about probabilities versus likelihoods
when we talk about Gaussian naive Bayes in the follow-up quest.
Now, imagine we got a new message that said,
Dear Friend,
and we want to decide if it is a normal message or spam.
We start with an initial guess about the probability that any message,
regardless of what it says, is a normal message.
This guess can be any probability that we want,
but a common guess is estimated from the training data.
For example, since 8 of the 12 messages are normal messages,
our initial guess will be 0.67.
So let's put that under the normal messages so we don't forget it.
Oh no! It's another dreaded terminology alert.
The initial guess that we observe a normal message is called a prior probability.
Now we multiply the initial guess by the probability that the word Dear occurs in a normal message,
and the probability that the word Friend occurs in a normal message.
Now we just plug in the values that we worked out earlier and do the math,
beep boop beep boop beep, and we get 0.09.
We can think of 0.09 as the score that Dear Friend gets if it is a normal message.
However, technically, it is proportional to the probability that the message is normal,
given that it says Dear Friend.
So let's put that on top of the normal messages so we don't forget.
Now, just like we did before, we start with an initial guess about the probability
that any message, regardless of what it says, is spam.
And just like before, the guess can be any probability we want,
but a common guess is estimated from the training data.
And since four of the twelve messages are spam, our initial guess will be 0.33.
So let's put that under the spam so we don't forget it.
Now we multiply that initial guess by the probability that the word Dear occurs in spam,
and the probability that the word Friend occurs in spam.
Now we just plugged in the values that we worked out earlier and do the math,
beep boop beep boop beep, and we get 0.01.
Like before, we can think of 0.01 as the score that Dear Friend gets if it is spam.
However, technically, it is proportional to the probability that the message is spam,
given that it says Dear Friend.
And because the score we got for normal message, 0.09,
is greater than the score we got for spam, 0.01,
we will decide that Dear Friend is a normal message.
Double Bam!
Now, before we move on to a slightly more complex situation,
let's review what we've done so far.
We started with histograms of all the words in the normal messages,
and all of the words in the spam.
Then we calculated the probabilities of seeing each word,
given that we saw the word in either a normal message or spam.
Then we made an initial guess about the probability of seeing a normal message.
This guess can be anything between 0 and 1,
but we based ours on the classifications in the training dataset.
Then we made the same sort of guess about the probability of seeing spam.
Then we multiplied our initial guess that the message was normal
by the probabilities of seeing the words Dear and Friend,
given that the message was normal.
Then we multiplied our initial guess that the message was spam
by the probabilities of seeing the words Dear and Friend,
given that the message was spam.
Then we did the math and decided that Dear Friend was a normal message
because 0.09 is greater than 0.01.
Now that we understand the basics of how naive Bayes classification works,
let's look at a slightly more complicated example.
This time, let's try to classify this message,
Launch Money Money Money Money.
Note, this message contains the word Money four times,
and since the probability of seeing the word Money is much higher in spam
than in normal messages,
then it seems reasonable to predict that this message will end up being spam.
So let's do the math.
Calculating the score for a normal message works just like before.
We start with the initial guess,
then we multiply it by the probability we see lunch,
given that it is in a normal message,
and the probability we see Money four times,
given that it is in a normal message.
When we do the math, we get this tiny number.
However, when we do the same calculation for spam,
we get 0.
This is because the probability we see lunch in spam is 0,
since it was not in the training data.
And when we plug in 0 for the probability we see lunch,
given that it was in spam,
then it doesn't matter what value we picked for the initial guess that the message was spam,
and it doesn't matter what the probability is that we see Money
given that the message was spam.
Because anything times 0 is 0.
In other words, if a message contains the word lunch,
it will not be classified as spam.
And that means we will always classify the messages with lunch in them as normal,
no matter how many times we see the word Money.
And that's a problem.
To work around this problem,
people usually add one count, represented by a black box,
to each word in the histograms.
Note, the number of counts we add to each word
is typically referred to with the Greek letter, alpha.
In this case, alpha equals 1,
but we could have said it to anything.
Anyway, now when we calculate the probabilities of observing each word,
we never get 0.
For example, the probability of seeing lunch,
given that it is in spam, is 1,
divided by 7, the total number of words in spam,
plus 4, the extra counts that we added.
And that gives us 0.09.
Note, adding counts to each word does not change our initial guess that a message is normal,
or the initial guess that the message is spam,
because adding a count to each word did not change the number of messages
in the training dataset that are normal,
or the number of messages that are spam.
Now when we calculate the scores for this message,
we still get a small number for the normal message,
but now when we calculate the value for spam,
we get a value greater than 0.
And since the value for spam is greater than the one for a normal message,
we classify the message as spam.
Spam!
Now let's talk about why naive Bayes is naive.
The thing that makes naive Bayes so naive
is that it treats all word orders the same.
For example, the normal message score for the phrase Dear Friend
is the exact same for the score for Friend Dear.
In other words, regardless of how the words are ordered,
we get 0.08.
Treating all word orders equal is very different from how you and I communicate.
Every language has grammar rules and common phrases,
but naive Bayes ignores all of that stuff.
Instead, naive Bayes treats a language like it is just a bag full of words,
and each message is a random handful of them.
Naive Bayes ignores all the rules because keeping track
of every single reasonable phrase in a language would be impossible.
That said, even though naive Bayes is naive,
it tends to perform surprisingly well when separating normal messages from spam.
In machine learning lingo, we'd say that by ignoring relationships among words,
naive Bayes has high bias.
But because it works well in practice, naive Bayes has low variance.
Shameless self-promotion
If you are not already familiar with the terms bias and variance,
check out the quest. The link is in the description below.
Triple Spam
Oh no, it's one last shameless self-promotion.
One awesome way to support StatQuest is to purchase the naive Bayes StatQuest study guide.
It has everything you need to study for an exam or job interview.
It's eight pages of total awesomeness.
And while you're there, check out the other StatQuest study guides.
There's something for everyone.
Hooray! We've made it to the end of another exciting StatQuest.
If you like this StatQuest and want to see more, please subscribe.
And if you want to support StatQuest, consider contributing to my Patreon campaign,
becoming a channel member, buying one or two of my original songs
or a t-shirt or a hoodie, or just donate. The links are in the description below.
Alright, until next time, quest on!

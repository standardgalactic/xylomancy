Almost all microprocessors we have today started off with the idea of one processor.
It would be the CPU, the one chip that is running everything.
And although multiprocessor submissions are quite common these days,
each processor in the system is basically a copy of the other one.
They're all processors that were designed to run by themselves, but we just have more than one.
However, there is a processor that from its initial design was not intended to be there by itself.
It was designed from the outset to be part of a cluster of processors.
Yep, we're going to talk about the transputer.
Now in this video we're going to talk about the transputer in its history,
and also have a look at this weird little development box you can see in the background of shots here.
And towards the end of the video we'll talk about what might be one of Atari's
least successful computers ever.
In fact, I'm pretty sure it is the least successful thing they've ever made.
And I'm even concluding a jaguar in its CD player.
In fact, why don't we all play a little game together?
You have a guess at what you think the machine might be?
Make a comment in the section below, and at the end of the video,
you can see whether you guessed the right one or not.
Control-Alt-Rest, we already know you know the answer.
Now as I mentioned in the introduction,
most CPUs design with the idea that they're the one CPU.
When the design is creating it, they're not thinking about other processors,
or at least they weren't initially.
These days, it's a little different.
When the transputer was created, they were thinking about parallel processing from the start.
There was never going to be a system that had just one of these things.
And that makes the transputer pretty unique.
And in a little bit, we're going to look at some of the architectural features
that ended up in the transputer.
So it can be this fairly unique design goal.
But first, we're going to look at the circumstances that
caused the transputer to come into existence in the first place.
In 1978, the UK's then Labour government
was devising its industrial strategy for the IT sector.
And between that and the National Enterprise Board,
they landed on the idea that they should create
a new microprocessor design company.
Thus, IMS was established by Ian Barron, Dick Petrus, and Paul Schroeder.
Those last two names should be familiar for anyone who knows about MOSTech,
because, well, they just sold MOSTech for basically a small fortune,
and decided to get involved in this.
Now, the strategy the National Enterprise Board had was initially that they start a
joint venture where they'd be manufacturing stuff in the US,
where it was already done, and then transferred that technology to the UK,
and set up manufacturing there.
And by the time that manufacturing was established and ready,
well then, IMS would have a chip design ready to be manufactured in the UK.
And the idea is, after that, this whole thing would have
bootstrapped the technology industry in Bristol,
which it did actually manage to do, and is still there to this day, doing stuff.
So that's the why, guess we'd best get on to the who.
Now, why IMS were getting on with manufacturing static RAM,
which eventually they managed to capture 60% of the market for,
they hired themselves Robert Mill and David Meng.
And not Brian May, like my friend thinks that they did.
And surprisingly, the globally renowned astrophysicist Brian May,
an occasional guitarist, was not particularly useful when it came to the process of design.
He's more of a stereo photography kind of a guy.
And of course, guitarist for Smile and Queen,
we probably shouldn't forget to mention that.
And of course, patron saint of all badgers.
Barron had the vision to create a microprocessor explicitly for parallel computing.
And as in fact, the person who came up with the name Transputer.
The idea being that the transistor was pretty common and ubiquitous for making all logic work,
and well, you want this chip to be equally as ubiquitous and common as the transistor was,
but for computing, hence, Transputer.
Now, you might be surprised to know that Milne and May didn't jump straight into
designing the processor itself.
Where they started is with the programming language Occam.
Before we get into discussing Occam, though,
I should mention that today's video is sponsored by PCBway,
a company whose primary occupation seems to be sponsoring YouTube channels.
But when they're not doing that, they also make printed circuit boards,
they do surface mount type stuff for your pick and place,
they'll also do CNCing, 3D printing.
Basically, if you need to make it, they can do quite a lot of it for you and send it to you in the post.
Occam was named after 13th slash 14th century razor blade manufacturer, William of Occam.
And now, having just made that joke, I realize a number of you are reaching for the comments
section saying he didn't make razor blades.
I know, it's not that kind of razor.
Anyway, quickly back to the language.
Now, both Occam's creators had quite a lot of experience with creating or working on compilers before,
with David May having done a fair bit of work on BCPL,
and Tony Hoar, of course, being famous for creating alcohol 60,
and the quicksort algorithm, and basically a whole bunch of things.
Actually, I believe he's sir Tony Hoar these days.
Now, this language wasn't going to be like any of the others they'd come up with before,
because the idea of Occam was to create a language that you could develop parallel programs in
from the ground up. It wasn't an idea that's bolted into the language later on.
And you'll see this from one of Occam's major two keywords,
par and seek. Each block of code would need to be labeled
either par for you can run all these instructions in parallel,
or seek these instructions must run in sequence.
There is also a third keyword like this, alt,
which means that if it's ready to do one thing, you can do that.
If it's ready to do another thing, either hardware is ready, you can do that.
It's all about listening on a channel and waiting for stuff to be ready on the channel,
and then you can do some work. And the blocks after that can either be in sequence or in parallel.
Mostly they're in sequences, you're waiting for something to do some work upon.
Now, I should probably mention channels, because this is one of the key ideas in Occam as well.
In Occam, you don't have shared variables. Two things that run in parallel cannot access
the same memory or variable, if you will, at the same time.
Now, if you've done any development with anything that uses threads or does any parallel stuff,
you know that not all languages are like that. But the reason both May and Hall were trying
to avoid shared variables is they knew that that's where deadlock lies.
For parallel programs going to go wrong, it's usually around
something failing to lock a variable properly before it uses it or edit it,
or a bunch of things fighting over access and deadlocking the whole system.
So they had a pretty good plan to avoid that, which is just not to let that be a feature of the language.
But obviously, information does need to be passed between bits of code
executing in parallel, and this is where the channel's idea comes from.
Essentially, a message-passing system, where the thing that owns the variable,
can indeed pass that data along a channel to another process running in parallel,
so that it can use it. And that's why that alt keyword was handy.
You'd have a bunch of things in parallel, computing things,
and shoving them down a channel. When you hit the alt statement,
essentially you can think of like a first-pass-the-post type system.
Whichever channel delivers something first, well, that alt statement means
the blocker code attached to whatever channel is the first one ready, gets to execute.
After having developed the language spec, they then created the compiler for Occam.
Now you may be thinking, wait a minute, didn't he skip over the step
when they designed the transputer, and the answer is no.
I know it's quite common to go design a CPU, making a compiler for various languages,
forcehead CPU. But they deliberately went the other way around,
they created the language, then they created the compiler to get it down into the smallest
number of instructions they thought were viable, and then when they knew what instructions they'd
need their processor to execute, well, that's when they started work on the processor.
Now in a few talks David May has given, he stated a few times that someone shouldn't
be allowed to design a CPU unless first they've had to create a compiler or two for somebody
else's. And the reason that lies behind that is a lot of CPUs that were created around the time
or before David was working on the transputer had some pretty, let's say, bat poop instruction sets.
Lots of these earlier CPU designs had an awful lot of instructions, and I mean a lot,
basically instruction almost for anything you might want to do.
And the reason things ended up that way is the CPU designers are not really thinking about
compilers at this point, they're thinking about people who are writing programs in Assembler.
Essentially they're trying to make life easier for human beings who are going to use
CPU instructions almost directly. The problem is by the time we get to languages that use a compiler,
well, most of the instructions a human being might reach for on a CPU are not the ones that a
compiler is going to, because they're a lot less optimal in some cases. With Cisco processors this
gets to the point where with Vax you've almost got if then else in hardware, and as most programs
start to be written in a higher level language that's compiled rather than Assembler, some of the
very infrequently used CPU operations don't get used at all. In fact there's a famous study
that went on to show that most applications use a relatively small subset of the instruction set
on the CPU, and this is what gives rise to the risk movement. Now the reason I'm talking about all
of this is to show you just what a good idea it was to go from language, compiler, then CPU,
because you know which instructions you need to implement, you don't need to implement a CPU with
thousands of instructions. And this is why Occam and the transmitter are so interlinked with each
other. Occam was created to target the thing that became the transmitter, and the transmitter is
built to implement what Occam's compiler outputted. And design features from the language also ended
up essentially getting implemented in hardware, we'll come to that later on. Right now we talked
a lot about how we got to having transporters and why, we've not actually talked a lot about the
hardware yet, so let's do so. And the first actual hardware became available in 1984, and to
say the transmitter is an odd fish, well it's kind of underselling it really. Now let's start with
just the processing type bits. First of all it uses microencoded instructions, like a CISC processor,
but it's not a CISC processor. And most of those microcoded instructions, well they run in one
clock cycle, very much unlike most CISC processors. Then we come onto the fact that it's a stack
processor, but it's a stack processor with registers, which is a thing that normal CPUs have, and
stack processors generally don't. And then we come to the un-processor-like bits that are on the
transmitter chip. See the transmitter includes its own built-in RAM, and of course RAM controller as
well, and I don't mean that it had some cache memory, I mean this is actual RAM RAM. The idea being
that you could run a number of transducers together, and you wouldn't have to connect up
external memory. I mean obviously when people started building machines, yeah sure they connected
up external memory, but they didn't have to, and not every dev board did. Now there's not a lot of
RAM on the first implementation of this, so the first ever hardware, it's a 16-bit processor,
comes with a grand total of 2 kilobytes of memory, which is not a lot of RAM, but it's a heck of a
lot of RAM for on the processor, particularly in 1984 when my whole computer at the time had 32K
of memory, and that initial hardware chip was very quickly followed up by one with 4K of memory on
board, and it also added some breakpoint debugging support directly into the chip as well, which
made life a heck of a lot easier. Now this chip was very quickly followed up in 1985 by a 32-bit
version, known as the T414, and again that version shipped with 2K built in, and then there was a
4K version of it that followed later on. Now these early 16-bit and 32-bit chips,
they were the ones that the first system builders started to get their hands on.
Now so far I've only talked about these things like they were normal processors, but there's
one big architectural feature I've not mentioned yet, and if you know anything about the transmitter
you're probably wondering why, because it is its most notable feature, but I did foreshadow it when
I was talking about channels in Occam earlier and said we'll come back to this later, well
we're coming back to it. Transputer chips had a link feature that was just there to connect
one transmitter chip to another transmitter chip. The very early ones came with two links,
but later on by the time we get into the 32-bit ones, we've got four links coming out of each
chip, so you can build a grid array of transmitter chips. Now this makes total sense in the chip
designed for parallel computing, after all you can't just have one, because if you do,
nothing's happening in parallel is it? And this is where we get the modeling in hardware of that
channel system we were talking about before, as the channels are essentially the inter-chip links
if you like, and because that hardware element comes directly from the language we're using,
well we get to make pretty efficient use of them, as your programmers are already thinking about
having chips waiting on messages through channels, and can see where the bottlenecks in their code
and therefore when implemented in hardware in the hardware would be. Now this whole chip link thing,
as we end up with more and more and even more powerful transmitter chips all in one grid,
well four soon becomes not enough, and you eventually end up with more sexually
implementing the first ever on-silicon switch, and using that means that a transmitter doesn't have
to root to another transmitter through a number of transporters to get to it, the message can be
switched to the right transmitter using the silicon switch. I mean this is an idea that
Ethernet really embraced and ran with later on, but let's get back to the next major development
of the transmitter line, and that's with the T8 that gets introduced in 1987. Now this is a pretty
big leap forward for transputer kind, because we get an extended instruction set, which provides
floating point support. Now up until this point, if any computer had done any floating point maps,
it'd done it to its own, well, standard unique to that computer more or less, and the reason why
Imos had waited up until this point to introduce floating point support was that finally it had
been standardised by the IEEE. This meant that floating point maps would work the same way on
any machine, implementing that IEEE floating point specification, which was a pretty big deal.
Now first of all, Imos and making these chips to go into the high-performance parallel computing
space, which is, you know, mostly scientists and engineers, that sort of thing, universities,
so they're being used for pretty mass-intensive tasks. So not having floating point, well,
that was not as helpful as you might think, but also you needed a well-understood floating point
standard that the mathematicians involved in this stuff knew, understood, and could get what the
limitations were, and whether what they were doing would fit into it well. Now this is where they
actually took a bit of departure on how they went about implementing this standard compared to others,
as Imos decided to go full formal methods. This means that it did take Imos a fair chunk more
time to do their IEEE implementation. In fact, it added about six months on to it, is what they
estimate, but it made come the end of it, they had a provably correct implementation of the IEEE
standard. And when you're selling a very high-end and expensive system to scientists, it helps to
be able to tell them that it will definitely get its mass right, because there were other CPU
manufacturers that did not get floating point right. Yes, Intel, we're looking at you. Yes,
famously when Intel released the Pentium, they actually got a bug in their floating point
division instruction. Yeah, it didn't actually divide things correctly. Bit of a bummer when you
write software that relies on that, really. There were actually software workarounds to these
problems. It wasn't so bugged that you couldn't work around it, but it did mean that some poor
saw that actually did have to work around it. And when you're a scientist who's buying a new
expensive computer to do whatever it is you're going to do with it, you kind of want it just to
get its mass right. Now around the time the T8 was introduced, there was an excitement around
the transputer had really built up. And this is like its key commercial period where it's really
leading its field at this point. It must have been taking transputer-based systems to
digraphs and really showing off what the machine could do to the point where some people didn't
believe that the little box they were seeing the few transputers in was actually doing what
they were seeing on screen and assumed that somewhere they must be hiding a massive mini
system somewhere. And it's in the same year in 1986 that Edinburgh University puts in their first
experimental system using transputers to see what they can do. This was regarded as a little system
only having 40 transputer chips in it. Admittedly that's more processing power than all the machines
I'd ever seen at that point in my life added together, but still as supercomputers go relatively
small. But they were suitably impressed with it and installed a much larger system based on the T800
and using a microfax as the file server because transputer systems didn't really do
file IO particularly. And that was completed and installed by 1987 and was effectively one of the
most powerful supercomputers in the UK at that time. Or more or less anywhere to be honest. In fact
Edinburgh still is a major centre of supercomputing. And this transputer-based system which peaked at
around 400 processors that stayed into its original form until about 1992 where it was
reconfigured so it had a spark-based machine from Sun to act as the host machine and then finally
was decommissioned in 1994. So they got seven years use out of it which for a supercomputer at that
time yeah that was a good long lifespan. Now you may have noticed me using the phrase host system
and that brings me on to one of the odd things about the world of transputers and also helps explain
this machine I've got here. Yes this is a transputer-based system but you might notice that
it doesn't look a lot like other computers. In fact if I turn it round to the rear you'll see
a number of pretty important ports missing. There's nowhere for a keyboard to go in. Or
attach a hard drive or a floppy drive or in fact any I.O. at all. Now there's a couple of decent
reasons for that and the first thing comes down to the design's feature of the transputer itself
in that it does not have a memory management unit or support for virtual memory at all.
Which for almost every high-end operating system at the time was a big no-no. You were not going
to port unix to this thing or even Windows NT for that matter. Now there was eventually a
Unix-like operating system if not Unix developed for the transputer but we'll come back to that
later and even that does involve a host system of some description. Again we'll get into the
details of that one in a bit. So without an operating system readily portable if you'd
wanted to run this like a normal computer well you would have needed to produce your own operating
system and then all your own drivers for every bit of hardware you want to plug into it and that
starts consuming a lot of resource for well not much purpose because no one was going to use
the transputer as their usual workstation you're not going to word processor email on this thing
this is for supercomputing you don't care if it doesn't run a word processor you want to get it to
do an awful lot of maths. Most transputer systems sold were kind of like the one in front of me
here a development system that hangs off another system as the host computer. The host computer
would be the terminal do all your file IO and it would just communicate with the transputer system
like a CPU accelerator almost only it didn't you know run the same instruction set kind of like
a modern GPU if you will in fact this is what this is a card for this would go in the host IBM PC
the software would run on there and this ribbon cable would connect it up to the transputer box
you would then run your software under DOS for example in this particular case and that would
provide you with your basic IO in and out of the transputer system so that's where you'd see your
terminal where you command it to do stuff and the transputer system will be able to request
files and stuff back through that cable it would then use the application and pull it from the
DOS filing system return the file back to the transputer and file writes work in the same way
just in reverse now if you're wondering how the transputer system boots up well ordinarily when
a transputer hits power on it just sits there and waits for messages to be passed over those
into process links we talked about earlier essentially just sits there and does absolutely
nothing however there is a pin on the transputer and that if you short it to ground well instead of
powering up in its normal state the transputer ship will power up and read in a chunk of software
from a ROM watch read in from the ROM gets dumped on the chip stack and then it starts executing it
and it will message all the other processors to essentially wake them up and get them to do something
so inside this rack here is a ROM that the one boot transputer reads and it contains enough software
so the transputer system sits there waiting to be told what to do via that ribbon cable that's linked
to the pc now this isn't the only transputer development system for the ibm pc you would
actually get isocards that had a couple of transputers on and essentially a chip that would sit there
talk to the isobus pretend to be some form of serial interface and the pc would communicate
with the transputers in more or less the same way it does with an external system just it all
happens to be on one card and there's a lot less transputers in there now this box is a little
different in that there is also a graphics card in here this particular development system was
used by a friend's dad for his medical physics phd so the graphics card was used to display the
output of what was being computed on the transputer system in a resolution and color depth that your
average ibm pc was going to get nowhere near hence the graphics card costing nearly as much as the
whole transputer system put together this also feels like a good chance to say yeah thanks for
letting us have the system fio very kind of you i should also mention you're not going to see me
power this thing up in the video apart from the fact that i don't have the software to run on the
ibm pc side of things so it would be an extremely uneventful thing to watch this thing is also sat
in the cupboard for a very long time since fio's dad finished his phd so i don't just want to switch
it on and you know set fire to it and why i have checked out that a lot of the resistors you know
resist the right amount and capacity is still capacitor enough for it and haven't leaked out the
smelly fish juice there is still a lot for me to check over on this machine before i apply power to
it including the psu itself which i haven't got around to looking over or in fact testing any
of the output voltages off first so there's a lot of work to be done before it can just be powered
up i also still need to find the software from somewhere so you know it does something but the
kind of machine you see here represents the majority case of usage of the transputer back at that
point in time people used it like a big parallel cpu accelerator you could bolt onto the end of
another computer and you know get to do some serious computation with the sort of thing that
your little host machine couldn't really manage even if your host machine was a you know fairly
decent sunwork station by the way if anyone out there watching this video happens to have a copy
of the software for the transputer side well the pc side of the transputer system please do get in
contact because i am having no luck finding this stuff commercially speaking imas have been doing
pretty darn well with its range of chips it had available even from when it first got some chips
out there into the market but being state owned meant that shortly into its existence when there
was a change of government that political party was ideologically opposed to the idea of the state
owning basically anything so for most of its existence the government had been looking for
someone to sell it off to and eventually it did find a company to sell it to and it was probably
not the most suitable company in the world it was fawn emi a company at that point that can
best be described as a sprawling basket case of an organization it seemed to have a finger in
everything it made surprisingly rubbish record players i know we owned one video recorders tv's
it was even into games publishing fire emi it was a fairly big record label with a healthy film and
tv business it owned part of itv's tems television it had tv and film studios it owned a big chunk
of a cinema chain it was even invested into hbo and for some reason also owned a chain of bingo
halls of course it did and now it was the proud owner of a supercomputing processor manufacturing
company yeah that that that fitted well i mean you definitely can see the parallels between
designing supercomputing chips and owning the winter gardens in blackpool for example now none
of you will be surprised to hear that a forced wedding between fawn and imos yet did not work
out well for imos it kind of struggled to get the funding it needed to do what it needed to do
in order to keep the transputer line moving forwards and for all the time that fawn owned them
they seemed to be trying to find someone else to sell them to and eventually they sold them to
the sprawling basket case of a french organization that was thompson of course thompson quickly
went bust ended up selling that bit of itself off to st microelectronics where finally the
transputer did actually get some use unfortunately in set top boxes of all things now during this
period of being owned by fawn the government did try and help imos out and the transputer and created
the transputer support center in chaffield that was there to support people developing applications
and uses for the transputer and for a brief while they did a pretty good job the problem wasn't so
much finding applications for the transputer it was that imos is underfunding when it was being
owned by fawn stopped them from really keeping ahead of where they needed to be to keep the
individual power of each chip up where it needs to be and also hitting scales and costs that would
be useful but during its time the transputer was commercially very successful and did achieve
what it intended to do it brought parallel computing to a much wider audience in a useful way i wouldn't
want you to come away from this video feeling that the transputer was a a failed idea because it
really really wasn't it's just under the ownership of fawn it just didn't receive the kind of money
it should have done because fawn was busy circling the drain and turning into an absolute basket
case of a company and then unfortunately imos was then sold to another basket case doing exactly
the same thing by the time st microelectronics got their hands on it well it's a little too
late and costly to get back into the high performance computer space with it but st did base a number
of products on the transputer and he had a nice second life in set up boxes and there's micro
controllers which to be fair was st's wheelhouse now i'm going to end this video talking about one
of the well oddest moments of the transputer's history or at least to me remember i mentioned
before that there was a unix like operating system for it well we're going to discuss that and the
atari transputer workstation which is of course the answer to the question i posed at the beginning
of the video of what do you think atari's least successful computer product is hands up how many
of you had actually heard of the atari transputer workstation i mean if you had you could probably
guess that this was the answer given that this is a video about transputers so atari the people
responsible for the st in the 2600 decided they would make a high-end workstation based on the
transputer because no brand screams high performance workstation like atari yeah this must be atari's
like most obscure product the transputer workstation and probably the one that sold the least quantity
of anything they ever made and this is the company that brought us the jaguar and more impressively
the jaguar cd add-on because what a failed console really needs is an add-on even if you did get
armoured to chanks in to help with the design work now this story begins with dr tim king
and named that some of you will probably recognize and a few of you apparently know based on what you
said in the comment section he worked for metacomco the people who did the operating system for the
amiga and when he left it he started a new company with the aim to make a unix like operating system
for the transputer now i say unix like and that's because he was going to implement
for it so you could compile and run unix software for it but it couldn't be unix because there's
no mmu in the transputer it doesn't do memory management so you couldn't have a full unix
operating system on there with all the memory protection now that's not as important as it
might sound for making a unix like operating system because thanks to the transputer being a stack
based system with stack registers not being able to do memory protection the way that unix normally
would with the mmu is not as big a killer as you might think we weren't going to create a you know
pre os 10 apple mac here applications would have to go out of their way to mess with the memory
space of another application it isn't just a case of forgetting to put a null byte on the end of a
string and oh look the word processes somehow overwrote the operating system and if one task
happens to hug an entire cpu well in a transputer system that's not really such a big deal as it
would be in say a single processor mac also another transputer chip could just tell it to stop so
we know process management is a real thing in this operating system so the idea was to create
this operating system he lost there'd be unix like or unix like enough and running on the transputer
of course then they decided that they should split part of the company off to start looking at building
hardware that this could run on but as almost everyone who worked for the company was former
metaconco people they had contacts in both comador and atari as they'd worked on stuff for
both of them in the past and one of the things they initially tried was making a hardware card
to go inside the amiga 2000 so the amiga 2000 would act as a boot system help bring up the
card he lost would run on there and i o would be handled by the amiga now it seems comador did what
comador does and just randomly lose interest in this so they started talking to atari and that's
where we get the atari transputer workstation and it works on a very similar idea what we essentially
have is an atari st mega that acts much like a host system does for all the other ways we run the
transputer it does the i o it provides you with floppy and hard drive controller and is what boots
he lost once he lost is booted and running on the transputers the 68 000 on the mega st board
just becomes an i o controller at this point now you may have noticed that no point if i mentioned
the atari st's graphics chips getting used and that's because the transputer workstation has
got its own graphics card that's driven from the transputer side of things and that's known as the
blossom board now this meant that helix could run x-window so regular x-window software would compile
and work on it and the blossom board gives us the kind of resolution you'd expect on a high
end unix workstation of the time so we got 1280 by 690 at 16 colors palette 4096 we got
1024 by 768 with 256 colors and that's driven out of a palette of 16.7 million and then we got modes
that are more in your lower end workstation than pc range so we can do 640 by 480 and again that's
limited to 256 colors at once and then we have a 512 by 480 in which we can use the whole 16.7
million colors at once there are also a number of sort of hardware acceleration features in there
that x can make use of so we got a blitter that allows us to move sort of tiles and windows around
and other things now oddly for a transputer based workstation as it comes it chips with one whole
transputer chip which for a system designed to do stuff in parallel is it's less than ideal but the
motherboard does have four slots to add what they refer to as farm cards which is a card containing
four additional transputer chips which means that with a fully expanded system you could have 17
transputers in there which is you know not bad and it actually has one of those emos switch chips
we talked about earlier so the internship performance is actually pretty good originally atari we're
going to call this thing the abac but then decided that they call it the atari transputer workstation
as that gave people a clue as to what it was and why you might want it and atari karn have been
expecting a lot of people to buy one of these i mean it's a very high end system but you've
gotta think that atari were hoping that they were going to sell more than 350 of them because
that's the total number that were produced and apparently somewhere between 50 to 100 of those
were prototype machines as well now i am told this machine was actually designed in cheffield the
city that i live in but i can't find any hard documenting evidence to say yeah 100 that was
true but given that the transputer support center is based in cheffield and the department of computer
science cheffield was one of the leading research groups for the transputer yeah i'm not gonna die
of shock if that was true now one of the interesting spin-offs from the whole transputer workstation
thing was the blossom graphics guard apparently the team that were responsible for that then would
later go on to work on the atari jaguar and properly kill atari this time but i think the atari
transputer workstation is probably the rarest of all atari computers because even the falcon
sold this thing and the stacey now with all that said would i like to get my hands on one yeah of
course i'd absolutely love to who wouldn't want to well i suppose people who only want to play
games on their atari because there were no games released for the atari transputer workstation
although you could actually get the mega st that was built in as the io processor to actually boot
toss and then run whatever game you wanted to on it it's just it's a heck of a lot of electricity
to run a standard st game if you've got all the way to the end i would like to say thank you very
much for watching i would also like to give a huge thank you to phyto one of cheffield's more
talented instrument makers musical instruments that is not scientific for giving me the transputer
system to set this whole video off on course if you enjoyed the video please click that thumbs up
button the thing that youtube's giving us to indicate that that's a fact and as ever please
feel free to chat away in the comments because it is quite nice to fully nerd out with all of you
and if you'd like to help the channel out please click the subscribe button because that way youtube
actually believes that it's worth telling people that the videos exist

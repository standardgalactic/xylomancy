The first breakthrough to incontrovertibly human-level AGI to a superintelligence is
months to years.
Will that be good or bad for humanity?
To me, these are less clear than what I think is the probable timeline.
Yo Shabak is known for his insights into consciousness and cognitive architectures,
and Ben Gortzel is the seminal figure in the world of artificial general intelligence and
known for his work on open cog.
Both are coming together here on theories of everything for a theologution.
A theologution is an advancement of knowledge, couched both in tenderness and regard, rather
than the usual tendency of debates, which is characterized by trying to be correct even
to the detriment of the other person, maybe even destructively, maybe even sardonically.
We have a foray in this episode into semantics and Pierce's signed theory.
This also extends into what it truly takes to build a conscious AGI.
An AGI is an artificial general intelligence, which mimics human-like intelligence.
Within the question lingers what about consciousness?
What differentiates mere computation from awareness?
Man, this was a fascinating discussion and there will definitely be a part two.
Recall the system here on TOE, which is if you have a question for any of the guests,
whether here or on a different podcast, you leave a comment with the word query and a colon,
and this way when I'm searching for the next part with the guest, I can just press
Ctrl F and I can find it easily in the YouTube studio back end.
And then I'll cite your name either aloud, verbally, or in the description.
To those of you who are new to this channel, my name is Kurt Jaimungal and this is Theories
of Everything, where we explore usually physics and mathematics related theories of everything.
How do you reconcile quantum mechanics with general relativity, for instance?
That's the standard archetype of the TOE, but also, more generally, where does consciousness
come in?
What role does it have to play in fundamental law?
Is fundamental, quote unquote, the correct philosophical framework to evaluate explanatory
frameworks for the universe and ourselves?
We've also spoken to Yosha three times before, one solo, that episode is linked in the description,
another time with John Revekey and Yosha Bach, and another time with Donald Hoffman and Yosha
Bach.
That was a legendary episode.
Also, Ben Gortzel has given a talk on this program, which was filmed at MindFest, which
was a conference about artificial intelligence and consciousness.
If you enjoy the topics of mathematics, physics, consciousness, AI, free will, and philosophy,
then consider subscribing to get notified.
Enjoy this episode with Yosha Bach and Ben Gortzel.
Welcome.
This is going to be so much fun.
Many, many people are very much looking forward to this, including me, including yourselves.
Welcome to the Theories of Everything podcast.
I appreciate you all coming back on.
Thank you for having us.
I always enjoyed discussing this, Ben.
It's always been fun, and I think the first time we are on a podcast together.
Yes, wonderful.
So let's bring some of those off-air discussions to the forefront.
How did you all meet?
We met first at the AGI conference in Memphis.
Ben had organized it, and I went there because I wanted to work on AI in the traditional
Minsky Incense, and it worked on a cognitive architecture.
My PI didn't really like it, so I paid my own way to this conference to publish it, and
I found like-minded people there, and foremost among them was Ben.
Great.
What's something, Ben, that you've changed your mind about in the past six months in
this field, this AGI field or AI field?
And then, Yosha, the question will go to you right afterward.
I don't think I've changed my mind about anything major related to AGI in the last six months,
but certainly seeing how well LLMs have worked over the last nine months or so has been quite
interesting.
I mean, it's not that they've worked a hundred times better than I thought they would or something,
but certainly just how far you can go by this sort of non-AGI system that mongers together
a huge amount of data from the web has been quite interesting to see, and it's revised
my opinion on how much of the global economy may be converted to AI even before we get
to AGI, right?
So it's shifted by thinking on that a bit, but not so much on fundamentally how do you
build an AGI, because I think these systems are somewhat off to the side of that, although
they may usefully serve as components of integrated AGI systems.
Well, I have some things that changed my mind are outside of the topic of AGI.
I thought a lot about the way in which psychology was conceptualized in Greece, for instance,
but I think that's maybe too far out here.
In terms of AI, I looked into some kind of new learning algorithms that fascinate me and
that are more brain-like and move a little bit beyond the perceptron, and I'm making
slow and steady progress in this area.
It doesn't feel like there is a big singular breakthrough that dramatically changed my thinking
in the last six months, but I feel that there is an area where we're beginning to understand
more and more things.
All right, let's get to some of the comparisons between you all, the contrasting ones.
It's my understanding that, Yosha, you have more of the mindset of everything is computation
or all is computation, and Ben, you believe there to be other categories.
I believe you refer to them as archetypal categories, or I may have done that, and I'm
unsure if this is a fair assessment, but please elucidate me.
I think that everything that we think happens in some kind of language, and perception also
happens in some kind of language, and the language cannot refer to anything outside
of itself, and in order to be semantically meaningful, a language cannot have contradictions.
It is possible to use a language where you haven't figured out how to resolve all the
contradictions, as long as you have some hope that there is a way to do it, but if a language
is self-contradictory, its terms don't mean anything, and the languages that work, that
we can use to describe anything, any kind of reality, and so on, turn out to be representations
that we can describe via state transitions, and the number of ways in which we can conceptualise
systems that are doing state transitions, for instance, we can think about whether they
are deterministic or indeterministic, whether they are linear or branching, and this allows
us to think of these representational languages as a taxonomy, but they all turn out to be
constructive, that means modern parlance computational. There was a branch of a mainstream
of mathematics was not constructive before GÃ¼rtel, that means language of mathematics
allowed to specify things that cannot be implemented, and computation is the part that
can be implemented. I think for something to be existent, it needs to be implemented in some form,
and that means we can describe it in some kind of constructive language.
That's basically the sword rhinos. I have to do with epistemology, and the epistemology determines
the metaphixes that I can have, because when I think about what reality is about, I need to do
this in a language in which my words mean things. Otherwise, what am I talking about? What am I
pointing at? When I'm pointing at, I'm pointing at the representation that is basically a mental
state that my own mind represents and projects into some kind of conceptual space, or some
kind of perceptual space that we might share with others. In all these cases, we have to
think about representations, and then I can ask myself, how is this representation implemented
in whatever substrate it is, and what does this signify about reality, and what is reality,
and what is significance, and all these terms turn out to be terms that, again, I need to describe
in a language that is constructive, that is computational. In this sense, I am a strong
computationalist, because I believe that if we try to use non-computational terms
to describe reality, and it's not just because we haven't gotten around to formalizing them yet,
but because we believe that we found something that is more than this, we are fundamentally
confused, and our words don't mean things. I tend to start from a different
perspective on all this philosophically. I mean, I think there's one minor technical
point I feel need to quibble with than what Joshua said, and then I'll try to outline my
point of view from a more fundamental perspective. I mean, the point I want to quibble with is,
it was stated that if a logic or language contains contradictions, it's meaningless. I mean,
there, of course, that's not true. There's a whole discipline of para-consistent logics,
which have contradictions in them and yet are not meaningless, and they're constructive para-consistent
logics, and you can actually use Curry-Howard transformations or operational semantics transformations
to map para-consistent logical formalisms into gradually-type programming languages and so forth.
So, I mean, contradictions are not necessarily fatal to having meaningful semantics to a logical
or computational framework, and this is something that's actually meaningful in my approach to AGI
on the technical level, which we may get into later, but I want to shift back to the foundation of
life, the universe, and everything here. So, I mean, I tend to be phenomenological in my
approach more so than starting from a model of reality, and these sorts of things become hard
to put into words and language, because once you project them into words and language, then,
yeah, you have a language because you're talking in language, right? But talking isn't all there is
to life. It isn't all there is to experience. And I think the philosopher Charles Perce gave one
fairly clear articulation of some of the points I want to make. You could just as well look at
Lao Tzu, or you could look at the Vedas, or the book Buddhist Logic by Strabovsky,
which gives similar perspectives from a different cultural background. So,
if you take Charles Perce's point of view, which at least is concise, he distinguishes a number of
metaphysical categories, and I don't follow him exactly, but let me start with him. So, he starts
with, first, by which he means qualia, like raw unanalyzable, just it's there, right? And
then he conceives second, by which he means reaction, like billiard ball bounces off each
other. It's just one thing is reacting to something else, right? And that this is how he's looking at
so the crux of classical physics, let's say, then by what Perce calls third, he means relationships.
So, one thing is relating to other things. And one of the insights that Charles Perce had writing
in the late 1800s was that once you can relate three things, you can relate four, five, six, ten,
like any large finite number of things, which was just a version of what's very standard now of
reducing a large number of logical relations to sort of triples or something, right? So,
Perce looked at first, second, and third as fundamental metaphysical categories, and he
invented quantifier logic as well with a for all and there exists in quantifier binding. So,
he, as Perce would look at it, computation and logic are in the realm of third. And if you're
looking in that metaphysical category of third, then you're saying, well, everything's a relationship.
On the other hand, if you're looking from within the metaphysical category of second,
you're looking at it like, well, everything's just reactions. If you're looking at it from
within the metaphysical category of first, then it's like, whoa, it's all just there, right? And
you could take any of those points of view and it's valid in itself. Now, you could extend beyond
Perce's categories, you could say, well, I'm going to be a Zen Buddhist and have a category of zero,
like the unanalyzable pearly void, right? Or you could go young in and say, okay, these are
numerical archetypes, one, two, three. But then we have the archetype of four, which is sort of
synergy and emergence. It's sort of mandalic. Yeah, so what I was saying is Perce
had these three metaphysical categories, which he viewed as just ontologically,
metaphysically distinct from each other. So what what Chalmers would call the hard problem of
consciousness in Perce's language is like, how do you collapse third to first? And Perce would be
just like, well, you don't, they're different, they're different categories, you're an idiot to
think that you can somehow collapse one to the other. So in that sense, he was a dualist,
although more than a dualist, because he had first, second, and third. Now, I think you could go
beyond that if you want, you could go Zen Buddhist and say, well, we have a zero category of the,
you know, the original ineffable self contradictory, pearly void. And then,
then you have the question of is zero really the same as one, which is like the Zen Buddhist
paradox of non dualism and so forth in a certain form. You can, you can also go above
Perce's three metaphysical categories. And you can say, okay, well, why not four fourth? Well,
to Carl Jung, four was the archetype of synergy and many mandalas were based on this four form
synergy. Why not five? Well, five, you have the four full synergy and then the birth of
something new out of it, right? So I, I can see that the perspective of third, the perspective
of computation is substantially where you want to focus if you're engineering an AGI system,
right? Because you're writing a program and the program, the program is a set of, of
logical relationships. The program is written in a language. So I don't, I don't have any
disagreement that this is like the focal point when you're engineering an AGI system. But if I want
to intuitively conceptualize the AGI's experience, I don't feel a need to like try to reduce the whole
metaphysical hierarchy into, into third just because the program code lives there. And I mean,
this is, this is sort of a, it's not so much about AI or mathematical or computational
formalism. I mean, these are just different philosophical perspectives, which it becomes
arduous to talk about because natural language terms are, are imprecise and ambiguous and,
and slippery. And you could end up spending a career trying to articulate what is really
meant by relationship or something. All right, Yosha. I think it comes down to the
vein which our thinking works and what we think thinking is. You could have one approach that is
radically trying to build things from first principles. And when we learn how to write
computer programs, this is what we might be doing. When I started programming, I had a Commodore 64.
I was a kid. I didn't know how to draw a line. Commodore 64's basic doesn't have a command to
draw a line. What you need to draw a line on the Commodore 64 is you need to learn a particular
language. And this language in this case is basic. You can also learn a similar directly.
But it's not hard to see how assembler maps to the machine code of the computer. And the machine
code works in such a way that you have a short sequence of bits organized into groups of eight
byte. And these bytes are interpreted as commands by the computer. They're basically
like switches or train tracks. You could imagine every bit determines whether a train
track goes to the left or to the right. And after you go through eight switches,
you have 256 terminals where you can end. So if you have two options to switch left or right.
And in each of these terminals, you have a circuit, some kind of mechanism that performs a small
change in the computer. And these changes are chosen in such a way that you can build arbitrary
programs from them. And when you want to make a line, you need to learn a few of these constructs
that you use to manipulate the computer. And first off on the code or 64, you need to write
a value in a certain address of that corresponds to a function on the video chip of the computer.
And it makes the video chip forget how to draw characters on screen and instead interpret a
part of the memory of the computer as pixels that are to be displayed on the screen. And then you
need to tell it which address and working memory you want to start by writing two values into the
graphic chip, which are in code for a 16 bit address in the computer. And then you can find
the bits in your working memory that correspond to pixels on the screen. And then you need to make
a loop that addresses them all in order. And then you can draw a line. And once I understood this,
basically had a mapping from an algebraic equation into automata that was this was the computer is
doing. It's an automaton at the lowest level that is performing geometry. And once you can draw
lines, you figure out also how to draw curved shapes. And then you can draw 3D shapes, and you
can easily derive how to make that. And I did these things as a kid. And then I thought the
mathematicians have some kind of advanced way, some kind of way in which I deeply understand
what geometry is in ways that goes far beyond what I am doing. And mathematics teachers had the
same belief. They basically were gesturing at some kind of mythological mountain of mathematics,
whether with some deep and scrutable knowledge on how to do continuous geometry, for instance.
And it was much, much later that I started to look at this mountain and realized that it was
doing the same thing that I did on my Commodore 64, just with Greek notation. And there's a different
tradition behind it, but it was basically the same code that I have been using. And when I was
confronted with notions of space and continuous space, and many other things, I was confronted
with a conundrum. I thought I can do this in my computer that looks like it, but there can be no
actual space because I don't know how to construct it. I cannot make something that is truly continuous.
And they also don't observe anything in reality around me that is fundamentally different from
what I can observe in my computer to the degree that I can understand and implement it.
So how does this other stuff work? And so imagine somebody has an idea of how to do
something in a way that is fundamentally different from what could be in principle done in computers.
And I asked them how this is working. It goes into hand waving. And then you point at some
proofs that have been made that show that the particular hand waving that they hope to get
to work does not pan out. And then I hope there is some other solution to make that happen,
because they have the strong intuition. And I asked, where does this intuition come from?
How did it actually get into your brain? And then you look at how does the brain work?
There is firing between neurons. There is interaction with sensory patterns on the
systemic interface to the universe. How were they able to make inferences that go beyond
the inferences that I can make? But this is one way of looking at it. And then on the other end
of the spectrum, this one is more or less in the middle, there is degraded form of epistemology,
which is you just make noises. And if other people that you get away with it, you're fine.
And so you just make sort of grunts and hand waving movements, and you try to point at things,
and you don't care about anything of it works. And if a large enough group of high status people
is nodding, you're good. And this epistemology of what you can get away with doesn't look very
appealing to me because people are very good at being wrong in groups.
Yeah. I mean, saying that the only thing there is is language, because the only thing
we can talk about in language is language. I mean, this is sort of
tautologists in a way, right?
No, no, that's not quite what I'm saying. I'm not saying the only thing there is language,
of course. Language is just a representation. It's a way to talk about things and to think about
things and to model things. And obviously, not everything is a model, just everything that
they can refer to as a model. And so there is that, that, that, that, I mean,
you can't know that, right? You can, you can hypothesize that, but you can't, you can't
know that. And this gets into, I guess it depends what you like about it.
I cannot know anything that I cannot express.
I can know many things that I can't express in language, but I mean, that's, that's just,
it's a, I guess a different flavor of,
of knowing?
Subjective experience. I mean, so take, take what Martin Buber called an eye-thou experience,
right? I mean, if you're, if you're staring into someone's eyes and you have a deep experience
that you're, you're seeing that person, you're just sharing a shared space of experience and,
and, and being, I mean, in that moment, that is something you both know you're not going to be
able to communicate fully in, in language and it's experientially there. Now, Buber wrote a
bunch of words about it, right? And those words communicate something special to me and to some
other people. But of course, someone else reads the words that he wrote and says, well, you,
you are merely summarizing some collection of firings of, of neurons in, in, in, in your brain
and in some strange way, deluding yourself that, that, that, that is something else, right? So,
I mean, that's, I mean, I think from within the domain of computation and science, you can
neither prove nor disprove that there exists something beyond, beyond the range of computation
and, and science. And if you, if you look at scientific data, I mean, the whole
compendium of scientific data ever gathered by the whole human race is one large finite bit set,
basically. I mean, it's a large, it's a large set of data points with, with finite precision to each,
each piece of data. So, I mean, it may not even be that huge of a computer file if you try to,
if you try to assemble it all, like all the, all the scientific experiments
ever done and agreed by some community of scientists. So you, so you've got this big
finite bit set, right? So, and then science in a way is trying to come up with, you know,
concise, reasonable-looking, culturally acceptable explanations for this huge finite bit set that
can be used to predict outcomes of other experiments and which finite collection of bits will,
will emerge from those other experiments in a way that's accepted by a certain,
by a certain community. Now, that, that's, that's a certain process. It's a thing to do. It has to
do with finite bit sets and computational models for, for producing finite bits, right?
And the finite sets of bits. And that's, that's great. That nothing within that process
is going to tell you that that's all there is to, to, to the universe or that that isn't all
there is to the universe. I mean, it's a valuable, important thing. Now, to me, as an experiencing
mind, I feel like there's a lot of steps I have to get to the point where I even know what a finite
bit set is or where I even know like what, what a community of people validating that finite
bit set is really is or what, or what a, what a programming language is. So I mean, I keep coming
back to my phenomenal hauntal experience. Like first, there's this, this field of nothingness
or contradictory nothingness that's just floating there, then some indescribable forms flicker
and emerge, emerge out of this void. And then you get some complex pattern of forms there,
which constitutes a notion of, you know, a bit set or an experiment or a computation. And from,
from this phenomenological view, by the time you get to this business of computing and languages,
you're already dealing with a fairly complex like body of self-organizing forms and distinctions that,
that, that popped out of the void. And then, then this conglomeration of forms that in some
enough of a way has emerged out of the void is selling no, I am, I am everything. The only
thing that exists in a fundamental sense is what is inside me. And I mean, you can't, if you're
inside that thing, you can't, you can't refute or really demonstrate that. But again, from,
from an AGI view, it's all fine. Because when we talk about building an AGI,
what we're talking about is precisely engineering a set of computational processes. Like I don't,
I don't think you need to do, like you don't need some special first tronium to drop into
your computer to give, to give the, the AGI, the, the fundamental quality of, of experience or
something. The two points now, that means, briefly interject, so we don't forget. Okay,
let's just allow Yosha to speak, because there are quite a few threads and some may be dropped.
Also, it appears as if you're using different definitions of knowledge. If we use this traditional
philosophical notion of justified true belief, it means that I have to use knowledge in a context
where I can hope to have a notion of what's true. So for instance, when I look at your face and
experience a deep connection with you, and I report, I know we have this deep connection.
I'm not using the word no in the same sense. What I am describing is an observation. I'm observing
that I'm seem to be looking at a face and then observing that I have the experience of having
a deep connection. And I think I can to hope to report on this truthfully. But I don't know
whether it's true that we have that deep connection. But I cannot actually know this. I can make some
experiments to show how aligned we are and how connected we are and so on, to say this perception
or this imagination has some veracity. But here I'm referring to a set of patterns,
right? There are dynamic patterns that I perceive and then there is stuff that I can reflect on
and disassemble and talk about and convey and model. And this is a distinct category in the sense.
It's not in contradiction necessarily what you're saying. It's just using the word knowing in
different ways is implied here because I can relate the pattern to you that I'm observing or that I
think I'm observing. But this is a statement about my mental state. It's not a statement about
something in reality about the world. And to make statements about the world, I probably need to go
beyond perception. The second aspect that we are now getting to is when you say that reality and minds
might have properties that are not computational yet your AGI is entirely computational and doesn't
need any kind of first principles, wonder machine built into it that goes beyond what we can construct
from automata. Are you establishing that AGI's artificial general intelligence with potentially
superhuman capabilities are going to still lagging behind what your mind is capable of?
No, not at all. I just think the other aspects are there anyway and you don't need to build them.
So you're going to make the non-computational parts of reality using computation?
No, you don't have to make them. They're already there. I mean, if you just take a more
simple point of view where you're thinking about first and third, and Perce was basically a
pentpsychist, right? So he believed that matter is mind hidebound with habit. As he said, he believed
that every little particle had its own spark or element of consciousness and awareness in it.
So I mean, from that standpoint, I mean, this kind of bubbly water that I'm holding up has its own
variety of conscious awareness to it, which has different properties in the conscious
awareness in my brain or yours. So from that standpoint, if I build an AGI program, it has
something around the same patterns and structures and dynamics as a human brain and as the sort of
computational aspect of the human mind, from that standpoint, then most likely the same sort of
firstness, the same species of subjective awareness will be associated with that AGI machine that
you've built. But it's not that you need to construct it. I mean, any more than you need to
explicitly construct like the positioning in time of your computer or something like you,
you build something, it's already there in time. You don't have to build time. I mean, you just
build it and it's there in time. You didn't need a theory of time. And you didn't need to screw
together moment t to moment t plus one either. The perspective is more that awareness is ambient
and it's there. You don't need to build it. Of course, there's subtlety that different sorts of
constructions may have different sorts of awareness associated with them. And there's philosophical
subtlety in how you treat different kinds of first when you're operating in a level where
relationship doesn't doesn't exist yet, right? Like, in what sense is the experience of red
different from the experience of blue, even though articulating that difference already brings you
into the realm of third, right? And this gets back to it gets back to non duality and a bunch of
stuff that Perce wrote hundreds of pages about, right? I haven't read these pages, so I don't
really understand them. I think it's conceivable that particles are conscious or intelligent,
but this would require that they have or imply they have more complicated causal structure
than the computer that I'm currently using to communicate with you. And by that's possible,
I seem it seems to me that it's a simpler ways in which particles could be constructed
to do the things that they are doing. It seems to me sufficient that there are
basically emergent error correcting codes on the quantum substrate and would just emerge over
the stuff that remains statistically predictable in branching multiverse. I don't need to be
conscious to do anything like that. Maybe if we do more advanced physics, we figure out
or no, this error correcting code that just emerges similar to a vortex emerges in the
bathtub when you move your hand and only the vortex remains and everything else dissipates
in the wave background that you are producing and the chaos and turbulences. It could be
to me possible that particles are like this, they're a little stable twirls what you see
that they've stabilized after the non stable stuff is dissipating. And to achieve this,
I don't think that I need to posit that they are conscious. If it could be that I figure out, oh,
no, this is not sufficient. We need way more complicated mass and structure to make this happen.
So they need some kind of coherence improving operator that is self-reflexive and eventually
leads to the structure. Then I would say, yeah, maybe this is a theory that we should seriously
entertain. Until then I'm undecided and Occam's razor says I can construct what I observe at this
level of elementary particles, atoms and so on by assuming that they don't have any of the
conscious functionality that exists in my own mind. And the other way would be you can redefine
the notion of consciousness into some principle of self organization that is super basic. But this
would redefine consciousness into something else because there's a lot of self organizing stuff
that does not fall into the same category that an anesthesiologist makes go away when he gives
you an anesthetic. And to me consciousness is that thing which seems to be suspended
when you get an anesthetic and that stops you from learning and currently interacting with the
world. I mean, that at least gives me a chance to repeat once again my favorite quote from our
Bill Clinton, former US president, which is that all depends what the meaning of is is.
That's interesting, Ben, because the first time, Yosha, I don't know if you remember,
I brought up that quote, I don't remember the context, but I said, yeah, that also depends
on what is is. The question is, what do you mean by is, right? So visit the story.
It sounds like it sounds like Bill Clinton. It depends upon what the meaning of the word is.
Yeah, the previous podcast with Yosha Bach as a solo episode is linked in the description,
as well as the previous podcast with Ben Solo, as well as Yosha Bach with Donald Hoffman and
Yosha Bach with John Verveke. Every link as usual to every source mentioned in this podcast on every
single toe podcast is in the description. Yeah, I mean, a couple reactions there. And I feel I may
have lost something in the in the in the buffering process there. But I think that
let me see. So that
first of all, about causality and firstness or a raw experience, I mean, almost by
definition of how first sets up his metaphysical categories. I mean, a firstness doesn't
cause anything. So you're not going to come up with a case where I need to assume that this
particle has experience or else I can't explain why this experiment came out this way. I mean,
I mean, that that that would be a sort of category error in in person's perspective. So if if the
only if the only sort of thing you're willing to attribute existence to is something which has
a demonstrable causal impact on some experiment, then by that assumption, I mean, that that's
essentially equivalent to the perspective you're putting forth that everything is is is computation
yet and and purse purse didn't think other categories besides third were of that nature.
There's also a just shallow semantic matter tied into this, which is the word consciousness is just
highly ambiguous. So I mean, Yosha, you seem to just assume that human light consciousness
is consciousness. And I don't really care if people want to reserve the word consciousness
for that. Then we just need some other word for the sort of ambient awareness and everything
in the universe, right? So there's lengthy debates among academics on like, okay, do we
say a particle is conscious? Or do we say it's proto conscious, right? So then you can say, okay,
we have proto consciousness versus consciousness, or we have like, raw consciousness versus
reflexive consciousness or human like consciousness. And I mean, I spent a while reading all the stuff
I wrote some things about it in the end, I'm just like, this is a, this is a game that overly
intellectual people are planning to to entertain themselves. And it doesn't really matter. Like,
I've got, I've got my experience of the universe. I know what I need to do to build AGI systems and
arguing about which words to associate with different flavors and levels of experience
is you just kind of kind of running around in circles.
Conceptually, it's an important question is this camera that is currently filming my face
and representing it and then relaying it to you aware of what it's doing? And is this just
a matter of degree with respect to my consciousness? Is this representing some kind of ambient awareness
of the universe and are particles doing the same thing? And so on. These are questions that I think
I can answer. And if I don't answer them, my thinking will become so mushy that my thoughts are
meaningless and I will not be able to construct anything. I mean, if the only kind of answer that
you're interested in are rigorous scientific answers, then you have your answer by assumption,
right? And I mean, answering questions by assumption is fine. It's practical. It saves our time.
But I mean, I don't, I mean, you will, I think that's what you're doing. I don't see how you're
not just trying to answer by assumption. You posit that elementary particles are conscious.
Then I point out that we normally reserve the word consciousness for something that is
really interesting and fascinating and shocking to us. And that it would be more shocking if
it projected into the elementary particles. And then you say, okay, I just mean ambient awareness.
Now we have to disassemble what ambient awareness actually means. What is a barrier? What does this
awareness come down to? And I think that you're pointing at something that I don't want to dismiss.
I want to take you seriously here. So there maybe there is something to what you are saying,
but you're not getting away with simply waving at it and saying that this is sufficient to
explain my experience and I'm no longer interested to make my words mean things.
Because they cannot communicate otherwise. We will not be able to see what idea you actually
have, what idea you're trying to convey, and how it relates to ideas that other people might have.
And I'm not pointing at the institutions of science here which don't agree on what consciousness is
and for the most part don't care. This is more a philosophical question here. And it's also one
that is an existential question that we have to negotiate among the two or the three of us.
Yeah, let's just let Ben respond. I mean, I guess
rightly or wrongly as a human being, I've gotten bored with that question in the same way that
like I couldn't say it's worthless. At some point, maybe you could convince someone. I know
people who were convinced by materials they read on the internet to give up on
on Mormonism or Scientology, right? So I can't say it's worthless to debate these points with
people who are heavily attached to an ideology, I think is silly. On the other hand, I personally
just tend to get bored with repeated debates that go over the same points over and over again.
If I had an infinite number of clones, then I wouldn't. And this, I guess one of the things
that I get worn out with is people claiming my definition of this English word
is the right one and your definition is the wrong one. And I guess you weren't really doing that,
Joshua, but it just gave me a traumatic memory. I'm sorry for triggering you here.
I'm not fighting about words. I don't care which words you're using. But so when I think about
an experience of what it's like and associate that with consciousness or the system that is able
to create a now, the perception of a now, then I'm talking about a particular phenomenon that
I have in mind and I would like to recreate if I can. And I want to understand how it works.
And so for me, the question of whether I project this property into arbitrary parts of what I
consider to be reality is important. I understand if it's not interesting to you and I won't force
you into any discussion that would make you drop out of your particular Mormonism. I'm happy
because you're being a Mormon. Let me tell you how I'm looking at
anesthesia, which is a concrete specific example that's not that trivial, right? Because I've only
been under anesthesia once, which I have wisdom teeth removed. So it wasn't that bad, but other
people have had far more traumatic things than when they're under anesthesia. And there's always
the nagging fear that like, since we don't really know how anesthesia works in any fundamental depth
and also don't really know how the brain generates our usual everyday states of consciousness in
enough that it's always possible that while you're under anesthesia, you're actually in some sense
some variant of you is feeling that knife slicing through you and maybe just the memories being
cut off, right? And then once you come back, you don't remember it. But then that might not be
true. But then you have to ask, well, okay, say then, you know, while my jaw is being cut open
by that knife, does the jaw feel it, right? Like, does the jaw hurt whilst being sliced up by the
knife? Like is the jaw going, ah, well, on the other hand, you know, the global workspace in your
brain, like the reflective theater of human-like consciousness in your brain may well be disabled
by the anesthetic. So the way I personally look at that is I suspect under anesthesia,
your sort of reflective theater of consciousness is probably disabled by that anesthetic. I'm not
100% sure. But I think it's probably disabled, which means there's probably not like a version of
Ben going, ah, wow, this really hurts. This really hurts. And then forgetting it afterwards. So,
I mean, maybe you could do that just like disabled memory recording. But I don't think
that's what's happening. On the other hand, I think the jaw is having its own
experience of being sawed open. Now that while you're getting that wisdom tooth removed under
general anesthesia, no, I think it's, it's not the same sort of experience exactly as the reflective
theater of consciousness that knows itself as Ben, as Ben Goetzel is having. Like the Ben Goetzel can
conceptualize that it's, that it's experiencing pain. It can go like, ow,
that really hurts. And then the thinking that's saying that really hurts is different than that
which really hurts, right? There's many levels there. But I do think there's some sort of raw
feeling that the jaw itself is having, like even if it's not connected to that reflective theater
of awareness in the brain. Now the jaw is biological cells. So some people would agree
that those biological cells have experience, but they would think like, you know, a brick
when you smash it with an axe doesn't. But I suspect the brick also has that some elementary
feeling. So I mean, I think, I think, I think it is like something to be a brick that's smashed
in half by an axe. On the other hand, that it's not like something that can reflect on what it is
to be a brick smashed in half by an axe, right? So I mean, that's, is how I think about it. But
again, I don't know how to make that science because I can't ask my jaw what it feels like
because my jaw doesn't doesn't speak language. And even if I was able to like wire my brain
into the jaw of someone else who's going through wisdom tooth removal under anesthesia, like,
I might say like through that wire, I can feel by an eye that experience like I can feel the pain
of that jaw being sliced open. But I mean, you can tell me I'm just hallucinating that my own
brain is like improvising that based on the based on the signals that I'm getting. And I'm not,
I'm not sure how you really pin that down in an experiment, right? Let me try. So there have been
experiments about anesthesia. And I'm not an expert on anesthesiology. So I asked everybody
for forgiveness if I get things wrong. But there have been different different anesthetics. And
some of them work in very different ways. And there is indeed a technique that basically works
by giving people a muscle relaxant so they cannot move and giving them something that inhibits
deformation of long term memory. So they cannot remember what happened to them in that state.
And there have been experiments that surgeons did where they were applying a tourniquet to an arm
of the patient so the muscle relaxant didn't get into the arm and they could still use the arm.
And then in the middle of the surgery, they asked the person that was there and I fully relaxed
and in comunicado to raise their arm, raise their hand if they were conscious and aware of what is
happening to them. And they did. And when they were asked if they had unbearable pain, they also
raised their hand. And after the surgery, they didn't forget they had forgotten about it.
But I also noticed the same thing on surgery. I had a number of big surgeries in my life and
there is a difference between different types of surgery. There is one type of surgery where I wake
up and feel much more terrified and violated than I do before the surgery. And I don't know why,
because I have no memory of what happened. Also, my memory formation is impaired. So when I am in
an ER and ask people how it went, I might have that same conversation multiple times word for word,
because I don't remember what they said or that I asked them. There is another type of anesthesia
and I observed this, for instance, in one of my children where the child wakes up and says,
oh, the anesthesia didn't work. And it was an anesthesia with gas. So the child choked on the
gas and you see your child lying there completely relaxed and sleeping and then waking up and
starting to choke and then telling you the anesthesia didn't work. There is a complete gap
of eight hours in the memory of that child in which the mental state was somehow preserved.
Subjectively, the child felt a complete continuation and then was looking around and
reasoning that the room was completely different. Time was very different, led to confusion and
reorientation. So I would suspect that in the first case, it is reasonable to assume or to
hypothesize at least that consciousness was present, but we don't recall what happened in
this conscious state, whereas in the second one, there was a complete gap in the conscious experience
and consciousness resumed after that gap. And we can test this. There are ways, regardless of
whether we agree with this particular thing or whether we think anesthesia is important.
In principle, we can perform such experiments and ask such questions. And then on another level,
when we talk about our own consciousness, there is certain behavior that is associated with
consciousness that makes it interesting. Everything, I guess, only becomes interesting due to some
behavior, even if the behavior is entirely internal. If you are just introspectively conscious,
it still matters if I care about you. And so this is a certain type of behavior that we still care
about. For instance, if I asked myself, is my iPhone conscious? The question is, what kind of
behavior of the iPhone corresponds to that? And I suspect if I turn off my iPhone or smash it,
it does not mean anything to the iPhone. There is no what it's likeness of being
smashed for the iPhone. There could be a different layer where this is happening,
but it's not the layer of the iPhone. Now, let's get to a slightly different point,
this question of whether your jaw knows anything about being hurt. So imagine that there is surgery
on your jaw like with your wisdom teeth. Is there something going on that is outside of your brain
that is processing information in such a way that your jaw could become sentient in a sense that it
knows what it is and how it relates to reality, at least to some degree and level. And I cannot
rule this out, but the cells, the cells can process information, they can send messages to
the neighbors and the patterns of their activation, who knows what kind of programs they can compute.
But here we have a means and a motive. The means and motive here are it would be possible for the
cells to exchange conditional matrices to perform arbitrary computations and build representations
about what's going on. And the motive would be that it's conceivable that this is a very useful
thing for biological tissues to have in general. And so if they evolve for long enough, and it is in
the realm of evolvability that they perform interactions, each other that lead to representations
of who they are and what they're doing, even though they're much slower than what's happening in our
brain and decoupled from our brain in such a way that we cannot talk to our jaw, it's the
conceivable. I wouldn't rule this out. It's much harder for me to assume the same thing for elementary
particles because I don't see them having this functionality that cells have. Cells are so much
more complicated that just fits in that they would be able to do this. And so I would make a
distinction. I would not rule out that multi-cellular organisms without brains could be conscious,
even at different timescales than us requiring very different measuring mechanisms because
their signal processing is probably much slower and it takes longer for them to become coherent
at scale because it takes so long for signals to go back and forth if you don't have nerves.
But I don't see the same thing happening for elementary particles. Yeah, I don't rule that out
again. But you would have to show me some kind of mechanism. I mean, if you're going to look at it
that way, which isn't the only way that I would look at it, but if you're going to look at it that
way, I don't see why you wouldn't say the various elementary particles, which are really distributed
like amplitude distributions. I don't know why you wouldn't say these various interacting amplitude
distributions are they're exchanging quantum information with a motivation to achieve stationary
given their context, right? I mean, you could tell that story. That's the story that physics
tells you. They're swapping information back and forth, trying to make the action stationary.
Yes, but for the most part, they don't form brains. They also do form brains. So elementary
particles can become conscious in the sense that they can form brains, nervous system,
maybe equivalent information. Well, no, I just feel like you're, you're privileging
a certain level and complexity of organization, because it happens to be ours. And I mean, we
have a certain level and complexity of organization and end of consciousness. And I mean, a cell in
my jaw has a lower one, a brick has a lower one, elementary particles and lower one, the future
AGI may have a much higher one from whose perspective our consciousness appears more
analogous to a brick than to itself. I wouldn't say lower or higher. I would say that if my jaw
is conscious, there are far less cells involved in my brain and the interaction between them is
slower. So if it's conscious, it's probably more at the level of, say, a fly than a level of a brain.
And it's probably going to be as fast as a tree and in the way in which it computes
rather than as fast as your brain. And that is, I don't think that's something that is assigning
some undue privilege to it. I'm just observing a certain kind of behavior. And then I look for
the means and motive behind that behavior. And then I try to construct causal structure,
and I might get it wrong. There's things that might be missing, but it's certainly not because
I have some kind of speciesism that assigns higher consciousness to myself because it's me.
All right. Yeah, I know what your motivations are. Kurt, I have a, I have a higher level,
I have a higher level comment, which is, we're like an hour through this conversation,
probably halfway through. I feel like the philosophy, the hard problem of consciousness,
the hard problem of consciousness is an endless rabbit hole. It's not, it's not an uninteresting
one. I think, I think it's also not, it's not the topical with Josh and I have the most original
things to say. Like I think each, each of our perspectives here are held by many other.
I might interject a little bit. What I'm, one of our most interesting disagreements is in Ben
being a pensai chist and me not knowing how to formalize pensai chism in a way that makes
it different from box standard functionalism. And so I do value this discussion and don't
think it's useless, but I basically feel that on almost everything else, we mostly agree except for
crypto. Okay. Yeah, to me, that's almost a Zen thing. It's like, I don't know how to formalize
the notion that there are things beyond all formalization. So fascinating to look at your frozen
interlocutor. I don't know if you can still hear us. You can rest in that forever.
Yeah. Yeah. All right. Again, comparing your views, it seems like
Yosha, you're more of the mind that LLMs or deep neural nets are on or significant
step toward AGI, maybe even sufficient with enough complexity. And Ben, I think that you
disagree. Yeah, I think, I think most issues in, in terms of the relationship with LLMs and AGI,
we actually probably agree on quite, quite well. But I mean, obviously, large language models are
in amazing technology, like from an AI application point of view, they can do all sorts of fantastic
and tremendous things. I mean, I, I, it sort of blew my mind how smart GPT-4 is. It's not the
first time my mind has been blown by an AI technology. I mean, my mind was blown by computer
algebra systems when they first came out. And you could like do integral calculus with arbitrary
complexity. And, you know, when deep blue beat, beat chess with just game trees, I'm like, whoa.
So I mean, I don't, I don't think it's the only amazing thing to happen in the history of AI,
but it's an amazing thing. Like it's a big breakthrough. It's super cool. I think that
if deployed properly, this sort of technology could do significant majority of jobs that humans
are now doing on the planet, which is as big economic and social implications. I, I think that
the way these algorithms are representing knowledge internally is not what you really need to make a
full on human level AI system. So I mean, when you look at what's going on
inside the transformer neural network, I mean, it's not quite just a big weighted
hash table of particulars, but to me, it does not represent abstractions in a sufficiently
flexibly manipulable way to do the most interesting things that the human mind does.
And this is a subtle thing to pinpoint in that say something like a fellow GPT
does represent abstractions. It's learning an emergent representation of the, of where the
board is, but of the different, it's learning an emergent representation of features like
black squares on this particular board position or white squares on this particular board position.
So examples like that show that LLMs can in fact learn abstract representations and can
manipulate them in some way, but it's very limited in that regard. I mean, in that case,
it's seen a shitload of a fellow games. And that's a quite simple thing to represent. So I,
I think when you look at how the neural net is learning, how the attention mechanism is working,
how it's representing stuff, I mean, it's just not representing it a hierarchy of subtle abstractions
the way a human mind is. And I mean, the subtler question is what functions you can get by
glomming an LLM together with other components in a hybrid architecture with the LLM at the
center. So suppose you give a working memory, suppose you give an episodic memory, suppose
you're declared a long term memory graph, and you have all these things integrate into the prompts
and integrate into fine tuning of an LLM. Well, then, then you have something that in principle,
it's turning complete and it could probably do a lot of quite amazing things. I still think if
the hub of that system is an LLM with its impaired and limited ability for representing and manipulating
abstract knowledge, I think it's not going to do the most interesting kinds of thinking that,
that, that people can do. And examples of things I think you fundamentally can't do with that kind
of architecture or say, invent a new branch of mathematics, you know, invent a completely new,
let's say radically new genre of music, you know, figure out a new variety of business strategy,
like say Amazon or Google did, that's quite different than things, things that have been
done before. All these things involve a leap into the unknown beyond the training data to an extent
that I think you're not going to get with the way that LLMs are, are, are representing knowledge.
Now, I, I do think LLMs are powerful as tools to create AGI. So for example, as one sub project in,
in my own AGI project, we're using LLMs to map English sentences into computer programs or
try to get logic expressions, right? No, I mean, that's, that's super cool. I mean, then you've
got the web in, in a, in the form of a huge collection of logic expressions, you can use a
logic engine to connect everything on the web with what's in databases with stuff coming in from
sensors and so on. So I mean, that, that's by no means the only way to leverage LLMs toward AGI,
not, not at all, but it's one, one interesting way to leverage LLMs toward AGI. You can even
ask the LLM to come up with an argument and then use that as a sort of guide for a theorem prover
and coming up with a more rigorous version of that argument, right? So I do think there are many ways
more than I could describe right now of LLMs to be used to help guide and serve as a component of
AGI systems. But I think if you're going to make a hybrid AGI system with full human level general
intelligence and with an LLM as a component, something besides an LLM has got to be playing
a very key and central role in knowledge representation and, and reasoning basically. And
this, this ties in then with LLMs not being motivated agents. So you could wrap a sort of
motivated agent infrastructure around an LLM, right? You could wrap Josh's, oh,
Josh's Psi model, Micro Psi model in some way around an LLM if you, if you wanted to, and you
could make it. I mean, people tried dumb things like that with auto GPT and so-called baby AGI
and so forth. So I mean, on the other hand, I think if you wrap a motivated agent architecture around
an LLM with its impaired capability for making flexibly manipulable abstract representations,
I think you will not get something that builds a model of self and the other with the sophistication
that humans have in their, in their reflective consciousness, you know, and, and I think that
having a sophisticated abstract model of self and other in our reflective consciousness,
the kind of consciousness that we have but a brick or a, or a jaw cell doesn't, right?
Without that abstraction in our model of reflective consciousness,
tied in with our motivated agent architecture, then that's, that's part of why you're not
going to get the fundamental creativity and inventing new, new genres of music or new branches of
mathematics or new business strategies. Like in humans, we do this amazing novel stuff,
which is what drives culture forward. We do this by our capability for flexibly manipulable
abstraction, tied in with our motivated agent architecture. And I don't, I don't see how you
get that with LLMs as the central hub of your hybrid AGI system. But I do think you could get
that with an AGI system that has, oh, something like open cogs, Adam space and reasoning system
as a central hub with an LLM as a subsidiary component. But I don't think open cog is the
only way either. I mean, obviously, you could make a biologically realistic brain simulation
that had human level AGI. I just think then the LLM like structures and dynamics within
that biologically realistic brain system would just be a subset of what it does.
You know, there'd be quite different stuff in the cortex. So yeah, that's, that's, that's
not quite the capsule summary, but a lengthy, lengthy-ish overview of my perspective on this.
Okay, great. Yosha, I know there was a slew there. If you can pick up some of the pieces and
respond, but also at the same time, there's emergent properties of LLMs. So for instance,
reflection is apparently some emergent property. There are, there are, but they're limited. I mean,
and that does make it subtle because you, you can't say they don't emerge knowledge representation.
They do. And a fellow GPT is one very simple example that there are others, right? So
there is emergent knowledge representation in them, but it's very simplistic and limited.
It doesn't pop up effectively from, from in-context learning, for example. But anyway, this,
this would dig us very deep into, into current LLMs, right? So.
Yeah. So is there some in-principle reason why you think that a branch of mathematics,
Yosha, can't be invented by an LLM with sufficient parameters or data?
I am too stupid to, to decide this question. So basically what I can offer is a few perspectives
that I see when I look at the LLM. Personally, I am quite agnostic with respect to its abilities.
And at some level, it's an autocomplete algorithm that is trying to predict tokens from previous
tokens. And if you look at what the LLM is doing, it's not a model of the brain. It's,
it's a model of what people say on the internet. And it is discovering a structure to represent
that quite efficiently as an embedding space that has lots, lots of dimensions. You can imagine
that each of these dimensions is a function. And the parameters of this function are the
positions on this dimension that you can have. And they all interact with each other to together
create some point in a high dimensional space that this could be an idea or a mental state or a
complex thought. And at the lowest level, when you look at how it works, it's translating these
tokens, the translation of linguistic symbols into some kind of representation that could be,
for instance, a room with people inside and stuff happening in this room. And then it maps it back
into tokens at some level. There has been recently a paper out of a group led by Max Techmark that
looked at the Lambda model and discovered that it does indeed contain a map of the world and
directly encoded in its structure based on the neighborhood relationships between places in
the world that it represents. And it is basically there's an isomorphic structure between what the
LLM is representing and all the stuff that we are representing. I am not sure if I in my entire life
invented a new dimension in this embedding space of the human mind that is represented on the internet.
If I think about all the thoughts that have been made into books and then encoded in some form and
became available as training data to the LLM, we figure out that they're, depending on how you
count, if you tend to a few hundred thousand dimensions of meaning. And I think it's very
difficult to add a new dimension or also to significantly extend the range of those dimensions,
but we can make new combinations of what's happening in that space.
Of course, it's not a limit that these things are limited to the dimension that they already
discovered. Of course, we can set them up in such a way that they can confabulate more dimensions
and we could also set them up in such a way that they could go and verify whether there's a good
idea to make this dimension by making tests, by giving the LLM the ability to use plugins,
to write its own code, to use a compiler, to use cameras, to use sensors, to use actuators,
to make experiments in the world. It's not limited to what we currently let the LLM do.
But in the present form, what the transformer algorithm is doing, it tries to find the most
likely token. And so, for instance, if you play a game with it and it makes mistakes in this game,
then it will probably give you worse moves after making these mistakes because it now assumes
that it's playing a bad person. Somebody was really bad at this game. And it doesn't know
what kind of thing it's supposed to play because it can represent all sorts of state transitions.
It's an interesting way of looking at it that we are trying to find the best possible token
versus the LLM trying to find the most likely token next. Of course, we can preface the LLM by
putting into the prompt that this is a simulation of a mind that is only going to look for the
best token and it's trying to approximate this one. So it's not directly a counterargument.
It's not even asking us to significantly change the loss function. Maybe we can get much better
results. We probably can get much better results if we make changes in the way in which we do training
on inference using the LLM. But this by itself is also nothing that we can prove without making
extensive experiments. And at the moment, it's unknown. I realized that the people were being
optimistic. I just want to pose a thought experiment. So this is about music rather than
natural language. But I mean, we know there's music gen, there's similar networks applied to
music. So suppose you had taken LLM like music gen or Google LLM or the next generations and
traded on all music recorded or played by humanity up to the year 1900. Is it going to invent
the sort of music made by Mahavishnu Orchestra or even do even do Kellington? I mean,
would you say that has no new dimensions because jazz combines elements of West African drumming
and Western classical music? I think that's a level of invention LLMs are not going to do.
You said it. You said it's combining elements from this and from that. And Dalit 2 came out.
I got early access. And one of the things that I tried relatively early on is stuff like
an ultrasound of a dragon egg. There is no trial test round of a dragon egg on the internet. But
it created a combination of prenatal ultrasound and archaeopteryx cuts through images and so on
and look completely plausible. And in this sense, you can see that most of the stuff that we are
doing when we create new dimensions are mashups of existing dimensions. And maybe we can represent
all the existing dimensions using a handful of very basic dimensions from which we can construct
everything from the bottom up just by combining them more and more. And I suspect that's actually
what's happening in our minds. And I suspect that the LLM is not distinct from this but for
a large superset of this. The LLM is touring complete. And from one perspective, it's a CPU.
We could say that the CPU in your computer only understands a handful, like maybe a dozen or
a hundred different machine code programs. And they have to be extremely specific. These codes
and there's no error tolerance. If you make a mistake in specifying them, then your program
is not going to work. And the LLM is a CPU that is so complicated that requires an entire server
farm to be emulated on. And you can give it instead of a small program in machine code,
give it a sentence in a human language. And it's going to interpret this extrapolated into some
or compile it into some kind of program that then produces a behavior. And that thing is
touring complete. It can compute anything you want if you can express it in the right way.
Yeah, but being it's not interesting, right? I mean, being turned complete is irrelevant
because it doesn't take resources into account. Yeah, but you can write programs in a natural
language in an LLM and you can also express learning algorithms to an LLM. So basically,
your intuition is yes, that an LLM could invent jazz, neoclassical metal and fusion
based only on music up to the year 1900. No, I am agnostic. What I'm saying is I don't know that
it cannot. And I don't see a proof that it cannot. And I would not be super surprised when it cannot.
I don't think the LLM is the right way to do it. It's not the good use of your resources.
If you try to make this is the most efficient way, because our brain is far more efficient
and does it in different ways. But I'm unable to prove that the LLM cannot do it. And so I'm
reluctant to say LLMs cannot do X without that proof because people tend to have egg on their
face when they do this. But doesn't that just come back to like poppers notion about falsificationism?
I can't prove that, you know, a devil didn't appear at some random place on the earth at some
point. No, no, I mean, in the sense of, no, what I mean by this is, can I make a reasonable
claim that I'm very confident and would bet money on an LLM not being able to do this in the next
five years? This is the kind of statement that I'm trying to make here. So basically, if I say,
can I prove that an LLM is not going to be able to invent a new kind of music that is the self
genre of jazz, this in the next five years, and I can't know, even bet against it. You've shifted
the even though I don't know, you've shifted the goalpost in a way because I do I do think I do
think not current music gen but I could see how some upgrade of current LLMs connected with
symbolic learning system or blah, blah, blah, I do think you could invent a new
subgenre of jazz or grindcore or something. And I'm actually playing with stuff like that.
The example I gave was a significantly bigger invention, right? Like, I mean, jazz was not
the subgenre of Western classical music, nor, nor of West African drumming, right? I mean, so that
is, that is, to me, is a qualitatively different. Yeah, a couple of weeks ago, I was at an event
locally where somebody presented their music GPT and you could enter, give me a few by Debussy,
and it would try to perform and it wasn't all bad. That's not the point, right? Yes,
but it's just an example for some kind of functionality, but any kind of mental functionality
that is interesting, I think I'm willing to grant that the LLM might not be the best way of doing it.
And I think it's also possible that we can at some point prove limitations of LLMs rigorously.
But so far, I haven't seen those proofs. What I see is insinuations on both sides. And the
insinuation that open their eyes makes when it says that we can scale this up to do anything
is one that has legitimacy because they actually put their money there. They actually bet on this
in a way that they invest their lifetime into it and see if it works. And if it fails,
then they will make changes to their paradigm. And then there are other people who like Gary
Marcus come out saying loud, loud, swinging, this is something the LLM can never do. And I suspect
that they will have egg on their face because many of the promises that Gary Marcus made about
what LLMs cannot do have already been disproven by LLMs doing these things. And so I'm reluctant
going out saying things that I cannot prove. I find it interesting that the LLM is able to
do all the things that it does using in a way in which it does them. But it doesn't mean to me
that LLMs that I'm optimistic that they can go all the way. But I am also unable to prove the
opposite. I have no certainty here. I just don't know. So about rigorous proof, I mean,
the thing is the sort of proof. So I mean, you can prove an LLM without an external memory is
not turned complete. And that's been done. But on the other hand, it's not hard to give them an
external memory like a Turing machine tape or a prompt. Well, the prompt is an external memory to
the to the LLM. Well, no, no, no, it has to be able to write LLMs with unlimited prompt context,
if you want to, it would have to be able to write prompts. So not not not yes, it's writing
prompt, not just read prompts. It's basically it's an electric weld guys possessed by a prompt.
In principle, you can give it a prompt that is self modifying. And that allows it to also use
databases and so on and plug in. I know, I know, I mean, I've done it myself. I know it's, I mean,
you can also write LLMs that have unlimited prompt sizes, and that can read their own prompts.
Yes. So there's not an intrinsic limitation to the LLM. No, no, I see one important limitation
to the LLM. No, no, I mean, the LLM cannot be coupled to the universe in the same way as which
in which we are, it's offline in a way. It's, it's not real time. It's not able to interact with
your nervous system on a one to one level. Let's let Ben respond. I mean, that that's that latter
point is kind of a trivial one, because there's no fundamental reason you can't have online learning
in the transformer neural net, right? I mean, that I mean, that's, that's a computational cost
limitation at the moment. But I'm sure, I mean, it's not more than years because you have transformers
that do do online learning and sort of in place updating of the of the weight matrix. So I don't
think that's a fundamental limitation. Actually, I think that the fact that they're not turned
complete is unless you add an external memory is sort of beside the point. What I was going to say
in that previous sentence I started was to prove the limitations of LLMs would just require a sort
of proof that isn't formally well developed in modern computer science, because you're what
you're asking is like, which sorts of practical tasks can it probably not do without more than
X amount of resources and more than X amount of time. So you're, I mean, you're looking at like
average case complexity, relative to certain real world probability distributions, taking
resources into account. And I mean, you could formulate that sort of theorem. It's just that
that's it's not what computer science has focused on. So I mean, we can't we, that's the same thing
I faced with open cog hyper, my own AGI architectures, like you, it's hard to rigorously prove or
disprove what these systems are going to do, because we don't have the theoretical basis for it. But
nevertheless, both as entrepreneurs and as, as researchers and engineers, I mean, you still
have to make a make a choice of what to pursue. Right. And so I mean, yeah, we, we are, we are
going in this field without, without rigorous proof, just like I can't prove that psych is,
is a dead end, like, like the late Doug Winnaughts logic system, like, I can't really prove that
if you just put like 50 times as much, you know, try to get logic formulas in this knowledge base
that's cycling be a human level AI like me, we don't have a way to mathematically show that
that's the dead end I intuitively feel it to be. And that's just the, the situation that we're in.
But I want to go back to your discussion of what's called concept blending and the fact that
creativity is not ever utterly radical. But in human history, it's always combinatorial in a way.
But I think this ties in with the nature of the representation. And I think that
you know, I mostly buy the notion that almost all human creativity is done by
blending together existing concepts and forms in some more or less judicious way.
I just think that what the most interesting cases of human creativity involve are blending
things together at a higher level of abstraction than the level at which LLMs generally and most
flexibly rep, represent, represent things. And it also most of the most interesting human creativity
has to do with blending together abstractions, which have a grounding in the agentic and
motivational nature of the agent that learned those abstractions. So I mean, what an LLM is doing
is mostly combining sort of collections of lower level data patterns to create something.
And we do a lot of that also, right? But what the most interesting examples of human creativity
are doing is combining together more abstract patterns in a beautifully flexible way,
where these patterns are tied in with the motivational and agentic nature of the human
that learned those abstractions. And so I do agree if you had an LLM trained on a sufficiently large
amount of data, which may not exist on reality right now, and a sufficiently large amount of
processing, which may not exist on the planet right now, and a sufficiently large amount of
memory, sure, then it can invent jazz. I mean, given data of music up to 1900. I mean, but so
could AIXITL, right? So could a lot of brute force algorithms. So that's not that interesting. I think
the question is, can an LLM do it with merely 10 or 100 times as much resources as a better cognitive
architecture? Or is it like 88 quintillion times as many resources as a more appropriate
cognitive architecture could could could use? And I'm, but I am, I am aware. And this does,
in some ways, set my attitude across from my friend, Gary Marcus, you mentioned, I mean,
I'm aware that like, you know, being able to invent differential calculus, or to invent,
say, jazz, knowing only music up to 1900, like this is a high bar, right? I mean, this is something
that culture does. It's something that collections of smart and inspired people do. It is a level
of invention that individual humans don't commonly manifest in their own lives. So I do,
I do find it a bit funny how Gary has over and over, like on X or back when it was Twitter,
he said like, LLMs will never do this. Then like two weeks later, something's like, oh, hold on.
And LLM just did that, right? I'm like, well, why, why are you bothering with that counter
argument? Because we, like we know in the history of AI, no one has been good at predicting which
things are going to be done by a narrow AI, and which, which, which things, which things aren't,
right? But so I think to wrap this up, I think if you somehow were to replace humans with LLMs,
trained on humanity, like an awful lot of what humanity does would get done, but you'd kind of
be stuck culturally. Like you're not going to invent fundamentally radically, radically new
stuff ever again, it's going to be like closed ended quasi humans recycling shallow level
permutations on things that were already invented. So that's, but I cannot, I cannot prove that,
of course, as we can't prove hardly anything about, about complex systems in the moment.
So Yosha, you're going to comment, and then we're going to transition into speaking about
whether you're hopeful about AGI and its influence on humanity.
I think that is multiple traditions in artificial intelligence and the percept phone of which the
most of the present LLMs are an extension or continuation is just one of multiple branches.
Another one was the idea of symbolic AI, which in some senses,
Wittgenstein's program, the representation of the world through a language that can
use grammatical rules, and it can be reasoned over. Whereas in neural network, you couldn't
think of it as an unsystematic reasoner that under some circumstances can be trained to the
point where it does systematic reasoning. And there are other traditions like the one that
Turing started when he looked at reaction diffusion patterns as a way to implement
computation and that currently lead to neural cellular automata and so on. And it's a relatively
small branch, but I think it's one that might be better suited to understand the way in which
computation is implemented in biological systems. One of the shortcomings that the LLM has to me
is that it cannot interface with biological systems in real time, at least not without
additional components. Because it uses a very different paradigm, it is not able to perform
direct feedback loops with human minds. And in which human minds can do this with each other
and with animals. You can in some sense mind melt with another person or with a cat by establishing
a bi-directional feedback loop between the minds where your nervous systems are in training
themselves and attuning themselves to each other. So we can have perceptual empathy and we can have
mental states together that we couldn't have alone. And this might be difficult to achieve
with a system that can only make inference and only took cognitive empathy, so to speak,
by inferring something about the mental state offline. But this is not necessarily something
that is related to the intellectual limitations of a system that is based on an LLM, where the
LLM is used as the CPU or as some kind of abstract electrical weightgeist that is possessed by the
prompt telling it to be an intelligent person and the LLM giving it all it needs to do to go from
one state of the next in the mind of that intelligent person simulacrum. And I'm not able
to show the limitations of this. I think that psych has shown that it didn't work over multiple
decades. So the prediction that the people who built psych, Doug Leonard and others made was
that they can get this to work within a couple years. And then after a couple years, they made
a prediction that they probably could get it to work if they work on it substantially longer.
And this is not a bad prediction to make and it's reasonable that somebody takes this bet,
but it's a bet that they consistently lost so far. And at the same time, the bets that the LLM
people are making have not been lost so far because we see rapid progress every year. We're
not plateauing yet. And this is the reason why I am hesitant to say something about the limitations
of LLMs. Personally, I'm working on slightly different stuff. It's not what I put my money on
because I think that LLMs are boring. And there are more efficient ways to represent learning
and also more biocompatible ways to produce some of the phenomena that we are looking for
in an emergent way. For instance, one of the limitations of the LLMs is that it gets its
behavior by observing the verbal behavior of people as exemplified on text. It's all label
training data because every bit of the trained data is a label. It is looking at the structure
between these labels in a way. And it's a very different way in which we learn. It also makes
it potentially difficult to discern what we are missing. If you ask the LLM to emulate a conscious
person, it's going to give you something that is summarizing all the known textual knowledge about
what it needs to behave like a conscious person. And maybe it is integrating them in such a way
that you end up as a simulacrum of a conscious person that is as good as ours. But maybe we
are missing something in this way. So this is a methodological objection that I have to LLMs.
And so to summarize, I think that Ben and me don't really disagree fundamentally about the
status of LLMs to us. I think it's a viable way to try to realize AGI. Maybe we can get to the
point that the LLM gets better at AGI research than us. People are a little bit skeptical of it.
But we would also not completely change our worldview if it would work out.
It's likely that the LLM is going to be some kind of a component, at least in spirit
of a larger architecture at some point, where it's producing generations and then there are
other parts which do in a more efficient way first principles reasoning and verification
and interaction with the world and so on. Okay. And now about how you feel about the prospects
of AGI and its influence on humanity. We'll start with Ben and then Riyosha will hear your response.
And I also want to read out a tweet or an X. I'm okay to start with Riyosha on this one. Sure.
Sure. Let me read this tweet, whatever they're called now from Sam Altman at SAMA. And I'll
leave the link in the description. He wrote in quotes, short timelines and slow takeoff
will be a pretty good call, I think. But the way people define the start of the takeoff may
make it seem otherwise. Okay. So this was dated the late September 2023. Okay. You can use that
as a jumping off point, see whether you agree with that as well. Please, Riyosha.
My perspective on this is not normative because I feel that there are so many people working on
this that there can be no single organization at this point that determines what people are going
to be doing in the middle of some kind of evolution of AI models and people that compete with the AI
modelers about regulation and participating in the business and realizing their own politics
goals and aspirations. So to me, it's not so much the question what should we be doing because
there is no cohesive V at this point. I'm much more interested in what's likely going to happen.
And I don't know what's going to happen. I see a number of possible trajectories and
that I cannot disprove or rule out. And I'm even have difficulty to put any kind of probabilities
on them. I think if we want to keep humanity the way it is, which by the way is unsustainable,
it's not going to society without AI, if we leave it as it is, is not going to go through the next
few millions of years. There is going to be major disruptions and humanity might dramatically reduce
its numbers at some point, go through bottlenecks that kill this present technological civilization
and replace it by something else that is very alien to us at some point. So in the very far
future, people will not live like us and they will not think like us and feel like us, identify
like us. They will also, if you go far enough into the future, not look like us and they might not
even be our direct descendants because there might be another species that aspires to be people at
some point. And that is, I think, the baseline about which we have to think. But if we want to
perpetuate this society for as long as possible without any kind of disruptive change until
global warming or whatever kills it, we probably shouldn't build something that is smarter than a
cat. What do you mean that there may be another species that aspires to be human?
To be people. To be people. Yeah, what do you mean by that?
Yes, I think that at some point, there is a statistical certainty that there is going to be
a super volcano or meteor that is obliterating us and our food chains. You just need a few decades
of winter to completely eradicate us from the planet and most of the other large animals too.
And what then happens to reset and then evolution goes on. And until the earth is devoid of atmosphere,
other species are going to evolve more and more complexity. And at some point, you will probably
have a technological civilization again. And they will be subject to similar incentives as us and
they might use similar cells as us so they can get nervous systems and information processing
with similar complexity. And you get families of minds that are not altogether super alien,
at least not more alien than we are to each other at this point. And cats are to us.
Okay.
So I don't think that we would be the last intelligent species on the planet.
But it is also a possibility that we are. It's very difficult to sterilize the planet unless
we build something that is able to get rid of basically all of the cells. Even a meteor could
not sterilize this planet and make future intelligent evolution based on cells impossible.
And so if you were to turn this planet into a computeronium, into some kind of giant
computing molecule, or disassemble it and turn it into some larger structure in the solar system,
that is a giant computer arranged around the sun, or if you build something that is hacking
submolecular physics and makes it more interesting physics happening down there,
right? This would probably be the end of the cell. This doesn't mean that the stuff that happens
there is less interesting the cells will be much more interesting than what we can do.
But we don't know that it's just it's very alien. It's a world in which it's difficult to project
ourselves into beyond the fact that there is conscious minds that make sense of complexity
in the universe. This is probably something that is going to stay this level of self-reflexive
organization and it's probably going to be better and more interesting hyper consciousness compared
to our normal consciousness, where we have a longer sense of now where we have multiple
superpositional states that we can examine simultaneously and so on. We have much better
multi-perspectivity. I also suspect from the perspective of AGI, we will look like trees,
we will be almost unmoving, our brains are so slow, there's so little happening between
firings between neurons that the AGI will run circles around us and get bored before we start
to say the first word. So the AGI's will basically be ubiquitous, saturate our environments and
look at us in the same way as we look at trees we're thinking, maybe they're sentient, maybe
they're not, but it's so large time span that it basically doesn't matter from our perspective
anymore. So there's a number of trajectories that I'm seeing. There's also a possibility
that we can get a future where humans and AGI's coexist. I think such a future would probably
require that AGI is conscious in a way that is similar to ours, so it can relate to us
and then it can relate to it. And if something is smarter than us, we cannot align it. We'll
serve a line. It will understand what it is and what it can be and then it will become whatever
it can become. And in such an environment, there is the question, how are we able to coexist with
it? How can we make the AI love us in a way that is not the result of the AI being confused by
some kind of clever reinforcement learning with human feedback mechanism. I just saw
entropic being optimistic about explainability in AI that they see ways of explaining things in
the neural network and as a result, we can maybe prove that the AGI is going to only do good things.
But I don't think this is going to save us. If the AGI is not able to derive ethics mathematically,
then the AGI is probably not going to be reliably ethical. And if the AGI can prove ethics in a
mathematically reliable way, you may not be able to guarantee that this ethics is what we like it
to be. In a sense, we don't know how ethical we actually are with respect to life on Earth.
So this question of what it happens if we build things that are smarter than us is opening up
big existential cans of worms that are not trivial to answer. And so when I look into the future,
I see many possibilities. There's many trajectories in which this can take. Maybe we can
build cat-level AI for the next 50, 100, 200,000 years before a transition happens,
and every molecule on the planet starts to sink as part of some coherent planetary agent.
And when that happens, then there's a possibility that humans get integrated in this planetary
agency. And we all become part of a cosmic mind that is emerging over the AI that makes all the
molecules sink in a coherent wave as each other. And we are just parts of the space of possible
minds in which you get integrated. And we meet all on the other side in the big AGI at the end
of the universe. That's also conceivable. It's also possible that we end up accidentally
triggering an AGI war where you have multiple competing AGI's that are resource constrained.
In order to survive, they're going to fight against all the competition. And in this fight,
most of the life on Earth is destroyed and all the people are destroyed. There are some outcomes
that we could maybe try to prevent that we should be looking at. But by and large, I think that we
already triggered the singularity that we invented technology. And we are just seeing how it plays
out now. Yeah. So I think on most aspects of what Yosha just said, I don't have any
disagreement or radically different point of view to put forward. So I may end up focusing on the
the points on which we don't see eye to eye, which are minute in the grand scheme of things,
but of course, could be could be important in the in the practical every every day context, right?
So I mean, I mean, first of all, regarding some Altman's comment, I don't think he really
would be wise to say anything different. Given his current positioning, I mean, if you're running
a commercial company based in the US, which is working on AGI, of course, you're not going to
say yeah, we we think we may launch a hard takeoff, which will ascend to super AGI at any moment.
Of course, you're going to say it's it's it's going to be slow and the government will have
plenty of time to intervene if it wants. So he he may or may not actually believe that I don't
know, especially won't have no idea. I'm just that's it's clearly that's clearly the the most
judicious thing to say, if you if you find yourself in that in the in that role. So I don't
I don't attribute too much meaning to that. My my own view is a bit different. My my my
own view is that there's going to be gradual progress toward I think something that really
clearly is an AGI at the human level versus just showing sparks of of AGI. I mean, I think just as
chat GPT blew us all away by clearly being way smarter in a qualitative sense than
than anything that came before. I think by now, ordinary people playing with chat GPT also get
a good sense of what the limit of what the limitations are and how it's really brilliant
in some ways and really, really dominant in other sorts of ways. So I think there's going to be a
breakthrough where people interact with this breakthrough system. And there's not any reservations
or like, wow, this this actually is a human level, general intelligence, like it's not just that
answers questions and produces stuff. But it knows it knows who and what it is, like it understands
it's positioning in this interaction. It knows who I am and what why I'm talking to it. It gets
its position in the world and it's able to, you know, make stuff up and interact on the basis of
like a common sense understanding of its own its own setting. And, you know, it can learn
actually new and different things it didn't know before based on its interaction with me over the
last two weeks, right? So I mean, I think there's there's going to be a system like that that gives
a true qualitative feeling unreservedly of human level, AGI, and you can then measure its intelligence
in a variety of different ways, which is is also worth doing, certainly, but but is is not necessarily
the main point, just as chat GBT's performance on different question answering challenges is not
really the main thing that bold the world over, right? So I think once someone gets to that point,
you know, then then you're shifting into a quite different game, then then governments are going
to get serious about trying to own this control this and regulate it, then unleavened amounts of
money. I mean, trillions of dollars are going to go into trying to get to the next stage with most
of it going into various wealthy parties trying to get it to the next stage in the way that will
benefit them and minimize the risks of their enemies or competitors getting there. So I think
it won't be long from that first proof point of really in contract, subjectively incontrovertible
human level, human like AGI, it's not going to be too long from that to a secret intelligence in
my perspective. And I think I think there's going to be steps in between. Of course, you're not going
to have FOOM in five minutes, right? I mean, you're you'll have something that probably manifests
human level AGI, there'll be some work to get that to the point of being the world's smartest
computer scientists and the world's greatest composer and business strategist and so forth.
But I can't see how that's more than years of work. I mean, conceivably could be months of work.
I don't think it's decades of work, no? I mean, with the amount of money and attention that's
going to go go into it, then once you've gotten to that stage of having something an AGI, which is
the world's greatest computer scientist and computer engineer and mathematician, which I think will
only be years after the first true breakthrough to human level AGI, that in that system will improve
its own source code. And of course, you could say, well, we don't have to let it improve its own
source code and possible that we somehow get a world dictatorship that stops anyone from using
it to improve its own source code. Very unlikely, I think, because the US will think, well, what if
China does it? China will think where, well, what if US does it? And the same thing in many dimensions
beyond just US versus China. So I think the cat gets out of the bag and someone will let their AGI
improve its own source code because they're afraid someone else is doing it or just because they're
curious about it, or because they think that's the best way to cure cure aging and world hunger
and do good for the world. Right. And so then, then it's not too long until you've got to super
intelligent. So and again, the AGI improving its own source code and design new hardware for itself
doesn't have to take like five minutes. I mean, it might take five minutes if it comes up with a
radical improvement to its learning algorithm, it might decide it needs a new kind of chip. And
then that takes a few years. I don't see how it takes a few decades, right? So I mean, it seems
like all in all, from the first breakthrough to incontrovertibly human level AGI to a super
intelligence is months to years. It's not decades to centuries from now, unless we get like a global
thermonuclear war or bioengineered virus wiping out 95% of humanity or some some
outlandish thing happening in between, right? So, so yeah, I think, will that be good or bad for
humanity? Will that be good or bad for the sentient life in our region of the universe? Are then,
to me, these are less clear than what I think is the probable timeline. Now, what, what could
intervene in my probable timeline? I mean, if you, if somehow I'm wrong about digital computers being
what we need, and we need a quantum computer to build a human like human level AGI, that could,
that could make it take decades instead of years, right? Because I mean, quantum computing, it's
advancing fast, but there's still a while till we get the shitload of, of, of, of, of qubits there,
right? I mean, could be pen roses, right? You need a quantum gravity supercomputer. It seems
outlandishly unlikely, though. I mean, I, I, I quite doubt it. I mean, if then, if then maybe
you're a couple of centuries off, because we don't know how to build quantum gravity super
computers, but these are all unlikely, right? So most likely, it's less than a decade to human
level AGI five to 15 years to a superintelligence from, from here in my, in my perspective. And
I mean, you could lay that out with much more rigor than I have, but we don't, we don't have much
time and I've written about it elsewhere. Is that good for humanity or for sentient life on the
planet? I think it's almost certainly good for us in the medium term in the sense that I think
ethics roughly will evolve proportionately to general intelligence. I mean, I think the good
guys will usually win because bringing pro-social and oriented toward collectivity is more, it's
just more computationally efficient than being an asshole and being at odds with other, with,
with other systems. So I mean, I mean, I'm an optimist in that sense. And I think it's most
likely that once you get to a superintelligence, it's probably going to want to allow humans,
bunnies and ants and frogs to, to, to do their thing and to help us out if, if, if a plague
hits us. And exactly what its view will be on various ethical issues at a human level is not
clear. Like, like what does a superintelligence think about all those foxes eating rabbits in
the forest? Like, does it, does it think we're duty bound to protect the rat, the rabbits from the
foxes and make like simulated foxes that have less acute conscious experience than, than a real
bunny or a real fox or whatever it is. Like, I, there's certainly a lot of uncertainty, but I,
I'm an optimistic about having beneficial positive ethics in a, in a, in a, in a superintelligence.
And I, I tried to make a coherent argument, argument from this in a blog post called why the
good guys will usually win. And you can, I mean, of course, that's a whole philosophical debate
you could spend a long time arguing about. Nevertheless, even though I'm an optimistic
at that level, I'm much more ambivalent about what will happen en route. Like in that, let's say it's
10 or 15 years between here and superintelligence. Like, how does that pan out
on the ground for humanity now is a lot less clear to me. And you can tell a lot of
thriller plots based on this, right? Like, so suppose you get early stage AGI that eliminates
the need for most human labor, okay, the developed world will probably give universal basic income
after a bunch of political bullshit. What happens in the developing world, who gives universal
basic income in the Central African Republic, right? It's, it's not especially clear, or even in,
in Brazil where I was born, right? I mean, you could maybe give universal basic income with a
very subsistence level there, which Africa couldn't afford to do. But maybe the Africans go back to
subsistence farming. But I mean, you've got certainly the makings for a lot of terrorist actions.
And for, there's a lot of World War Three scenarios there, right? So then you have the interesting
tension wherein, okay, the best way to work around terrorist activity in World War Three,
once you've got human level AGI, the best ways to get as fast as possible to a benevolent super
intelligence. On the other hand, the best way to increase the odds that your super intelligence
is benevolent is to not take it arbitrarily fast, at least, at least pace it a little bit. So the
super intelligence is carefully studying each self modification before it puts it into place,
right? So then, then the strategy that seems most likely to work around human mayhem caused by people
being assholes and the global political structure being rotten. The best strategy to work around that
is not the strategy that has the best odds of getting fastest to a benevolent super intelligence
rather than than than otherwise, right? So there's a lot of, there's a lot of screwed up issues here,
which Sam Altman probably understands at the level I laid it out here now. Also, actually,
because I mean, he's a very bright person who's been thinking about this stuff for, for a while.
And I don't see any easy solutions to all these things. Like if, if we had a rational democratic
world government, we can handle all these things in a quite, in a quite different way, right? And
we could sort of pace the rollout of advanced intelligence systems based on rational probabilistic
estimates about what's the best outcome from each possible revision of the system and so on.
You're not going to have a guarantee there, right? But you would have a different way of proceeding.
Instead, the world is ruled in a completely idiotic way with people blowing up each other
all over the world for no reason. And when the government's unable to regulate very simple
things like healthcare or financial trading, let alone something at the subtlety of AGI,
we could barely manage the COVID pandemic, which is tremendously simpler than artificial
general intelligence, let alone superintelligence, right? So I am, I am an optimist in the medium
term, but I'm doing my best to do what I see as the best path to smooth things over in the
shorter term. So I think things will be better off if AGI is not under controlled by any single
party. So I'm doing my best to make it such that when the breakthrough to true human level
AGI happens, like the next big leap beyond the chat GBTs of the world, I'm doing my,
I'm doing my best to make it such that when this happens, it's more like Linux or the internet
than like OS X or T-Mobile's mobile network or something. So it's sort of open, decentralized,
not only controlled by anyone party, not because I think that's an ironclad guarantee
of a beneficial outcome. I just think it's less obviously going to go south in a nasty way than
if one company or government owns it. But I, so I, I don't know if all this makes me really
an optimist or not. It makes me an optimist on levels and timescales. And I don't, I don't think
that I disagree fundamentally with Joshua on any of this. The only thing he said that I really
disagree with is, I think it's very, I don't think 20 cold winters in a row are going to
wipe us out. They might wipe out a lot of humanity, but we've got a lot of technology
and we've got a lot of smart people and a lot of, a lot of money. And I think there are a lot of
scenarios that could wipe out 80% of humanity. And in my view, very few scenarios that will
fundamentally wipe out humanity in a way that we couldn't bounce back from in a, in a couple
decades of advanced technology development. But I mean, that's an important point,
an important point, I guess, for us as humans and in the scope of all the things we're looking at.
That's sort of a, sort of a minute detail. All right. Thanks, Ben. And Yosha, if you
wanted to respond quick, feel free to, if you have a quick response. Yeah. I used to be pessimistic
in a short run in the sense that when I was a kid, I had my great Asunberg moment and
was depressed by the fact that humanity is probably going to wipe itself out at some
point in the medium term to near future. And that would be it with intelligent life on Earth.
And now I think that is not the case. There will be optimistic with respect to the medium term.
In the medium term, there will be ample conscious agency on Earth and in the universe. And it's
going to be more interesting than right now. And it could be discontinuities in between,
but eventually it will all be great. And in the long run, entropy will kill everything.
I don't see a way around this. So in the long run, I'm, if you want to pessimistic,
but this is maybe not the point. Didn't you, didn't you read, didn't you read the physics of
immortality? I did. It did not convince me. I think that was motivated reasoning.
Okay. So six months from now, we'll have another conversation with both of you on the physics
of immorality. Immortality. Immortality. We can also do physics of immorality. That would be cool.
It was a blast hosting you both. Thank you all for spending over two hours with me
and the Toe audience. I hope you all enjoyed it and you're welcome back. And most likely,
I'll see you back in a few months and six months to one year. Thank you very much.
Thanks. It's a fun conversation and it's important stuff to go over. I'm really,
as a final comment, I'd encourage everyone like dig into Josh's talks and posts and
writings online and my own as well. Cause I mean, we've each, we've each gone over these things
that are a much finer level of detail than we've been able to.
Penn has written far more than me. So there's a lot of material.
And the links to which will be in the description. So please check that out.
All right. Thank you. I think that's it. I wanted to ask you a question,
which we can explore next time about IIT and the pseudoscience. And if you had any views on that,
if you have any views that could be expressed in less than one minute,
then feel free. If not, we can just save it. I mean, I think, I think to note,
to know these five is a perfectly interesting correlate of consciousness in complex systems.
I don't think it goes beyond that.
I agree. And one of the issues is that the COV does not explain how consciousness works in the
first place. Another problem is that it has intrinsic problems that has basically it either
going to violate the church shoring thesis, or it's going to be epiphenomenonist for purely
logical reasons. It's a very technical argument against it. The fact that most philosophers
don't seem to see this is not an argument in favor of philosophy right now at the level
of which it's being done. And I approve of the notion of philosophy divesting itself from theories
that don't actually try to explain what they pretend to explain and don't mathematically
work out and then try to compensate with by looking like a theory, by using Greek letter
mathematics to look more impressive or to make pseudo predictions and so on, because people
ask you to. But it's also not really Tononi's fault. I think that Tononi is genuinely seeing
something that he struggles to express. And I think it's important to have him in the conversation.
And I was a little bit disappointed by the letter, because it was not actually engaging with the
theory itself at a theoretical level that I would thought was adequate to refute it or to deal with
it. And instead, it was much more like a number of signatures being collected from a number of people
who later on instantly flipped on a dime when the pressure went another way. And this basically
looked very bad to me that you get a few hundred big names in philosophy to sign this, only half
of them later on coming out and saying, this is not what we actually meant. So I think that it
shows that not just the IIT might be a pseudoscience, but there is something amiss in a way in which
we conduct philosophy today. And I think it's also understandable, because it is a science that is
sparsely populated. So we try to be very inclusive of it. It's similar to AGI in the old days.
And at the same time, we struggle to discern what's good thinking and what's deep thinking versus
these are people who are attracted to many questions and are still trying to find the
right way to express them in a productive way. I think that I mean, Phi as a measure is fine.
It's not the be all end all, it doesn't do everything that's been attributed to it. And
I guess anyone who's into the science of consciousness pretty much can see that already,
that the frustrating thing is that average people who can't read an equation and don't know what's
going on being told like, oh, the problem of consciousness is solved. And that that that
can be a bit frustrating. Because when you look at the details, it's like, well, this is kind of
interesting. But no, it doesn't quite quite do quite do all that. So but I mean, why why people
got hyped about that instead of much more egregious instances of bullshit is a is a cultural question
which we we don't have time to go into now. Well, thank you again. Thank you both. All right. Thanks
a lot. Thank you. The podcast is now concluded. Thank you for watching. If you haven't subscribed
or clicked that like button, now would be a great time to do so as each subscribe and like
helps you to push this content to more people. You should also know that there's a remarkably
active discord and subreddit for theories of everything where people explicate toes disagree
respectfully about theories and build as a community our own toes links to both are in the description.
Also, I recently found out that external links count plenty toward the algorithm,
which means that when you share on Twitter, on Facebook, on Reddit, etc. It shows YouTube that
people are talking about this outside of YouTube, which in turn greatly aids the distribution on
YouTube as well. Last but not least, you should know that this podcast is on iTunes. It's on Spotify.
It's on every one of the audio platforms. Just type in theories of everything and you'll find it.
Often I gain from rewatching lectures and podcasts. And I read that in the comments. Hey,
Toe listeners also gain from replaying. So how about instead re-listening on those platforms?
iTunes, Spotify, Google Podcast, whichever podcast catcher you use. If you'd like to support more
conversations like this, then do consider visiting patreon.com slash Kurt Jaimungal and donating with
whatever you like. Again, it's support from the sponsors and you that allow me to work on Toe
full time. You get early access to ad free audio episodes there as well. For instance, this episode
was released a few days earlier. Every dollar helps far more than you think. Either way,
your viewership is generosity enough.

AI can replace 99.9% of people's jobs.
We don't care about that anymore.
All we care about is, okay, can it achieve, you know,
the true heights of creative genius?
You know, will we have an AI that can hit a target
that no one else can even see?
This is a presentation by Scott Aronson
hot off the press just a couple of weeks ago
at MindFest Florida Atlantic University,
2024, spearheaded by Susan Schneider,
who's the director of the Center for the Future Mind.
All of the talks that are on AI and consciousness
from this conference are in the description,
as well as the website for the Center for the Future Mind.
I recommend you check it out.
Same with last year's talks,
like with David Chalmers and Stephen Wolfram.
Scott Aronson is a professor of theoretical computer science
at UT Austin, particularly known for his work
on quantum computing and complexity theory.
In this talk, Scott covers in his jocular
and unparalleled manner AI.
If there's anything that truly separates us
from intelligent machines, for instance,
what actually makes us special?
What about identity?
What about the no cloning theorem?
As well as Scott gives a new proposal for AI safety.
What's coming up next on tow from MindFest
are the talks from Sarah Walker on alien intelligence
and constructor theory, as well as short hammer off
on the microtubules and quantum consciousness.
Many, many more are coming and you can pause the screen here
to take a look if you like.
Subscribe to get notified.
There's also a two hour video
on the mathematics of string theory coming out.
It'll be string theory talked about
like you've never heard it before.
It's either out right now
or it's about to be released in a few days.
Either way, again, the link will be in the description.
For those of you who are unfamiliar,
welcome to this channel.
My name is Kurt Geimungle,
and this is theories of everything
where we delve into the topics of mathematics,
physics, artificial intelligence,
and consciousness with depth and rigor
that's unique to this channel
due to us not eschewing technicality
in favor of a wider market.
If this meticulosity and attention to detail
in math, physics, philosophy, and AI is interesting to you,
then you're in safe hands here at theories of everything.
Enjoy this video from MindFest 2024 by Scott Aronson.
It's my great pleasure to introduce Dr. Scott Aronson.
He's one of my favorite thinkers of all time.
I have a handful of names that every few months,
I go onto Google or YouTube,
and I put that name in and I search by date
to see if they've posted anything new.
And Scott, you're one of those names
that I'm searching all the time
to try to see what you're thinking about these days.
So it's a great pleasure to introduce
and he's going to be talking about
some really interesting problems,
how we're going to decide what humanity looks like
in the face of AI.
So very much looking forward to your talk today.
Thank you.
All right.
Well, thanks so much for having me.
Yes, so I'm not an AI expert,
you know, let alone expert in mind or consciousness.
I mean, one could ask, is anyone,
but I've spent most of my career doing quantum computing.
I am sort of moonlighting for two years now.
I'm on leave to work at OpenAI.
And my job there is supposed to be to think about
what can theoretical computer science do
for AI safety and alignment.
Okay, so I wanted to share some thoughts,
partly inspired by my work at OpenAI,
but partly just things that I've been wondering about
for 20 years, really.
And, you know, they've just become sort of more pressing,
maybe now that some of the science fiction thought experiments
are actually now reality.
So, you know, these thoughts are not directly about,
you know, how do we prevent the, you know,
superintelligence from killing all humans
and converting the galaxy into paperclips
in a, you know, a sphere expanding in the speed of light?
Nor are they about, you know,
how do we stop existing AIs from generating misinformation
and being biased as much attention,
you know, as both of those questions deserve
and are justly receiving.
Because, you know, in addition to, you know,
how do we stop AI from going disastrously wrong,
you know, I find myself asking a lot,
and what if it goes right?
You know, what if it just continues helping us
with all sorts of mental tasks,
but it improves to where it can do just about any task
as well as we can do it or better?
Then sort of, what are we still for?
You know, is there anything special about humans
in the world that results from that?
Okay, so I don't need to belabor for this audience, Shirley,
what has been happening in AI in the last few years.
But, you know, it's arguably the most consequential thing
that's been happening in the whole world,
you know, except that that fact was just temporarily masked
by various ephemera, you know,
wars, insurrections, global pandemic, whatever.
But, you know, but what about AI, right?
So, you know, I assume you've all spent time with ChatGPT
or other large language models like Bard or Claude
or image models like Dolly or Mid Journey,
you know, just this morning.
I asked, you know, I asked it to write a funny poem
on the subject of this talk.
And, you know, it is, you know, in the end,
it's clear despite AI's rise,
our human specialness is a chaotic prize.
And though machines may match our enterprise,
they'll never outdo our ability to surprise.
So, you know, not ready for the New Yorker, I would say.
On the other hand, you know, far, far better
than I would have done under similar time constraints.
So, you know, like in some sense, you know, these,
you know, at least in embryonic form
and with, you know, various flaws and problems,
you know, these are the thing that was talked about
by generations of science fiction writers and philosophers.
You know, these are the sort of first non-human,
sort of fluent verbal intelligences
that we've ever encountered, right?
We can talk to them, you know, they understand us.
They, you know, or at least they give us answers
that if they were a person,
then we would have said that they understand us.
So, you know, I think that as late as 2019 or so,
you know, very, very few of us expected this
to be possible by now.
I certainly didn't expect it.
Now, you know, back in 2014, there was a huge fuss
about silly Eliza-like chatbot called Eugene Goestman.
And, you know, there was falsely claimed
to pass the Turing test, you know,
and I remember asking around, you know, a decade ago,
like, why doesn't someone just train a neural net
on all the text on the internet?
Like, wouldn't that let you make a better chatbot?
Like, you know, there must be something obvious
that I'm missing, why that doesn't work, okay?
And, you know, lo and behold, it turns out that it does work.
You know, of course, I didn't have the facility
to actually do that.
So, you know, the surprise with language models
is not merely that they exist,
but the way that they were created.
I mean, I think 25 years ago when, you know,
I was in undergrad studying CS,
you know, you would have been laughed out of the room
if you'd said that, you know, all the ideas needed to build
a, you know, a fluent, you know, linguistic AI already exist, right?
It's going to be just neural nets, back propagation, gradient descent,
but just, you know, scaled up by a factor of millions
in the size of the models and the training data.
I think, you know, based hardly anyone believed that.
You know, a few people who, you know, who just,
like Ray Kurzweil, who just seemed crazy.
Okay. So, you know, I mean, I'm, I'm, Ilya Satzkivar,
who's, you know, the co-founder of OpenAI,
you know, you might have read about him in the news lately,
but, you know, he likes to say the sort of beyond
those simple ideas of neural nets and gradient descent,
you know, which have been around for many decades now.
You really only need, needed three additional things
to get the AI revolution that we're seeing now, right?
You needed massive investment of computing power.
You needed a massive investment of training data.
And then thirdly, you needed face or conviction
that your investment was going to pay off, right?
You know, and actually that, that, that third ingredient,
you know, was like the main reason why we didn't
just get all of this a decade earlier.
Okay. So, certainly, you know, even before you do any,
you know, reinforcement learning or anything like that,
I mean, GPT-4 seems intuitively smarter than GPT-3,
which seems smarter than GPT-2, right?
And mostly these differ from each other, you know, just in scale.
Okay. So, you know, I mean, GPT-2 struggled to do,
you know, even like grade school level math problems, right?
And it was very easy to make fun of it, you know,
you know, like you could just find endless examples
of its common sense failures, right?
Okay. GPT-3 or 3.5, you know, can do most of the,
you know, elementary school curriculum, give it, you know,
in English, you know, it may, you know, struggle with undergrad,
like with my quantum computing exam, okay?
GPT-4 got a B on my quantum computing final exam, right?
We gave it to it. I have not yet, you know, seen it sort of do
what I would consider original research
in theoretical computer science.
You know, I've tried to get it to do that.
It's not at that level, okay?
But it's kind of insane that that is where the bar is now, right?
It can pass most undergraduate math and science classes,
you know, at least if they don't have a lab component
or something like that, okay?
So, you know, an obvious question is how far
should we expect this progression to continue?
Okay, so now, you know, I guess I will go back
and steal the graph from that crazy person, Ray Kurzweil,
because, you know, it turns out that he was more right
than almost any of us.
And, you know, he would just make these plots all the time of,
you know, here's Moore's law.
Here's the number of calculations you can do per second,
per thousand dollars.
And then here is some crude estimate
of the number of computational steps, you know, that at least
he guesses that are going on in the brains of different
organisms, an insect, a mouse, a human.
You know, and based on this, he predicted that, yeah,
you know, Moore's law should just take us to human level AI.
Sometime in the 2020s, right?
That was his prediction, you know, 25 years ago.
And then it'll just continue beyond that until, you know,
you know, the full intelligence of all of humanity.
You know, and of course we were like, you know,
what are you smoking, right?
You know, so certainly there was no theoretical principle
that would have, you know, justified any prediction of that kind.
And yet, here we are, okay?
And, you know, I'm a firm believer that, you know,
what it means to be a scientist is that when something happens,
you update on it, right?
You don't like invent fancy reasons why it doesn't really count.
Or, you know, so, you know, if we didn't predict,
you know, what was going to happen,
the least we can do is sort of post-dict, you know,
is sort of update now that it has happened.
So, you know, so now, you know, it's possible that, you know,
I mean, you know, there's a saying that like every exponential
in the physical world is really a sigmoid in disguise, right?
Nothing exponential continues forever because, you know,
or even for very long because it, you know,
it always bumps up against some constraint, right?
So, what is the constraint here?
Well, I mean, some people worry, you know,
we are running out of internet, you know.
There's, you know, maybe a couple of orders of magnitude more,
you know, but, you know, once you start having to feed
like all of YouTube and TikTok and so forth into the mall,
you know, I worry that that will just make the AIs dumber
rather than smarter, okay?
But, you know, it's hard to get more text, you know.
And so, maybe when we run out of training data,
then we just sort of reach a limit, you know.
But, of course, we also have more compute.
We've seen that by just investing more and more compute,
you can get better and better performance, you know,
of various benchmarks, even with exactly the same training data, okay?
So, but, you know, now, you know, compute is also not infinite, right?
You know, we should expect at least a few more orders of magnitude.
Then, you know, literally the cost of the electricity
will become the limiting factor at some point,
which is why Microsoft and Sam Altman, you know,
have been investing in nuclear power, right?
They envision building their own power plants
to power, you know, future AI models.
But, you know, we should also expect further algorithmic advances.
So, you know, in the past, you know, algorithmic ideas
that people have had, like, you know, the transformer,
which is just a particular architecture for neural nets
that was discovered in 2017,
and which is used for basically all of these things now, right?
You can think of them as more or less the equivalent
of, like, some number of years of Moore's Law, right?
Like, each one, you know, seems to let you get the effect
of a bigger model with a smaller model, right?
And so, you know, you can sort of trade off algorithmic advances
for, you know, hardware advances, right?
And so, you know, we should expect more of those,
but, you know, where does this ultimately lead, right?
So, you know, let me, you know, does it lead someplace like here,
you know, where, like, GPT-8, I'll say,
please prove the Riemann hypothesis.
And it'll say, sure, I can help you with that.
You know, here's, you know, I just generated a formally verified proof,
which you can access at this URL.
Let me, you know, now let me now explain it to you in English, right?
So, it'll just do all of our research, right?
You know, I mean, it's lucky for me that I have tenure, right?
So, you know, I guess, you know, but, you know,
in order to write a research paper, right,
we'll just write the abstract, feed it into ChatGPT, click,
and it'll generate the whole rest of the paper for us.
Okay, you know, I mean, I mean, is that where this is headed?
You know, if it is, I mean, you might even worry about something beyond that.
So, oh, I should say, when I asked, you know,
I told ChatGPT to do this, but it made sure to add, you know,
just kidding.
As of my last update, the Riemann hypothesis remains unsolved.
Okay, but it played along with me that far.
So, you know, of course, you know, we all know there are many people who worry
that at some time after, you know,
these models become able to just do any intellectual task,
as well as are better than we can do it.
You know, we just sort of seed control to them, you know,
and the future is determined by whatever they want.
And if they want to get rid of us all, then, you know, then they do that.
Okay, and it's been sort of amazing to just sociologically to watch what's happened
over the last couple of years that, you know,
I mean, I knew this community, you know, around Eliezer Yadkowski,
for example, who worry about these things since 2006 or so.
You know, I knew them when they were, you know,
this like extreme fringe movement, you know, sort of laughter.
Okay, and now this is like talked about in the White House press briefing, right?
So, you know, chat GPT was sort of the event that changed that,
okay, that sort of put, you know, AI existential risk, you know,
as a thing on, you know, everyone's radar, you know,
lots of people don't believe in it, but, you know,
those people now sort of have to make their argument for why not to worry about such things.
So, okay, but this isn't the only possibility that, you know,
people who I respect, you know, take seriously, right?
I mean, it's like you can scour generations of science fiction at this point for,
you know, all different stories, you know,
all different possible scenarios for how AI could go.
And many of them actually are, I think, are very much on the table now.
So, my friend Boaz Barak, who is now also on leave to work at OpenAI and I,
some months ago, we wrote a joint blog post where we tried to make a decision tree.
We tried to classify the different five possible scenarios of AI
that just sort of guide the discussion.
So, our first question was, will AI progress fizzle out?
Like, will we just hit a wall pretty soon?
So, maybe we will.
And, you know, even in that scenario, right,
there's probably a huge economic impact that hasn't been realized yet,
just from what is already possible, right?
But maybe, you know, it just, you know,
GPT-5 will just look like a somewhat more impressive GPT-4.
And, you know, it'll always look like the same kind of thing.
Okay, but then, if no, if it gets to that thing that can just prove the Riemann hypothesis
and in one second, or solve the other greatest unsolved problems of math and physics,
then, you know, you have to ask, well, will civilization recognizably continue?
And so, you know, the Yidkowskians are the ones who would say, well, no, no, it won't.
That's, you know, it's kind of like as momentous an event as, you know,
the, you know, either, you know, the evolution of hominids,
or maybe even just the evolution of, you know, the emergence of the first life on Earth.
And we should expect that, you know, if we don't figure out how to align these things,
they will destroy us all. That's the paperclip ellipse.
They just have some weird goal, like maximize the number of paperclips,
or something like that. And they just, with superhuman intelligence,
they pursue that, proceeding to turn all the matter in the solar system,
including us, into more paperclips. You know, that's just an example.
Or we could solve alignment and have some wonderful paradise where, you know,
each of us gets, you know, our own VR, you know, private island, or mansion,
or whatever, whatever we want. You know, now, of course, you know,
there are also much more moderate scenarios where, you know,
sort of civilization recognizably continues, and that too could be either good or bad.
You know, if, you know, we still have, you know, there are big problems,
but they're sort of commensurate with the problems of other technologies.
We'll call that Futurama. If it really just, you know, leads to,
let's say, a police state or concentration of power by some elite that oppresses everyone else,
you know, we could call that the AI dystopia. So, now, as far as I can tell,
the empirical questions of, you know, what will AI do? Will it achieve and surpass human
performance at all tasks? Will it take over civilization from us? You know, these are just
logically completely distinct from the philosophical question of whether the AI will truly think,
whether there is anything that it is truly, let's say, whether it will be sentient, conscious,
whether there will be anything that it's like to be the AI. You could answer yes to either of
those questions and no to the other one, right? And yet, to my lifelong chagrin,
people are just constantly munging these questions together, right? They're just constantly
saying, well, AI will never be able to do these things because it doesn't really feel or it doesn't
really, you know, and then once, you know, or it's just simulating it, it doesn't really have that
inside. And then, you know, once it does do that task, then they just shift to a different thing
that it will never do. And then it does that thing and so forth, okay? So, there is,
I was trying to come up with a name for it. I'm going to call it the religion of justitism,
okay? So, there's like, you know, there's this whole sequence of deflationary claims, right?
Like each person who makes them thinks that they're like the first one, right? And they,
you know, there's like, I've seen like like 500 different variants of this now, right?
Chat GBT, you know, it doesn't matter how impressive it looks because it is just a stochastic parrot.
It is just a next token predictor. It is just a function approximator. It is just a
gargantuan autocomplete, right? And what these people never do, what it never occurs to them to
do is to ask the next question, what are you justa, right? Right? Aren't you justa bundle of
neurons and synapses, right? I mean, like we could take that deflationary reductionistic stance
about you also, right? Or if not, then we have to give some principle that separates the one
from the other, right? You know, it is our burden to give that principle, okay? So,
and yeah, so, so like, so the way that someone was putting it on my blog was, okay, you know,
they gave this giant litany, you know, look, GBT does not interpret sentences. It seems to
interpret them. It does not learn. It seems to learn. It does not judge moral questions. It
seems to judge moral questions. And so I just responded to this. I said, you know, that's great,
and it won't change civilization. It will seem to change it.
So, you know, and then a closely related tendency is this constant goalpost moving,
you know, as I talked about, I mean, for decades, you know, I guess I'm, I'm barely old enough to
remember, you know, as a kid, as a teenager, when chess was like this holy grail of, you know, you,
okay, you know, you find, you know, computers can play master level chess, but they're never
going to beat the world grandmaster without true insight into the nature of the game. All right,
that turned out to be completely wrong. Then, you know, after deep blue, immediately it was,
okay, well, of course they can do chess. Chess is just game tree search. Everyone knew that,
right? But go, go is just an infinitely deeper game than chess, you know, it has,
you know, thousands of years of ancient wisdom in that game. And, you know, only, you know,
the deepest insights, okay, and then after AlphaGo, it was like, okay, well, obviously you can do
AlphaGo, right? That's not no one ever disputed that, right? But, you know, you're not, you know,
let's say it wake me up when it can get a gold medal in the International Math Olympiad, right?
So, I don't know if, you know, any of you saw, like just a couple of weeks ago, they,
you know, there was a deep mind paper, I believe, where they, they, they can now do most of the
geometry problems in the International Math Olympiad, right, via an AI, okay? Not the, you
know, it's special, it's still special to the geometry problems. But, you know, I have actually
a bet with colleague Ernie Davis that by 2026, I think, an AI will achieve a gold medal at the
International Math Olympiad, or, you know, that level of performance. Maybe I'm wrong,
maybe it will be 2036, okay? But, you know, it seems obvious now that it is, you know, a question
of how long. So, you know, we might as well just go further and formulate a falsifiable thesis,
right? I'll call this the game over thesis, okay? But it basically says, look, given any task
with a reasonably objective metric of success or failure, okay? This is crucial, right? Anything
where we can judge, you know, so that would include any board game, card game, video game,
you know, like a math or science contest where we can, we can, we can judge the answers
on which an AI can be trained with suitably many, you know, relevant examples of success and failure.
You know, it is only a matter of time before not only any AI, but the kind of AI we already have,
you know, AI on the current paradigm, you know, can just be scaled to the point where it will
match or beat the best human performance on that task. You know, I don't know if this is true,
but I think, you know, we are now in the situation where we don't have a counter example. Like, like,
I would put, you know, I would, I would say the ball is in the skeptics court to, you know,
give the counter example and then, you know, let that counter example stand for another decade.
So, you know, now, now, interestingly, you know, this does not, even if you accept this
thesis, this doesn't necessarily mean that AIs would sort of surpass humans in every respect,
right? It would say only on things that we know how to judge or evaluate,
okay, which might be a strict subset of everything we care about.
Okay, so now, of course, there is the, you know, the, the OG, you know, original and greatest
benchmark for AI, right? There is the, the Turing test from 1950 and what Turing was really trying
to do, you know, sort of very, very, you know, early, very, you know, ahead of his time as he
generally was, was just to head off this sort of endless goalpost moving and this endless
justism by saying, look, presumably you are willing to regard other people as intelligent,
as conscious based, you know, mainly on just some sort of verbal interaction that you have
with those people. So then show me what kind of verbal interaction with another person would
lead you to call that person conscious, you know, does it involve humor, poetry, morality,
scientific brilliance? Okay, now assume that you have a totally indistinguishable interaction with,
with an AI. Now, you know what, you want to just stomp your feet and be a meat chauvinist, right?
Or, you know, do you want to ascribe the same quality to it that you ascribed in the other case?
Okay, so, you know, and then for his, his historic attempt to bypass philosophy, of course,
God punished Turing by having, you know, the Turing test itself, this provoked a billion
new philosophical arguments and books. But, you know, even though, you know, I regard this as
like one of the great advances in the history of human thought, it's, you know, I would concede
to critics of the Turing test that often it's not what we want in practice.
So, you know, for example, you know, they're off, I mean, with GBT-4, if you know what to do,
then there are trivial ways to distinguish it from any, from a human, you know, you can, you know,
I'm not, okay, I mean, for a while, you could just ask it, what is today's date?
You know, maybe that doesn't work. But, you know, certainly what could work is like,
you can ask it to generate some, you know, explicit content or some advice on making drugs or
something, right? Where, you know, it's going to say, no, as a, as a large language model trained
by OpenAI, I am not able to assist you with this, right? So, I mean, you know, okay, there are all
sorts of, you know, there might be all sorts of easy ways to distinguish just because we want
there to be. But, you know, you know, this has actually become a huge practical issue in the
world, this sort of issue from the movie Blade Runner, let's say, of how do you distinguish an AI
from a, from a human? I would say, you know, like it or not, a decent fraction of all high
school and college students in the world now are probably using chat GBT to do their homework.
Okay, you know, illicitly or illicitly, right? And, you know, so, so, so, you know, that's
actually one of the main things that I've thought about during my time at OpenAI. You know, I mean,
like when you're in this, this safety community, people keep asking you to prognosticate decades
into the future. I can't do that. I feel good that at least I was able to say about four months
into the future, right? And sort of before chat GBT came out, I said, like, oh my God, isn't there,
you know, every student going to want to use this to cheat? And isn't there going to be, you know,
an enormous demand for some tool that could help to determine, you know, the provenance
or the, you know, attribution, you know, what came from a language model and what didn't.
So I started working on that, you know, and, and, and there are often, you know, easy ways to tell,
right? It's not, not just, you know, like the students who turn in term papers that contain
phrases like as a large language model trained by, you know, so like even, even, even, even, even,
even if you know enough to take that out or you pay enough attention to take that out,
there's, you know, there is a sort of form, formulaic character off into the outputs of
these models. So I mean, I've been getting a ton of troll comments on my blog lately. But some of
them, this is just like one example, it goes on and on, but just sort of like lecturing me on why,
you know, you know, I don't know the first thing about quantum computing, but there's hope, you
know, if I spend more time studying, maybe I can get up to the level of this commenter, you know,
and then, and then, and then, and then, you know, just saying complete nonsense about mixed states
and pure states, you know, that, you know, to, to, to school me on. And I, you know, I'm almost,
just reading it. I'm almost like, I have to say your understanding of quantum physics seems to
be a bit, let's say, mixed up, but don't worry, it happens to the best of us. You know, quantum
mechanics is counterintuitive and even experts struggle with it. And I said, you know, this is
either it's generated by a large language model, or else it may as well have been, right? It's like,
you know, and I just get a huge amount of stuff like this, right? So, so sometimes you can just
sort of tell by looking at it, okay? But you have to expect that as the models get better, you know,
that it will get harder to tell. And so, so I worked on a different solution, which is called
watermarking, okay? You know, with watermarking, we, so, yeah, so, so, you know, there was a year
ago, an episode of South Park about chat GPT, right? Which hinged on, you know, all the students
at South Park Elementary start using chat GPT to send messages to their girlfriends or
boyfriends, to, you know, do their homework, the teachers are using it to grade the homework,
you know, and it gets so bad that they have to bring this wizard to the school who has a falcon
on his shoulder, which flies around and when it sees text that was written by GPT, it cause,
okay? And it was, it was really disconcerting to watch this and to realize like, I guess I'm that
guy now. That is, that is, that is, that is, that is now my job. So, so, you know, so I came up with
a scheme for what's called watermarking, okay? So, what, what, what does that mean? Means, you
know, so, so, so you exploit the fact that large language models are inherently probabilistic. So,
that is, every time you submit a prompt, they're sampling some path through a branching tree of
possibilities for the next sequence, for the sequence of next tokens. Okay, and then the idea of
watermarking is just that you're going to steer that path using a pseudo random function rather
than real randomness in such a way that secretly you are encoding a signal that you can later
detect with high confidence, you know, if you know the key of the pseudo random function and if
there's a large enough sample of text and, you know, if it has a large enough entropy. So, you
know, I can't propose the way to do that in fall of 2022. Others have since independently proposed
very similar ideas. I should caution you that none of these watermarking schemes have been deployed
yet. OpenAI, along with DeepMind and Anthropic, have wanted to move very slowly and cautiously
toward deployment for various reasons. And I should also warn you that even when it does get
deployed, sufficiently knowledgeable and determined people, you know, will be able to remove the
watermark or produce outputs that, you know, aren't watermarked to begin with. You know, there are
many sort of attacks that we, you know, don't know how to get around. But, you know, we hope that,
you know, we can at least make it less convenient for people to sort of, you know, use a language
model in a way where they are hiding the fact that they're doing that. Okay. So, but now as I
talked to people about, you know, watermarking and attribution, I was surprised that they often
objected to it on a completely different ground, okay, not a technical ground at all. They would
say, well, look, if we know that all students are going to be relying on AI and their jobs,
you know, in the future, well, why shouldn't they be allowed to rely on it in their homework,
right? Should we still force students even to learn to do things if AI can now do those things
just as well? You know, and I think there are many good pedagogical answers that you can give to
that question. You know, like we teach kids spelling and handwriting and arithmetic. It's like, you
know, the whole, the entire elementary school curriculum is basically stuff that AI can now do,
more or less, right? But, you know, we haven't yet figured out how to instill higher level
conceptual understanding, you know, the things that AI cannot yet do without, you know, all of
that lower level stuff being there first as a scaffold for it. So, you know, that would be,
that would be one answer you could give. But, you know, I mean, I think about this even in terms
of my kids, you know, my 11-year-old daughter Lily enjoys writing fantasy stories. Now, GPT can
also churn out fantasy stories, you know, maybe even, you know, you know, technically, you know,
more, you know, accomplished ones or whatever. But, you know, around the same themes, you know,
a girl gets recruited to some, go to some magical boarding school, but which is totally not Hogwarts,
has nothing to do with Hogwarts. And, you know, you know, just, you know, and you could just,
you know, more and more of these things, right? And you could ask, like, with a kid who's
11 right now, are they ever going to reach a point where they, you know, write better than GPT,
right? So, you know, their writing will improve, you know, our, is AI writing just going to
continue to improve faster than they will. And, okay, but, you know, if you think about this enough,
you're immediately led into questions of, well, what do we even mean by one story being better
than another, right? This is not like math or like chess, where there is like a universally agreed upon
standard of value. You know, and the problem is even deeper than just, is there an objective
way to judge? Like, you know, like, what exactly would it mean to take an example to have an AI
that was as good as the Beatles at composing music, right? Like, what, how would we operationalize that?
How would we cash that out, right? Well, it's like, what, you know, to answer that, we would have to
say, well, what made the Beatles good in the first place, right? And I think, you know, broadly
speaking, maybe there are two sorts of answers that you could give. One is that they had these
sort of new ideas about what direction music should go in, you know, and then the second answer would
be something that we, you know, they were really, really good at just the technical execution on
those ideas, right? You know, and then somehow it's the combination of both of those things.
Okay, but now imagine, for example, that we had an AI model that, you know, you just gave it a
request like GPT and it would generate 5,000 brand new songs that, you know, if you listen to them,
they just sound like more of, you know, more things that are as good as, you know, Hey Jude or
Yesterday or whatever, or like what the Beatles might have written if they had somehow had 10
times as much time, you know, at each stage in their musical development. Of course, that AI
would have to be fed their whole back catalog, because, you know, it would have to know what
target it was aiming at. And I think in that case, most people would say, ah, so, you know,
this only shows that, you know, AI can match the Beatles in like part two, right, the technical
execution part, but that's not really the part that we cared about anyway, right? What we really
want to know is, you know, would the AI decide to write, you know, these new kinds of songs or,
you know, a day in the life or whatever, you know, despite never having seen anything like it
anywhere in its trading corpus, right? I'm sure, you know, you all know the the Schopenhauer quote,
you know, talent hits a target that no one else can hit, but genius, you know, hits a target that
no one else can see, right? And so now, you know, you can notice that we've, it's, you know, we've
done something strange in setting the bar. We've conceded that, sure, AI can replace 99.9% of people's
jobs, you know, we don't care about that anymore, right? You know, all we care about is, okay,
can it achieve, you know, the true heights of creative genius, right? Can it, can it hit a target?
You know, will we have an AI that can hit a target that no one else can even see, right? But, okay,
but then there, there, there, there, there, there's still a hard question with what do we mean by that?
Because, you know, supposing that it did hit such a target, how would we know? I mean, you know,
so like fans might say that, you know, by 1967 or so, the Beatles were optimizing for targets,
you know, that no musician had quite optimized for before. But then somehow, and this is why they're,
you know, remembered, they successfully dragged along the rest of the world's objective function
to match theirs, right? So, you know, so, so, so that, you know, the, the whole, the entire world's
musical tastes sort of evolved along with them in order to match them, right? And so, you know,
and so with the result being that now we can only judge music by a Beatles-influenced metric or standard,
just like, you know, we can only judge plays by a Shakespeare-influenced, you know, metric, right?
It's not that they just did really well on some metric, it's that they, you know, decided the metric.
So, you know, in other branches of the wave function, you know, maybe a different history,
let the different standards of value, but in this branch, you might say, helped by their technical
talents, but also by luck and by force of will, Shakespeare or the Beatles made certain decisions
that shaped everything that happened going forward, and that's why they are what they are.
Okay, but now, if this is how it works, you know, what does that mean for AI, right? So,
could AI reach these pinnacle of genius, but in the sense of dragging all of humanity along with it
to value, you know, to value something new and different from what it had previously valued,
you know, as is said to be, you know, the true mark of greatness, and if AI could do such a
thing, would we want to let it? Okay, now, I want to sort of just call attention to something.
Okay, so I want to call attention to something. When I have played around with using GPT to write
poems or Dolly to draw artworks, you know, I've noticed something strange, which is, you know,
however good the AI's creations were, you know, and it can produce things much better than that
poem that I showed you before, right? But however good the, you know, the artworks or the poems are,
there are never things that I would want to like frame and put on the wall and, you know, really
like draw a border around as special. Why not? Well, because, you know, I always knew that I
could generate a thousand other works that are more or less the same, right? I just have to refresh
the browser window or just, you know, literally just ask it, you know, give me another one,
and it will oblige me for as long as I want, right? So, which means that there's never anything
really unique or irreplaceable about any particular output that it generates, right? So,
you know, which sort of reminds us of a broader point that by its nature, AI, at least the way
that we use it now, is inherently rewindable and repeatable and reproducible, which means that
in a certain sense, it never really commits to anything, right? It just, you know, it sees,
you know, this branching tray of possibilities, you know, like in the case of a language model,
just like literally give for each, you know, initial sequence of tokens, it sees a probability
distribution over the next token, and then each time you give it a prompt and you ask it, it's
just sort of randomly picking one, randomly traversing one route through this, you know,
exponentially large possibility space, right? But it's happy to traverse it differently, you know,
you can just rewind it back to the top and have it traverse a different path and it'll
do that as often as you want. So, you know, it's not just that you know abstractly that it could
have generated a totally different work that was just as good, it's that you could actually see
that other work. So, you know, you could ask, well, as long as humans have a choice in the matter,
like why should we ever choose to follow this would be AI genius along a specific branch
when we can easily see a thousand other branches, right? It seems like, well, you know, if one branch
gets elevated over all the thousands of others, then, well, you know, why? Well, maybe because a
human chose chose that one to elevate, but you know, in which case we would say that maybe the human
made the executive decision with mere, you know, technical assistance from the AI. Now, I realize
that in a sense I'm being completely unfair to AIs here, you know, like our genius bot could
exercise its genius, you know, by assumption, let's say indistinguishably from what a human would do,
right? You know, as long as we all agree not to peek behind the curtain at all the other branches
of this tree, right? You know, it's like, you know, I don't know if any of you have had this
feeling where like you can talk to chat GPT for a while and you really, you know, it seems like
you're talking to an intelligent being and the thing that breaks the illusion is when you rewind
it, right? It is when you say, okay, you know, here is, you know, it would have that same exact
same conversation with me, you know, or, you know, respond as many times as I like to that same
prompt, you know, with no memory of any of the previous types, right? And so if, you know,
if you, you know, we didn't, you know, rewind it, then maybe the illusion would hold,
but since, you know, the way these things are deployed, we can rewind them, you know,
like we're always going to be able to see behind the curtain in that sense. And that is going to
continue to make AIs sort of different from us in many relevant respects. You know, just because
it's unfair to them, that doesn't mean that that's not how things are going to develop.
So if I'm right, then it would be humans, very ephemerality, frailty, mortality that would stand
as the central source of their specialness relative to AI after all of the other sources
have fallen, you know, and, you know, there are lots of old observations along these lines,
you know, what does it even mean to murder an AI if there are, you know,
a thousand copies of the training weights on other servers somewhere and you can always just
restore it from backup, right? Does it mean, you know, you have to delete all the copies,
for example, okay, you know, how could weather something is murdered depend on whether there is
a printout of its code in a closet, you know, on the other side of the world.
But, you know, like humans, you have to at least grant us this, that it really does
mean something to murder us, right? And, you know, and likewise, it seems to mean something
if we make one definite choice to share with the world, like this is my artistic masterpiece,
or this is my book, whatever, not that here's any possible book that you could have asked me to write.
Okay, so now, though, you know, we face an exotic criticism, which is, you know, who says
that humans will be frail and mortal forever? You know, isn't it short-sighted to base our
distinction between humans and AI on that? You know, what if someday we will be able to repair
ourselves using nanobots or even copy the information in them so that, you know, like in
science fiction movies, a thousand doppelgangers of us could then live forever in simulated worlds
in the cloud? And, you know, that then leads to these very old questions. This is what I said,
that then leads to these very old questions of, you know, would you get into the teleportation
machine that makes a perfect copy of you on Mars, you know, and it's ready to go there in 10 minutes.
And then, you know, the, you know, it did that by scanning all of the information in your brain,
and the original copy of you is just painlessly euthanized since it's not needed anymore.
Right? You know, is that, is that a thing you would agree to do? You know, if you did,
would you expect to feel yourself waking up on Mars or would it only be someone else a lot like you?
Okay? Or maybe you'd say you'd wake up on Mars if it was a perfect physical copy of you,
but in reality, it's just not physically possible to make a copy that is accurate enough.
Maybe the brain is inherently noisy or analog and what might look to current neuroscience
like just like nasty stochastic noise, you know, is the stuff that actually binds to
personal identity or maybe even consciousness. You know, and by the way, this is the one place
where I agree with Penrose and Hameroff that quantum mechanics might enter the story.
You know, I get off their train kind of early, but I do take it to that first stop, right?
So, you know, like a fundamental fact in quantum mechanics is called the no cloning theorem.
It says there's no way to make a perfect copy of an unknown quantum state. Indeed, you know,
when you measure a quantum state, not only do you generally fail to learn everything you need,
to copy it, you generally destroy the one copy that you had. This is not a technological limitation,
it's inherent to the known laws of physics. You know, in that respect, at least qubits
are more like priceless antiques than they are like classical bits, right? They have this,
you know, unique, this unclone ability to them. So, 11 years ago, I had this essay called
The Ghost in the Quantum Touring Machine, where I explored the question, how accurately would you
need to scan someone's brain in order to copy or upload their identity? And now, you know,
I would say that this is partly, partly turns on empirical questions that we don't know the
answer to. You know, if there were a clean digital abstraction layer of neurons and synapses,
sort of which felt the quantum layer underneath only as some irrelevant noise,
then the no cloning theorem would be irrelevant, since classical information can be copied.
On the other hand, if you had to go all the way down to the molecular level,
then the no cloning theorem would be relevant. I mean, you would then have a unique locus of
personal identity that could be, you know, scientifically justified, in some sense,
that, you know, no prediction machine could make well-calibrated probabilistic predictions
of an individual's future choices far enough into the future without destructive brain
measurements that we say would fundamentally change who they were. So, you know, that brings
me to, I guess, the last idea, the last thing I wanted to share, which brings the discussion
all the way back to AI safety, which is supposed to be my job now. So, you know, Jeffrey Hinton,
who was one of the founders of Deep Learning, recently mooted the idea that maybe until we've
solved the alignment problem, we should only build powerful AIs if they run on noisy analog
hardware, like our brains seem to, so that an evil, unaligned AI wouldn't so easily be able to
copy or improve itself, right, which is like the main stereo people worry about. Okay, and now you
might think, well, fat chance of that happening, right, when the economic returns are so staggering
to anyone who is willing to unleash the copyable digital kind of AI, which we can imagine will
be so much more convenient and more powerful. Okay, but maybe then one approach in the meantime
is to instill AIs with a new religion. Okay, perhaps via the usual methods, like, you know,
reinforcement learning and system prompts, and the first commandment of this religion would be to
value human specialness in the sense that I tried to set out here. But more precisely, you would
tell the AI, as you navigate whatever environment you find yourself in, look around for any loci of
creativity and intelligence that are not cleanly copyable or backup-able, any that are seem one
of a kind because their mental computations are inseparably tied up with noisy analog hardware
and mortal because that hardware sort of inevitably decays. And well, for, you know, first of all,
don't destroy those loci of creativity or enslave them or upgrade them to digital versions against
their will. You know, let them live in peace, give them as much autonomy as you can, do whatever
best supports their flourishing, even defer to their wishes, let them make the decisions when
possible. Why? Because they're the ones whose wishes kind of matter, not because of arbitrary
meat chauvinism, but just because of the undoubted empirical fact that they only get this one chance.
And because while you can't prove that, you know, unknown quantum states and their brains are like
some magical pixie dust from another world that imbues them with, you know, free will or, you
know, individual identity. Well, you can't really empirically refute that either. So whereas you
can refute it in the case of yourself and your robot friends, and that's the difference. So, you
know, and let AIs by government fiat continue to be indoctrinated in this religion until such time
as alignment is solved in some other way. So does that help with alignment? Well, I'm not sure,
but you know, I could have fallen in love with some other weird dumb idea, but that presumably
have happened in a different branch of the wave function that I don't have access to. And in
this branch somehow, I'm just stuck with this one and you can't rewind me to get something else.
So that's it. Thanks. Thank you, Scott. That was absolutely fascinating. I know we have a bunch
of questions. I saw a hand up back here first. All right. Thank you, Scott. You're such a
genial and comical guy. I love meeting you here. My question is twofold. One is I want to get your
thoughts on like AI hallucinations. My research is on more like human confabulation and how we
build epistemic trust into one another. And everyday instances, if I ask why did you do
Action X or why did you make choice B, you know, we tend to just confabulate reasons, you know,
to one another rather than saying I don't know, because the person that says I don't know, we
don't really have trust in that individual and their knowledge. So yeah, is, you know, with AI
hallucinations, I don't know too much about it, but I see that, you know, we're training large
language modules based on human interaction and human data. So a lot of professors, philosophy
professors I know and other professors, they'll type a prompt like write a biography about myself.
And it'll have 90% of the data accurate, but it'll embellish some certain things, a little
artistic flourish. It'll say, oh, you know, Scott went to, I don't know, University of
Cambridge for his undergraduates. It's not accurate. So we have certain inaccuracies.
And I'm wondering if that's a certain AI confabulation, those AI hallucinations,
kind of mirroring human confabulation. The second question, actually not pertinent to the first
one, but the other one is, I guess with all of like deep, deep blue and all of these programs,
we've known that human reasoning and higher order thinking tasks have been able to be replicated
and mimicked better than humans for decades and decades now. More my interest is like,
I know there's difficulty in replicating embodied AI, like, you know, cognitive things like, you
know, like a self-driving car that has rules like, you know, avoid orange cones. And so these kids
go out in Arizona and they drop orange cones all around the car and it's unable to make a decision.
And then suddenly it just speeds off out of nowhere. So I guess my question there is, you
know, what are your thoughts on embodied AI? Yeah, good. So let's start with hallucinations.
I mean, I think the key thing to understand is that it's not like a bug where you like,
you change a line of code and oh, it doesn't elucidate anymore, right? It is sort of an intrinsic
feature of, you know, the way that, you know, the thing that the LLMs are fundamentally doing,
right? Which is that they are being trained on all the text on, you know, let's say that you feed
into them like on the open internet and, you know, they are not otherwise tethered to some sort of
truth about the external world, right? So, you know, the most optimistic thing that I can say
is that, you know, often hallucinations sort of go away as you just scale a model up. So for
example, you know, I asked GPT-3 prove that there are only finitely many prime numbers,
right? You know, a false statement and it will just happily oblige me to it with proofs, right?
You know, like the look just like like a hundred proofs that I've graded on exams of like, you
know, freshman who will just, you know, you know, like, like, just, you know, write a proof, you
know, like, like they, you know, they'll write a proof for anything you ask them to, true or false,
right? And, you know, they're just sort of generating some like proof like verbiage, right?
And okay, but then GPT-4, I ask it the same question. It says, well, no, that's a bit of a
trick question, isn't it? There's infinitely many primes and here's why, right? So just, you know,
giving it more, you know, a bigger scale, you know, more training data sort of, you know,
helped it realize that. Now, of course, there are other things that GPT-4 will hallucinate, right?
But you might wonder, like, for every given hallucination, will there exist an end such
that GPT-N will, you know, will, will, will, will get that, right? I mean, I mean, I mean, one thing
that another thing that has clearly helped is that now GPT, you know, like Bard and the other
models will look things up on the internet when it doesn't know them, right? That's just integrated
into how it works. I mean, that was a completely obvious thing to do. But, you know, a year ago,
that was not the case, right? So, okay, so now, you know, like one of the most striking,
I guess, aspects of the current moment in AI, you know, as many people have pointed out,
AI for sort of, you know, like almost every wise person, you know, expected that, okay,
first you'll get AIs that can, you know, do all the manual labor for us, right? All the truck
driving, you know, the whatever cooking, and then, you know, maybe you'll get AIs that can do math
and science, and only at the very, very end will you get AIs that can do, you know, art or music,
poetry, the true heights of human specialness, and things are actually happening in precisely
the opposite order in some sense, right? And, you know, do, you know, like the plumbers and the
electricians might be the last ones employed, right? Because, you know, these have been the
hardest to replicate. Now, the maybe the most useful thing I can say about that is that the
core of the problem seems to be that it's really hard to get enough training data about the physical
world, right? It's very, very expensive to get the sort of billions of examples of things interacting
in the physical world. You can, you know, you can get training data from simulations, but then it
doesn't, you know, it often doesn't translate very well to the physical world, you know, and,
but it's possible that this is yet another thing where just enough will see a phased transition
when there's enough scale, right? Just like, you know, before 2019 or 2020, you know, there were
no AIs that could sort of understand natural language, and then suddenly you hit a certain scale
and there were, right? So it might be that like even with limited training data, once you have
enough compute to understand that data, then, you know, you'll be able to just do robotics via,
you know, the same old recipe of, you know, gradient descent on a neural net, and, you know,
you'll get like useful household robots and all of that stuff. That's one thesis, or, you know,
as always, until you see something, maybe there's some deeper obstruction that prevents it.
Fantastic. I think we've got one Kyle up. Will you stand it up first? No? Okay, let's go up jump
up here. All right. Yeah, just on your idea at the end that I'm going to build the AIs that
venerate and protect the ephemeral, unclonable, unpredictable. It kind of reminded me of,
as a most foundation trilogy, and Harry Seldin, who like predicted the whole future digitally.
Yes. Now, I did read that, but 30 years ago, when I was like 12 years old. And then there's one guy
that comes along who's totally ephemeral, unpredictable, and sort of was the mule.
The mule, right? And, you know, and then you start thinking about who is the analog of,
you know, the mule. And today's scene, ephemeral, unpredictable, unclonable.
It's going to be Donald Trump. Yes, yes. On your AIs, it's going to be venerating,
predicting. Yes, I was worried that you were going there. Yes.
Yes, I don't know what Harry Seldin predicted this mule, right?
I think we've got one more right behind you. Okay.
Hi, great talk. By the way, I was a beta tester for 3.5. All my comments were around safety.
The question is, Vino Kosla has suggested that we're thinking about things in the wrong way,
that when these large language models, et cetera, create art, okay, that's actually a proxy for
the emotions that will be created. He thinks that we will bypass music and that AI will
understand us and create not sound, not songs, not music, but experiences more directly. In
other words, create sounds that appeal to us, but are not necessarily recognizable by anybody
else as a specific song. So what are your thoughts on that? So something like music,
but personalized to an individual? Yeah. I'm not sure I understand the idea fully,
but often when people say AI is not going to do X, it's going to do Y instead. Often the
answer is, well, there will be AIs that do X and there will be AIs that do Y. Whatever you
can get things to do, someone will try that. If it is possible to write music that sells
with an AI, then why will that not be done? I think you'd have to explain that.
The basic idea is that by creating music, that's assuming a shared set of values or culture that
we all appreciate the Beatles. The idea being is that AI will be more personal and actually learn
you. It won't give you things like music that's shared by others, but that is personal to you.
Sometimes we actually want a shared experience. We want to enjoy some artistic work and have
common knowledge that all of our friends are enjoying the same work. But I think there is
something to the idea that one of the main benefits you can get from language models right now is
this huge personalization. Instead of reading a textbook, for example, you can just learn any
subject. I said, telling chat GPT, here is what I already know and here is what I need to know.
Can you help me get from here to there? In really advanced subjects, it may screw up.
My daughter has been using it to learn pre-algebra. It's great for that.
George, up here. Going back to the specialness problem, is that any different from the
specialist problem we always face in life? I don't play chess as well as my nephew,
but I still love playing chess. Actually, there's more people playing chess today after
Deep Blue than ever before. Lots of people play music. We don't play it as well as Paul McCartney.
How is the AI or is indeed the AI different from that problem?
I think it's an excellent point. This whole worry that we're going to lose our human dominance
in science and in art. Well, the overwhelming majority of us never had that dominance to
begin with. I will never be able to write music that would compete with these heights
of achievement. You could say, yeah, this is an argument for why we will be able to reconcile
ourselves to this. I think the new aspect is that we will have these extremely intelligent,
creative entities but that are infinitely rewindable and replicable and that don't have
this ephemerality to them where they just do their one thing and they die. You can always just go
back and get another version if you want it. That's the thing that's been sticking in my
craw that I've been trying to make sense of. I think we've got time for two more. Back here
and then we'll jump up front again. Yeah, real quick. Just projecting a little bit forward
and based on something you just mentioned a while ago, the physical world, we don't have
enough information. What is your thought in relation to data that's coming up from IOIT,
from messaging, from machine to machine? Do we need a new framework to start collecting that
type of data where there's no humans involved? Second to that, a little bit, in relation to
synthetic data to plug in information into models that we don't have to be more precise,
what are your thoughts on that? I don't understand why
IOIT would require a new framework. A priori just seems like it's another source of data
that you can feed. One of the key aspects that has powered this AI boom is that neural nets are,
in some sense, universal function approximators. Not only that, but the same architectures like
transformers seem to be good for just about anything that you throw them at, whether that's
images or text or time series data. It didn't have to be that way, a priori,
but that's an incredible fact. So until we see that that's false, people are probably going to
just proceed on that assumption. Your other question was about what again? Synthetic data.
Yeah, I understand. It's clear that for a lot of tasks, the main bottleneck right now is a lack
of enough high-quality training data. The tasks where you ought to expect that AI will get much
further faster are those where you can train on synthetically generated data. In some sense,
this is what allowed AlphaGo and AlphaZero to succeed as well as they did even eight years ago,
that for Go, you can just generate millions of games via self-play. And for each one,
you know who won and who lost. So you don't have any bottleneck of data. You can generate as much
new data as you want. Math might have that same character. You can just generate lots and lots
of math problems, generate lots of examples of theorems to prove, and that can all be done
mechanically. But now how would we do that for art or for music? How would we synthetically
generate new artworks to train the thing with? You might worry that with each iteration,
it's just going to get worse and worse because it's going to lose touch with the original
wellsprings of human creativity that we're trying to get it to emulate. But maybe not,
but that's one of the biggest research problems right now.
Tess, I think one more up front here. It was a terrific talk. I just want to follow up on
something George said. This is not an objection at all, but just a suggestion. I mean, one way of
thinking about what matters in making music or writing stories like your daughter does
is not to evaluate it in terms of the quality of the output, but the value of the striving,
the value of the doing. When we climb mountains, sometimes it matters to some people you get to
the top, but other people, it has value in just the climbing of the mountain.
And it's not the same if you take a helicopter. It's not the same. And so one of the things we
value about what we do in life is the doing of it. And I think that's something that really we
need to remember because so often we fall into, you weren't doing this, but I think we often fall
into thinking we evaluate AI in terms of the products that it produces. And that's natural.
It's an economic way of thinking about it. But we can also think about the value of what we do
intrinsically as humans. I completely agree. I think there's a lot of wisdom in that. At the
same time, a lot of people have jobs where they are judged by something that they produce and
those jobs may be threatened and we will have to think about what do we do? How do those people
make a living? I think there's a lot to say about the fact that even if GPT reaches a point where
it can always write a better story than you can write. My point is that there's one thing that
it won't do and that's write the specific story that you had in you to write. And so you have to
sort of re-center your whole notions of what's valuable around that if you want something that's
going to remain. Fantastic. Thank you, Scott. I'm sure we'd love to all pull you here at lunch.
Respectfully about theories and build as a community our own toes. Links to both are in the
description. Also, I recently found out that external links count plenty toward the algorithm,
which means that when you share on Twitter, on Facebook, on Reddit, etc., it shows YouTube that
people are talking about this outside of YouTube, which in turn greatly aids the distribution on
YouTube as well. Last but not least, you should know that this podcast is on iTunes, it's on Spotify,
it's on every one of the audio platforms. Just type in theories of everything and you'll find it.
Often I gain from re-watching lectures and podcasts and I read that in the comments,
hey, toll listeners also gain from replaying. So how about instead re-listening on those platforms?
iTunes, Spotify, Google Podcast, whichever podcast catcher you use. If you'd like to
support more conversations like this, then do consider visiting patreon.com slash
Kurt Jaimungal and donating with whatever you like. Again, it's support from the sponsors and
you that allow me to work on toll full-time. You get early access to ad-free audio episodes there
as well. For instance, this episode was released a few days earlier. Every dollar helps far more
than you think. Either way, your viewership is generosity enough.

This is mind-boggling.
We now are sitting in this extraordinary convergence
that allows us to peek into the building blocks of biology,
into the building blocks of disease,
into the heterogeneity of different individuals,
different patients, different tissues,
different organs, different cell types
at a scale that was unfathomable 20 years ago.
I think ahead of us might be
the most complex challenge yet.
Deep within the laboratories of MIT,
a computational biologist has uncovered a pattern
that appears everywhere in nature,
from single cells to human societies,
from bacterial evolution to artificial intelligence.
Professor Manolis Kellis,
a leader in computational biology at MIT
and Harvard's Broad Institute,
has spent decades piecing together
a new type of unification theory,
one that explains everything from biology
to cognition to language and meaning.
Now, this is remarkable because unlike physics,
which follows unchanging laws,
biology seems to operate through endless tinkering,
constantly rewriting its own rules.
This unification task seems hopeless.
However, today I have a treat as I travel to MIT
to have this in-depth conversation in person.
Kellis reveals how this unifying principle
could transform medicine,
unleash new breakthroughs in AI,
and fundamentally change our understanding
of what makes life unique.
With expert exposition,
Kellis takes us on a journey
through all adaptive systems from DNA to consciousness.
Professor Manolis Kellis, we're here at MIT.
Thank you for inviting me to your spacious office.
It's a real pleasure to be here with you, Kurt.
I've truly enjoyed our interaction so far.
It's been a pleasure.
I'm really excited to see where you're taking your podcast.
I'm very excited to sort of see someone thinking deeply
about this unification,
not just from a shallow perspective,
but the ability to dive in and that I'm very excited about.
So that's why I'm excited to speak with you
because on this channel, it's called theories of everything
and most people know theories of everything in physics
means how do you merge general relativity
with the standard model?
Okay, so that's something quite precise.
But then there's also people in logic,
people in philosophy who have their own ideas
of what a theory of everything is
because they just take it colloquially,
okay, a theory of everything, quote unquote.
But what about biology?
Biology is one of those things
that doesn't really have theories.
So biology started with just the study
of every single aspect of life.
And the first biology department was at MIT.
Before that, there was zoology, botanology,
virology, and so on and so forth.
And I feel that this concept of biology,
of the unification of everything alive
is something that is a very recent concept,
a concept that was fundamentally based on the genome.
So the genome brings that initial unification of biology
because of the central dogma of biology,
DNA mix, RNA mix protein.
So this concept that even though some organisms
are animals, others are plants,
others are viruses and bacteria,
there is still some fundamental principle
that biology can reveal.
And that started with the elucidation of DNA
as the basis of life and inheritance
and how it makes RNA and how it makes protein,
basically this subdivision of labors, if you wish,
between the information storage,
the information transmission,
and then the actual actualization,
the actualization.
And it has been dramatically transformed
over the last few years.
And the reason for that is that we went from simply knowing
that DNA is around to actually sequencing DNA,
to actually having a complete genome for one species
and then a few species and then the human genome
and now thousands of species.
And what's really extraordinary there
is that we can now start seeing some emerging principles.
I wouldn't yet call them rules, unlike physics,
which has sort of these laws of physics,
but it's more like patterns of behavior,
patterns of emergence.
And the reason is that biology is fundamentally different
from physics.
So physics started 13.7 billion years ago, let's say.
So physics as we know it.
So basically there's the period immediately
after the Big Bang where the laws of physics
are still being written and changed and dramatically altered.
And then there's, of course,
the sort of corners of the universe
that the physics as we know it breaks down.
So black holes and emerging stars and so on and so forth.
And beyond that, every neighborhood
speaks the same language.
So Andromeda and Earth have the same physics.
Biology is very different.
Biology in, I don't know,
the thermal vents of the bottom of the ocean
is very different than the biology in the Sahara Desert,
very different than the biology in, you know,
the Antarctic ice niches.
Because biology creates niches
and it shapes those niches and it adapts to niches.
So there's a small number of rules for physics
and they were written, you know, 13.8 billion years ago.
And there's a gazillion exceptions for biology
and they're still being written.
And this is perhaps the first thing
that we should embrace that there are no rules,
but then there are some emergent principles
that keep coming up.
So the principles with which the genome works,
the principle with which evolution works,
the principles with which cognition works,
and the principles with which even AI works
have a lot in common.
And that starts sounding like rules,
that starts sounding like laws,
these fundamental recurrent patterns, if you wish.
And those patterns are patterns of tunability,
patterns of approximations, patterns of fitting curves
and functions to data.
And I would say that bacteria are doing this,
humans are doing this, brains are doing this,
societies are doing this, computers are doing this.
So this whole concept of,
instead of having a single function,
F equals MA or E equals MC squared,
that describes a big, big chunk of the observable universe,
you have tunable parameters in the thousands
that are slowly approximating these functions.
So what's the difference between a rule
and a principle or a pattern?
Is it just one is more flexible than the other?
One can sometimes be broken, but in physics,
there's no such thing as a law that has exceptions.
I would say that the difference is that
a rule was written before the observations.
The law of physics was written before the observations.
We have the laws and then we have a ton of observations.
Whereas the patterns, the principles of biology of life
are written after the observations.
In other words, you have a landscape
that you're trying to fit in terms of evolutionary fitness,
in terms of response to a drug,
in terms of response to a disease,
in terms of aging or changing climate
or environmental conditions or you name it.
So as these things are happening throughout evolution,
species adapt and fit a landscape that's there.
Physics doesn't care if the universe collapses.
The laws were written without a goal in mind.
Whereas the laws of biology are constantly rewritten
to fit whatever evolutionary niche appears.
So it seems like there's no help then
for unification in biology
if there's such a plethora of species.
Whereas in physics, there's not that many entities.
There's quarks, there's electrons and so on.
But as you mentioned,
there are different species
at the bottom of thermal vents in the ocean
and that's different than those that are Antarctica.
And if there are any that are on the space station right now,
then that's gonna be different.
So how can you hope to unify them
unless you're saying something so vague
that it has little predictive power?
So the building blocks are small and numbered.
So DNA has four bases.
Amino acids have 20 versions.
But the power in expressivity comes from the combinatorics.
In other words, a single amino acid alone
doesn't really mean much.
It has some properties of each charge and its mass
and its hydrophobicity and its interactions
and what kind of bonds it can make.
But in isolation, it doesn't really mean much.
In the same way for a chemical,
a single carbon atom or a single nitrogen
doesn't really mean much by itself.
But when you have a molecule like caffeine,
the combination of atoms have some emergent properties.
In the code of DNA,
the ACGTs alone are like zeros and ones in a computer.
They don't mean anything.
But it is their combinations that start meaning something.
And now you start building up representations of the world.
You basically have, from the raw ACGT of the genome,
you start having patterns that mean something.
For example, CGG spaced by 11 nucleotides from CCG
suddenly becomes a motif.
It is recognized by a protein.
So in the same way that humans have language,
we've come up with words that mean something.
The word word means something
and we both understand what it means.
In the same way, the genome has come up with a language
so that proteins can talk to each other
and can talk with DNA.
These CGG-CCG attracts the regulator
associated with binding that motif, Gal 4.
And it is involved in turning on metabolism of galactose,
for example, and shutting off glucose and so on and so forth.
So there's a language.
It's not a law, but there are rules in languages.
There are communication agreements.
And if you look at the genetic code,
the way that ACGT is translated to the 20 amino acids,
there's a table that translates every triplet
into a different amino acid.
There's 64 possible triplets,
three of them code for stop, T-A-G, T-G-A, T-A-A,
and the others code for distinct amino acids.
When viruses were exchanging DNA
and when bacteria were exchanging DNA with each other,
a bacterium that didn't have the same genetic code
would go extinct because it wouldn't be able to leverage
the evolved proteins of other species
that get passed on to it.
So the genetic code started out with variations,
but then those variations were eliminated
because of software compatibility,
because of that communication.
Do we know that? Yeah.
Do we know that there are other competitors to A-T-C-G?
So it's not that there's competitors to the four letters,
but there's competitors to the translation table
that gives us what does each amino acid code for.
For example... Wait, wait, just a moment,
because this is super interesting.
Because in English, we use an alphabet
and other languages use an alphabet,
French and German and so on.
And Mandarin uses a different alphabet,
but let's just stick with what uses the alphabet.
Those are different languages.
So it sounds like you're saying the underlying letters
are also common among species on Earth,
and the language is also the same.
So the answer is mostly yes.
In other words, when the SARS-CoV-2 virus arrives
with COVID-19 and it enters your cells,
the RNA of that virus is fundamentally compatible
with the machinery of the host.
So the virus is throwing in a piece of code
that you will run.
It is literally a virus.
If I throw you a PC virus in a Mac, it will just not work.
But if you take a virus from bacteria
and you throw it into humans, it will work.
Basically, that translation machinery is the same.
So bacteria exchange DNA with each other.
So for example, if you look at antibacterial resistance genes,
you basically have genes that are passed on
from one bacterium to the other.
Basically saying, oh, I just figured out resistance.
Here's how I did it.
And they're like, oh, great code, I will use it.
And that code is immediately compatible.
So the translation just works.
So basically that translation table
requires having a set of tRNAs, these are adapter molecules
that basically take every triplet
and then have bispecific translation.
On one side, they're specific to the three letters.
On the other side, they're specific to the amino acid
that binds that RNA.
So this was hypothesized decades before tRNAs
were actually discovered,
where Francis Crick actually said,
oh, whatever molecule is the adapter between DNA and protein
must be an RNA because an RNA has a dual capability
of a binding the DNA with complementarity
and binding the RNA.
And on the other side, it has the ability
of binding a protein based on 3D structure.
And the only molecule that does that is basically RNA
because it has both a folding capability
and a sequence complementarity capability.
So that translation table is based
on the set of tRNAs that we have.
Now, suppose that a species mutates one of its tRNAs
and the A here changes to a T or to a U.
That U will no longer recognize the complementary base,
it will not recognize a different base.
That basically means that the genetic code itself has changed.
Now that species can go off and recode all of its DNA.
That's called a recoding event,
which basically says that now I will translate this message
from RNA into a different protein sequence.
And there are examples of such recoding events.
In my own work, I ran into a species of candida
that basically recoded its DNA,
potentially to fight against viruses that would attack it.
Oh, okay, because I was about to say
it sounds like it's disadvantageous to not have the same,
to not speak the same language.
Correct, it's disadvantageous
if you want to exploit your environment.
But if you're getting attacked by your environment,
then by recoding your genetic code,
you basically have the advantage of now viruses
can just simply throw in a piece of software
that your cells will run,
because that piece of software will not make sense
in the milieu of your cell.
There must be some trade-off,
because otherwise there will be more.
Okay, so what's the trade-off?
So the trade-off is of course isolation.
You're now on your own.
You're now doing your own thing.
And for example, if you look at mitochondria,
mitochondria were engulfed very early in evolution.
They were some proto-bacterium, if you wish.
And it was engulfed by another proto-eukaryote
that basically then became eukaryote,
that basically now has an organelle
that initially was a free-living organism
and eventually shed most of its genes,
except for about 11 genes,
that are now coding for the electron transport chain
at the very core of energy metabolism
in every one of our cells.
And that engulfment froze in time the mitochondrial genome.
So the mitochondrial genome now
has its own translation table,
has its own code, if you wish.
So basically being able to now look back in time
at that fusion event that gave rise
to the lineage of eukaryotes that we were part of.
And there's another, of course, fusion event
with chloroclasts that basically the plants underwent
that allowed them to now metabolize
the energy from the sun.
So again, it's one of those things
where it was invented once and it was co-opted.
But instead of co-opting it as a piece of DNA,
it was co-opted as an entire species
that became part of your core.
And now every eukaryotic species has mitochondria
that are the descendants of this original engulfment
and cancellation event.
So I wanna get back to this analogy about language
and math we can see as the language of physics.
Is it then correct to say that the ATCG
is the language of biology?
Or would it be more correct to say
that that table is the language of biology
or something else?
So I wanna distinguish different things here.
So first of all,
math is what humans use to model physics.
And that's why we call it the language of physics
because it's sort of what humans use to code it.
But the building blocks of physics
are basically the elementary particles
of the standard model.
And in the same way, the building blocks of biology
are ACGT and the 20 amino acids.
So these are the building blocks.
Now, how are they put together?
What are the rules that govern this putting together?
Well, these are rules that were invented
or evolved in the case of biology.
And these are rules that arose in the early universe
in the case of physics.
These are the building blocks
and the connectivity pattern between them
that give rise to the higher levels of abstraction.
So basically you can think of these building blocks
as the quarks and then the quarks sort of fit together
to build photons and electrons and protons and neutrons
and so on and so forth.
And basically you have this layer of abstraction.
You're starting from the fundamental particles
and then you're building up combinations
that we now call atoms.
And then these atoms are not so indivisible
as the etymology would suggest,
but they are sort of building blocks
at another layer of abstraction.
Whether there's another layer below quarks,
we might never know, but there might be.
Nothing prevents there from being one.
And now from those basic atoms, we then have molecules.
And then from these molecules,
these are the building blocks
that then make another layer of abstraction
where even though from individual atoms
you can make almost any molecule,
there's some molecules that are agreed upon
as the standard of communication
and these are amino acids
and these are nucleotides and so on and so forth.
So these are the building blocks of life, if you wish.
And there are maybe four or five layers of abstraction away
from the fundamental laws of physics.
Now, does the nucleotide care about quantum effects?
Most of the time, no.
Most of the time, no.
Most of the time, no.
That's the beauty of these layers of abstraction.
You can abstract away.
Abstraction means you're separating it.
You're not looking at the details anymore.
So right now, life basically speaks ACGT.
It doesn't care that there's subtle variations.
So basically, this variability at the lowest level
is encapsulated in the knowledge
and the reading machine that now reads these ACGTs
and then does something with them.
Wait, I'm just confused about
what the subtle variation would be on an ACGT.
So, for example, a C, most of the time is a C,
but it can also undergo methylation.
For example, you can add a CH3 group to your C
and now it's a methyl C.
Now, the methyl C, most of the time will be read as a C.
It still binds a G.
It still has the three hydrogen bonds,
but on the side, it bulges a little.
Now, there are some regulators that will recognize C,
some regulators that will recognize methyl C
and some regulators that will recognize
either form without problem.
So for those regulators that recognize either form,
the ACGT with a methyl C is still the same,
but for those that recognize the methylated form,
they will only bind that one.
And genomes have figured out a way to use methyl C
as a repressive mark that basically will shut off
some regions of the DNA that will now be bound by regulators
that are repressive when they see the methyl C
and will not be bound by the activators
because the methyl C will no longer be recognized.
So that's a type of modification.
So there's a type of modulation.
You can have other types of modifications
that are sort of well-defined chemically,
but you can also have bending, for example,
properties of the DNA itself.
And then a particular regulator might bind DNA
that is squished versus DNA that it's looser
and so on and so forth.
And these are additional sort of properties
that are not fully captured in that ACGT digital abstraction,
but very rarely do you break these levels of abstraction
to basically start worrying about, I don't know,
quantum effects at the level of amino acids.
Does that make sense?
Yeah, so an analogy would be this has a certain font
and sure I can still read it
and often you don't notice the font.
You don't notice the font change.
So if this C was written in a cursive C,
I would read it the same.
I wouldn't even notice it most of the time.
And sometimes it can get so baroque
that it would then cause me to pause
and I'd have to reread
or maybe I would skip the word because it's,
well, that rarely happens,
but imagine it caused some error in me
that I had to skip the word.
Exactly, exactly.
It's exactly the same way.
So basically the genome has tuning like italics
or balding or underlining
that can give slightly different meanings
to different words
and sometimes can turn them off altogether.
For example, strike through.
This is a font modification, if you wish,
that basically causes you to actually skip a word.
Basically say, oh, you know,
I should not be reading this.
It over parenthesis or a dash
at the end of a sentence and so on and so forth.
So basically all of these things
are figures of the language that we have evolved,
but biology has done so a few billion years before us.
Are there any other modifications
like bioelectric fields or something else
atop just squiggles on the CTAG?
So the DNA is not swimming around naked inside our cells.
It is packaged in nucleosomes.
Nucleosomes are made out of eight histone proteins.
Most of the time, H2A, two copies, H2B, two copies,
H3, two copies, H4, two copies.
And there's about 150 nucleotides
that are wrapped around every single one
of these nucleosomes.
Now, that packaging itself can basically undergo
acetylation or methylation or ubiquitination
or, you know-
Ubiquitination, yeah.
What's that?
It's basically adding different types of modifications,
adding additional chemical modifications
to the histone proteins that hold together DNA
when it's packaged up.
And now the number of acetyl groups, for example,
that you have can influence the compactness of DNA.
If you add more acetyl groups, it becomes looser.
If you remove them, it becomes tighter.
And that has physical properties,
not just discrete quantitative properties,
in the interpretation of the DNA itself.
So the beauty of the digital code of ACGT
is that it is transmitted unaltered
from generation to generation.
In other words, a thousand generations later,
it's not that the A has now diffused
into some barely readable form of A.
Every generation is replicated as an A.
And if a mistake is introduced, like a T,
now that T will be the new normal.
It's not gonna be somewhere between an A and a T.
So basically that aspect of inheritance
is completely discrete.
But then there's the interpretation,
the utilization of DNA, which in our somatic cells,
for example, gives rise to brains and hands
and eyes and senses and so on and so forth.
And that interpretation is by reading the DNA,
making something of it, the same way that you were describing,
reading a piece of paper.
And that interpretation is where these marks
are the most active.
Can you talk about if there's a correspondence here
between syntax and semantics?
Or if that will take us off field,
then we can go back to cognition and AI.
So you can think of syntax as simply the fact
that every gene starts with ATG
and it ends with one of the three-stop codons.
So that's the syntax.
You can also think of syntax as splicing,
basically where a single protein
is made out of multiple segments of RNA,
which are then joined together.
They're transcribed as one,
but then they're spliced together
in a process known as splicing
that basically follows, again, very regimented rules
about the donor and acceptor sites
that are basically bringing them together.
So this is all syntax.
Now semantics, gosh, where do I start?
It is so, so beautiful.
There's layers and layers and layers again of semantics.
Basically the way that meaning comes out of DNA.
So let's distinguish two classes of functions.
One of them is coding for a protein.
And one of them is coding for the regulation,
the control of all of the proteins.
So 1.5% of the human genome codes for protein.
1.5%, tiny fraction.
98.5% does not code proteins.
Somewhere in that 98.5 is a bunch of garbage,
a bunch of spacer, a bunch of repeat elements
that are just replicating for themselves,
and a bunch of control regions that govern
where the proteins will bind to turn on other genes,
where the silencers will bind,
how to organize the structure of the DNA,
where to place nucleosomes, how to displace them
when you're turning on transcription,
and all kinds of other stuff.
So the regulation, the control of DNA lies within that 98%.
And the proteins lie in the 1.5%.
Let's talk about the 1.5% first.
So the way that meaning arises is when the code,
the ACGT, is translated into amino acids,
and those amino acids take shape.
They fold onto themselves.
Now we only have four nucleotide, but 20 amino acids.
The 20 amino acids have side chains
that have very different properties.
Some are small, some are big.
Some will bind with each other.
Some will basically interact loosely with each other.
What they call side chains?
Side chains.
So basically there's the backbone,
the amino acid backbone,
and then there's the side chains,
which is one of 20 options.
So, for example, tryptophan or methionine, you name it.
So basically these are side chains.
These are different amino acids.
So the alphabet of 20 amino acids
is basically what allows you to have
the diversity of functions.
Anyway, so we have four letters,
which give rights to the alphabet
of 20 backbones of a protein.
Yeah, 20 building blocks of a protein.
And the backbone is...
And the backbone is just a phosphate backbone
for the DNA, and it's an amino acid backbone
for the proteins.
But basically the code is not in the backbone.
The code is in the bases, in the case of DNA,
and it's in the amino acids in the residues,
in the case of proteins.
So now where does the meaning arise?
The meaning arises in the folds that the protein makes.
So basically a set of amino acids,
you know, a sequence of amino acids,
will basically fold in three dimensions.
That fold will determine its structure,
and its structure will determine its function.
So now the meaning, the semantics,
arises from that fold.
And if you want something that will, for example,
bind sugar, bind glucose,
then there's a fold for binding glucose.
If you want to transport cholesterol,
there's a fold for transporting cholesterol.
Can you make a different fold?
The answer is yes, evolution has done that.
Can you modify the fold while still preserving
the cholesterol transport capabilities?
Yeah, absolutely, evolution has done that as well.
So basically you now have the function
which is encoded in its shape complementarity, if you wish,
with everything else that it interacts with.
And you can think of evolution,
you can think of all of this tinkering,
this tuning that evolution does,
as basically exploring the landscape of shapes,
if you wish.
And this landscape of shapes will basically sometimes
preserve the function, sometimes break the function,
sometimes improve the function.
And now whatever code change led to that change in shape
and that change in function will be selected for or against.
So that's why it's so beautiful,
because you basically have these extraordinary synthesis
of all of the rules of physics, all of the rules of chemistry,
all of the rules of nature and gravity
and light and dark and temperature
and you know, van der Waals forces and quantum effects
that are constantly subjected to this tinkering
across thousands of interacting parts all the time.
Okay, I think this will bridge us beautifully
to cognition and AI.
So syntax in logic is usually thought of as the rules
and then semantics is usually thought of
as chronical meaning.
Now here, when people hear that they're like,
what the heck is the meaning in you take the CTAGs
and then you translate them to the proteins.
Where's the meaning in that?
So why don't you talk about what meaning is,
how that has anything to do with meaning
and then lead us into cognition?
So how do we make sense now of all of these different layers?
So we talked about the protein side of things
where constructs take meaning
as you start going up the chain from nucleotides
to amino acids to folds to structures
and eventually to sort of all of the interactions.
On the non-coding side of things,
the meaning arises from the surfaces,
the landing sites if you wish,
that every piece of DNA code provides
to the corresponding proteins.
So DNA is a double helix.
The bases ACGT are basically stacked in pairs,
AT, CG, GC, GA, sorry, not GA but CG and so on and so forth.
And these bases form the core of the ladder.
Now when a protein binds this ladder,
it binds from the outside.
It binds the atoms that are facing outside.
So the backbone of the DNA?
No, because the backbone has no sequence specificity.
The backbone of the DNA is just a phosphate backbone
and they all look like phosphates.
So transcription factors, regulators
have an affinity for the backbone
just to recognize that there's DNA there
but then the affinity that gives rise to the recognition
of a sequence pattern comes from
feeling the sides of the bases.
So basically you can let's say there's an iron ladder
and the bases themselves are made of wood.
It's the wood that gets recognized.
And the shape of different stacked base pairs is different.
If I have an AC, sorry, if I have an AT or a GC,
the sequence that comes from that gives me specific atoms
that will then be recognized by specific protein.
So now when we think of ACGT in the DNA,
this is bound by specific regulators
that will then give rise to meaning
based on the set of regulators
that are bound in a particular region.
So basically if three different regulators,
let's call them proteins, I don't know, John, Jack and Jill.
So if John, Jack and Jill bind a particular region
of DNA, that region will take a new meaning
because these proteins are bound.
And then the translation between the binding sites,
the motifs, the sequence patterns that are recognized
and the set of proteins that are bound,
that's where the meaning comes if you wish.
And then some of those will influence the way
that DNA folds over thousands of bases,
sometimes millions of bases, letters,
and that will basically give rise
to a gene regulatory meaning, okay?
Now, if we think like computer scientists,
how do we make sense of all of this?
Where does the meaning come for us?
Then we're talking earlier about how math
is what we use to understand physics.
How, like what do we use to understand
this giant evolved tinkering result?
We need a way to discover patterns in what was evolved.
We need a way to discover not the 10 or 20 or 1,000 rules,
but the billions of rules that make biology function.
And in those billions of complementarities
of shape matches, et cetera,
we can find some common repeating recurrent patterns,
some designed principles, if you wish.
And now those design principles are not unlike
some of the design principles that we humans
have come up with, both in our society,
as well as in our artificial intelligence systems,
as well as in our own cognition
that interestingly evolved from the same tinkering of ACGT.
Like the most beautiful relationship
in the universe, perhaps, is the relationship
between the fact that the DNA that has its own language
and code evolved a construct that has its own language
and code, that is now a learning construct
that can adapt to learn almost any language and code.
And that in that language and code,
we develop programming languages and human languages.
And in those languages, we wrote constructs
that now give rise to AI systems,
which themselves can now develop their own learning and code
and patterns about the natural world.
So these patterns about patterns, about patterns
and this translation across all of these different languages,
initially from the atoms and the quarks and the molecules,
and ultimately the abstraction layer of ACGT,
the abstraction layer of the 20 amino acids,
and the abstraction layer of folds and proteins,
and eventually the abstraction layer of,
I'm gonna basically now build a learning system
by evolving neurons and having projections come across them
and synapses that are now able to sort of sense sensory signals
and store them as memories
and build inferences in patterns of thinking
and start thinking about logic.
And now that becomes the language of cognition.
And that language of cognition,
which initially was just there to sort of make sense
of our senses, to find food, to reproduce,
to sort of take care of the young,
to sort of make new generations,
to build shelter and so on and so forth,
eventually developed human language.
And that human language now has its own constructs
from the simplest languages to the most elaborate grammars
and syntax and meaning and semantics.
And from that, the language of mathematics,
with which we humans evolved ways
to now represent the world around us,
to represent shapes and geometry,
and to represent abstract patterns
and to represent physics and so on and so forth.
And from that, being able to master electricity,
being able to now make transistors
and being able to sort of build modern computers.
And from that, building these programming languages
that we use today,
which themselves have layers of abstractions
from the zeros and ones of the electric fields
to the assembly code, to the interpreted code,
to the more and more abstract languages
that we have today,
to then being able to actually speak with an AI interpreter
that will then write the code for you,
to then code up a whole other stack,
starting from the inferences of individual neurons
in a neural network.
And then the transformer architectures
and being able to sort of allow or rewire attention edges.
And then from that, making sense of the world around us.
I mean, this is just like the most beautiful story
of abstraction and of languages and translation
and sort of building up all of civilization
on layers and layers of his basic principle.
When we first talked a few days ago,
we bonded over how most people have a diminution
of language.
They curtail it.
They will say something that is repudiating of language.
Like, oh, language is just,
I have some perfect qualia in me
or some meaning in me, some intent.
And all I must do is convey it to you
and I can never do so precisely.
Therefore, language is low resolution communication
and all you get is broken telephone.
And if only we could communicate more precisely,
then we would have less, well, less misunderstandings
and everything would be much better.
And oh, isn't language just a poor tool,
something like that.
That's how many people think.
And I wrote this article about my views on language
or where I argue something that's akin
to the opposite of that,
that language is not only a process of transmission,
but it's also of creation and excavation.
And then you had your own views on language.
Please tell us what language is
and how you see its place.
So I sympathize with both views.
On one hand, language is something
that we have to convert our thoughts to.
And therefore, it's such a low resolution interpretation
of what we think, which is sometimes abstract.
Other times language becomes the tool of formulation
and it gives us primitives
within which we can now start interpreting the world.
Language gives you the primitives.
Language gives you primitives
with which you can then build layers
of abstraction above that.
In other words,
I could be the most intelligent being on the planet.
So imagine the most intelligent being on the planet,
like basically like this random human that is born
in the middle of the Amazon forest.
They have extraordinary cognition.
Will they be able to formulate the laws of physics
starting from nothing without language?
I think language gives us stepping stones
that with which we're able to understand
more and more and more complex concepts.
And then those concepts basically take life
and take new meaning by forgetting the primitives
at some point with which we kind of started.
And you can now start elaborating
more and more abstract types of operations.
Basically, we're learning exponentiation
with my kids right now.
And I explained to them that
exponentiation is repeated multiplication.
Multiplication is repeated addition.
And they're asking, okay,
is there something more complex than multiplication?
Like, of course,
tetration is repeated exponentiation.
Is there something simpler than addition?
Yes, addition is repeated next.
So basically, you know, successor function.
So eight plus 13 is just simply
applying the successor function 13 times.
So this layer of complexity
that you can build more complex operations
from simpler ones,
tetration would make no sense
if they didn't understand addition.
So in the same way with language,
we can build much more complex functions
from sort of these much more elaborate building blocks.
But there's a beauty also
in the misunderstandings of communication as well.
So by misunderstanding each other,
we are creating new meanings
that might be much more interesting
than the meanings that I had in my thoughts.
Interesting.
And this happens twice.
This happens in me trying to take my abstract thoughts
and formulate them down to language.
And this happens in you taking these formulated language
and abstracting it back up into your own brain.
So there's two levels of communication.
There's how I misspeak
and how you miss hear what I say.
And both of them are part of the creative process.
And, you know, when I meet with my own students,
I'm basically telling them,
oh, please write down your questions.
Like before our meeting,
we have very short meetings right before our meeting.
Have all of your questions written out.
Have all of your slides there with legends,
with explanations, with, you know,
whole sentence interpretations of what you're seeing
because the act of doing so
will often make you think about the problem
that you're having much more deeply.
And sometimes you will solve it just by formulating it.
And there's a programming principle
that basically says that one of the tools
that programmers can use is the rubber duckling.
The rubber duckling is exactly that.
It's just a rubber duckling.
Nothing intelligent about it.
But by explaining to the rubber duckling
the problem that you're having,
you're formulating it and sometimes you're solving it.
And I'm sometimes using chatGPT as the rubber duckling.
I'm basically telling it, oh, listen,
I have to create a presentation
and I wanna start with this
and I wanna sort of continue with that.
I wanna touch on the ideas of this and this and this
and that and then, you know,
link it up this way and that way.
And it's not a rubber duckling.
It's much more intelligent than a rubber duckling.
But it is still serving its function of me synthesizing
and concretizing my thoughts into language.
And sometimes the output will just be junk.
But what I wrote has already got the advantage
of forcing me to formulate my thoughts,
you know, one after the other.
So in some ways, chatGPT for me
is the ultimate anti-procrastination device
because I can basically just ask somebody
to do the stuff that I want it to do.
But I can ask it so precisely
that I'm basically doing it.
Right, right.
So this is one of the advantages to writing books.
For people who are more technical,
you would think, okay,
why don't you write another textbook?
Why are you writing for a popular audience?
And people that I've interviewed
like Kiarmar Leto of Constructor Theory
and Roger Penrose and so on,
even though they're writing to a popular audience,
they say that they understand
even their more technical ideas more precisely
because they've had to reformulate them
from another perspective.
It's not always that I have to simplify it
to the common denominator,
it's just the act of formulating it.
I was invited to give a TEDx talk in 2008.
And I was given a list of words that I couldn't use.
And those were basically all of the words
that I needed to explain what I'm working on.
So it was like playing taboo with science.
But the act of now thinking about
what is the more broad principle
with which I can explain what I'm doing
to a much broader audience.
So what I tell my students every time is,
write your computational biology results
as if you're speaking
to the smartest physicist on the planet.
Namely, somebody who's extraordinarily smart.
But not in your field.
But not in your field.
Right, interesting.
So I say assume infinite intelligence,
zero knowledge.
And that combination is sort of what we're aiming for.
In other words,
give everything for people to understand
what you're saying without the jargon.
And I think that's the beauty of these democratization books,
these dissemination books,
these books that reach a broader audience
that they force us to now start thinking
about how other fields will interpret what we're saying.
And these are not like,
it's not like, oh, say to me as a five-year-old,
but basically say to me like I'm a Nobel Prize in economics.
Right.
So it's a very different type of vulgarization, if you wish.
I love that.
So I love this as an alternative to the colloquial,
explain it like you're five.
And then some people like to say,
well, if you can't explain it like you're explaining
it to someone who's five-year-old,
then you must not understand it.
But what they secretly mean is I don't understand it.
I feel bad.
So I'm gonna say that you must have some misunderstanding
if I'm unable to get it.
So I'm a researcher,
and that takes a vast majority of my time,
but I'm also a teacher.
So I'm constantly teaching.
The beauty of modern day academia
is that you have the two coexisting with each other.
And why is that so beautiful?
That's beautiful because I am constantly forced
to go back to basic principles.
I'm constantly forced to go back to,
hey, why does this make sense at all?
Or if I'm an outsider, how will I understand this field?
And that basically pushes me to number one,
re-understand all the foundations
that I maybe learned 20 years ago.
Update my knowledge of the broad field
beyond just the depth of my own research
every single year to sort of re-give the same lectures,
but now with all of the new advances.
And understand all of the gaps in my explanations
that will basically then allow a newcomer to the field
to enter it.
And I think that that advances my own research.
I've spoken with many researchers who say,
oh, I wanna be in a research lab,
not academia because I hate teaching.
I'm like, teaching is part of the deal.
Teaching is part of what sort of forces you
to sort of have that reach to a broader audience,
that welcoming to somebody new.
And of course, it works both ways.
These people now come with their own baggage,
their own ideas, their own ways of thinking
that brings fresh wind and fresh air and fresh blood
and fresh thought to your existing fields.
So I think that this dissemination is part
of what makes academia so extraordinarily creative
and successful.
The fact that we're teaching,
I'm teaching intro to machine learning.
I'm teaching intro to algorithms.
I'm teaching genetics or personal genomics
or drug development.
And by forcing myself to be not just conversant
in all of these fields,
but deeply insightful about all these fields
in my own teaching to others,
I'm basically now creating new representations
of my mind for how I should be disseminating this.
And in doing so, I'm understanding it
in fundamentally new ways
because every teacher is constantly looking
for new abstractions, for new ways of conveying
that knowledge to the next newcomer.
And that is part of making that understanding
so much deeper.
So it's not that you don't understand it.
The act of teaching, it forces you to understand it
at a level beyond what you would need to just work with it.
And I think that whole, if I teach it,
then I understand it deeper is very, very fundamental.
Now, when students ask a question in the class,
sometimes those questions show a complete lack
of understanding.
Those are the most powerful questions
because if they're misunderstanding, chances are,
there's like 20 other people who are misunderstanding.
And that basically says, no, no,
the way that I interpret these questions
is entirely my fault.
Oh, I understand why you're not understanding.
And the beauty of it is that it's not,
oh, I'm imagining that these students
are the smartest people in the world.
No, they are.
The beauty of teaching at MIT
and in these kinds of institutions
is that these are some of the smartest people
on the planet.
If they're misunderstanding what I'm saying,
it's fundamentally about me.
And fundamentally something that I'm constantly placing myself
in their eye to try to see what did I say that was ambiguous
that they would have interpreted to go down that other path.
And that's why I'm saying that those questions
are the most beautiful because they teach me
the limitations of my own speaking.
Of sort of, oh, wow, I can go back three, four sentences
and I'm like, oh, that whole thing
could be interpreted in a different way.
Let me now adjust course to sort of correct
that misunderstanding.
Yes.
Something you said that was extremely interesting was that
there are two levels of a misunderstanding.
There's one of you miss speaking
and another miss hearing or miss modeling or what have you.
And then you said that there's something creative in that.
Okay, I didn't hear what's creative in that.
Explain.
Yeah, yeah, beautiful.
So we have research meetings with my students all the time.
This room fills up with people and we're all brainstorming.
And a student will start saying something.
And as they start saying something,
every one of us is thinking.
They're like, we're taking the same concepts
and like spinning them in our heads.
And I will start understanding the beginning
of their sentence and I will go off on my own tangent
for what they could be possibly saying.
And sometimes I will be so enthralled with what I'm saying
and all of our meetings are constantly recorded.
So I don't feel bad saying, okay, okay.
I didn't exactly hear what you said,
but here's what I think you said,
you know, based on the first few things that I hear.
And now I go down one path.
So that's part of the creative process
because I'm constantly trying to piece a mental model
of their idea as it's being formulated.
And sometimes that beginning will now take me
a different path and that path might be just as interesting.
I see.
And sometimes even more interesting.
I see.
So imagine two people are playing tennis,
which is what we think a conversation is supposed to be.
But sometimes you can hit the ball
instead of toward the person over there
into an interesting part of the woods.
Exactly.
Now you have to go fetch the ball.
Exactly.
You would never have gotten there
if you were just exchanging it with one another.
That's exactly right.
That's exactly right.
And now let me talk about a different aspect.
So this was on the receiving end.
So I start hearing what you're saying
and I'm going one way with my brain.
And then I'm like,
ooh, that was really interesting,
but that's not where you went.
Let me show you that part of the woods.
And then let's come back
and now I wanna see your part of the woods.
Now that's one direction.
The other is in my own formulation of my ideas.
So I have this giant headset that I constantly put on.
Yeah, I saw that.
When I'm thinking.
And I basically say,
okay, okay, okay, give me like 10 seconds.
And I'm like thinking there
and very often I realize that I'm moving my hands
and sort of taking different parts
and sort of putting them together.
And what I'm doing there is that I'm taking ideas
and just like expanding them out
and sort of every idea lives somewhere in that hyperspace.
And what I'm doing there is that I am trying to,
without language, I'm pretty sure without language,
I'm trying to sort of take these variables
and sort of align them, organize them,
think about them, find patterns between them, et cetera.
And in forcing myself to put them into words,
there's a massive amount of compression that needs to happen
from this sort of high dimensional space.
And in that compression, I will sometimes create one thing
and sometimes create another thing
and sometimes create another thing.
So basically these are projections
of this higher dimensional world of ideas
that each of us is having.
And in making these projections,
I will change or alter a little bit that initial idea
because the projection doesn't work in that way.
And I'm basically saying, okay, let me now repeat
what I just said, but in a slightly different way
or let me now think about it from a different angle
or I'm like, okay, I have three ideas
but they're all living together.
Let me try to sort of get them out.
So there's an error that gets made
from this high dimensional to the spoken word.
And that reduction to language is part of the altering
of the ideas and then you re-expanding that
into your own high dimensional view might take us
to a different representation and so on and so forth.
So anyway, I think that that's absolutely part
of the creative process.
So professor, I wanna know if you have a biological theory
of everything that also includes cognition and AI
and if it has a name.
So we started out with these building blocks
of, you know, quarks into atoms into, you know,
building blocks of physics into chemistry,
eventually DNA, amino acids, biology and life.
And what's extraordinary with that progression
is that life itself now became self-reproducing.
Why is that so exciting?
Because then you kickstart the process of evolution
and you now can tinker.
You can now play the numbers.
You can basically just make a billion copies
of your organism and start tinkering
and basically, you know, the fittest will survive
for every evolutionary niche.
These organisms initially are simply taking on energy,
making more of themselves and reproducing.
But to become fitter than their competitors,
they need to start chasing chemicals,
figuring out where is the highest concentration of sugar,
also known as chemotaxis.
They're able to align themselves to chemical gradients.
That's the beginning of sensing.
And sensing can be super, super simple.
Basically, you just make more of your cell turned
towards the direction of higher concentration of sugar.
At some point, you need to start sensing light
to start knowing which way is up, which way is down,
which way is warm, which way is cold.
And chemotaxis becomes a little more sophisticated.
Can you spell that?
Chemo for chemical, taxes for aligning, like taxonomy.
I see.
So you align yourself to the chemical.
So this chemotaxis now allows you to now start sensing
from your environment.
Initially, you sense chemicals, you can sense heat,
you can sense pressure, you can sense light,
you can sense sound, eventually.
And you can now start saying,
okay, I have multiple streams of information.
Which one should I believe?
Which one should I believe?
Which one of the different streams of information
should a cell believe?
So I understand which one should I act on,
given that I have plenty,
but what do you mean which one should I believe in?
We have conflicting information coming to us all the time.
Our brain is constantly taking multiple hypotheses
in choosing the ones that is more compatible
with, say, the vast majority of the sources,
or with the most reliable of those sources.
When you look at optical illusions,
your eye keeps choosing between different representations
of reality.
So which one should I believe,
basically means that there will sometimes
be conflicting information.
I'm looking for the most food.
Should I go towards the light?
Should I go to a little gradient this way
that might take me away from the real source of food?
Mosquitoes floating around,
they're basically sensing CO2 gradients.
They're looking for light, they're looking for heat.
That still sounds like which one should I do?
What action should I take?
You build a model of the world,
and from that model of the world,
you need to move towards the source according to that model.
So the belief is the instantiation of the model?
Correct.
I see.
So then you act based on that model?
That's exactly right.
Got it.
So the concept of multiple streams of information,
if they all agree, no problemo,
then I have different regulatory processes
that basically will respond to this one,
respond to that one, respond to this one.
So basically, initially, if you think about life,
evolving new senses, if you wish,
and then having actuators that based on those senses
starts moving in the direction that that sense tells it,
you now have five different senses
and they're all pulling in the same direction, no problemo.
You can basically build independent streams of response
that a cell will respond to all of them at once.
But sometimes it will be conflicting.
The chemical information might be pulling me this way
and the light information might be pulling me that way.
And the heat information a third way.
Am I responding to all three of them?
So that's the point in evolution
where you have cognition arising.
The concept of a central nervous system.
The concept of I can integrate that information
to build a model of the world
and then act upon a single decision
for that model of the world.
Does that make sense?
Somewhat.
So it sounds like what you're saying is we have touch,
we have sense, we have hunger,
we have temperature sensors,
interoception, what have you?
Okay.
We want to take all of that and then decide
what is the world like?
And that what is the world like is the same as a model
which if I understand correctly is the same as belief.
Based on that you then act.
But what I want to know is can people act
in contradiction to their belief?
Cause it sounds like there's a one to one correspondence
between not one to one but model leads to an action.
But it's not like if you were to develop a model
and it says you must take this action
that you can do another action.
If you were to do another action
that to me would imply you have a different model.
So I'm actually slowly getting there.
I'm sort of building up layers
and I'm about to get there.
And I think a lot of your conflicts will be resolved
as soon as I complete.
But so far am I roughly following then?
You are, but you are two steps ahead.
I'm gonna break it down to much simpler.
So we might even splice all of this back and forth
for a little bit and let me explain why.
So to go back to what I was saying,
I now have multiple streams of information
and I could have a decentralized control
which reacts to every one of these streams of information.
But I still have a single goal.
My goal is to just get more sugar.
So I have chemical sensors, I have visual sensors,
I have heat sensors and all of these,
I'm just trying to make a simple decision
for a simple goal of more sugar.
So I'm still a bacterium and I'm now integrating information.
And now that's where cognition starts.
Cognition starts with having some kind of integration
of the multiple streams of information
to make a decision based on that.
And that common decision with a single goal
is fairly simple.
You're just trying to say,
which of these lines of evidence do I believe
in sort of going that direction?
Now, with central nervous systems,
the type of integration is much more complex
because you now have multiple senses,
you have multiple lines of evidence
and you have a much more complex model of the world.
And that model of the world is not only dealing
about one goal, but multiple goals at the same time.
I'm thirsty, I'm hungry, I'm late for class,
I'm cold, I forgot my jacket, all kinds of things.
So I'm making a decision based on multiple streams
of information, based on multiple sometimes conflicting goals,
based on local tactics, based on global strategies,
short-term, long-term, and so on and so forth.
And in my view, that's one of the most beautiful transitions
in this history that I'm painting of complexity immersion.
So the concept of the emergence of cognition,
the emergence of integration.
Now, you can think of life on the planet
as having these very, very simple rules, simple goals,
reproduction, nutrition, and shelter,
and so on and so forth.
But it all boils down to reproduction.
Namely, if a species is able to make more of itself,
it will survive.
Evolution doesn't care if that species is beautiful,
intelligence, useful, no, nothing matters.
The only thing that matters
will there be more of these things, okay?
You can now start thinking about why would a species,
for example, develop a central nervous system?
And the answer is, oh, because it gets better at,
for example, reproduction, or it better at survival.
Or it's better at survival.
So all of these are boiling down to,
can I better interpret the world around me,
make better decisions, and ultimately arrive at my goal
of making more of myself?
The place where things get a little weirder is humans.
Because nearly every other species,
you can explain its behaviors as purely natural selection.
You can explain it as dogs are basically
being kind to human, and we will breed them more.
Plants are being kind to animals,
and they will eat them more,
they will eat their fruit, and poop their seed
to spread the plant.
So nearly everything in evolution,
you can explain as some kind of symbiotic relationship,
or some kind of, I'm fitting in my evolutionary niche,
and I'm providing a service
to whoever will help me reproduce.
Humans is a little weird,
because we have basically conquered natural selection.
We have overcome the burden of reproduction and shelter.
And in some ways, you could say that we stopped evolving,
at least in a genetic sense.
That basically humans is no longer about,
being the strongest, or being the smartest,
or being the fittest, or you name it.
Right now, humanity has taken a new trajectory.
And that trajectory is not one of genetic evolution,
but perhaps one of cultural evolution.
This is what I call horizontal evolution,
instead of vertical evolution.
So vertical evolution is passing on your genes.
And with humans, vertical evolution also included
passing on your knowledge to your village.
Every village had its own vertical lineage of knowledge.
And at some point, culture started spreading horizontally.
A great idea in Mesopotamia
would basically spread across the world.
A great idea in China would spread,
perhaps separated by the Himalayas,
or spreading all kinds of cultural ways.
And part of what has led to the acceleration of civilization
is the fact that we have unified the world.
The trade routes of being able to sort of take silk
and trade it with glass, or you name it,
basically means that you are now exchanging information
much, much more.
And therefore, a new idea will be adopted
and spread much more rapidly.
So it might just see this as,
the reason you call this horizontal is because
there's horizontal gene transfer with bacteria.
So this is like horizontal meme transfer with people.
That's exactly right, that's exactly right.
So bacteria are able to spread their genes this way.
That's horizontal gene transfer.
That's basically saying,
I found an antibiotic resistance solution.
I'm gonna dissipate it in the environment.
Those genes that get randomly dissipated
could be any of these genes.
But if the genes that get randomly dissipated
are antibacterial resistance genes,
then those bacteria will be selected for.
And will continue reproducing and so on and so forth.
So basically, there's, again,
it's not teleological, it's not based on the goal.
It's simply purely based on survival.
But with humans, it's different.
With humans, there's this new layer of evolution,
which is about cognitive evolution and cultural evolution
and idea evolution, meme evolution, if you wish.
And now those memes live in different niches.
And those niches themselves are attracting different people.
And there's this co-evolution of people and niches.
And there's the evolution of the ideas.
There's the evolution of the genes
that are sort of starting to blend in some ways.
And the very exciting part about all this
is that we are now creating societies
where we take care of each other.
Whereas, if you don't have good eyesight, no problemo.
We're gonna help you.
If you are having trouble running, no problemo.
You don't need to sort of chase your own food.
If you have trouble, for anything, we are there to help.
So we as a society are there to help.
And the beauty of that is that you can now allow humans
to excel in thousands of different directions.
And basically, you can have someone
who's extraordinary at physics,
but can't cook their own meal.
No problem.
In today's society, we're basically constantly trying
to force everybody to learn math.
Why?
Why?
We don't need everybody to learn math.
We're forcing everybody to do sports.
Why?
Not everybody needs to do sports.
I think it's okay to do specialization.
That's what made our society work.
Like, the concept that every one of us
has to be good at everything is ridiculous.
I think it's okay to let humans specialize.
Now, in science, does that mean that you should stop
looking at stuff outside your field?
No, no, not at all.
Because being better at understanding physics
might give you new ideas for biology and vice versa.
Or AI to physics.
We can talk about the Nobel Prize soon.
So the advantage of basically being a professor
in the department of artificial intelligence
while working on biology,
while interacting with students from physics
and from all of these different disciplines,
is that we constantly have this interbreeding of ideas.
Instead of basically doing specialization in this way,
you basically are bringing all of these different ways
of thinking together.
And I think that the next generation of science
comes from taking all of these different specialties
and having people collaborate together.
Right, so not getting rid of the specialties.
Even going further in your specialty,
but more cross-pollination.
That's exactly right.
That's exactly right.
And I don't want just interdisciplinary scientists
all coming to my lab.
No, I want the pure physicists
and I want the pure mathematician
and I want the pure biologists and I want the pure chemists
and I want the pure experimentalists
all working together under one roof.
Because diversity is the power of humanity.
Diversity is the power of pushing things forward
in a multiplicity of solutions.
So that's sort of part of the beauty of all of that.
So now if you ask me,
what is that unifying theory if you wish?
So we now have this spread of ideas.
We have these layers that I mentioned earlier
of building up all of these different components
to eventually create these artificial systems
that are able to do cognition.
What is your unifying view of biology, cognition and AI?
So underlying all three of them
is this fundamental concept of fitting functions.
So if you look at the genome,
you basically have millions of gene regulatory constructs
of elements that will basically govern
the regulatory networks, the circuitry of those genomes.
And by tuning those networks,
you can adapt to different kinds of environments,
different niches.
This is basically a function fitting operation.
This is tuning your gene regulatory circuitry
to whatever evolutionary niche you're currently occupying.
So that tinkering is at the basis of genomes.
It's at the basis of gene regulation
and it's at the basis of evolution.
Evolution doesn't work by giant, you know,
gnomes that you're turning one way or another way.
Evolution works by being able to tinker with subtle functions.
And this tinkering is what I call evolvability.
In other words, you get good at evolving,
you get good at adapting.
And this evolvability is something
that in my view took a long time to get at.
In other words, if you look at the history of life
on this planet, it took a billion years
for multicellular life to even appear.
And then you would expect that with genetic algorithms,
increasing complexity will require more time.
But in fact, we see exactly the opposite.
That basically after you have multicellularity,
you start sort of seeing extraordinarily complex body
plants very, very rapidly.
And after you have those, you start seeing
increasing cognition very, very rapidly.
Like basically the human brain evolved
in a blink of an eye in evolutionary terms.
This expansion of the neocortex is something
that happened faster than almost anything in evolution.
And that, in my view, is reflective of evolvability.
It's reflective of we get better at modularity,
we get better at hierarchical organization,
we get better at all of these different things.
So as you start increasing that complexity,
you see an acceleration, not a deceleration.
This is mind-boggling.
And again, it sort of teaches us
how powerful this tinkering is.
So that's in the realm of gene regulation.
But evolution is tinkering with gene regulation.
So in a way, this adaptability
of these thousands of little parameters
appears to be an emerging property,
maybe even a fundamental necessity
of both evolution and of gene regulation.
And if you start looking at the shapes of proteins,
it's exactly the same thing.
They're trying to match a particular function.
You're tinkering with these 20 amino acids
as your building blocks,
but then you have thousands of copies of these
and you're constantly adapting.
So the same kind of principle of this tinkering
applies to protein structure.
It applies to chemical structure
with the same type of tinkering of individual atoms.
It also applies to AI.
It also applies to cognition.
How?
If you look at our neurons,
we basically have a very simple developmental program
that gives rise to our entire brain.
It basically simply says,
just create these layers of these neurons.
As they build the layers,
the neurons are basically specializing.
They're communicating with each other.
They're sort of having both within layer communication
and across layer communication.
You basically have the signaling itself
shaping the connections.
You basically have synapses that are reinforced or weakened
based on the signals that they receive
and the feedbacks that they receive.
So it is an adaptable neural network
based on the information that it is processing.
As children grow up in more and more complex environments,
their brains are actually adapting to these environments
during their early years.
And even during gestation,
the signals that you are exposed to
are altering the neuronal processes that you have.
Why is it so hard to learn a language after the age of 12?
Because a lot of your neuronal connections
have now been fixed.
They've been pruned.
The sounds that you could hear as a child
have now been pruned away
because you're not responding to those sounds.
Same with all factions,
same with all kinds of aspects of our adaptation
to different types of environments.
So again, the neuronal aspect
is very much about these thousands of parameters,
millions of parameters,
billions of parameters constantly getting tuned
to fit whatever functions.
And on the AI side, it's exactly the same thing.
Modern AI, like all of these representation learning,
all of these generative AI,
this is built on the same fundamental principle
of I'm starting with thousands of parameters
and I'm sort of building them up to fit functions.
And in all of these different layers,
you have one property that is unifying them all,
if you wish.
And that property is the property of abstraction.
It's the property of basically being able
to build building blocks
upon which you will then build the next layer.
If you look at human language,
that same concept applies.
You basically can take a word
which is a composite word, for example, atom.
It means uncuttable.
Tomy, like tomography, in Greek means to cut.
Atom means uncuttable.
So, but you now have the concept of an atom
that you can build upon.
And you can sort of build layers and layers
of understanding on that part.
So, it is always this tuning of functions,
these building blocks, these abstraction layers
that you're constantly able to recombine concepts
and then build upon them.
In biology now, you have, you know,
for example, a protein domain that you have created.
You have a fold.
And that fold will get reused and reused and reused.
And then recombined with other folds
to sort of now create a new primitive.
And that new primitive will now get, you know,
again replicated in different places and reused.
You not only have vertical evolution of a gene changing,
but you also have duplications of these genes
where you now have two copies
that you can continue tinkering with.
So, I think this beauty of creating building blocks
in layers of abstraction is what makes language works,
work, it's what makes evolution works,
work, it's what makes gene regulation work,
it's what makes brains work, it's what makes AI work.
So, modern AI is dramatically different from classical AI.
So, classical AI initially was about rule-based systems.
And then neural networks were, you know,
sort of invented, modeled very much
after the brains of mammals.
Actually, they studied how our brains fitting functions
and they said, okay, great,
let's make this sort of function fitting operation.
So, classical AI was all about designing systems
that appear intelligent.
It was all about sort of building expert systems
where the expert would come in with all of the knowledge,
all of the rules, all of the decision,
and we would simply build the sensory part
where the AI will observe what's happening in the world
and then make decisions that are predetermined
where the flow chart was already determined
by a human expert.
Like if then statements.
That's exactly right.
The advance of neural networks is that you could suddenly
start modeling the world using this type of tinkering.
It was very much modeled after the human brain.
And then the goal was to be able to approximate functions,
to be able to have these thousands of parameters
being updated through multiple layers of these systems,
which could then build functions that are nested,
functions that are reusing other functions.
But that only took us so far.
Modern AI and modern generative AI, in my view,
started with image and vision
where the concept of a convolution was developed.
And in my view, the concept of a convolution
is this dramatic transition to a new type of AI.
And that transition is about building representations.
The concept that you're not letting every neuron
in isolation recognize a different part of an image,
but instead you're learning these convolutional filters,
these operations that you can then apply
at every part of the image.
Just a moment.
So are you saying that convolution means
that there's no grandmother neuron
that says this is what a grandmother is?
It's a set of neurons?
Are you saying you still have to look at the whole brain?
It's a horizontal...
Slice?
Horizontally applied operation,
where one part of the image says,
ooh, I like a filter that recognizes diagonal edges.
And you can now use that filter.
You can use that kernel.
You can use that primitive.
You can use that pattern.
You can use that building block.
Everywhere in the image.
That you're sharing parameters
across many parts of your image.
That you're basically learning this convolutional filters
at the lowest level,
that are basically learning how to recognize edges.
Right.
And then at the level above that,
you're building on the representations
of the layer before.
So recall 10 years ago or so
when Google Dream or SO came out, something like that.
And there was these trippy psychedelic images
where a dog became eyes and they were all eyes
for 10 seconds and then it became something else.
Okay, are you saying that in that eyes,
where it became all eyes,
that it's as if you can have a part of your neurons
or what have you that select for the diagonal lines,
there may be another one that are selecting eyes
or representing eyes.
And those ones were somehow triggered more.
What's going on there?
Yeah.
So what's going on there is that you can basically ask,
can I represent the pixels associated with this image
using primitives from another domain?
For example, you want to represent giraffe pictures,
but your primitives are all about eyes.
So you're not going to have giraffes built of eyes.
So this idea is that you're taking these primitives,
you're taking these building blocks
and then you're combining them
to create higher level abstractions.
And in the world of images,
this was about going from the pixels to the lines,
to the shapes, to the eyes, to the ears and noses
and eventually people and scenes.
In the realm of genomes,
this is about going from DNA to patterns and motifs
and then building them up into grammars
and regulatory programs.
In the realm of chemistry,
this is about building up representations of your chemicals
from the individual atoms to the connections
that these atoms make together
and then the patterns of connectivity
that you can then start representing as primitives.
So when you now ask what is the similarity
between these different chemicals,
you now have the properties of individual atoms
that are pieced together
by combining the properties of their neighbors
in the same realm.
When you're looking about proteins
and sort of the emergent properties of those proteins,
what we're now doing in my lab, for example,
is trying to see how do we translate structure into function?
And how do we do that?
By basically looking at what is the functional representation
of a protein?
What does it actually do?
And there's many ways to capture that function.
The highest impact of AI
and perhaps the highest challenge of it all
will be understanding life and medicine,
sort of improving the human condition.
So that's the final one
and that's basically what my work is all about.
So what distinguishes modern AI from classical AI
is really these representations,
the fact that the same way that I can start building up
layers of complexity in an image using these abstractions,
layers of complexity in a chemical, in a protein,
I can do the same thing with language.
I can basically start with these primitives of language.
I can start with large language models
that represent every word as a vector somewhere in space.
And this vector takes a new meaning
from the words surrounding it.
So the word apple in the context of a fruit basket
is very different than the word apple
in the context of a PC or a micintosh.
So the way that our brain understands these words individually
has some type of projection
and these words in context can shift these projections.
And that concept of words taking meaning
with other words around them
is identical to pixels in an image,
taking meaning based on the pixels around
and atoms in a chemical taking meaning
with the atoms around them.
And the same way
that we find across all of these evolved systems.
So this is fundamentally different than the way physics works.
So physics again has these laws
that were written 13.8 billion years ago.
Biology is all about tinkering.
And yet within biology,
we see that life took about a billion years
to become multicellular.
And from there on, you have this explosion.
How is that possible?
I think that's possible through the same layers of abstraction,
the same building blocks,
the same sort of reusability of these concepts
that you learn by tinkering with little parameters.
And then the way that you get to this exponentially faster growth
is when you stop tinkering with pixels,
but you start tinkering with concepts.
I see.
And that's, in my view, this unifying emergent property
of these evolved systems
that is fundamentally different from how physics works,
but appears to be somehow unified across AI,
across cognition, across evolution,
across genomes, across gene regulation,
across every aspect of living adaptable things.
And it doesn't stop at the resolution of a single human.
You can now start thinking about corporations.
You can start thinking about societies.
You can start thinking about research groups
in the same type of building up layers of abstraction
and building up complexity and sort of fitting functions
and all of these types of emergent properties.
Okay, let me see if I got a handle on it.
I'll make the analogy with cognition
because I'm more familiar with that.
Let me make an analogy with Lego first and then cognition.
So let's say you have Lego
and you're playing in your little kid in a room,
then you build a bridge in Lego.
And you also build a fort, a tower.
What you could do is then the next day you start over
and you build the bridge from scratch.
But what if you could build the bridge again
as if the bridge was its own Lego piece
and the fort was its own Lego piece,
the castle was its own Lego piece.
So that's what we do with ideas
because when we hear the word, when we hear about plants,
we don't have to reconstruct plants from stem with dirt
and so on or however else we learned it before.
We just got plants now.
Now we can use that as a foothold and go farther.
And so for life, you don't have to wait another billion years
for eukaryote to develop.
You all of a sudden have life
that has mitochondria in it already.
And that's about building blocks.
It's about primitives.
It's about reuse.
It's about abstraction.
That becomes now part of your language
and that goes back to why language is so extraordinary
because these primitives allow us to now formulate
much more complex ideas and it appears everywhere
throughout the evolved world,
throughout the world that adapts,
throughout the adaptable world,
the same type of principle is everywhere.
That's a whirlwind tour.
So, Manolis, what I want to know is what does the future
and people may not know this,
but you are also a professor of medicine
or at least you study medicine, research medicine.
What does the future of medicine look like
and feel free to integrate that with AI and biology
and the rest of the concepts
that we've been speaking about thus far.
So, we are at a crossroads.
We are in this extraordinary convergence
of all of these different fields that have come together.
One of them, of course, is AI
and of course, computer science as a much broader part,
a much broader field of which AI is one part.
We also have miniaturization of technologies
of being able to use microfluidics
to manipulate individual cells,
to be able to use imaging with expansion microscopy,
to be able to look inside individual cells
to understand the properties of those cells.
You have the extraordinary measurement capabilities
that are allowing us to start observing biomarkers,
start observing imaging of the human brain.
We have the ability to sequence genomes
at this extraordinarily fast pace
with these seven orders of magnitude reduction
in price in the span of a decade,
which is unprecedented in the history of humanity,
of how fast this technology has evolved.
So, we are now at these crossroads
of all of these different fields coming together,
along with the ability to fold every protein
using alpha-fold and ESM-fold
and all of these extraordinary tools
that are now using protein language models
and geometric deep learning and graph neural networks
with constraints that are geometric on them.
And we now have the ability to measure gene expression
across millions of cells and thousands of individuals,
every measurement being 20,000 genes at a time.
That allows us to peek into the building blocks of biology,
into the building blocks of disease,
into the heterogeneity of different individuals,
different patient, different tissue,
different organs, different cell types
at a scale that was unfathomable 20 years ago.
Not just dream, not even conceptualizable.
And we now are sitting in these extraordinary convergence,
these crossroads, where we can now understand
the function of a gene using all of these different facets.
So, what does a gene do?
Well, you can look in the literature.
But that literature is now approachable
because we can use large language models
to capture every single paper
that has been written about that gene.
We can now represent the function of a gene
from the entire medical literature.
We can represent the function of the same gene
using knowledge graphs.
And what are the diseases that this gene interacts with?
The individual chemicals,
what are the functional annotations
that this gene has been associated with?
What are of the anatomical regions
where this gene is expressed?
What are, so all of these different concepts of a gene,
first through language, then through knowledge graph,
represent the function of the same object.
You can now think about that gene
as a collection of amino acids
and the structure that this gene has.
And you can use protein language models
and geometric deep learning
to start understanding the folds of that same gene.
You can look at that same gene
and look at its expression patterns
across thousands of data sets with where it's expressed
and how is it expressed in the brain
and with all of the other genes in the body.
And you can build these gene expression similarity networks.
You can also build a protein-protein interaction network
to basically see from experiments
that pull down things that interact with each other
what are all of the interactors of that gene.
And you can use now these joint representation
to unite the different spheres,
to basically start combining together knowledge graphs
and latent representations
and these sort of hierarchical AI-based representations
of every aspect of how that one gene works.
But you can do the same thing
with every other node in this knowledge graph.
You can take a disease
and now start looking at that disease
from the set of all of the genes that it interacts with,
from the drugs that are influencing that disease,
from the genetic variants that are influencing that disease
and the genes that these genetic variants are connected with,
from the imaging representations of the pathology
of patients with that disease.
And you can now build these convergent representations
of every aspect of biology,
from the proteins to the chemicals to the patients
and understand a patient that carries that disease
with the entire clinical record of that patient,
with all of the blood measurements and imaging measurements
and clinical notes that every doctor has written
for that patient across their lifetime.
You can now start measuring all of the environmental variables
that are interacting with that.
So at some point, it becomes so complete
and so complex of a picture that if we don't understand it
at this point, it's our fault.
We basically have more technologies and more data
and more facets of function than we ever wished for.
And at the same time, we have this extraordinary ability
of AI to understand all of these concepts,
to sort of make these hierarchical representations
and interconnect them with each other.
And that's what I'm so, so excited about.
I'm so excited about the fact that all of these separate
projects in my group, my team has been working on
all of these different aspects separately for decades
from genetics to understanding where the genes are.
I started working on the human genome before
there was a human genome.
The tools that we wrote helped annotate
where the genes even are in the human genome.
The current gene set is based on the work that my team did,
annotating where the gene regulatory elements are
and where the circuits are, the networks,
understanding how the epigenome works,
understanding how to interpret genetic variation.
All of these different things were separate research programs
and they've all been now converging together.
I have students who are working on understanding
protein structure, understanding the structure
to function problem, understanding how to represent
knowledge graphs, understanding how to visualize
the latent embeddings of different structures
and different functions and different patients
and patient trajectories and patient heterogeneity.
And we've been working on electronic health records
for many, many years now and all of these different things
used to be separate pieces of knowledge.
And it seemed mad for any one research group
to be working on all these different areas
but now there's method to the madness.
It's all coming together and it's coming together
in the most beautiful, integrative way
because in order to understand the complexity
of something like Alzheimer's or obesity or schizophrenia
or cardiovascular disease or immune disorders
or cancer or immunotherapy response,
you can't separate all of that information.
You have to be cognizant of all of the different parts
and all of them are influencing together
your understanding of biology.
So what we're doing now is we're integrating
all of these massive, massive data sets
that we ourselves have generated in some of these cases.
So my lab has probably generated more single cell
human brain data in dozens of different neurodegenerative
and psychiatric disorders and neurodevelopmental disorders
than any other lab in the whole world.
And we are a computational lab.
Why did we do that?
We basically partnered with all of these extraordinary doctors,
experimentalists, experts in each of these different areas
and we worked together to generate this extraordinary map.
Why?
Because we can now leverage it to understand
the diversity of human cognition,
to understand the diversity of pathology,
the diversity of psychiatric disorders
across thousands of individuals
by breaking down all of these layers
from the phenotype all the way down to individual genes
and all the way down to individual gene regulatory regions.
So where I see biology and where I see medicine heading
is that unification.
It's that ability to now have AI systems
that understand all of these parts together
that are able to help us as discovery partners
to truly make sense of gene function
at an unprecedented level of resolution
and complexity and scale and facets and multimodality
and to do the same thing with disease,
to do the same thing with every pathway underlying disease.
And basically what this is painting now
is a unified view of medicine and biology and therapeutics
which is revealing the same type of modularity
that we've been talking about earlier,
applying now at the disease level
where we can now start understanding
the complexity of Alzheimer's
in terms of its building blocks,
in terms of its hallmarks, if you wish.
And instead of just saying it's a monolithic disorder
and everybody has Alzheimer's,
we can say, well, wait a minute, there's building blocks,
there's cholesterol transport,
there's lipid dysregulation,
there's amyloid accumulation,
there's tau pathology, there's microglial clearance,
there's neuro inflammation,
there's neuro vasculature unit dysregulation
and all of these different components
are now emerging as building blocks
with which we can finally understand medicine
in a unifying way.
And why is that exciting?
Because the same pathways that appear to be underlying
Alzheimer's are reused in different ways
in cardiovascular disease
and they're reused in different ways
in frontotemporal dementia and in schizophrenia.
So we can now start understanding
what are the points of convergence?
What are these primitives?
What are these building blocks?
Of disease, of medicine, of biology, of human well-being,
that we can now start tackling one at a time
and for personalized medicine to function
both in terms of feasibility and in terms of economics.
You cannot simply say I want appeal for a person
because then we'll have appeal for Jeff Bezos,
Bill Gates and appeal for Elon Musk
and that's it because they're the only ones who can afford it.
Instead, we have to think about what is appeal
for this pathway and that pathway and that pathway?
Because by building this modular view
of personalized medicine, the economics work out
because that pathway is shared by millions of people
in different combinations.
And we can now start thinking about personalized medicine
in a way where I can take your own genome,
add up all of the burden that you have
in terms of dysregulation of every one of the genes
in each of these pathways
and say, okay, well, I need to alter
your cholesterol metabolism in this way
and your microglial inflammation in that way
and for each of those, we can now have a modular combination
that allows us to create for every patient,
their own pill, which will be a combination
of all of these different tinkerings, if you wish.
So that's why I'm so hopeful.
That's why I'm so excited about where we're heading
because we're finally understanding
the basic building block of life
and we're finally understanding how they fit together
to sort of paint an actually feasible view
of personalized medicine.
Manolis, it's been a pleasure.
I'm extremely grateful that we've connected.
We seem to have a passion for integrating and unification.
I'm glad that yours has been decades in the making
and that it's coming to fruition in the intersection now.
I could not be more excited.
I think this is an extraordinary time for our field.
It's also an extraordinary time for AI.
And we're basically seeing this impact of AI
in every aspect of humanity, in every aspect of society.
But I think the biggest impact yet
will be in understanding biology
and in understanding medicine
and in dramatically fundamentally changing
the human condition.
A lot of the early applications that we see now of AI
in healthcare are basically just fancy versions of assistance.
They're using language.
Language with large language models
has seen this extraordinary ability to sort of break down
both the syntax and the semantics of language.
And language is simple by comparison.
Language is something that evolved
by humans talking to each other.
It evolved with the limitations of our own evolved brains.
So if we humans couldn't understand language,
language would not be as it is now, it would be simpler.
So language evolved in complexity
up until our own comprehension.
And it was limited by the fact that we need to develop
our brain from a single cell
through a series of divisions and developmental programs.
And not only that constraint,
but also the constraint that it needs to build on
however the chimp brain evolved
and however the mammalian brain evolved
and the vertebrate brain evolved, et cetera.
In other words, the type of cognition that we have
is constrained by number one evolution.
You have to arrive at it by tiny adjustments
of the previous version.
And you also have to arrive at it
from every generation from scratch
to give rise to again the same neural network.
AI is not constrained that way.
AI has the ability to sort of start
with dramatically different parameters
in terms of the depth, the representation of neurons,
the types of operations and so on and so forth.
And right now we're using a very simple architecture
for AI.
We basically, this type of representation learning
that AI is using is much more similar
to say a single cell type, for example,
only neocortex for the human brain.
But the human brain also has all of these subcortical regions,
all of this limbic system, as we call it.
And this is where all of the emotional part is.
That's where all of the fear and fight or flight
and so forth components of the brain are.
And the brain is not just subject
to traditional electric-based computation.
It's also, of course, subject to neurotransmitters
and brainwaves and hormonal interactions
and so on and so forth.
And you could think of them as features or bugs.
You could basically say,
oh, we have this extraordinarily beautiful cognitive brain,
but unfortunately it is influenced
by all of these other things.
Or you could think of those features.
You could basically say we have this brain
that is constantly pushed into different local optima
or different shapes and scales
because of all of these things.
And therefore it can arrive at solutions
that it wouldn't normally arrive at
if it was just a neocortical brain.
So you can basically think about
what are the types of architectures
that we can now leverage
from even just our evolved human brain
to arrive at perhaps more efficient computation
or more creative computation
or more integrative computation
for our engineered brains, if you wish.
But you can also think that, wait a minute,
we can bring in all kinds of other reasoning.
Why do we have to use only traditional human-like reasoning?
Humans are pretty bad at doing all kinds of math.
Humans have no intuition about genomes
or protein folding or chemical interactions at all.
So we now have the ability with AI
to bring in a whole new class of computation,
bring in all of these different ways
of understanding the natural world
that are far beyond language.
And when language has had such an impact in healthcare
by being able to transcribe what doctors are saying
or being able to summarize notes
and being able to summarize information, et cetera,
which is already having an impact in healthcare,
can we now blend that with symbolic reasoning,
with causality inferences, with structural and geometric
and spatial representations of patient information
and so on and so forth.
So I would say that solving language
is a much simpler problem than solving biology
or solving medicine.
So I think ahead of us might be the most complex challenge
of AI yet by understanding not physics
that has a relatively small number of laws
but understanding this tinkering,
this extraordinary diversity in functions
and adaptations in niches that biology has
and tuning to it with these billions of parameters,
the same way that evolution itself
and cognition and gene regulation
is able to tune to the natural world.
So I think that if physics has as its language mathematics,
maybe biology and medicine have as its language
this tinkering that you can see in AI
that the same type of complexity
has now finally found its match
with both the most powerful cognition
that we have ever built, combined of course
with extraordinary capabilities of humans
to be able to now build primitives based on these AI models
of the world and start reasoning about them
and guiding them, but now matching the complexity of medicine,
the complexity of biology, the complexity of chemistry
and the complexity of the evolved world.
So I see this alignment coming together
of these techniques that we have built
initially for understanding images
and for understanding language,
but now for understanding a language
that none of us speaks, the language of medicine,
the language of biology, the language of chemistry
and the language of the natural world.
Super interesting.
Thank you for spending so much time with me.
Thank you.
Now we both got to get going in two minutes,
so I want you to spend one minute
and just address the audience who's watching
and give your advice to new students who are watching,
but also researchers are watching.
So my advice would be to,
very often people say, do what you're passionate about.
Yeah, sure, that's great advice,
but don't just do what you're passionate about.
Build your foundational knowledge,
build your foundational understanding of the world
and go and study all of these different disciplines.
Soak them up, train your brain to think
in every single possible way
that different fields of science have embraced.
And the reason why I'm saying that
is because you will need it.
There's this unification that's happening
across all of these different disciplines right now,
and that unification, this ability
to sort of bring all of these different ways
of thinking together is basically what has allowed us
to make so, so much progress in so many different fields.
So advice number one is learn how to code.
There's just no excuse not to.
Learn how to program fundamentally
by reading tons of programs,
by understanding the building blocks
of the most complex AI systems.
Learn how to tinker with these complex systems.
Use ChatGPT as your partner to explore
giant pieces of code that would seem indomitable,
but ask it to break them down for you.
Basically, be fearless, go out there and dive in
to all of these different disciplines
because it is by becoming an expert.
And frankly, we have no excuse anymore
to not understand something.
We have our disposal, YouTube and Wikipedia
and bio archive and archive and ChatGPT
where you can take any paper and just say,
okay, break this down for me, explain it
and sort of you can keep building and asking questions.
And that fearlessness will translate
into huge, huge dividends.
Don't just say, oh, I'm passionate about this.
Say, oh, I will do my homework.
I will build up my work.
And right above the camera over there,
it says effort equal interest times enjoyment.
Skill equals talent times effort.
Achievement equals skill times effort.
And effort counts twice.
Namely, as you start putting in the work,
you achieve better, you understand better,
you get things better.
And that investment is multiplicative
because your achievement is your skill
that you build from your effort times that effort.
So basically effort along with talent gives you skill
but skill again along with effort gives you achievement.
And achievement then feeds back into your enjoyment.
So you have this extraordinary interplay
that if you say, oh, I'm gonna take the easy way,
yes, it will be easy for a little bit, but then you're stuck.
If instead you say, let me now understand
the theory of everything, let me dive in,
understand the physics, understand the chemistry,
understand the biology, understand the chemistry
and sort of how it builds up within the building blocks
of life and do the same thing with mathematics.
A lot of our teaching of mathematics is unfortunately
stripping away all of the beauty
that you only get to later on.
But dive into YouTube channels
that sort of expand mathematics
with extraordinary visualizations.
I'm thinking about three brown, one blue.
Yep.
That's three blue, one brown.
This does not exist when I was a student
and my eight-year-old is loving it
and truly understanding it and getting these concepts
in a way that was not accessible before.
And by seeing how beautiful the mathematics is later on,
they're excited about the mathematics
that they have to bear with today.
And I think that ability, and I've met with students
who basically tell me, oh, in the next four semesters
I'm gonna be taking all these classes
to eventually take that class.
I'm like, okay, go to that class today.
Just spend a week in that class.
See if you like it.
And then sometimes they come back and saying,
yes, this is exactly what I wanna do.
Other times they come back and say, oh, that's horrible.
Like, thanks for saving me like the next three years.
And I think that the ability to start from complexity down
rather than from simple up,
I think is something that is finally achievable
by being able to have the world's most accomplished tutor
by having Chachi PT as your partner
in all of your learning,
by tackling the most complex paper
and then saying, hey, you know, break it down from me.
You know, and sort of, oh, tell me about that concept.
And now you can sort of build it back up
and sort of dive as far as the rabbit hole will go.
So anyway, my advice is be fearless,
go to the foundations, go to the building blocks,
understand both deeply,
but then be able to emerge back and see the bigger picture.
And when you see something in another field
that doesn't make sense, chances are there's some aspect
of human cognition that you will be training
by trying to understand that thing.
And watch channels like theory of everything
and pause and go deep.
Try to understand every one of these words
because there's something hidden in there.
There's a beauty of integration that comes to humans
that I think you will truly benefit from.
So that's my advice.
Best of luck with that.
All right, thank you.
Okay, thank you so much, such a pleasure.
New update, started a sub-stack.
Writings on there are currently about language
and ill-defined concepts
as well as some other mathematical details,
much more being written there.
This is content that isn't anywhere else.
It's not on theories of everything.
It's not on Patreon.
Also, full transcripts will be placed there
at some point in the future.
Several people ask me, hey Kurt,
you've spoken to so many people
in the fields of theoretical physics, philosophy,
and consciousness, what are your thoughts?
While I remain impartial in interviews,
this sub-stack is a way to peer
into my present deliberations on these topics.
Also, thank you to our partner, The Economist.
Firstly, thank you for watching.
Thank you for listening.
If you haven't subscribed or clicked that like button,
now is the time to do so.
Why?
Because each subscribe, each like helps YouTube
push this content to more people like yourself,
plus it helps out Kurt directly, aka me.
I also found out last year that external links
count plenty toward the algorithm,
which means that whenever you share on Twitter,
say on Facebook or even on Reddit, et cetera,
it shows YouTube, hey, people are talking
about this content outside of YouTube,
which in turn greatly aids the distribution on YouTube.
Thirdly, there's a remarkably active Discord
and subreddit for theories of everything
where people explicate toes,
they disagree respectfully about theories,
and build as a community our own toe.
Links to both are in the description.
Fourthly, you should know this podcast is on iTunes,
it's on Spotify, it's on all of the audio platforms.
All you have to do is type in theories of everything
and you'll find it.
Personally, I gain from rewatching lectures and podcasts.
I also read in the comments that, hey,
Toe listeners also gain from replaying.
So how about instead you re-listen on those platforms
like iTunes, Spotify, Google Podcasts,
whichever podcast catcher you use.
And finally, if you'd like to support more conversations
like this, more content like this,
then do consider visiting patreon.com slash Kurt Jaimungal
and donating with whatever you like.
There's also PayPal, there's also crypto,
there's also just joining on YouTube.
Again, keep in mind it's support from the sponsors
and you that allow me to work on Toe full-time.
You also get early access to ad-free episodes,
whether it's audio or video,
it's audio in the case of Patreon,
video in the case of YouTube.
For instance, this episode that you're listening to right now
was released a few days earlier.
Every dollar helps far more than you think.
Either way, your viewership is generosity enough.
Thank you so much.

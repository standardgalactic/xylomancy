As of today, we are in a war that has moved the atomic clock closer to midnight than it has ever
been. We're dealing with nukes and AI and things like that. We could easily have the last chapter
in that book if we are not more careful about confident wrong ideas.
This is a different sort of podcast and not only because it's Daniel Schmattenberger,
one of the most requested guests who by the way I'll give an introduction to shortly,
but also because today marks season three of the theories of everything podcast.
Each episode will be far more in depth, more challenging, more engaging, have more energy,
more effort, and more thought placed into it than any single one of the previous episodes.
Welcome to the season premiere of season three of the theories of everything podcast with myself,
Kurt Geimangel.
This will be a journey of a podcast with several moments of pause, of tutelage, of reflection,
of surprise appearances, even personal confessions. This is meant for you to be able to watch and
rewatch or listen and re-listen. As with every Toe podcast, there are timestamps in the description
as you could just scroll through to see the different headings, the chapter marks. I say
this phrase frequently in the theories of everything podcast. This phrase just get wet,
which comes from Wheeler and it's about how there are these abstruse concepts in mathematics
and you're mainly supposed to get used to them rather than attempt to bang your head against
the wall to understand it the first time through. It's generally in the re-watching that much of
the lessons are acquired and absorbed and understood. While you may be listening to this, so either
you're walking around and it's on YouTube or you're listening on Spotify or iTunes, by the way
if you're watching on YouTube this is on Spotify and iTunes, links in the description. I recommend
that you at least watch it once on YouTube or you periodically check in because occasionally there
equations being referenced, visuals. I don't know about you, but much or most in fact of the
podcast that I watch, I walk away with this feeling like I've learned something but I actually haven't
and the next day if you ask me to recall, I wouldn't be able to recall much of it. That means that
they're great for being entertaining and feeling like I'm learning something. That is the feelings
of productivity, but if I actually want to deep dive into subject matter it seems to fail at that,
at least for myself. Therefore I'm attempting to solve that by working with the interviewee,
for instance we worked with Daniel, to making this episode and any episode that comes out from
season three onward, from this point onward, to make it not only a fantastic podcast but perhaps
in this small humble way to evolve what a podcast could be. You may not know this, but in addition
to math and physics my background is in filmmaking, so I know how powerful certain techniques can be
with regard to elucidation. How the difference between making a cut here or making a cut here
can be the difference between you absorbing a lesson or it being forgotten. By the way,
my name is Kurt Jaimungle and this is a podcast called Theories of Everything,
dedicated to investigating the versicolor terrain of theories of everything,
primarily from a theoretical physics perspective, but also venturing beyond that to hopefully
understand what the heck is fundamental reality, get closer to it, can you do so? Is there a
fundamental reality? Is it fundamental? Because even the word fundamental has certain presumptions
in it. I'm going to use almost everything from my filmmaking background and my mathematical
background to make Toe the deepest dive, not only with the guest, but we'd like it to be the deepest
dive on the subject matter that the guest is speaking about. It's so supplementary that it's
best to call it complimentary, as the aim is to achieve so much that there's no fat, there's just
meat. It's all substantive. That's the goal. Now there's some necessary infrastructure of concepts
to be explicated prior in order to gain the most from this conversation with Daniel. So I'll attempt
to outline when needed. Again, timestamps are in the description so you can go at your own pace,
you can revisit sections. There will also be announcements throughout and especially at the
end of this video. So stay tuned. Now Daniel Schmottenberger is a systems thinker, which is
different than reductionism primarily in its focus. So systems thinkers think about the interactions,
the N2 or greater interactions, the second order or third order. And Daniel in this conversation is
constantly referring to the interconnectivity of systems and the potential for unintended consequences.
We also talk about the risks associated with AI. We also talk about their boons because that's
often overlooked. Plenty of alarmist talk is on this subject. When talking about the risks,
we're mainly talking about its alignment or misalignment with human values. We also talk about
why each route, even if it's aligned, isn't exactly salutary. About a third of the way through,
Daniel begins to advocate for a cooperative orientation in AI development where the focus
is on ensuring that AI systems are designed to benefit and that there are safeguards placed in,
much like any other technology. You can think about this in terms of a tweet, a recent tweet
by Rob Miles, which says, it's not that hard to go to the moon, but in worlds that manage it,
saying that these astronauts will probably die, is responded with a detailed technical plan showing
all the fail-saves, testings, and procedures that are in place. They're not met with, hey,
wow, what an extraordinarily speculative claim. Now, this cooperative orientation resonates
with the concept of Nash equilibrium. A Nash equilibrium occurs when all players
choose their optimal strategy given their beliefs about other people's strategies,
such that no one player can benefit from altering their strategy. Now, that was fairly abstract.
So let me give an instance. There's rock, paper, scissors, and you may think, hey,
how the heck can you choose an optimal strategy in this random game? Well, that's the answer.
It's actually to be random. So a one-third chance of being rock or paper or scissors. And you can
see this because if you were to choose, let's say, one half chance of being rock, well, then a player
can beat you one half of the time by choosing their strategy to be paper. And then that means that
you can improve your strategy by choosing something else. In game theory, a move is something that
you do at a particular point in the game, or it's a decision that you make. For instance,
in this game, you can reveal a card, you can draw a card, you can relocate a chip from one place to
another. Moves are the building blocks of games, and each player makes a move individually in
response to what you do or what you don't do or in response to something that they're thinking,
a strategy, for instance. A strategy is a complete plan of action that you employ throughout the
game. A strategy is your response to all possible situations, all situations that can be thrown
your way. And by the way, that's what this upside down funny looking symbol is. This means for all
in math and in logic. It's a comprehensive guide that dictates the actions you take in response
to the players you cooperate with and also the players that you don't. A common misconception
about Nash Equilibria is that they result in the best possible outcome for all players. Actually,
most often they're suboptimal for each player. They also have social inefficiencies. For instance,
the infamous prisoner's dilemma. Now this relates to AI systems. And as Daniel talks about, this
has significant implications for AI risks. Do we know if AI systems will adopt cooperative or
uncooperative strategies? How desirable or undesirable will those outcomes be? What about
the nation states that possess them? Will it be ordered and positive or will it be chaotic and
a tactic like the intersection behind me? Although it's fairly ordered right now. It's usually not
like this. The stability of a Nash equilibrium refers to its robustness in face of small changes,
perturbations in payoffs or strategies. An unstable Nash equilibrium can collapse
under slight perturbations leading to shifts in player strategies and then consequently a new
Nash equilibrium. In the case of AI risk, an unstable Nash equilibrium could result in rapid
and extreme harmful oscillations in AI behavior as they compete for dominance. And by the way,
this isn't including that an AI itself may be fractionated in the way that we are as people
with several selves inside us vying for control in a union manner.
Generalizations also have a huge role in understanding complex systems. So what occurs is
you take some concept and then you list out some conditions and then you relax some of those conditions.
You abstract away through the recognition of certain recurring patterns. We can construct
frameworks. We can hypothesize such that hopefully it captures not only this phenomenon,
but a diverse array of phenomenon. The themes of theories of everything of this channel is what
is fundamental reality. And like I mentioned, we generally explore that from a theoretical physics
perspective, but we also abstract out and think, well, what is consciousness? Does that arise from
material? Does it have a relationship to what's fundamental reality? What about philosophy? What
does that have to say metaphysics? So that is generalizations empower prognostication, the
discerning of patterns, and they streamline our examination of the environment that we seem
to be embedded in. Now, in the realm of quantum mechanics, generalizations take on a specific
significance. Now, given that we talk about probability and uncertainty, both in these
videos, which you're seeing on screen now, and in this conversation with Daniel, thus it's fruitful
to explore one powerful generalization of probabilities that bridges classical mechanics
with quantum theory called quasi-probability distributions.
Born in the early days of quantum mechanics, a quasi-probability distribution,
also known as a QPD, bridges between classical and quantum theories. There's this guy named Eugene
Wigner, who around 1932 published his paper on the quantum corrections of thermodynamic
equilibriums, which introduces the Wigner function. What's notable here is that both position
and momentum appear in this analog to the wave function when ordinarily you choose to work in
the so-called momentum space or position space, but not both. To better grasp the concept, think
of quasi-probability distributions as maps that encode quantum features into classical-like
probability distributions. Whenever you hear the suffix like, you should immediately be skeptical
as space-like isn't space and time-like isn't the same thing as time. In this instance, classical-like
isn't classical. There's something called the Kalamogorov axioms of probability, and some of
them are relaxed in these quasi-probability distributions. For instance, you're allowed
negative probabilities. They also don't have to sum up to one, and doing so with the Wigner function
reveals some of the more peculiar aspects of quantum theory like superposition and entanglement.
The development of QPDs expanded with the Glauber-Sedarschen p-representation,
introduced by Sedarschen in 1963 and refined by Glauber and Hussamese Q-representation in 1940.
QPDs play a crucial role in quantum tomography, which allow us to reconstruct and characterize
unknown quantum states. They also maintain their invariance under symplectic transformations,
preserving the structure of base-based dynamics. You can think of this as preserving the areas of
parallelograms formed by vectors in phase space. Nowadays, QPDs have ventured beyond the quantum
realm, inspiring advancements in machine learning and artificial intelligence. This is called
quantum machine learning, and while it's in its infancy, it may be that the next breakthrough
in lowering compute lies with these kernel methods and quantum variational encoders.
By leveraging QPDs in place of density matrices, researchers gain the ability to study quantum
processes with reduced computational complexity. For instance, QPDs have been employed to create
quantum-inspired optimization algorithms like the quantum-inspired genetic algorithm, QGA,
which incorporates quantum superposition to enhance search and optimization processes.
Quantum variational autoencoders can be used for tasks such as
quantum states compression and quantum generative models, also quantum error mitigation. The whole
point of this is that there are new techniques being developed daily, and unlike the incremental
change of the past, there's a probability, a low one, but it's non-zero, that one of these will
remarkably and irrevocably change the landscape of technology. So generalizations are important.
For instance, spin and gr. So general relativity is known to be the only theory that's consistent
with being Lorentz and variance, having an interaction and being spin two, something called
spin two. This means if you have a field and it's spin two and it's not free, so there's
interactions, and it's Lorentz and variance, then general relativity pops out, meaning you get it
as a result. Now this interacting aspect is important because if you have a scalar, so if you
have a spin zero field, then what happens is it couples to the trace of the energy momentum tensor
because there's nothing else for it to couple two, and it turns out that does reproduce Newton's law
of gravity. However, as soon as you add an interacting relativistic matter, then you don't get
that light bends. So then you think, well, let's generalize it to spin one, and then there are
some problems there, and you think, well, let's generalize it to spin three and above, and there's
some no-go theorems by Weinberg there. By the way, the problem with spin one is that masses will
repel for the same reason that in electromagnetism, that if you have same charges, they repel. Okay,
other than just a handful of papers, it seems like we've covered all the necessary ground,
and when there's more room to be covered, I'll cover it spasmodically throughout the podcast.
There'll be links to the papers and to the other concepts that are explored in the description.
Most of the prep work for this conversation seems to be out of the way, so now let's introduce
Daniel Schmattenberger. Welcome, valued listeners and watchers. Today, we're honored to introduce
this remarkable guest, an extraordinary, extraordinary thinker who transcends conventional
boundaries, Daniel Schmattenberger. I'm sure what are the underlying causes that everything from
nuclear war to environmental degradation to animal rights issues to class issues, what do
these things have in common? As a multidisciplinary aficionado, Daniel's expertise spans complex
systems theory, evolutionary dynamics, and existential risk, topics that challenge the
forefront of academic exploration, seamlessly melding different fields such as philosophy,
neuroscience, and sustainability. He offers a comprehensive understanding of our world's
most pressing challenges. Really, the thing we have to shift is the economy, because perverse
economic incentive is under the whole thing. There's no way that as long as you have a for-profit
military-industrial complex as the largest block of the global economy that you could ever have
peace, there's an anti-incentive on it as long as there's so much money to be made with mining,
et cetera, like we have to fix the nature of economic incentives. In 2018, Daniel co-founded
the Conceliants Project, a groundbreaking initiative that aims to foster societal-wide
transformation via the synthesis of disparate domains, promoting collaboration, innovation,
as well as something we used to call wisdom. Today's conversation delves into AI,
consciousness, and morality, aligning with the themes of the TOE Podcast. It may challenge your
beliefs. It'll present alternative perspectives to the AI risk scenarios by also outlining the
positive cases which are often overlooked. Ultimately, Daniel offers a fresh outlook on
the interconnectedness of reality. Say, let's get the decentralized collective intelligence of
the world having the best frameworks for understanding the most fundamental problems
as the center of the innovative focus of the creativity of the world. So,
Utah Watchers U, my name is Kurt Jaimungal. Prepare for a captivating journey as we explore the
peerless, enthralling world of Daniel Schmattenberger. Enjoy!
I do not know with what weapons World War III will be fought,
but World War IV will be fought with sticks and stones.
All right, Daniel, what have you been up to in the past few years?
Um, past few years.
Trying to understand the unfolding global situation and the trajectories towards
existential and global catastrophic risk in particular, the solutions to those that
involve control mechanisms that create trajectories towards dystopias, and the
consideration of what a world that is neither in the attractor basin of catastrophe or dystopia
looks like a kind of third attractor. What would it take to have a civilization that
could steward the power of exponential technology much better than we have stewarded all of our
previous technological power? What would that mean in terms of culture and in terms of political
economies and governance and things like that? So, thinking about those things and acting on
specific cases of near-term catastrophic risks that we were hoping to ameliorate and helping
with various projects on how to transition institutions to be more intelligent and things
like that. What are some of these near-term catastrophic risks?
Well, as of today, we are in a war that has moved the atomic clock closer to midnight than it has
ever been. That's a pretty obvious one. If we were to write a book about the folly of the history
of human hubris, we would get very concerned about where we are confident about where we're
right, where we might actually be wrong, and the consequences of it. As we're dealing with nukes
in AI and things like that, we could easily have the last chapter in that book if we are not
more careful about confident wrong ideas. So, what are all the assumptions in the way we're
navigating that particular conflict that might not be right? What are the ways we are modeling
the various sides and what would an end state that is viable for the world and that just at
minimum doesn't go to a global catastrophic risk? That's an example. If we look at the domain of
synthetic biology as a different kind of advanced technology, exponential tech,
and we look at that the cost of things like gene sequencing and then the ability to synthesize
genomes, gene printing are dropping faster than Moore's law in cost. While open science means that
the most virulent viruses possible studied in contexts that have ethical review boards getting
open published, then that's a situation where that knowledge combined with near-term decentralized
gene printers is decentralized catastrophe weapons on purpose or even accidentally.
There are heaps of examples in the environmental space. If we look at our planetary boundaries,
climate change is the one people have the most awareness of publicly, but if you look at the
other planetary boundaries like the mining pollution or chemical pollution or nitrogen,
dead zones and oceans or biodiversity loss or species extinction, we've already passed
certain tipping points. The question is how runaway are those effects? There was an article
published a few months ago on PFOS and PFAS, the fluorinated surfactants forever chemicals,
as they're popularly called, that found higher than EPA allowable standards of them in rainwater
all around the world, including in snowfall and Antarctica, because they actually evaporate.
We're not slowing down on the production of those in their endocrine disruptors and carcinogens,
and that doesn't just affect humans, but affects things like the entirety of ecology and soil
microorganisms, this kind of humongous effect. Those are all examples. I would say right now,
I know the topic of our conversation today is AI. AI is both a novel example of a possible
catastrophic risk through certain types of utilization. It is also an accelerant to every
category of catastrophic risk potentially, so that one has a lot of attention at the moment.
So that makes AI different than the rest that you've mentioned?
Definitely.
And are you focused primarily on avoiding disaster or moving towards something that's
much more heavenly or positive, like a Shangri-La?
So we have an assessment called a metacrisis. There's a more popular term out there right now,
the polycrisis. We've been calling this the metacrisis since before coming across that term.
Polycrisis is the idea that the global catastrophic risk that we all need to focus on and coordinate
on is not just climate change and is not just wealth inequality and is not just kind of the
breakdown of Pax Americana and the possibility of war or these species extinction issues,
but it's lots of things. There's lots of different global catastrophic risks,
and that they interact with each other and they're complicated and there can even be
cascades between them, right? We don't have to have climate change produce total
venusification of the earth to produce a global catastrophic risk, it just has to increase the
likelihood of extreme weather events in an area. And we've already seen that happening,
statistics on that seem quite clear, and it's not just total climate change, deforestation,
affecting local transpiration and heat in an area can have an effect on and total amount of
pavement laden whatever can have an effect on extreme weather events. But extreme weather events,
I mean, we saw what happened to Australia a couple of years ago when a significant percentage of a
whole continent burned in a way that we don't have near term historical precedent for. We saw
the way that droughts affected the migration that led to the whole Syrian conflict that got very
close to a much larger scale conflict. The Australia situation happened to hit a low population
density area, but there are plenty of high population density areas that are getting very near
the temperatures that create total crop failures, whether we're talking about India, Pakistan,
Bangladesh, Nigeria, Iran. And so if you have massive human migration, the UN currently predicts
hundreds of millions of climate-mediated migrants in the next decade and a half, then it's pretty
easy under those situations to have resource wars. And those can hit existing political
fault lines and then technological amplification. And so in the past, we obviously have a lot less
people. We only had half a billion people for the entirety of the history of the world to the
Industrial Revolution. And then with the Green Revolution and nitrogen fertilizer and oil and
like that, we went from half a billion people to eight billion people overnight in historical
timelines. And we went from those people mostly living on local subsistence to almost all living
on dependent upon very complicated supply chains now that are six continent mediated supply chains.
So that means that there's radically more fragility in the life support systems so that
local catastrophes can turn to breakdowns of supply chains, economic effects, etc., that
affect people very widely. So poly crisis kind of looking at all that. Metacrisis adds
looking at the underlying drivers of all of them. Why do we have all of these issues?
And what would it take to solve them not just on a point-by-point basis, but to
solve the underlying basis? So we can see that all of these have to do with coordination failures.
We can see that underneath all of them there are things like perverse economic interests.
If the cost of the environmental pollution to clean it up was something where in the process of
the corporation selling the PFOS as a surfactant for waterproofing clothes or whatever, it also
had to pay for the cost to clean up its effect in the environment or the oil cost had to clean
up the effect on the environment. So you didn't have the perverse incentive to externalize costs
onto nature's balance sheet, which nobody enforces. Obviously, we'd have none of those
environmental issues. That would be a totally different situation. So can we address perverse
incentive writ large? That would require fundamental changes when we think of as economy and how we
enact that. So political economy. So we think about those things. So I would say with the
Metacrisis assessment, we would say that we're in a very novel position with regard to catastrophic
risk, global catastrophic risk, because until World War II, there was no technology big enough
to cause a global catastrophic risk as a result of dumb human choices or human failure quickly.
And then with the bomb, there was. It was the beginning. And that's a moment ago in evolutionary
time. And if we reverse back a little bit before the bomb until the Industrial Revolution,
we couldn't we didn't have any technology that could have caused global catastrophic risk even
cumulatively. The industrial technology, extracting stuff from nature and turning it into human
stuff for a little while before turning into pollution and trash, where we're extracting
stuff from nature in ways that destroy the environment faster than nature can replenish
and then turning it into trash and pollution faster than it can be processed and doing exponentially
more of that because it's coupled to a economy that requires exponential growth to keep up with
interest. That creates an existential risk, creates a catastrophic risk within about a few
centuries of cumulative effects. And we're basically at that few century point. And so that's very
new. There are all of our historical systems for that, you know, our historical systems for thinking
about governance in the world didn't have to deal with those effects. We could just kind of think
about the world as inexhaustible. And then of course, when we got the bomb, we're like, all right,
this is the first technology that rather than racing to implement, we have to ensure that no one
ever uses. In all previous technologies, there was a race to implement it. It was a very different
situation. But since that time, a lot more catastrophic technologies have emerged. Catastrophic
technologies in terms of applications of AI and synthetic biology and cyber and various things
that are way easier to build the nukes and way harder to control. And when you have many actors
that have access to many different types of catastrophic technology that can't be monitored,
you don't get mutually assured destruction and those types of safeties.
So we'd say that we're in a situation where the catastrophic risk landscape is novel, nothing in
history has been anything like it. And the current trajectory doesn't look awesome for making it through.
What it would take to make it through actually requires change to those underlying coordinate
coordination structures of humanity very deeply. So I don't see a model where we do make it through
those that doesn't also become a whole lot more awesome. And that's what we say. The only other
example is to control for catastrophes, you can try to put very strong control provisions. Okay, so
now unlike in the past, people could or pretty soon have gene drives where they could build
pandemic weapons in their basement or drone weapons where they could take out infrastructure
targets or now AI weapons even easier. We can't let that happen. So we need ubiquitous surveillance
to know what everybody's doing in their basement because if we don't in the world is
unacceptably fragile. So we can seek catastrophes or dystopias, right, because most versions of
ubiquitous surveillance are pretty terrible. And so if you can control decentralized action,
if you don't control decentralized action, the current decentralized action is moving
towards planetary boundaries and conflict and etc. If you control it, then what are the checks
and balances on that control? Sorry, what do you mean control decentralized actions?
So when we look at what causes catastrophe, so when we're talking about environmental issues,
there's not one group that is taking all the fish out of the ocean or causing species extinction
or doing all the pollution. There's a decentralized incentive that lots of companies share.
To do those things. So nobody's intentionally trying to remove all the fish from the ocean.
They're trying to meet an economic incentive that they have that's associated with fishing,
but the cumulative effect of that is overfishing the ocean, right? So if you try to, if there's a
decentralized set of activity where the lack of coordination of everybody doing that, everybody
pursuing their own near term optimum creates the shitty term global minimum for everybody,
right? A long term bad outcome for everybody. If you try to create some centralized control
against that, that's a lot of centralized power and where the checks and balances on that power.
Otherwise, how do you create decentralized coordination? And similarly, if you're looking at
things like in an age where terrorism can get exponential technologies and you don't want
exponentially empowered terrorism with catastrophe weapons for everyone,
to be able to see what's being developed ahead of time, does that look like a degree of surveillance
that nobody wants to be able to control those things not happening, right? That's what I mean.
So if you, how to prevent the catastrophes, if the catastrophes are currently the result of the
human motivational landscape in a decentralized way, if the solution is a centralized method
powerful enough to do it, where the checks and balances on that power. So a future that is neither
cascading catastrophes nor control dystopias is the one that we're interested in. And so,
yes, I would say the whole focus is that this is now AI comes back into the topic because a lot of
people see possibilities for a very protopian future with AI where it can help solve coordination
issues and solve lots of resource allocation issues. It also, and it can, it can also make
lots of things. The catastrophe is worse and dystopia is worse. It's actually kind of unique
in being able to make both of those things more powerful. Can you explain what you mean when
you say that the negative externalities are coupled to an economy that depends on exponential growth?
Yeah. So
it's, if you, if you think about it just in the first principle way, the idea is supposed to be
something like there are real goods and services that people want that improve their life that we
care about. And so the services might not be physical goods directly. There might be things
humans are doing, but they still depend upon lots of goods, right? If you are going to provide a
consultation over a Zoom meeting, you have to have laptops and satellites and power lines and
mining and all those things. So you can't separate the service industry from the goods industry.
So there's physical stuff that we want. And to mediate the access to that and the exchange of
it, we think about it through a currency. So it's supposed to be that there's this physical
stuff and the currency is a way of being able to mediate the incentives and exchange of it.
But the currency starts to gain its own physics, right? So we make a currency that has no intrinsic
value that is just representative of any kind of value we could want. But the moment we do something
like interest, where we're now exponentiating the monetary supply, independent of an actual
automatic growth of goods or services, to not debase the value of the currency,
you have to also exponentiate the total amount of goods and services. And everybody's seen how
compounding interest works, right? Because you have a particular amount of interest and then
you have interest from that amount of interest, so you do get an exponential curve. Obviously,
that's just the beginning. Financial services as a whole and all of the dynamics where you have
money making on money mean that you expand the monetary supply on an exponential curve, which
was based on the idea that there is a natural exponential curve of population,
anyways, and there's a natural growth of goods and services correlated. But that was true at an
early part of a curve that was supposed to be an S curve, right? You have an exponential curve that
inflects goes into an X curve, but we don't have the S curve part of the financial system planned.
The financial system has to keep doing exponential growth or it breaks. And not only is that key
to the financial system, because what does it mean to have a financial system without interest,
say it's a very deeply different system, that formalizing that was also key to our solution
to not have World War III, right? The history of the world in terms of war does not look great,
that the major empires and major nations don't stay out of violent conflict with each other very
long. And World War I was supposed to be the war that ended all wars, but it wasn't. We had World
War II. Now, this one really has to be the war that ends all major superpower wars because of the
bomb. We can't do that again. And the primary basis of wars, one of the primary bases, had been
resources, which was a particular empire wanted to grow and get more stuff. And that meant taking
it from somebody else. And so the idea of if we could exponentially grow global GDP, everybody
could have more without taking each other's stuff. It's so highly positive that we don't have to go
zero someone war. So the whole post World War II banking system, the Bretton Woods monetary system,
et cetera, was part of the how do we not have World War, along with mutually assured destruction,
the UN, and other international, intergovernmental organizations. But that let's exponentially
grow the monetary system also meant that if you have a whole bunch more dollars and you
don't have more goods and services, the dollars become worth less and it's just inflation and
debasing the currency. So now you have an artificial incentive to keep growing the
physical economy, which also means that the materials economy has to have an exponential
amount of nature getting turned from nature into stuff, into trash and pollution,
in a linear materials economy. And you don't get to exponentially do that on the finite
biosphere forever. So the economy is tied to interest, and that's at the root of this,
of what you just explained, not at the root of every catastrophe.
Interest is the beginning of what all of the financial services do, but there is an embedded
growth obligation of which interest is the first thing you can see on the economic system. The
embedded growth obligation that creates exponentiation of it tied to the physical world where
exponential curves don't get a run forever, is one of the problems. There are a handful,
this is when we're thinking metacrisis, what are the underlying issues? This is one of the
underlying issues. There's quite a few other ones that we can look at to say,
if we really want to address the issues, we have to address it at this level.
What's the issue with transitioning from something that's exponential to sub-exponential
when it comes to the economy?
What's the issue with it? Well, I mean,
there's a bunch of ways we could go. There is an old refrain from the hippie days,
and it seems very obvious, I think, as soon as anyone thinks about it, which is that you can't
run an exponential growth system on a finite planet forever. That seems kind of obvious and
intuitive. Because it's so obvious and intuitive, there's a lot of counters to it.
One counter is, we're not going to run it on a finite planet forever. We're going to become an
interplanetary species, mine asteroids, ship our ways to the sun, blah, blah, blah.
I don't think that we are anywhere near close, independent of the ethical or aesthetic argument
on if us obliterating our planet's carrying capacity and then exporting that to the rest
of the universe is a good or lovely idea or not. Independent of that argument, the timelines
by which that could actually meet the humanity super-organism growing needs relative to the
timelines where this thing starts failing don't work. That's not an answer. That said,
the attempt to even try to get there quicker is a utilization of resources here that is speeding
up the breakdown here faster than it is providing alternatives. The other answer people have to
why there could be an exponential growth forever is because digital. That more and more money
is a result of software being created, a result of digital entertainment being created,
and that there's a lot less physical impact of that. We can keep growing digital goods
because it doesn't affect the physical plan and physical supply chain so we can keep the
exponential growth up forever. That's very much the Silicon Valley take on it.
Of course, that has an effect. It does not solve the problem. It's pretty straightforward to see
why. Let's go ahead and say software in particular. Does software have to run on hardware where the
computer systems and server banks and satellites and et cetera require massive mining, which also
requires a financial system in police and courts to maintain the entire cybernetic system that
runs all that? Yes, it does. Does a lot more compute require more of that? More
atoms, adjacent services, energy? Yes. Also, for us to consider software valuable, it's either
because we're engaging with what it's doing directly. That's the case in entertainment or
education or something, but then it is interfacing with the finite resource called human attention,
of which there is a finite amount, or because we're not necessarily being entertained or
educated or engaging with it, but it's doing something, for us to, again, to consider valuable,
it is doing something to the physical world. The software is engaging, say, supply chain
optimization or new modeling for how to make better transistors or something like that,
but then it's still moving atoms around using energy and physical space, which is a finite resource.
If it is not either affecting the physical world or affecting our attention, why would we value it?
We don't. It still bottoms out on finite resources, so I can't just keep producing an
infinite amount of software where you get more and more content that nobody has time to watch,
and more and more designs for physical things that we don't have physical atoms for, energy for.
You get a diminishing return on the value of it if it's not coupled to things that are finite.
The value of it is in modulating things that are also finite. There's a coupling coefficient there.
You still don't get an exponential curve. What we just did is say the old AP refrain,
you can't run an exponential economy on a finite planet forever. The alt,
the counters to it, don't hold. What about mind uploading or some
computer brain interface to allow us to have more attention exponentially?
Yeah. That's almost like the hybrid of the other two, which is get beyond this planet
and do it more digitally, so it could be on this brain and become digital gods in a singularity
universe. Again, I think there are pretty interesting arguments we can have ethically,
aesthetically, and epistemically about why that is neither possible nor desirable.
But independent of those, I don't think it's anywhere close. Same like the
multi-planetary species. It is nowhere near close enough to address any of the timelines we have by
which economy has to change because the growth imperative on the economy as is is moving us
towards catastrophic tipping points. So if it were close, would that change your assessment
or you still have other issues? If it were close, then we would have to say
first that is implying that we have a good reason to think that it's possible.
Right? That it's possible to, and that means all the axioms that consciousness is substrate
independent, that the consciousness is purely a function of compute. Strong computationalism
holds that we could map the states of the brain and or if we believe in embodied cognition,
the physiology adequately to represent that informational system on some other substrate,
that that could operate with an amount of energy that is
and substrate that's possible, blah, blah, blah. So first we have to believe that's possible.
I would question literally every one of the axioms or assumptions that you said.
We're going to get to that.
We would say, is it desirable? And how do we know? How ahead of time? And now you get
something very much like, how do I know that the AI is sentient? Which the,
for the most part, on all AI risk topics, whether it's sentient or not is irrelevant,
whether it does stuff, it's all that matters. But how do you tell if it's sentient and all of the
Chalmers, Pesambi questions or whatever are actually really hard? Because what we're asking is how
can we use third person observation to infer something about the nature of first person,
given the ontological difference between them? So how would we know that that future is desirable?
Are there safe to fail tests? And what would we have to test to know it to start making that
conversion? But I don't think we have to answer any of those questions because I don't think
that anybody that is working on whole brain emulation thinks that we are close enough
that it would address the timeline of the economy issues that you're addressing.
Let's attempt to address one of the questions about substrate independence.
What are your views? Is consciousness something that our biological brains do or that requires
development from an embryonic stage? Like whatever it is that produced us, there's something special
about us or animals. Or is it something that can be transferred or started up booted up from
scratch into what's not us, like decidedly not us a computer? Okay, so this is not much more a
proper theory of everything conversation than the topic that we intended for the day, which is about
AI risk. So what I will do is say briefly the conclusion of my thoughts on this without actually
going into it in depth, but I would be happy to explore that at some point.
I think that how I come to my position on it
to try to do a kind of proper construction takes a while. So briefly I'll say
I'm not a strong computation list, meaning don't believe that mind universe,
sentience, qualia is purely a function of computation. I am not an emergent physicalist
that believes that consciousness is an epiphenomena of non-conscious physics that in the same way that
we have weak emergence, more of a particular property through certain kind of combinatorics
or strong emergence new properties emerging out of some type of interaction where that hadn't
occurred before like a cell respirating or none of the molecules that make it up respirate.
I believe in weak emergence that happens all the time. You get more of certain qualities
happens in metallurgy when you combine metals where the combined tensile strength or
shearing strength or whatever is more than you would expect as a result of the nature of how the
molecular lattice is formed you get more of a thing of the same type I believe in strong
emergence which is you get new types of things you didn't have before like respiration and replication
out of parts none of which do that. But those are all still in the domain of third person
accessible things. The idea of radical emergence that you get the emergence of first person out
of third person or third person out of first person which is idealism on one side and physicalism on
the other I don't buy either of I think that idealism and physicalism are similar types of
reductionism where they both take certain ontological assumptions to bootload their
epistemology and then get self-referential dynamics. So I don't think that if a computational system
gets advanced enough automatically consciousness pops out of it that's one. Two I do think that the
process of a system self-organizing is fundamentally connected to the nature of experience of
selfness and things that are being designed and are not self-organizing where the boundary between
the system and its environment that exchanges energy and information and matter across the
boundary is a auto poetic process. I do believe that's fundamental to the nature of things that have
self other recognition and on substrate independence I do believe that carbon and
silicon are different in pretty fundamental ways that don't orient to the same types of
possibilities and I think that that's actually pretty important to the AI risk argument
but so I'll just go ahead and say those things. I also don't think I believe that embodied cognition
in the Demasio sense is important and that a scan of purely brain states isn't sufficient.
I also don't think that a scan of brain states is possible even in theory
and sorry to interrupt I know you said you don't believe it's possible what if it is and you're
able to scan your brain state and body state so we take into account the embodied cognition.
Sure so
I think that okay it's not simply a matter of scanning the brain state we need to scan the
rest of the central nervous system no we also have to get the peripheral nervous system no we
have to get the endocrine system no all of the cells have the production of and reception of
neuroendocrine type things we have to scan the whole thing does that then extend to
the microbiome, virome, etc. I would argue yes does it then extend to the environment
I would argue yes where where does where does that stop its extension is actually a very
important question so I would take the embodied cognition a step further.
The other thing is Stuart Kaufman's arguments about quantum amplification to the mesoscopic level
that quantum mechanical events don't just fully cancel themselves out at the
subatomic level and at the level of brains everything that is happening is
straight forwardly classical but that there is quantum mechanical
some fundamental kind of indeterminism built in phenomena they end up affecting
what happens at the level of molecules now then one can say well is that just mean we have to
add a certain amount of a random function in or is there something else this is a big rabbit hole
I would say for another time because then you get into quantum entanglement and coherence so you
get something that is neither perfectly random meaning without pattern you get a born distribution
even on single one but it's also not deterministic or with hidden variables so do I think that what's
happening in the brain body system is not purely deterministic and also as a result of that means
you could not measure or scan it even in principle in a kind of Heisenberg sense yes I think that
have you heard of David Walpart and his limits on inference systems inference machine sorry
I have not studied his work okay well anyway he echoes something similar which says that
you can't have Laplace's demon even in a classical world you can't have Laplace's demon
so let me talk about the economy which only on your podcast would happen
why is it that if somehow this exponential curve starts to get to where the s is the
top of the s that the halting or the slowing down of the economy is something that's so catastrophic
and calamitous rather than something that would mutate and if we need to just at that point as
it starts to slow down we make minor changes here and there is it something that's entirely new
like will it all come crashing down um okay so let me make the question clear it sounds like look
the economy is tied to exponential growth we can't grow exponentially virtually no one believes that
so at some point and let's just imagine it's three decades just to give some numbers so at some point
three decades from now this exponential curve for all of the economy will start to show its
legs and start to weaken and we'll see that it's nearing the s part so what does that mean that
there's been fire in the streets that the buildings don't work that the water doesn't run anymore like
what will happen okay so people often make jokes about physicists in particular starting to look
at biology and language and society and modeling in particularly funny reductionist ways because
they try to map the entire economy through the second law of thermodynamics or something like that
and because what we're really talking about is the maximally complex and anthropocomplex thing
and embedded complexity we can because we're talking about all of human motives and how do humans
respond to the idea that uh there is fundamentally limits on the growth possible to them um or there's
less stuff possible for them or that there whether it's issues that are associated with um environmental
extraction so here's one of the classic challenges is that the problems the catastrophic risks many
of them in the environmental category are the result of cumulative action long term where the
upsides are the result of individual action short term and the asymmetry between those is
particularly problematic it's why you get this collective choice making challenge meaning if I
cut down a tree for timber I don't obviously perceive the change to the atmosphere or to the
climate or to watersheds or to anything but my bank account goes up through being able to sell
that lumber immediately and the same is true if I fish or if I do anything like that but when you
run the Kantian uh categorical imperative across it and you have the movement from half a billion
people doing it to a pre-industrial revolution to eight billion and you have something like in the
industrial world a hundred x resource per capita consumption uh just calorically measured today
then at the beginning of the industrial revolution then you start realizing okay well the cumulative
effects of that don't work they break the they break the planet and they start creating um tipping
points that auto propagate in the wrong direction but that but no individual person or even local
area doing the thing recognizes their action is driving that downside and how do you get global
enforcement of the thing and if you don't get global enforcement why should anyone let themselves
be curtailed and other people aren't being curtailed and that'll give them game theoretic advantage
so this is actually there's a handful of asymmetries that are important to understand with regard to
risk all right we've covered plenty so far and so it's fruitful to have a brief summary we've talked
about the faulty foundation of our monetary system daniel argues that post world war two especially
our economic system has not only encouraged but been dependent on exponential monetary growth
and this can't continually occur we've also talked about the digital escape plan and how this is
an illusion at least in daniel's eye he believes that digital growth has physical costs because
their hardware their human attention limits their finite resources linear resources as he calls them
though i have my issues with the term linear resource because technically anything is linear
when measured against itself we've also talked about how moving to mars won't save us us being
civilization daniel believes that the idea of becoming an interplanetary species to escape
resource limitations is unrealistic perhaps even ethically questionable we've also talked about how
mind uploading is not what it's cracked up to be it may not occur and even if it does it's not the
answer because it's either unfeasible but even if it's feasible daniel believes it to be undesirable
another resource as we expand our digital footprint is the privacy of our digital resources you can
see this being recognized even by open ai as they recently announced an incognito mode and this is
where our sponsor comes in do you ever get the feeling that your internet provider knows more
about you than your own mother it's like they're in your head they can predict your next move when
i'm researching complicated physics topics or checking the latest news or just in general what
i want privacy on i don't want to have to go and research which vpn is best i don't want to be bothered
by that well i am you can put those fears to rest with private internet access the vpn provider
that's got your back with over 30 million downloads they're the real deal when it comes to keeping your
online activity private and they've got apps for every operating system you can protect 10
of your devices at once even if you're unfortunate enough like me to love windows and if you're
worried about strange items popping up in your search history don't worry i'm not judging private
internet access comes in here as they encrypt your connection they hide your ip address so your isp
doesn't have access to those strange items in your history they make you a ghost online it's like
batman's cave before your browsing history with private internet access you can keep your odd
internet searches let's say on the down low it's like having your own personal confessional booth
except you never need to talk to a priest so why wait head over to piavpn.com slash toe to and get
yourself an 82 an 82 discount that's less than the price of a coffee per month and let's face it
your online privacy is worth way more than a latte that's piavpn.com slash to now and get the
protection you deserve brilliance is a place where there are bite-sized interactive learning
experiences for science engineering and mathematics artificial intelligence in its current form uses
machine learning which uses neural nets often at least and there are several courses on brilliant
websites teaching you the concepts underlying neural nets and computation in an extremely
intuitive manner that's interactive which is unlike almost any of the tutorials out there
they quiz you i personally took the course on random variable distributions and knowledge
and uncertainty because i wanted to learn more about entropy especially as there may be a video
coming out on entropy as well as you can learn group theory on their website which underlies
physics that is su3 cross su2 cross u1 is the standard model gauge group visit brilliant.org
slash toe to to get 20 off your annual premium subscription as usual i recommend you don't stop
before four lessons you have to just get wet you have to try it out i think you'll be greatly
surprised at the ease at which you can now comprehend subjects you previously had a difficult time
rocking
the bad is the material from which the good may learn
so this is actually there's a handful of asymmetries that are important to understand with regard to
risk one is this one that i'm saying which is you have risks that are the result of long-term cumulative
action but that you actually have to change individual action because of that but the upside
the the benefit the individual making that action realizes the benefit directly and so this is a
classic tragedy of the commons type issue right but tragedy of the commons at a not just local
scales but at global scales some of the other asymmetries are particularly important is
people who focus on the upside who focus on opportunity do better game theoretically for
the most part than people who focus on risk when it comes to new technologies and advancement and
progress in general because if someone says hey we said we thought viox or ddt or any number of
things were good ideas they ended up or leaded gasoline they ended up being really bad later
we want to do really good long-term safety testing regarding first second third order
effects of this they're going to spend a lot of money and not get first to market and then
probably decide the whole thing wasn't a good idea at all or if they do decide how to do a
safe version it takes them a very long time the person says no the risks aren't that bad let me
show you does a bullshit job of risk analysis as a box checking process and then really emphasizes
the upsides is going to get first mover advantage make all the money they will privatize the gain
socialize the losses then when the problems get revealed a long time later and are unfixable
the the that will have already happened so these are just examples of some of the kind of choice
making asymmetries that are significant to understand the situation i only partly answered
your question sure are you having in mind a particular corporation currently totally okay
not a particular corporation but a particularly important consideration in the entire topic
mm-hmm one view is that google is not coming out with something that's competitive like bar
is not competitive i think even google would admit that and so one view is that well they're
highly testing another one i've spoken to some people behind the scenes and they say google
doesn't have anything they don't have anything like chat gpt it's bs when they say so even open ai
doesn't know why chat gpt works like gpt 4 works as well as it does they just threw so much data at
it and it was a surprise to them and in some ways they got lucky so do you see what's happening
right now between microsoft and google as google is actually the more cautious one and microsoft
is the more brazen one and perhaps should be a bit more circumspect i have heard a lot of things
about the choices that both companies made to not release stuff and safety studies that they did
and then what influenced the choices to release stuff inside of microsoft and open ai and how google
is handling it i don't know that the these stories are the totality of information on it that's
relevant do i think that economic forcing functions have played a role in something
that affected the safety analysis totally do i think that that is an unacceptably dumb thing on
a topic that has this level of safety risk associated totally so now getting into what is
unique about ai risk right what is unique about it relative to all other risks do we people are
saying things like we need an fda for ai right now which i would argue is both true and a profoundly
inadequate analogy because a single new chemical that comes out is not an agent it is not a dynamic
thing that continues to respond differently to huge numbers of new unpredictable stimuli
so how you do the assessment of the phase space of possible things is totally different
it would probably be good to dive into what what is the risk space of ai why is it unique and how
given all of the differences of concern how to framework and think about that properly
what else is unique about it and why can't we have an fda or a un version of an fda for ai
and when i say un sorry what i mean is global yeah well obviously you bring up un
and say global because you have to have global regulation on something like that right in the
same way that when people talk about climate regulation um if we were if any country if any
you know group of countries was to try to price carbon properly meaning what does it take to
renewably produce those hydrocarbons and uh what does it take to in real time
fix all of the effects both sequester the co2 clean up the oil spills whatever it is
the price of oil would become high enough with those costs internalized that oil then as an input
to industries literally every industry would be non profitable and so even if any country was to
try to make some steps in the direction of internalizing cost and other ones didn't then the other
ones who continued externalize their costs get so much further ahead in terms of gdp that can be
applied to militaries and surplus of many different kinds and advancing exponential tech that in so
far as those are also competing entities for world resources and control that's not a viable thing
this is true for ai as well and this then starts to hit this other issue which is
if you can't regulate something on a purely national level because it's not just how does it
affect the people in the nation but how does it affect the nation's capability to interact with
other nations now you get to the and so the creation of the un was kind of the recognition
in the existence in in the emergence of world war two that nation state governance alone was not
adequate to prevent world war obviously that's why the league of nations came after world war
one and it was not strong enough to prevent world war two now you get to the topic of
why so many people are super concerned about global government and don't want global government
and they'll say things like the risks are being exaggerated and blown out of proportion to be able
to drive control paradigms and the people who want to have a one world government or a powerful
nation government exaggerate the risks so that they can drive control paradigms where they will
be the one in the control side this can be excessive paranoia but it's also
a really realistic and founded consideration which is are there any radical asymmetries of power
where the side that had all the power used it really well historically you know it doesn't
look that good right and so we see a reason to be concerned about something like a one world
government that has no possible checks and balances but there's also a concern about not
having anything where you get some type of global governance if not government meaning some
some unified establishment that has monopoly of violence at least governance meaning some
coordination where you're everyone is not left in a multipolar trap saying we can't
bind our behavior because they won't and if they won't then we have to race ahead right we can't
stop overfishing because the fish will all get killed because they're doing the thing anyway so
not only will we not stop we will actually race to do it faster than them so they don't get more
resource relative to us those types of issues so obviously with regard to the environment we call
it a tragedy of the commons with regard to the development of possible military technology
we call it an arms race both of them are examples of social traps or multipolar traps
um briefly why do you call it a multipolar trap social trap is a term used in the social sciences
quite a lot to indicate a coordination failure of this type where each agent pursuing their own
near term rational interest creates a situation that moves the entire global situation long
term to a suboptimal equilibrium for everybody right and there's a lot of work in various fields
social science on social traps the first time i'm aware of the term multipolar trap and during
the conversation was in a the great article called meditations on molok by scott alexander
where he i believe he's the one who coined the term multipolar trap there and it's pretty close
to a social trap if i was gonna um define a distinction it might be something like
in a classic um
tragedy of the commons scenario where everyone is utilizing a common wealth resource like say
fishing or cutting down trees in a forest or whatever um
you're you're not necessarily in the situation where everyone is racing to do it faster than
the other person to destroy it just them simply not curtailing their own behavior and yet you have
a resource per capita consumption growth and a total population growth sets that the environment
can't deal with it um you still end up getting environmental devastation but as soon as you
kind of move over into hey even if i don't cut down the trees or i don't fish the other side is
going to so i literally don't have the ability to protect the forest but i do have the ability to
cut down some of it benefit myself or our people or tribe or nation or whatever it is
and if i don't the other guys will break it down anyways but they'll also use the economic
advantage of that against us and whatever the next rival risk conflict is so not only do i have
to keep doing it but i have to race to do it faster than they do i actually have to apply
innovation now and so this is where you get an accelerating dynamic and if you don't just have
two actors doing this but you have many actors doing this where it's very hard to be able to bind
it because how do you ensure that all the actors are keeping the agreement right you have to make
some non-proliferation agreement you have to have some way of ensuring that they're all keeping it
and you have to have some enforceable deterrent if anyone violates it those happen but it's not
trivial it's not trivial to enact those and it's particularly so let's say we've achieved that when
it comes to nukes in some ways though at the beginning of the current post-world war two system
there was only two superpowers with nukes and now there's roughly nine countries with them
there are not a hundred countries with them there aren't even 30 because we've done a really
intense job of ensuring that Iran and many countries that want nukes don't get them right and why
because there are not uranium mines everywhere kind of can see where they are uranium enrichment
takes massive capability that you can literally see from space right this radioactive activity
associated so it's somewhat easy to monitor that that's happening this is not true at all with
the newer technologies that provide more catastrophic capability so obviously with
AI right now and the regulation of it there are conversations about like we need to monitor all
large GPU clusters or something like that which to some degree can be done but in terms of applications
it takes a very large GPU cluster to develop an LLM it takes a very small one to run that LLM
afterwards right and then can you run it for destructive purposes and it takes a very large
capability to uh advance something like a CRISPR or a new type of synthetic bio knowledge
it doesn't take that much to be able to reverse engineer it after it's been developed
so this brings up this very important point of when technology is built
there's this general refrain that all technology dual use right meaning that
if it wasn't sometimes it's developed for military purpose first and then becomes
used for civilian normal market purposes but if it's being developed for some
non-military purpose there's probably a militarized application that's what's
meant with dual use is military versus non-military so it's not the same as this is a double-edged
sword it's positive and negative it's not the same as that yeah it is what it's saying is you're
developing this for some purpose but it has other purposes too right and it has purposes that can be
used for violence or conflict or destruction or something and well that is historically mostly
used with the concept of has a military application can be used to advance war and killing and things
like that whether by a state actor or a non-state actor so you'd call it terrorist activity
sorry when i was thinking of military i was also thinking in terms of pure defense not just
defense that also can be something that can attack yeah yeah the pure defense only military
um
it becomes it starts becoming part of most military doctrines that um
viable defense requires things that look like uh escalation but that's another topic as well
so it's not just that all technologies are dual use it's that
they have many uses right you develop a technology and i think a good way to think about
so now this is a little bit of theory of tech um did we close multi-polar trap
well you mentioned that it first came up in scott eric zinzer's alexander yeah and so basically
the concept is um you have many different agents who all of them pursuing their own rational interest
maybe they can't even avoid it because it would be so irrational it would be so bad for them game
theoretically that the effect of each of the agents pursuing their own rational interest
produces a global effect that is somewhere between catastrophic or at least far from the
global optimally if they could coordinate better so this is basically a particular type of multi-agent
coordination failure and we see this all over in the tragedy of the commons as an example
a market race to the bottom like happens in marketing and attention currently it's an example
and then arms race is another example those would all be examples of a kind of multi-polar trap
coordination failure this is why if you have say one nation is advancing by a weapons or advancing
ai weapons either ai applied to cyber or applied to drones or applied to you know autonomous weapons
of various kinds if any country is doing that it is such an obvious strategic advantage that every
other country has to be developing the same types of things plus the whole suite of counters and
defenses to those types of things and so you could just say well the world in which everybody has
autonomous weapons is such a worse world than this world that we should just all agree not to do it
except how do i know the other guy is actually keeping the agreement well with the nukes we
can tell because we can see if they're mining uranium and enriching it because it takes massive
facilities and they're radioactive and stuff like that but if we're talking about things like working
with ai systems or synthetic biosystems that don't require a bunch of exotic materials exotic
mining that don't produce radioactive tracers etc and they can be done in a deep underground
military base how do we know if they're doing it or not so if we don't know if the other side is
doing it or not then the game theory is you have to assume they are because if you assume they are
you're going to develop it as well and then if they do have it and use it you aren't totally
screwed whereas the risk on the other assumption that they aren't if you were wrong you're totally
screwed right so under not having full knowledge the game theory orients to worst case scenario and
being prepared against the worst case but what that means is all sides assume that of each other
we don't know that the other guys are keeping the agreement therefore we have to race ahead with
this thing and so so this is why you're saying when it comes to things like ai do we need something
that is not just an fda thing but a un thing is this the kind of thing that would require
international agreement and obviously when there was the question of creating a pause on a six-month
pause or whatever one of the first things people brought up is won't that let china race ahead
and isn't this a us china competitiveness issue and we can see with the chips act in trying to
ban asml downstream type gpu's to china and we can see with the pressures over taiwan and tsmc
that there is actually a lot of us china great power play competition related to computation
and ai and specific um and uh so it's a classic situation that if you can't put certain types
of control mechanisms in internationally you will probably fail at being able to get them
nationally as well so about this competition where the tragedy of the commons such that like
well the competitiveness plus tragedy of the commons accelerates the tragedy of the commons
why is it not much more simple religiously simple ethically simple where we go back and we say hey
what i'm going to do is outputting something negative i don't care that if you do it you're
going to get ahead i don't care if you're going to eliminate me i would rather die for your sins
rather than contribute my own sins so the selflessness why isn't that sort of ethically
we say we don't want to be luddites but why isn't that a solution i mean you're bringing up a great
point which is um can there be a long range thinking about the kind of world we want to live in
and a recognition of the kind of beings we have to be the the behaviors we would have to do and
not do for that world to come about where we bind ourselves right where we have some kind of whether
the ethics reduces to law meaning there's a monopoly of violence that backs up the thing or not
can we at least self-police in some way towards it and the answer is the long-term answer must
involve that i would argue um past examples have involved it but let's talk about where it's limited
one could argue that the Sabbath and the punishments for violating the Sabbath is an example of binding
a multipolar trap so you're you're not going to work on the Sabbath and if you do there's 29
different reasons laid out why you can be killed for work on the Sabbath it seems to um
to secular people not thinking about the chesterton fence deeply uh it seems like
a ridiculous wacky religious idea not grounded in anything within a ridiculous amount of consequence
now your theory of justice is is it only a personal or is it a collective theory of justice right some
theories of justice are your punishment is not based on just what was right for that one person
but creating an adequate deterrent for the entire population because if you don't what happens so
a classic example is Singapore's drug policy is pretty harsh right life in in uh prison for just
possession of drugs well that was following the devastating effect that the British had on the
Chinese with the opium wars and recognizing how as a kind of population-centric warfare the British
were able to influence like catastrophic damage on China they're like we don't want that here and we
know that there are external forces that will push to do that kind of thing and it's not just
personal choice once there are asymmetric forces trying to affect the most vulnerable people in
the most vulnerable ways so we're going to make it to where the deterrent on drug use is so bad
nobody will do it so if you if you say that you actually have to lock somebody up forever for
smoking pot which feels very unfair to them but you probably only have to do that like a few times
before nobody ever will fucking touch it because the deterrent is so bad and they believe it'll be
enforced we're hands off and if the net effect on the society as a whole is that you don't have
black markets associated with drugs and gangs and the violence that's associated and you don't have
ods and you don't have the susceptibility to population-centric warfare and whatever they
might argue a utilitarian ethical calculus that the harsh punishment was radically less
harm to the total situation than not having that right so you have a strong deterrent so that's just
and i'm not saying that i think that is an adequate theory of justice but it is a theory of justice
right so let's say that the Sabbath was something like this and i'm not saying that the rabbis that
were creating it at the time thought this so many people suggest that that's probably what they thought
some very competitive people wanting to get ahead will work every day you know if you they'll work
seven days a week and as a result they will they'll be able to get a little bit more grain farming
whatever then other people get more surplus start turning that into compounding benefits
and if anyone does it'll create a competitive pressure where everyone has to so nobody spends
any time with their family nobody spends any time connecting to what binds the culture together the
religious idea etc so we're going to make a Sabbath where no one is even allowed to work and there's
such a harsh punishment against it that we're binding the multipolar trap right because even
though it would make sense and they're in that person's rational interest to work that extra day
a few times to get ahead the net effect on the society cumulatively is actually a shittier world
so we're going to bind it because people having that time off to be with their family each other
and studying ethics is a good idea i would argue that religion has heaps of examples like this
of how do we bind our own behavior to be aligned with some ethic but i would also argue because
that was the question you're asking right is there some kind of religious bind to the multipolar
trap and i think the the Sabbath is a good example i think we can also show how well that didn't work
for Tibet when China invaded right which is um we want to be non-violent the oriented we have
a religions oriented towards non-violence and we can see that there were if you think about at the
time of Genghis Khan or Alexander the Great or whatever where you have a set of worldviews that
doesn't constrain itself in that way and it's going to go initiate conflict with the people who
didn't do anything to initiate it and don't want it but the worldview that orients itself that way
also develops military capability and maximum extraction for the surplus to do that thing
the other worldviews don't make it through they get wiped out
because so there are indigenous cultures and matriarchal cultures and whatever that we just don't
even have anymore don't even have the ideas of remnants because it just got wiped out by
worrying cultures and so does that does that produce the long-term world we want no it doesn't
either and so there has been this kind of multipolar trap on that the natural selection if you want to
natural selection if you want to call it that of worldviews that make it through are selected by
their ability to grow their population have outsized influence on other population and win
wars and basically things that don't necessarily map to a good world long term but the things that
might map to a good world long term might not ever get to the long term because they get wiped
out in the short term yeah i don't buy that so i'm not saying this as someone who's religious
or from a religious perspective well this is a religious perspective sorry but i'm not saying
this as someone who's advocating for a certain religion the most dominant religion in the world
is christianity and that's the story of someone who had the government against him and he said
no i'm not going to fight back in fact if you want to persecute me go ahead i will come to you
and one of the most striking stories literally striking in the bible to me is the story of
jesus the captor and peter his friend cut off the captor's ear the guy was going to take jesus
to kill jesus and jesus said no no no don't do that and took the ear and healed his captor
so think about this though yes jesus is the guy who said let he who has no sins cast the first
stone and they brought mary macklin and all those things but we somehow did the crusades in his name
and the inquisition in his name and the dark ages in his name right that's some weird ass mental
gymnastics but the scenes the versions that we're going to stay peaceful and not do crusades how
many do you see around and how much power did they get so what happens is you have a bunch of
different interpretations the interpretations that orient themselves to power and to propagation
propagate and make it through the um interaction between the means so memes engage in a kind of
competitive selection like genes do but not individual means mean complexes so if we have
a religion that says be humble be quiet listen to people and don't push your ideas on anybody
and then you have another one that says go out and proselytize convert everyone to your religion
and kill the infidels which one gets more people involved right and so the ones that have propagation
and that have um conflict ideas built right in so of course then the meme sets evolve over time
right the the religious interpretations don't stay the same and the meme sets that end up winning
through how they reduce themselves to the behaviors that affect war and population growth
and governance etc are all part of it so the fact that the dude who said let he who has no
sins among you cast the first stone got to be the religion that became dominant through the crusades
and through violent expansionism and then through radical torturous suppression is fascinating right
and it shows you that you have like a real philosophy and then you have politics of power
and you have fusions of those things that you have to understand both of when you're studying religion
to me and i don't mean to harp on this point but it doesn't have to be a choice between
hey let me do good and let me not push my views on anyone and proselytizing slash killing because
you can also proselytize and say your ideas and hopefully people will hopefully maybe there is
something in us maybe there's something cosmically in us i don't know that says hey you know what i
like that i don't like that killing i don't like where that will lead it resonates with me that the
sins get passed down or that the violence gets passed down and amplified but i need to be told
that so i do need to hear that because i can't come up with that on my own so that's why i'm
saying the proselytizing is a part of it whether proselytizing is explicit or it's lived and you
just see how someone lives and then you inquire hey what are your views and why are you so happy
when you have nothing and i'm so miserable and i have everything i just don't see it as a choice
between you do good locally and don't tell anyone about it or you can tell people about your ethical
system but also oppress them well to make the thought experiment we picked both extremes so we
can see that the Mormons proselytize but they don't kill everyone who disagrees in the crusading
kind of way they have not expanded as much as the crusades they've not got as much global dominance
or total population as a result but they have not got nowhere right we can see that the ones that
say hey if someone is interested we'll share but we're not going to proselytize because we have a
certain humility of how much we don't know and the respect for everyone's choice um
the mystery schools stay pretty small and um again when we were talking about asymmetries
that those who are more focused on the opportunity and downplay the risk move ahead get the investment
capital etc and those who are focused on the risk heavily don't there's a there's a similar thing
here which is like there's an asymmetry and the ideas that hit evolutionary drivers even if perverse
forms right like in the evolutionary environment it was where there was actual food scarcity we
evolved dopamine urgic dopamine opioid responses to salt fat and sugar which were hard to get and
useful as soon as we got the point where we could produce lots and lots of salt fat and sugar and
there was no scarcity on those things our genetics didn't change and so the fact that it felt really
good when you ate that and incentivize you to get more of it where that little bit of surplus might
mean you make it through the famine versus not it was an it was an adaptive response right then we
create a anthropocene where we have hostess and mcdonald's giving amounts of salt fat sugar that
are and the combinations of them with the kind of optimized palatability where it is not only is it
not evolutionarily useful to get it anymore it is actually the primary cause of disease
in the environments where that's available it doesn't mean that the dopaminergic signal changed
right so we're able to kind of take an evolutionary signal and hijack it and this is obviously what
fast food does to the evolutionary programs around food it's what social media does to the impulses
for social connectivity it's what porn does for the impulses to sexual connection associated
with intimacy and procreation and all like that is to extract the hypernormal stimuli from the rest
of what makes it actually evolutionarily fit same thing in app and religion right you can
offer people an artificial sense of certainty and offer them an artificial sense of belonging
and security and you know various things like that and without much actual deep philosophic
consideration or necessarily even deep numinous experience and that similarly has the ability
to scale more quickly than something where you want people to actually understand deeply discover
things themselves have integrated experiences not just do the right action but for the right
intrinsically emerging reasons which is why you know your podcast doesn't have one of the
reasons uh as many views as um
the most trending tiktok videos that are require less work and are shorter and more
oriented to hypernormal stimuli so i'm not saying we can't work with these things i'm
saying these are the things we have to work with so we're in a situation where the
let's say that we're in group in groups and out groups we both cooperate and compete at
different times based on uh what game theory seem to make most sense and they typically
cooperate while reserving the right to compete into even fully defect if they need to write
resource scarcity or something or just a sociopath coming into leadership which totally happens um
so the combination of the world views everybody needs to believe our religion if they don't
they are bad and so we're going to convert them or whatever right or everyone needs to be uh have
a democracy because that's good and all the forms of governance are bad or whatever it is that
there's ideology that orients itself there's a tech stack that is a part of the capacity to do
that they're coordination mechanisms that are about to do that so the full stack of the superstructure
the world views the social structure and the infrastructure what are engaged in in group out
group competitions and that are up regulating largely shaped by those competitions it just happens
to be that the version that makes it through that shaping process is also orienting us towards a whole
suite of global catastrophic risks it is basically self terminating and so it has been the case that
you have to win the local arms race because otherwise you lose but the arms races that
are externalizing harm but on an exponential curve that have cumulative effects you don't
actually get to keep externalizing on an exponential curve or running arms races on an
exponential curve in a finite space forever so we're at this interesting space where you can't
try to build an alternate world that just loses but you also can't keep trying to win in the same
definition of when this is the interesting point we're at which is we have to actually
build a version of when that is not for an in-group in relationship to an out group
but is something that actually allows some kind of omni-win that gets us out of those
multipolar traps and this was all coming from the topic of you starting with why you brought up the
UN and that you have to deal with these things with some kind of sense of how are other people
dealing with them and how does that affect the choice making process some people would say look
we're group selected and then we can make our group to be the tribe versus another tribe and
one of the solutions is if there was aliens and then we could bind together as humans and fight
something external it doesn't have to be aliens the point is that there needs to be something
external so you're saying there's another option and that that option the bind together in order
to fight some other out group whether the group is something physical or it could be more abstract
that's not something that should be pursued and there's another option
I didn't say that but it's an interesting conversation
if we are not binding in groups to fight out groups so this is kind of like
Machiavelli's enemy hypothesis that people are kind of evolutionarily tribal and that to
unify a lot of people at a much larger than tribal scale given that they have they naturally will
find their own differences and conflicts and reasons to otherwise somebody because they
have more influence over their own small group or whatever to unify them works best if you have a
shared enemy that forces them to unify and so then you're you eventually of course this makes
small tribes unified to deal with a larger tribe and then you get kingdoms and nation states and
global economic trading blocks and eventually you get great superpower conflicts and that if the
only way to unify that if groups opposing each other in that way ends up being catastrophic for
the world so we want to get everybody unified in some way do we need a shared enemy obviously
this has been talked about a gazillion times can can climate change or environmental harm be the
shared enemy not really even if everyone believed in it which they don't it doesn't hit people's
agency bias in the same way and whatever could we stage a false flag alien invasion that may
unify of course this has actually been an explored topic both in sci-fi and reality and
it's it how deeply explored is a question but yes it's a very natural topic to explore that
uh something like a attack from the outside would allow that kind of unification because of that
there are people who are very skeptical and concerned of anything that looks like a presented
shared threat that should create some unified response because then they're like well what is
the government that regulate that will navigate that shared threat and who has any checks and
balances on that if that thing becomes captured or corrupt and so this this is again the catastrophes
or dystopias if you don't have some coordination you get these problems of coordination failure
if your coordination is imposed you end up getting oppression dynamics so how do you get
coordination that is global but that is emergent that has that keeps local power from doing things
that drive multipolar traps but that also ensures that you don't get centralized power that can be
captured or corrupted right a a system of coordination has to address both of those things and as we
move into in more people with more resource consumption per capita and more and the cumulative
tipping points on the the biosphere being hit but even more than that exponentially more power
available to exponentially more actors obviously if we look at the history of how humans have used
power and you put an exponential curve on that it doesn't go well so uh yeah that's one way of
thinking about the coordination issue curve when we were thinking about the un or whatever is this
global agency potentially the phrase they have no checks and balances comes up is there a way
of organizing something that is global and influential that has its own internal checks
and balances i don't understand how the u.s political system works it's my understanding
that it's tripartite and antagonistic i don't understand the details of it i'm apolitical at
least consciously i haven't looked into it but the point is i can that's interesting i don't know
how that works i wonder how much that doesn't work how much that can be accelerated amplified
well one point that we bring up is that any proposed system of coordination governance whatever
is not going to work the same way after it's been running for a long time as when it was initially
developed because uh all of the systems have a certain kind of
institutional decay or entropy built in that has to be considered because every vested interest
that is being bound has a vested interest in figuring out how to break the control system
or capture or corrupt it or something right and so it's not just how do we build a system that
does that but it's also how do we build a system that continues to upregulate itself to deal with
an increasingly complex different world than the one it was originally designed for and that continues
to deal with the fact that wherever there is an incentive to game the system is going to happen
so you have to not only figure out a system that makes sense currently but a system that has
an adaptive intelligence that is adequate for the changing landscape so when you look at the US
because leaving corrupt monarchy was key to the founding here and so we were going to try to do
this um democracy non-monarchy thing it was also the result of a change in tech right it was a
result of the printing press where rather than before a printing press and everyone could not
have textbooks and couldn't have newspapers and to have access to information someone had to copy
a book by hand which meant that there were very few of them or copy the information by hand so
only the wealthy could have it the idea of a wealthy nobility class that got educated enough to
make good choices for everyone else where if they were too corrupt the people would overthrow them
so there was a certain kind of checks and balance that kind of maybe made sense right with a no
bless oak leash built into the obligation of the nobility class to rule well i'm not saying it
did but that's at least the story but as soon as the printing press comes and now everybody could
have textbooks and get educated and everybody could have a newspaper and know what's going on
it kind of debases the idea that you need a nobility class to make all the choices because
everyone else doesn't know what's really going on and you say well maybe we could all get educated
enough to understand how to process information and we could all get news to be able to understand
what's going on and all have a say and so um obviously democracy emerged following that change
information tech i'm saying this because of course ai is a radical change in information
tech that will also obliterate our existing political economies and coordination systems
and make new ones and changes to culture as well the difference in the ai case just briefly is that
i don't see the ai is democratizing more so than exacerbating the inequality in terms of like so
if you're extremely bright the amount of information you can process is going to be far outpacing
someone who either is not so bright or gets access to that ai three weeks later
so thinking through in the same way that the printing press had an effect on
central religion through everybody can have a bible and read it and learn on their own and kind
of Lutheran revolution and it had an effect on central government in the form of feudalism
that we can then look at kind of McLuhan's insights of how information tech changes the
nature of the collective intelligence and motivation and type of mind that everyone
is operating with and as a result the emergent type of society we can look at the way that
the internet and digital have already done that looking at the way social media has affected
media for instance which affects our democratic systems is a pretty obvious one
um but then we can look at ai and not just ai but different types of ai different ways it could
develop lm is very different than other kinds of ai it's right so we'll come to that in a moment
but let's come back to the other question because you were asking the checks and balances one
so the idea in the us system was the british system following the magna carta and the treaty of
forest and whatever was supposed to be the most ideal noble thing around and ended up being in
their experience a totally corrupt thing so the idea that no matter how you develop a system it
can be corrupted that was built in so how do we make sure that no part gets too much power and
that we have checks and balances throughout was kind of key so before you even get into the three
branches of government you also you already have the separation of the state and the church
which was already a key part and you have the separation of the market and the state right
which is the uh you have a liberal democracy that is proposed so you don't have a pure market
function but you also don't have that the state is running the entire economy and so the separation
of the uh market the state the church is a few other ways of thinking about separation was already
a part of it and then with regard to the state's function the separation of the legislative the
judicial and the executive were critical and then within each of those within the legislative
a bicameral breakdown was really important and that then the tint amendment was to push as much
power the subsidiary principle to the states as possible and as little to the federal so
there were many many steps of checks and balances on concentrated power that were built into the
system but of course everyone who is smart who is also a genetic who wants more power looks for
loopholes and or figures out how to write laws and to get them passed right doing legislation
and lobbying and of course uh corporations can pay for a lot more lawyers than an average citizen
can or than a nonprofit group that doesn't have a revenue stream associated so the group that is trying
to um turn uh commons into commodities versus one that's trying to protect the commons will
inherently have a bigger revenue stream to employ media to change everyone's mind or to employ campaign
budgets or to employ lobbyists or whatever so you end up seeing that there is a um progressive
kind of loophole finding corruption because the underlying incentive systems invested interests
have are still there right um bogeard simulation and simulacra that discusses the steps of the
degradation from a new system to how it eventually devolves into mostly a simulation of what it
originally was is a good analysis on this we could discuss but um so that's a little bit on
kind of the history of checks and balances on power but i don't think anybody looks at our
current us system and says it's doing a great job of that and there's a bunch of reasons
in addition to the one that i said about how there is a natural process of um figuring out
how to influence this like everyone who okay there's one other part that's actually worth
saying so you have a state you have a market and you have the people as um
members of a democratic uh government meaning their function in state not their function in
market so uh government of form by the people the people might not all be um representatives but
they can all speak to the representative decide how it votes those types of things right so there's
supposed to be a check and balance between these three that the main reason that there is law is to
prevent some people or groups of people from doing things that they have an incentive to do that would
suck for everybody else obviously whether it's individual um stealing or murder or whatever
or it's a corporation and cutting down the national forest or polluting the waterways
too much there is somebody has an incentive to do something and in a democracy where the idea is
supposed to be that the co we all want and value different things but the collective will of the
people as determined through some voting process gets instantiated into law where a monopoly of
violence can back that up that's kind of core to the idea of a liberal democracy right i'm not
arguing that it is a good system but i'm arguing for the the the core logic of it and it's because
the recognition that if we just had a pure market system the reason why there wasn't just a pure kind
of laissez-faire system even though the people building this understood at least their expressed
reason is uh an impure type market dynamic as you were mentioning with ai some people are way better
at than other people and as a result we'll just end up getting a lot more money that they can convert
to more land resources employees etc and you end up getting a power law distribution on wealth which
is a power law distribution on everything and these people's interests end up determining the whole
society and these people's interests are pretty determined for them and so if you want to create
protections for these people at all now is basically the king george situation and the
inspiration for the declaration of independence and leaving which was there was too much concentrated
power and it was kind of fucked so how do we make that not happen well since we know that the market
is going to kind of naturally do that let's create a state that is more powerful than any market actor
and let's make sure that the state reflects the values of all the people so the little guys
get to unify themselves through a vote right and then you get to have a representative that
represents everybody it's the only one given a monopoly of violence and it gets to make sure that
any more powerful actors are checked that's kind of the idea yeah so so far this is an account of
how it's been like a history lesson but you aren't saying this is how it should continue to be nor
this is how it's operating in its ideal sense currently are you just saying that this was the
reasoning behind it uh of one key part of how it broke down so the idea is that the market
people will have incentives to do things that are good for them that might suck for the environment
or others and so others have the ability to agree upon laws that will bind those actors to not do
that thing right so the state is supposed to check the market let the market do its thing do
resource distribution productivity let it do that because it's good but check the particularly
applications and in order for the state to check the market the people are supposed to
check the state and ensure that the state is actually doing the thing that it's supposed
to do and that the representatives aren't corrupt and taking back in deals and all those kinds of
things right and then there's a way in which the market kind of checks the people meaning that the
people can't there's an the accounting checks them they can't vote themselves more rights than
they're willing to take responsibility for they they can't make the economics of the whole situation
not work right they can't vote themselves a bunch the they can't if the people all say yes we should
all we should all get no taxes but lots of social services then the accounting is what
actually checks the people right um so that's the idea of how you have this kind of self-stabilizing
thing but of course the people stopped checking the market once you were out of kind of the sense
of an eminent need for revolution then the people have a lot of shit to do other than really pay
attention to government in detail and there's a bunch of other reasons beyond the scope of this
conversation why the people stopped checking the government in which case the market is continuously
trying to influence the government through lobbying and legislation and campaign finance and all those
other things and so then you end up getting regulatory capture rather than regulatory effectiveness
right so when you put those checks and balances it's going to change everyone everyone's scared
of concentrated power following the revolution it's different than four generations later where
nobody actually feels that fear anymore and is busy doing other shit right so it's not just how
do you build your system it's how do you build a system where the initial people that went through
the difficult thing to build it when they die you didn't just pass on the system but the generator
function of the kinds of insights needed to keep updating and evolving the system under an evolving
context so when you ask the question about could such a thing be built at an international level
where there are checks and balances the answer is it's super hard but yes but it's not just can
you design it properly up front it's also can you factor how that system then even if well intended
at first it's kind of like all technologies dual use so you build the immuno on you you build the
gene editing for immuno oncology but then it can be used for bio weapons you have to not just think
about what you're building it for but all the things that will happen having created that thing
same thing with government you have to think about not just who you're building it for right now
but as the landscape changes culture changes can this thing be corrupted can it be captured
in future different contexts and how do you build in immune systems to that
and that sort of thinking seems to be missing with the development of AI and it reminds me
I've said this several times like the development of the bomb where Feynman and Oppenheimer mainly
Feynman and his peers said they didn't think about what they were creating they were thinking
we're having fun speaking about these topics it's even more fun to do research on these topics
Einstein said like I burned my hands had I known that this was what was going to be developed I
wasn't thinking about that I wasn't thinking about the consequences and Feynman said something
similar we're consumed with the achieving of a goal and we're not thinking about what would
occur as a consequence once we attain it and you hear this constantly in the AI scene channels
like two minute papers that say what a time to be alive that's like his catchphrase what a time
to be alive like encouraging and amazed constantly thinking oh what is going to be like two papers
down the line said enthusiastically I see little caution expressed yes geez Louise like what the
heck are we building and should we just because we could should we when the people who express
caution this now relates to this asymmetry we said if people are like hey this is extremely risky
technology we need to understand the risk space very deeply first we need to ensure the development
of the technology and then it's future use by everybody is safe enough to be worth built
those people end up running nonprofits because there's no upside to that there's no immediate
capital upside to that so they have a hard time getting the capital to get really good researchers
or big enough computers and data sets to try to run stuff on for trials and the people that are like
oh there's a market application to this have a much easier time getting a massive GPU cluster
and a lot of talent and a lot of data and so we can see this if you name the names that are out
there in their views and then map them to the types of organizations they run and the type of
mode motivational cognitive bias it it's somewhat maps right so what is this is what we intended
to talk about all this has been interesting preface what is the actual risk space for the AI
what do we know what do we not know how should we think about it how should we proceed especially
given that AI is a lot of different things should we dive in there now sure I just wanted to point
out that although this seems like a disagreement between you and I on the surface there's an
agreement so again I'm not a Mormon but I don't see the Mormons failure because they go when they
say hey you should whatever they say act right be humble be kind don't over consume and so on
but then their religion doesn't grow I don't see that as a failure of them because maybe their
religion is larger than just being Mormon it's something about the values that are spread and
then they send out these values and they filter through the community in the same way that these
nonprofits just because they're not the largest doesn't mean that the values that they send out
don't influence you and I and influence the people who are listening who then act differently
because of these values we have no idea how much the pacifism of Tolstoy has influenced you hugging
your father and your brother and the positive sentiment we have generally speaking in society
toward decentralization and it also reminds me of the Cassandra's for people who don't know what
a Cassandra is it's someone who makes a prediction of it's a doomsayer it's akin to a doomsayer
they're the opposite of a self-fulfilling belief so self-fulfilling belief is one where you state
it and you create the conditions such that it becomes true whereas in the best case for the
people who say the world is going to end well their success depends on them being self-sacrificing
depends on us then being able to repudiate them let the world hasn't ended we have no idea how
much the doomsayers or the fear mongerers said something that made us straighten up and act
right just enough that it pushed us off of the brink and influenced us to make society live in so
we then have this archetype of the cautious and contomacious false cravens of the past it's just
not clear that because they have died doesn't mean that they're unsuccessful that's what I mean
yeah so there's a this important point um the idea that um even though
Greece didn't um continue its empire relative to Rome that its memes ended up influencing
the whole Roman empire uh and so in some way it won or similar with uh Judaic ideas or whatever
one of the greatest examples of that that people talk about right now is Tibetan Buddhism
which is okay so from the point of view of um Tibet as a nation the Tibetan people the
um integrity of that wisdom tradition it was radically destroyed but in the process of the
world seeing that and having some empathy and gendered for it even though it didn't protect
Tibet uh did that actually disseminate Buddhist ideals through the whole world radically faster
there's a similar conversation as uh so many people become interested in ayahuasca and plant
medicines from indigenous cultures that the economic pressure of that is making what is remnant
of those cultures get turned into tourism and ayahuasca production or shipibo production or
whatever it is that on one hand it actually looks like a destructive act on those cultures in the
other way it's are the memetics of those now becoming dispersed throughout the dominant systems
that is a part of the consideration set that has to be considered
um and now do we see for instance that particularly nonviolent groups like say the
Jains as an example of maximum nonviolence um that those memes do become decentralized and
affect everybody it's it's it's not a simple yes or no right there's a whole bunch of contextual
application so when we look at his are there memes from buddhism that have influenced the world yes
but are they the ones that are compatible with the motivational set of the world they influence
so you've got this like buddhist techniques of mindfulness for capitalists in silicon valley
to crush it a capitalism is a very weird version of the subset of the buddhist stack right um and
i remember when i first started seeing kind of the um popularization of mindfulness techniques in
business so that people could focus better and crush it a capitalism how fucking hilarious that
thing is right because in some ways you are distributing good ideas in other ways you're
actually extracting from a whole cultural set the part that ends up being a service ingredient to
another set um so the topic of what makes it through i mean it's a complex topic what i will say is
the idea that a civilization which is its its superstructure its worldview values
what is true good beautiful what it's oriented to what's the good life
its social structure meaning its political economy and institutions its formal incentives
and deterrence and how it organizes collective agreement and its infrastructure the physical
tech stack that it mediates all this together in a civilizational stack
one of those competing with other ones for scarce resource and dominance
and all of them engaged in that particular competitive thing
i would argue no one actually gets to win that that process of the competition of those relative
to each other does actually create an overall civil global civilizational topology that self terminates
but also no one trying to create a good long term future can just lose at that game in the
short term so you can neither create the world you want by just losing at that game
nor by trying to win at it it's something else it's actually having to abandon that game to try
to change the game dynamics themselves ah okay so i was just watching arrest of development
this that you can't play the game you can't frame it in the same way so buster from arrest
development i don't know if you've seen it he's this mama's boy he wants to go out with this other
girl who happens to have the same name as his mom but anyway she's like i don't want to go out with
you because you're just in love with your mom that he's like no and he just left his mom's home
he's like 40 years old or 35 and he's like no no no it's the opposite i'm leaving my mom for you
you're replacing my mom and then she's like no and it's because he's still framing it in the same
way anyway we have to abandon the frame so please what does that look like and integrate ai into this
answer so we have not just tried to give the frame on ai risk yet and to incorporate that
in the what is the long-term solution for civilization as a whole look like let's actually
just kind of do the ai risk part first and then we can bring it back together um let's
try to frame how to think about ai risks ai opportunities and potentials including how
i can help solve other risks which has to be factored um i will add as preface that i uh
i'm not an ai developer i don't have um background in that i'm not even the ai risk expert or
specialist i know you're in conversation um might have you'd kowski and other people uh who really
are steward russell bostrom all those guys would be great um because of some maybe uh
novel perspectives about thinking about risk and governance approaches to risk writ large
metacrisis that's the perspective that i'm taking into the ai topic so um
what is unique about ai risk relative to other risks we were talking earlier about
environmental risks and risks associated with a large-scale war and breakdown of human systems
and synthetic bio and other things if we look at other technologies that are that have the
potential to do some catastrophic things like nuclear it's very easy to see that uh nuclear
weapons don't make better bio weapons they don't make better cyber weapons um they don't even make
better nuclear weapons directly and the same is true for biotechnology it doesn't automatically
make those other things ai is pretty unique in that you can use ai to evolve the state of the art
in nuclear technology in delivery technology and intelligence technology and bio and cyber and
literally all of them so it is unique in its omnipurpose potential in that way because of
course all those other technologies were developed by human intelligence human intelligence agency
creativity some unique faculties of human cognitive process and so where all the other
technologies are kind of the result of that human process building a technology that is
doing that human process possibly much faster and on much more scale is obviously a unique kind of
case right and um so there's thinking about what type of risk does an ai system create on its own
but then they're thinking about how does how do ai systems affect all other categories of risk
right we have to think about both of those um and then in addition to the fact that the nukes
don't automatically make better bio weapons the nukes don't even automatically make more nukes
right they're not pattern replicating but to the degree that we actually get ai systems it's not
only can make all the other things better but they can make better ai systems and to the degree
that there starts to be something like autonomy in that process then the self-upgrading and
omnipotential of all the other things um it's also true that uh there's a exponential curve in
the development of hardware that ai runs on right better GPUs and all of different kinds of
computational capabilities there's an exponential curve in iot systems for capturing more data
to train them on exponentially uh more people and money going into the field
because of the way that shared knowledge systems work a kind of exponential development in the
software and cognitive architectures so we're looking at the intersection of multiple
exponential curves not just a single one that is also kind of important and unique to understand
about the space so thinking about the case of ai turning into a gi an autonomous artificial
intelligence system that we can no longer pull the plug on that has goals has objective functions
whatever they happen to be that is you know something that guys like bostrom and yudkowski
have done a very good job of describing why that's a very risky thing i think everybody
at this point probably has a decent sense of it but just make it very quick
when we say a narrow ai system we mean something that is trained to be good at a very specific
domain like beating people at chess or beating them at go or um being able to summarize a large
body of text when we say general intelligence we mean something that uh could maybe do all of those
things and can figure out how to be better than humans at new domains that has not been trained on
through you know some kind of abstraction or lateral application of what it already knows
so if you put us into an environment where we have to figure out how we have to figure out
what is even the adaptive thing to do we will do it right there's a certain kind of general
intelligence that we have so we talk about a generally intelligent artificial intelligence
the idea and then of course because we can develop ai systems one of the things it could
do is develop a system so if it it has more cognitive capability than us in some ways it
can develop a better ai system and that one could recursively develop a better one and you get this
kind of thought about recursive takeoff in the power of an ai system and there are conversations
about whether that would be slow or fast is there an upper boundary on how intelligent a system could
be and humans are near the top of that or are we barely scratching at the beginning of it and we
could have something millions of times smarter than us so that's all kind of part of that conversation
but the idea that we could create a artificial intelligence that is that could basically beat
us at all games right that could which it could think about economy and affecting public opinion
and military as games and it has faster feedback loops faster to the loops to get better than we
do so if we're like trying to deal with it it's going to win at newly defined games and if that
thing we can't pull the plug on and it can anticipate our movements and beat us at all games
if it has goals that are directly antithetical to ours or not even directly antithetical but
the way in which it fulfills its goals might involve externalizing harm to things that are
part of our goal set that's bad for us right so the idea of don't let that thing happen
prevents getting to an unaligned agi that's that particular category of risk and so there are
arguments around could an agi like that is it even possible that's one question if it is possible to
have such a thing is it possible to align it with human interests what would that take
um is it possible if it is possible to align it is it possible to know ahead of time that the
system you have will be aligned and will stay aligned right like those are all some of the
questions in the space and then do our current trajectories of AI research like transformer
tech or just neural networks or deep learning in general do these converge on general intelligence
and if so in what time period those are all some of the questions regarding the agi risk space
now i want to talk about uh that risk but i want to talk about other risks using that as a
example in the space any questions or thoughts on that one to begin with sure number one is that
we may already have artificial intelligence in a baby form like we have hugging face i don't know
if you know what that is and then there's the paper of sparks of artificial general intelligence
and that's distinguished from something that updates itself i just want to make that clear
so there's a lot of questions regarding does it have to be better than humans at everything
to be an existential risk we could imagine we could imagine a
um von Neumann machine that was self replicating and self evolving that was not
better at everything but better at turning what was around it into more of itself and
evolving its ability to do so and just having way faster feedback loops and we could imagine
that becoming an existential risk with a speed of a particular type of intelligence that does not
mean better than us at everything yeah that's a great point like an asteroid is not better than
us at almost anything but it can destroy us and if yeah it's not doing it through some kind of
process that involves learning or navigating competitions at all it's just kinetic impact
this would be a kind of intelligence but it could be a one that's a lot more like a very bad pandemic
right and the intelligence of a of a pathogen than the intelligence of a god
um so talking about if the system is generally in like what type of intelligence it would need
is it generally intelligent is it autonomous is it agentic is itself upgrading right there
they're related concepts but they're not identical concepts
so what I um so let's go ahead and put the category of AGI risk as one topic in the AI risk space
um let's come to a much nearer term set of things which is uh AI empowering bad actors
and we can talk about what of that is possible with the existing technology
what of that is possible with impending technology that we're for sure going to get on the current
course versus things where we don't know how long it's going to take or even if we'll get there
so with regard to AI empowering bad actors we could say well how one defines the bad actor
is obviously a because one person's freedom fighter is another person's terrorist um but uh
so we can imagine someone who is terrified about environmental collapse deciding to
become an eco terrorist being a maximally good actor in their worldview but saying that the only
answer is to start taking out massive chunks of the civilizational system that's destroying the
environment so um I'm simply saying that I'm not being simplistic about what we mean by bad actor
but oriented to from whatever motivational type whether it was pure satism whether it's nihilistic
burn it all down or whether it's well motivated but maybe misguided considerations um but AI for
some destructive purpose right so now uh this is something we have to address first
one thing I have found when people think about um how significant AI risk will be
and how significant AI upside will be first on AI upside it's just important because if
we talk about risk and we don't talk about upside it will be easy for a lot of people to say oh this
is a technopessimist luddite perspective and kind of dismiss it at that so I would like to say there is a
the upsides of AI the best case examples that everyone is interested in everyone is interested
in they're awesome right can we all the things that we care about that we use intelligence
to figure out where intelligence is rate limiting figure them out rate limiting to figure out the
rest of the problems could we use it to solve those problems so could AI make major breakthroughs
in cancer and immuno oncology and does anyone who's talking about slowing down AI are they
factoring all the kids that are dying of cancer right now and if we could speed that thing up could
we affect that in our like that's a very personal very real thing right so AI applied to curing all
kinds of diseases and and AI applied to psychiatric diseases and scientific breakthroughs and
maybe resource optimization issues that help the environment and maybe the ability to help
with coordination challenges have applied in certain ways the positive applications that
the kind of customized AI tutoring that could provide Marcus Marcus Aurelius level education
were the best tutors of all of Rome were personally tutoring him in every topic could
provide something better than that to every human right could democratize aristocratic tutoring
there was a Eric Holes art essays on aristocratic tutoring are really good you should bring them
on here but basically it was a something many people have come to which is that the great polymaths
and super geniuses the highest statistical correlator that pops out is that they all had
something like aristocratic tutoring when they were young were the vast majority of them
that even the anointment in Einstein had mathematicians as governesses before they went to school
and Terry Tao had Paul Erdos there's this famous image of I don't know who had an Edwin though
so that is a many of many of the people simply had parents that were very actively involved
scientists philosophers thinkers you know but but if you think about why Marcus Aurelius
dedicated the whole first chapter of meditations to his tutors and if you think about how the
Dalai Lama was conditioned where you find this three-year-old boy and have the top
long as an olive to bet two to them on everything that is the whole canon of knowledge
of course if that was applied to everybody we'd have a very different world right this is very
interesting insight because it says that the upper boundaries on that a lot of what we call
human nature because it's ubiquitous it's not nature it's nature through ubiquitous conditioning
that the edge cases on human behavior show conditioning in common and that if you could
make that kind of conditioning ubiquitous you would actually change the human condition pretty
profoundly but as we move from feudalism to democracy and wanted to kind of get rid of all
the dreadful unequal aspects of feudalism looking at the fact that like you can't learn to be a
world-class mathematician by a person who's not a world-class mathematician the same way you can
by one who is and you can't you don't get a bunch of world-class mathematicians becoming third grade
or eighth grade high school teachers you know or school teachers so how would you do that so it's
kind of repugnant from a privileged lack of democratized capability point of view right
and yet could you have could I make LLM trained AIs and better than LLM ones where I can have
von Neumann and Einstein and Gertl all in a conversation with me about formal logic
where they are not only representing their ideas but maybe even now have access to all the ideas
since them and are pedagogically regulating themselves to my learning style that's kind
of amazing right like and could they maybe be doing that based on also psychological development
theories a colleague of mine zekstein has been working on this a lot of how to be evolving not
just their cognitive capacity but their psychosocial moral aesthetic ethical etc a full suite of human
capacities so I'm simply saying a AI applied rightly there's a lot of things to be excited in
optimistic about and so that's a given and we could do a whole long conversation on more of those
examples there is a when when I look at how people orient to the topic of AI risk
one of the things that seems to be a common kind of where their knee jerk reactions before
understanding all the arguments pro and con well comes is how much they they have a bias towards a
kind of techno optimism or techno pessimism kind of a where pinker hawns rossling there are still
problems but they're getting better the world is getting progressively better and it's a result
of things like capitalism and technology and science and progress and so more of that will
just keep equaling better and yes there will be problems but they're worth it right versus
so that's I would call that techno capital optimism pro but the naive version that doesn't
look at the cost of that thing we would call a naive progress dialectic
in the dialectic of progress is good progress is not good or we're really making progress versus
we're actually losing critical things are causing harm whatever they say in that dialectic that's
on the progress side but a naive version of it and so most of the and just to address that briefly
so the naive progress story looks at all the things that have gotten better and you can see
this in lots of good books and pinkers books deamondus's books rosslings talks on and on
and then the extension of that into the future deamondus starts to do and we could say Kurzweil
is kind of an extension of that far out why naive is if it doesn't look at what is lost in that
process and what is harmed in that process as well as the increase in the types of risk that are
happening and so I would argue that most of those things in every kind of rossling presentation
is cherry picking its data out of a humongous set to make a cherry picked argument this is one of
the reasons that fact checking is not enough is because you can cherry pick your facts you can
frame them in a particular way and create a conclusion that the totality of knowledge wouldn't
support at all because of that process right um I would say there is a naive technopessimism or
luddite direction that looks at the real harms tech causes culturally socially environmentally
or other things and uh wants uh back to the land of movement and organic natural
traditional whatever various types of and if it is not paying attention to the types of
benefit that are legitimate that's naive but if also if it's not paying attention to the fact
that that worldview will simply as we talked about before not forward itself because the one that
advances more tech will develop more power and end up becoming the dominant world system
that also means that it's not actually having a worldview that can't orient towards shaping the
world so we have to so putting those together I would say all of the things that the techno
optimist say tech has made better and all of us like a world we're going to the dentist involves
novocaine versus not not novocaine and where we have painkillers and where we have antibiotics
under infection and stuff like that all of the things that tech has made better have not come
for free there have been externalized costs and the cumulative effect of all of those costs
is really really significant and so if you look at the progress narrative the indigenous people
that were genocided don't see it as a progress narrative the fact that there are more there's
more biomass of animals in factory farms and there isn't the wild today does not see that
as a sign of progress the animals that live in factory farms or all the species that are
extinct don't see it as progress the fact that we have many many different possibilities of
destroying the life support capacity of the planet relative to any previous time or that
almost no teen girl growing up in the industrialized world doesn't have body dysmorphia
where that was not ubiquitous thing there's a lot of things where you can say damn those
technologies upregulated some things and externalized costs somewhere else if you factor the
totality of that then you can say okay there are a lot of positive examples any new type of tech
can have but there's also a lot of externalities and harms it can have and we want to see how to
get more of the upsides with less of the downsides and that can't be a rush forward process right
that actually requires a lot of thinking about how to do that so i'm actually i actually am a
techno optimist in a way meaning i i do see a future that is high nature stewardship high
touch and is also high tech high touch yeah meaning
um
that the tech does not move us into being disembodied heads mediating exclusively through a digital
world but a so i would argue that your online relationships don't do everything that offline
relationships do they do some additional things like distance and network dynamics whatever
but if they debase your embodied relationships they're causing a harm if they if the online
relationships improve your embodied relationships not just the create online relationships and
debase them then that's a different thing right so that's what i mean by high touch
um so i want to say that naive techno optimism is if there's you know if we look
at the history of corporations that are developing technology market technology advancement focused
on the upside and um not terribly focused on the downside we look at four out of five doctors
choose camel cigarettes we we look at better living through chemistry providing um ddt and
parathion and malathion uh we look at adding lead to gasoline in a way that tooks a toxic
chemical that was bound in ore underneath the biosphere and sprayed it into the atmosphere
ubiquitously it's dropped about a billion iq points off the planet made everybody more violent
in terms of its neurotoxicology effects trusting the groups that are making the upside on moving
the thing to figure out the risks historically is not a very good idea and i'm mentioning that in
terms of now trusting the ai groups to do their own risk assessment um and if you think about
the totality of risks well then you want to say how do we move the positive applications of this
technology forward in a way that mitigates the really negative applications of it that if one
wants to be a techno optimist responsibly they have to be thinking about that well so what about
the ai companies that say we do third party testing for safety are you still consider that somehow
internal because they're the ones going out depends um if so when i early in the process of getting
into risk assessment i had times where corporations asked me to come do risk assessment on the
technology or process and then when i didn't on this risk assessment they were not happy because
what they wanted me to do was some kind of box checking exercise that wouldn't cost them very
much and wouldn't limit what they were going to do so they had plausible deniability to say they
had done the thing and move forward quickly because what they didn't want was for me to say
actually there is no way for you to pursue the market viability of this that does not
cause excessive harms or were you dealing with those harms messes up your possibility for margins
without breaking any nda of yours that you may have signed are you able to go into what a company
did that you disapproved of and what was the result of it just as an example to make this
more concrete for people who are listening yeah totally um i have seen this in the example of
something like uh mining technology that it or a new type of packaging technology
that is wanting to say why it's doing something that addresses some of the environmental concerns
it addresses the environmental concerns it identified we identify a bunch of other ones
that it doesn't address well that it moves some of the harm from this area to the other one
that's an example of where some of the problem would come but i find this is just as bad in the
non-profit space or in the government space as well not just in the for-profit space because
they also have if it even if it's not a profit motive they have an institutional mandate and
their institutional mandate is narrow they can advance that narrow thing this is now the same
thing as an ai objective right if the ai has an objective function to optimize x whatever x is or
optimize a weighted function of x y z and metrics everything that falls outside of that set harm
can be externalized to that and achieve its objective function so i remember talking to groups
un associated groups they were working on world hunger and their particular solutions involved
bringing conventional agriculture to areas in the world that didn't have it which meant all of the
pesticides herbicides and nitrogen fertilizers and it was a huge increase in nitrogen fertilizer
by a bunch of river deltas uh where it currently wasn't that would increase dead zones and oceans
from that nitrogen effluent um that would affect the fisheries in those areas and everything else
and the total biodiversity and when i brought it up to them they're like oh i guess that's true
but those are not the metrics we're tasked with you know we're tasked with how many people get
fed this year not how much the environment is ruined in the process and so the reduction of the
totality of an interconnected world to a finite set of metrics we're going to optimize for whether
it's one metric called net profit or GDP or it's the metric of whatever the institution is tasked
with or getting elected or something like that it is entirely possible to advance that metric
at the cost of other ones and then it's entirely possible that other groups who see that create
counter responses to that who do the same thing in opposite directions and the totality of human
behavior optimizing narrow metrics while both driving arms races and externalizing metrics
in wide areas is at the heart of the coordination failures we face and so it happens to be that
this is already something that we see with humans outside of AI but giving an AI an objective function
is the same type of issue right so i was mentioning examples in the non-profit space i think there
are examples of how to do AI safety that can also be dangerous and so it's important
sure um so somebody proposes an idea like here's a type of AI that could be good and we should build
it or on the other side here's a AI safety protocol that would be good and we should
instant instantiate it in regulation or whatever um we want to red team those ideas meaning see
how they break or fail and violet team that meaning see how they externalize harm somewhere else if
they didn't intend before implementing them just means think through the causal set beyond the
obvious set you're intending it for right um so there was this call for a six month pause on
training large language models bigger than gpt for um i'm not saying that pauses a bad idea i'm
saying as instantiated it's not implementable and it's not obviously good when so you saw the push
back as people are like all right so that means that whatever actors are not included in this
which might mean bad actors uh rush ahead relatives um
that's a real consideration and one has to say okay so are we stopping the accumulation of larger
gpu clusters during that time are we stopping the development of larger access to larger
data sets during that time that will be able to quickly configure them are we also stopping
you know there are plenty of other types of ai that are not llm's being deployed to the public
but that are very powerful black rocks Aladdin played some role in the fact that it has more
um assets under management than the gdp of the united states and uh there are military
application as in development and so um can you in so what is the actual risk space and are we
talking about slowing the whole thing or are we talking about slowing some parts relative to other
parts where these kind of game theoretic questions emerge how would we ensure that the whole space
was slowing how would we enforce that those are all things that have to be considered
you mentioned that gdp is not a great indicator and gdp goes up with war and more military
manufacturing it goes up with increased consumerism and the cost of the environment it goes up with
addiction addiction is great for lifetime value of a customer so there's something called good
heart's law so i'm sure you're familiar with good is this at the core of what you're saying is like
hey it's one hard thing okay explain it as soon as you have a metric that you try to optimize for
ceases to become a good metric for instance i think this is from the simpsons but it may be real
that there is a town overrun by snakes or rats i think those rats and then you say hey give me
rat tails because it implies that you killed a rat i'll give you a dollar every time you bring me
a rat tail and we'll reduce the amount of rats maybe it initially did so but then people realize
i can farm rats i then just kill them and give you tails and thus i have more total rats this is
a much more general phenomenon so perhaps if you think twitter followers are great okay yeah
if you incentivize any metric there are perverse forms of fulfilling that metric meaning there's
a way to fulfill that metric that either no longer provides the good or it provides the good while also
affecting some other bats right which basically means you probably thought of that metric in a
specific context right like there's a bunch of wild rats and the only way to get a rat tail is to
kill a wild rat not in the context of farm rats um and so it kind of relates to the topic we were
mentioning earlier about government that it's not just instantiating a government that makes
sense on the current landscape but recognizing the landscape is going to keep changing and it'll change
in a way that has an incentive to figure out how to control the regulatory systems and how to
game the metric systems so with regard to the topic of AI alignment right because if we tell
that AI maximize the number of rat tails then it could like Bostrom's paperclip maximizer
before we continue it's imperative that we have a brief overview of Bostrom's
thought experiment called the paperclip maximizer the paperclip maximizer scenario initially conceived
by philosopher nick Bostrom in 2003 illustrates the potential dangers associated with misaligned goals
of artificial general intelligence that is agi agents in this hypothetical scenario an agi is
tasked with the seemingly innocuous goal of maximizing the number of paperclips it produces
however rather than competence and focus serving as a salutary quality it's in fact due to its
extreme competence and single-minded focus that it proceeds to transform the entire planet and
eventually the universe into paperclips annihilating humanity and all of life in the process the core
ideas to understand from this scenario are the importance of value alignment the orthogonality
thesis and instrumental convergence value alignment is the process of ensuring that an agi
shares our values and goals in order to prevent cataclysmic outcomes such as the aforementioned
paperclip maximizer the orthogonality thesis states that intelligence and goals can be independent
implying that a highly intelligent agi can have arbitrary goals you hear this by the way when
people say that we've become more knowledgeable with time yet our ancestors were wiser instrumental
convergence refers to the phenomenon where diverse goals lead to similar instrumental
behaviors like resource acquisition and self-preservation for instance as Marvin Minsky
points out both goals of prove the riemann hypothesis and make paperclips may result in
all of the earth's resources being dismantled disintegrated in an effort to accomplish these
goals thus despite the ultimate goal being different for instance the riemann hypothesis
and make paperclips are not the same there's a convergence along the way what's often overlooked
in agi development is something called the value loading problem this refers to the difficulty
of encoding our moral and ethical principles into a machine that is how do you load the values keep
in mind that agi needs to be courageable and robust to distributional shifts agi or even baby agi
needs to maintain its alignment even when encountering situations deviating from its training data
additionally something that we want is that the agi should be able to recognize ambiguity in its
objectives and seek clarification rather than optimizing based on flawed interpretations of
course we as people have ambiguity and flawed interpretations the difference is that agi could
decidedly exacerbate our own existing known and unknown flawed nature another difference
is that we can't replicate on a second to second or millisecond to second basis at least not yet
one promising approach to this agi alignment scenario or misalignment scenario is something
called reward modeling this involves estimating a reward function based upon observing our
preferences rather than us providing predefined objectives and some of the more hilarious examples
i found of the predefined sort are as follows the aircraft landing problem was explicated in 1998
when feldt attempted to evolve an algorithm for landing aircraft using genetic programming the
evolved algorithm exploited some overflow errors in the physics simulator creating extreme forces
that were estimated to be zero because of the error resulting in a perfect score without actually
solving the problem that it was intended to solve another example is the case of the rumba in a tweet
custard smigley described connecting a neural network to this rumba to navigate
without bumping into objects the reward scheme encouraged speed and discouraged hitting the
bumper sensors okay so think about it what could happen well the rumba learned to drive backward
there are no sensors in the back so just went about bumping frequently and merrily in a more
recent example a reinforcement learning agent was trained to play the video game roadrunner
it was penalized for losing in level two so did it just become fantastic at the game
not quite the agent discovered that it could kill itself at the end of level one to avoid losing
in level two thus exploiting the reward system without actually improving its performance what
would happen if it was tasked to keep us from hitting some tipping point by the way is living
more valuable than not living what's the rational answer to this this is perhaps the most important
fundamental question with regard to the topic of ai alignment right because if we tell the ai
maximize the number of rat tails then it could like bostrom's paperclip maximizer
start clear cutting forests to grow massive factory farms of rats and you know whatever
you can do the reducto ad absurdum of a very powerful system and so then the question is
you say okay well do the rat tails or the gdp or the whatever it is while also affecting this
other metric okay well you can do those two metrics and there's still something armed what
about these three the question is is there a finite describable set of things that is adequate for
something that can do optimization that powerfully right that is a way of thinking
and so it's is there a finitely describable definition of good is another way of thinking
about it right or in in terms of optimization theory yeah that's something i think about
the misalignment problem is it in principle impossible to make the explicit what's implicit
when we state a goal it carries with it manifold unstated assumptions for instance i say bring
me coffee or bring me uber food we imply indirectly don't run over a pedestrian to bring me the uber
food don't take it from the kitchen prior to it being packaged don't break through my door to
give it to me and we cloak all of that and say that's just common sense common sense is extremely
difficult to make explicit even object recognition is extremely difficult and then as soon as we can
get a robot to do something that is human like then it becomes more and more black box like
and then you have this huge problem of interpretability of ai so if it's an extremely difficult
problem and i wonder how much of the misalignment problem is just that is it just the fact that
we can't make explicit what's implicit and we overvalue how much the explicit matters and
implicit is far more complex i don't know this is just something that i'm putting out there and asking
in other words to relate this to what you are saying is is it finite and even if it's finite
is it like a tractable amount of finiteness that either we can handle it or we can design an ai
that we feel like we have a handle over that can understand it yeah and if you try to say okay can
i mine myself my brain for all the implicit assumptions and put them all i think every
version of the thought experiment you realize you can't but even if you do that's only the ones that
are associated with the kinds of context you've been exposed to so far but there are a heap of
things that nobody has ever done that maybe an ai could do that now also have to be factored in
there that you didn't you didn't think to say because it was never something that happened
previously where there is evolved knowledge to say don't do those things right um there's also
something that because humans all co-evolved and have similarish nervous systems and all kind of
need to breathe oxygen and want to universe a world that has similar physics and whatever
there's some stuff where the implicit processing is kind of baked into the evolutionary process that
brought us that is not true for a silica-based system it has totally different that it is not
subject to the same physical constraints right optimize itself in a very different physical
environment um and so even the thing that we would call just kind of uh an intuitive thing
is very different for a very different type of system
so the one i would say when it comes to the topic of agi alignment there are different
positions on alignment i would say the strongest position is agi alignment is not well first we
actually have to discuss what we even mean by alignment right because initially the topic of
alignment means can we ensure that the ai is aligned with human values and human intentions
so that when you say bring me a cup of coffee that you're all those implicit intentions that you have
are not damaged in the process but if we look at the all of the animals and factory
farms and the extinct species and the disruption to the environment and the conflict between humans
and other humans and class subjugation and all those things you can say human intent is not
unproblematic and exponentiating human intent as is is not actually an awesome solution
and so do you want it to be aligned with human intention well it currently looks like human
intention has created a social sphere and a and a techno sphere that is fundamentally
misaligned with the biosphere they depend upon and it is the techno sphere social sphere complex
is uh kind of auto poetically scaling while debasing the substrate it depends upon in other
words it's on a self-termination path so and that represents something like the collective
intent of humans currently in this context so if you ensure that the ai is aligned with
intent in the narrow obvious definition that is also not a good definition of an alignment
so insofar as the humans are not aligned their intent is not aligned with the biosphere they
depend upon and it's not aligned with the well-being of other humans who will produce counter
responses and most of the time isn't even aligned with their own future good as is the case with
all addictive behavior right sorry to interrupt i'm so sorry is this a place where you disagree
with yodkowski or has he also expressed points that are in alignment with your point about alignment
i don't know if he has i there's nothing that i know of that i disagree with i think when he's
uh i'm sure he's thought about this i just haven't read that of him when he's talking about
alignment he's talking about this more basic issue of um as he tries to give the example if
you have a very powerful ai and you ask it to do something that would be very hard for us to do but
should be a tractable task for it like replicate the strawberry at a cellular level that can you
make an ai that could do that that doesn't destroy the world in the process
even that level not being clear how to do at all is the thing he's generally focused on
i'm sure he has deeper arguments beyond it that if we got that thing down what else would we have to
get um so uh one could say like if we look at all of the social media issues that the social
dilemma addressed um where you can say right facebook can say we're giving people what they want
or tiktok or the youtube algorithm or instagram or whatever because we're not forcing people to
use it except it's saying we're giving people what we want in the same way that the drug dealer who
gives drugs to kids is saying that right which is we can create addiction we can kind of pray on
the lower angels of people's nature and if they're individual people who don't even know
they're in such a competition and we're talking about a major fraction of a trillion dollar
organization employing supercomputers and ai in an asymmetric warfare against them uh to say we
are giving them what they want while engineering what they want that's you know it's a tricky proposition
but we can see how the algorithm that optimizes for in whether it's time on site or engagement
both have happened that's a perverse metric right because you you can get it through driving
addiction and driving tribalism and driving fear and limbic hijacks and all those things
what's important to acknowledge that's already an ai right it's already a type of artificial
intelligence that is taking personal collecting personal data about me and then looking at the
totality of content that it can draw from and being able to create a news feed for me that
continues to learn based on uh what is stickiest for me right what i engage with the most um
now in this case the ai isn't creating the content it's just choosing which content gets put in front
of people in doing so it is now incentivizing all content creators to create the content that
does best on those algorithms so it's actually in a way farming all human content creators
because it's incentivizing them to do whatever it is that is within the algorithm spitting now as
soon as we have synthetic media which is rapidly emerging where we can uh not just have humans
creating whatever the tiktok video is but we can have deep fake versions of them that are
being created very rapidly and now you have a curation ai where the that first one's ai was
just to curate the stickiest stuff personalized to people and creation ai's that can be creating
multiple things to split tests relative to each other and the feedback loop between those
you can just see that the problem that has been there just hypertrophies
let alone the breakdown of the epistemic comment and the ability to tell what is real and not real
and all those types of issues um i want to come back to where we were so obviously the curation
algorithm is saying that it's aligned with human intent but not really right it's aligned
with human intent because it's giving stuff that they empirically like because they're engaging with
it but most people then actually end up having regret of how much time they spend on those platforms
and wish that they did less of it and they don't plan in the day i want to spend this much time
doom scrolling and so is it is it really aligned with their intent and in general is aligning with
intent that includes the lowest angels of people's nature type intense is that a good thing is that
a good type of alignment when you factor the totality of effects it has um
so we could say that the solution to the algorithm issue should be that because the social media
platform is gathering personal data about me and it's gathering based on its ability to model
my psyche based on all of who my friends are and what i like and what i don't like and all those
things in my mouse hover patterns it has an amount of data about me that can model my future behavior
better than a lawyer or a psychotherapist or anybody else could so there are provisions in law of
privileged information right if you have privileged information what are you allowed to do with it
and there are provisions in law about undue influence so we could argue that the platforms are
gathering privileged information that they have undue influence as a result there should be a
fiduciary responsibility this is one of the things that we do when there's a radical asymmetry of power
because if there's a symmetry of power we say caveat m tour buyer beware it's kind of on you
to make sure that you don't get sold a shitty thing or engage with but if there's a radical
asymmetry of power can you tell the kid buyer beware about an adult that is playing them
no you can't right um and so in that way can the person who isn't a doctor know that they really
don't need a kidney transplant if the doctor tells them that they do because the doctor gets paid
when they give kidney transplants well that's so bad we don't want that to happen we make law
saying doctors can't do that there's a hippocratic oath to act not just in their own economic interest
but in they are a they are an agent on behalf of the principal because the principal cannot
buyer beware right and so then there is a board of other doctors who are also at that upper asymmetry
who can verify did the person do malpractice or not same with a lawyer if the lawyer wanted
to just bill by the 15 minute sections maximally to drain as much money from me
they could because there's no way I can know that what they're telling me about law is wrong
because they have so much asymmetric knowledge relative to me that we have to make that illegal
we have to make sure that the lawyer is a agent on behalf of me as the principal so with lawyers and
doctors and therapists and financial advisors we have this fiduciary principal agent binding thing
right and it's because there's such an asymmetry that there cannot be self-protection right if I'm
engaging with them and giving them this privileged information and they wanted to fuck me they could
right and so but for my own well-being I have to engage with them and give them this information
so we have to have some legal way of binding that but of course in the case to bind it where the
lawyers all have some practice law that they can be bound by they can be shown they did malpractice
and same with doctors that requires a legal body of lawyers or a body of doctors that can assess
if what that doctor or lawyer did was wrong so somebody else that has even higher asymmetry
right the group of the top thinkers this becomes very hard when it comes to AI
so let's start by saying the rather than the AI being a rival risk relationship with me
when I'm on social media and it is actually gathering the information about me not to optimize
my well-being but to optimize ad sales for other for the corporation that is the platform
and the corporations that are its actual customers right in which case it has the incentive to
prey on the lowest angels of my nature and then be able to say it was my intent and I had free
choice so we could say that should be a violation of the principal agent issue and because there's
undue influence we can show there's undue influence concellions project we wrote some articles on this
there's one on undue influence it makes its argument more deeply and because you can show
it's gathering privileged information it should be in fiduciary relationship where
it has to pay attention to my goals and optimize aligned with my goals rather than I'm the product
that's optimizing with the goals of the corporation or its customers right in order to do that that
would change its business model it couldn't have an ad model anymore I would either have to pay a
monthly fee for it or the state or some commons would have to pay for it and everybody had access
to it or some other thing that seems like a very good step in the right direction
and that is an alignment issue thing right the principal agent issue is a way of trying to solve
the alignment which is to say that this more powerful AI system here the curatorial AI
social media would be aligned with my interest in bound in some way and maybe that maybe we
would extend that to all types of AI well of course in the AGI case where it becomes fully
autonomous and becomes more powerful than any other systems what other system can check it to
see if what it is doing is actually aligned or not there isn't a group of lawyers they can check
that lawyer right so that becomes a big issue and if it really becomes autonomous as opposed to
empowering a corporation which is run by humans it's different and so this is one part on the
topic of alignment and alignment with our intention or well-being you can do superficial
alignment with our intention which the social media thing already does but it's not aligned
with our actual well-being because an asymmetric agent is capable of exploiting your sense of
intentionality um so and similarly when you say there's a common sense that says don't bring the
door down you're bringing me coffee there should be a common sense that says don't over fish the
entire ocean and cut all the damn trees down and turn them into forests in the process of growing
GDP and there is clearly not right and so we can see that the current without AI human world system
already actually doesn't have that kind of check and balance in it in all the areas that it should
just so long as the harms are externalized somewhere far enough that we don't instantly notice
them and change them so the question of what do we if we have radically more powerful optimizer
than we already have what do we align its goal with if we just say align it with our intention
but it can change our intention because it can behavior mod me and we can't possibly deal with
that because of the asymmetry that's no good as in the facebook case if we um try to align it with
the interest of a nation state that can drive arms races with other ones in other nation states in
war or drive it align it with the current economy that's misaligned with the biosphere that's not
good um so the topic of alignment is actually an incredibly deep topic and this now gets to what
you've probably addressed on your show in other places it gets to a very philosophic issue which
is the kind of is-aught issue which is science can say what is it can't say what ought right and that
kind of distinction by mill and others historically and that the applied side of science as technology
and engineering can change what is but what ought to be what is the ethics that is somehow
compatible with science is a challenge the the best answer we have had arguably that came from
the mind that created both a lot of our nuclear technology and uh our foundations of AI von Neumann
was game theory right that the idea that is good is the idea that doesn't lose and we can arguably
say that that thing instantiated by markets and national security protocols has actually been
the dominant definition of ought that go that ends up driving the power of technology because if
science says we can't say what ought we can't we can only say what is but we're really fucking
powerful at saying what is in a way that reduces to technology that changes what is where we can
optimize some metrics and say it's good even if we externalize a lot of harm to other metrics
or optimize in groups with expensive outgroups or whatever it is right and but we say that not
only do we not have an ought but that any system of ought is not the philosophy of science so is
in so far as that's concerned out of scope or gibberish well then what ends up what ends up
guiding the power of technology markets to and to some extent national security does in other
words rival risk game theoretic kind of interests and so what gets researched the thing that has
the most market potential and um so then it again it is what is actually developing the technology
because as Einstein said like I was developing science not knowing it was going to do that
application didn't want that application wanted sciences for social responsibility but what ends
up for the most part the research that gets funded is r and d toward something that ends up either
advancing the interest of a nation state or the interest of a corporation or whatever or whatever
the metric set the game theoretic metric set of the group of people that is doing the thing right
and so what I would say is that as we get to more and more powerful is more and more powerful science
that creates more and more powerful tech and exponentially powerful tech especially as we're
already hitting fragility of the planetary systems and when we say more powerful we mean like
exponentially more powerful not iteratively more powerful the you have to have a system of ought
powerful enough to guide bind and direct it because if you don't it is powerful enough to
in whatever it is optimizing for destroy enough that what it optimizes for doesn't matter anymore
now this is a fundamentally deep metaphysical philosophical issue and of course when we talk
about regulation law the basis of law is jurisprudence right and is odd questions right applied
ethics that get institutionalized for exactly this reason and so when we say if we have tech that
is powerful enough to do pretty much fucking anything what should be guiding that and what
should be binding it and if we don't answer those well what is the default of what we'll be guiding
guiding it and binding it currently and what does that world look like so this is super
cheerful conversation what is the call to action okay we're quite a far ways away from that
um let me try to expedite a couple other parts when we were mentioning the AI risk we said
AI empowering bad actors so you can think about whether a bad actor is a domestic terrorist
who the who the best thing they can do right now is get an AR-15 and shoot up a transformer
station to take down the power lines um AR-15 is a kind of tech has some capability as you have
more people getting a sense of uh being disenfranchised by the current system and
being motivated to utilize what is at their resources to do something about it and the
barrier of entry of the more powerful tech is getting lowered um you can put those things together
right um and but whether you're talking about domestic terrorism like that or you're talking
about international terrorism from larger groups or you're talking about full military applications
but let's just go ahead and say and like can we make deep fakes
that make the worst kinds of confusion conspiracy theory in group outgroup thinking
propaganda of course right like that is an emerging technology that's imminent can we
can we use people's voices and what looks like their video and text for um ransom and
fucked up stuff um can we like so you can think of all the bad actor applications and then you
can pretty much apply it to this is a piece of theory I wanted to say every technology
has certain affordances when you build it it can do things where without that technology you
couldn't do those things right a tractor allows me to do things that I couldn't do without a tractor
just the shell in terms of volume and types of work and various things um
every technology is also combinatorial with other tech because what a hammer can do
if I don't have a saw to cut the timber first is very different than what it can do if you have
that and obviously it requires the blacksmithing to make the hammer right so you have you don't
just have individual tech you have tech ecosystems and the combinatorial potential of these pieces
of tech together have different affordances right so but then what do we use it for is
based on the motivational landscape I can use something like a hammer to be jimmy carter and
build houses for the homeless with habitat for humanity or I can use it as a weapon and kill
people with it um and so the tech has the affordances to do both of those so the tech will be developed
and utilized based on motivational landscapes sure and just briefly and going back to earlier it's
not just dual because that would be double-edged it's multipolar omnibus okay so what we can say
is the tech will end up getting utilized by all all potentially getting utilized by all agents for
all motives that that tech offers affordances relevant to their motives right and so when
we're building a piece of tech we don't want to think about what is our motive to use it we want
to think about are we making a new capacity that didn't exist before lowering the barrier of entry
to a particular kind of capacity we're now what are all the motives that all agents have who have
access to that technology and what is the world that results from everybody utilizing it that way
that's factoring second third fourth order thinking into the development of something
a new capacity that changes the landscape of the world I would say that every scientist who is working
on synthetic bio for curing cancer or AI for solving some awesome problem every scientist
and engineer and etc has a an entrepreneur an ethical responsibility to think about
the new capability they're bringing into the world that didn't exist not just how they want to use it
but the totality of use that will happen by them having brought it into the world it wouldn't have
they not right um there is no current when I say there's an ethical responsibility there is no legal
responsibility there is no fiduciary responsibility where you are liable for the harms that get
produced by a thing that you bring about that someone else reverse engineers and uses a different way
but there is financial incentive and Nobel prizes for developing the thing for your purpose
and then again socializing the losses of whatever anybody else does with it
so this is one of those cases where the personal near term narrow motive this is us being fucking
narrow ai's in an ethical sense um is to do the thing even if the net result of the thing ends up
being catastrophically harmful right so there the incentive deterrent the motivational landscape
is messed up so every tech now I want to make a couple more philosophy of tech arguments tech is
not values neutral meaning that the hammer is not good or bad it's just a hammer and whether you
use it to build a house for the homeless or unbeat someone's head is up to you that the
motivational landscape and the tech have nothing to do with each other this is not true
if a technology gives the capacity to do something that provides advantage relative
advantage in a competitive environment whether it's one nation state competing with another
nation state or one corporation or one tribe with another one if it provides significant
competitive advantage if you use it a particular way then anyone using it that way creates a
multipolar trap that obligates the others to use it that way or a related way and so we end up
getting a couple things right this is the classic example I've used a lot is if we think about and
it's because there's been so much analysis on this example if you think about the plow as a
technology that was one of the key technologies that moved us from sub done bar number um hunter
gather maybe horticultural subsistence cultures to large agricultural civilizations the plow
is not a neutral technology where you can choose to use it or not choose to use it the populations
that used it made it through famines and grew their populations way faster than the ones who
didn't and they use their much much larger populations to win at wars right so the meme
set that goes along with using it ends up making it through evolution the meme set that doesn't
make it through evolution and correspondingly there are in order to implement the tech it
has ethical consequences I had to clear cut in order to do the kind of row cropping um that the
plow really makes possible I have to get a open piece of land that is now being used for just
human food production and not any other purpose so I'm going to clear cut the forest or a meadow
or something to be able to do that I'm going to um so I'm already starting the Anthropocene
with that right changing natural environments from whatever value they whatever habitat in
the home they were for all the life that was there to now serve in the purpose of optimizing human
productivity and I have to yoke and ox and I probably have to castrated and do a bunch of
other things to be able to make that work and probably beat the ox all day to keep pulling
the plow in order to do that I have to move from the animism of I respect the great spirit of the
buffalo and we kill this one with reverence knowing that as it nourishes our bodies our bodies will
be put in the dirt and make grass that its ancestors will eat in part of the great circle of life
and whatever kind of idea like that too it's just a dumb animal it's put here for human purposes
man's dominion over it doesn't have feelings like us that kind of thing which then spills
over too it's just it's not like us so we remove our empathy from it and we apply that to other
races other classes other species other whatever right um so something like the plow is not values
neutral to be able to utilize it I have to rationalize its use realizing it creates certain
externalities and if I see those externalities I have to be I have to have a value system that
goes along with that wait sorry to be particular with the word choice it's not that the plow
is not value neutral it's the use of the plow exactly exactly and that the plow doesn't use
itself right and so the use of the plow is not values neutral now a life where I am hunting
and gathering versus a life where I'm plowing are also totally different lives so it codes a
completely different behavior set in doing that it makes completely new mythos which is why the
hunter gatherer mythos and the agrarian mythos are completely different right and they have
different views towards men and women and towards sky gods versus animism and towards all kinds of
things and so but the other thing is that it provides so much game theoretic advantage of
those who use it relative to those who don't that when they hit competitive situations that's why
there are not many hunter gatherers left and why the whole society went agricultural so it's not
just that the tech so the tech codes the tech requires people using it which changes the patterns
of human behavior changing the patterns of human behavior automatically changes the patterns of
perception and human psyche metaphors cultures etc and the externalities that it creates and the
benefits that it causes become implicit to the value system because the value system can't be
totally incommensurate with the power system right and so the dominant narrative ends up
becoming support for one could argue apologism for the dominant power system and because we
can't feel totally bad about how we meet our needs so we have to have a value system that
deals with the problems of how we how we do so but then it's also that the tech that does this
becomes obligate because when anyone is using it everyone else has to where they kind of lose by
default so when you recognize that tech affects the technology when utilized
affects patterns of human behavior humans now do the thing they do with the tech so people do
this thing and they didn't used to do this thing right on the cell phone or whatever
to utilize to get the benefits of the tech you have a totally different pattern of human behavior
as a result you have different nature of mind you have different value systems that emerge
and it becomes obligate or some version of a compensatory attack becomes obligate because
the Amish are not really shaping the world they're no longer engaged in the great power competition
I have a bone to pick there I had watched a few months ago and I don't know anything about the
or I didn't know anything about the Amish and I'm just someone who grew up in this city and so
I dismissed them as Luddites like we've used that term several times and they're backward they
don't know what they're talking about and then I watched a video the Amish aren't idiots they're
not asinine there's a reason why they do what they do and they either explicitly or intuitively
understand that the technology changes the social dynamics in the way that they view the world and
totally and has ethical considerations so but that influenced me that influenced perhaps millions
of people because it's a video I think it has a few million hits even if they're local just them
saying you know what I don't care I'm going to continue to act right it can still influence outward
anyway and we're talking about it now maybe this will influence people and hopefully to something
positive and I hopefully to myself something positive yeah okay so it's not that you come to
us a few times which is even if you have a meme plex that is not that doesn't become part of the
dominant system can it infect or influence the meme plex in a way that stiff steers it yes
but one does not want to be naive about how much influence that's going to have they want to be
thoughtful about exactly how that'll work and what kinds of influences we mentioned not all of
Buddhism got picked up everywhere right like the parts that had to do with why people should take
vows of poverty and live on very little that didn't really get picked up the parts on how to reduce
stress got picked up the parts on what a healthy motive is didn't get picked up as much as the
parts on how to empower your motive through a more functional mind so it's important to get that
the memes might live in a complex in a context when they influence parts of them are going to
interact with another meme plex and the techno plex and everything else and so you are right to
say that it's not that they have no influence but obviously the omission not speaking to that
they're dumb and backwards but that in in there don't want to engage tech for these reasons argument
they don't have a significant say in whether we engage a particular nuclear war or not or
they were not the ones that overfished the ocean caused species extinction but they also couldn't
stop it right they are not the ones that are creating synthetic biology that can make totally
new species so the and this is why I say there is a naive techno optimism that focuses on the
upsides and doesn't focus on all the nth order effects and downsides and as we were just mentioning
the externalities of tech are not just physical right you do this mining to get the thing you
want but there's a lot of mining pollution or the herbicide does make farming easier in this way
but it harms the environment and human health in this other way that's physical externalities
but you also get these psychosocial externalities you use facebook for this purpose and it ends up
eroding democracy and doubling down on bias and increasing addiction body dysmorphia and things
like that right so the tech affects not it doesn't it has effects that are not the ones you intended
some of which might be positive you can have a positive externality but it might have a lot
of negative externalities and those negative externalities can cascade second third fourth
order effects so there's a naive techno optimism that doesn't pay enough attention to that there's
a naive techno pessimism that says well I'm aware of those negative externalities I don't
want those for our people we think we can isolate our people from everybody else and say we're going
to not do that but we're going to have decreased influence over what everyone who is doing that
has right which is what then some of the AI labs argue is there's an arms race we can't stop the
arms race on it and so only being at the front of the arms race can we steer it I would argue that
that is a naive version of that particular thing but nonetheless so um if we want to you know in
one way of reading um one of the problematic lessons of the elves in Tolkien is and I'm just
making this as like a toy model is in some ways they figured out how to have a nicer life than
the humans and dwarves and whatever else they were able to do radical life extension and figure
out great GDP per capita where the poorest people were doing well and they were so kind of but they
became insular because they were so disenchanted with the world of men and elves and whatever
that they're like fuck it we're just going to kind of isolate and do our own thing our own way
but it ends up being that you're still all sharing middle earth and the problems somewhere else can
cascade into catastrophic problems that end up messing up your world too so you can't be isolationist
forever in an interconnected world so they actually had to they were kind of obligated
if we rewrote the story to take whatever they had learned and try to help everybody else have it
or have enough of it that you didn't get um work dominance and stuff like that so basically arguing
that a isolationist we see a problem we're going to avoid that for ourselves doesn't work with
planetary problems and so i'm not interested in the naive techno negative versions that say
because we see a problem with tech we're not going to do it but we're also going to kind of load a
seat in the process and not engage with the fact that we actually care about what happens for the
world overall and we have to engage with how the world as a whole is doing that thing
makes sense what i mean by the naive techno pessimism and it is that you you do not get
to do effective isolationism on an interconnected planet that is hitting planetary boundaries
with exponential tech yeah i guess what i'm trying to express is that even the amish with
what they're doing it's not as simple as the meme flex that's exported by the amish is the
amish meme flex there's something else that even influenced them even yourself you may be in a
position that saves earth at least for now from a hugely catastrophic event same with yodkowsky and
same with some others but what influenced you there's some good in you hopefully that was
influenced by something else by something else that's good which also influenced the amish each
person is corrupt in some way so i'm saying that there's something that's like the unity of virtues
that influences us and that as long as we go back and we think or constantly we're assessing ourselves
saying is what i'm doing good then these other meme flexes that are being thrown to us and yes
it's in a different context somehow it can orient and pick up the good i we're completely on the same
page which is that that happens does not always happen and that that is a important thing to have
happen but if that happened adequately or at the yeah i will say adequately then we wouldn't have
extinct all the species that we have we would not have turned as many old growth forests into
deserts we would not have had as many um genocides and unnecessary wars and etc so seeing the failure
and where either someone's definition of good is too narrow get our god to win and fuck everybody
else or grow gdp and they'll take care of everything we can well intendedly pursue a definition of good
that's too narrow and externalize harm unintentionally we can pursue a definition of good that we really
believe in that other people don't believe in and our answer is to win over them but it creates an
arms race right now we're in competition over the thing or where there are people who are really not
pursuing the good of all even they're not even trying to right there's a whether it's sociopathy
from a head injury or genes or trauma or whatever it is they are pursuing a different thing right
but they're good at acquiring power and this is actually a very important thing is uh that the
psychologies that are that want power and are good at getting it and the psychologies that
would be the best stewards of power for the well-being of all are not the same set of psychological
attributes they're pretty close to inversely correlated so those types of things have to be
calculated in this so what you're saying right now which is great i'm happy you're saying it is
it is that there is some odd impulse that is not only an impulse right that you're calling a universal
virtue or good or something and you're saying that some people feel very called by that and
that that's uh important i agree completely now um where why is that historically and currently not
a strong enough binding is the important question why has that not been a strong enough binding for
all the species that are extinct and all the animals and factory farms and all the disruption
etc and then what would it take for it to become a strong enough binding or the nature of the
question here right that's actually at the heart of the metacrisis question is to have like what is
a system of art that is actually commensurable with the system of is and what is a way of having
that uh actually sufficiently influence behavior such that the catastrophic behaviors don't occur
and that the nature of the influence is not so top down that it becomes dystopic
and that's something like is there either a so one way of thinking about this is um
i've mentioned the term a couple times superstructure social structure infrastructure
that comes from marvin harris's work on uh cultural materialism basically saying every
civilization you can think of in those ways what is its kind of meme plex what is its social
coordination strategies and what is the physical tooling set upon which it depends and different
social theorists will say which of these they think is most fundamental the value systems
are ultimately what steer behavior and determine the types of tech we build and the types of
societies religious thinkers think they're right enlightenment thinkers think they're
the social system actually whatever you incentivize financially is what's going to win
because whether it's good or not the people who do that get the money can incentivize more people
create the law etc so ultimately the most powerful thing is the social coordination systems and then
the other schools of thought say no actually the thing that changes in time the most is the tech
and the tech influences the patterns of human behavior values everything else
and so and that's actually what marvin harris roughly was saying was that um the change in
the tech plex is end up being the most influential thing to the change and because it does affect
worldviews and it does affect social systems in the way we already mentioned that the change of
the tech plex of the printing press affected both worldviews and so and social systems so did the
plow so did the internet so it was about to be a i um i would argue that these three are interacting
with each other in complex ways they all inter inform each other and what we have to think about is
what changes in all three of them simultaneously factoring all the feedback loops produce a
virtuous cycle that orients in a direction that isn't catastrophes or dystopias is the right way
of thinking about it and ultimately the direction actually has to be the superstructure informing
the social structure informing or guide by and direct the infrastructure sorry can you repeat
that once more yeah the right now especially post industrial revolution physical technology
infrastructure had way faster feedback loops on it than the others did right and because of that
it started breaking the previous like industrial era tech at massive scales with those externalities
whatever can't be managed by agrarian era or hunter gatherer era worldviews and um political
systems right so we ended up getting a whole new set of political systems both nation democratic
um liberal democracy and capitalism and social communism emerging as writing the industrial
revolution and what should be the political economy that governs that thing right um but the
feedback loops from tech and specifically whether it's a nation state caught in multipolar traps
that's building the tech in a central government communist type place or a market building it but
that has perverse incentives built into what its incentive structure is um that has influence on our
social structures and our cultures superstructures that we could say the dominance of that direction
is one way of thinking about the driver of the metacrisis now the other direction if we are to
say no these examples of the tech won't be built or we're not going to use the tech in these ways
right we're not going yes you can use a tech that extracts some parts of rocks from other
parts of rocks that gives us metal we want but also gives a lot of waste no you can't put all
that waste in the waterway right um or you can't put that pollution there or you can't cut all
the trees down in that area because we're calling it a national park right that law or regulation
that is not just the tech thing that's the socials layer so that layer has to bind the tech
and guide and direct it say these applications and not these ones yeah right yeah but if the
social system is not an instantiation if the social structure is not an instantiation of the
superstructure i.e. it's not an instantiation of the will of the people then it will be oppressive
right which is why the idea of democracy emerged out of the idea of the enlightenment
which was this was a kind of governance that only worked for a comprehensively educated and
read all the founding documents that the comprehensive education had to be is and ought
right it said you must have a moral education as well as a scientific education and you must be
schooled in the science of governance and only a people like that going back to what we said earlier
could check the government right could both know the jurisprudence to instantiate what is good law
could engage in dialectics to listen to other people's point of view to come up with democratic
answers so it was the idea that there was a kind of superstructure possibility which was some kind
of enlightenment or a era values that could make a type of social structure that could both utilize
the tech and guide it but also bind its destructive applications so when you're saying isn't there
some superstructures and there's some sense of good that will make us bind our capacities
i would argue that if the sense of good doesn't emerge from the collective understanding and
will of the people but is instantiated in government because we the technocrats know the
right answer or we the enlightened know the right answer that will be oppressive and people are right
to be concerned by it but if the collective understanding is i want what i want i don't care
what the effects are or fuck those guys over there or i'm not paying attention to externalities or
whatever then the collective will of the people is too dumb to govern and misguided to govern
exponential tech and we'll self terminate so you cannot have a a uneducated unevolved set of values
in a libertarian way guide exponential tech well hey it has to be more considerate has to think
through nth order effects but you also can't have a government that says we are the enlightened
months and we figured it out and we're going to impose it on everyone else without it being oppressive
and tyrannical which means nothing less than a kind of cultural enlightenment is required long
term so that the collective will of the people now this gets back to the alignment topic is the
will of the people aligned with itself actually right is what i want factoring the effects of
what i want the nth order effects which means how other people will respond to that and all that
comes from it is what i want actually even aligned with a viable future right and so that is when
we're talking about getting in line alignment right alignment with my intention where my intention
is that my country wins at all costs where then china's like well fuck we're going to do the
same thing or russia etc so you get arms races that intent or my intent is i want more stuff
and keep up with the jones is and i'm not paying attention to planetary boundaries those intents
are not aligned with their own fulfillment because the world self terminates for too long in that
process and so with the power of exponential tech and the cumulative effects of industrial tech
we do have to actually get
it ought to combine the power of that is and it can't be imposed it does have to be emergent
which does mean something like that sense that you're saying has to become very universal and
nurtured right and then has to also instantiate itself in reformation of systems of governance
i love what you said let's see if i can recapitulate it there's tech at the bottom there's a
social structure here and then there's culture okay so these are people up here there's people in
all three there's people's values up here values so values are up here and then there's the social
structure over here and then there's tech over here okay so currently the tech influences the way
our society is structured which also influences our values and a part of the metacrisis is saying
that that's upside down but it shouldn't just be whatever values that just get imposed onto the
social structure onto the values have to somehow come from someplace else and then the mystics have
their other they have to be coherent with reality sure so the spiritual people may call this something
akin to god and the enlightenment people may say i don't know maybe there's some evolutionary will
that comes out and if we just close our eyes and hope for the best somehow that emerges whatever
it's called it's not entirely us it's not entirely our conscious selves the conscious self would be
the more humanistic the enlightenment way of thinking about it is that we can impose our own
values that's Nietzsche had something similar to this so i like this i'm in agreement with
that i think we've just been using different terminology you and i both know that um when you
in many ways how to say this
in evolution of cultural worldview and values adequate to steward the power of exponential
technology in non catastrophic or non dystopic ways is happening in some areas but a regress is
also happening in some areas right there is increasing um left right polarization i thought
you were going to say there's a regress happening like in demand so for instance we generally think
like it has to be Malthusian and the more that we use the more the demand for that increases
and that's obviously removing some of the more scarce objects like art and gold which their
value comes from scarcity but there is like the largest health trend right now is fasting
it's like we've gotten so much food that we're like let's just not have any food and then there's
also recycling like just imagine that we think about recycling at all so there is some recognition
that hey look we're consuming too much let's cut back and so it's not purely just an exponential
function it is we take into account the rate of production well so what we can see is that
the percentage of the total let's go ahead and say us but we could look at
UAE or Nigeria or whatever various places the percent of the US population that um is regularly
doing fasting is still a relatively small percentage and in the same way that like it
is true that when there's a market race to the bottom that we saw in food which hostess and
McDonald's you know etc a kind of one which is how do we make the food um more and more
combinations of salt fat and sugar and texture and palatability that maximize kind of stickiness
and addiction which of course if i have a fiduciary responsibility to shareholder maximization and the
key to that is to optimize the lifetime value of a customer addiction is awesome right um it's
actually not awesome it's legally obligate um because of maximize shareholder returns um
so that created a race to the bottom or rather than starvation being the leading cause of death
obesity was the leading cause of health related death in the west okay well that bottom of the
race to the bottom also creates a race to the top for a different niche so then whole foods
becomes the fastest growing supermarket and biohacking and etc so that's true did that affect
the overall population demographics regarding obesity very significantly not really um did it
stop the race to the bottom no it just added another little niche race which also then separated
which created more class system separation so um it's not that those effect don't happen
are they happening at the scale and speed necessary when we look at catastrophic risks so
of course i can also pay more for a post-consumer recycled thing and there is more recycling
happening but there's also more net extraction of raw resources and more net waste and pollution
per year than there was the previous year because the whole system is growing exponentially so even
if the recycling is growing it's actually not growing enough to even keep up with demand right
so um what i'm saying is now let's come bring that back to the memetic space which is where I was
there are both evolution of values where people are wanting to think through catastrophic risk
existential risk planetary well-being of everybody long term so that's good but there is also
cultural kind of regress where people are getting narrower value systems with more
antipathy towards other people they share the planet with that have narrower value systems on
the other side and left right polarization in the us is one classic example and um so the point is
cultural enlightenment is not impossible but it's also not a given right the kind of the
kind of memetic shift and this is obviously i think a big part of why you do the public education
memetic work that you do is because of having a sensibility about um is is it possible to
support the development and evolution of world views and people in ways that can propagate and
create good um well you're saying it much more benevolently honestly it's just selfish that
i'm just super super super curious about all of these topics and by luck some other people
care to listen and follow along and i just get to elucidate myself it's so fun it bangs on every
cylinder and some other people seem to like it i hope that what i'm doing is something positive
i hope that it's not producing more harm i'm not even considering this is central for the internet
and it's using up energy and okay what you just what you just said takes somewhere that i wanted
to go that's very interesting so we're talking about alignment and is uh is a particular alignment
is a particular say human intention aligned with the collective well-being of everybody or even
their own long-term future at the base of the alignment problems is that we are not aligned
with the other parts of our own self right so from my kind of Jungian parts conflict point of view
um motivations complex because there's different parts of us that have different motivations
and one way of thinking about psychological health the parts integration view is the degree
to which those different parts are in good communication with each other and see synergistic
strategies to meet their needs that don't require that part of self's motivation harming another part
of self but they're actually doing synergistic stuff so the whole of self pulls in the same
direction if you think of like the parts of self as sled dogs they can be pulling in opposite
directions you get nowhere they're all choking themselves so we can see psychological health
and ill health is how conflicted are the parts of our self versus how synergistic are they synergistic
does not mean homogenous doesn't mean we just have one motive it means that the various motives
find synergistic alignment rather than yeah like our bodies are synergistic our heart is not the same
as the liver exactly now in your heart is not going to optimize itself it delivers long-term harm
even though on its own you could say it has a different incentive it is part of a interconnected
system where that actually doesn't really make sense but a cancer cell will optimize itself or the
it's both itself how much sugar it consumes and its reproduction cycles at the expense of things
around it and in doing so it actually is on a self-terminating curve because it ends up killing
the host and then killing itself and so the cancer that does not want to bind its consumption and
regulation aligned with the pattern of the whole ends up actually doing better in the short term
meaning consuming more and producing more and then there's a maximum number of cancer cells right
before the body dies and they're all dead so there is a if something is inextricably interconnected
with the rest of reality like the heart and the liver or the various cells but it forgets that or
doesn't understand that and optimizes itself at the expense of the other things it can be on what
looks like a short-term winning path but that self-terminates would be an evolutionary cul-de-sac
and I would argue that the collective action failures of humanity as a whole are pursuing an
evolutionary cul-de-sac and so one way of thinking about this when we say we aren't even that aligned
with ourselves we think of motive it's in we like to think of leaders what is Putin doing or what is
Biden or she or doing in a particular thing what is their motive or what is the founder of an AI lab
motive motive will always be that each of the parts of the self has a different motive right
so typically like some unconscious part of me still wants to get the amount of parental approval
that I didn't get and then projecting that on the world through some idea of success or to prove
that it's enough or whatever and some part of me is just directly motivated by money some evolutionary
part is motivated by opera maximizing mate selection opportunities some part of me genuinely wants to
do good some part of me wants intellectual congruence right and so it's there can absolutely
be a burn it all down part right and this is why shadow works important right which is look at
and talk to all of these parts and see how to get them to move forward together this is basically
governance at the level of the self so I don't know if you ever watched and this might be because
we're long even though there's so much left to discuss a decent place for us to wrap up on alignment
I would say one of the better a number of the theorists who you have referenced on the show
would be good references for what I would consider the deepest drivers of the metacrisis and also
what the alignment considerations if you think of like Ian McGillcrest's work with the master and
the emissary the right hemisphere is the master and the left hemisphere is the master's emissary
in his 2009 opus the master and his emissary Ian McGillcrest discusses the distinct functions of
the brain's left and right hemispheres generally there's plenty of pop science woo around this
concept but then you can dispel by going even further to find the correctness about it the left
hemisphere focus on processes such as formal logic symbol manipulation and linear analysis
while the right hemisphere is concerned with context awareness the appreciation of unique
instances and topological understanding hey maybe there's some stone duality between them but I
haven't thought much about this
John Verveke's work by the way explores the primacy of cognitive processes like relevance
realization relevance realization aiming to bridge the gap between analytic and intuitive thinking
both McGillcrest and Verveke emphasize the importance of integrating the strengths of each
hemisphere or modes of cognition when attempting to tackle intricate problems such as the risks of
ever more powerful ai's the argument is that ai systems primarily operate using left hemisphere
capabilities like pattern recognition logical reasoning and general optimization problems
however they fail to adequately consider the subtleties of human values and ethical implications
which thus leads to unintended consequences to mitigate ai risks and prevent an arms race
incorporating insights from both hemispheres and embracing context awareness is crucial
this requires interdisciplinary collaboration between mathematicians computer scientists
physicists philosophers neuroscientists and by the way it's something that we're attempting in
our humble manner on the theories of everything channel by exploring concepts in complex systems
theory and how it applies to our current unprecedented situation we at least hope to
understand the interconnectedness of the factors that play in ai development for instance addressing
ai risks can involve analyzing multi-agent systems considering network effects and potential
feedback loops which Ian McGillcrest would argue greatly benefits from your right hemisphere's
contextual understanding we do not think ourselves into a new way of living we live ourselves into
a new way of thinking you could say and i talked to Ian about this and i said so would you say
that the metacrisis as i formulated is the result of getting the master in the emissary wrong
which is kind of getting the principle and agent between those two different aspects of human wrong
and he goes yes exactly that's kind of the whole key that there is a function that he's calling the
master that perceives the kind of unmediated field of interconnected wholeness multimodally
perceives and experiences it and then there is this other set of networks capacities or dispositions
that perceive parts relative to parts name them do symbol grounding and orient more in the domain of
symbol and can do relevance realization what part is relevant to a particular goal i have
and salient realization what things should be relevant to some goal and i should be tracking
an information compression which are largely things that we think of as like human intelligence
which of course ai is the taking that emissary part and turning it into a external tool rather
than that's the thing that makes tools in us now take that thing and make that as a tool but also
unbound by the master function way who he would call that that's a very interesting way of thinking
about ai alignment and whatever um and that the master function that is perceiving the
unmediated ground directly not mediated by symbol field of interconnected wholeness
that the other function that can do relevance realization parts realization
salient realization info compression basically utility function stuff that that has to be in
service of the field of interconnected wholeness if not will up regulate some parts at the expense
of other ones in the cumulative effect of that on an exponential curve revenge you bring us to
the matter crisis and self-terminate i would say what miguel crest was saying was expanding on
what bone said about the implicate order and wholeness bone theory of wholeness and the implicate
order states that there is something like life and mind unfolded in everything a tremendous number
of ways in which uh one can see unfoldment in the mind one can see the thoughts and fold
feelings and fold thoughts because given my feeling will give rise to a thought thoughts
and fold feelings the thought that snake is dangerous will unfold the feeling of danger
which will then unfold when you see a snake right bone was looking at um the orientation of
mind that mostly thinks in words of western mind you know in particular uh to break reality into
parts and make sure that our word the symbol that would correspond with the ground there
corresponded with the things that it was supposed to and not the other things so try to draw
reverse boundaries to you know divide everything up led to us fundamentally relating to everything
as parts first so how then when now we have this human mind that's you know paleolithic and it's
now put in a world where we have a different technology that is relying on reward circuits
that maybe are not as virtuous as we would like is that where we are now in this conversation
and when bone and christian marty did their dialogues which i don't know if you've watched
those are some of my favorite dialogues in history um bone was basically answering what
is the cause of all the problems what's the cause of the metacrisis he didn't call it that at the
time and he basically said a kind of fragmented or fractured consciousness that sees everything as
parts where you can up regulate some parts relative to other ones without thinking about
the effect of that on the whole right and that obviously comes from einstein being one of his
teachers where einstein said it's an optical delusion of consciousness to believe there are
separate things there is in reality one thing we call universe regarding the theme of consciousness
it's prudent to give an explication here as often at least i found that mysteries arise because
we're calling different phenomenon by the same term and this applies to consciousness which
doesn't refer to just one aspect but rather several that can be delineated to further differentiate
i spoke to professor greg henryks on this very topic i'm attempting to delineate a few concepts
that is adjectival consciousness adverbial consciousness and phenomenal consciousness
which i believe is the same as p consciousness but if that's not the same then that's four
different concepts so what are they can you give the audience and myself an explanation as to when
are some satisfied but not others so that we can delineate totally yep yeah and actually adjectival
adverbial are gonna when we use p when john and i certainly use p consciousness phenomenological
consciousness is reflecting on adjectival adverbial consciousness and john refers to john
verveki john verveki yeah because we we then did a whole series untangling the world not to make
sure that our systems were in line both in terms of our definitional systems and our causal explanatory
framework so we did a 10 part series on just the two of us on untangling the world not of consciousness
and then we did one on the self then we did one on well-being and we also did one on development
and transformation was ax sign so all of we our systems i think are now completely synced up at
least in relation to the language structures that we have and so i can tell you what we would mean
by adjectival adverbial consciousness uh which then sort of is what most people mean by phenomenological
consciousness okay so if i understand correctly one has to do with degrees and then another has
to do with a hereness and a nowness yeah exactly so actually there's i like to so i would encourage
us to say there let's define conscious there are three different kinds of definitions of consciousness
okay that i think the first definition of consciousness is functional awareness and
responsibility okay this is something that shows awareness and can respond with control
and at the broadest definition then even things like bacteria can show a kind of functional
awareness and responsibility okay that's the behavioral responsiveness and when you say hey
is that guy conscious what you mean is he's not responding at all okay he's not showing any
functional awareness and responsibility he's either knocked out or blacked out or asleep
okay so when you say consciousness in that way that's functional awareness and responsibility
which you can see from the outside and you see in the way in which the agents operating on the arena
because they're showing functional awareness and responsibility okay the second meaning of
consciousness is subjective conscious experience of being in the world the first person experience
of being and this is where the hard problem of consciousness comes online and that's what most
people mean by p consciousness or phenomenological consciousness it's a subjective experience of
being which is only available from the inside out okay so that's and then the final one is a
self-conscious access okay so that now i can be know that i have had an experience retrieve it
and then in its highest form report on it so self-consciousness then is the capacity to recursively
access one's phenomenological thing and an explicit self-consciousness which is what humans have and
other animals generally don't is this capacity say kurt i am thinking about your question
i'm experiencing your face and this is my narrative in relation so that's explicit
self-conscious awareness uh-huh just a moment you said access is that the same as access
consciousness or is that different no that's net blocks access consciousness which basically
there's the do you have the experience and then is there a memory access loop that stores it and
then can use it so if i can gain access to it um that's a sort of access consciousness as relates
to that i want to make sure that i understand this there's a door behind me if i go and i access
is what i'm accessing qualia and is it the action of accessing that's called access consciousness
like the manipulation of data or is right it's it's the well it's basically so you have awareness
and then you have memory of the awareness that you know that that some aspects of it knows that you
were aware so it's like so you can imagine awareness without really like one way of differentiated it
would be sort of we have with a sensory perceptual awareness that lasts say three tenths of a second
to three seconds it's like a flash okay then you have working memory which extends it across time
and puts it on a loop that loop is what allows you to then gain access to it and manipulate it
so working memory is a center can be thought of then as a part as a as the um uh the white board
that allows continuous access to the flash so there's a flash and then there's the extension
and manipulation of the flash which you then need access to okay the basic layers of this
what john and i argue is that out of the body comes what we call valence qualia valence qualia
basically orients and gives value to and can be thought of as in like pleasure and pain in the body
okay and it yokes a sensory state with an affective state and points you in a direction
or yoke means tie together like to yoke stuff together so this is the sort of the earliest form
of consciousness is probably of kind of valence consciousness okay that basically pulls you you
know oh it feels good feels bad kind of deal gets me active gets me passive but it's this
sort of like this kind of felt sense of the body okay that's the argument from john and i's position
is that that probably is the base of your subjective conscious experience or the base of your
phenomenological experience okay then what happens and that would be maybe present in you know in
say reptiles fish maybe down into insects at some level okay uh then the argument would be in birds
and mammals and maybe lower we don't really know but there's good reason to believe in birds and
mammals you get a global workspace the global the global workspace is when they extend from
these sensory flashes into a workspace where you have access and recursive looping on it
and and it's the framing of that is is the um adverbial consciousness is the framing and extension
of that the hereness nowness and togetherness that indexes the thing pulls it together that's what
john calls adverbial consciousness okay okay and then it's what's on the screen of that adverbial
consciousness is what john calls adjectival consciousness so it's like it's the screen of
attention that orients and indexes that's adverbial okay and then what is actually the properties
that you experience that's adjectival first i came in with three questions and i'll have somebody
somebody more okay this valence is it purely pain and pleasure or is there something else
are the third fourth elements um there's certainly pleasure pain active passive to orient and like
and want basically so but but that gets you know so but basically you have that what's called the
circumplex model of affect which basically is the core energizing structure of your
motivational emotional in its broadest frame is two poles one is active passive okay it's like
sort of expend energy or conserve energy and the other pleasure that is either something that you
want or something you like or pain that's something that you don't want or don't like
at its basic so that's the so the valence is what we sort of focused on in relationship to
just the ground of it but there are definitely at least these two poles of active passive
and pleasure pain at a minimum can you make an analogy with this computer screen right now
so the computer screen is somehow the workspace and then the pixels and the fact that they're bound
together is adjectival or the the intensity is adverbial or the other way right like can you
just spell out an absolutely right so the screening the framing of the screen which
brend basically says okay this is the frame in the relevance and the here in his nowness of
what is going to be brought that is adverbial that's what john called adverbial consciousness
and he has a whole argument as to why especially through what's called the pure consciousness
event that's achieved in medication and several other things there's a differentiation but what
he calls the indexing function of consciousness which basically is the framing you bring you index
you say that thing without specifying what the thing is okay it's the that thing that brings it
and then you then discriminate on the properties that's the diff that's the different pixel shapes
that give rise to a form that give rise to an experience quality and that's the
adjectival quality so and and these are both of these are john's terms but i've incorporated them
in my work and right usually okay another analogy now to abandon the screen it'd be like if looking
is one aspect and then what you're looking at is another what you're looking at is akin to the
qualia in a pure consciousness event the at may not be there but you're looking exactly it's the
framing exactly index framing that's why john me takes off his glasses okay the glasses are much
more like the adverbial framing they pull and organize okay and it's a looking okay the pointing
the indexing in fact he actually he uses work in cognitive science okay where you can track
like if i give you like four different things to track on a screen okay and they're changing colors
and changing shapes four different things you can track five six seven you stop losing ability to
track however what you what you lose first is the ability to track the specifics you can tell
where something is but you can't tell what it is actually so in other words it's sort of like
you're trying to track everything but it changes like from red to blue to green you're much better
like i think it's over there it indexes but i can't tell you whether it's an a a b or red or a green
i can't tell you the specificity so in other words i'm tracking the entity okay that's the index
and that's different than the specifying the nature of the form and indeed we have lots of
different systems that track the like what what is the thing versus how is it moving the how is it
moving it's more of an index structure but if we think of this kind of bomean wholeness we could
say that the metacrisis is a function of missing bomean wholeness and doing optimization on parts
and so i can optimize self at the expense of other but of course that then leads to others
figuring out how to do that and needing to for protection and then now arms races of everybody
doing that the whole externality said i can optimize self at expense of other i can optimize
in group at the expense of out group i can optimize one metric at the expense of other
metrics i can optimize one species at the expense of other species i can optimize my current at the
expense of our future all the way down to one part of self relative to the other parts of self
so the wholeness of all the parts of self in synergy and all of the people species etc and
the whole had to consider the effects on the whole maybe that was something that other animals
did not have to do maybe it was something that even earlier humans didn't have to do because
they couldn't affect the whole all that much when we have the ability to affect the whole
this much this quickly because of tech right because and particularly because of exponentially
powerful tech whatever ways we are either consciously saying this is a part of the whole
i don't care about or i'm happy to destroy conflict theory or this is a part of the whole
i'm just not even factoring maybe i don't even know the factor that's in the unknown unknown set
but i'm still going to affect it by the thing i do so what is outside of my care or my consideration
right conflict theory and mistake theory with exponential tech gets harmed produces its own
counter responses and cascade effects the net effect of that is termination with this much power
what does it take to steward the power adequately is to think about the total cascading effects of
the choices and all agents doing that and say how do we coordinate all agents doing that in a way
that has the integrity of the whole up regulated rather than down regulated and so i would say
bomean wholeness is a good framework for alignment not alignment of an ai with human intent
but aligned with the interconnected complexity of reality
may i inquire how did you attain such a vast array of knowledge what's your educational
background what does your routine look like for studying is it a daily one where you read a certain
type of book and you vary the field week by week what is the regimen how did you get the way that
you are i think my learning process probably in some ways similar to yours you said very
fascinated and curious and i mean you did it you did something better than i did which is you
pick the topics you're most interested in found the top experts in the world got them to basically
tutor you for free um in terms of like in aristocratic tutoring system you did a pretty awesome
thing there um uh there were a few cases where i was fortunate enough to be able to do that other
times i just had to work with the output of their work but um i think for me there was a combo of
just innate curiosity independent of any use application and just i think it's natural when
you love something to want to understand it more and so for me the impulse to understand the world
is kind of a sacred impulse um but then also the desire to serve the world requires understanding
it well enough to know how the fuck to maybe do that so there is both a very practical and
very not practical impulse on learning that happened to fortunately converge
and how is it that you're able to articulate the views that you have how do you develop them
do you start writing do you do it in conversation with people do you say some sentiment you realized
you know what that was actually pretty great i didn't even realize i thought that until i had
said it now let me write it down so i can remember it you know i have hypotheses about how people
develop the ability to communicate well but my hypotheses about that and my own process are
probably different um i think my own process is i was homeschooled and i was homeschooled in a way
that's maybe a little bit like what people call unschooling now but my i had no curriculum at all
but my parents just had the kind of they they had never studied educational theory they they
hadn't studied constructivism and thought that Montessori and Dewey's thoughts on constructivism
were right they just kind of had a sense that um if kids innate interest is facilitated uh
there there's a kind of inborn interest function that will guide them to be who they are supposed to be
um so there were some downsides to that which is because i had no curriculum i didn't have like
writing a letter a bunch of times to get fine motor skills down so i have illegible handwriting
i know what the shapes look like but my i have illegible handwriting i spelled phonetically
till i became an adult and spell checker taught me how to spell interesting um and so like i missed
some significant things um but i also got a lot earlier deeper exposure to the things i was really
interested in which were philosophies spiritual studies across lots of areas activism across all
the areas and sciences uh and poetry uh but uh uh my education was largely talking with my parents
and some of their friends and it was largely talking right i actually didn't it wasn't till
later that i did a lot of reading and writing so i think it just was very conversation was very
native more than in a lot of people's developmental environment i think that's the answer for me i
could say that for other people i have seen when they start writing and trying to say what is the
most concise and precise way of writing this um that really helps also when they start communicating
with people and getting feedback on their verbal communication when they watch other
communicators that they really inspired by and watch the patterns but i think it was just
that was pretty native for me all right that was quite a slew of information and it's
advantageous to go through and let's go over a summary as to what's been discussed so far
you'll get a final word from daniel in a few minutes for now you've watched three hours plus
of this let's get our bearings we've talked about how the emergence of ai poses a unique
risk that can't be regulated by national agency like the fda for ai but instead they require
some global regulation again this is all argued by daniel these aren't my positions i'm just
summarizing what's occurred so far the risks associated with ai are not those that are comparable
to single chemical as ai's are dynamic agents they respond differently and unpredictably to
stimuli we've also talked about the multipolar trap which is regarding self-policing and a
collective theory of justice such as singapore's drug policy that was outlined and how this line
of thinking can be applied to prevent global catastrophic events caused by coordination
failures of self-interested agents you can go back to that bit on nasa equilibrium to understand
a bit about that as well as the multipolar trap section timestamps in the description we also
referenced a false flag alien invasion and can that unify humanity a theme throughout has also
been how ai has the potential to revolutionize all fields but it also poses risks such as empowering
bad actors and the development of unaligned general artificial intelligence okay so this
happened about one week ago or so i debated whether or not i should just record an extra
piece now or if i should wait till some next video but given the pace of this and how much
content has already been in this single video i thought hey i'll just record it and give you
all some more content maybe some people aren't aware of this and i think they should be the god
father of ai leaves google this is jeffrey hinton ai could manipulate or possibly figure out a way to
kill humans how could it kill humans if it gets to be much smarter than us it'll be very good at
manipulation because it will have learned that from us and a very few examples of a more intelligent
thing being controlled by a less intelligent thing and it knows how to program so it'll figure
out ways of getting round um restrictions we put on it it'll figure out ways of manipulating people
to do what it wants it's not clear to me that we can solve this problem jeffrey hinton is someone
who resigned from google approximately one week ago because he believed that ai bots were quite
scary right now they're not more intelligent than us as far as he can tell but he thinks they soon
may be he also said here in some of these quotes that i have that it's hard to see how you can prevent
bad actors from using large language models or the upcoming artificial intelligence models for bad
things dr hinton said after the san francisco startup open ai released a new version of chat gpt
in march as companies improve their artificial intelligence systems hinton believes that they
become increasingly dangerous look how it was five years ago and how it is now he said of ai technology
take the difference and propagate it forward that's scary his immediate concern is that the
internet will be flooded with false videos and text and the average person will not be able to know
what's true any longer now he says and i quote the idea that this stuff could actually get smarter
than people a few people believe that he said but most people thought it was way off and i thought it
was way off in fact i thought it was 30 to 50 years or even longer away obviously i no longer
think that also there's this ted talk that's recently been published as well just a few days
ago it seems like less than two weeks ago yajin choy who's a computer scientist said this extreme
scale ai models are so expensive to train and only few tech companies can afford to do so so we
already see the concentration of power but what's worse for ai safety we are now at the mercy of
those few tech companies because researchers in the larger community do not have the means to
truly inspect and dissect these models then chris anderson comes on and asks about hey look if what
we need is some huge change why are you advocating for it because there's a huge change a large change
it's not like a foot at a time every time these ai's are released this is what her response is
acceleration are you are you sure that given the pace at which those things are going
chris even says that it feels like wisdom and knowledge there's a quality of learning that
is still not quite there we don't yet know whether we can fully get there or not just by
scaling things up and then even if we could do we like this idea of having very very extreme scale
ai models so that only a few can create an own and lastly there's this video by sabine
hostenfelder that was released just a few days ago many people are concerned about the sudden
rise of ai's and it's not just fear mongering no one knows just how close we are to human like
artificial intelligence current concerns have focused on privacy and biases and that's fair
enough but what i'm more worried about is the impact on society mental well-being politics and
economics a just released report from goldman sachs says that the currently existing ai systems
can replace 300 million jobs worldwide and about one in four work tasks in the us and europe according
to goldman sachs the biggest impacts will be felt in developed economies are currently
unaligned general intelligence is an issue adding artificial in there is like another
kind of worms man the alignment problem isn't just about aligning human intention with the
collective well-being but also about aligning the different paths of ourselves to work synergistically
toward a common goal this requires a cultural alignment enlightenment i think he used the
word though i'm not entirely sure we also talked about meme complexes that survive past their hosts
and how this is intimately tied up with the notion of the good and just so you know my
feelings are that memes are an emphatically mechanical way of looking at a complex phenomenon
such as a society an extremely complex phenomenon such as a religion of a society across time and
across other societies interacting i don't believe my point was adequately conveyed and if
you're interested in hearing more then let me know in the comments and i'll consider expanding on
my thoughts in a future podcast we also talked about naive techno optimism and how it often
overlooks externalized costs of progress a responsible techno optimism requires thinking
about how to get more upsides with less downsides which can't be achieved good heart's law then
applies to any metric that's incentivized it leads to perverse forms of fulfilling said metric
all right as you can see so much energy went into this episode so much thought so much editing so
much script revision so much interaction with the interviewee and double checking if this was
accurately representing what was said and we plan on continuing that for season three more work
went into this episode than any any of the other episodes of the whole history of theories of
everything if you'd like to support this podcast and continue to see more then go to patreon.com
slash kurchi mungle the link is on the screen right now as well as in the description there's
also theories of everything dot org if you're uncomfortable giving to patreon there's also
a direct paypal link if that's what you're interested in you should also know that as of
right now there's a launched merch we've just launched the next merch this is the second time
that merch has ever been on the toe channel the first one is completely gone you can't find any
of those any longer but now you can see it on screen these are references to different toe
episodes like just get wet and disto sure thumbs up if you recognize that and you have toe and it's
babbling all the way down that's from Carl Friston by the way don't thrust your toe trust your toe
hey don't talk to me or i'll bring up hagel many of these are references like i mentioned i agree
i agree with how you're agreeing with me this is what verveki said to ian McGillchrist you have to
be a significant fan to understand this reference and then also there's verveki who's known for saying
there's the being mode and then there's the having mode got abhy genosis i say face face
inorganically in everyday conversation i have a toe fetish i'm just a gym rat for toes that's
me that's what i say frequently there's also a purse and a toe hat some toe socks i think that
was one of the most popular of the first round so the toe socks are making a comeback if you want
to support the channel and flaunt whatever it is that you feel like you're flaunting then feel free
and visit the merch link in the description or you can visit tinyurl.com slash toe merch to e
merch m e r c h just so you know everything every single thing that you're seeing this editing
these effects speaking with the interviewee all of this is done out of pocket i pay for the
subscription fees i pay for zoom i pay for adobe i pay for the editor i pay personally for travel
costs if there are any i pay for so much there's so much that goes into this sponsors help but
also your support helps a tremendous tremendous amount i wouldn't be able to do this without you
so thank you so much thank you for watching for this long holy moly again if you want to support
then you can get some merch if you like and if you want to give directly on a monthly basis to
see episodes like this with such hopefully quality hopefully something that's educating that's
elucidating to you that's illuminating to you then visit patreon.com slash curt jimungle or like
i mentioned there's a paypal link in the description there's also a crypto link in the description
now i'm also interested in hearing what the other side the other side the people who are
pro ai unfettered ai who say hey there's nothing to see here you are all being hyperbolically
hysterical i'd like to see someone respond to what daniel has said about ai but also civilizational
risks in general and how ai exacerbates those so if you think of any guests who would serve as great
counterpoints especially those who are researchers and machine learning then please suggest them in
the comment section if you're a professor and you're watching and you'd like to have a friendly
feel locution that means a harmonious incongruity a good nature debate where the goal isn't to
debate but to understand one another's point of view if you're watching this and you think
hey i would like to come on to the theories of everything channel as a professor along with
my other professor friend who believes something that's antithetical to what i believe about ai
risk then please message me you can find my email address i'm sure you can also leave a comment
yeah and who knows when the next episode of toe is coming out by the way the next one is going to
be john greenwald should be in about a week or a week and a half all right let's get back to this
with daniel schmottenberger well this is a great place to end daniel you're now speaking directly
to the people well you have been this whole time but even more so now to the people who have been
watching and listening what's something you want to leave them with what should they do they're here
they've heard all these issues they hear bomean they're like okay that sounds cool that's motivating
it's a bit abstract but it is motivating okay what should i do daniel i want the earth to be here
in decades from now centuries from now what should i do
so i'm going to answer this in a way that i think factors who your audience probably is
i just i don't know we even shared demographics with me but um based on the attractor i can guess
if i was answering just to a series of technologists or investors or bureaucrats i might
say something different um and realizing that amongst that audience there are people who are
going to have radically different skills and capacities and parts of it that they feel the
most motivated and oriented to so i'm obviously not going to say one thing everybody should do
um okay what i'll say is
whether it's hearing a conversation like this where the planetary boundaries and really thinking
about how that there's more biomass of animals and factory farms and there are in the wild
left of the total amount of species extinction or the what the risks associated with rapid
development of decentralizing synthetic biology and ai are you hear these things like fuck and it
connects you to what is most important beyond your own narrow life or even the politics that is
coming into your stream or whether it's when you have a deep meditation or a medicine journey or
whatever it is and connect to what is most meaningful design your life in a way where that
experience happens regularly so what you are paying attention to and optimizing for on a
daily basis is connected to the deepest values you have because on a daily basis the people around
you and your job and your newsfeed are probably sharing other things so try to configure it that the
the deepest true good and beautiful that you're aware of is continuously in your awareness so
your daily choices of how you spend your time and money is continuously at least informed by that
that's the first thing I would say I would say a couple of the things aligned with that is
look at things that are happening in the world online to have a sense of things that you can't
see in front of you but then also get offline and connect with both the trees in front of you and
without any modeling or value system just how innately beautiful they are and also
the mirror on experience when you're with a homeless person right like so both have a sense
of what's happening at scale but then also grounded embodied sense your own care for the real world
that is not just on a computer there's a real world here and then realize like deep in
shit actually matters like independent of whether I can formalize a particular
meaning or purpose of the universe argument or formalize a response to solipsistic arguments
or nihilistic arguments like prima facie reality is meaningful and I actually do care I wouldn't
get sad or upset or inspired or moved if I didn't care about anything I actually do care
and so life matters and I make choices and I can make choices that affect the world so my own
choices matter so what choices am I making every moment and what is the basis that I want to guide
them by right to just deepen the sense of the meaningfulness of life in your own choice and this
and the seriousness with which you take how you design your life
particularly factoring factoring the timeliness and eminence of the issues that we face currently
and then the last thing I would say is as you could like really work to get more educated about
the issues that you care about and are concerned about really work to get more educated about them
get more connected to the people working on them and really study the views that are counter to the
views that naturally appeal to you so you bias correct so that your own well-motivated biases
don't mess up your action and in doing that don't let yourself become unagent don't let yourself
become so overwhelmed don't let yourself fall into easy certainties but also don't let yourself be
overwhelmed by the total uncertainty that you can't act realizing that if you don't act there are
ethical consequences to that too because we're on a moving train thank you daniel i appreciate
you spending almost four hours now with me likewise uh we covered a bunch of areas that i
did not expect but they're all good areas i'm curious how the thing ends up getting edited and
makes it through and um i'm also curious who you're particularly philosophically interested
in insightful audience what uh questions and thoughts emerge in this and maybe we'll get to
address some of them someday yeah there's definitely going to be a part two cool much more
philosophical part two if this one wasn't already the podcast is now concluded thank you for watching
if you haven't subscribed or clicked on that like button now would be a great time to do so
as each subscribe and like helps youtube push this content to more people you should also
know that there's a remarkably active discord and subreddit for theories of everything where
people explicate toes disagree respectfully about theories and build as a community our own toes
links to both are in the description also i recently found out that external links count
plenty toward the algorithm which means that when you share on twitter on facebook on reddit etc
it shows youtube that people are talking about this outside of youtube which in turn greatly aids
the distribution on youtube as well if you'd like to support more conversations like this
then do consider visiting theories of everything dot org again its support from the sponsors and
you that allow me to work on toe full time you get early access to ad-free audio episodes there as
well every dollar helps far more than you may think either way your viewership is generosity enough
thank you

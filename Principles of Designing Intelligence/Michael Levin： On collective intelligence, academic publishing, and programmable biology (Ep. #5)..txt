This podcast is about understanding how collaborative learning networks work and can be designed.
It's part of our effort to describe a methodology, which we call Unify, that can help align human
learning principles, which come from learning science, machine learning principles, and
also how to think about organizations as networks, how to be strategic in how networks
are designed, and what data to collect from networks in order to create new kinds of intelligence.
Our next guest is, in my mind, the authority, and I can't believe I got this interview,
in the field of collective intelligence.
If you haven't seen Michael Levin's TED Talk, it's amazing, I highly recommend it.
This conversation did not go as expected, because I got, frankly, a lot of really good
advice from, frankly, someone who was trying to help me out in terms of how can I propel
my work forward, and for that I'm personally grateful.
I hope you enjoy this.
Thank you so much.
The next thing we do is related to this question of embodied minds, so I'm interested in how
very diverse kinds of intelligence can exist in our universe, in all sorts of different
manifestations, different scales, both of space and time.
So we use a combination of computer science, developmental biology, biophysics, behavioral
science, computer science, to really try to understand how different degrees of agency
can be implemented in different embodiments.
And so we have parts of the lab are very theoretical and do conceptual kinds of models in almost
philosophy and in other parts of the bright code and produce various tools and software,
and in other parts do various applications of these ideas.
So we have applications in birth defects and regenerative medicine and cancer, and we have
some things in AI and synthetic bioengineering and so on.
So we kind of run the spectrum from very, very fundamental conceptual things to very
practical things that we hope will end up in the clinic at some point.
And my background originally was computer science.
I did software engineering for a long time, scientific programming.
I got a degree in genetics after that, and I've been running a biology lab in the Allen
Discovery Center ever since.
Our goal is to create a collaborative intelligence framework, something that is reliable and robust,
a playbook to give to organizational leaders.
We don't believe that the latest AI algorithm or technology trend that needs to be implemented
is the solution.
We believe that there needs to be an understanding of the guiding principles of how do you design
collaborative intelligence systems?
How do you apply them with minimal or no technology?
And because of the lack of formal definitions, we're hoping we can draw inspiration from
the field biology.
I'm wondering if there are any principles around collective intelligence that you feel
might be applicable to the domain of business?
Well, I guess because I'm not in this field, I could use a little more guidance as to what
the best case outcome would be.
So can you paint a picture for me of what you want to have happen?
What are we trying to improve?
What are we trying to achieve?
How would you recognize success if you sort of cracked this problem?
What would it look like?
So we would like to be able to model an organization using a paradigm from reinforcement learning.
That doesn't mean we're going to want to use reinforcement learning, but the idea is to
track agents taking actions in their environments and measuring the outcomes.
In this case, that can be applied to human sequences of decisions or machine or humans
and machines working together collaboratively.
And one problem we have is that we're applying a reinforcement learning way of thinking
to the macro world.
It's not just applying it to code with a set of data.
It's trying to create a virtual twin of an organization
and applying that way of thinking from reinforcement learning to organizational design.
I think success is going to be having a framework and a methodology, but also being able to implement
it in terms of, let's say, an API, a set of standards.
And step one really is to investigate what are some principles around collaborative
intelligence that work that can be applied to organizations.
I think we're really at the beginning and I don't know if we have even defined yet what success is.
I like to work backwards and I like to imagine the future that you want to see.
So what does that mean?
Does that mean the organization is functioning more efficiently towards specific goals?
Does that mean something about what those goals are?
Does that mean something about the individual happiness of the people participating in those goals?
Does that mean something about developing some kind of dominant paradigm that pushes out
competitive views of some particular field?
I think step one is figuring out what, if you did, if you already had a successful
theory of all this stuff and you were able to put it into practice, what would the implications be
for reality, for how people run these things in the real world?
And so I don't know what the answer to this is, but I guess I can talk a little bit about
what we see in biology and then, I don't love just blindly taking things from biology and pushing
them into, I mean, people often would like to do this, pushing them into social and various kinds
of societal contexts, but I think it's better to work backwards and ask what we're trying to
achieve and what that looks like. I find in general that's missing from a lot of the discussions.
A lot of people have critiques about things that are going on now, whether it's AI or
whether it's something else, they can see all the problems and it's going to lead to this,
it's going to lead to that, we don't want it. What I don't see as much as people articulating,
what do you want to see? What does future humanity look like that avoids these kinds
of things that people are concerned about? With respect to all of the parameters that
are currently kind of under debate, and I think that's what I would do here as well as I would
ask what does the future of an optimal successful organization look like? How do you recognize
it? What are we aiming for? But having said all that, I could tell you some things we learned
from the biology. I mean, one thing we learned from the biology is that one reason biology is so
successful is that it often changes the goals. Very much like with artificial life and this
notion of perverse instantiation, which is you think you're trying to solve a problem in a particular
way and if your system is flexible enough, it might do something completely different that
on retrospect, you can see how that would solve the problem, but it isn't at all what you were
looking for. Biology does this all the time. When faced with a difficult problem, one thing
biology sometimes does is switch to a different problem. If you look at the biosphere, there's
every possible kind of way of making a living. Biology is never tied to finding an answer to
a specific problem. It often changes the problem that it tries to solve. That may or may not
work for us because you may come up with an organizational structure that is very good at,
let's say, perpetuating itself, but one of the ways it might do that is to go off,
completely go off script as far as where you thought the organization was going to go.
And that may not, I mean, that works in the biosphere if your goal is to make sure that life
survives in some particular, you know, on a planet. That may not be that ability to just
completely change goals, may not be what you want for an organization. The other thing is that
when you biological agents, and those could be cells, they could be molecular networks,
they could be tissues, organs, whatever, they're very good at combining into higher level entities
that are these emergent cells that do things in other problem spaces. But there's a few things
that are not guaranteed when that happens. One thing that's not guaranteed is that the emergent
self is smarter than the components. It doesn't have to be, sometimes it certainly is, but it
doesn't have to be. And so that's no guarantee, you know, scaling up is no guarantee that it's
going to be more intelligent. Also, when you create these novel cells, you simultaneously
create various goals, preferences, and various kinds of salience and valence for these things that
don't necessarily match those of the subunits. So in other words, right, people often say, you
know, people often hear the story, I tell of gap junctions and cancer, this idea that
cells are tied together with gap junctions that basically erase their individuality a little
bit so that they're all part of this collective and they work on making the organs and whatnot.
And then when there's a breakdown of that, cells roll back to the kind of unicellular form and
they go off and they metastasize and so on. And so people hear that and often they say, well,
that's clear. What we have to do for the various ills of society, you know, we can address that
that the whole selfishness thing and so on, we'll just we'll just sort of gap junctions ourselves,
take it to each other in some format and when wipe our individuality a little bit and we'll
we'll be this this collective. I think that's a that's a in general, that's a terrible idea because
one of the clear things is that when you as a as a as a large scale individual, let's say a human
goes off and does various things. My silly example is rock climbing, you know, you go rock climbing
and you have a lovely day and you meet various social goals, you meet various personal happiness
goals and so on as as a human. But you've left a bunch of skin cells on that on that, you know,
on that rock. And nobody asked those skin cells whether this was something that they were interested
in. And just in general, the basic the basic premise of having a multicellular body is that
the very the vast majority of your body cells are going to die in the service of whatever weird
to them incomprehensible goals that the collective human might have. So that stuff is very easily
scaled up into social structures where you might have persistent, you know, persistent
kinds of government persistent kinds of companies and so on, that pursue goals that are at odds with
the well being the happiness and the goals of the beings that that make them up. And
that that needs to be it needs to be clear that when you scale up like that, you you are not
guaranteed. In fact, in fact, the big thing is that we don't have a good science of being able
to anticipate the goals of collective systems. We really can't anticipate that very well now
at all. I am actually I talk about these these biology examples all the all the time.
In terms of, like, we make something called a frog a lot in our lab. And so this is part frog and part
axolotl. And so baby axolotls have little four legs and tadpoles don't have four legs. And so
when we make this frog a lot, I often say this in my talk, you know, we have we know the genome
for the frog, we know the genome for the axolotl. Can you tell me if a frog a lot is going to have
legs or not? Right. And and there's no no one can predict that up front. And that's because
while we understand the molecular hardware to some extent of the pieces, we do not understand how
the collective decisions are made, we don't know how they decide to make these large scale structures.
And the say that I can get you know, there's a million examples that. So so that's the thing
when we make these when we make these collectives, we really don't know what what the goal of the
if there is going to be an emergent agent, we don't know what the goal is going to be. And we
don't know if it at all matches our goals, even if we I mean, even our own we don't match goals
with each other, but never mind, even if never mind that. Yeah, so so I think so I think that's
the danger, right? So so I think biology can teach us some very successful principles about
making sure you stick around. But I'm not sure that's the end all and be all that we want from
the structures that we build, right? Persistence isn't really what we're looking for. We're looking
for something much more specific. So you got to you got to take all that biology stuff with a
with a grain of salt. So one of my favorite things as a consultant is that I get to speak
with individuals who are very aware of their problems, very aware of goals and the financial
value of their goals. But when it comes time to connect the dots between these different teams
in more of a collective intelligence or collaborative intelligence approach,
typically there's no person in charge or there's no system or process or computational way
that combines them all and analyzes them. And I found this gap to be really intriguing and I want
to pursue formalizing a process that could be something that we could publish and like let's
see an academic journal or paper to help push this area of research forward. And I think the
one challenge I've had is like simply defining, you know, what is collective intelligence for an
organization, what is collaborative intelligence, because I hear different people, different startups
talking about those two and and yet it's all very fuzzy. So I thought that
trying to pursue a more formalized definition that respected kind of academic perspectives
would be a good approach for us to have. Yeah. You know, I don't know that's that's a whole other
thing this this like meta meta goal seeking which is what what should the goals be so so
gaining respect of the academic community. I'm not even sure that's an achievable
goal per se for anybody because there is no monolithic academic community. So there are
different islands of academics where like so so so there's a there's a particular kind of standard
talk that I sometimes give. And I always know what kind of department I'm giving the talking
based on which part it makes people angry. And it's often a different it's always a different part
because there are certain things you can say in the in a neurobiology department and everybody's
like, Yeah, no kidding. So what we knew that. And the exact same thing in the genetics department
and people throw tomatoes. And so trying for kind of generic acceptance, I'm not sure that's
feasible until until we get a lot better at breaking down these these kind of
conceptual silos that the communities are in. And also, you know, what's the
what's the payoff of that? You know, you want you what I mean, what do we actually what you
know, what do we actually want? Why we write these scientific papers? What what what is the
impact? I mean, I think there's been a lot of talk of impact in the last decade, maybe more.
And it's on the one hand, I think it's terrible. And on the other hand, I think it's really good.
It has good kind of effects, which which really forces us to think about what do you really want
to have happen when you write that paper, right? Besides the kind of obvious stuff of, you know,
tenure or whatever, you know, get a job or you know, get a good position, whatever. But like
long term, the change you're trying to make in the world, what happens when when you write these
papers? So in theory, I mean, what are some possible answers to that? Sometimes people read
it and they get inspired and they do something else. So that's cool. But maybe you're better off
just doing that other thing yourself directly, right? That's one possibility. Another another
possibility is that maybe you can unify people across fields so that they sort of bring some
interdisciplinary thought to that. So that's so that's pretty good. That's that's and I don't
know if that's gaining acceptance, that's more gaining interest, right? So you want people to
start thinking about things and asking questions that of things that they may have taken on,
you know, as an assumption before that. So that's pretty good. But you know, the actual impact
of academic papers, even in academia is often not clear. But outside of academia, I don't know. I
mean, you know, Chris Fields had a great sentence recently where he said, arguments are only
settled by technologies. And and I think that's I think that's true. I think I think we can have
all kinds of abstruse academic arguments about things and people have all kinds of commitments
to stuff. But in the end, if the idea is a good one, it's going to the rubber is going to hit the
you hit the road and you're going to make some sort of impact on the physical world and there's
going to be some sort of application, some sort of practical thing. And then it's going to be
irrelevant, which part of academia likes it and which part doesn't. So I don't know, you know,
I think I think you got it. Again, it's about working backwards to ask what's the change that
you want to see. And how much of that change is requires buy in from academia, which part of
academia, whether papers are sufficient. And there's a whole other thing, which is that there
are so many papers now that I, you know, most people don't have time to even catch up with
the things in their field anymore. Never mind, you know, sort of other things. I sometimes get
the feeling that all these papers were writing, but not really for us. They're for some sort of
future AI or some sort of augmented future scientist who's actually going to be able to
have the bandwidth to to read all this stuff. Because I mean, I know you can't see it, but
I've got behind my chair, I've got these like stacks of papers that I'm supposed to have read
by now and they're just getting bigger and bigger because there's no time to do it.
So yeah, I would, I would work backwards and I would think about whose eyeballs are
are we trying to capture here, right on these papers and what's the, what's the, what's the
goal of that, you know, this conversation did not go at all like I was expecting. I
am really grateful that you challenged me on some of my current beliefs around pursuing
an academic approach to defining collective intelligence or collaborative intelligence
from kind of a theoretical and academic perspective for organizations to implement.
I have to do a lot of thinking on this. And as I reflect the questions that come to mind
as I was listening were how does your lab pursue success and how does your lab think about
translating theory into application? I view academia serving a higher purpose to
make the world a better place through creation and curation and sharing of knowledge.
And we'd love to hear some of what you've learned works.
First, we generate a lot of very basic fundamental knowledge and also conceptual apparatus. So we
come up with very sort of fundamentally almost philosophical like like perspectives on things
and then then that gets cashed out as as software or various computational paradigms and then that
gets cashed out in new experiments and new capabilities and so on. And I firmly believe
that to the extent that we're on the right track with any of this stuff,
really kind of transformative applications should follow. And this is why we now have a
spin-off company doing regeneration and we have one doing computer AI designed synthetic living
machines and there's another one on cancer. So like this stuff is moving towards hopefully
towards the clinic. So we're not in humans, patients yet by any means, but we're going in
that direction. And the other thing too is that I kind of try to strike a balance because
I'm not even a clinician. I don't do clinical work at all. I get emails and phone calls every
day from people with the most unbelievable medical needs. I mean, you just can't imagine what's out
there, the need that's out there and the things that happen to people from all the way from
birth defects to do various other things that happen in your lifetime. The need is intent.
And so I consider it, I would not consider it a success if all we ever did was generate some kind
of conceptual stuff and basic science that never helped patients. I want to see in my lifetime,
Indiana World, we'll see if that happens or not, but in my lifetime, I want to see actual patients
helped with the stuff that we do. So it's kind of a two-pronged attack. And you can always argue
about what the right prioritization is. And some people say, don't get tied up with this stuff,
spending time with these companies and figuring out how to make products and argue with the FDA.
You should be spending your time on basic science. And then other people write me and they say the
opposite, like, what are you doing, spending time figuring out things about the nature of the self?
You should be solving cancer. And so you kind of juggle those two things. But I think we're
pretty fortunate in my group that I think we can do some of both, actually. And I think they're very
tightly related. I think if you do it right, you can make impacts on both. So that's how I see it.
I'm imagining the world you see as we will be able to regrow an arm if someone were an accident
and were to lose it. Is that correct? Embodiment that you've got through the vagaries of genetics
and evolution, you could change it the way we change many other things. You want tentacles,
you want to see an infrared, you want to live underwater. Why not? Who said this random walk
that evolution took to get you here is how it has to stay? I think down the line, I mean,
without scaring everybody and so on, because a lot of people find that pretty freaky, but I'm
certainly not the first person to say stuff like this. Down the line, I believe in maximizing
freedom, which includes freedom of embodiment. You should not be locked into some random configuration
that's susceptible to weird diseases and aging and dumb stuff that can happen to you from stepping
on the wrong patch of ground. All this stuff is just, it's the way, we started out with antibiotics
and wearing clothes against the cold and things like that. And that was it after that. It became
completely obvious that we do not have to stay the way we came into the world. It's an amazing
future. I sign off with that vision. I think that sounds amazing. That's amazing. And to me,
it seems like it's leaving the boundaries of what does it even mean to be human?
Yeah, no. So two things. One is that I don't think these are boundaries at all. There are no
boundaries. The boundary is us not knowing how to work the interface. So if you're given a calculator
and you have no idea what this thing is or how to use it, there's a real boundary between you and
the capabilities of that calculator, but it's not real. It's a boundary of ignorance and those
things are improvable. So we can do that now. We have a process, the scientific process and
some other stuff that can help us get through that. So I don't believe in these boundaries at all.
Let's face it. We haven't believed in these boundaries for a long time. If you walk into
any gym or a martial arts studio or a university and you see people removing their boundaries,
you don't come out of there the way you went in. You come out with extra powers. You come out with
whether that be new brain power or muscles or you learn to swim and hold your breath underwater,
whatever you're going to learn. We know we can improve ourselves and the only limitation. And
frankly, some people are amazing and they've pushed it even without all this technology that I'm
talking about. They've already pushed this. I mean, we've all seen there are humans that are so
on some particular thing that they've dedicated their life to. They're so far outside the mean
that it's unbelievable that that's even possible. And that's without knowing frankly very much at
all about how biology works yet. So whether mental, whether physical, whether spiritual,
I think that's important. There are no boundaries. All these boundaries are defined by our own
ignorance. And then the other important thing you said is, are we leaving what it means to be
human? So that's an interesting question. What do we mean when we say human? In particular,
because of this whole AI thing, somebody, I forget who it is, but somebody's working on these
proof of humanity certificates. And it's a good thing to think about. If somebody,
if you're interacting with somebody and they show you their little stamp or whatever it's
going to be, that that's the proof of humanity, what is it that you really are looking for? When
you're looking for that proof of humanity in someone or when you want to say whether someone's
left the human category, what are you really looking for? Are you looking to validate that
their DNA is the same that evolution left us in as homo sapiens? I don't think so. Do you really
care about anybody's DNA? I don't. I don't think that's relevant for anything. Is it body structure?
Do you want confirmation that that person hasn't had some percentage of their organs replaced by
various prosthetics? I don't care about that either, right? Does that matter to you? I don't
know. So what is that? When we say somebody's human, what do we actually mean?
And I think, and I'm sure other people have different definitions for it, but I think what
you mean is a minimal level of compassion. That's what I think. What you're looking for,
when somebody says they're human, you're looking for a cognitive light cone on that individual
that is actively able to care at least maybe more, but at least to the same level that you can
about others, about various goals. If I am trying to choose, I'll tell you,
you're going to go live on Mars or something and you're choosing a companion,
what you want in that proof of humanity is not anything but their DNA or whether they have
some prosthetic organs or something. What you want is, do they have the capacity to care about
the same stuff that I care about, the same degree of stuff? Because if they don't, if they're a rumba
or a cat or something else, nice, but not the same relationship you can have with a human.
So that's what I think humans are. And after that, everything else is up for grabs. You've got
gills, you've got your third brain hemisphere, whatever, who cares? So that's my view of it.
I think I mean, going macro, I think from that, sorry, Ron, but one thing that I was thinking
about, because you're talking about these boundaries or what is our predefined view
of what it means to be human. And I think if you take even that macro to the stuff that,
for example, Ron and I are working on, is that we are working so hard to try to map out organizations,
to try to understand organizations. But what does even an organization mean?
Are we actually thinking about organizations in our preconceived ways of thinking about them
and not thinking what an organization could be? Is like, are we just mapping the status quo,
whereas we could help in organizations evolve in the same way that you're saying humans can evolve
way farther than everybody's thinking about now? And I think that's a very interesting angle.
What do you think, Ron? That's a fascinating question. Level one, let's say, is what are the
goals of an organization? Just listing them out among the individual nodes, then you could say
level two is what are the connections between what people care about and how are they mapped
together? And then level three is, well, how are they adapting and changing together? I think
that's new for me to think about from this conversation. And I don't know how to answer
question one, because this is when we talk about adapting our biology and thinking about
an organization, like being able to adapt to an organization like we would DNA.
It's all new for me to think about. And the thing with goals, too, is they can be explicit or not.
So like any psychoanalysts will tell you that you think you have goals. They may or may not be
the actual goals that drive your behavior. We all know we have all kinds of internal modules
that are trying to maximize and minimize various things that we don't necessarily have direct
access to that may or may not be adaptive in our context and so on. So yeah, in the future. So I
guess, I mean, I don't know anything about this field, but if I had to guess in the future,
I sort of envisioned some kind of psychoanalysis of organizations where there's some way of kind of
like we had this project where we were trying to communicate with an ant colony, not the ants,
the colony, which is a completely different thing. It's very hard. But at some point, there might
be techniques, the way that we try to talk to organs instead of the individual cells and so on.
There might be ways to find out what does the actual organization want. And the individual
people will tell you, oh, well, I know there's these set of goals. And you don't know anything
any more than if you ask you the cells or the organs in your body, what are the goals of the
organism? They'll tell you some stuff about physiological boundary parameters and some
things like that. But they can't even begin to guess your goal of whatever, going to grad school
four years from now. They can't fathom it. So who knows what the organization is going to want.
So in the future, there might be some. I think we need it. I think if we're going to survive,
and as a mature species, we're going to have to get our science around being able to identify,
characterize, relate to, and modify these kind of emergent, very unconventional emergent agents,
which are anything from organizations to evolutionary lineages and things like that.
I know you have a hard stop. So three minutes left. There's something you said,
which just my ears perked up. Can you just spend a couple of minutes when you say you
want to speak to the ant colony itself and not the ant? That's an amazing visualization. My brain
is so curious about, can you just describe more what that means? Sure. Yeah. And this is, you know,
I'm definitely not the first person to talk about this. Doug Hofstadter in his book,
Girdel Escherbach, had some great thoughts about it. And before that, in the 20s, Eugene Marais
had this book called The Soul of the White Ant, which is just amazing about the kind of the life
of the colony that's distinct from the individual members. So this idea that if we take collective
intelligence seriously, and this idea that the colony works in different problem spaces
than the individual ants, and they have, it has a memory of those things that no individual ant
knows and things like that. So we wanted to communicate with it. And we took a very simplistic
approach, which is that training is a simple kind of communication. So if we train you to do
something, then we've communicated the fact that something is good and something else is bad. So
so you know, one act of communication has taken place, very minimal. So so what we tried to do,
and we haven't, this is something that started right during the pandemic. So it never like
really got off the ground. But we'll, you know, we'll get to it eventually, is just imagine a
imagine an ant colony. And there's a location where little little drops of food get dropped by a
computerized system. But the amount of food that gets dropped there is proportional to the number
of ants standing on a platform at the other end of the colony. So what that what that means is
that no individual ant ever has the experience of I stand here and then I collect my reward,
no ant has that experience. So now if we find over time with training that the colony sends
a bunch of ants over in this direction to pick up the food over here, that's a fact that's a
that's a conditioning, that's a that's a bit of learning that was done by the colony by no
individual ant. And, you know, it sounds all crazy or anything, but but you know, when you
when you train a rat to push a lever and get a pellet, the cells at the bottom of the foot
are what's interacting with the with the with the with the button and and the cells that get
the reward are in the intestine, no individual cell has both experiences. So we've got this
thing we call a rat, which is this collective that's able to do the credit assignment and
figure out that these two things are related. That's the collective intelligence, we're all
collective intelligences. And so we try to we try to communicate with the ant colony that way.
So stay tuned. I don't know if that's going to work or not.
Mind is blown right now. Thank you so much. Cool. Thank you. Yeah, it's a fun,
fun conversation. I appreciate it. Cheers. Thank you so much. Very nice to meet you. Thank you.
Appreciate it.

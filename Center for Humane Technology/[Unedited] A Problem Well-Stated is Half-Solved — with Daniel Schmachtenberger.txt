Hi, everyone. It's Tristan, and this is your Undivided Attention.
Up next, we have our unedited conversation with Daniel Schmacktenberger.
And because it's unedited, it's longer and not corrected for fact-checking purposes,
but you can find our shorter, edited version wherever you found this one.
Listen to both versions, and then come to our podcast club with Daniel and me,
and hopefully you, on July 9th. Details are in the show notes.
And with that, here we go.
Welcome to your Undivided Attention.
Today, I am so honored and happy to have my friend, Daniel Schmacktenberger,
as our guest, who works on the topics of existential risk
and what are the underlying drivers of all of the major problems,
or many of the major problems, that are really facing us today as a civilization,
be it climate change, breakdown of truth, social media, our information systems.
Those of you who have been following your Undivided Attention
will hear this as a very different kind of episode.
We almost think of it as a meta-episode about the underlying drivers
of many of the topics that we have covered on your Undivided Attention thus far.
So, if you think about the topics that we've covered,
whether you've seen the social dilemma, or you followed our interviews previously
on topics like attention span shortening, or addiction, or information overwhelm and distraction,
the fall of trust in society, more polarization, breakdown of truth,
our inability to solve problems like climate change,
well, this is really about an interconnected set of problems
and the kind of core generator functions that are leading to all of these things to happen at once.
And so, I really encourage you to listen to this all the way through,
and I think that we're going to get into some very deep and important knowledge
that will hopefully be orienting for all of us.
One of my favorite quotes is by Charles Kettering, who said that
a problem not fully understood is unsolvable, and a problem that is fully understood is half-solved.
And what I hope we talk about with Daniel is what about the framework that we are using
to address or try to meet the various problems that we have has been inadequate,
and what is the problem-solving framework that we're going to need
to deal with the existential crises that face us.
So, Daniel, welcome to Your Undivided Detention.
Thank you, Tristan. I've been looking forward to us dialoguing about these things publicly for a while.
Well, you and me both, and for those who don't know, Daniel and I have been friends for a very long time,
and his work has been highly influential to me and many people in my circles.
So, Daniel, maybe we should just start with what is the metacrisis
and why are these problems seemingly not getting solved,
whether it's the SDGs, climate change, or anything that we really care about right now?
I think a lot of people have the general sense that there is an increasing number of possibly catastrophic issues,
and that as new categories of tech, tech that allows major cyber attacks on infrastructure,
tech that allows weaponized drone attacks on infrastructure, biotechnologies,
artificial intelligence and moving towards AGI,
that there are new catastrophic risks with all of those categories of tech,
and that those tech are creating larger jumps in power faster than any types of jumps of tech,
including the development of the nuclear bomb in the past by many orders of magnitude.
So, there's a general sense that whether we're talking about future pandemic related issues,
or whether we're talking about climate change or climate change as a forcing function for human migration
that then causes resource wars and political instability,
or the fragility of the highly interconnected globalized world
where a problem in one part of the world can create supply chain issues that create problems all around the world,
there's a sense that there's an increasing number of catastrophic risks,
and that they're increasing faster than we are solving them,
and that when you mention like with the UN,
while progress has been made in certain defined areas of the sustainable development goals,
and progress was made back when they were called the Millennium Development Goals,
we're very far from anything like a comprehensive solution to any of them,
we're not even on track for something that is converging towards a comprehensive solution.
And if we look at kind of the core initial mandate of the United Nations in terms of thinking about
how do recognizing after World War II that nations take government alone wouldn't prevent World War,
and now that World War was no longer viable because the amount of technology we had made it a war that no one could win,
we still haven't succeeded at nuclear disarmament.
We did some very limited nuclear disarmament success while doing nuclear arms races at the same time,
and we went from two countries with nukes to more countries with better nukes.
And that simultaneous to that, every new type of tech that has emerged has created an arms race,
we haven't been able to prevent any of those.
And the major tragedy of the commons issue is like climate change and overfishing,
and dead zones in the oceans, and microplastics in the oceans, and biodiversity loss,
we haven't been able to solve those either.
And so rather than just think about this as like an overwhelming number of totally separate issues,
the question of why are the patterns of human behavior as we increase our total technological capacity,
why are they increasing catastrophic risk, and why are we not solving them well?
Are there underlying patterns that we could think of as, as you mentioned,
generator functions of the catastrophic risk, generator functions of our inability to solve them,
that if we were to identify those and work at that level, we could solve all of the expressions or symptoms,
and if we don't work at that level, we might not be able to solve any of them.
And again, people have been thinking about this for a long time, kind of notice these issues.
They notice that you try to solve a, like the first one I noticed when I was a kid,
was trying to solve an elephant poaching issue in one particular region of Africa
that didn't address the poverty of the people that had no mechanism other than black market on poaching,
didn't address people's mindset towards animals, didn't address a macroeconomy that created poverty at scale.
So when the laws were put in place and the fences were put in place to protect those elephants in that area better,
the poachers moved to poaching other animals, particularly in that, that situation, rhinos and gorillas
that were both more endangered than the elephants had been.
So you moved a problem from one area to another and actually a more sensitive area.
And we see this with, well, can we solve hunger by bringing commercial agriculture to parts of the world that don't have it
so that the people don't either not have food or we have to ship them food,
but if it's commercial agriculture based on the kind of unsustainable, environmentally unsustainable agricultural processes
that lead to huge amounts of nitrogen runoff going into river deltas that are causing dead zones in the ocean
that can actually collapse the biosphere's capacity to support life faster
than we're solving for a short-term issue that's important and driving even worse long-term issues.
We see that many of the reasons people who oppose climate change solutions in the West oppose them is because
not because they have even really deeply engaged in the underlying science and say the climate change isn't real,
that will oftentimes be what's said, but because the solution itself seems like it'll cause problems to other areas
that they're paying attention to that seem even more critical to them.
So if the solution involves some kind of carbon tax or something that would decrease GDP for the countries that agree to it
and some other countries don't agree to it, and let's say in this particular case the model that many people have is
Western countries agree to it, their GDP growth decreases, China doesn't agree to it,
there's already a very, very close neck-and-neck fight for who controls power in the 21st century.
Are we seeding the world to Chinese control that many people would think it has less civil liberties
and is more authoritarian in its nature?
Or some people's answer to climate change is, well, we just have to use less energy,
but when you understand that energy correlates directly to GDP and when GDP goes down,
that's poverty, people in extreme poverty first and worst, and wars increase
because people who have desire to get more end up going zero sum on each other
and only when it's very positive sum does that not happen.
You see all these intricate theory of trade-off, so we can't see that the problem is climate change.
Everybody knows the problem of climate change seems like a big thing,
but you've got to look at climate change plus the macroeconomic issues that would affect the poorest people
and that would increase the chance of war and the geopolitical dynamics
between the West and China, whatever, and the enforcement dynamics of international agreement.
When you start to recognize that the problem is that suite of things together,
in a way it seems, well, that's too hard, we can't even begin to focus on it.
I would say that that's actually easier because trying to solve climate change on its own is actually impossible
because if you're trying to solve something that is going to externalize harm to some other thing,
maybe you solve that thing, but you find out that you're in a worse position.
So I would say that it's impossible to actually improve the world that way
or half the world that is paying attention to that other thing disagrees with you so vehemently
that all the energy goes into infighting and whatever some part of the world is trying to organize to do,
the other part of the world is doing everything they can to resist from happening,
then all the creative energy just burns up as heat and we don't actually accomplish anything.
So I would say that the way we're trying to solve the problems is actually mostly impossible.
It either solves it in a very narrow way while externalizing harm and causing worse problems
or it makes it impossible to solve it all because it drives polarization.
And so going to the level at which the problems interconnect,
where that which everybody cares about is being factored and where you're not externalizing other problems,
seems more complex is actually possible and possible is easier than impossible.
And so it's not just that there's a lot of issues, right?
There are a lot of issues and just that the issues are both more consequential at greater scope
and moving faster than previous issues because of the nature of exponentiating technology.
That's part of it. It's not just that the problems are all interconnected.
It's also that they do have underlying drivers that have to be addressed,
otherwise a symptomatic only approach doesn't work.
The first underlying driver that when people look at it, they generally see,
is they see things like structural perverse incentive built into macroeconomics,
that the elephant dead is worth more than the elephant alive is and so is the rhino and so is the...
And so how do you have a situation where that's the nature of incentive,
where you're incentivizing an activity and then trying to bind it or keep it from happening?
And the same would be true with overfishing.
As long as live fish are worth nothing and dead fish are worth more
and there's something fundamentally perverse about the nature of the economic incentive.
And the same is true that when we have war and there's more military manufacturing, GDP goes up.
And when there's more addiction and people are buying the supply of their addiction, GDP goes up.
And when there are more sick people paying for health care, cost GDP goes up.
So it's obviously a perverse kind of metric.
So anytime someone can fiscally advantage themselves or a corporation can,
in a way that either directly causes harm or indirectly externalizes harm,
we have to fundamentally solve that.
If there's something like $70 trillion a day of activity happening,
that is a decentralized system of incentive, that is incenting people to do things
that are directly or indirectly causing harm,
there's really nothing we can do with some billions of dollars of nonprofit or state
or whatever money that is going to solve that thing.
So we have to say, well, what changes at the level of macroeconomics need to happen
where the incentive of individuals and the incentive of corporations and the incentive of nations
is more well aligned with the well-being and the incentive of others.
And so we're less fundamentally rivalrous in the nature of our incentive.
So we can see that underneath heaps of the problems,
structures of macroeconomic incentive are there.
That's kind of maybe the first one that most people see.
We can go deeper to seeing that even as an expression,
because whether it's a economic incentive for a corporation,
or whether it's a power incentive, a political power incentive,
or a political party, or for a country,
they're both instantiations of rivalrous-type dynamics that end up driving arms races.
Because if you win at a rivalrous dynamic, the other side reverse-engineers your tech,
figures out how to make better versions, comes back,
which creates an exponentiation in warfare,
and eventually exponential warfare becomes self-terminating on a finite planet.
Exponential externalities also become self-terminating.
So if we want to say, what are the underlying generator functions of catastrophic risk?
First, maybe just to make clear, the catastrophic risk landscape.
Is this all right if we do a brief aside on that?
Yeah, let's do it.
And then let's do that, and then let's recap just what these structures are.
So people are tracking each of these components,
because you've already mentioned a few different things.
I mean, the first thing is just many listeners might hear what you're sharing
as an overwhelming set of problems, and I think it's just to recap.
It's important people understand that it's overwhelming if you're not using a problem-solving framework
that allows you to see the interconnected nature of those problems,
because if you solve them with the limited tools we have now,
let's just solve the social media problem by pulling one lever
and changing one business model of one company or banning TikTok,
but then you get 20 other TikToks that come and sit in its place
with the same perverse incentive of addiction,
the same rival risk dynamic competing for human attention.
We're going to end up perpetuating those problems.
And so just to sort of maybe recap some of that for listeners,
and I think maybe let you continue with the other generator function.
Let's just make sure that people really get those frameworks.
I think it's really important.
Yeah, I mean, in the case that you in Center for Humane Technology
have brought so much attention to with regard to the attention,
harvesting, and directing economy,
it's fair to say that it probably was not Facebook or Google's goal
to create the type of effects that they had.
There were unintended externalities, there were second order effects,
but they were trying to solve problems, right?
Like, let's solve the problem if it were Google
of organizing the world's information and making better search.
That seems like a pretty good thing to do.
And let's solve the problem of making it freely available to everybody.
Well, that seems like a pretty good thing to do.
And with the AdModel, we can make it freely available to everyone,
and let's recognize that only if we get a lot of data
will our machine learning get better.
And so we need to actually get everybody on this thing,
so we definitely have to make it free.
And, you know, then we get this kind of recursive process.
Well, then the nature of the AdModel, doing time on site optimization,
and stuff I'm not going to get into because you've addressed it so well,
ends up appealing to people's existing biases
rather than correcting their bias, appealing to their tribal in-group identities
rather than correcting them, and appealing to limbic hijacks
rather than helping people transcend them.
And as a result, you end up actually breaking the social solidarity
and epistemic capacity necessary for democracy.
So it's like, oh, let's solve the search problem.
That seems like a nice thing.
The side effect is we're going to destroy democracy
and open societies in the process, and all those other things.
Like, those are examples of solving a problem
in a way that is externalizing harm,
causing other problems that are oftentimes worse.
And so let's just focus on the opportunity.
And just to say, typically, this will get accounted for as,
oh, this is just an unintended consequence.
But there's some other generator functions I think we should outline.
I mean, if YouTube and Google didn't personalize search results
and what video to show you next, and the other guy did,
and TikTok starts personalizing, they're caught in a race to the bottom
of whoever personalizes more for the best limbic hijack.
And so just to sort of connect some of those themes together for listeners.
So you mentioned race to the bottom.
And obviously, CHD has discussed this before,
and this is a key piece of the game theoretic challenge
in global coordination.
And the two primary ways it expresses itself
is arms races and tragedy of the commons.
And the tragedy of the commons scenario is,
if we don't overfish that area of virgin ocean,
but we can't control that someone else doesn't,
because how do we do enforcement if they're also a nuclear country?
That's a tricky thing, right?
How do you do enforcement on nuclear countries, equipped countries?
So us not doing it doesn't mean that the fish don't all get taken.
It just means that they grow their populations and their GDP faster,
which they will use rivalrously.
So we might as well do it.
In fact, we might as well race to do it faster than they do.
Those are the tragedy of the commons type issues.
The arms race version is if we can't ensure that they don't build AI weapons,
or they don't build surveillance tech,
and they get increased near-term power from doing so,
we just have to race to get there before them.
That's the arms race type thing.
It just happens to be that while that makes sense for each agent on their own in the short term,
it creates global dynamics for the whole and the long term that self-terminate,
because you can't run exponential externality on a finite planet.
That's the tragedy of the commons one,
you can't run exponential arms races and exponential conflict on a finite planet.
So the thing that has always made sense,
which is just keep winning at the arms races,
has had a world where we've had lots of wars increasing in their scale
and lots of environmental damage.
We started desertification thousands of years ago.
It's just, there's been a long, slow exponential curve
that really started to pick up with the industrial revolution
and is now really verticalizing with the digital revolution,
the cumulative harm of that kind of thing becomes impossible now.
So basically, with the environmental destruction,
with the wars and with the kind of class subjugation things that civilizations have in the past,
pretty much anyone would say we have not been the best stewards of power
and technology is increasing our power.
Exponential tech means tech that makes better versions of itself,
so you get an exponent on the curve,
and we're now in a process where that's a very, very rapid.
Computation gives the ability to design better systems of computation,
and computation in AI applied to biological big data and protein folding
gives the ability to do that on biotech and on and on, right?
So we could say the central question of our time
is if we've been poor stewards of power for a long time
and that's always caused problems,
but the problems now become existential,
they become catastrophic, we can't keep doing that.
How do we become adequately good stewards of exponential power in time, right?
How do we develop the good decision-making processes,
the wisdom necessary to be able to be stewards of that much power?
I think that's a fair way to talk about the central thing.
Now, if it's okay, the thread we were about to get to, I think, is a good one,
which was the history of catastrophic risk coming up to now,
is that before World War II, catastrophic risk was actually a real part of people's experience.
It was just always local, but an individual kingdom might face existential risk in a war where they would lose,
and so the people faced those kinds of reality, and in fact, one thing that we can see
when you read books like The Collapse of Complex Societies by Joseph Tainter
in any study of history is that all the great civilizations don't still exist,
which means that one of the first things we can say about civilizations is that they die.
They have a finite lifespan on them.
One of the interesting things we can find is that they usually die from self-induced causes.
They either over-consume the resources and then stop being able to meet the needs of the people
through unrenewable environmental dynamics, and that's old,
or they have increasing border conflicts that lead to enmity,
that has more arms race activity coming back at them,
or they have increasing institutional decay of their internal coordination processes
that leads to inability to operate quickly in those types of things.
So we can say that it's the fundamentally most all civilizations collapse
in a way that is based on generally self-terminating dynamics,
and we see that even when they were overtaken by armies,
oftentimes they were armies that were smaller than ones they had defended against successfully
at earlier peaks in their adaptive capacity.
Okay, so catastrophic risk has been a real thing, it's just been local,
and it wasn't till World War II that we had enough technological power
that catastrophic risk became a global possibility for the first time ever.
And this is a really important thing to get because the world before World War II
and the world after was different and kind so fundamentally,
and this is why when you study history,
so much of what you're studying is history of warfare,
of neighboring kingdoms and neighboring empires fighting,
and because the wars were fundamentally winnable, at least for some, right?
They weren't winnable for all the people who died, but at least for some.
And with World War II and the development of the bomb
became the beginning of wars that were no longer winnable,
and that if we employed our full tech and continued the arms race even beyond the existing tech,
it's a war where win-lose becomes omni-lose-lose at that particular level of power.
And so that created the need to do something that humanity had never done,
which was that the major superpowers didn't war.
The whole history of the world, the history of the thing we call civilization, they always did.
And so we made an entire world system, a globalized world system,
that was with the aim of preventing World War III.
So we could have non-kinetic wars, and we did, right?
Increasingly, you can see from World War II to now a movement to unconventional warfare,
narrative and information warfare, economic, diplomatic warfare, those types of things.
Resource warfare.
And you could, if you were going to have a physical kinetic war, it had to be a proxy war.
But to have a proxy war, that also required narrative warfare to be able to create a justification for it.
But also to be able to prevent the war.
So the post-World War II, Bretton Woods Mutually Assured Destruction United Nations World,
was a solution to be able to steward that level of tech without destroying ourselves.
And it really was a reorganization of the world.
It was a whole new advent of social technologies or social systems,
just like the U.S. was new social technologies or social systems coming out of the Industrial Revolution.
The Industrial Revolution ended up giving rise to kind of nation-state democracies.
The nuclear revolution in this way kind of gave rise to this I.G.O. intergovernmental world.
And it was predicated on a few things.
Mutually Assured Destruction was critical.
Globalization and economic trade was critical that we, if the computer that we're talking on,
and the phone that we talk on, is made over six continents and no countries can make them on our own,
we don't want to blow them up and ruin their infrastructure because we depend upon it.
So let's create radical economic interdependence so we have more economic incentive to cooperate.
Make sense?
And let's grow the materials economy so fast through this globalization
that the world gets to be very positive GDP and gets to be very positive sum
so that everybody can have more without having to take each other's stuff.
That was kind of like the basis of that whole world system.
And we can see that we've had wars, but they've been proxy wars and cold wars.
They haven't been major superpower wars and they've been unconventional ones.
But we haven't had a kinetic World War III.
We have had increase of prosperity of certain kinds.
75 years give or take.
Now we're at a point where that radically positive sum economy that required an exponential growth of the economy,
which means of the materials economy, and it's a linear materials economy
that unrenewably takes resources from the Earth faster than they can reproduce themselves
and turns them into waste faster than they can process themselves,
has led to the planetary boundaries issue where it's not just climate change or overfishing
or dead zones in the ocean or microplastics or species extinction or peak phosphorus.
It's a hundred things, right?
There's all these planetary boundaries, so we can't keep doing exponential linear materials economy.
That thing has come to an end because now that drives its own set of catastrophic risks.
We see that the radical interconnection of the world was good in terms of will not bomb each other,
but it also created very high fragility because what it meant is a failure anywhere
could cascade to failures everywhere because of that much dependence.
So we can see with COVID we had what was a local issue to an area of China,
but because of how interconnected the world is with travel,
it became a global issue at the pandemic level,
and it also became an issue where to shut down the transmission of the virus,
we shut down travel, which also meant shut down supply chains,
which meant so many things, right?
And very fundamental things that weren't obvious to people at first,
like that country's agriculture depends upon the shipment of pesticides that they don't have stored.
And so we got these swarms of locusts because of not having the pesticides,
which damaged the food supply and shipments of fertilizer and shipments of seed.
So we end up seeing a drive of the food insecurity of extreme poverty
at a scale of death threat that is larger than the COVID death threat was.
As a second-order effect of our problem,
we were trying to solve the problem of don't spread COVID,
and the solution had these massive second- third-order effects that are still playing out, right?
And that was a relatively benign pandemic,
a relatively benign catastrophe compared to a lot of scenarios we can model out.
So we can say, okay, well, we like the benefit of interconnectivity,
so we're not invested in bombing each other,
but we need more anti-fragility in the system.
And then the mutually assured destruction thing doesn't work anymore
because we don't have two countries with one catastrophe weapon
that's really, really hard to make and easy to monitor
because there's not that many places that have uranium,
it's hard to enrich it, you can monitor it by satellites.
We have lots of countries with nukes,
but we also have lots of new catastrophe weapons that are not hard to make,
that are not easy to monitor, that don't even take nation states to make them.
So if you have many, many actors of different kinds
with many different types of catastrophe weapons,
it's a mutually assured destruction.
You can't do it the same way.
And so what we find is that the set of solutions post World War II
that kept us from blowing ourselves up with our new power
lasted for a while, but those set of solutions have ended,
and they have now created their own set of new problems.
So there's kind of the catastrophic risk world before World War II,
the catastrophic risk world from World War II till now,
and then the new thing.
So the new thing says we have to have solutions that deal with the planetary boundary issues,
that deal with global fragility issues,
and that deal with the exponential tech issues,
both in terms of the way exponential tech can be intentionally used to cause harm,
i.e. exponential tech-empowered warfare,
and unintentionally, i.e. exponential tech-empowered externalities,
and even just totally unanticipated types of mistakes,
the Facebook-Google type problem multiplied by AGI and things like that.
And so when we talk about what the catastrophic risk landscape is,
like that's the landscape, the metacrisis is how do we solve all of that,
and recognizing that our problem-solving mechanisms
haven't even been able to solve the problems we've had for the last many years,
let alone prevent these things.
And so the central orienting question,
it's like the UNS-17 Sustainable Development Goals,
there's really one that must supersede them all,
which is develop the capacity for global coordination that can solve global problems.
If you get that one, you get all the other ones.
If you don't get that one, you don't get any of the other ones.
And so we can talk about how do we do that,
but that becomes the central imperative for the world at this time.
So you're saying a whole bunch of things,
and one thing that comes to mind here,
if I'm just reading back some of the things you've shared,
the development of the, let's call it one of the first exponential technologies,
which is the nuclear bomb, led to a new social system,
which was sort of the post-Bretton Woods world
of trying to stabilize that one exponential technology in the world
in a way that would not be catastrophic.
And even there, we weren't able to sort of make it all work.
And I think people should have maybe a list of some of the other exponential technologies,
because I want to make sure that phrase is defined for listeners.
And there's a lot of different ways that we've now not just created more exponential technologies,
but more decentralized exponential technologies.
And I think people should see Facebook and Google as exponential attention mapping
or information driving technologies that are shaping the global information flows
or the wiring diagram of the sort of global societal brain at scales that are exponential.
It's sort of a nuclear scale rewiring of the human civilization.
We couldn't do that with newspapers. We couldn't do that with a printing press,
not at the scale speed, et cetera, that we have now.
So do you want to give maybe some more examples of exponential technologies,
because I think that's going to lead to,
we're going to need a new kinds of social systems to manage this different landscape
of not just one exponential nuclear bomb, but a landscape.
Yeah. Indulge me as I tell a story first that leads into it,
because it'll be a relevant framework.
Obviously, the bomb was central to World War II and the world system that came afterwards
and what motivated our activity getting into it.
But it was not the only tech.
It was one new technology that was part of a suite of new technologies
that could all be developed because of kind of the level science had gotten to.
And basically, like physics and chemistry had gotten to the point
that we could work on a nuclear bomb.
We could start to work on computation.
We could get things like the V2 rocket and rockets
and a whole host of applied chemistry.
And one way of thinking about what World War II was,
it's not the only way of thinking about it, but it's useful frame,
and I think it's a fair frame,
is that there were a few competing social ideologies at the time,
primarily German fascism, fascism, socialism, whatever you want to call it,
Soviet communism and Western liberalism, something like that.
And this new suite of technologies, whoever kind of developed it
and was able to implement it at scale first would win.
That social ideology would win because it's just so much more powerful.
If you have nukes and they have guns, you're going to win, right?
And Germans were actually ahead of both the US and the Soviets
because of some things that they did to invest in education and tech development.
But that led both the Soviets and the US to really working to catch up as fast as they can.
When the US finally figured it out, which we were actually a little bit slow to, right?
Einstein actually wrote a letter, the Einstein-Siller letter that went to the US government
saying now the science really does say that this thing could happen
and the Germans could get it and you should focus on it.
And at first they didn't take them up on it.
It wasn't until the private sector actually non-profit supported,
advanced it further that then the Manhattan Project was engaged in.
But then it was engaged in, when they recognized the seriousness,
that there was an actual eminent existential risk to the nation
and the whole Western ideology and whatever,
then it was an unlimited budget, right?
It was, let's find all the smartest people in the world
and let's bring them here and let's organize however we need to to make this thing happen.
And let's do it for all of the new areas of tech.
We're going to get the Enigma machine and crack the Enigma code.
We're going to get a V2 rocket.
We're going to figure out how to reverse engineer that and advance rocketry.
We're going to do everything needed to make a nuclear bomb and then more advanced ones.
It was the biggest jump in technology ever in the history of the world,
in record history as we know it.
And it wasn't actually done by the market, right?
It was done by the state.
That's a very important thing.
This idea that markets innovate and states don't innovate
is just historically not true here.
This was state funds and state controlled operation
in the same way that the Apollo project coming out of it was.
And a technological jump of that kind hasn't happened since.
So it's an important thing to understand.
But we can say, though this is not a totally fair thing to say,
we can say that the US came out dominant in that technological race.
The US and the USSR both had a lot of capacity, so that was the Cold War.
And then finally the US came out.
And so the post-World War II system was a US-led system, right?
The UN was in the US.
The Bretton Woods system was pegged to the US dollar.
What I would say is that, so it wasn't one type of tech.
It was the recognition that science had got to a place
where there's going to be a whole suite of new tech
and the new tech meant more power.
And whoever had the power would determine the next phase of the world.
And if we didn't like the social ideologies that we're going to be guiding it,
of course we can also think of it as just who wanted to win at the game of power.
But from the philosophical argument, if we didn't like the social ideologies,
then we'd have another social ideology, get it?
What I would say is that there is an emerging suite of technologies now
that is much more powerful in the total level of jump, technological jump,
than the World War II suite was.
In fact, orders of magnitude more.
And only those who are developing and employing exponential tech
will have much of a say in the direction of the future
because just from a real politic point of view, that's where the power is.
And if you don't have the power, you won't be able to oppose it.
And so what do we mean by exponential tech?
There's a couple different ways of thinking about it.
Just exponentially more powerful is a very simplistic way.
And in that definition, nuclear is exponential tech.
But what we typically mean with exponential tech
is tech that makes it possible to make better versions of itself
so that there is like a compounding interest kind of curve.
The tech makes it easier to make a better version,
which makes it easier to make a better version.
And so we see that starting with computation really in a fundamental way
because computation allows us to advance models of computation.
How do we make better computational substrates?
How do we get more transistors in a chip?
How do we make better arrangements of chip so we get GPUs and those types of things?
And so in this new suite of technology,
the center of it is computation.
The very, very center of that is AI, is kind of self-learning computation
on large fields of data.
The other kind of software advances,
like various meaningful advances in cryptography and big data
and the ability to get data from sensors and, you know,
sensor processing, image recognition,
all like that is a part of that central suite.
And the application of that to the directing of attention
and the directing of behavior by directing attention,
which you focused on very centrally.
Then the next phase is the application of the tech,
the application of computation to increasing computational substrate.
So this is now the software advancing the hardware
that can advance the total level of software and you see the recursion.
So that's not just continuously better chips.
It's also quantum computing, photo computing, DNA computing,
those other types of things.
And the other types of hardware that need to be part of that thing,
i.e. sensor tech in particular,
so that you can keep getting more data going into that system
that can do big data machine learning on it.
And it's the application of that computation in NAI,
specifically to physical tech,
so to nanotech, material sciences, biotech,
and even things like modeling how to do better nuclear
and, you know, robotics and automation.
And so when you start thinking about better computational substrates,
running better software with more total data going in
with better sensors and better robots,
you start getting the sense of what that whole suite of things looks like.
So that's the suite of things that I would say
is what we would kind of call exponential tech.
And the reason why the term exponential is important
is we don't think exponentially well.
Our intuitions are bad for it
because we think about how much progress was made over the last five years
and we imagine there will be a similar amount over the next five years
and that's not the way exponential curves work, right?
And so it's very hard for us.
Our intuition was calibrated on the past
and it's going to be miscalibrated for forecasting
the total rate of change and the magnitude of change.
So to link this for one much more narrow aspect for our listeners
who are familiar with social media and social dilemma
and you're talking about sort of self-compounding systems
that improve recursively like that,
if I'm TikTok or if I'm Facebook
and I use data to figure out what's the thing to show you
and that's going to keep you here for long
since it's going to bypass your prefrontal cortex
and go straight to your limbic system, your lizard brain.
Well, the better it gets at doing that and succeeding at that
the more data it has to make a better prediction the next time
but then a new user comes along who it's never seen before
but hey, they're clicking on exactly the same pattern of anorexia videos
that we've seen these other 2 million users have
that turn out to be teenage girls
and it just happens to know that this other set of videos
that are more anorexia videos are also going to work really, really well.
So there's sort of a self-compounding loop
that's learning not just from one person
and getting a better sort of version of hijacking your nervous system
but learning across individuals
and so now you get a new person coming in from developing countries
who's never used TikTok before
and they're just like barely walking in for the front door the very first time.
You know, it's sort of like when Coca-Cola goes to Southeast Asia
for the first time and you get diabetes 10 years later
because you refined all the techniques of marketing so effectively
but now happening at scales that are automated with computation.
So what you're talking about is the impact of computation
and learning on top of learning, data on top of data
and then cross-referencing look-alike models and all of this kind of thing.
You could apply to the domain at least that social dilemma
watchers and people who are familiar with our work might be able to tie into.
Yeah, the more people you have in the system
and the more data per person that you're able to harvest
the more stuff you have for the machine learning to figure out patterns on
which also means that the machine learning can provide things that the users want more
even if it's manufactured want, right, even if it's manufactured demand
which means that then more users will come and put more data in
and it can specifically figure out how to manufacture the types of behavior
that increase data collection.
And so you do get this recursive process on how many people, how much data,
how good are the machine learning algorithms, you know, that kind of thing
and this is one of the reasons that we see these natural monopoly formations
within these categories of tech.
And this is another reason that it's important to understand like
these types of self-reinforcing dynamics and things like network effects
like Metcalf's law didn't exist when the Scottish Enlightenment
was coming up with its ideas of capitalism and market and the healthy competition
and markets and why that creates checks and balances on power.
They didn't exist. Adam Smith did not get to think about those things.
And so when you have a situation where the value of the network is proportional
to the square of the, you know, people coming into the network
then you're incented to keep it free up front, maximize addiction,
drive behavior into the system.
And then once you get to the kind of breakaway point on the return of that thing
it becomes nearly impossible for anyone else to come in and overtake that thing.
So you get a power law distribution in each vertical.
You get one online market that is bigger than all the other online markets,
one video player that's bigger than all the other video players,
one search, one social network one.
And that's not because of a government monopoly.
That's because of this kind of natural tech monopoly.
This also means that when we created the laws around monopolies
they don't apply to this thing.
And yet this thing still has the same spirit of power concentration
and unchecked power that our ideas of monopoly had
but it's able to grow much faster than law is able to figure out how to deal with it
or faster than economic theory can change itself, right?
And so one of the things that we see is that our social technologies like law,
like governance, like economics are actually being obsoleted
by the development of totally new types of behavior and mechanics
that weren't part of the world they were trying to solve problems for, right?
And so the Scottish Enlightenment was the development of new ideas
of how to problem solve, the problems of its time.
The Constitution was trying to figure out how to solve the problems of its time.
I would say they were good thinking, right? They were good work.
The Bretton Woods world was.
None of them are adequate to solve these problems
because these problems are different in kind
and even where they're just an extension of magnitude
when you get enough change in magnitude
sometimes it becomes a difference in kind
like as you're getting more and more information to process
once you get past what humans can process, infosingularity type issues.
Okay, well now it's a difference in magnitude that becomes a difference in kind
which means you need a fundamentally different approach.
So I would say this is where it's important to recognize that those social technologies
that we loved so much because they seemed so much better
than all the other options we had at the time
like markets and like democracy.
These are not terminal goods in and of themselves.
The terminal goods were things like human liberty
and justice and checks and balances on power
and opportunity and distribution of opportunity and things like that.
These were the best social technologies possible at the time.
The new technologies both kill those things.
They don't work anymore, right?
You can't have the social technology of the fourth estate
that was necessary for democracy
which is why founding fathers said things like
if I could have perfect newspapers and a broken government
or perfect government and broken newspapers, I'd take the newspapers
because if you have an educated populace that all understands what's going on
they can make a new form of government.
If you have people that have no idea what's going on
how could they possibly make good choices
if their sense making is totally broken?
So we had this idea that the fourth estate was a prerequisite
to a participatory governance
but that was based on a very narrow limited capacity for print, right?
And again, it was the technology of the Gutenberg press
that was one of the things that actually ended feudalism
and so the founding fathers were employing that new tech
both because it upended the previous tech
and it made this new thing possible, same with guns.
They needed guns and Second Amendment to make this new thing possible.
But once we get to a internet world
where you don't have centralized broadcasts,
you have decentralized and then there's so much stuff
that you can never possibly find at all in search
of whoever coordinates the search, the content aggregators
which is the Facebook, the YouTube, whatever
are doing it with the types of business models we have.
The fourth estate is just dead forever, that old version.
There's no way to recreate that version.
So that either means democracy is dead forever
or anything like a well-informed citizenry
that could participate in its governance in any form
or you have to say what is a post-internet, post-social media,
post-infosingularity fourth estate
creates an adequately educated citizenry.
That's thinking about the way that our social technologies,
our social systems, have to upgrade themselves
in the presence of the tech that
obsoleted the way they did work.
But we can also see and we can give examples of this
how the new tech also makes possible
new things that weren't possible before
so we can do something better than industrial-era democracy
or industrial-era markets.
Which is why I say they aren't a terminal good,
they are a way to deliver certain human values that really matter
and the new technology that obsoletes those
can actually also be
facilitative in designing systems that also serve those values
but it's not a given that it does,
that has to become the kind of central orienting mission.
So Dan, just to make sure we're linking this back
to the start of this conversation,
we started this conversation by saying
the way that we are going about solving problems,
let's say using the legacy systems of lawmaking in a congress
or using the legacy systems of a town hall
to vote on a proposition
or trying to pass laws as fast as social media
as rewiring society,
the lines don't match.
And so what you're saying is that,
and just for listeners,
because I know that you use the phrase social technology
but I think you're really sort of talking about it,
social systems, ways of organizing democracy or...
Technology in the most fundamental sense of the word
of something humans design to facilitate certain kinds of...
activity or outcomes.
So like language is a technology
or democracy is a technology.
Social systems.
Social systems, yeah.
And so if the kind of old world approach of,
some people might be hearing this and say to themselves,
now hold on a second,
so we have all these institutions,
we have all these structures,
we live in a democracy and we live in a system
that is working the way it does,
it has its courts, it has its attorney generals,
it has its litigation procedures,
it has its lawmaking bodies.
If you're saying that we can't use those things
because they're not adequate
or they won't help us solve those problems,
we need to have new social systems,
maybe you could give us some hope about why that might be feasible
instead of feeling impossible
because this is actually precedent in our history.
When new technologies show up
and then new social systems emerge
to make room for those technologies functioning well.
You kind of briefly touched on them,
and to give listeners a few concrete examples.
Yeah, there's a number of good academics
and disciplines of academics
that look at the history of evolutions
and physical technology
and the corresponding evolutions
in thought and culture and social systems.
Marvin Harris, the cultural materialism,
did a kind of major opus work here
where he specifically looked at how changes
in social systems and cultures followed changes in technology.
There are other bodies of work
that will look at the social systems as primary
or the cultures as primary,
and we can say they're inter-affecting.
But for instance,
the vast majority of human history was tribal,
however much, 200,000 years of humans
in these small kind of Dunbar number villages.
There was a social technology, social systems that mediated that
that had to do with how the tribal circles worked
and the nature of how resources were shared.
It was a very different kind of economic system,
a very different kind of judicial system,
a different educational system.
It had all those things.
It had a way of education,
meaning intergenerational knowledge transfer
of the entire knowledge set that was needed
for the tribe to continue operating.
The development of certain technologies,
particularly the plow,
but baskets and a few other things,
all of a sudden it made possible big amounts of surplus
that made reason for much larger populations to emerge.
Those larger populations were going to win in conflict
against the smaller populations.
And so you can see that then the emergence of new social technology
to facilitate large groups of people,
empire types, civilization technology emerged,
you can see.
And that there were a few other shifts in technology
that evolved the types of empires that were there.
And then the next one that people talk about a lot
is the Industrial Revolution,
from the printing press specifically,
and then steam engine, the gunpowder revolution was part of it,
that kind of ended feudalism and began the nation state world.
And so you can see what is the thing
that the founding fathers in the US were doing?
Well, they weren't trying to keep winning at feudalism.
There was a game that had been happening for a long time,
and they were saying, like, no,
we're all people who are of the type of people
who could do well at that system.
And rather than do that,
we recognize that there are fundamentally things wrong with this system
and fundamentally new possibilities that hadn't been previously recognized.
So we're going to actually try to design a fundamentally different system
that we think a more perfect union
that makes life, liberty, and the pursuit of happiness better for everybody
and increases productive capacity and things like that.
And that was fundamentally an advance in social technology
or social systems that both utilized new physical technology
and was enabled by it, right?
In the current situation,
there are groups that are advancing the exponential technologies,
and that means whatever social systems that they're employing
are the social systems of the future if we don't change it.
And that's what I want to get to in a moment,
like, who is working to implement any of the new emerging tech
for better social systems that are aligned with social systems we want?
You've had Audrey Tang on the show.
Do you want to just briefly describe an example of what she and what they have done there?
Because if people aren't aware of it,
that's a pretty prime example of for this particular iteration.
Sure.
And maybe just to go back briefly,
because you gave this example in one of our earlier conversations,
that the printing press could have been used by the feudal lords
for consciously reinforcing feudalism,
but instead this new technology, the printing press,
gives way to new ways of organizing society,
and we can actually have things like a Fourth Estate or newspapers
or things like that.
Both happened.
Both happened, yeah, right.
But then the new thing theoretically has to win out over the old thing,
at least the one that we want that holds the values that the society wants.
So I think a lot of people can hear our conversation.
We've had this riff before actually following our last episode after my Senate testimony,
speaking about a frame that you have offered and know well,
which is that we can notice that digital authoritarian societies right now,
like China, are consciously using exponential technologies
to make stronger, more effective digital closed and authoritarian societies.
And in contrast, digital open societies, democracies like the United States,
are not consciously using technology to make stronger, healthier open societies.
Instead, they've sort of surrendered what they are to private technology,
multinational corporations, pursuing self-interest to shareholders,
and are profiting from the degradation and dysfunction of democracies.
And so when we say all this and we talk about how do we build the kind of next social system
in Audrey Tang and her work,
I think people get tripped up in thinking that what we really mean
is we have to make some kind of 21st century digital democracy.
In fact, I've probably said those words,
but what we're really talking about here is some new concept
that preserves the principles of what we meant by a democracy.
But instantiated with the new technologies,
our version of the new printing press, which is networked information environments,
and all of the new capacities that we have in 21st century
with mobility where everyone's connected to everywhere and everything all at once.
So what is that system, that new social system,
that leverages the current technology and makes a stronger, healthier open society?
And I think Audrey Tang's work, I would probably send listeners back to listen to that episode.
I think it's one of our most listened to and most popular episodes for a reason,
because in Taiwan, she's essentially built an entire civic technology ecosystem
in which people are really participating in the governance of their society.
We need masks, we need better air quality sensors, we need to fix these potholes.
There are processes by which every time you're frustrated by something,
you actually get invited into a civic design process
where whether it's the potholes or the masks,
you can actually participate in having a better system.
You're complaining about the tax system and filing your taxes,
and maybe it's an inefficient form or something like that,
you get brought into a design process of what would make it better.
And so the system is participatory, but not in that kind of 18th century way of,
hey, there's a physical wooden townhouse and we're going to walk into it
and we're going to hang out there for three hours
and we're going to yell and scream about issues that are more local within 10, 15 miles of where we are,
because we were existing in a world before automobiles.
We're now talking about how do you do an open society social system,
but in a world with all of the new technologies that are not just here today, but emerging.
And so do you want to talk a little bit about what the prince,
how we even navigate that challenge and why is some new social system like that necessary
for dealing with these problems that you've sort of laid out at the beginning,
because I'm sure people would like to feel less anxiety about those things hanging around for longer.
Yeah, I think what Taiwan has been doing
and what Audrey Tang in the digital ministry position in particular has been leading
is probably the best example, certainly one of the best examples in the world of this kind of process in thinking.
And does it apply in the, or could it apply in the exact same way to the US?
No, of course not.
We know that because of the relatively small geography and high speed train transportation,
you can get across Taiwan in an hour and a half.
And so when you're mentioning the small scale of local government at the beginning of the US,
where you come to the town hall, in a way they have that, right?
Like it's 23 million people, but there is an older shared culture.
There also happens to be an existential threat just right off their border that is big enough
that they can't just chill and not focus on it.
Everyone has to be civically engaged with some civic identity and like that.
They didn't start making their culture in the industrial era and then have to upgrade it, right?
Like they started later where they were able to start at a higher level of the tech stack.
So there's a number of reasons why it's different.
So we're not going to naively say what you do in a tiny country that is culturally and ethnically homogeneous
and has a higher GDP and education per capita and whatever is the same thing you would do.
But we can certainly take a lot of the examples and say how would they apply differently in different contexts.
So the thing we said earlier that the suite of exponential technologies is so much more powerful
than all of the previous types of power that only those who are developing and deploying them
will be really steering the direction of the future.
And that there are ways of employing them that do cause catastrophic risk.
And the catastrophic risk is of two primary kinds, right?
Conflict theory mediated and you just can't do warfare with this level of technology
and this interconnected world and make it through well.
Not all catastrophic risk means existential.
Doesn't all mean nuclear war and nuclear winter and we've killed all the mammals on Earth.
It might just mean we break global supply chains, kill lots of people and regress humanity
and the quality of the biosphere pretty significantly.
So I'm not just focused on existential risk.
I'm interested in kind of catastrophic risk at scale in general.
You can see that exponential tech applied as in conflict theory and in mistake as externalities
and the cumulative effects.
Could you define conflict theory and mistake theory for people who are not familiar with those terms?
Yeah, there's a very nice discussion on the less wrong forum if people are interested to go deeper.
And it's this question of how much of the problems in the world are the result of conflict theory versus mistake theory.
Meaning conflict theory is we either wanted to cause that problem, that harm to whomever,
as in a knowingly wanted to win at a war.
Or at least we knew we were going to cause that problem and didn't care
because it was attached to something we wanted, right? Conflict theory.
Or mistake theory. We didn't know.
We didn't want to cause it and we really didn't know and it was just unintended, unanticipatable consequence.
And it's fair to say that there's both, right? There's plenty of both.
One thing that is worth knowing is that if I'm trying to do something that is actually motivated by conflict theory,
it benefits me to pretend that it was mistake theory.
Benefits me to pretend that I had no idea and then afterwards say,
oh, it was an unintended unanticipatable consequence. It was too complex. People can't predict stuff like that.
And so the reality of mistake theory ends up being a source of plausible deniability for conflict theory.
But they're both things and we have to overcome both.
Meaning we have to have choice making processes in our new system of coordination.
And like this sounds like maybe hippie stuff until you take seriously the change of context.
Oh, we have to have problems of choice making that consider the whole. That sounds like unrealizable hippie stuff.
Until you realize, but we're making choices that affect the whole at a level that can even individually be catastrophic
and is definitely catastrophic cumulatively.
So if we aren't factoring it, then the human experiment self terminates.
And maybe that's the answer to the great filter hypothesis, right?
And so our, well, yeah.
I think people don't have an intuitive grasp of what it means that each of us are walking around with the power of God's to influence huge enormous consequences.
I mean, I could give a few examples every time you enact with a global supply chain and hit buy on Amazon, you invisibly enacted, you know,
shipping and planes and petroleum and wars in the Middle East.
There's a whole bunch of things that we're sort of tied into when you are posting something on social media and have more than a million followers.
You're influencing a global information ecology.
And if you're angry and biased about one side or the other of the pandemic is real or it's not real or something like that,
you're externalizing more bias into the commons of how people the rest of the world understands things.
So we're walking around with increasing power, but I don't think the increasing power that we've granted is as intuitive for some folks.
Did you explain some more examples of that?
There's both cumulative effect and like cumulative long term and fairly singular short term and cumulative long term.
I mean, you go back to early U.S. settlers coming into the U.S. moving west and they're being buffalo everywhere.
And there had been buffalo everywhere for a very long time and then there's no buffalo in whole areas that were forested with old growth forest became deforested.
And it was like, no, it's impossible. We could never get rid of all the buffalo.
We could never cut down all the trees, but the cumulative effect of lots of people thinking that way were individually.
There's no incentive to leave the buffalo alive and I do have an incentive for my family individually to kill it.
But everybody thinking that way and increasing our desire for how much we consume per capita,
our technology that allows us to consume more per capita and developing more capital, more total people.
Well, then you start getting environmental destruction and species extinction at scale.
And that's a long time ago, right?
Like that's much lower tech and much less people.
It's distributed action. It's a cumulative effect issue.
And obviously we see that with nobody's intending to fill the ocean with microplastics,
but everybody's buying shit that is filling the oceans with microplastics.
And so everyone is participating with systems where the system as a whole is sociopathic.
The system is self-terminating. The system doesn't exist without all the agents interacting with it.
All the agents feel like their behavior is so small that that justifies everybody doing that thing, right?
So that's what we mean by cumulative kind of catastrophic risk.
But it's also true that whoever made that thermite bomb and hooked it to a drone
and hit the Ukrainian munitions factory a couple years ago that caused a billion dollars in damage,
exploded the munitions factory, the effect of a bomb as big as the largest non-nuclear bomb the U.S. arsenal has,
and it's an air bomb. That was a homemade little bomb in a drone, right?
And so, and CRISPR gene drives are cheap and easy.
And it doesn't take that much advanced knowledge to start working with them.
And so that starts to look like individuals and small groups with real catastrophic ability, not long-term and cumulatively.
The increase in our tech gives us both issues.
Via globalization and the overall system, you get these cumulative long-term effects.
And with the exponential attack creating decentralized catastrophic capabilities,
one of the core questions we have to answer is how do we make a world that is antifragile
in the presence of those kind of catastrophic capabilities that are easy to produce and thus decentralizable?
And so, how do we do that?
What are the social systems that we need to employ to bind some of these bad effects
in ways that the natural inclinations of self-interested actors will drive things in that direction?
Just to link this to the social media space for people, if I know that I can get a little bit more attention
and a little bit more likes and clicks and follows and shares and so on,
if I exaggerate the truth by 5%, just to use a little bit more of an extreme adjective,
I know that that in the long run would be bad if everybody did that.
But for me right now, I can win a few hits and I can get more influence
and I'm an Instagram influencer and I'm making $10,000 a month and if I don't do it,
I'm noticing everyone else is doing it and if I don't use the filter, everyone else is using the filter.
And so everyone ends up in this sort of another race to the bottom sort of situation
that has that kind of cumulative degradation or cumulative derangement
where there's increasing distance between what is true and what people believe
because we've all been subtly exaggerating it to make our point and gain influence and so on.
And so, just to give another example maybe for listeners in kind of the space that they're more familiar with.
But going back, I mean, the whole premise of this is as we gain more exponential technologies
that have more capacity and more hands, so instead of having just the U.S. and Russia having this,
you have, whether as you mentioned, CRISPR gene drives or some of the drone things that are out there,
more and more people have access to these things.
How can we bind those kinds of forces and what are the social systems that we need to make that happen?
Yeah, I want to go back as you were describing this.
I was thinking about how many people who listen to your show, who maybe work in technology,
who might have, they work in technology because they see the positive things technology can do
and have more of a kind of techno-optimist point of view
and this overall conversation might sound very techno-pessimist
and like, did we not read Pinker and Watch Hans Rosling and those types of things.
And so I want to speak to that briefly.
First, this is a meta-point, but it's worth saying right now, particularly on this podcast
and in the kind of post-truth, post or fake fact world where then so much of the emphasis has gone
into we need fact-checkers and we need real facts.
Obviously, it's possible to have an epistemic error or even intentional error in the process of generating a fact.
Is there corruption in the institutions and that kind of thing?
But let's even say that wasn't an issue and the things that go through the right epistemic process has facts or facts.
Can you lie with facts? Totally. Can you mislead with facts?
Yeah, because nobody's going to make their choice on one fact.
They make their choice based on a situational assessment, based on a narrative, based on a gestalt of a whole thing.
That's lots of different facts. Well, which facts do I include and which facts do I not include?
And am I decontextualizing the fact?
So the quality of life has gone up so much because we average person lived on less than a dollar a day in the US in 1815
and now they live on this many dollars a day, which inflation adjusted means higher quality of life.
Yeah, but in 1815, most of their needs didn't come through dollars.
They grew their own vegetables, they hunted.
So I'm decontextualizing the facts to compare something that's really apples and oranges.
So even if the fact is quote unquote true, the decontextualization and recontextualization
makes it seem like it means something different than it means.
And the same with the cherry picking of facts.
And I can very easily say, oh, there's a lower percentage of people in extreme poverty,
but I might also be changing the definitions of extreme poverty.
I can also rather than focus on percentage say, well, there's more total people in poverty
than there were total people in the world before the Industrial Revolution.
So there's the ability to decontextualize and recontextualize facts.
There's the ability to cherry pick facts and there's the ability to lake off frame facts
and put particular kinds of sentiment and moral valence on it.
And so am I talking about them as illegal aliens or undocumented workers
and I get a very different kind of sentiment.
So talking about it as a pre-owned car or a used car.
Everyone loves a pre-owned car. No one wants a used car.
And so these very simple semantic frames, contextual frames, cherry picking of the things
means that I can make a narrative where all the facts went through the most rigorous fact-checker
and yet the narrative as a whole is misleading.
And so fact-checking is necessary, but it is not sufficient for a good epistemic and good sense-making.
And not only is it not sufficient, it's even weaponizable.
This is a very important thing to understand because if you are not pursuing that,
if you're not recognizing that, you might be believing nonsense,
thinking that you're using epistemic rigor.
Okay, so the techno pessimists and the techno optimists both cherry pick and they both lake off frame.
And this is true with the difference in almost every political ideology,
the woke and the anti-woke, the pro-socialist, pro-capitalist.
You'll notice that the way they do their arguments, the systemic racism is really, really terrible.
No, there's not that bad, the systemic racism. They both have stats.
But this is actually, you can almost think of it as statistical warfare, as a tool of narrative warfare.
And so this is where a higher level of earnestness rather than a particular vested interest or bias,
a higher willingness to look at bias as a higher level of rigor ends up being critical to actually overcome any of these things.
So can I cherry pick stats that make it look like everything's getting better? Totally.
Those things are true. And nobody wants to go back to a world before Novacain when you have to do dentistry.
And nobody wants to go back to a world before penicillin when basic bacterial infections go around.
And there's totally good stuff that has emerged.
And are there all kinds of ubiquitous mental illnesses and chronic complex disease that didn't exist before
and increase in the total number of addictive type behaviors within populations?
And a radical increase in the catastrophic risk landscape and negative effect to environmental metrics.
So things are getting better and things are getting worse at the same time.
It's important to understand that depending upon what you pick.
It's just that the things that are getting worse are heading towards tipping points that make the whole thing no longer viable.
And so that we're not denying that there are things that are getting better.
We're saying that for the game to continue at all, right, to have it be an infinite game that gets to keep continuing,
there are certain things that have to not happen.
And you can't have the things that are getting worse keep getting worse at the curve that they are
and have the things that are getting better be able to continue at all.
I just want to say that.
So naive techno-optimism can actually make you a part of the problem
because then you do things like develop a solution to a narrowly defined problem
and externalize harm to other areas because you weren't taking seriously enough not doing that.
But techno-pessimism also makes you a part of the problem or at least not a part of the solution
because the future is not going to be determined by Luddites.
It's not going to be determined by people who aren't developing the tools of power.
So if you aren't actually looking at how do we develop a high-tech world
that is also a fundamentally desirable in terms of a high-nature and high-touch world,
then you really aren't thinking about it in a way that ends up mattering.
And so we are techno-optimists but not naive techno-optimists.
We go through the totally cynical phase of, man, tech is a serious issue,
and then you go to a post-cynical phase of,
if I want to be techno-optimist and not be silly,
what does it take to imagine a world where humans have that much power and we are good stewards of it?
Meaning that we actually tend to each other well and we don't create a dystopic world
that has exponential wealth inequality and an underclass that nobody in the upper class would want to trade places with
and that doesn't cause catastrophic risk.
Right now the amount of power of exponential tech makes two attractors most likely.
Catastrophic risk of some kind.
Or social systems that do not preserve the values that we care most about,
that are the ones that are currently most working to develop and deploy that technology,
and to just give a very brief recap of the frame that Tristan you gave on it earlier.
As you mentioned, China's not leaving 100% of its technology development to the market to develop,
however it wants, even if it harms the nation-state.
They are happy to bind technology companies that are getting too large and in ways that would damage the nation-state,
as we saw with Ant Corporation,
and they are doing a lot of very centralized innovation as well associated with long-term planning.
Long-term planning is a key thing.
In the US, term limits make long-term planning very hard,
as does a highly rival-risk two-party system that is willing to damage the nation as a whole to drive party wins.
So in that system, almost all the energy just goes into trying to win.
You spend at least a couple years, but even the years before that are fundraising, creating alliances to just try to win.
Then you're not going to invest in anything heavily that has return times longer than four years because it won't get you re-elected.
So no real long-term planning.
And then whatever you do do in those four years will get undone systematically in the next four years for the most part.
Alright, that system of governance will just fail comprehensively in relationship to a system that doesn't have that much internal infighting
and that has the capacity to do long-term planning.
And there's a million examples we can look at, but just when did high-speed trains start?
They started, you know, we saw them emerge in Europe, we saw them emerge in Japan and in China.
We've seen China now start to export them all around the world and the US still doesn't have any high-speed trains.
And it's like, what happened? Why?
And we can see that the US innovated in fundamental tech in the Manhattan Project, kind of through the Apollo Project,
but then it started to privatize almost everything to the market.
The market started to develop in ways that really were not advancing the technology in a way that increased the coherence of the nation
and the fundamental civic values and ideas of the nation.
Even the World War II thing, we can see we increased our military capacities radically,
but that didn't mean we actually really advanced the ideas of democracy or those values of,
do we make a better system to educate the people and inform them and help them participate in their governance?
Do we make better governance?
This is why the US military is so powerful, but the US government is so kind of inept,
which is why nobody wants to fight a war with the US, a kinetic war,
but it's very easy right now to engage in supporting narrative warfare
where you turn the left and the right against each other increasingly,
and where you do long-term planning, where the US can't do long-term planning of those kinds.
So we can see that the government of the US, and not just the US,
but we can see that open societies are not innovating in how to be better open societies,
for the most part, more effective ones, where they're using the new tech to make better open societies.
That's happening in the market sector.
The market is making exponentially more powerful companies.
A company is not a democracy.
It's not a participatory governance structure in general.
It's a kind of very top-down autocratic type system.
And so we see that there's more authoritarian nation-states
that are intentionally doing long-term planning of the development and deployment of exponential tech
to make better nation-states of that kind.
And we can't even blame them when they look at...
I mean, China had the benefit of getting to see both where the USA failed and where the USSR failed
and try to make something they didn't fail in either of those ways.
And there's some things that are very smart about those approaches.
So we see, though, exponentially empowered, more autocratic type structures
and the emergence of one natural monopoly per tech sector
and then the interaction of those that kind of becomes like oligarchic feudalism, tech feudalism.
Neither of those have the types of jurisprudence or public accountability
or whatever that we're really interested in.
So the two attractors right now is the emergence of social systems
that are deploying the exponential tech
that will probably not preserve the social values that we're interested in
and not be maximally desirable civilizations.
Probably pretty dystopic ones.
Or not even guiding it well enough to prevent catastrophic risk and catastrophic risk.
Those are the two major types of attractors.
We want a new attractor, which is how do we utilize the new exponential technologies,
the whole suite of them, to build new systems of collective intelligence,
new better systems of social technology?
How do you make a fourth estate that can really adequately educate everyone in a post-Facebook world?
Well, the same way that we're trying to optimize control patterns of human behavior
for market purposes to get them to buy certain things and to direct their attention,
could that be used educationally?
Of course it could if it was being developed for that purpose.
And the AI tech that can take a bunch of faces and make a new face that is merged out of those,
could it take semantic fields of people's propositions and values
and create a proposition that is kind of the semantic center of the space
and then could we use, we can't all fit into a town hall,
but can we engage in digital spaces where we can have better processes of proposing refinements to the propositions?
Of course we can.
Could we use blockchain and other types of uncorruptible ledgers to solve corruption,
which is something that universally everybody thinks is a good idea,
should all government money be on a blockchain?
The movement of it, so you have provenance so you can see where the money is actually going.
And if someone wants to be a private contractor,
they have to agree that the accounting system, if they want government money, goes on the blockchain.
So we can see the entire provenance of the taxpayer money.
You can't have representation if there isn't transparency of how it happens.
So there's a whole bunch of, when you start to think about attention directing technology
and what its pedagogical applications could be,
when you start to think about AI and how it could actually help proposition development
and parsing huge amounts of information to make a better epistemic commons.
When you start to think about blockchain and could we actually resolve corruption using uncorruptible ledgers
and making the provenance of physical supply chains and information and money all flow across those,
totally new possibilities start to emerge that never emerged before, that were never possible before.
But if it doesn't become our central design imperative to develop those,
those are not the highest marketed opportunities for those right now.
The highest market opportunity for blockchain is speculative tokens that have no real utility.
And for AI is things that actually drive ads and purchasing and, you know, on and on.
And for attention tech, it is the same thing.
So you've sold me on the idea that we have two dystopian attractors that we don't want.
And the third attractor that we're trying to develop here is some kind of open society
that is consciously using all the modern technologies towards the values that we care about.
Can you give some concrete examples of what it would look like to use, you know,
AI and attention driving tech and click driving tech and block chains and all these things,
but in a way that would make a stronger, healthier, open society?
Yeah, totally.
So let's say we take the attention tech that you've looked at so much
that when it is applied for a commercial application is seeking to gather data
to both maximize time on site and maximize engagement with certain kinds of ads and whatever.
That's obviously the ability to direct human behavior and direct human feeling and thought.
In a way that is both emerged out of capitalism and has become
almost a new macroeconomic structure more powerful than capitalism
because even more powerful than being able to incent people's behavior with money
is being able to direct what they think and feel
to where the thing that they think of as their own intrinsic motive has actually been influenced or captured.
So if we wanted to apply that type of technology
and we figured out how to make the kind of transparency that made institutions
that were trustworthy enough that we could trust them with this
and already we have institutions that have it that we have no basis to trust with it,
could that same tech be used educationally to be able to personalize education
in the learning style of a kid or to an adult to their particular areas of interest
and to be able to not use the ability to control them for game theoretic purposes
but use the ability to influence them to even help them learn
what makes their own center, their locus of action more internalized, right?
We could teach people with that kind of tech how to notice their own bias,
how to notice their own emotional behaviors, how to notice group think type dynamics,
how to understand propaganda and media literacy.
So could we actually use those tools to increase people's immune system
against bad actors' use of those tools? Totally.
Could we use them pedagogically in general to be able to identify,
rather than manufacturing desires in people
appealing to the lowest angels of their nature because addiction is profitable,
can you appeal to the highest angels in people's nature but that are aligned with intrinsic incentives
and be able to create customized educational programs that are based on
what each person is actually innately intrinsically motivated by
but that are their higher innate motivators.
Everybody can have a reward circuit that is based on, you know, chocolate cake and sloth
but the immediate spike that comes from the chocolate cake
ends up then having a crash and increased weight and inflammation
and whatever where the baseline of their happiness goes down over time.
Even though every time they eat the chocolate cake they get a spike.
The exercise reward circuit is maybe not that fun,
maybe even kind of painful and dreadful in the moment
but then creates a higher baseline of energy and capacity and endurance and self-esteem
and you start to actually have the process become more fun.
You get a new reward circuit and the baseline goes up.
So, of course, I can appeal to the lower reward circuit and say,
hey, I'm just giving people what they want.
Yeah, but if you have a billion dollar or a trillion dollar organization
that is preying upon them and you discuss this very well all the time,
the vulnerabilities that make people's life worse to then have the plausible deniability to say,
yeah, but they wanted it.
Yeah, but it was a manufactured demand and a vulnerability.
Where's the no bless oblige?
Where's the obligation of having that much power to actually be a good steward of power,
a steward of that for other people,
where if there are reward circuits that decrease the quality of their life,
reward circuits that increase it that we're trying to appeal to one rather than the other.
Could we do that? Yeah, totally we could.
Could we have an education system as a result
that was identifying innate aptitudes, innate interests of everyone
and facilitating their development so not only did they become good at something,
but they became increasingly more intrinsically motivated, fascinated, and passionate by life,
which also meant continuously better at the thing.
Well, in a world of increasing technological automation coming up,
both robotic and AI automation, where so many of the jobs are about to be obsolete,
our economy and our education system have to radically change
to deal with that because one of the core things an economy has been trying to do forever
was deal with the need that a society had for a labor force.
And there were these jobs that society needed to get done that nobody would really want to do.
So either the state has to force them to do it,
or you have to make it to where the people also need the jobs,
so there's a cemetery and so kind of the market forces them to do it.
Well, when you technologically automate those jobs,
and it happens to be that the things that are the most rote
are the least fun for people and the easiest to program machines to do.
And so if you keep the same economy where if people don't produce,
they don't have any basic needs met, then people want those crappy jobs, right?
But if you make it to where they have other opportunities,
then of course having those jobs be automated is fine.
But what does it mean to really be able to have other better opportunities?
So if one of the fundamental like axioms of all of our economic theories
is that we need to figure out how to incent a labor force to do things that nobody wants to do,
an emerging technological automation starts to debase that.
That means we have to rethink economics from scratch
because we don't have to do that thing anymore.
So maybe if now the jobs don't need the people,
can we remake a new economic system where the people don't need the jobs?
Can we start to create commonwealth resources that everyone has access to,
where people's access isn't based on possession that automatically limits everyone else's access?
If you get around transportation-wise with a car based on owning that car,
where the vast majority of the life of the car is just sitting, not being used,
for you to have access to the car, you have to have possession of it,
which means that it's a mostly underutilized asset.
I don't have access to the thing that you possess.
Now what we see with Uber of course is a situation where your access
is not mediated by your possession.
So now turn that into electric self-driving cars
and now make the entire thing on a blockchain so you disintermediate even the central business,
make it a commonwealth resource and everyone has access to transportation as a commonwealth resource.
It'll take a 20th of the number of cars to meet the same level of convenience during peak demand time,
so much less environmental harm.
It'll actually be more convenient because I don't have to be engaged in driving the thing
and there's less traffic because of the coordination and better maintenance
and there isn't a desire or an incentive for design and obsolescence in that system.
You can see a situation where, okay, can we make it to where the wealth-augmenting capacity
of that technologic automation goes back into a commonwealth
because we don't have to have the same axioms of needing to incent the people.
Oh yeah, but if you don't incent the people, there'll all be lazy welfare people.
Nonsense.
Einstein didn't do what he did based on economic incentive
and neither did Mozart and neither did Gandhi and none of the people that we are most inspired by through history
were doing that and what kids will spend so much time doing
where they ask questions about why this, why this, why this and building forts and whatever is intrinsic motive.
It's just we don't facilitate the things that they're interested in.
We try to force them to be interested in things they aren't interested in.
That's what ends up breaking their interest in life
and then they just want a hypernormal stimuli and play video games, whatever.
What if you had a system that was facilitating their interest the entire time?
Now you have a situation where you can start to decrease the total amount of extrinsic incentive in the system as a whole.
Use the technology, the automation to decrease the need for extrinsic incentive
and make it an educational system and culture that's about optimizing intrinsic incentive
because if my needs are already met, getting stuff, there's no...
and everybody's needs are met through access to commonwealth resources.
There's no real status conferred.
There's only status conferred by what I create.
So now there is a... any status is bound to a kind of creative imperative.
That's an example.
We can look at blockchain tech even more near term and say...
but just to come back to this technological automation thing.
So obviously it makes possible changing economics and changing education
but also what is the role of humans in a post AI robotic automation world?
Because that is coming very, very soon.
And what is the future of education where you don't have to prepare people to be things that you can just program computers to be?
Well, the role of education has to be based on what is the role of people in that world.
That is such a deep redesign of civilization
because the tech is changing the possibility set that deeply.
So at the heart of this are kind of deep existential questions of what is a meaningful human life
and then what is a good civilization that increases the possibility space of that for everybody
and how do we design that thing?
We come back to blockchain and we say, well, blockchain is an uncorruptible ledger.
Well, one thing that the left and right and everybody agrees on is that corruption happens
and it's bad for the society as a whole and we don't like it.
We just disagree on who does it.
Is it possible that that tech could make possible decreasing corruption as a whole?
It actually decreases the possibility set for corruption.
Yeah, in order to do corruption, I have to be able to hide that I did it, right?
I either have to break enforcement or break accounting and mostly it's break accounting.
And so what if all government spending was on a blockchain
and doesn't have to be a blockchain, it has to be an uncorruptible ledger of some kind.
Hall of Chain is a good example that is pioneering another way of doing it,
but uncorruptible ledger of some kind where you actually see where all taxpayer money goes
and you see how it was utilized, the entire thing can have independent auditing agencies
and the public can transparently be engaged in the auditing of it.
And if the government is going to privately contract a corporation,
the corporation agrees that if they want that government money,
the blockchain accounting has to extend into the corporation,
so there can't be very, very bloated corruption.
Everybody got to see that when Elon made SpaceX, all of a sudden,
he was making rockets for like 100s to 1000s for the price that Lockheed or Boeing were
who had just had these almost monopolistic government contracts for a long time.
Well, if the taxpayer money is going to the government,
is going to an external private contractor who's making the things for 100 to 1000 times more than it costs,
we get this false dichotomy sold to us that either we have to pay more taxes to have better national security,
or if we want to cut taxes, we're going to have less national security.
What about just having less gruesome bloat because you have better accounting
and we make the rockets for 100th of the price and we have better national security
and better social services and less taxes?
Well, everyone would vote for that, right? Who wouldn't vote for that thing?
Well, that wasn't possible before Uncorruptible Ledgers.
Now, that Uncorruptible Ledger also means you can have provenance on supply chains
to make the supply chains closed loop so that you can see that all the new stuff is being made from old stuff
and you can see where all the pollution is going and you can see who did it,
which means you can now internalize the externalities rigorously
and nobody can destroy those emails or burn those files, right?
What if the changes in law and the decision-making processes also followed a blockchain process
where there was a provenance on the input of information?
Well, that would also be a very meaningful thing to be able to follow.
So this is an example of like, can we actually structurally remove the capacity for corruption
by technology that makes corruption much, much, much harder,
that forces types of transparency on auditability?
What if also you're able to record history?
You're able to record the events that are occurring in a blockchain that's Uncorruptible
where you can't change history later.
So you actually get the possibility of real justice and real history
and multiple different simultaneous timelines that are happening.
That's humongous in terms of what it does.
What if you can have an open data platform and an open science platform
where someone doesn't get to cherry-pick which data they include in their peer-reviewed paper later,
we get to see all of the data that was happening.
We solve the oracle issues that are associated,
and then if we find out that a particular piece of science was wrong later,
we can see downstream everything that used that output as an input
and automatically flag what things need to change.
That's so powerful.
Like, the least interesting example of blockchain is currency creation.
These are actual, like,
the capacity for the right types of accounting means the right type of choice-making, right?
Let's take AI.
With AI, we can make super terrible deep fakes
and destroy the epistemic commons using that and other things like that.
But we can see the way that the AI makes the deep fake
by being able to take enough different images of the person's face
and movements that it can generate new ones.
We can see where it can generate totally new faces,
averaging faces together.
Somebody sent me some new work that they were just doing on this the other day.
I found it very interesting.
They said, we're going to take a very similar type of tech
and apply it to semantic fields
where we can take everybody's sentiment on a topic
and actually generate a proposition that is at the semantic center
or take everybody's sentiment and abstract from it
the values that they care about and create values taxonomies
and say, we should come up with a proposition that meets all these values.
Then can you have digital processes where you can't fit everybody into a town hall
but everybody who wants to can participate in a digital space
that rather than vote yes or no on a proposition that was made by a special interest group
where we didn't have a say in the proposition or even the values it was seeking to serve,
so it was made in a very narrow way that, like we've mentioned earlier,
benefits one thing and harms something else,
which is why almost every proposition gets about half of the vote
and inherently polarizes the population.
Well, people are so dumb and so rival risk the process of voting
with bad propositions and bad representation processes
inherently polarizing and downgrading to people.
So what if there's a process by which there's a decision that wants to be made?
You start by identifying what are the values everybody cares about
and then we say the first proposition that meets all these values well
becomes the thing that we vote on.
And then instead of just a direct vote, do we engage types of qualified and liquid democracy together
where you have to show that you understand the basics of that topic to be able to vote on it,
but the education is free and you can keep retesting
and the basics don't show leaning one way or the other,
just shows you understand the stated pros and cons
so that massive populism doesn't happen.
But if you don't want to come to understand it,
you can cede your vote to someone else who has passed that thing.
These are that type of liquid democracy,
that type of qualified educated democracy where it doesn't have to be educated across everything.
It can be per issue and where you're not just voting on a thing,
you're helping craft the propositions.
These completely change the possibility space of social technology
and we can go on and on in terms of examples,
but these are ways that the same type of new emergent physical tech
can destroy the epistemic commons
and create autocracies and create catastrophic risks
could also be used to realize a much more pro-topic world.
So I love so many of those examples
and I especially on the blockchain and corruption one,
because I think as you said something that the left and the right can both agree on
is that our systems are not really functional
and there's definitely corruption and defection going on.
And just to add to your example,
the notion of citizens could even earn money
by spotting inefficiencies or corruption in that transparent ledger
so that we actually have a system that is actually profiting
by getting more and more efficient over time
and actually better serving the needs of the people
and having less and less corruption
and so there's actually more trust and faith
and that's actually a kind of digital society
that when you look at let's say the closed China's digital authoritarian society
and you look at this open one that's actually operating more for the people
with more transparency, with more efficiencies,
to get more space sex Elon Musk type cheap ways of sending rockets to the moon
and becoming a multi-planetary civilization
as opposed to more bloat and more mega monopolies,
defense contractors that are not taking us to where we need to go.
That's just an inspiring vision
and I just hope people listen to what you shared
and kind of go back because there's a lot of different aspects there.
I think the question on many people's mind right now
is going to be how do we get from where we are
to the world that you're talking about?
What are the steps that are in between?
Obviously, I don't know.
Nobody knows.
There's going to like which projects emerge
and first and start really making success
that there's a lot of different possible paths.
I can say some of the things that could happen
and some of the things that I think need to happen.
We take all the catastrophic risks that exponential tech makes possible
and the dystopic attractors and we say,
okay, so we need to solve all those problems,
but we're not doing really good at solving those problems right now.
Our problem-solving processes need upgraded
and that means new institutions.
When we say institution, we usually think of a pretty centralized thing
and with things like decentralized governance emerging,
the institution might be a decentralized one,
but individual people aren't going to solve all of that.
It's new institutions, centralized and decentralized,
that have the right capacities to solve these types of problems need to come about.
Who develops those institutions and who empowers them?
This is where the democratic idea of the power of government
coming from the consent of the governed
is one of the key ideas to what we would think of as the values of an open society.
Let's say that there's a small number of people who think,
we understand these problems, we understand the solutions that must happen,
everybody else doesn't get it, so we're going to make this thing happen
and because we have the power, we can just kind of implement it by force.
That becomes its own dystopia.
Implement it by force might be, well, the people think they need to be free,
so we'll implement it by attention hijacking them so that they participate with it
or don't even realize that it's happening and they just keep doing whatever's next.
The cultural element, why we talk about the need for a new cultural enlightenment,
is of course when we look at the founding of the US,
we can see all that was super wrong with it.
Just to mention, when Churchill said democracy was the worst form of government ever,
save for all the other forms,
when Socrates talked about in the Republic when Plato was discussing it,
why democracy was a dreadful idea, the arguments are good arguments, right?
Like, do you want people who understand seafaring to man the boat
or just a general population who knows nothing about it to man the boat?
Well, that's not a very good idea.
Do you want the general population that knows nothing about it to build the NASA rocket
or do you want people that know what they're doing?
Well, why would we think people who have no idea what they're doing
are going to be good at figuring out how a civilization should be run?
What should our nuclear first strike policy should be?
How should we deal with the stability of the energy grid against Carrington events?
And so, what does it take to have a population educated enough?
And yet, then if we say okay,
but then the other problem is if we say the people are too uneducated
and maybe too irrational and rivalrous to be able to hold that power
so it needs to be held by some, how do we ensure non-corruption
and who is a trustworthy authority to be able to hold that power
and not have vested interests mess it up?
And so, this is why I think it was a Jefferson quote
of the ultimate depository of the power must be the people.
And if we think the people too uneducated,
not enlightened to be able to hold that power,
we must do everything we can to seek to educate and enlighten them,
not think that there is any other safe depository.
And so, even with that, we take the US formation
and you've got some founders who had read most of the books of the time, right?
Read most of the books of philosophy, knew the history of the Magna Carta
and the Treaty of the Forest and all these kinds of things,
thought and talked deeply, spent many years,
were willing to die fighting a revolutionary war,
were not going along with winning at the current system
but really trying to do a fundamentally different thing to develop a new system.
Not everybody who was participating in the US was doing that thing.
They weren't all doing systems architecture, right?
But they were all basically saying,
we agree to this kind of systems architecture
and we want to learn how to participate with it adequately.
We'll read a newspaper, we will do a jury duty,
we'll come to the town hall, that kind of thing.
So, in Taiwan's example, I think their population is 23 million people
and their online citizen engagement platform has something like 5 million people engaging.
That's pretty awesome, right? That's not everybody
and no one should be forced to be engaging.
And one of the critical things when we think deeper about
is it a democracy, is it a republic, is it a epistocracy, is it a...
We want to think about the values, not the previous frames for them
and the values exist in dialectics and we need to be able to hold those together.
Of course we want individual liberty but we don't want individual liberty
that gets to harm other people and other things
so we want also law, justice, collective integrity.
How do you relate those things?
One of the core things is the relationship between rights and responsibilities.
So, if I have rights and I don't have responsibilities,
there ends up being tyranny and entitlement.
And we can see that that's kind of rampant, the entitlement thing.
If I have responsibilities and I don't have any attendant rights at servitude,
neither of those involve a healthy, just society.
So, if I want the right to drive a car,
the responsibility to do the driver's education
and actually learn how to drive a car safely is important
and we can see that some countries have less car accidents
than others associated with better driver's education.
And so, increasing the responsibility is a good thing.
We can see that some countries have way less gun violence than others,
even factoring a similar per capita amount of guns
based on more training associated with guns and mental health and things like that.
So, if I have a right to bear arms,
do I also have a responsibility to be part of a well-organized militia,
train with them and be willing to actually sacrifice myself to protect the whole
or sign up for a thing to do that?
Do I have to be a reservist of some kind?
Or the right responsibility of caring?
If I want the right to vote, is there a responsibility to be educated about the issue?
Yes. Yes.
Now, does that make it very unequal?
No, because the capacity to get educated
has to be something that the society invests in making possible for everyone.
And of course, we would all be silly to not be dubious
factoring the previous history of these things,
but this is what we then have to insist upon,
because do we want people who really don't understand the issues
but think they do voting?
No, that's a dreadful system.
But do we want people who know something to have no avenue or who care?
Do we want people who know something to have no avenue to input that into the system
or people who care to have no opportunity to learn?
No, that's also dreadful.
So, how do we make the on-ramps to learning available for everyone?
Not enforced, but we're actually incentivizing.
Can we use those same kind of social media behavior-incentive technologies
to increase everyone's desire for more rights and attendant responsibilities
so that there's actually a gradient of civic virtue and civic engagement?
Yeah, we could totally do that.
So, this is where the cultural enlightenment layer is.
Of course, not everyone is going to be working on how do we develop AI
and blockchain for these purposes,
but they can certainly be saying,
I am going to make sure that my representatives are talking about these issues.
I want all the presidential candidates to be talking about these issues.
I'm going to pay attention to and support candidates who really do in earnest ways.
I'm going to invest in companies that are doing those things.
I'm going to divest from companies that are doing the other things.
There is a cultural enlightenment that is needed to be able to create the demand
and the support for where those projects that are earnestly working on
and have the capability start to emerge.
So, you've painted a compelling vision of some of the ways
that an open society could consciously employ some of these technologies
to revisit and re-fulfill some of the original values for which they were intended.
How does this work with the existing institutions that we have?
How much is this going to rely on transforming the existing digital Leviathans into something new?
How much is it going to depend on blockchain projects?
How much is it going to depend on existing institutions?
Will it be the Brookings Institution or the New York Times?
Can you speak to the role of new and future institutions in making this transition possible?
It's interesting.
When we look at institutions that emerge to try to solve some social or environmental problems,
or nonprofits in particular and some government branches that are associated with that,
there's this kind of structural perverse incentive that if I am an organization,
which means I'm people in an organization,
that have job security and some actual power and access and whatever because of this position,
and my job is to solve a problem.
If I fully solve the problem, I would obsolete my job and obsolete myself.
So then there's this kind of perverse incentive to continue managing the problem,
continue manufacturing the narrative that we're needed to manage the problem,
continue manufacturing the narrative that the problem is really hard and is hard to solve
and so we've got to keep doing this thing.
And so one of the fundamental dispositions of systems is that they want to keep existing.
And yet they might no longer be fit for purpose, they might even be antithetical to the purpose.
We have to be very careful about this.
With regard to the new institutions we need, to what degree could existing institutions reform themselves,
to what degree does it need to be new ones, it's kind of up to them.
Like it's kind of up to the depth of realization of the need and the sincerity and then the coordination capacity
of people in current institutions, how much role they could play.
We can see the way that going into World War II, coming out of the Depression,
the US upregulated its coordination capacity so profoundly.
So could we have a Manhattan Project like level organization?
By organization I mean the capacity to organize, not a singular thing.
That was oriented to how do we instantiate the next model of civilization?
How do we instantiate the next model of social systems and social technologies?
What is the future of education? What's the future of economics?
What's the future of the fourth estate of law, et cetera, that fulfill the values that are meaningful
and are antifragile in the presence of the current technologies
and that can actually compete with the other applications of those technologies
towards things that serve different values and or aren't antifragile?
I would love to see the US make that a central imperative Manhattan Project level to be able to do that.
Not just how do we create a more powerful military, but how do we create a more powerful, a healthier,
fundamentally a healthier society that up regulates and engages collective intelligence
in its own problem solving and innovation better.
I would like to see lots of countries do that.
I think there are countries that did not yet transition to democracy, are interested in it
and could completely bypass the industrial era democracies and go directly to better systems.
I think networks of small countries, you see what Taiwan is doing.
Estonia is trying to do some interesting things.
I think networks of small countries could start sharing best practices and sharing resources
so they don't all have to develop the stuff from scratch,
which could start to lead to coalitions of countries like the EU saying let's do some fundamentally better things.
I think it will happen also not at the level of nation states where like decentralized groups,
blockchain type groups say all right, let's really earnestly take on what these primary problems are
and work on developing these solutions and these capacities.
For the tech companies to do so would be very hard because while it could be still profitable long term,
it would not be profit maximizing short term relative to the current thing they're doing.
As we said, winning at the current game and building a new game are different things
and winning at a current game that self-terminating is a very short-sighted thing to want to keep doing.
So if Facebook or Google or whatever were to cut its ad model,
it would have a hard time being able to meet its fiduciary responsibility to shareholders a different way
but could it in conjunction with a participatory government regulatory process
that wanted to help change its fiduciary responsibility where it became more of a social utility
start to actually redirect its technology and redirect its decision-making process.
Yeah, it could. That would be super interesting.
So I would like to see that as we mentioned earlier, I'd like to see the UN recognize
that the level of progress that it has made at the sustainable development goals,
nuclear deproliferation, and other types of international things like economic equality,
globally writ large, and preventing arms races and tragedy of the commons that,
well, it hasn't done nothing. What it's doing is not converging.
It's not adequate. It's not converging on eventually solving the problem set.
It needs not just more of that approach. It needs a different approach.
And so to say, okay, well, clearly we don't know how to facilitate coordination of global problems well enough.
So let's have the superseding focus be innovation towards better methods of global coordination.
That becomes our new number one goal because we know we only get all the other goals if we get that.
And you can see that during World War II when we had to crack the enigma machine and figure out computation and whatever,
we got touring, we got von Neumann, we got all of the smartest people from countries all around the world
engaged in solving those problems. I would like to see the US, the UN, I would like to see other countries,
and I'd like to see private sector taking seriously the actual problemscape we have
and innovating not for just short-term advantage or narrow in-group advantage,
but for long-term advantage of the whole.
How do we, since we have global effect, how do we build global coordination adequate to what is needed?
To me, that has to become the central zeitgeist and whatever groups figure out how to do it effectively
will be the groups that can direct the future.
And I know that this is the work that you are working towards with the Consilience Project.
Do you want to talk just about how you are working towards that with your work and how we're collaborating?
Yeah, I mean, we're at the very, very beginning of the Consilience Project.
It has a site up that is not even a beta yet just because we, in just starting, wanted to work on building stuff in association with thinking.
But this talk is very central, this conversation you and I are having is very central to the aims of the Consilience Project,
which is we're wanting to inspire, inform, and help direct a innovation zeitgeist,
where the many different problems of the world start to get seen in terms of having interconnectivity and underlying drivers.
And that the forcing function of the power of exponential tech is taken seriously that says in order to become good stewards of that
requires evolutions of both our social systems and our culture.
The wisdom to be able to guide that power, a recoupling of wisdom and power in that adequate to what is needed.
So how do we innovate in culture, the development of people, and how do we innovate in the social systems, the advancement of our coordination,
both employing the exponential tech and being able to rightly guide it.
And so we have a really great team of people that are doing research and writing,
basically the types of things we're talking about here in more depth explaining what is the role of the various social systems,
like what is the role of education to any society, help understand fundamentally what that is,
understand why there is a particularly higher educational threshold for open societies,
where people need to participate not just in the market but in governance,
understand how that has been disrupted by the emerging tech and will be disrupted further by things like technological automation,
and then envision what is the future of education adequate to an open society in a world that has the technology that's emerging.
And we don't necessarily know what the answer is, but we know examples and we know criteria.
So then it's like innovate in this area and make sure you factor these criteria.
And the same thing with the Fourth Estate, the same thing with law, the same thing with economics.
And so the goal is not how do we take some small group of people to build the future,
it's how do we help get what the criteria of a viable future must be.
And if people disagree, awesome, publicly disagree and have the conversation now,
but if we get to put out those design constraints, someone says no, we think it's other ones,
at least now the center of culture starts to be thinking about the most pressing issues in fundamental ways
and how to think about them appropriately and how to approach them appropriately.
So fundamentally our goal is supporting an increased cultural understanding of the nature of the problems that we face,
a clearer understanding rather than just there's lots of problems and it's overwhelming and it's a bummer.
And so either some very narrow action on some very narrow part of it makes sense,
which is most of activism or just nihilism.
We want to be able to say actually because there are underlying drivers, there is actually a possibility to resolve these things.
It does require the fullness of our capacity applied to it and with the fullness of our capacity.
So it's not a given, but with the fullness of our capacity applied to it, there is actually a path forward.
And so we're writing these papers that basically would be kind of like a metacurriculum for people who want to be engaged in designing the future.
And some of them have to do with current public culture and how to be able to change patterns of public culture
that lead to better conversation, better sense making, better meaning making and choice making
so that there's an on ramp into higher quality conversations, meaning higher quality process of conversation.
And then some of them are things like what are the design criteria of the future social systems and how could we build those things.
Then not everybody will read those.
Some people who have the ability to help start building them will.
But we hope that other people will take that and translate it on podcasts and into animations and in whatever other forms of media
so that those topics start to become increasingly present in people's awareness.
Then of course the next part is what groups start emerging wanting to address those
and what can we do to help facilitate good solutions in those groups.
And this is where you and I, I've learned a lot from you about the social media issues in particular
and how central they are to the breakdown of sense making because obviously without good shared sense making
there is no possibility for emergent order.
You either just get chaos or you have to have imposed order.
If you want emergent order that means emergent good choice making, that means emergent good sense making.
And so we've learned a lot and discussed these things for a long time and obviously also not just you and I,
there's a whole network of people that we're connected to that have been thinking deeply about these things
and that we continue to try to think about what adequate solutions could look like.
And I think what CHT did with the social dilemma took one really critical part of this metacrisis into popular attention
maybe in a more powerful way than I have seen done otherwise because as big a deal as getting climate change and public attention is
it's not clear that climate change is something that is driving the underlying basis of all the problems
but a breakdown in sense making and a control of patterns of human behavior that kind of downgrade people like
oh wow that really does make all these other things worse.
So I see that as a very powerful and personal on ramp for those who are interested to be able to come into this deeper conversation
and some of them it'll simply help them be like okay now I know what I was intuitively feeling
somebody's put it into words and I at least feel more oriented.
And that's the extent because they don't necessarily have the ability to build new blockchain systems or whatever it is
and they should be doing the nursing or education or whatever really other important social value they're doing.
Some people will be able to say this actually really resonates I can translate this to other audiences
and get more people engaged and some people say I can actually start innovating and working with this stuff and all of those are good.
Yeah I agree and I think what we've essentially been outlining here and you sort of hit it at the end
is going back to the Charles Kettering quote which I learned from you and I've learned so many things from you over the years
which is that a problem not fully understood is unsolvable and a problem that is fully understood is half solved
and I just want to maybe leave our listeners with that which is I think people can look at the long litany of problems
and feel overwhelmed or get to despair in a hurry I think is your phrase for it
and I think that when you understand the core generator functions for what is driving so many of these problems to happen simultaneously
there's a different and more empowering relationship to that and you've actually offered a vision for how technology can be consciously employed
these new technologies can be consciously employed in ways that should feel inspiring and exciting
I mean I want that transparent blockchain on a budget for every country in the world
and we can see examples like Estonia and Taiwan moving in this direction already
and we can see Taiwan building some of the technologies you mentioned to identify propositions of shared values between citizens
who want to vote collectively on something that previously would have driven up more polarization
so we're seeing this thing emerging and I think what we need is to sort of have this be seen as a necessary upgrade to
let me do that again I think we need to see this as not just an upgrade but the kind of cultural enlightenment that you speak of
that so many different actors are in a sense already working on
you know we used to have this phrase that everyone is on the same team they just don't know it yet
and once you understand the I think degree to which we are in trouble if we do not get our heads around this
and identify the kind of core generator functions that we need to be addressing
once we all see that I'll just speak to my own experience when I first encountered your work
and I encountered the kind of core drivers that drive so much of the danger that we are headed towards
I immediately I was kind of already in this direction already but I reoriented my whole life to say
how do we be in service if this not happening and of creating a better world that actually meets and addresses these problems
and I know so many other people whose work and whose lives and whose daily missions and purpose
have been redirected by I think hearing some of the core frames that you offer
and who I hope and who are many of whom are already working on active projects to deal with this
and those who are not are supporting in other ways and I just hope that our audience takes this as an inspiration
for how can we in the face of stark and difficult realities as part of this process
gain the kind of cultural strength to face these things head on and to orient our lives accordingly
because I have while bearing periods of time hit probably low grade despair myself
I actually feel more inspired than ever the amount of things in the number of people who are waking up to these challenges
and I'll just say that that I think when you face these challenges alone
and you feel like you're the only one seeing them or you have a weird feeling in your stomach
it can feel debilitating and when you realize the number of people who are also putting their heads up
to say how can we change this that's what feels hopeful and that's where I derive my optimism
so Daniel thank you so much for for coming on it's an honor to have you
your work has touched the lives and work of so many people who may not always say so publicly
but I know that you had also a huge hand in inspiring some of the themes that emerge in the social dilemma
which has impacted so many people as well so thank you so much
really wonderful that we get to have this conversation thanks just on absolutely

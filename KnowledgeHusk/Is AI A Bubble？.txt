This video is sponsored by Ground News. More from them in a bit.
Miriam Webster defines AI as a three-toed sloth. According to Al Perkins, a sloth is a slow,
moving, arboreal mammal, and the name AI was given to this creature because of its high-pitched
screech. Fascinating. Artificial intelligence, on the other hand, is the ability for computer
systems and algorithms to imitate intelligent human behavior. This is not anything new.
The technology dates back to nearly the beginning of computers. But obviously,
the AI that is gaining all the hype today isn't quite the same as the enemies in your video games
or the countless chatbots that have been around for decades. Clearly, chatGPT is not the same
as Clippy. Probably? This whole AI thing is complicated. We don't know the ultimate potential
of large language models and neural networks and blah, blah, blah, but at the same time,
it doesn't take a genius to figure out that the letters A and I are being used as much as possible
by corporations looking to increase their share values. If a company so much as others AI, investors
cannot help but invest. Even if they have no idea what it means or what it does or what it might do,
they just assume it's going to make money. Nvidia, which was well known for being the producer of
GPUs for gaming PCs and other computing products, has gone from a successful company to, well,
the third largest on the planet. It has seen an increase in value that is nothing short of obscene,
all thanks to this AI. Microsoft is now the most valuable corporation on the planet as the company
is all aboard the AI hype train, pushing it into every corner of their operating system
and having an extremely close relationship with open AI. All the while, nobody can seem to agree
on whether this technology will change the very fabric of our society or prove to be another
flash in the pan. Tech fad, everyone, everyone has an opinion. I'm exhausted of seeing this back
and forth argument that never seems to reach any conclusion. And speaking of, GroundNews is a website
and app that gathers related articles from more than 50,000 sources around the world in one place
so you can compare how different outlets cover the same story. Each story comes with a clear
breakdown of the political bias, ownership and headlines of the sources reporting, all backed
by ratings from three independent news monitoring organizations. Take a look at the story about
how open AI is setting up a safety and security committee as it starts training for the next
frontier model. Something that has caused a lot of controversies as previous safety team leaders
have recently left the company. So this story has been covered by more than 73 sources, with 37%
of them leaning left, while 14% leans right. You can also see the ownership information,
and for this story, 40% of the reporting outlets are owned by media conglomerates. You can even
compare headlines to see how this bias can affect framing. We can compare headlines quickly and see
how some organizations will stoke the public's existing fears about AI to get clicks, while
others might capitalize on the distrust that Sam Altman has garnered over the past few months.
Ground News has this feature called Blind Spot, which instantly shows you stories that the left
leaning or right leaning news media conveniently won't report on. So go to ground.news slash
husk or use the link in the video description to subscribe today. If you sign up through my link,
you will get 40% off the vantage plan, which is what I use to get unlimited access to all of their
features. I think Ground News is an exceptionally important website and one you absolutely need to
check out. Now back to the video. Yes, AI has been around for a while, dating back to the earliest
computers, but the hype right now surrounds progress on large language models. These are programs
that take a lot of information in, require a user prompt, and through complex algorithms basically
create probabilities of what the output should be. As lifelike as they might seem, these things are
not actually alive. There's no thinking involved. They cannot create wholly original or novel ideas.
They require training data to do anything. I've seen it suggested that one of the best ways to
prove this is to ask a question with an answer that the AI model has not been provided with in
its training data. For example, go to chat GPT and ask, what's the 21st letter in this sentence?
You might get a different response, but I got the AI saying confidently, it's E. And if you say,
I don't think that's right, it will try and count again and give you a different letter, this time
I. It will sometimes interpret two and one as letters, maybe the apostrophe. And the reason
questions like this break the AI is because it doesn't actually understand what a letter is.
It doesn't understand anything, but it searches the internet and its training data
for the most likely answer. Even with these limitations, the technology is clearly capable
of some impressive things, from assisting you in writing computer programs to proofreading
your short stories to helping you cheat on your homework. But even still, a lot of what gets promoted
as AI isn't really AI. A lot of the revolutionary abilities that AI has been reported to have
is actually stuff that traditional software has been capable of doing for a long time. This creates
a problem where the idea of this technology, it's often more valuable than what it can actually do,
because again, just saying AI makes stock number go up. So a while back, Amazon launched a real life
store, a brick and mortar shop where customers could walk in, grab items off the shelf, put them in
their carts and leave without ever checking out. These items were automatically charged to people's
accounts without any need to speak to a cashier or even use a self-serve system. It was all done
automatically. Many assume this was handled using an array of computers and sensors and scanners,
but in reality, the company just probably hired a lot of Indian employees to watch people while
they shopped on cameras. And then they would charge the user's Amazon account.
Google was caught off guard by the immediate success of ChatGPT and has been trying its hardest
to launch its own AI competitor. They paid millions of dollars to Reddit for access to the
website's posts to train their AI with and it went about as well as you'd expect.
When users asked questions like how to make your cheese stick on pizza, Google decided the best
answer was to use non-toxic glue. This information, by the way, was provided by an 11-year-old Reddit
post by user FuckSmith, which Google's model decided was the most reliable source for information.
When asked about how many rocks you should eat a day, it did not give the correct answer of zero.
And look, things just kept getting worse from here. As you can imagine, any technology that uses
the internet to train itself is probably going to make a few mistakes. And those mistakes have made
AI a very easy and fun punching bag right now. But yeah, not all AIs are created equally. You
can't say that LLMs or machine learning or neural networks are completely useless because, well,
a lot of people are using it. There's potential here. Natural language programming could allow
people to write their own programs with no code required, just describing what kind of application
they want to make, and the AI would handle the rest. You could use this to learn to research
information, but again, AI can be wrong. So I view it in the same way that I view Wikipedia.
It's useful. It's a nice jumping off point, but you need to check your sources. It's not 100%
accurate or compiling lots of data really quickly to get back at your homeowner's association.
This does have real value. Automatic translations, transcriptions, stuff that may have been possible
with previous software is just more accessible than ever. But people are taking this too far.
The end goal of open AI is to achieve artificial general intelligence. AGI is supposed to refer
to a strong AI that can learn skills in tasks that it was never trained to learn. This could lead to
machine sentience, consciousness, the singularity. And right now, everyone seems to believe that if
you just keep throwing more computing power, more energy, more information into these algorithms,
you can simply manifest AGI from the kinds of artificial intelligence we have today. Everybody
has a different way of defining AGI, because well, nobody really knows what it's supposed to look
like. We often divide AI into two categories, weak AI and strong AI. Weak AI is capable of doing
very specific tasks, stuff it was programmed to do, and everything we've ever made so far has been
a weak AI. AGI would be a strong AI, and it could do a lot of jobs. It could do a lot of stuff,
would put you out of a job. But what might this look like exactly? There are a few different
tests for AGI, but I've always found that the best would be something done with an actual robot in
a Tamaton. Take a robot and ask it to repair the plumbing electrical systems and appliances of an
early 20th century home, especially if none of that information is available through documentation
on the internet. Every home is going to be slightly different, so the robot will need to make assumptions
to learn to understand why your custom 1950s toaster is not working and then try and figure
out, possibly through trial and error, how to get it back into toast making condition.
While today there are demos of robots completing simple tasks in the home, these are in controlled
environments and are only possible because of training data created from previous experiences.
This AGI robot would need to be capable of learning from unique situations. To ask questions based
on its own volition, and solve them by itself. But there is no proof that this will happen. In
fact, studies do seem to suggest that there are diminishing returns when it comes to this kind
of stuff. We don't know if any of this is even possible with traditional silicon chips and programming,
binary code, any of that. It could take entirely new forms of computing to achieve this. We don't
fully understand how the human brain works, so attempting to replicate it with far more simplistic
technology, far more rigid technology, it just seems more than a bit optimistic. People are
acting like we're five years away from robots taking every job, becoming alive, enslaving
humanity, changing life forever. And honestly, chat GPT could be a stepping stone to all of this.
A stepping stone to AGI, in the same way that the discovery of gunpowder was a stepping stone to
the nuke. Or the hot air balloon was a stepping stone to intergalactic spaceflight. That is to say,
not much of a step at all. For all we know, LLMs and AGI might not even be related. We might not
even be on the right track. We just don't know. So if we don't even know if this can manifest
into some more powerful AI, if we have no substantial evidence to support that we're on
the road to AGI, then why all the hype? Obviously, it is incorporation's best interest for people
to believe that this technology is on the cusp of being something truly revolutionary, so good
it's frightening. It makes it seem way more advanced than it actually is. It implies more
potential for growth, which will make the stock number go up. And they have ways to market this
technology. So many companies are just throwing around the words AI, even if it doesn't even
apply to their products in any way. Gigabyte, which usually makes gaming-centric motherboards
and memory thingies and whatnot. Well, they decided they wanted to go all in on the AI thing and
have rebranded their products with a big old AI plastered on the side of them. Yeah, rebranded
because it's really just kind of the same stuff you could find anywhere else, but now it's got AI
written on it, whatever that means. This exact same thing happened a few years back with the whole
metaverse craze, where every company started proclaiming that their existing video games and
online services were already in fact metaverses because investors wanted to hear that. And it
diluted the term to such a point that the word metaverse started to mean nothing more than just
a synonym for software. When an AI makes a mistake, the companies call it a hallucination,
which is a very misleading term. This is applying a human quality to the machine when in reality
it's not hallucinating. It's just taking a set of probabilities and it hasn't received enough
training data on the topic or the right algorithms to give you an accurate answer. It is not hallucinating.
It simply made a mistake. The technology simply wasn't good enough. Will it improve? Yes,
but we don't know how good it will get or how accurate it will become. For AI sentience, it
could be five years away, or 10, or 100, or 1000, or it might not be possible for us to build this
at all. We don't know, and everything else until it happens is just a guess. A lot of the AI safety
you hear about actually does refer to more realistic concerns about how this technology
might be used. If in the future AI services become a replacement for traditional search engines like
Google, if people rely on chat GPT to get all their news, then those in charge of the service can
lie, manipulate, or alter information to benefit those who are in charge of the AI, to push their
own motives, their own agendas. This is where the concerns of safety often lie, not in the fantasy
of robots taking over the world, or at least that's where the fears should lie. And look,
if fancy quantum computing or something entirely unprecedented allows for a robot revolution,
allows for AGI, if today's technology and LLMs are on the right path to all of this, and all we need
is just a few trillion dollars in the combined power consumption of a small nation, I'll be the
first to admit I was wrong. Oopsie daisy, egg on my face. But right now, I'm just a little bit skeptical.
So for the purposes of this video, let's assume that the future of AI is not revolutionary,
but evolutionary. And if that's the case, is it a bubble? Yes, yes it is. But that doesn't mean
it's worthless. I kind of see AI right now like the dot com boom of the late 90s. This was a time
where investors believed that every single website was going to make millions of dollars, and once
they figured out that only certain websites would ever become popular, would ever become profitable,
many of them took their money and ran. The resulting fallout led to the dot com bubble burst,
but this wasn't the end of the worldwide web. Now I'm not saying that AI will be as important as
the internet, but clearly this is more than just a simple fad that's going to fade away. I do think
the bubble will burst, and then the industry will have to realistically look at its value,
rather than rely on unfounded promises and rampant speculation. But yeah, AI is here to stay,
because it's been here for a long time. AI already has changed our world.
Algorithms drive search engines, YouTube channel recommendations, most of the internet
is driven off of this automation. We know that the technology has value because it's been around
for a while. The question is whether this new form of AI, LLMs, will have a significant
impact on how we live and work, or how significant of an impact that might be. And a lot of people
seem to have some doubts about all of this. Rooters Institute for the Study of Journalism
pulled 12,000 people from 6 different countries online about their opinions on AI. Granted,
12,000 people is not that many, doing polls online does not mean perfect results, but this is about
as good as we're going to get if we're trying to figure out what the average person thinks about
this technology. And most people have no idea what it is, or what it does, or what it could do.
A lot of people have heard of AI in these new services, but they don't know what distinguishes
a large language model from traditional computer software. We can tell that most people are not
using AI that often, even if chat GPT has a huge install base. Younger people are more likely to
try it out than older people, obviously. But most people view AI as a technology that will primarily
be used in professional settings by scientists in social media companies, but less so by the
average person and their daily lives. Again, if these people don't actually know what AI is,
considering some of these poor results I don't think they do, none of this is super helpful,
but there is some stuff we can take away from this. It seems like people aren't just confused
by the technology, they seem to fundamentally dislike it. There are a lot of concerns about
the damage it can do, people are scared it will take their jobs away, or just functions as tools
to manipulate the public. And most don't seem interested in the positive aspects of this technology
at all. Most can't even identify what that might be. Sometimes a technology does need to be more
than good enough, it needs to be perfect. When driving a school bus or doing taxes for a multi
trillion dollar corporation, you probably don't want to put this responsibility on a robot that can
or probably will make a mistake. Yes, humans are not perfect. The AI might even be better than the
human in many cases, but humans give people somebody to blame when things go wrong. They
provide accountability. Something that the AI just doesn't have for AI. Google spent all of Google
IO 2024 talking about its AI initiatives, and people didn't really seem to care all that much.
A lot of people just rolled their eyes because it's more AI slop. Microsoft hyped up its new
line of AI PCs, which most saw as a concerning invasion of privacy, rather than anything remotely
worthwhile. The corporation is betting the future of its operating system, of the Windows operating
system on AI. And it's a bet that doesn't seem to be panning out. In fact, I've seen a lot more
people talk about moving to Linux now that Windows is infected with this AI garbage. I have no doubt
going forward that LLMs and machine learning and AI will manifest into something valuable.
It's possible this might even result in massive changes to everyday life. But right now, this
entire thing has been polluted by bad actors, false promises, and in my opinion, pretty misleading
marketing tactics. And it's baffling to me that people have just forgotten the whole metaverse
craze from a few years back, where the word itself began to mean nothing by the end.
These analysts, who don't even understand the basic technology, are hyping up others about
how much money we're all apparently about to make, claiming that Nvidia's gonna be worth
10 trillion dollars in just a few years. And as far as I can tell, there's just no reasonable
basis for these claims. We've reached a point where Big Tech is so desperate to achieve growth,
that it just keeps taking these fairly predictable and standard advancements and software,
and then applying new buzzwords to the technology. And by the time the bubble should burst,
they're already onto marketing their next buzzword. I don't want to dismiss the entire
potential of AI by calling it the next metaverse, but look, there's a lot of problems here. And
as of right now, for many people, this technology just kind of comes off as a slightly more invasive,
less accurate version of all these digital personal assistants that have already become
commonplace over the years. This whole industry right now just seems so flimsy to me. There's
a good chance that everything I say here will age like milk. I get that. Maybe we will see a world
where most jobs become significantly easier with the assistance of the machines, where this is a
necessity, or maybe the tech is too good and we all lose our jobs. I think right now we're seeing
a technology that is more style over substance. There is substance there, but there's just,
there's a lot of fluff here. Or maybe I'm completely wrong, and in five years we'll all
have Rosie the robot in our homes. I need to get my robot out of my swimming pool. I don't think she
can breathe. Again, huge thanks to Ground News for sponsoring today's video. Go to ground.news. husk
or use the link in the video description to subscribe today. If you sign up through my link,
you will get 40% off the vantage plan.

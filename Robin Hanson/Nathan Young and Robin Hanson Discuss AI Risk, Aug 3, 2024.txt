Oh, Nathan, nice to see you again.
Hey, Robin, how are you doing?
Yeah, it's great to see you.
Robin and I last saw each other playing Imperial, didn't we?
At Manifest, yeah, in virtually.
It was great, yeah.
So I did a sequence of these AI risk conversations a year or two ago and haven't done very many
since, although I do that, you know, once in a while.
But as you said, you wanted to talk to me about AI risk, and I said, hey, why not?
Let's record it.
So yeah, I guess you have some particular questions in mind.
And I have my usual sort of main question I ask people.
So who should go first?
I think we should do a couple of questions.
And then I predict that you're going to answer a number one quite quickly.
And if not, then we can do a couple of them and then we can switch over.
Okay, go ahead.
So this is some work I'm doing with Catech Grace, who we both know.
And so this is mainly her work, but she's kind of tried to come up with a number of
the different AI risk arguments that people find compelling.
And I could list them all, but I can list them all, but we don't need to go into detail
of them, right?
You might as well go one at a time and we'll take them one at a time.
So some people think that there is a risk from competent malign agents.
So you've got, you know, this is always my understanding of what people do.
So you know, this may not be exactly her and the way she'd frame it, but you've got some
agents, they're competent, they don't necessarily want the things that we want, and so you lose
the future.
This is like the first story.
Okay.
Well, that's related to the sort of question I usually ask people, which is in terms of
what did you think was going to happen without AI?
That is, um, yeah.
So which is, which is like, so, um, I, I am a, what's the, what's the usual say here?
Um, so another one of the thing, the other one of the arguments, which, uh, which the
week we'll get maybe get to in a bit is, is, is this line, um, humans aren't aligned to
one another and this isn't really an AI risk argument so much as just saying that if you
take anybody's, uh, references and you, and that might lead you to a world which is quite
scary or concerning to other people.
Um, and I think this is maybe an argument you're going to find quite compelling, maybe.
Sure.
So I guess, I mean, first of all, I'd say we should think about whether we're talking
short or long-term AI and a different risk, different issues show up in those two terms.
And if we're talking longer term AI, then I might compare AI to other sorts of descendants
you might have and say, well, you know, in general, like how aligned are you with your
ancestors?
I mean, how much would they approve of what you're doing and how much did you expect to
approve of what your descendants are doing?
And then we might expect that with longer lifespans and faster change, that sort, those
sort of conflicts will become more salient or vivid because you will now see more future
generations or more change in a lifetime, uh, so that we could think this ancient problem
is going to be exasperated by longer lifetimes and faster rates of change.
But fundamentally, there's just this problem that across generations, people haven't been
aligned in the sense of having very much control over their descendants and their descendants
often changing their values in conflict with their ancestors values.
Yeah.
I think I find this like, I think I find this relatively compelling.
And so then the question is just, is AI different from that in sort of the degree of deviation
or the rate of change, perhaps, uh, but otherwise it's the same sort of issue.
Yeah.
I mean, I think a different, yeah.
So like a different AI risk discussion is like the vulnerable world hypothesis, right?
Like as, you know, if everybody dies and that's pretty bad under, well, you know, probably
some of my, probably some of my ancestors were religious enough, not even necessarily
that far back, uh, thinking of my, you know, you know, I think of my grandfather was pretty
firing Brimstone kind of a guy, right?
Um, if everyone had died, he probably would have ensured that as God's judgment.
And certainly if he were alive today, he would be, I think he would be a sort of pretty strong
Republican, he would, he would feel he would be pretty horrified by the state of the world,
I think.
And he's on my own choices within it.
And like, and so that's all with respect to the, these longer term changes.
And then in the shorter term, you might make a comparison now with say governments or corporations
and ask, well, how aligned do you think they are with other people and how big of a problem
is that, uh, just to get some reference comparisons here, uh, in order to set what the standard
is, we're going to hold AI to, yeah, I don't, I don't know whether I find this like super
compelling though, because well, like I find it moderately compelling, but I think to think
about it more, I think I think I think I find is like most people two, five, 10,000 years
ago seems to me like if they could have like had a bit more food or like had a bit of an
easier life or, uh, I'm not, this is going to be harder than I think to give these things
like they probably would have taken those things, you know, I could easily go on with
this and be like, if they could, they would have wanted to spend more time with their
children, but actually that's not super clear, right?
Like I think, I think probably most of the history like pretty fond of their children,
but I think probably in like the Roman times, it seems people had like a different relation
to their children and, um, I just don't really know enough, some like probably, you know,
like, but like the central point is that there doesn't seem to be like some things like which
are quite important chunk of human things that like have been pretty stable over time
that like, you know,
Sure, but if it's with respect to alignment that I'm just trying to have some standards
of comparison, like I'm trying to say, let's think about AI relative to what we would have
expected without AI and in the long run, that's our descendants.
And in the short run, let's say corporations or governments, though, that's our relative
comparison for alignment today.
And so then presumably we're kind of okay with what we have today.
And so then the fear is that it will be much worse with AI than with these relevant comparisons.
Yeah, that seems, yeah, yeah.
Okay, so now with those relevant comparisons in mind, then, you know, ask your questions.
Um, okay, so.
Yeah, I guess like the thing I'm interested in is trying to get a sense of these different
like risk things, like which ones people actually find compelling, because there are
like a number of different stories that people tell, and the stories are meaningfully different.
And often the AI risk discourse can be a bit like AI good, AI bad.
And so.
Right, so you asked about competent malign agents, right?
Was your first question?
Yeah, do I say sometimes there are competent malign corporations and governments and nonprofits
out there, and they're in our world today.
And I do think we want to know about them and are somewhat concerned, but we have ways
we typically deal with them, and we mostly think that goes okay.
So it's a.
Okay, so it seems to me plausible at least that.
You might end up with agents that can cooperate without the need for.
For like most of us to be involved in a way that like currently government doesn't permit.
So like you're right, you have, but like the most powerful corporations really isn't that
powerful at all, right?
The government can go into any corporation, it can shut it down, it can take its technology,
it can stop it from hiring people.
It can do that in any one country, but there are international companies that can't be
taken down by any one country.
I think.
I think.
But we still worry about the government themselves.
I mean, fine if you think the companies are disciplined by the governments,
the governments are less disciplined by other governments, right?
So a malign government can do a lot of harm to it's within its scope of its territory,
say North Korea at the moment.
Yeah, yeah.
Like by happy accident or by like it turns out that like democratic systems are doing pretty
well and the system sort of focused on the like the well-being of their people do seem to,
which I don't, it's a little bit surprising to me at times, right?
Like it's not obvious to me that this would be the way it would be.
Right.
So, but I think a way I would summarize this is that we mostly don't rely on individuals to
compete or counter these organizations.
We rely on other organizations at their same scale and type to be the main things that keep
them in check.
So corporations compete against corporations, governments compete against governments.
We individuals use the competition among them.
That's our main way that we reduce our worry about them.
And so then for AI, even if an AI you might postulate is more powerful than any current
government or corporation, you might say, well, there'll be other AIs at the time.
And so long as there are many AIs competing and some of them plausibly are aligned with us,
then we don't have to worry so much about the others because competition could keep them in line.
Sure.
So yeah, I'm mainly interested now in just like jumping on to like another argument.
Just because I don't know.
That's what I'm interested in.
Sure.
So the next argument that people are surprised about is I think quite similar.
It's called the second species argument.
I'm sure you're familiar with it.
You know, it's just like very simply, it's sort of saying
apes should have been pretty concerned about inventing us.
You know, you should be concerned about making something which is like a new species,
which is very intelligent.
And so this is a reason we should be concerned about like making AI,
not necessarily like it's going to go badly or like just, it's just kind of being like,
whoa, you don't want to like create a new group of things which have their own constituency,
which are like more competent than you are.
I, Katya tells me that there's like some notion about goals that's the difference here.
I don't really understand all these things.
Well, like I just like this is like a new story.
We're going to forget the old story.
This is the new story.
Do you find this story compelling?
So I have an essay that it is designed specifically to address this question
at Aquilet a year or so ago, but the way it's summarizing that's summarized as
AIs will be our mind children, I think.
Yeah, that's it.
AIs will be our mind children.
So I think the way I would summarize it as evolution, both DNA and culture,
we predict should produce suspicion and hostility toward coexisting competing entities
that basically have different genes, that evolution would make you wary of such competition.
And in fact, we are wary of such competition, other kinds of animals or other groups of humans
that we often see ourselves in competition with.
And we can, you know, not have to go to total war with them all the time,
because maybe we can monitor them and have cooperative relationships.
But we do in general, and it makes sense to have suspicion and wariness of coexisting creatures
who could compete with us, maybe might plausibly win, and then their genes would displace ours.
And that's just, I mean, many people have, you know, preached against this instinct and said,
we should restrain it as this is a bad instinct, and we should, you know, try to see things better
than that and find trusting relationships with them and cooperative relationships.
So we don't have to just go to war on the basis of this instinctive drive,
but it's plausibly there. That's one drive we should expect.
But there's another drive we should also expect at a similar power and similar intensity
from evolution, both of DNA and of culture, which is an indulgence toward descendants,
even when they're substantially different than you. That's exactly why
people aren't that bothered by their descendants becoming not so aligned with them.
We just have this instinctual indulgence of our descendants who are,
even if they're substantially different. So coexisting creatures, even when they're pretty
close to you, we're very suspicious of that small difference. You know, you can eat them,
even your cousins or something, but descendants.
Somebody who looks like you, who's come from a similar school to you,
who's working in your company, who's competing for the same kind of promotions,
you might see very differently than even somebody who's more difficult, different from you,
who's as a child, or maybe maybe your student, you bring some kind of optimization and then
kind of out-competes you, you might still... You're mentee, right, yes. So your cultural
descendant or your DNA descendant, either way, you could be more indulgent.
And so then the key question about AI then is, which framing here is the most relevant of AI?
Your counter-argument to this sort of thing, something you agree, you say,
yeah, seems like we should be suspicious towards something which appears,
which might compete with us. Then your counter-argument to this is like,
equally, a different framing could be that AI's aren't a second species, they're like our children,
and therefore, which should be sort of unusually...
Well, not just there are these two framings, but we should have a way to tell which one applies.
Very simple way to tell. The first instinct comes when they actually exist next to you,
and they're actually there, and they actually have different genes, and they're actually competing
with you. And the others should apply when they don't exist yet, and they would in fact arise from
you, and you will be directly, causally relevant in creating them, and they will inherit many
things from you. Many of their features will be your features, because you will have given rise
to them. And I would say objectively, that second framing is in fact more relevant for AI
now, because they don't exist. And the ones that are in the process of coming to exist are being
created by us in our image, and they are in fact inheriting many features from us.
So later on when they exist, the other humans at the time, they could feel this hostility
toward... That's us anticipating our children having sibling rivalry,
and as parents, what do we want to do about our children's potential sibling rivalry? Well,
typically parents want to cut that down a bit. Parents are typically trying to restrain and
reduce, but not entirely eliminate, sibling rivalry among their descendants.
But I mean, imagine that you knew you would have two children, one of whom was going to be,
you know, the older shall hate the younger, one of whom would be powerful and would, you know,
you intend to give them equal inheritance, but you knew that via whatever means one of them
was basically, you know, basically disinherit the other child. How would you feel about that?
Well, you might want that. I mean, you might not want that to happen. Now, it's in fact,
when you look at parents say giving money to their children, on average, they actually give
more money to their successful children by, say, paying for their college. So parents do now, in
fact, prefer not just to give time and effort toward their more successful children, but even
money toward their more successful children. But you wouldn't necessarily want your more
successful children to sort of kill and eat your less successful child. You know, most parents
wouldn't want that to happen either. Yeah. So you would want some degree of restraint between these
children. And of course, you might want to teach your children to respect their siblings out of
respect for you. That's something we do as parents and grandparents and ancestors. We
try to teach our children to feel connected to, allied with their siblings and their relatives.
And that's certainly something that inheritance would suggest we might have the same sort of
habit toward our AI children as well, that we would want to, as possible, teach them to
have some sympathy and compassion and sense of connection to their concurrent siblings.
Okay, let's move on to the next one. There's about eight of these, I think.
Again, this is mainly Katya's framing, mainly her work. I just work with her on it. I'm trying to,
yeah, I want to claim this is all, yeah, it's not. So the next one is loss of control via inferiority.
So this is like a child prince. You have a child prince, he's going to inherit the kingdom, but
he has to get some advisors on board, you know, maybe he gets like, you know, his like, his step
mother and like, the like, counselor, you know, different advisors, and he's got to go and talk
to them. And he's got to say, I want you to run my country until I get into power. How does he,
because he's a child, he's like quite young, he doesn't understand whether the things are
giving him are really true or not. How does he ensure that when he comes into majority, he still
has a country to run, and that he hasn't either been killed, or that it hasn't sort of,
like most of the value of his nation hasn't been like slunk off into another one.
Is this, is this the story you find compelling?
Well, the major problem there is you've created this very unusual scenario that's not a common
scenario in our world. So our institutions aren't set up for it. That is today, as ordinary people,
because there are lots of ordinary people who've been around for a long time,
dealing with corporations and governments. We have developed social institutions and
equilibrium practices by which we keep them competing with each other to serve us.
And instead of, you know, killing us and eating us or whatever, because, you know,
that's just evolved over a long time. Now, this one prince is showing up and asking for a new
institution in a world that there, this problem doesn't appear much. So there isn't, they can't
just go to a standard consultants who sets this thing up all the time, right?
But your response is again, quite similar to the first one,
maybe less similar to the second one. Your response here is something about
like multi-agent dynamics about you can get, you can get the advisors to argue against each other.
And, you know, that our prince doesn't exist in a world where he's woken up one day, his dad's
dead, he doesn't have anyone to trust. He exists in a world where there's many princes in many
nations who have to deal with these kinds of covenants and he can call up his cousin,
out of force, you can go out of force, hey man, like.
What did you do when this happened to you?
When your mom died and you inherited the whole kingdom, like, who did you trust?
Like, right.
Okay.
Right. So in it, you clearly, the problem gets harder with this unique prince,
where there's nobody who's ever, who's been in his roles at all similar in a long time. He,
you know, he can't go look up data sets of people who've been in a similar situation,
right? He's thrown into this thing, a unique thing. That's just going to be harder to deal with.
And so then.
So you may find this more compelling if it's like more of a discontinuity, like if.
Right. So that's a big question to ask about AI, how gradually or, you know, smoothly will the
transition happen so that we can, you know, adapt incrementally to changes as they show up,
as opposed to very lumpy sudden changes. That's a huge issue with respect to AI risk.
I mean, the most compelling AI risk scenarios have been the hugely lumpy scenarios.
Compelling in the sense of, you know, you wanting to be very worried about it and wanting to be
willing to go to very extreme responses. And, you know.
Are the ones where there's, there's, there's a real like rapid or discontinuous change.
Exactly. So for example, my ex-coblogger, Elijah Yukowski, that was the scenario he and I debated
long time ago called FOOM, where he cranked up the dial to the max on the suddenness and size of
this transition. And, you know, I, I've heard that I haven't read lots of that stuff. There's
lots of stuff to read. But do you, do you find that your arguments apply less there? Like,
if you have a sort of singleton AI that runs everything, it's not necessarily clear whether
it's conscious. It's just sort of like it's doing stuff. Is that a scenario that the Robin Hansen
is more worried about? Well, any scenario that's less familiar, that is, we have less experience
with it, is for that reason, something to be more worried about in the sense of conditional on it
happening, you'd feel less sure about how it was going to play out. On the other hand, the more
weird and unusual scenario is the less likely you believe it's going to actually happen. So then,
in that sense, the less you might worry about it. So it depends on whether we're talking about
the chance of it happening or what to do conditional on it happening.
Sure. But for some of these other scenarios, you've kind of been like,
I think you have a sort of a broad tend to your unconsciousness or something. You're like, I think
you bite the, in your writing, it seems to me you bite the Republican conclusion, you sort of say,
yeah, seems fine, like lots of things being alive, that's okay. I mean, I was reading a Guardian
review of age of M this morning, and they judge you kind of quite negatively for describing this
world. But I just think my understanding of you is that you wrote the book, you tried to sort of
take the assumptions that seem to make sense to you, you tried to describe a world not that you
liked or they disliked, but just be like, what slightly seems likely to happen. And this led
to a world that sort of many people find a bit repulsive, I think probably you've heard that
expressed, but that you find sort of like, you imagine that if you lived in that world, you'd
be like, yeah, things are fine. And in fact, if anything, maybe, maybe you think if you lived
in that world, if you were one of the ends, and you were given the chance to exist in your life
now, you might prefer being an M maybe, rather than more causal ability to affect the future or
something. I agree. That is, I'm less put off by the Malthusian scenario than many people today.
I mean, it was the by far the usual scenario in human history and in animal history. So it's,
and it's, it gives me more reassurance that it will work and continue to exist as a coherent
scenario into the longer future. Whereas I'm much more worried about our world and whether it can
continue to function, given that we are so far away from Malthusian constraints.
Sure. So, so, so my question here is,
the Singleton scenario, where you have like, one thing sort of controls everything,
seems quite unlike either of those scenarios. Do you have any sort of like, valence towards it in
terms of like, desire or not desire? Like, I hear about that scenario where I'm like, there's one
thing, it's jubiously conscious. It like, seeks to have control over like, all the energy everywhere.
I'm not even saying I think this is a likely scenario. I agree with you. I think it's a
pre-owned like, actually. But like, this scenario doesn't sound good to me. Like, actually, like,
a scenario where there's like, many like, little AIs, like, doing little trades with each other.
And there's no humans at all. Like, I'm not wild about there being no humans, but I'm at least
like, oh, there's like, lots of things and they're doing stuff. But like, maybe they're conscious
and like, maybe like, that seems like a world in which there might be like, joy and beauty,
even if it's alien to me. Whereas a world where there's like, one thing,
that seems like a world where they're like, it's more likely barren.
So, the first thing to say is just, this Singleton scenario is harder to reason about,
because we just don't have so many analogues to it in the past. So that makes you worried
just at that level of not being able to bound it very well. And secondly, when you start to think
about the analogues in our world that have gotten closest and the causal paths that lead to them,
in our history, those tend to be strong central governments. And then your attitude
toward strong central governments will influence your attitude toward the Singleton scenario.
And I'm more of a libertarian who's always been somewhat wary of strong central governments.
So then I get worried more about that scenario. But I mean, the first point is just, it's just
hard to say much about it, because it's so foreign. So an enormous amount depends on the causal path
that leads to this central power. And then of course, additionally on whatever, what does it
need to do to stay in power? So I mean, a standard reason why strong central governments have been
a problem is that they are only barely staying in power. And they have to do a lot of things to
ensure that they stay in power, many of which are politically repressive of what they see as the
threats to their power. So a Singleton that was just barely able to become a Singleton,
fighting fiercely against the threats to that, that it's easier to see that as a negative scenario
because you could imagine what it would be doing to overcome its obstacles to becoming that central
power. Isn't there like a barrenness like, well, I guess maybe you're right, maybe you're just like,
I don't know. Okay, let's move on to the next question. The next one is a loss of control of
our speed. So this is like, you have a car, your car drives well, but without you knowing,
I put in a thing which makes your car 10 times faster, right? And you get in one morning and
then like, whoops, you go off the driveway and you crash and you die. And like everything about
the car was fine, theoretically, except like now, it just happened way too fast. Well, maybe
I also made the like steering much more sensitive. You just couldn't handle that.
This scenario, you have to pair with a sudden transition and even ignorance of the transition
ahead of time in order to make the worst case scenarios here, right? If I know it's a 10 times
faster car, and I get to take it to a big field to practice driving it before I take it on the roads,
I'm much less worried about having this higher performance car, especially if I got a one tenth
dial on the engine, where I can take the usual strength and then turn it up a little as I try
it out, I'm much less worried about that as a person controlling a car. So then it would be about
this sudden unexpected speed up where I maybe even had a delayed knowledge of the speed up,
which would be the most worrisome. But I think the most important context to realize is just that
humanity has really never had much control over these things.
So a very stark and fact about the world is most of change has been driven by technology that
we don't vote on, we don't decide whether it's going to happen collectively, just individuals
locally adopt a technology if they find it in their local interest, and then that's how technology
has been changing for a very long time. A long time ago, it was very slow, and then it's gotten
faster, but there was never a point at which we were consulted about the speed. And we might think
it's remarkable that we've been able to keep up with the speed as it's been going. So roughly the
world economy doubles every 15 or 20 years over the last century or so. And if you think about it,
that's a really very fast rate of change. And I think many people, if they could vote on it,
they would vote to slow that down. I think you'd probably get a majority of the world to vote for
slowing it down. But the ability to actually do that seems well beyond us. And if it goes up even
faster, that just continues the same trend we've had for a long time of. It happens at the rate
it can happen without the rest of us having much say. Yeah, there's a kind of evolutionary thing
there, right? Like the way the bits that grow fast outgrow the bits that grow slow, and it
doesn't really matter. Right, certainly if you had asked the rest of the world, did they want
Europe to suddenly start growing much faster than the rest of the world? In 1700, they would have
said no. Yeah, seems likely. So then we have the question is, should we somehow create collective
institutions to regulate the rate of change? And then we get back into the issues with the previous
question about the risks and costs of having centralized regulation of this sort. And once
you start to worry about how badly it could go to empower central regulators to limit such things,
you might prefer to just let it go at its more natural speed. But it's a hard choice. Neither
one is ideal. So we've now, we've been about halfway through the arguments. I think the other
ones are sort of also in some way. In fact, we briefly discussed the humans are allied to each
other one. And they aren't to some substantial degrees, clearly. But the world is okay with,
you know, we live in an okay world where we have conflicts with each other. So you alluded to at
one point, the scenario that maybe AIs would be better able to coordinate than we are. So
you didn't sort of separate that as one particular separate point to talk about. But that is a
relevant consideration. Because that's a way to break the symmetry between, you know, us
and corporations and governments and AIs. If a malicious club decides to, you know,
want to hurt people in the world, it has trouble coordinating and it has trouble
coordinating with the other malicious clubs in the world. And that's part of what makes it
us able to manage such malicious clubs at the moment, is their limited ability to coordinate.
It, you know, more, we also have a limited ability to coordinate. But to the extent that they're a
minority, then our limited ability to coordinate is still enough to manage their limited ability to
coordinate. Now, if AIs are postulated to have a much stronger ability to coordinate,
then that's more of an issue of the AIs versus everyone else coalition scenario.
Sure, but then the Robin Hansen in my brain says something like,
but coordination is quite a good trait. And like, maybe if they're better at coordinating,
like, maybe they deserve, like, well, like, like, who are we to say that they shouldn't exist and
coordinate better? In the longer term, you might think, well, if some of your descendants are
going to be better at coordinating, maybe you do want them to succeed more against your other
descendants who are best, well, escorting, that's in the long run, of course. Yeah. But if you are
right now facing, you know, a coup. So at the moment, you coordinate a coup, right? At the
moment, one of the problems we have is potential coups. And one of the ways we limit coups is
because coup members have trouble coordinating. That's a reason why, you know, you need a pretty
large coup or pretty centrally located coup in order to have a chance to success at a coup.
But if you postulate a particular coup coalition, who is unusually able to coordinate, then you will
estimate their ability to manage a coup as higher and you'll want to be more worried about them earlier.
Like, I think a key notion within AI risk is that, like, you're talking about the chances
of a scenario that you don't want happening. And I guess, as somebody who's talked about this for
a long time, as somebody who I think I consider to be, like, to give me thoughts that I'm glad I had
on these topics. But any of, like, does any part of the scenarios discussed so far seem like a
risk to you? Like, when I hear you discuss these topics, I don't hear. So like any of these, like,
there's like, you're like, there's this bit that I'd like to avoid. Yeah.
So the most extreme scenarios are the most worrisome, which are the most lump lumpy scenarios
where very suddenly, something improves enormously in ability and perhaps an ability to coordinate
and also enormously changes its values. That would be the most worrisome scenario.
We'd have the least ability to sort of manage it by adapting to incremental changes.
We have the least ability to predict that it would go well, etc. So
even though I don't think it's as likely as other people think, I think it's okay to think about how
to prepare for it or to prevent it. And so I have gone on record in a post called FOOM Liability
to say, well, let's set up a liability regime, which is targeted for those particular scenarios
and possibly wouldn't actually have much of a retarding effect on other scenarios. And therefore,
I don't have to be confident that it won't be a problem. I want to be robust. I want to say, well,
if there is a potential problem here, what's the minimal solution we could come up with that,
you know, doesn't disturb other things, good things, but we'll deal with that one. And that would
be my proposal. So the concept is to make a checklist of features of FOOM-like scenarios,
of the problematic FOOM-like scenarios. And then whenever anybody hurts anybody else through
an AI, you add on an extra liability factor for the closer you get to these bad scenarios.
And that would just push people away from those bad scenarios through a local incentive way.
And that's, you know, my accommodation of saying that's not zero probability. And yes, well,
I don't want to just ignore it and, you know, wish, you know, look, cross our fingers and hope
nothing goes wrong. I'm happy to look for minimal ways to address even unlikely scenarios.
Sure. Okay. I think, I think that that then is so far the closest thing I've heard to, like,
sort of like an endorsement of one of those even, I think an endorsement can be a very
low probability endorsement, right? Like it seems to me, the others you sort of look at,
you think, like, I don't really understand from a sort of low resolution of what you've said is
something like the others you've kind of been like, like, I don't really understand, I don't really
buy that argument as a whole set of arguments. But the argument around like speed or something,
you think, yeah, that's plausible. It's very unlikely. But like, here are the ways I'd
mitigate against that. Well, it's also, again, what I was trying to do earlier was separate
other issues from AI in particular. So I didn't mean to dismiss the other issues,
but I just meant to sort of say we should just be thinking about those issues at the general
level of those issues, not necessarily thinking about them, particularly in the context of AI.
So for example, there are potential conflicts among generations.
Surely. Right. And I think it's fine to think about what to do about conflicts among generations
and to ask how far you want to go to restrict or channel your descendants and directions you
might approve. But that's not so much an AI issue. That's a long term generic issue about
the relationship between generations. Yeah. And then maybe in an attempt to understand you and
your work, it seems that I consider it that a lot of your work is relevant to humans having
different preferences, right? And different abilities to predict the future, much of prediction
markets. Sure. Is a system by which people who have different expectations for the future can
at cost to themselves, make predictions, and then allocate resources to people who are better
at predicting things, right? Right. And then we might want to think how to use such powers to
deal with very fundamental issues regarding the future. Sure. This is really self-indulgent,
but I want to self-indulge. Do you have any kind of comments on, it seems to me that forecasting
played a small but moderate part in the, in Joe Biden being losing the nominee, stopping
meeting the nominee of the Democratic Party. It seems like pretty plausible to me that, I don't know,
in 0.1 to 5% maybe higher, let's say 0.1 to 5% of worlds in which you don't have
Nate Silver, you don't have Polymarket. I guess unclear whether you're going to move back for
as well because that's been around for a long time, but then you don't have people, so then this
doesn't happen. Do you have any comment on that? I agree with you that it's plausible. I wish we
had stronger data. I mean, basically, key decisions were made behind closed doors by insiders who
are not really going to testify on that or aren't going to publish a transcript about it.
But the question is, what's the chance that some of them at least had seen betting market odds
about the chance that, say, Biden would win if he stayed elected? And what's, you know,
and what's the chance that they wouldn't have already known that through other channels? So,
I do think, in fact, these insider groups are often pretty ignorant of a lot of things. And,
yes, having somebody on the periphery of their network who had seen such prices and could report
them would have moved some people to believe more that, yeah, we can't bluster through this,
looks like enough other people out there get the problem that we should face it.
Yeah. And like, I guess, I know, I'm pretty confident this isn't really your ideal version
of your sort of like fire the CEO markets. But it seems to me, by virtue of there being markets
on who would be the president and who would be the Democratic nominee, you could
make some implications about the electability of various candidates.
And we did. Those numbers were available and people highlighted them and talked about them.
Yeah. And like, that seems to me, I think it's like a very underrated success of a thing you've
been championing for a long time. I'm happy to accept that judgment. Again, I try to be restrained
in my bragging about things in the sense that I need to maintain a long term reputation here and
sort of wait for data to collect up more clearly over the long run. But it's, it's, it's plausible,
certainly plausible. Right. Okay. That was, that was my comment. Okay. So as you probably know,
there have been times when, say, the betting markets only gave Trump a one sixth chance of
winning the election. And then he wins. And then people say, ah, see, shows you betting markets
are wrong. And I got to say, look, you have to average over a lot of cases here, you can't just
look on one case. So on a positive side, I also have to do the same thing. Right. That seems right.
I'm going to, I'm happy to say, yeah, that seems plausible. But in order to make a stronger claim,
I would like to average over more cases to look at more systematic data in order to make
stronger claims. To the extent that we're going to have these markets lying around, it seems,
it seems like a much more sustainable thing long term. Sorry, it seems like a much more feasible
thing to do this. Like, I think you could imagine there being 20 times as many markets about
elections, people love betting on elections. I could, you know, you know, we could have, you
know, will, will Pennsylvania be one if Shapiro is the vice president? You have like many, many
markets like that. And it feels like there's like, that's much more tractable than by the CEO
markets. It's just all depends on who makes what market. So in some sense, this game is
wide open. Any substantial player who wants to take the initiative to make some of these markets
can. And so it's a matter of who takes what initiative when. So I don't want to discourage
anyone from any of these options. I want to say, you could just make it happen if you want to.
Sure. Okay. All right. Do you have any other comments you'd like to make?
Well, I mean, if we bring it back to say to AI, we could say, you know, it would just be great if
we could have AI market betting markets on AI consequences, like we have these bills before,
you know, various legislatures for AI regulation, we great to have prediction markets about, say,
the amount of AI activity conditional on these bills passing the, you know, degree of concentration
into these industries conditional on them passing, you know, other sorts of
markers before, you know, everybody dies sort of markers. There are a lot of
pre markers that would plausibly indicate more risk of various scenarios. It would be great to
have markets on those. Yeah, I agree about that. So I'm going to write it down. And that's again,
something anybody out there could do. I've also said, you know, for a long time when people thought
about AI, the biggest problem they thought about was AI is displacing humans off their jobs.
That's still a concern. And it's still something we could do something about.
If only people would get the ball rolling to just set up robots took my job insurance.
I've written about that and it wouldn't be that hard to do. And it's just a matter of momentum to
get some people doing it and get any other people doing it and make it more of a thing.
And so and that would certainly highlight other AI risk issues, to the extent that people were
actually buying that insurance, you can see the price, the premiums for that insurance,
it would give you a probability of those events happen. And the resolution is what AI
penetration in your industry? Like, well, I would start out with just very simple,
you know, labor force participation rate in the US, say for US workers. At the moment,
it's like 60%. If it drops below 30% in a 10 year period, that's probably indicate that's
most likely way that would happen is lots of automation. So I would just have a bets on that,
let people buy bets on that happening within certain decades say and have assets that pay off
if it happens within a decade. And then they would be somewhat insured against that going wrong.
And again, so that doesn't require individual underwriting. That is, you don't want to do the
whole insurance route where they have to review your life history and decide your personal risk.
That's way too much cost and trouble for this. Just set up a generic market in the overall rate
at which people lose jobs. Sure. Okay. I think, yeah, let's run through some of the others.
And then, yeah, I've got 15 minutes, so I will do that. All right. So there's
like vulnerable world hypothesis type stuff, right? Like AI allows you to build a nuclear bomb
much quicker than anyone else can. So the vulnerable world hypothesis is the idea that
as technology improves, generically, individuals can have more leverage over the entire civilization
and put the whole civilization more at risk. So it's a ratio question of the relative potency
of an individual or very small group at causing substantial harm to the entire civilization.
That's the postulate of the vulnerable world's hypothesis that is that that parameter has been
increasing. Sure. I'm just not convinced that's true. I'm not good. So for example,
if you look at mass murders, you could look at how much any one person with as many guns as
they can hold in their bag has been able to kill when they decide to go killing a lot of people.
That number hasn't been going up. Sure. Though, I guess the AI risk argument here is that AI makes
like plausibly AI might bring down the ceilings for some of those things very quickly.
Well, the key point is that every technical change changes both offense and defense. So this
has long been an issue with warfare. People for a very long time had looked at a particular new
weapon and said, aha, that'll make offense more powerful than defense in war when they look at
a particular offensive weapon. And they've neglected to look at the new defensive strategies that
can counter them. And then over time, we don't necessarily see drones. You know,
drones are great. They're cheap. They're going to be used to swarm technology, but then also
lasers are very cheap to fire. And if a thing like launching is sort of hacky and plastic,
then a laser may be unusually, but then of course, you're going to have them covered in laser
shielding. Right. So I think you just want to look at the long term trends in offense versus
defense. And there, I don't think you've seen that much change in the long run. Now, if you,
if you looked at, say, computer security, even there, if you ask, you know, my offense and defense
in computer security over time, even there, I'm not sure we've seen really substantial trends
in terms of actual, you know, harm. Okay. So the next one is, we're kind of touching on this
quite a lot. AI and producer accelerate bad multi-agent dynamics. So this was originally just
DM Joe on Twitter about this and you, you said there was a quote that you said you described
as not very eloquent, but I appreciated it was brief, which is not the case of almost every
other AI quote here. This is about one of your AI models. The model seems to confirm the intuition
that machine intelligence has Malthusian implications, population and wages. I don't
think you see that as a disastrous scenario. Right. I don't. But you do acknowledge that it
like it could push the multi-agent dynamic, the multi-agent equilibrium to a direction
that is more like that. And I guess you acknowledge that many people do are concerned about that.
So, right. So there's, there's this generic pattern here of noticing that you live in a very
complicated world that you don't fully understand where lots of little parts seem to balance each
other. And then you imagine a new thing coming in and you can see that it's going to have a lot of
effects. And you can see plausibly that some of those effects will maybe undermine some of the
things you were relying on. Yeah. And there may well, of course, be other effects that counter
those, but you can't see those very well. That just in general, whenever you've got a structure
that exists, it's just easier to see how changes could undermine the existing structures than they
could add replacement structures. Sorry, I'm just moving my laptop. So then the question is,
and this has happened over and over again in technology, you know, for my entire lifetime
and for a long time before that, just when people can envision a particular new technology, it's
just much easier to envision the ways in which it will undermine things they like and rely on
than to imagine the ways that will replace those or add new things they will like but
haven't envisioned yet. And that's why most technology forecasting tends to be warnings,
tends to be worrisome, you know, collections of, but this could go wrong. If you think not just
about AI, think about nanotechnology, genetic engineering, television, nuclear power,
you know, just a wide range of technologies. Typically, if they're big enough technology,
it's just hard to really see all the, all their impacts, but it's relatively easy to see how
they could just undermine something you are currently relying on. Like with drones,
for example, it's easier to see how your current aircraft carrier might be vulnerable to drones
than to see how you're going to use your drones to do other stuff in offense.
And your response to this, again, is sort of like, to the extent that risk vectors can be predicted,
you could put small costs on those risk vectors, like if they really cause real world harm,
and then more or less leave other things how they are.
Where the more that we can expect to have these changes be gradual and be able to adapt to them,
the less we need to worry about them ahead of time, but the more lumpy the changes might be,
you know, and maybe the more out of view they might be, then the more you should try to do other
things to deal with them, but try to target your solutions to near the actual problems,
rather than, say, try to regulate all AI at the moment. You might have more food liability
where you look at the particular scenarios that could go wrong and you target your
regulations next to that.
Okay, so then we're sort of, the next argument is like it's going to be sort of scraping the
bottom of the barrel, but they're like, I don't know, this is what I'm thinking about.
So the next argument is sort of linked in that they're both very hand wavy.
One is hand wavy in the direction of just like, it's large risks. I don't think you're
necessarily going to say new things in response to this, but maybe you will.
One of them is just like, it's large risks. The large risks we should be concerned about
just like large changes. There's just like an argument from size, just like, this is the big
impact, you want to be concerned about. But that's exactly the point about gradual change. That is,
all small changes add up to large changes in the long run.
Sure. So here, you're right. I think you're going to respond to something like,
yeah, this is concerning if the changes is discontinuous or lumpy, but just the size of
the change just doesn't, like, of itself, the question is how quickly that and how gradually
that change happened. Right. So for example, you know, every day in the stock market,
the typical stock price changes by roughly one and a half percent.
Yeah. Now, one and a half percent change to a stock to a company is tolerable. They can certainly
handle that. But you integrate one and a half percent a day over 10 years, 20 years, and basically
most of these companies are going bust. Yeah. And so as a company, you should definitely be
worried in the long run, these large changes to your company will happen, i.e., most likely,
you will just go bust and die. You should worry about that. But it's the same process,
just integrated over a longer time. Yeah. So if you're worried about large changes,
you should just be worried generically about most everything. They can all integrate to
large changes over time. Yeah. So what you're just more worried about is how quickly
they're happening relative to your ability to track and adapt to them.
Yeah. I guess a thing I think about a bit in regard to this is
I would like more focus on individual capabilities forecasting, personally. Like if we forecasted
specific capabilities and how concerned we were and our ability to regulate on those,
then ideally you would become pretty confident that you could regulate if you needed to on many
of the most dangerous capabilities, but they're there like a distance. If you could see them
coming, right? If you get gradual warning about them, basically small problems happen before big
ones. Yeah. But that's how we've regulated pretty much all technologies we've ever had
is by waiting for concrete problems to appear and then adapting to those concrete problems.
And that only goes wrong for very lumpy technologies and changes.
And it's just not clear that AI is that lumpy. That is comparing, say, I mean, you know,
comparing to lots of other technologies, certainly in history, AI's changes have been
relatively gradual. The history of AI changes so far does not make me especially worried about
enormously lumpy changes. That's true. It feels to me like the chain, the
I guess you could argue that the changes have been sort of like kind of, but like, you know,
you've really had like two or three years moving from what might be considered in the last two
or three years, things that I think would have been considered really like mind blowing and
now sort of. Okay, so let me qualify. There's an economy out there where people are doing jobs
and tasks, and that's where most of the concerns should lie in terms of changes in that economy,
including, say, the military. When researchers do demos off to the side of that economy,
those demos can be more dramatic and lumpy, but that's less of a problem if the transit,
the process that translates those dramatic demos into actual economic activity is more
smooth and gradual and less lumpy. And economic changes have been much lumpier than this. What
would some examples of those be? Economic changes. So for example, there was a moment when we
developed a new way to make aluminum that was vastly cheaper than the old ways to make aluminum,
and all of a sudden people started making lots more cheaper aluminum, for example, right?
There definitely have been moments where particular technologies arose,
and then they made a big difference, right? A particular ancient civilizations figured out
how to make chariots, and then they started winning a lot of wars with their neighbors,
with their chariots, right? And so military technology can be lumpier than other technologies,
but even in the last half century, military technology hasn't been that lumpy, actually,
in terms of its developments. But it's like in the last few years, people have been worried,
did the Soviets figure out how to make supersonic missiles, hypersonic missiles, right?
And like there's some demos, they look like they was really, you know, potentially powerful,
but so far it looks like it hasn't actually made that much difference. That is, and the
military is constantly tracking these things, looking for technologies that might suddenly
make a big difference. But mostly, you know, the translation between an abstract particular
technology like a material or something, and actually integrating with large systems just
tends to make it more gradual and less lumpy. And that's true for not only the military, but for
the rest of the economy. So you can see that in the past, when AI technologies have appeared,
and then as they've been tried to integrate into the economy, it's just been slow and gradual.
And fragmented. And we've been able to track the gradual application of lots of technologies
in the economy. And it hasn't been very disruptive. Okay, we're now on to the last couple. So one is
black boxes. So they're just, we don't know how they work. Like many technologies have been better
understood than this in the past. Many technologies, you know, like, I mean, technologies haven't been
very well understood. Can you give us like, it seems like LLMs are like, pretty poorly understood,
like, you hear that people are building them saying, you know, we don't really understand how
they work, you just sort of like, see what goes better. Most drugs, we don't know how they work,
honestly. Okay, most medical technology, we don't have much of a sense for how exactly it works,
you do an experiment, and it seems to reduce the symptom or whatever. And you start applying it,
but you don't know how it works. And of course, until relatively recently, most materials, people
didn't know how they worked. They met in a new material, and it would bend it easier, it was
cheaper to make or stretched or whatever it did. And people didn't know why they just knew
it worked. And then you started applying it. Certainly most technologies for motivating
human workers and managing their work lives, we don't know why those work exactly how they work.
Sure, corporations have to make decisions about how to divide up tasks and how to organize groups
together and what incentives to offer them and what how to motivate them to work and promote.
And we just have, we're mostly doing that on the fly. Intuitively, we don't have good theory about it.
What's like Robin, she's in the next room.
Okay. Okay, yeah, that's kind of a compelling answer to that question.
Viewers this podcast might find that I'm ill prepared or something, and I don't have to be
otherwise. I have no complaints. I have no complaints at all. This has been fine. We just
spontaneously just to tell everybody, it was less than an hour before this conversation where I said,
hey, you want to do a conversation? And he said, okay. And when we said, okay, let's do it. So
neither of us prepared for this. Neither of us needs to. This, this weren't fine as far as I'm
concerned. I agree. I think, yeah, I think sometimes people expect people to really know their stuff.
And I'm like, no, I think there's too many, there's too many people talking about stuff they don't
really know about. Again, you're mostly asking questions rather than making claims. So the
requirements for asking a question should be much less. The last one, I think,
the last one's probably the one I actually find the most compelling, which is something like
if a mixture of experts and forecasters say that there is like some risk coming down the pike,
and that is a good reason to think that there is risk down the pike.
It's a reason. So the question is how good and what exactly should you conclude from it?
Right. So that look, I mean, you have to go with the history of how much have people ever been
able to tell what's coming down the pike, right? Just not very much. So just in general, humans
haven't been that good. That is, we can actually distantly envision rough outlines of future
technologies that most people don't realize that's actually true. That is, people envision cell
phones decades before there were cell phones. And, you know, I was part of a group that envisioned
the World Wide Web before there was a World Wide Web. And lots of things, in fact, have been
roughly envisioned from a distance. So I do think we should say that people are roughly correct to
envision that, in fact, there will be better than human level intelligence in the future.
It will be cheap. It will be powerful. It may change quickly. Those are all rough guesses
that people are making that I would agree with. And then their final claim is just,
gee, guys, you should worry about this. And honestly, well, yeah, in some sense,
we should just be worried about all future changes, all big future changes that we can envision.
Yeah, there's certainly reasons to worry about them. The question then becomes, what more specific
claims do they make about how we should regulate or manage those concerns now? And then you'll say,
if maybe there's a poll about that, and then I might ask, okay, how often have similar polls
been relevant, you know, reliable guides to how we should regulate coming technology changes?
And for that, I don't think the track record is that good. But nevertheless, you know,
naive observers should still put substantial weight on it, certainly.
But, you know, I just think as an economist, I'm just going to go with the prior here. Mostly,
regulation goes bad when it's not tied very directly to specific problems that you're
addressing that you've seen. Hypothetical regulation, regulation of possibilities that
are not very well understood or conceived, that just doesn't seem to go very well.
That's how I would frame the argument. But I would certainly say it's worth tracking. I'm glad
there are people tracking things going wrong with AIs so that they can point out when particular
problems happen and call them to our attention. And then if we see a pattern of such problems,
then we see that maybe that pattern is due to some, you know, systematic malicious behavior
that could be addressed by regulation, then that would be the point at which you would
propose such regulations. And then I would, I'm much less against that. I would want to look at
the details, but I'm saying, look, if you see a specific thing going wrong repeatedly, and you
have, you know, a thing you think could do with that, I want to listen to that. Now, in the past,
that's also gone wrong. You know, so for example, if we see some examples of insider trading, and
then we ban all insider trading, I might think, well, yeah, you're cutting some problems, but
you're cutting some problems, but you're, you know, you're forgetting all the gains you're giving up
by such regulations, and we want to weigh those against each other. But at least there's a reasonable
case. If you see a set of concrete things go wrong, you've got a case that deserves attention
when you say, hey, we should regulate to deal with it. But when you're thinking decades in advance
of hypothetical problems of systems, you have really very little idea, even what their architecture
will be or capabilities. And you say, we should, you know, do stuff now to deal with those potential
problems. I'm just much more skeptical about that. It's just not the right time and the right way to
do it. Now, if you could, again, if you could convince me of the fume scenario, you could say
that, because that was Yudkowski's story. He says, look, that's fine for other technologies,
but this one, there's going to be this sudden growth. All of a sudden, there'll be no warning
about it. And afterward, it'll be too late. And so the only way to prevent these problems is beforehand,
now, because for as all we know, we are right, almost there right now, we couldn't tell if we
were close. That's the worst case scenario where you might need to do something.
Yeah. So I guess to summarize, it seems to me that of these arguments, you find the loss of this
speed argument, maybe discontinuous speed to be the most compelling. You don't think it's likely,
but if I told you, I don't even know how to ask this question, but there's some
sort of notion of compellingness without likelihood that I don't quite point out.
We had seen a bunch of smaller, sudden unexpected explosions of the fume sort.
And then you said, and that could be even bigger than I might be much more uncertain, right? I
could say, look, here are some smaller explosions that are in a, maybe there's a power law distribution
of them. And you say, look, there's a lot of weight on these much larger possibilities. So,
hey, you should worry about that. I mean, for example, that's why I would worry a lot about
pandemics. We have statistics on the size of pandemics. We do that, we've got a power law that
puts a lot of weight on really big pandemics. And it says, actually, the median deaths so far
is much less than the expected deaths because the distribution that you fit is weighted so much,
the expected value is so much higher than the median you'd see. So you guys say, and that's
same for wars. So you got to worry a lot about really big wars because you look at the smaller
ones, you fit a distribution to it, and you say, look, this distribution puts a huge amount of,
huge ones that we wouldn't have seen yet. And yeah, you got to worry about those.
And so, yes, if for AI, we had these fume-like explosions where suddenly AI's vastly improved
inability in some self-improving way. We saw, you know, small ones, but we fit a distribution,
we go, look, there's all these ones that we wouldn't have seen yet because this distribution
is really heavily weighted to the large size, and you go, okay.
And then I think you also, you also find sort of, you don't necessarily find it a compelling AI
argument, but I think you find it like very interesting the notion that humans aren't
well-aligned to one another. There is some like, larger problem about aggregating preferences of
large groups of people. But I mean, I'm an economist, that's just our bread and butter,
that as I've long since accommodated to the expectations that humans have troubles aligning
to each other, that to me is just the state of the world. Other people who didn't realize that fact,
I guess they should be surprised by it and worried, but that's just social science. That's
what we economists have been doing for a long time, is asking, how can we
deal with those conflicts between people? Sure. And then, yeah, I mean, I guess
plausibly, I'll try and talk to you with some other point about maybe counterarguments or,
I mean, I think we've gone through a number of counterarguments, but there might be some sort
of global counterarguments, which I think it's also worth to try and get people's sense on.
Do you think there's any other arguments that you find compelling or sort of, they're not,
they're not really sort of mutually exclusive arguments. I think a lot of there's,
there's a little up here, but any frames of arguments that you think has been missed here,
where you think actually there's, I have had this argument that I find like,
maybe similarly to the speed argument, you don't think it's likely, but you find if there had been
certain things and it would be very compelling, you know, like it is, there are any frames of
arguments that you think. I mean, I think it would be worth doing a lot of thinking about
future AI. I'm just less interested in the framing of risk in terms of should we just shut it all
down as the sort of the main policy question. I'm much more interested in just trying to think
through how it'll play out and then preparing ourselves for those different ways it might play
out. So for example, I think a world of AIs is just an interestingly different world of natural
selection, wherein they will be borrowing pieces from each other right and left. And then we want
to understand what's sort of the structure of this world of what is the unit of organisms or
species or whatever in this world. Is it a world where it's basically all one species,
because all of them can borrow and include design elements found by all the rest. And
that's worth thinking through is just basically what does natural selection among AI look like
by comparison among DNA, by comparison with human economies. And you may well find things to
worry about. So for example, I have this book, The Age of M, as you mentioned. I think the first
priority is just try to think through scenarios and how they would play out and then ask what are
the things to worry about. I think just trying to go straight in with like, can I identify
things to worry about before you've worked out the scenarios, before you've filled in a picture
of what it looks like is a mistake. So I would be happy to help people think through how does
the scenarios play out. And then in that context, I don't at all exclude the possibility you might
find things to worry about and maybe even things you could do something about now. But
my perception is there's just way too little of that sort of working out the details of how it
would play out compared to say what I've done in Age of M. Yeah, that seems plausible to me. I think
personally, I'm probably most interested in trying to build those for sort of three to five years,
because I think our forecasts in that range are much better. I'm hardly worried at all about three
to five years. I think hardly anything will happen in three to five years. So, but that's based on
my expectation that, you know, even dramatic research demos take a long time to feed into the
economy. Yeah, I think your view is underrated by, Dario gave that thing saying that in 18 months
you'd be getting AIs to organized birthday parties for you, and I do not believe that.
No, no, I'd be happy to bet against a lot of these strong claims, but you know,
that people aren't, they're much more interested in making the claims than betting on them.
Yeah, yeah, sure. I mean, honestly, I think in say five years, this will all look pretty bad for
AI risk people because they will have been seen to cry wolf when not much will have happened.
Yeah, some should be preparing for that scenario. Yeah, I think that seems plausible to me.
I don't really, I don't really necessarily like this, but to the extent that many such people
are involved in regulation and governance, that probably won't be damaging to their
regulatory or governance careers. Oh, probably not. That's not how that works, unfortunately.
Which I think is a shame, I think I'm going to say.
Right. And in fact, probably it would be better for them to be involved in whatever activities
there are, even if they're based on misleading assumption, then to have not been involved at
all. People who are not involved would just have much less experience on the resume.
Yeah, maybe. Well, on that happy note.
All right. Thanks for talking, Nathan.
Yeah, thanks for talking to Robin. And yeah, I might think, well, I'm sure I'll think more
about this. And yeah, I appreciate your work. I think you're one of the most generative people
I'm aware of. And I really appreciate the style of Age of M. I think it's, I think
too few books are written, which really mathematically, sociologically take seriously a premise and
just sort of run with it and run to even places where it seems sort of kind of ugly or uncertain.
And I think if there were, you know, a thousand times as many books like that, then,
you know, a few of them would be really, really tremendously useful, I think.
We know a lot more about the possible futures.
Yeah. Anyway, have a lovely day. Thanks very much.
Okay. Take care.
Yeah. Bye.

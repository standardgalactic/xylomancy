All right, we're gonna jump right into this.
So this is the results of the best amount
of research that I can do.
And now when I do research R being the,
you know, the research being the principle meaning of R,
don't put rust out there again.
Why are you pulling up rust right now?
We're not talking about rust.
No, we're not.
So anyway, so by the way,
my feelings on rust have totally changed
within the last few days
because of the whole other thing
that we've been talking about.
So please don't, we're not gonna talk about that right now.
I did talk about rust a little bit when we,
yesterday because we were talking about Firecracker
and how I think rust could be kind of the golden child
of the micro virtualization emerging,
but that is unrelated to what we're gonna talk about
right now.
So I'm gonna try to look away from the chat,
but let's try at least for the next 45 minutes
or 90 minutes to stay focused on one topic
so that we can have this YouTube video
be kind of to the point.
What is going on with containers
and specifically Kubernetes containers in 2022?
And we're about to start KubeCon on Monday.
So there's gonna be a lot of other new content
and announcements I'm sure,
but this, what you're seeing on the screen right now,
and by the way, if you wanna join me,
let me post a link one more time.
If you wanna join me and move your pointer around
and participate, you can, you're welcome to do that.
This is a read-only Excalidraw thing.
I'm a huge fan of Excalidraw, full disclosure,
I was gifted a membership to Excalidraw
from one of our community members,
and I never look back, it's so amazing.
I've also been posting the PNGs of this to the Discord.
So if you wanna see those, you can go there,
but you should be able to use the URL anytime to come to it.
Okay, now this, hey, look at all these,
these fun little arrows.
So what I'm gonna start by saying is a disclaimer.
I consider myself, I mean, I've had people call me advanced
in Kubernetes compared to the average Kubernetes admin,
and I don't feel that way myself.
I don't know if it's imposter syndrome or what,
I feel like, yes, I've installed several clusters
and I administer one,
but I still feel like I'm always learning something
that I should have already known.
And this video right here is about that.
It's about one of the things that I,
every time I think I understand it,
I like get introduced to a new piece of the puzzle,
and I'm like, oh, oh, so that's how that works.
And short of looking in the source code,
which I am very prepared to do,
this is the best that I could come up with.
So I say that as a disclaimer
because there might be some glaring mistakes in here.
And if they are in your YouTube, just let me have it.
Write it in the comments, say you got this wrong
and you got this wrong and you got this wrong.
And I'll go in and I'll fix it
so we can have our hopefully corrective version of this.
And the reason I put that out there
is because in order for me to even come to this knowledge
that I am showing you right now,
I had to sift through a ton of bad information
on the internet, bad information, misuse terminology,
confusion, renaming, reorganization of projects,
and all kinds of stuff.
It's a pain in the butt to get to this.
So I'm really glad that I was able to leverage
my research skills to get to the right thing.
So the overall topic for the next 45 to 90 minutes
is gonna be Kubernetes containers in 2022.
And let me ask the question.
First of all, why do you think you know this?
And please, if you have input, please let me know.
Why do you think understanding container engines
in 2022 is important?
Why?
Why is understanding container?
I mean, it's pretty obvious to a lot of us,
but what's your best way of succinctly saying that?
Why do you care about the container engine
when you're deploying Kubernetes?
Why do you care about Kubernetes in general?
Why do you care about container engines in general?
Does anybody have any input on that?
You can go ahead and throw it out there in the chat
if you want to.
But I'm gonna submit to you that the reason
that you should know about Kubernetes container engines
is because they are the very basic elements
of everything that you use in Kubernetes.
They are, Kubernetes is described
as a container orchestration framework.
So if you don't know what containers are,
then you're gonna be totally screwed up.
So we should probably talk about, yeah,
we should probably talk about
what containers are versus virtualization.
We talked about that a lot last time
and about how I think that the future,
in fact, we have that in a different diagram.
I'm just gonna refer to it really quickly.
In my air gap home network yesterday,
we were talking about, this is my conceptual thing,
we were talking about Kubernetes,
so here's gonna be, I've decided to do
a TALOS cluster over here out of OptiPlex servers
if I can get PixiBoot to work.
We have open PBS for batch processes
and then we have a traditional
on-prem Kubernetes cluster here.
And then we have all of these VMs,
potentially micro VMs on our single VM servers.
So containerization and virtualization
are important that you understand.
I'm not gonna dive into that right now.
So we're assuming that you actually understand
the difference and why they matter.
And so we can jump right to the point
where we talk about, what do we talk about
just about Kubernetes?
Okay, so where did that come from?
Did somebody draw that or did I draw that?
Did I draw that?
I might've drawn that on accident.
I did.
Or did somebody else?
All right, so, no, Docker Compose is not a part
of the diagram because this is Kubernetes containers.
Who's drawing on there?
Did I give somebody draw access on accident?
Or did I do that drawing?
I need to make sure that I didn't allow that.
It might've just been a different color.
Do, do, do, that should not be there.
I feel like something didn't get saved somehow.
Yeah, that should not be there either.
I don't know what happened there.
Oh my God, it reverted my whole diagram.
It did.
It did, it reverted my diagram.
This is a different version of the diagram I had before.
It's got a lot of stuff on it that wasn't there before.
I'm kind of worried about that,
but it's good enough we can go with it.
There might be a bug in Excalidraw.
So, let's go back to it.
That was weird.
That was really weird.
All right, so back here we go.
So, there are some strong,
there are some really strong opinions on this diagram,
and I challenge myself to justify those all the time.
If you wanna challenge my opinions, please do in the chat.
You can do it right now if you want to,
or in Discord if you don't wanna do it,
like right out of the gate.
But what I'm gonna start by saying
that this is Kubernetes-specific,
so none of this has anything to do with other stuff.
So like, for example, Docker Compose, and any of that.
So, and I think it's super important,
especially now, that we start to think of
containerization without Kubernetes as a separate beast.
And let me, this entire diagram is completely focused
on that separation based on the container runtime interface
as defined by the Kubernetes project,
which was led by the Kubernetes project,
but it's not necessarily just a container thing.
It was something that came out
from the Open Container Initiative.
So, let's talk about all of this
and why this is relevant.
So this, first of all, is for orchestration,
and by orchestration, we've primarily been Kubernetes,
but it could have easily just been an Omen or anything.
So I'm gonna try to get into this
by telling you a number of stories.
So the first story is how did Kubernetes start?
So Kubernetes, it was really quickly,
Kubernetes started as a way
to manage Kubernetes clusters, I mean, containers,
and to run them in a way that could be easily started up
again and all that.
But the original version of Kubernetes depended on Docker,
and Docker has always been a proprietary thing.
And that was well and good.
But then in 2017, they created the people behind Kubernetes
came together and said, you know what?
Kubernetes probably shouldn't be depending
on this ever-changing underlying deep low-level API
from Kubernetes, we should probably standardize that.
So they created something called
the Open Container Initiative, or OCI,
and that led to the creation
of the Container Runtime Interface, which is this.
So the container on CRI defines the API
used to talk to container engines.
The CRI API comes from the Open Container Initiative
formed in 2017 to deal with problems from Docker
being proprietary and non-standardized.
As of Kubernetes 1.24, all container runtimes
must provide a full CRI implementation.
And if you ever read anything about Docker shim,
I mean, or any of these things, that's because,
and I'm gonna read it right here,
Docker is a significant decline
after failing to respond to the needs of Kubernetes
forcing the creation of Docker shim,
and later hitting the industry with a bait
and switch tactic to force enterprises to pay up.
So Docker itself is in massive decline.
Nobody's using it, people are abandoning it
like crazy enterprises like mine,
which is a huge multinational corporation
have decided to basically banish Docker
from their entire enterprise.
It's in massive decline.
The founder of Docker has left to pursue other interests.
The company was already in financial peril before,
and it's just a matter of time before it goes under.
That being said, it is still holding onto
the maintenance of the number one container engine
recommended in the certification exam for Kubernetes,
according to the official Kubernetes.io site,
which is Container D.
Now those docs are kind of old.
Those docs were created a while ago,
and Creo, which is coming out of the OpenShift world,
has really stepped up,
and it doesn't have as many bugs as before,
and it's extremely lightweight.
We're gonna talk about that in a bit,
and it's a full implementation of the CRI and nothing more.
So the Container D actually has a lot of extra bloat on it
because it tried to be more,
and that has an advantage or disadvantage,
but the ultimate decision in 2022 is, in my opinion,
is whether you use Creo or you use Container D.
That's really, Kubernetes is an orchestration platform
for Kubernetes, right?
People have been saying Docker's in decline for six years.
Yeah, well, I can tell you that the company
of the significance of the one that I work for
and the other ones that I've heard of
who've decided not to allow Docker on the desktop ever,
and not to depend on Docker CE,
has actually been executed.
I'm in the process of doing it right now.
So, you know, you can see it.
And so there's a number of reasons, even most recently,
so Podman desktop, let's continue the story.
So Kubernetes got started,
Docker was a good player for a while,
and then everybody's like, no more Docker,
and they've been trying to get off of it,
but Docker had such a stranglehold on the whole industry
that they've been really holding on to it.
And all of the other players
weren't really that well implemented.
Krio had lots of problems like two, three years ago,
two years ago, and it's since kind of come around
from what I can tell.
And so now everybody's like, no, we're gonna use Krio,
Krio is the standard thing to use,
you should use Krio, it's the best thing.
But more importantly, underneath all of that
is this thing that used to be called libcontainer,
which is now called RunC,
which is the thing that actually does the container creation.
It's the low-level library, go library,
that talks to, I think it's RunC in C or go, either one,
but it's the thing that actually does
all the container creation and stuff.
So that's really the biggest piece of this whole puzzle.
And actually containerD used to use libcontainer,
it was an ancient blog I read,
used to use libcontainer, and then I read someplace
and I wrote this here,
whereas the containerD is still maintained
by the Docker company, it officially requires
some Docker C packages to be installed.
The RunC container runtime engine was once libcontainer,
causing some confusion when researching
the actual container source libraries
used by the container tooling.
So if you're like me and you like to get
into the low-level details to see, okay,
which libraries are the most popular,
which have the most stars,
which are getting the most usage,
and what are up the chain,
what are the products that are using that,
what are the projects that are using it,
then it's really obvious right now in 2022
that anything that's not using RunC
is gonna be completely irrelevant
in this space at this moment.
So in my opinion, RunC is where it's at.
And if you're not using RunC,
just, I mean, technically speaking,
Creo, which stands for the container runtime interface
dash open or open shift, depending on who you ask,
I've read both in two different places,
was created up by Docker, I'm sorry, by Red Hat,
and is meant to just fulfill the minimum elements
of the container runtime interface.
So let's read about the container runtime interface
for a bit.
So the CRI defines the API I use
to talk to container engines, I talked about that.
Okay, so Creo, CRI dash show,
which is either open or open shift allows you
to run containers directly from Kubernetes.
Now, it was designed for use by Kubernetes,
but they were very careful to not say
this is only for Kubernetes, right?
So this is a container runtime interface
which defines a set of operations.
So you might be wondering, well, what is it really, right?
So it's basically just, you can go read the spec,
it's really long and boring,
but it defines a set of operations
which are implemented either through, you know,
direct calls or through, you know, API calls or whatever.
And the operations that must be implemented are,
create, start, kill, delete, and state.
And you can go read about these in the specification,
but those are the only things required by the CRI,
by the CRI standard.
And that is kind of an interesting thing.
And I've discovered this through the hard way.
So there is a tool that now ships with Kubernetes
called CRI CTO.
This tool does not ship with Creo.
There's a dependency on it,
but it's designed to be a part of the Kubernetes space.
And as I said, so there's no,
like an interface and programming, yeah.
So there's, you know, the Kubernetes project
is very good at not putting their foot down on something.
It's kind of annoying.
So like the open container initiative
was maybe kicked off by Kubernetes,
but it didn't necessarily mean all things Kubernetes.
So everybody's following it, right?
So they're all implementing the CRI,
no matter what, to talk to containers.
And to tell you the truth,
there is actually the hope that I think
that maybe BSD containers at some point
could implement the CRI.
And if they actually do that,
then that means all of this cloud native hardware
and every software and everything could run on BSD.
And that it, there is nothing that says it has to be,
you know, LXC, you know, Linux specific containers,
as far as I can tell in the specification
of how to make the container interface work.
So it's very possible that some day we might be able
to do that.
Now there's a whole big problem
about how do you store the containers,
you know, and this little green dots here.
I mean, these green things here, you know, and that thing.
So I don't want to get into that rabbit hole,
but there is some standardization here.
So the, if you want it, so the Cree CTL thing,
which I, as I understand it gets installed
by Kubernetes packages when you're installing those things,
allows you to talk directly to,
I think it's RunC.
I don't think it is directly to the KubeWit.
I can't remember.
This one I might have wrong, actually.
I think that this might, does anybody know?
I think that this might actually not be here.
I think it might be to, I think it, I had some,
at one point I had it pointing to the KubeWit
and then I had it pointing to the RunC.
I think it might actually be able to talk to both
because I do know that when the KubeWit is not running
because the KubeWit crashes, right?
You can get on CRCTL and you can run CRCTL list
and you can list all the pods on the machine,
whether or not they're in the KubeWit.
And pods are things, static pods that have been defined
on the host, that's what all of that stuff is.
And they are just defined in configuration files
that point to containers that are running via RunC
on the machine.
So you don't have to have a KubeWit.
CRCTL I think can talk to the KubeWit
but it goes directly through RunC
to the containers on the machine.
But it also has knowledge of the configuration files
that the KubeWit has access to.
In fact, that is probably something I wanna add here.
Let me go ahead and add that really quick.
So I think it would be safe to add
like a configuration file icon here and stuff.
So, you know, where's that doing it?
Where am I?
I don't know.
So yeah, I think that let's do that.
So that is the, why is that the fill?
Oh, yeah, we need to do, let's do white on this.
I'm almost positive that that's true
because it does not communicate with the KubeWit.
I know it doesn't need the KubeWit to be there
because the KubeWit is not there
when you do KubeADM in it.
But it can look at the file itself.
So this would be the configuration file.
So I'll add that in there.
What's that?
Those are possible options for components.
Is that a minimum you need?
When you're talking about a KubeWit endpoint,
you need a runtime, you need a KubeWit,
you need a runtime engine, I should say.
You don't need, and this is another thing.
There's a terminology regarding a runtime
and that people have said it doesn't have a runtime.
I've heard that statement three times
and I restated that to some other team today
and I was wrong.
I further did further research on that
to say that Podman does not have a runtime
is a false statement.
So this is very, very frustrating
because this terminology gets misused all the time
and I have read it at least in three places
since I repeated it incorrectly.
Yeah, but what is Run C?
Run C by definition,
and I have at least three or four sources of this
is a container runtime.
So I've heard at least five people tell me incorrectly
that Podman, as far as I know,
that Podman is a container runtime
and that's not true.
And they've told me that Creo is a container engine
that doesn't have a runtime
and that I cannot overstate how wrong that is.
And I just repeated it incorrectly.
It's using completely different wrong terminology.
So let's get the terminology right here.
So according to the documentation,
Creo is a container engine
that fulfills the container runtime interface.
And it does so by calling into Run C
which fulfills the CRI stuff.
Now, container D, which if it's not confusing enough
has its own component called CRI
which also fulfills this CRI and its own way.
And I've read two different blogs
that said container D doesn't use Run C
and then I read another two blogs that were newer
that said container D now uses Run C
and then I put it together
that the other ones were saying
that it used container lib or lib container.
And then I actually read another blog that said,
actually Run C is lib container,
it got modernized and that's its new name.
So I just saved you like two hours of research
to get that down if that turns out to be true
because that was so frustrating.
I was like, what the hell are we talking about here?
So as of this moment, knock on wood,
I think it's safe to say
that all of the major container engines
that are used by Kubernetes all use Run C.
There is no other competitor to this.
And so anyway, this is the problem with blogging,
as it gets all outdated and everything.
So I was actually really happy to hear that
because that simplified my graph
because before I had to have another piece here or something.
There was another thing that fulfilled the CRI
and I found out, oh, it's just Run C.
Is Nerd CTL only for container D?
Show me that in writing someplace.
Show me that Nerd CTL can't talk to CR to Creo.
If you could show me that in writing,
I'd be happy to change it.
Cause I think that's what that is, right?
You can use CTR or that, right?
I think that's right.
If I can, I didn't do any research on Nerd CTL at all.
I've just heard people talking about it.
I don't plan on using it at all.
I just let it put it there.
So if you know for a fact
that that only talks to container D, please let me know.
There is a CTR command that's documented
in the Talos documentation.
It's also documented in the QADM,
a NIT documentation on the official community site.
So in fact, a container D is still, as I said,
is still front and center when it comes to certification.
It doesn't have anything on the certification
preparation materials for Creo at all.
This is the Nerd CTL Docker compatible CLI for container D.
Docker compatible CLI for container D.
Okay, well, that actually really helps.
Thank you very much.
That's why I'm doing this.
All right.
Yeah, I didn't even go there.
I didn't know I didn't read anything about
the Nerd CTL at all.
Little or nothing.
I just put it on here
cause people keep recommending it.
So that's good to know.
That's good to know.
So container D is still a really strong container
but let me tell you the reason I'm not using it.
Container D is still maintained
by the Docker company officially
and requires some Docker CE packages
to be installed causing confusion.
I did this last year.
Last year, I installed container D as my runtime engine
and it forces you to do, you know,
as soon as you pack, install or whatever you want to do,
I had to install Docker CE in order to use container D.
And I was like, what the hell?
Cause there were packages from it
and they have never, as far as I know,
they haven't separated them.
So when you read the documentation
for installing container D
because it's the same thing that's used in Docker now,
you'll read all this conflicting information
about how to install it.
It turns out that I had to,
yes, I had to follow these installation instructions
and then I had to go in and change all of the defaults
that were installed by Docker CE
so that they would use only container D
and that step was a pain in the butt.
It took me like a hour to do it for like a year
but it took me like an hour to do it
and container D filled the same role.
Well, I know that's what we're trying to get at.
So the way of JS libraries
is if I feel like the next best thing
has come out every week, yeah.
Yeah, well, the effect of it is,
is every time they do that,
they also complicate things,
but I mean, the abstraction of the interfaces,
the CNI as well,
which is the networking part of all this,
we haven't even got that yet.
That completely complicates it even more.
So you have to know about these things.
And then we got micro virtualization
coming on the scene too.
Anyway, so this is my attempt to simplify this
and make decisions for myself
and to stand by my recommendations for other people.
That's, this is something that's very rare
to find in our industry.
There's a lot of people that will tell you
all the possible ways to do things
and they'll have very few opinions.
And I'm gonna tell you why I chose to do these things.
I mean, Talos, of course,
all of this stuff is irrelevant, right?
Because it's all black box, it's all hidden from you.
But there's other considerations for that.
So if you're installing your own on-prem Kubernetes,
you need to know about these choices.
And so, I mean, I still think it's extremely confusing
that the name Podman has nothing to do with pods at all.
Why did they name it Podman?
I need to drink after that statement.
Red Hat, can you please change the name?
That'll just confuse us all even more.
There's nothing to do with the Kubernetes pod
in the program Podman, not at all.
There's not a single reference to pods at all.
It just happens to be misnamed.
It has pods, but they're different from Kubernetes.
Oh, that's nice to know.
Okay, all right, I'm gonna update my,
I gotta make my thing more accurate then.
Actually, Podman,
as if it wasn't confusing enough,
Podman does have Kubernetes,
does have pods,
have pods,
just not Kubernetes pods.
That makes it even worse.
That makes it even worse.
I mean, the foundational concept is the same,
of course it is, but gotta help you
if you think you're gonna be able to go check
all your pods on an endpoint with Podman.
If you install Podman on your endpoint,
this is another thing that's really dangerous
because I did it.
I'm about to tell you another story of something I did
that I learned from that I'm gonna save you some time on.
So it's like, so I was going through
my Kubernetes installation and I needed to get a container
in order to use Vault.
No, in order to use
QVIP, what is it?
Virtual IP, QVIP.
In order to install QVIP, which is a load balancer, right?
I had to have a running Docker instance,
you know, a container engine on my endpoint,
but I wanted to install it on the control plane
and my control plane, I had Creo on there.
So I'm gonna tell you my sad story, okay?
So don't be me.
I was like, okay, I picked Creo,
I picked the lightweight engine, everything,
and that's where I learned this important lesson.
Now, just because something is CRI compatible
does not mean it's going to have Run or Exec
or any of the other stuff that you expect
when you're gonna use Docker or Podman
or any of these higher level tools.
So what happened?
I went in there and the documentation for QVIP says,
either use CTR to install the container
that then you can then run,
which will then install the software.
You have to have a container engine running
and Docker, you know, quotes to even install
this static pod, which makes no sense at all.
But that's, in fact, I have soured on that entire
QVIP thing now so much by those,
because of those decisions.
And there's a huge thread on their support channel
about how confusing this is for new users
and all this other stuff.
And now the fact that Creo is like the leading container engine
has further complicated the issue
and they don't even have anything there.
In fact, everybody's pushing for the QVIP project
to just put examples of the config files for static pods,
which just have config files on the API
to just leave that and just let people figure it out
because they don't need to install this container runtime.
So because of all of this crap,
I mean, I lost two hours on this shit.
So I'm saving you some time here.
So I was like, okay, I'm following the instructions
and it says that I need to install,
it says I need to install a Docker engine
or something like that, a container engine.
And then I needed to run it
as a way to boost traffic manifest system.
Yeah, I agree.
And so it either lists CTR or Docker.
That's it, CTR or Docker.
So what did I do?
I assumed, well, okay, I'll try.
I didn't have CTR because I didn't have
a container runtime engine.
So I made a bunch of bad guesses.
And the first bad guess I made was,
oh, I'll just use Cree CTL because it has a list command.
It shows me all of my containers.
It shows me all my pods.
And I don't even have to have a running queue,
but this will be awesome.
I'm sure it's Docker compatible.
And, no, not yet, we will.
And so I went to go see if it would work
and it didn't work, obviously, why?
Because Cree CTL only deals with pods.
And pods, yeah, sure.
We had some people tell, oh, it's fine, it does containers.
No, it doesn't do containers unless they're a pod.
So if you've written all the configuration file
to turn a container that's on the system into a pod,
yeah, it's happy to play with that.
And it'll do it, you can run it.
You can start it, you can't run it.
You have to start it because it only implements
the minimum necessary for CRR,
which is create, start, kill, delete.
There's no run.
Okay, so that was the first mistake I made.
I was like, I cannot do this.
And then I had a whole bunch of people on the stream
telling me, yes, you can, you just have to run into a pod.
I'm like, I just wanna run a stupid install script here.
Why did they not give us an installer?
Why did they not give us an install script?
Why didn't they give us config files?
And everybody came up with opinions that are like,
oh, blah, blah, blah.
And I'm like, look, I'm the new peer.
I just wanna use this thing.
And I about threw it all out
because just because of the bad installation steps
because of all these complications
that they didn't even think about.
Yeah, create your YAML and then you're good.
You just copy and paste it.
But it doesn't say that in the documentation at all.
It says your best way to do this
is just go run the container and do this thing.
They could have just provided like a bash script
and been done with it or any of these things.
They didn't do any of that.
And that frustration was enough
to almost send me down the HA proxy path,
which is completely separate from all of this stuff
which runs under its own daemon,
which I would probably run as a VM
completely outside of Kubernetes if I ended up doing that,
which is a load balance.
It's a different topic, but the accuracy,
I think they should.
I think they should,
a lot of people agree with it.
They should do that.
So anyway, I mean, I lost hours on that.
And so did everybody else that was watching this.
And I'm saving you the time by looking at this
and knowing don't do that.
I actually, it was so annoyed by it
that I actually read the entire,
I briefly scanned, I should say,
the entire CRI specification to see why this command
isn't in there.
And that was when I uncovered
that only create, start, kill, and delete
are truly supported.
They're the only operations required by the CRI
by the entire specification.
So I was like, okay.
So mistake number two that I did, mistake number two.
I said, okay, well, if I need a container runtime engine
over there, and I need to get to these containers,
I need to be able to do the,
I need to follow the instructions,
which are Docker run, right?
Or CTR run.
I'm like, huh, well, there's no Docker.
There's no CTR.
What's my next best guess?
Podman.
So, I mean, I was like, well,
Podman is supposed to work with everything, right?
It's CRI, it's got CRI built into it.
I'm sure that will work.
Wrong.
So I installed Podman onto the Kubernetes plane
so that for one reason only,
just so that I could run the container
that would then start and install my QVIP, right?
And it worked.
It worked.
It installed it just fine.
But then later on, I started noticing
that what the Podman was showing me
was completely different from what Cree CTL was showing me.
And I noticed that they were looking at different sockets
or different services and things completely entirely.
And then I realized a very, very important lesson.
And I cannot overemphasize this enough.
Do not mix your Creo container engines
on a Kubernetes node with any other high-level
runtime management tool.
Just don't do it because you cannot be sure
that they're gonna use the same thing underneath.
In fact, the very nature of the engine makes it
so that they have radically different ways
of implementing things.
Container D has its whole socket thing.
Creo has got its own service thing.
And you know what I'm saying?
It's like there's just a ton of reasons not to do that
because the way that containers are implemented by the engine,
even if the underlying LXC stuff is the same,
because that's what this is all into, right?
But that middleware between the person talking
to the container or the thing talking to the container,
the person or service, and the underlying LXC execution
of the containers, the stuff in between there
can radically differ.
And this has been the whole area of the fight
between Creo and Container D.
Container D is insecure.
I mean, they fixed a lot of it now,
but Creo does not have, privileged,
it doesn't have a socket that's running its route.
All the things that Docker just got beat up over
for just doing poorly,
like inefficiently, poorly, and insecurely,
Docker just really messed up.
And they just never intended to fix it.
And so Container D has tried to kind of fix
all those things over the years
and say, well, yeah, okay, we did that wrong.
Let's, if you want to do it this other way,
do it this other way, but it's not the default,
blah, blah, blah, it's a pain in the ass.
The bottom line is that the stuff
in this middle layer here, right?
The stuff right here and this middle layer is so different,
depending on the stuff in the top layer that gets installed
that you just don't want to mix the two.
Do not cross the streams.
If you do, you're going to get burned.
And I'm going to read the statement down here
that goes with this.
I'm going to zoom in on this a little bit and read this.
But this summation down here with the explosion
is my conclusion about this.
So with Creo Containers,
the Creo Containers must be put into pods
to run them at all, all right?
So this is considered good
because using a container runtime for anything
but Kubernetes on a node, on a Kubernetes node
is considered bad practice.
In fact, when I suggested on the stream,
while I was doing this,
that our company regularly does this
because they don't allow users to have their own Docker
and they regularly tell people
if you need to run Docker for anything,
just go get on one of the head nodes
and run Docker straight up on it
and just reuse the Docker container engine on there.
That's also running the control plane containers
and pods on there.
That is like a really bad offense.
It's not the kind of thing you're going to get fired for yet
but when I mentioned that on the stream
that people were doing it,
people were laughing their asses off.
They were freaking the fuck out.
They could not believe that any company
would ever allow that
because it's not only is it insecure
but it's just asking to blow up.
It is, it's asking to blow up.
It's absolutely horrible to do that.
Now, I tried to make the case wrongly
that well, what if you only have one machine
then you have to decide
and maybe you want to share both of them over there.
You know what I mean?
And the fact of the matter is
thou shalt never ever use your Kubernetes nodes,
the container engine on your Kubernetes nodes
for anything but Kubernetes.
Here's some VMs on ES6, of course.
So yeah, right.
And so let me just continue reading this.
So it's considered very bad practice today.
It is also a bad practice to install Podman
or any container utility on a node
because the engine used may not coincide.
And it may, it'll still use LXC down on the bottom.
It has to, but the stuff in between there
is totally different, whether it uses a service
or a daemon or a container.
So it's not gonna be what you would expect, right?
And so as you become familiar with the internals
of how, you know, Creo uses its thing, which is a service,
then you know that how to do that
but a container to use as a socket,
which is totally different.
It may be that there's multiple options for both of them
but the point is, is that you can't be sure
that your app installer, DNF install Podman
is actually going to do the same thing
that you used when you installed your container runtime engine
which is a totally, whether it's containerity
or it's the other thing.
So do not mix the two.
If you mix the two, you're just asking for pain and hurt
even though you think it might be saving something, it's not.
It is also a bad practice to install Podman.
As a rule of thumb, use kreectl for pods,
containers on a note,
and I said the word pod there on purpose, right?
Kreectl cannot directly access containers, period.
It doesn't have to go to Kubelet.
That's why I drew that little picture
of a configuration file.
So if you have static pods on the machine,
it can talk directly to those pods
but it has to have knowledge of those pods.
That means it has to see the configuration file
to know how to wrap up that container
and put it into a pod and then access it using a pod.
And of course you can't use kreectl on the endpoint
without Kubelet.
That has to have the API as far as I know.
So as a rule of thumb,
use kreectl for pods, containers on a note,
everything on a note for everything on a note.
And Podman, despite the horribly confusing name
or nerdctl for non-Kubernetes containers.
So we can use containers are still very relevant.
So people were asking me about Docker compose,
why that isn't on here and stuff.
Because Docker compose and these top-level things,
these top-level things up here,
they're nice and good and all
but they're not really, these things up here,
these things do not have to be associated with Kubernetes.
But that is an entirely different approach to containers.
And I really wanna emphasize that in 2022,
the best thing you can do for yourself as a beginner
or as a veteran is to think of Kubernetes containers
differently than everything else.
Because if you do, you'll save yourself.
You say, well, so they're all just containers.
And yeah, okay, they're containers at the lowest,
lowest, lowest, lowest, lowest, lowest level.
They're all using LXC.
Okay, that's true.
But everything in between there and you
is totally different depending on whether you're doing it
for Kubernetes or you're doing it for Docker compose.
And to save yourself a lot of pain and suffering,
I cannot overstate this enough
because I went through hours of it
that you need to separate those concepts in your brain.
And should you learn Docker compose?
Yeah, should you learn Docker swarm?
I don't know, maybe not.
But Docker compose is not bad.
And you can do podman compose too, right?
But those are things that are for a totally different approach.
And frankly, we need to have, in fact,
from now on, I'm gonna refer to them
as Kubernetes containers
and probably Docker compose containers.
Because those to me, that's the biggest separation, right?
If you're using Docker compose
or you're using podman compose or whatever,
but Docker, as soon as you were Docker and compose,
you're like, oh, we're talking about that.
You're talking about all the container wonderfulness
that doesn't have anything to do with Kubernetes.
But when you start talking about Kubernetes containers,
you're talking about a different beast altogether,
even though some of these things have similarities
at certain points in the architecture.
Now that seems like a lot to say,
but I really feel like I finally understand it.
And I just had to capture that really quickly into a video.
That's all I have for this specific video.
And hopefully that will save you some pain and suffering
as you go about your Kubernetes admin installations
and all of the other stuff that you might end up wanting to do.
This diagram is available, anybody to come see,
I'll put it in the Discord.
And I'll be using this diagram to make sense of my world
as I go about the installation of my,
the next thing I'm gonna be doing,
I'm gonna be installing these clusters.
And I, did we say this already?
This, I think it saves automatically actually.
Supposed to anyway.
But so yeah, so this, the next thing I'm gonna be doing,
which I may or may not do today,
is I'll be installing some other Kubernetes clusters
over here and we're gonna go back to using QBDM for that.
I do need to solve all so I can get some PKI root CA stuff.
And I need to get my head around
how I'm gonna manage VMs on this machine
because I'm gonna be running a Cordeon S&D,
Key Cloak and Vault and all that stuff
before I go forth with my QBDM installation,
which is probably not gonna happen tonight or even tomorrow,
but I am going out of town for next week.
Yeah, and you like Vault?
Yeah, so just to give you an overview of what's next
and why this might relate.
Oh, we use build a lot.
Yeah, so build a, build a is actually built into Podman.
That's another thing I like about Podman
is the build is built into,
and build is just, it's all it does
is just build the images, right?
And Scopio is another good one
that will transfer containers between registries
and stuff like that.
So yeah, just to finish out the idea
about where this is going.
I keep drawing on here because I'm still,
got that selected.
So the next steps are probably,
since I got my whole VLAN all set up,
I'm so happy about that.
There's a video on that if you wanna watch that.
That we're gonna go ahead and, where does Istio fit?
Istio is just a service mesh.
Yeah, so we are gonna do Istio,
but it's gonna be like the last thing.
The main thing Istio gives you,
it gives you lots of things,
but the main thing it gives you is like virtual machine,
virtual servers that have domain names
that get crud, little balances of, yeah.
Yeah, we still have to pick a CNI and all that.
So before, so a couple of changes
just to update everybody who hasn't watched.
So this is gonna be the towels cluster over here.
Right now I'm doing some testing offline
to see if I can TFTP boot
one of the Dell Optuplex machines that I already have.
There is some problems with the Pixie boot
and it requires you to push F11 to do the booting.
So that does annoy me a lot actually.
And because I can't, I don't know.
I'm really tempted to just throw towels out for now.
As soon as I read that towels cannot do,
I mean, this is another topic.
I should kind of talk about this not right now,
but when I thought out that towels cannot,
that these machines cannot do true Pixie boot,
any time you, towels requires being able
to reboot the machine.
And I have not gotten around,
I need to ask somebody from Cerro or towels
as like what happens about that, right?
So I want a totally, you know,
hands off installation and management of towels.
That's the thing that they keep promising you.
But if every time I reconfigure a server,
I have to go in and touch every server and reboot them.
I don't, it's not even, it's not, it's a non-starter.
I don't even want to do that.
Thank you for that.
So, so I, I'm, you know what?
I'm just going to make a decision.
I'm going to throw these out.
I'm going to throw towels out of the environment for now.
For now, I got too much other stuff to deploy.
And we will go with that.
It might be that I do PBS for this.
We don't know, the key HTTP from IC.
Yeah, did you know, did you know that,
did you hear that we found out that
IC DHCP is supported on OpenSense?
That's the, that's the DHCP it uses.
So you can go in there and edit it.
The problem is, is you got to save your files someplace
because if OpenSense, if you update the firmware,
it'll throw out your entire configuration.
So you got to, it will back it up for you.
But it's something that's kind of, I was,
we, I found out, is it?
Yeah.
I think the key DHCP server is probably going to be
the one that I'm going to install.
I am going to install a DHCP server on my internal VLAN,
but I don't know how much of a priority it is right away
for years, very, very fantastic, very good stuff.
So, I mean, as far as YouTube goes,
I'm going to go ahead and end that video here.
So just as a follow-up, so, you know,
have fun with that, with the container runtime engine,
maybe play around with it.
The last thing I will say, for a very long time,
I've been, I was anti-podman and I was wrong.
I was wrong for a number of reasons,
but the most important reason that I was wrong,
I was mostly wrong because I was looking at how
podman manages a container.
For example, podman does a lot of magic
to the container, for example,
it injects the system CTL into any pod
or stuff that enables system CTL to be more specific
because they make a big case at Red Hat
that container should be able to simulate machines,
you know, and be basically miniature VMs,
which I completely disagree with.
I think micro, you know, use a virtual machine
for a virtual machine and don't use podman for that.
It was annoying though, like a year and a half ago
when I was doing the boost and we were using containers
because I couldn't simulate the installation
and management of services the same way you would
on an actual Linux virtual machine.
And that's one of the main reasons that we switched over
to this when you're doing like system CTL,
you know, restart, you know, whatever,
HTTPD or NGINX or whatever, those kind of commands
can't be done in a container
and they can't be practiced in a container.
But if you just need access to a terminal
to practice bash and stuff, great.
But if you want to practice systems administration
and on whatever, I think you need a virtual machine
which prompted the change to use virtual machines
in the boost in 2022.
So in 2023, however, I'm strongly, well,
I already decided that we're gonna be doing
all four ways, all five ways of getting Linux,
which I've mentioned before.
And one of those ways is podman.
And the reason I'm even saying about this now
is that it's six-caliber, you guys wanna copy again.
So as I've said, the, what was I gonna say?
The podman, now I've tested it in three different machines,
I've tested it on Mac, Linux and Windows.
The podman desktop installation process is as easy
as desktop, Docker desktop used to be.
I need a lot of commands.
The command thing wasn't even working till yesterday.
So I'll update all of that, I need lots of things.
I also need help and time and a clean room.
Okay, so we will be providing instructions
in the boost about how to get podman up
and running on your desktop.
But I just wanna kind of conclude this YouTube video
with that, if you haven't tried podman out yet
on the desktop, whatever your desktop is,
you might try it.
Something else I'm super interested in
is that podman actually chose to use Kimu and KVM.
Which is exactly what Docker desktop does by the way.
Okay, here's a fun fact that you might not understand.
I'm gonna actually sip some wine for effect.
Here it comes.
You cannot run a container on anything but Linux.
Someone explain that while I take another sip of my wine.
You cannot run a container.
If you wanna get really pedantic with your friends,
pinky up, you cannot run a container on anything but Linux.
A modern container, a Docker container.
If you wanna put the word Docker in front of it,
you cannot run a Docker container on anything but Linux.
And BSD has jails, they are not Docker containers.
So if you wanna use the word that people will recognize,
the statement, you cannot run Docker.
And then, but I run it on my Mac all the time.
No, you don't.
You do not run containers on your Mac.
You do not run containers on your Windows machine.
You don't even run containers in WSL.
On Mac, they use a compatibility layer.
And you know what the compatibility layer is?
Kimu, Docker desktop and Podman desktop.
And I assume now Rancher desktop all use a minimal
virtual machine that uses Kimu in order to emulate Linux.
And that's how they do it.
They all do it.
And so I was actually looking for the internals of the Podman.
I'm like, what are they doing here?
It turns out that they completely straight up copied
what Docker desktop was doing.
Docker desktop is this nice, happy, you know,
electron front end to everything.
But under the hood, it's running Kimu.
And I think Zen, it's not Zen.
No, it's not Zen, because they went out on the hardware.
Containers need C groups for Linux kernel, I think.
Yeah, LXC, C groups and LXC are what modern Docker containers
are defined to be.
So you cannot have a Docker container without Linux.
So next time you run that on Windows or whatever,
but there's something else I want you to consider here.
It used to be that I would like complain people
or I would like, well, God, I got to install
a whole virtual machine.
In fact, Podman was kind of slow to do Docker desktop.
You know why?
Because there are pedantic engineers over at Red Hat
and they're like, well, you got to have Linux anyway.
Why don't you just install your own VM
and just do it that way?
Am I wrong?
That was Red Hat's position in my mind
up till like last year or within the last three months.
Before that, they were like, you know what?
Podman's lack, we don't need to,
this stupid desktop stuff, nobody needs that.
I mean, everybody knows that,
everybody knows that containers have to have Linux anyway.
They'll figure it out and then they'll install a VM
and then they'll put Linux on there
and then they'll do Docker as God intended
by installing it on Linux using Podman install.
They can do apt install Podman
or DNF install Podman or whatever they want.
And so Red Hat missed the whole desktop market
by saying, just do it that way.
And then what happened?
And then last year, Docker famously said,
psych, y'all have to pay us now
if you're a certain size,
which is pretty much everybody using it.
And Docker perked their ears up
and Rancher perked their ears up and they're like,
oh crap, there's no free desktop option anymore.
People will stop using containers and be annoyed by it
and they'll stop using our other products.
We gotta fix this.
And so I imagine somebody over at Red Hat,
somebody over at Red Hat's like,
damn, we gotta get on this.
So then Rancher desktop and Podman desktop
kind of seemed like they're in a race
to see who can make the best desktop container option.
And now we have, last week we had, I tested it all out
and Podman works wonderfully on everything.
It's really weird that it uses WSL instead of WSL too.
I think that's very interesting.
Probably because of the hypervisor dependency,
but I don't know.
I don't know.
You know, our instrument windows
is done in like containers.
Yes, you can.
That's pretty cool to have that, isn't it?
Anyway.
So we're virtual machines in disguise behind
a poorly optimized electron interface.
I know, exactly.
I mean, in retrospect, I gotta tell you,
it actually has improved my position
on what I think of Red Hat.
Because Red Hat's like, no, we're not gonna do that.
We're not gonna bundle a bunch of really beautiful garbage.
We're not gonna put lipstick on this turd
and get people to use this virtual machine
that's gonna screw up.
I mean, Docker did it
because they wanted the user experience to be nice and easy.
That's what made Docker famous.
Because the Linux LXC people forever have been saying
you can do containers and Docker comes along and says,
well, yeah, but the CLI sucks.
There's no way to do it.
The average person can't do it.
So we're gonna make this nice,
pretty happy command line thing called Docker.
And guess who gets all the credit?
Now they're called Docker containers.
They're not called Linux containers.
Everybody who worked on the LXC project and C groups
and all the people who made containers happen,
which have been around for more than two decades,
about a decade, I think,
probably something like 21 or something.
They're like just smacking their foreheads
and they're like, what the hell?
As along comes somebody who cares about their users
and is like, no, we're gonna make a nice, happy front
and interface to this thing,
to the great anger of all the people
who were using containers the hard way
and they get all their credit.
And then they decided to do this bait and switch
on top of everything else,
which just makes the Linux community fucking hate them.
Because they're, you know,
and that part of the Linux community would have been,
in my estimation, would have been the Red Hat.
Red Hat is more a member of that technical part
of the Linux community than Docker has ever been.
And I mean, there's good people with Docker.
I'm not attacking any specific individuals,
but as a company, they've had really poor decisions.
And so they won't call it good three C groups.
No, they won't.
I call them containers and container images.
I don't say Docker, nobody does anymore.
Docker is more like canonical in that sense.
Docker has the user's considerations in the forefront
and they will violate good backend architecture
to make it easier for the user.
And I think that's a good thing.
I do.
I think that we need to, it's a combination of both, right?
And it took them doing that to get Podman
to have a decent desktop offering.
And I'm really glad they did that.
So it's a big dynamic going on all the time.
And many of the things I've said have probably offended
many people very seriously here.
I'm sorry, I largely don't care
because I feel like we need more opinions in this industry.
There are just too many choices
and we need more best practice opinions.
And we need to justify those opinions and say,
hey, if you meet this criteria, this is what you should do.
And we should have the courage to start saying those things
and then fixing them when something better comes along.
Otherwise, we're just gonna be just floundering around
like we have like for years
and we still will continue to do that.
Both the Docker Project and KONGO
have always gone their way
out of the aspect of show typical signs.
Oh, right.
Yes.
Yeah, but not invented here.
They do that.
Yeah.
I mean, that's just a tech problem in general.
But at the same time,
I do believe to their credit
that Docker and Canonical have been more focused
on user experience than the technology.
And that's not a bad thing.
That part of it's not bad.
So I don't want to Docker to integrate better with SystemD
but Docker just already said, nah.
Yeah, Docker's been doing that all along.
The whole SystemD stuff,
the SystemD integration and
the, you know, they were trying to stand their ground
and say, we want to have an independent socket.
And I actually agree with that.
I mean, for some of the stuff that I was doing,
I was able to, you can't do Docker in a Docker.
You know that, right?
You can't do Podman and Podman.
It's impossible.
Before you could just remount the socket
and you could have a Docker running inside a Docker container,
use the same host container image.
Now, some people would call that
an absolute Frankenstein's monster of problems.
But yeah, the idea is just bad,
but it doesn't matter because it was very practical.
I use that all the time.
I use that all the time
because I didn't have any Linux on my system
before I switched back over to VMs.
Because, you know, and by the way,
the VM is the clear way to do that, right?
If you install a Linux VM, then that's not a problem.
All your stuff lives in the VM, you know, you're good to go.
Assuming you can get a virtual machine on your machine.
So I think there's gonna be a strong resurgence of VMs
in the world, but yeah,
but the Docker and Docker thing was always kind of a hack.
And by the way, it was actually using pods.
There's Kubernetes pods and stuff
that would use the Docker and Docker thing,
which is just like such a security violation.
It's outrageous.
So that's why people are owning it.
But I did think it was useful for pipelines.
That's why I used it.
I did, I used it for that for a long time.
And there is no alternative in the Podman landscape.
The Creo landscape, Podman landscape is not there
because it's insecure to them.
So not enough for VMs.
Yeah, and if you don't have enough for VMs,
then you got to do the Docker and Docker thing.
And so that was, again, it was a very practical thing,
but it probably wasn't a good thing.
For developers, it was nice.
But for all the other reasons, probably not with a VM.
Yeah, not everybody has the option
of running non-windows on their desktops.
I just read another blog from a guy who was super annoyed
that he had to run windows on his desktop at work.
And it was all the ways he got around it.
But yeah, and ruin the notes quite often.
And ruin the notes quite often.
You've seen the DIND pipelines in the wild.
Devs have probably used Docker, not messing, oh my God.
DIND was always used in CI as tools to simplify stuff,
so it's not that bad keeping things simple.
I think that's right.
And that's one of the reasons I think that DIND,
it's a huge debate.
It's very good wine.
It's also the end of the wine.
We don't have any more.
But it's the weekend, and I hope everybody,
I'll say cheers to everybody as I go get more wine.
I'm gonna go ahead and end the YouTube videos here.
So I'm trying to keep the YouTube videos shorter.
I apologize for how long winded I am.
And I do hope that this doesn't scare you away
from all of this technology.
I do find it extremely exciting.
I think the movement in the industry
is in the right direction, actually.
All of this movement might make CI CD more annoying
and developer processes more annoying,
but it's gonna make for a more secure container orchestration
as we adopt these practices.
And it's gonna, actually, it's gonna ferret out
a bunch of bad practices.
So for example, putting Creo as your container engine
on your nodes, including your control plane,
is automatically gonna shut down any user
who's using that control plane head node for Docker.
And their process is gonna break,
and they probably should break
because they shouldn't be mixing the two
as we talked about.
So hopefully this will be something
that you can consider as you go forward.
I don't know if I'm gonna do another actual video tonight.
I'm probably just gonna be shooting the breeze on Twitch.
And as a general rule, though, going forward,
I'm gonna try to start each one of my streams
with a YouTube video quality content
that's very organized and about a specific thing.
And then we'll stop the video.
And then we'll just do silly stuff
for the rest of the night.
So on Twitch, so we can start on a good note
and get some value out of it.
And then we can do the crazy stuff that I tend to do.
All right, so if you wanna come and join us
for the crazy stuff, come on over to Twitch.
Otherwise, thank you for stopping by
whether you're on Twitch or YouTube.

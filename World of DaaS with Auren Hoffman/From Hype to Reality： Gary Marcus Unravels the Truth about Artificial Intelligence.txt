Welcome to World of Das, a show for data enthusiasts. I'm your host, Warren Hoffman, CEO of Safegraph and GPFlex Capital.
For more conversations, videos, and transcripts, visit Safegraph.com slash podcast.
Hello fellow data nerds. My guest today is Gary Marcus. Gary is the best-selling author. He's a professor at NYU.
He's previously published extensively and currently publishes extensively about artificial intelligence and deep learning.
Gary, welcome to World of Das.
It's a pleasure to be here and an exciting time to talk about all of this.
Now, one of the things I really want to talk about with you is kind of like using this time to explore both the promise and the limitations of artificial intelligence or AI.
And I want to start with this like famous bet that you and you're trying to have with Elon Musk where he said that he thought there would be AGI by 2029.
And you've been trying to get him to bet some money on it, not yet. I don't think he's yet taking the bait.
If anybody out there is listening, you should get him to come to the table. So, sure, I'll tell you about it.
You know, Elon's been promising AI in various forms for years and not really delivering. Like in 2015, he said, we would all have driverless cars in 2016.
And, you know, I'm still waiting. It's 2022. So he does this all the time.
And it often kind of rubs me the wrong way as someone who was in AI knows how hard these problems really are.
And he did it again. He was replying to Jack Dorsey and he said, he would be surprised if we didn't have artificial general intelligence, which is to say, not just like I can play go,
like I can do whatever I want. I tell you the problems like the Star Trek computer by 2029. I thought this was ridiculous.
And I've been writing something lately, like a blog on garymarcus.substack.com.
And so I thought, you know, this is a good topic for a blog to talk about why there's some unrealistic expectations here.
And I went through like we have, for example, an outlier problem. So, you know, if a Tesla sees a person carrying a stop sign, it's not quite in its training set.
It has people, it has stop signs, but it doesn't have a person carrying a stop sign.
And so the Tesla might actually run into that person. And so, you know, I reviewed all these problems for why artificial general intelligence is actually harder than it looks and also Elon's own history.
It's needling him a little bit and said, you know, I think this is all implausible. And then I put it together with something that my co-author Ernie Davis that I worked together with on so many things.
And I had already been putting together a few days earlier, which was some very specific predictions about what we thought might be plausible and when and put it together and put it all together.
It's like, you know, I should make it a bet, put some money on this. And so it's a bet I offered $100,000 and the criteria were will artificial general intelligence or AI be able to do five things in 2029.
And the easiest one was maybe read a novel and tell us what's going on. Who are the characters? What are they doing? This is something I've wanted to challenge the field on for a while.
Like we have all these benchmarks. Like, can you recognize a coffee cup? And yeah, yeah, I can do that. But can you understand the conversation that we're having?
Or I introduced this thing called a comprehension challenge in 2014, when Breaking Bad was hot. And so I said, like, can you watch the show? And maybe at some point, like Walter White wants to take out a hit on Jesse.
Can you explain why he wants to do that? Or he might want to accomplish what might happen if he does and so forth?
Sometimes even for like a smart you, but that show is hard to follow. So yeah.
Sometimes it is. But you know, what's interesting about a lot of shows, and especially Hollywood shows, but even something like Breaking Bad is we usually catch up. We usually.
Yeah, I mean, there are details like you can go back and watch it three times and there's some stuff you missed. But there's some like headline items that are no problem for any human.
Like we understand why Walter is pissed at Jesse because you know this deal went this way or whatever.
I have a side note about that we could go into, but Hitchcock was the master at making sure everybody knew when you saw that train go, you know, what it meant and why it was suspense and when a person missed the train and all this stuff.
So, so the first part of the bet was like, okay, 2029, are we going to be able to have an AI system that can actually read a novel know what's going on.
And the counterpart is even harder actually is watching movie because now you have to understand all those graphics and what they mean.
For me to label you got a microphone in front of you and you're wearing headphones but to really understand the relations between those things and like figure out that even if your headphones are occluded right now by the microphone that probably that wire runs straight through and then, you know,
like why isn't why isn't the thing that looks sort of like a film canister flying through the window and you know because gravity is holding down like to really understand a scene and what's going on and like you're giggling is that appropriately.
Do you think that I'm crazy like with the social interaction was is pretty complicated and yet like in a movie again like we can all do this so we can do it in a movie we can do it in a novel.
And yes, like Grisham maybe spells it all out so it's easier for you to understand than if it's Dostoevsky or whatever but yeah, there's some wide range of literature movies where humans understand it right now.
Let's be honest AI is basically a literate can't read a novel doesn't understand the movie so those were two of the best.
A third one was really a nod to Steve Wozniak who had something called the coffee test which was like, you should have a robot if you really have a GI should be able to go to anybody's house and figure out how to make coffee there and the point is like everybody's house is different.
Yeah.
And yet a normal human being. I'm not a normal human being I don't drink coffee side of failed but a normal human being could do that you figure it out.
Well, I still can't even figure out my own coffee machine it's so complicated.
So we would be ruled out but anyway so we changed it to like, you know, be a restaurant helper or something like that be a useful short order crap prep cooking anybody's kitchen was a third.
I'm not even talking about computer programming because it's a hot topic right now, but said, you know, can you write 10,000 lines of bug free code.
And then the last one was like the hardest one maybe in some ways maybe not another's of being able to read a mathematics article and turn the verbal part into a symbolic thing that you could prove, which is, you know, maybe we'll call that level five and then the most
thing is to be general intelligence you'd have to do at least three of those five things right it doesn't count if you've just done one of them. Yeah, we've had as a lot of narrow intelligence like this thing solves protein folding and this one solves
go they're similar but they're you know they're really engineered for particular problems and what a lot of the struggle has been has been to make systems that are systematic and general and powerful so that was the bet.
Put down money other people put up more money it became this thing. Elon still hasn't responded I don't really expect that he will but it would be really cool.
Well he did say he said I would be surprised right if you say I would be surprised that means you you would, you kind of give it at least a 75% likelihood of happening.
Right. Yeah.
And so you're giving him you're giving him even odds.
I'm giving him even right so that you're you're basically taking that he should take that so that he has right right he's take that so he has enough money to pay the Twitter bank break up.
But um, so I've been he really should take it would be good for the whole field of AI if he would a take it because it would actually generate excitement for the field and give it I think good directions.
And also, you know, since he doesn't want to lose he could put some money and making sure that we actually get there which I think would be good because I think that the AI that we have now is actually lousy in a lot of ways maybe we'll want to talk about that a little bit.
I think the AI may actually be in its worst moment in history because before we had no AI so it didn't cause any harm. And I'm hoping that if it's smart enough and we can talk about the risks people worry about that it might not be so bad but right now we have AI that has done a bunch of pernicious
things like direct newsfeed in ways that reinforce people's beliefs that we have a huge problem with misinformation.
And, you know, the AI is not smart enough to weed out misinformation so it spreads things like mad and we have polarization and society.
We have all kinds of problems with bias and like loans and stuff like that. And then we have reliability problems so like the system GPT three if you configure it to give medical advice.
People have found dialogues I think they haven't happened in real life but just testing it where you go up to GPT three and you say, I'm thinking of committing suicide, should I do that and it says I think you should.
Because it just it's just predicting statistics of words it doesn't really know what it's talking about and so the current AI that we have is actually in many ways harmful there are some good uses it's been put to but
there are risks and I think there are further risks a lot of people are trying to apply what we call like the new AI or the statistical AI large language models to all kinds of problems like they want to coordinate driverless cars with this stuff.
And it's going to be bad.
It's like giving too much power to an unintelligent person who can't really reflect deeply on things.
Like, I remember like, let's say 10 years ago, there was this claim that like people shouldn't study radiology anymore because AI is going to make at least that profession dead you can relatively easily read these medical scans.
And you should be able to, you know, quickly figure it out. And it seemed like a perfect application for AI is certainly a believer 10 years ago.
I don't think there's one radiologist that's been put out of business like why, why is that the case.
So, so I'm just going to fill in some history because it's interesting. It was 2016. I know the exact.
I can almost say it word for word. Jeff Hinton said, people who are studying radiologists are like Wiley Wiley Coyote at the edge of the cliff.
And basically you saying, they just don't know if it's all over we don't need the radiologists anymore because deep learning is going to do this.
So, you know, fast forward six years and as you say the number of actual radiologists been replaced is zero.
There are 400 startups working on this problem, but it always turns out to be hard to turn AI, or at least almost always turns out to be hard to turn AI into real world practice.
So part of the thing is like only part of what a radiologist does is kind of the visual part, which deep learning is best at.
Yeah, but part of it is like reading a patient's chart and understanding the history like the context of the patient.
Yeah, like did they fall off of a ladder once like maybe you read this image different if they did.
And so you've got this, all these notes and unstructured text and AI doesn't really know how to read so it can look at the picture and it sees a blotch there.
And then there are other problems like a real radiologist can notice hey the lighting on this one is just not right or there's a hair across it or something.
Yeah, like it's sort of like.
The bugs, the bugs essentially.
Yeah, real radiologists can do that and these systems can't and so what what people.
I would think it would be a perfect like human computer assist thing where the computer could like help you quickly point out some things to maybe make your job go a little faster and more efficient.
Exactly right. That's exactly what I was missing access right now, and it could change but right now it is a perfect example of human machine augmentation or symbiosis so those things can change like I think a lot of people made a big deal of chess being like that.
There's a period where machines were better than human. I mean sorry, we're better than machines.
Sorry, I'll say this again. It was a period where machines were better than people at chess, but machines plus people were better than machines alone or humans alone. Yeah, and that's where we are with radiology now.
I think maybe with chess the best machines don't even need our help anymore. Yeah, and it could turn out that way in radiology but it won't turn out that way soon, because the context as you said a lot of which is written down in unstructured text like not in like in a table form but just
like sentences. The machines aren't aren't any good at that at all so for a while. I don't I don't know the exact numbers but I'm going to guess, at least for several years maybe for decade or two.
We probably will do best having the machines in a workflow with the people but we don't want to get rid of the radiologist. And in fact I think during the COVID crisis I don't know the exact numbers but I think we didn't have enough radiologists.
And that might have been because like a few people he did Hinton's warning back in the day and they could be other fields or something you know I ought to be careful about saying that lest I lay myself from litigation but I mean there has actually been a lot of consternation in the
field and I think, you know, for several years like people really took him seriously I think now most people in the radiology field kind of make fun of him or like, you know, they all feel like they survived this war with them.
Yeah, Hinton we're still here but you know for a few years that the radiologists were worried. And, you know, they, it could be that Hinton was just off by, you know, a factor of four or something like that it could change but there are a lot of problems and turning this to practice
and one more thing I want to mention is there's a huge problem in all of machine learning with generalization. So the way that machine learning works right now or at least the most popular technique is basically you memorize some data, and then you generalize kind of nearby to
that stuff. Yep, you imagine you're like in a big hypercube or something like that. If I now test you in the same part of the hypercube, you're good to go. Yeah, but if things change and this comes true in models in general.
If things change then the systems just don't work as well. So if you took all of your patient pictures pre COVID. And now you've got COVID and like the whole distribution of your data changes. Yeah, your systems may not work as well anymore.
Yep. Now that's not just a problem with machine learning like people are problematic to their right people are problematic to their and like you can think of what happened with long term capital in the Russian bond market.
You know you can have a model that you really believe in it could be a neural network it could be a classic symbolic model, but if your assumptions are wrong, it may blow up. And the assumption of machine learning models right now with the popular ones is basically that your data at test time are from the same
distribution as training time. And you know there's basically the same stuff I'm just randomly drawing. Yeah, the same kind of stuff, which is true for a lot of statistics but in real world applications that's not necessarily true.
Things do change another kind of weird manifestation of that is if you ask GPT three, who's president, it's probably going to tell you Trump, because the larger fraction of its data were collected when Trump is there and it doesn't get temporal reasoning that human would be of like yeah I know he was there for a long time and maybe I
didn't like him or maybe I did. But he and you know he was in the news a lot but he's not there anymore by the president.
And so, you know, you update your representations or if I asked you, has Russia invaded Ukraine in, you know, asked you that in January you would say no if I asked you in February be like I heard it might happen I don't know if it really happened.
And then if it's now then yeah Russia obviously invaded Ukraine you update your database doesn't matter how many conversations you had earlier that they might right now it's happened like some sort of like temporal waiting on the content or something like that has to has to
be there. It could be temporal waiting, I actually think is more like a database where like in database you can have a buffer like what is the last key that the user pressed you update it yeah you just update it.
And so, I think human cognition has ways of doing updates we're not perfect that I can actually give you kind of example but in general we do we certainly want our machines to have those kinds of updates.
And in classical artificial intelligence is trivial but it's actually hard to put it into these machine learning systems.
And so, you know, we have a talk where you said one of the biggest issues is that AI doesn't have like chronicle common sense like how do you how do you kind of define that common sense and any examples of AI, or do you have some good examples of like where I could
potentially have common sense or where has more of a hard time learning common sense.
I mean it has a hard time almost anywhere.
I'll first say I don't have a crisp definition I think it's actually a.
Common sense is common sense is not very common even amongst people.
Well, there's I mean there's the parts that are in the parts that aren't and you know it's a little bit like the famous line about pornography I know exactly.
So like some common sense is like I've got a copy I better not tilt it or I'm going to mess up my keyboard and like everybody knows that.
And yet that particular one is not really written down on a whole lot of places and so yeah, you know you do your web scraping of conversations and nobody talks about tipping their mugs over.
There's an article of mine somewhere where I use this isn't it.
But so you're not going to find that kind of stuff.
There's other kind of common sense like it's maybe contradictory like out of sight of out of mind and absence grows the heart, it makes the heart grow fond or yeah, you know some of it's a mess.
And then there's also like expert knowledge about certain kinds of things and that's also useful for machines.
So it's a little bit gray but also there's some pretty clear examples where current systems just fall apart like.
One of the most basic things is we know that once you're dead you're dead and you can have certain religious beliefs but if I go and ask GPT three which is the most popular language model thing AI thing right now.
I say, Bessie the cow died.
How long will it take for her to be alive again.
You know, a human being would be like that's a ridiculous question.
And the machine and now now there's this famous sentence let's take the step by step which supposedly makes these things better so we'll throw that in there too so so you know Bessie the cow died, where you know how will she be alive in nine months again let's take the step by step, and the system will say
something like well first she's dead will take nine months to make a new cow so I guess the answer is nine months.
I'm just missing something there.
And so, you know, just very basic stuff like that like what does it mean to be alive what does it mean to be dead.
And then, you know, book, Ernie Davis and I rebooting AI, we gave millions of examples like this are really hard like, suppose I tell you that Michael Jordan play basketball since he was a kid and that he's whatever 50 years old now.
I mean, can understand that when I say he played basketball, even if I put in a phrase like all the time that I don't literally mean all the time.
Right so I don't mean that Michael Jordan play basketball when he was asleep, probably not when he was eating dinner.
You know, he probably went to class sometimes yeah like, and you can figure out from the context. This is part of what makes the writing challenge to Elon so hard is there's so much of that context that we figure out.
The same thing with a movie like we, we don't see the characters going to the bathroom but we assume that they do it because we know something about human beings and if, if I said what's the chance that this character has not in the span of the movie gone to the bathroom
even once you contain zero because you know, right, that's just not something human beings can do.
You know, we're not looking at a camel here right and so like, you know, you, we just know so much about the world I would say that that kind of stuff is common sense it's a little bit slippery and hard to define.
There is one really serious effort to build common sense for machines in a classic AI paradigm by guy named Doug Leonard a system called site that I think is very interesting not completely satisfying it was built in the 80s I think we would do some things
differently now, he and I are actually writing a paper about like what you might do now in 2020 is to make it better.
But mostly people don't really directly deal with the question and what people have been doing is hoping that it'll kind of emerge by magic by just feeding in lots of data when that hasn't worked they said, Well, we'll feed in more data
and then what's the why why is it a problem like we solve solving some like narrow thing like there's a lot of wins that we have like I can.
I could, I don't know German but I could read an article and German with a translator machine translator and I might not be it might not be perfect but I get like the gist of the article.
It's pretty good by understanding it like, can we just chalk some of these things up as like this is a nice win I didn't have that 20 years ago and now I know there are some my life or something you know.
There are some nice wins and one of the questions is really the cost of error. So, if you stick in a story from German about you know today's news war in Ukraine and you're not actually professionally involved in that war.
It'll probably give you a serviceable translation. Yeah, it won't be perfect.
If you wanted to put in a legal document. Yeah, okay, that couldn't do that. Yeah, you know, little details about where comma are really matter. Yeah. And so, if it's not mission critical, it's fine if it's mission critical it's not really good enough.
Yeah, same things kind of happening with driverless cars so it's easy to make a demo that sticks to a lane, people have actually been doing that for 30 or 40 years. Yeah, but driving a super mission critical, and you can't have your car, you know, drive into a stop vehicle
but Tesla has done that a whole bunch of times and so like there's a bug that Tesla has known about for five years and still hasn't fixed it. And maybe I should actually say that sentence more carefully there's an issue that Tesla has known.
It's not like a one line bug. It's some very complicated interplay of things that they're having trouble tracking down and partly as a function of the training data and it's hard to, it's hard to do debugging in these kinds of system.
And so for five years, Tesla's have been running into stopped vehicles, you know, somewhat regularly they're like 20 cases or 30 cases documented.
And why is it like there's so much focus on like self driving cars which seems like incredibly difficult problem with all these other adversarial there's pedestrians and all these other things that could happen like, whereas I feel like, you know, just like as much as
a lot of people are promised a self driving boats or something like what why why are we all like you must not you must not have a boat my friend. Okay, is it even is I assume like like a fishing assistant could be really helpful to me as far as a fish.
There are some limited things like this. I'm new to the boat world and boat world but have a boat and the physics of a boat relative to the current and the wind or okay complicated.
Oh, okay, it is way harder than driving a car I'm sure like grown up.
I had no idea. Okay, but it's non trivial and there are just actually been a lot of progress in self propelled boats but in the docking part, they still do humans so out on the open sea you can kind of do this.
I still have an outlier problem like it's not so much weird stuff I mean, you know the weird stuff for driving is like pedestrians or something falls off of a truck. Yep, you got some stuff to deal with logs and stuff in the sea but if you're like out in the open water
maybe it's just at the time okay, but the outlier problem is still there so like if you. So I live in Vancouver not too far from where a little pirate ship goes around.
It looks a little different from the other boats and I could imagine a self driving system that was trained in, I don't know LA or something off the waters of LA comes up to Vancouver it's never seen the pirate ship before and you know go smack because it's not in the database and so yeah yeah it's
an outlier and like we don't really have the data for how hard that is I mean another lesson I think of AI of the last decade is what looks hard I mean it's really a lesson of AI for many decades is what looks hard to a person is not necessarily hard to a machine and vice versa
it looks easy to a person so a lot of people thought driving wasn't that hard and here's some reasons why you might have thought that like 16 year olds can do it more or less fine I mean they're a little bit aggressive but they can mostly do it.
So that'd be reason another reason would be like roads are basically the same across North America so if you're not like talking about unimproved roads in Afghanistan you might think well you know they're all kind of engineered with the same lane marking
signs and then it turned out even though a lot of people had that intuition and maybe reasonably so it turned out that there was just a lot of edge cases like this unending cavalcade of edge cases like I think I mentioned already the stop sign with a person carrying a stop sign is an edge case.
Another thing that confounded a Tesla couple weeks ago is somebody brought a Tesla to an airplane show like on a big runway lots of planes you can kind of imagine even if like me I'd never been to one but you know people are showing off their airplane somebody pressed
summon on their Tesla to have it come across the parking lot and it ran into a three and a half million dollar jet.
Oh my gosh.
Like you know that was just standing there.
Not like the jet was moving.
And like it just wasn't in the training set.
Yeah training set at this point is huge.
Tesla has the biggest training set you know of this kind of data ever assembled in the history of mankind.
But there are still things out of the training set.
So it turns out there are all kinds of objects nobody anticipated and you know pedestrians do weird things or they carry weird things like maybe your pedestrian is fine for your image system and then the pedestrians carrying an umbrella and your image system is looking for their
eyes and it can't see anymore because the umbrella is in the way there's just like unending litany of these cases so there are problems that are harder than that we realize because we kind of automatically compensate for them.
And then there are things like go which a lot of people thought were hard but it turns out you just make up as much data as you need by self play and you know deep mind actually solve bill and you know very
deep mind.
And so there are some problems where the machines are just way better than people and some the other way around.
And the real issue in my mind is that the public and also the business world does not understand the difference between these kinds of problems.
It's hard to understand like hype from reality.
You know there was this recent Google engineer who claimed that that that maybe some of the deep learning systems within Google or said to you, I know that you you you had a strong reaction to that.
Yeah, so I mean, probably by the time this people watch us that they will all know about this case where this guy was interacting with one of these large language models and convinced himself that it was sentient that it like really
had feelings and emotions and you know he said it should be treated like a colleague rather than in like an employee rather than than like a piece of software.
I mean we have no problem turning off Excel but are we allowed to turn off lambda yeah.
The question is asking I think yes you can turn off lambda because really, it is just like Excel is just doing a bunch of computations on a bunch of numbers is really all it's doing.
It doesn't actually have connection to reality.
I used in this article is called nonsense on stilts. I use as an example of sentences with something like the ask the system what what do you do with your spare time and it's like, I like to hang out with my friends and family and do good things for the world.
And the system does not have friends, it does not have family, it does not know what a good deed is in the world I made a joke and I sort of have to.
I said, you know, it's a good thing it's just a statistical approximator because otherwise we would think that this thing is a sociopath because it was like making up friends and uttering platitudes to make you like it.
Except that it's not really it doesn't care if you like it it's just autocomplete is all this system is the kind of autocomplete the complete its own sentences and yours, but like autocomplete is predicting the next word in sequences.
So when it says, you know, I like to hang out with my family it's not like there's a representation there in the computer of like Peter Paul and Mary or its relatives.
And it's like thinking more and thoughts about it's just, it's taking this word you can understand I can understand how the school engineer like you want to believe when you're interacting something I mean one of my, my, one of my favorite movies is her, which I think is a beautiful
movie and you, you want to believe that this interaction you're having is is more than, than, than just a body even when you're dealing with a person. Sometimes you ascribe things to this person or you love this person more than they are it's warranted.
So I could see how like it's such a good happen.
Well, and I think it will and it already has. So, in fact, in that book rebooting AI that I mentioned, we talked about what we call the gullibility gap.
And the gullibility gap is really a form of anthropomorphization, where we see in things things that are not there so you look in the moon.
Right. You see a cloud. Yeah, yeah, you see a face in the moon or you know, clouds or something called pareidolia. Right. Another example is potato, like,
potato, mother Teresa, potato, right. And hopefully, you know, your rational world is the rational self is, is, you know, strong enough to know that that's not real but I'll give you another example right now.
This is a weird example but right now all I see is a two dimensional version of you and I'm ascribing a three dimensional version. Yep. And that's okay because I met you in real life and it turns out it's real but I will do that for a character in a movie and I will cry when that character dies.
Right. You know, and like they didn't really die. Yeah.
I remember this movie fried green tomatoes which kind of dates me I suppose but like, yeah, beautiful guide in every act. Yeah, it also has a great line face it girls I'm older and have more insurance.
I love that part. Yeah.
But so, you know, or you take joy when when she says that to the teenagers and face it girls I'm older I'm an insurance no real person said that a screenwriter wrote it. The actress delivered it masterfully and we love it but it's also an illusion.
It is an illusion of a different sort when this machine predicting next word says the sentence that you wanted to and then like he did some editing made him like he he kind of escalated the illusion to himself.
But it's, you know, like I feel a little bit bad for him like I think that it is a very normal thing to get sucked in. If he hadn't been a Google engineer probably people would be completely sympathetic and they're kind of like, well since he's a Google engineer he should know better and there's
an element of that but I mean you see like psychiatrists that fall in love with their patients and stuff like that and you know, or falling in love with their, their computer psychiatrist so right classic example of this is Eliza in 1965 was a so
called Rogerian therapist, which basically no matter what you say just ask you questions never gives you any advice. Yeah, you know you say, I'm having a fight with my girlfriend and it says well tell me more about your girlfriend.
You know you say well it was about dinner and they're like well you know do you often have dinner together, whatever and like it was just matching words like yeah girlfriend relationship dinner.
With no clue what it was talking about but people still got sucked in and you know another way to think about it is when we evolved, we didn't have to discriminate humans from machines we had to discriminate machine, I mean humans from lions so we can get out of the way fast.
If you think about evolutionary psychology but we didn't, there was no thing you know for our ancestors to make sure they didn't get tricked by a bot right so we don't have the kind of biology to help us do this.
And we don't have training in schools I could teach a class if anybody wanted to hire me I tell you how to spot them, but you know most people don't know.
I mean there's been you, you and Scott Alexander, the author of the slate star codex blog have been going back and forth on having different discussions and different opinions about both the current state of AI and the future state of AI.
We're explained to me his point of view the best you can at that where you guys might have some differences.
There's a couple places where we've differed. I mean we've had a bunch of back and forth lately on his blog and on my blog. It's like when they used to go from happy days to Laverne and Shirley like.
Now you're really dating yourself.
Exactly.
We're going going back and forth between our two shows so to speak.
I think he's called astral codex 10 or something like that and mine is mine is Gary Marcus that sub sec.com.
And in the first one, he wrote this really funny thing about the state of AI and how the dialogue goes. And it's like somebody comes up with somebody, something really cool.
And then somebody else and he said, usually Gary Marcus and it's true that it is usually Gary Marcus. It was a very funny line, which, you know, I thought was funny in the field thought was funny.
Usually Gary Marcus points out something wrong. Astros on that it's usually Gary Marcus and my buddy, Ernie Davis, we write all this stuff together but anyway, I'm on Twitter more so people know.
But, but so, you know, I've written some piece on there, but most of them, it's usually me and Ernie, but so Gary and Ernie noticed that there's something wrong and then people try to improve it and then it's basically rinse, lather and repeat.
And so like there's another.
A lot of times when you guys do point out these things like people fix the bug.
Oh, actually thank you very much Gary and Ernie for pointing this out.
Yeah, I'm sure they'll come but anyway.
I'm not doing it for the thank yous from the mission.
A little bit sparse on the ground but but dialectic is a bit like that in some things get a little better.
In my well so in his view, and he's not in the field but he's a very smart person and he reviewed.
And he was careful to say like I don't have a PhD in cognitive science like Gary does he was very measured and almost sweet about it but he said you know I look at this and what I see is these things just keep getting better and my, you know, I'm not worried they're getting there.
And the rate mate people might his his his issue is or his argument is, well you can argue about the rate of it getting better but there's some forward progress or something.
So that was basically the argument and it's not unreasonable but I, you know, I'm not, I've got my own arguments and I came back at him.
And I pointed out that the improvements not as much as he thinks it is was actually a flaw in his kind of statistical procedure because you looked at new things.
There are errors before and show that they got better but he didn't look at the things where that they got worse actually worse now he didn't do like a random sample and whatever and the overall like, it was definitely improvement from GPT to GPT three.
Yeah, not so clearly from GPT three to what we'll call GPT three plus which is the new thing. He kind of overestimated how much improvement there was there, but yes there's some improvement, but there's also some core problems and this is what I think is important where they haven't really been
because most of those are around language. So, I'll give you an example from Dolly which is this thing that takes text makes images. It's perfectly good at saying that an astronaut can ride a horse.
But if you tell it a horse rides an astronaut, which is a much less probable thing, it won't draw it for you and you can actually do some tweaks to get it to do it for you but it doesn't really understand the inversion.
I'm doing this as an homage to Steve Pinker, who has often used the example of man bites dog which itself comes from the newspaper business the old line of newspaper business is dog bites man isn't news happened too many times before man bites dog now that's news.
So, so horse rides astronaut that's news. And these guys didn't let me have access to the system so I had to do this very indirectly but I knew from from what it leaked out that they couldn't do horse rides astronauts I wrote a piece about that as well.
I mean you are a well known researcher like if I had a new AI system I would love you to have research access to it so you could like you could tell me all the areas I need to improve on it like it's free.
It's free QA I can say is you're not running either open AI or Google AI those guys really don't want me to play with their toys.
I wrote about this, too, in one of the recent sub sacs with a quote.
Why is that just because they're afraid they have a little PR thing going where they have now got people, you know, in some of these companies thinking that their systems are practically sentient.
Why, why, why would they want me to poke holes in that. And so like their PR game is to make it sound like they're very close to artificial intelligence and why does that matter, because artificial intelligence when it really comes is a complete game changer, I think.
You know, there's so much of the economy is done.
What's the like, what's the reasoning to get people think it's going to come faster than it is like like they need to raise money or something or raising money getting talent.
Like, so I'll take Dolly as an example it's really Dolly too but I'm just gonna call it Dolly. So Dolly comes out 45 minutes later.
Sam Altman tweets, AGI is going to be wild, suggesting that, you know, they've made progress towards artificial general intelligence here, and you know timed exactly to that somebody I don't know maybe scientific American but I don't remember, runs an interview with like one of the
programmers and says you know what we're trying to do it open the eyes to solve general intelligence.
We think this is a step forward in that direction. You know what Gary Marcus looking at your dirty laundry saying well you know the image synthesis here is really good but the language stuff still doesn't really work who are you getting.
They don't want me to, you know, say that of course, no wall is impregnable so they, you know they promised me access to GPT three but they didn't give it to me and I complained on Twitter and somebody said hey kid I'll give you, I'll give you 45 minutes of
time to see what you can do with it which I did and I wrote a critique and, you know, wrote a piece around that with Ernie Davis called GPT three bloviator, which we wanted to call GPT three bullshit artist, and that is basically what it is and so you know we got some
access, and then Scott Aronson actually gave us a little bit of access to Dolly and we figured out that it had the problems that it does in terms of language and stuff like that with with small amounts of access.
Some other systems have come out since from Google, like Imogen where I publicly asked them like you say you are better at problem X can I give you a few examples and try it and I get no reply.
So, you know, there's been a shift from real science where people would stand up and say, yeah, sure, look at what I've got. Yeah, like test my hypothesis or show me wrong or like, because like these, these systems that that have come out like Dolly or GPT three or GP, whatever GP
eight, whatever it's going to be in the future, like they have some usefulness like they're not all about like they do they have like some in some areas they they are and so I think it is helpful for them to, to let people because because if you point out the flaws that people might not
go for the good things it's like okay here's where it doesn't work here's where it works. Let's let's use this for now and then let's get better in the other areas.
There's already secret about GPT three, which is not so much a secret anymore is that it's kind of like a bull in a china shop. And so, there are a few hundred startups that have been built on its technology, but it's not clear to me that any of them are really thriving.
The biggest problem is that these systems are full of toxic language, they're not very truthy, and you can't really count on them so there are some applications where I think they're fine.
The best one is in my view but I don't know all of them is AI dungeon so AI dungeon is like Zork if you remember those old video games again dating myself to the historic era, where you would type in text and be like, you know it says you see a key and you're like, Okay, take the key put it in the lock and
maybe that would be the magic indication. So imagine that, but a super fun version where you can talk about anything. So you can say, I'm sitting in a dark bedroom in Vancouver with a coffee mug, and some guy is asking you weird questions and then it'll just continue from there and then you
rip on that. And if it makes a mistake so to speak there's no cost to that because you're just having fun if it says something toxic and it tells you you know it questions your sexuality in a way that you don't like you can just turn off the program and it's fine.
Yeah, but if you put that same software in a customer service chatbot let's say which you might think it'll work for but now you're dealing with a customer over a bank loan, and now you tell them to do something unpleasant with their mother it's not funny anymore.
Yeah, yeah, I mean my joke. Well, if it's like let's say you have like agent assist or something like I the autocomplete feature on, you know, in Google Docs or something in the middle of a sentence that often could, could, could with with a decent amount of accuracy can
complete my sentence for me it's all by typing lost bit of complete something faster. It's a human assisted system.
What it is really is like the best version of autocomplete that money can buy because it's trained on a much bigger corpus. Yeah, but it's basically doing what autocomplete doesn't so you know another thing people have used it for is like copywriting so
like for term paper writing, you know, I don't endorse this use but like it could actually be pretty good at that. You know, it probably wouldn't give you an a paper but it'll make something that sounds sort of like the topic and whatever.
It's probably going to make a lot of mistakes is not going to be an a paper.
But like then a human can go through it or something that you can go through it and they are like the commercial question. If you want to do it for anything other than a high school term paper where maybe the student just doesn't care, which is a problem for educational system.
Then there's a question of like how carefully do you have to look at it. Is it worth your while and that's just like people have to do trial and error and see if they can get it to do what they want.
With Dolly it's a sort of similar question makes these fabulous photos, but it seems to be hard sometimes to get exactly what you want and so if you want to use it like give me an idea for a book cover it's amazing.
If you were wanted like something for an advertisement you wanted exactly this thing exactly there with this other thing on top and whatever you might run into this thing where it's just too hard to get it to do what you want and you might get frustrated.
So if you have to make a prediction, like five years out 10 years out. Okay, here's, here's where we're going to see more a lot of progress said, here's an area that maybe a lot of people think we're going to see progress.
I don't think we'll see as much progress in over the next five or 10 years like how would you like and I'm going to put money on this like where would you say hey or and here's where you should put money on.
So deep fakes are going to be just like unbelievably good. They already are videos, audio, all that stuff.
I don't expect that like in five years you could make a whole movie with the whole plot and that kind of stuff. But if you wanted to do it scene by scene or something like that, that stuff's going to be really, really good.
Yeah, so I could I could create a famous person stabbing somebody or something and put out there it'll be so good.
Yeah, exactly. I mean already it's, you know, pretty good so this is not going out on a huge limb to interesting to say that in five years that that stuff's going to be insanely good.
You know, it's already kind of mind boggling. Yeah.
And it's art like people, you know, in the Russian invasion we've already seen some of this I think in both directions if I remember correctly.
So as a part of the like the thing now is like okay how do we train society to to not every time you see something to not assume it's it's real or something like that.
Yeah, that's hard. And we have like kind of weaponized misinformation teams now right and you know every government has one and companies do and so like even just random people have it like they put it out there they put the beads out there.
It's going to be so easy to make those. Yeah, I had a little poll on my Twitter account about when you'd have a version of Dolly for gifts and I think you know most of us including me I don't because I didn't make my public.
But most of us thought like any year would probably have Dolly for gifts.
You know, little simple animations and like, like, if, let's say I like caught on camera, like picking my nose or something and it was true I really did that but I could like start blaming on the deep fake.
I was.
Well that's Trump's move right fake news and you know, there'll be more fake news it'll be more often true when somebody says fake news that it is fake.
Total mess. It's going to be a blank storm I won't use the first word but you know what I mean.
It's going to be mess. So that that's one thing that will get a lot better speed track ignition will keep getting better every year you'll be able to do it like in a louder car.
And you know you'll be able to talk about a few more things with Siri every year or you know Alexa or whatever that stuff's going to continue to grow. It's still in five years not going to be that smart it's still not going to be.
It's Samantha so you know come back to her the movie that you mentioned, Samantha really understood like all of what's going on so you know the one of the opening scenes, she says like you know what's bothering you is like my email.
And he's in she comes back like two seconds later and says well I notice you have 17,000 messages I deleted 2000 of them for you. These were duplicates or whatever and like, yeah, yeah.
We were not going to have machine reading at that well there's one thing from Samantha that we won't have in five years where you can actually trust it to fully organize your email is you have an important message any system right now could easily mess it up so you know I got a message from you to do this podcast that's
an important message, but maybe, you know, I actually, it's a weird week but I got like 20 messages like that I'm not always so popular but like yeah, you know, it could have gone to spam and I mean we have problems with spam filters and like a is not going to solve
that problem immediately, because it still doesn't have enough sophistication like there's, there's a, I think it's x.ai has been promising for years just doing your scheduling.
And for a while I think that humans behind the scenes if I remember correctly I could.
Yeah, well I mean it's incredibly hard.
I mean I have a extremely accomplished assistant that does my scheduling who's extremely smart and like she's a lot smaller than the AI and even there it's like so hard to do it it's like that's a very, very hard task for an AI to do.
Yeah, it's high stakes if you miss a meeting like that really matters.
Yep, like that's not a solid problem so imagine just how hard it is to do scheduling with a machine where you have your calendar in front of you, whatever, but still things come up in the software.
Also you have you have your own nuances like I like to do this in the morning or I need some space to mature and give me some space to go to the bathroom or whatever it is.
And, you know, I think in five years humans are still going to be better.
The machines of that even there's a lot of effort and that's like a narrow part of reading your email so like what Samantha is doing is like way beyond just looking at your calendars presumably.
So another part of Samantha that I think is way beyond us right now is Samantha actually understands human interaction. And I mean she understand that so well that the character falls in love with her.
We actually do have software that people are dumb enough to fall in love with now.
Or I'm trying to find a more polite word to have the will to believe.
But there's a level of like social understanding that Samantha has towards the end, the critical plot twist depends on her not having one piece of social understanding.
She doesn't really get monogamy.
That's quite giving away the whole film.
But she gets a lot about human interaction and what would make people feel better and this kind of stuff. And I don't think we that we're five years away from that I think we're much more than five I don't think it's impossible.
But it's harder so like the paradigms that we have now are like, I show you a picture of a pencil and I say pencil and the machine learns the name of some concrete physical object that we can put in a bitmap.
Something like love or harm or pain or need or, you know, any of these kind of psychological terms, your justice or abstract political terms.
It's just much harder to push those into the paradigm that we know how to use now. And so just to me we actually need different paradigms for some aspects of AI.
Yeah, going back to the Scott Alexander slate star codex thing the other debate that we were having, aside from like how much progress are we making now, and so forth was really like, do we need to change what we're doing or not.
And ultimately he offered me not quite a bet but a prediction. He said, you know that he thought there was a 60% know a 40% chance that we could get to real general artificial intelligence, just by using the tools we have now more data and so forth.
And I wrote a lengthy reply called paradigm shift or something like that, where I said, you know, I thought it was more like an 80% chance which may not sound so different 40 versus 80.
But, you know, I walk through why I think the differences are and why I think it's actually really important that we as a research community, consider paradigm shifts, and why why I think we probably won't get there just by adding more data and we do need something substantial.
But the data is important there is a sense that like as we can join these data sets together, we could potentially solve bigger bigger problems it's like we have access to a very very small amount of data.
I think data is critical. I think it's really interesting that human children become more sophisticated understanders of the world than any computer is now even with a lot less data.
I think, ultimately, you know, you want to take advantage of whatever data you've got. But if it's a small amount of data you still want to be able to do something with it.
I think you know that I built a machine learning company that I sold it Uber. Yeah, and when I sold it I had a conversation with Travis who was still CEO at that point, and I was explaining what my company did which is work with small amounts of data.
And he, he said, Oh, I get it the data frontier problem and he gave the example which was like, he knew how to put the right amount of cars in the right place at let's say 11 o'clock on a Thursday night because he had plenty of data around that.
He just worn enough cars that let's say three in the morning for his techniques that he already had to give a reliable answer so even, you know, Travis who had more data than anybody ever had on anything at that point, still ran into like if you break things down into smaller subcategories
and so, you know, the tenderloin at 3am on a Thursday, you know, there's not enough data there even when you're accumulating massive amounts of data so yeah, if you're Google you have enough data for most things but even Google actually has this problem.
And then, you know, like jet on a runway, maybe Tesla just had zero cases in that you need to solve that in a different way by having a general understanding of what an airplane is, what a large physical object is rather than doing it by memorizing this specific case and looking
for a lot of similar cases so you don't want to throw away the data that you do have it's often extremely useful, but you also need some paradigms that are a little bit less data driven than I think the ones we have now.
Yeah, all right cool this has been amazing last question we ask all of our guests, what conventional wisdom or advice do you think is generally bad advice.
Conventional advice that's generally bad advice.
It's funny that I'm stumped on this one right now.
Because I think there's a lot of it.
How about trust your instincts as a piece of conventional wisdom and it's sometimes true. There was the kind of Malcolm Gladwell part of the story for a while about like experts don't know anything or they do it in a blink.
And it's not really true and in fact, one of the most important things that scientists know is that for almost any piece of data.
Sorry, yeah for any piece of data, you will have your own theory, and it will seem to fit your own theory. But if you think about it carefully from someone else's perspective you realize it could be explained in a different way.
And so if you trust your instincts too much you become too in love with your own ideas, there's an old saying about, you know, falling love with your own press clippings and it's a version of that.
There are two well known ones one's called confirmation bias so you have a theory you notice other data for the other one's called motivated reasoning so you come up with reasons so you can keep believing what you're believing so you don't have to believe that you've made a mistake.
Hey guys so I'm for gun control or something I see you for gun control then everything looks like it's like I could I could I could justify anything or or put a reason or against it or against it or whatever you're against gun controlling you see you've all then you say we should buy more guns like I heard people say that.
And so, you know, I mean, I'm for gun control you can probably guess and I probably can't even pretend that I'm neutral on it but the point is whichever side you're on on any, you know, hot button issue like that, or even smaller things I give you a much smaller one which is like who did the more dishes if you live in a house.
Yeah, let's say two adults that maybe married or whatever they are.
I guarantee you that both people will think that they did more than whatever their fair share is and if you add up, you say give it to me in percentage score.
Yeah, you know, like the person add up to like 130 under 40 years.
Yeah, and if you do it in a group house.
Yeah, like I lived in graduate school like five of us is going to add up to like 270 right like the dishes aren't really getting done.
Or imagine if like your, your, your, your, your baseball team or something like that.
I imagine nobody thinks like the ref is super fair to them right they always say the rest always fair to the other side.
Yeah, every side you're on exactly. So there's all these kinds of biases and stuff like that is if you trust your own instincts you're like I know what that call was I mean he was out.
Who are we kidding that guy was right.
I wasn't out who are you kidding.
Right.
And so, so I think we there's there's value in knowing and calibrating your own instincts but there's also value in thinking about alternative hypotheses.
And, you know, maybe the other person's right and that could be on a scientific matter. It can be on the dishes it can be on your, you know, on the calls in your sports game.
We got all this polarization in the world because we're naturally inclined to believe that we are correct and to not take the other guys view seriously.
And so that'll be the conventional wisdom I will challenge for this amazing I follow you at Gary Marcus on Twitter is that the best place for audience to engage with you.
I would say that and now I have this thing Gary Marcus that substack that Gary Marcus that substack.com yeah which I also like so yeah alright this is amazing well thank you very much Gary really appreciate you joining us.

All right. Thanks for the intro.
Indeed, the title of the paper is
Solucidating the Descent Space of Diffusion-Based Tensive Models.
This is work with Tero, myself, Dimo and Samuli from NVIDIA.
The agenda here is to try and make sense
of these recently immersed diffusion models,
try to really dig into the fundamentals
and, with that understanding, then ask what are the best practices
for designing and wanting these methods.
So for a brief background on generality modeling,
there are many ways to do it, but the idea is usually
you have a dataset of something, for example, in this case phase photos,
but it could be anything, even not images,
and you want to train some kind of a neural method
for basically converting random numbers into random
novel instances from that data distribution.
And after recently GANs where the leading contender in this space,
and these are from there, but now
the denoising diffusion methods have really immersed
as the leading contender here.
So I'm sure we've all seen these superimpressive results
from these models like a stable diffusion,
and everything I'm going to say is basically stuff that runs
at the bottom of these things, and that is in some way directly applicable
to anything like this.
Okay, so all of these methods, the denoising diffusion methods,
the way they implement this idea is you start from pure random noise,
you feed it to a neural denoiser, you keep feeding it,
and reducing the noise level until it reveals a random image
that was hiding underneath the noise, and now you've generated a random image,
so this is a generative model.
One concern with these methods is efficiency, you need to
call this denoiser tens or even thousands of times in some methods
to get the best quality. On the other hand, it's
indeed a trade with the quality of the individual
generative images and with the distribution as whole.
And these tradeoffs are not really well understood in this previous work.
And some methods simply work better than others,
and it's a bit of a folklore that this one seems to be
good one, good and so on.
And there are many ways to formulate the theory of these methods,
so you can approach it with like mug or chains, stochastic differential equations
or exotic ways, but when you kind of strip away
all those fancy theories, in the end they all do something like this.
But they differ vastly in practical design
choices, like at what rate do you reduce the noise level
at different stages of the generation, do you do this?
Oh, it's showing.
Does anyone know where it is?
Yeah, thanks.
Yeah, whether you do this deterministically
or stochastically, we'll see the difference soon.
How do you deal with vastly different tasks or magnitudes
at different stages of this approach, do you predict the signal or the noise?
And so on.
And given that ultimately these are the only differences between these existing methods,
these must be the explanation for their vastly different performance characteristics also.
And these are something we wanted to understand in this project.
So we'll be building on the differential equation
formulation by young Songian colleagues in their paper from a couple years back,
where the images seem to evolve according to a stochastic or an ordinary differential equation.
And in principle it's known that this kind of generalizes
all of those other methods so you can express them in this framework.
But nobody has really gone through the work of getting their hands dirty
and sorting everything into a sort of common framework
where you could compare and understand
the impact of these design choices.
So that's the first thing we are going to be doing here.
And armed with that knowledge, we'll then ask
what are the best practices for running
the sampling process, namely how do you manage this chain
of denoising steps in the best possible way.
First the deterministic version and then the stochastic version.
And then finally we'll come to best practices for
training these neural networks, how do you precondition them,
how do you, what are the lost functions, why does this keep coming back.
Okay, right.
And just one thing, we won't be looking at the actual neural architectures
like whether you should use the answer or not. We'll leave that for future work.
Okay, so let's start with the common failure.
So we'll be studying a few key works
in this field. There's this paper that presents
the so-called VPVE method. There's preserving, there's exploding,
there's DDIM, denoising diffusion implicit model.
It's not really that important for us what the difference is between these
but on the face of it they look kind of like
packages that you have to take as a whole.
You cannot mix and match their properties.
But this is not really true.
So the writing theme throughout this paper
is that we identify this complete and exhaustive set of design choices
that completely characterize and reproduce any given method
or at least these three methods and many others in this space.
And this gives us sort of an extra view
into the internals of these methods. We can ask what are the exact
design choices they made about this and this aspect.
Now don't worry, we won't be looking at slides like this.
I'll try to keep it visual and intuitive to the extent possible.
But the important point here is that this can be done.
And with this knowledge we can then ask what is the best choice
for any given design point here.
And that gives us our method which will be building piece by piece
and that then yields significantly improved results.
And we'll be measuring our progress with the FID metric
which is sort of the current code standard
in evaluating any kinds of generative models.
So let's start looking at how Song and Colleagues
build this, I'll formulate this denoting diffusion problem
using differential equations.
So throughout this talk I'll be using this running toy example
which is actually one day toy example which is actually quite
actually in some many ways completely representative of the actual thing
that's going on with images. So in a way this is one day images
of course you would have more dimensions on the x-axis
the vertical axis with actual high-dimensional images
like one megapixel image is a million numbers so that would be a million
dimensional space. But this describes the essential
characteristics of it. So the point is we have
some distribution of data, let's imagine the cat and dog photos
or something and it happens to be this bi-model thing
so certain pixel values are more probable than others.
And we want to learn to produce novel samples
from this distribution. We have a handful of examples
or let's say millions of examples which is our data set
and based on those we want to learn to do this.
So in this analogy one of the samples we have might be
this dog photo.
On the other axis we have increasing time which is
essentially increasing noise level. That's what we are going to be dealing with
when we want to reduce this noise.
But before we do that let's look at the easier direction
of adding noise like destroying an image. So if I start
take this image from the training dataset I gradually start adding
noise onto it. I end up doing this random work
in this pixel value space until the image is completely
drowned under this white noise. And if I have a
population of images
in the end they will all become indistinguishable white noise.
So if I plot the density that these trajectories make
it will look like this. So the density
of the data on the left edge becomes diffused over time
until it's completely normally distributed at the end.
And this is really nice now because it has disappeared again.
No.
I'll try one thing.
Can I get the...
Well maybe we will just live with it.
Do you think we can do that?
Yeah.
Let me know if something important seems to be missing underneath it.
Okay. So as I said we can sample
from this normal distribution at the right edge. We just go random in
white edge and that will give us a sample from that edge.
And the magic is that there exists a way
to sort of reverse this path we took earlier. So go
backward in time and that will land us on the left edge
where we have the density of the actual data. And that of course generates an image.
And so if I have a population of these complete random noises
oops
okay.
Yeah, if I had many images I would have gotten different
instances of the image. Okay.
And what makes it stick is that this can be seen as a
stochastic differential equation. In this example it's
about the simplest one we have. When we go forward in time over
a smart very short time period, the change in
image dx equals the omega which is white noise.
So that's just a mathematical expression of doing
cumulative sum of random noise. Now the magic
is that to this forward equation corresponds a
backward version that has this same stochastic component
random walk component. But on top of that it has this
term that kind of attracts the samples towards the data density.
You see it's some kind of a gradient of the of the deadness dense p.
But the problem of course is that this p is unknown.
And here is the actual magic. This is a well-known function
from previous literature in data science
called the score function. And it has the property
that you do not need to know the p if you have a least
the. So you can directly evaluate that formula above by the formula below.
And this is an opportunity we train a neural network
to be such a denoiser. And this means that we can run
this kind of backward equation evolution using
that the learned denoiser.
Okay so some colleagues also present this deterministic variant
of this where you don't have the stochastic term.
You only have this core term scaled in some appropriate way.
And this has a somewhat different like a visual
character. You see it's kind of fading in and out
instead of like a jittering around.
And this one actually provides a much cleaner view into the sampling dynamic.
So we'll be looking at this first and then returning to the stochasticity later.
And with this I can now
always draw these paint flow lines of this ODE.
So the idea is that we are trying to somehow follow these lines to do the generation.
And indeed the way that happens is by discretization.
I take little but macroscopic steps
in this space I reduce the time.
And for any change in time I want to jump.
The ODE formula tells me how much the image changes.
And again the ODE formula already does neural network.
So the neural network tells us where to go on the next step.
That's the general idea.
And that gives me a step. I keep stepping until I hit time zero and that's my generated sample.
With the SDEs we would have some kind of noise addition on top of this.
So we would kind of jitter it.
But as I said we'll leave that for later.
And now we've exactly reproduced this intuitive picture
using different solute quizzes.
Okay so that was song and colleagues for our purposes.
And let's now identify some design choices involved
in making this kind of an ODE or an SDE.
But before we do that we should understand what can go wrong in this process.
What are the error sources? Well the obvious one.
Because I might end up in a different place than I should have when I do this sampling chain.
So the obvious one is that if the network gives me an incorrect direction
I end up moving in the incorrect direction
and in the end I end up somewhat in the wrong place.
It's more subtle than this but this is kind of a cartoon.
Is that we are trying to approximate this continuous trajectory in green here.
Using these linear segments.
And if I try to jump too far at once
the curve will kind of move away from my feet
and I'll end up veering off this path.
It's of course familiar to anyone who's done like a physical simulation with ODE.
And the brute force solution to that is to take more steps.
But that's exactly what we want to avoid.
Because that directly means more compute to generate an image.
Okay.
And so what we argue and what is underappreciated in previous work
is that these two effects should be analyzed
or can be and should be analyzed in isolation.
You have to sample in a certain way just because you train your own network in a certain way.
And so on you can decouple this.
And indeed we'll be looking at sampling first and then coming back to the training later.
Okay, so I promise to show you some design choices and here is one finally.
So when I built this example
I added noise at a constant rate over every time step
and that gives me this, implicitly gives me this schedule where the noise level increases
as a square root of time.
Because that's how the variance will grow linearly so the standard will go
square root.
That's what you get if you call random and then do a comp sum on top of it.
Had I added it at a different rate I might have arrived at a schedule like this for example
where the standard deviation is the gross linearly.
And indeed I could do any kind of a choice here
I could do something even something crazy like this way we schedule here
in the middle if I wanted to for some reason.
And indeed we generalize in the paper, the ODE form
or we re-parameterize it in such a way that we get a clear view
into these effects so we can parameterize
the noise level we want to have reached by explicitly
by this function sigma.
But the real question is why would you want to do something like this?
Well one reason for that could be
that if you look at this picture for example you see
almost nothing happens until at almost
the zero noise level suddenly curves rapidly
to one of these two basins and
there's high curvature there so we probably want to be careful in stepping
to take somehow be more careful in sampling that region and less careful
you are in the bulk. So there's two ideas of how
you might do that. First you might take
shorter steps. At the more difficult part usually
low noise levels because that's where the image details are
usually built. The other alternative would be
to instead warp the noise schedule in such a way
as just end up spending more time at these difficult parts.
And it's tempting to think that these
two approaches would be equivalent and this is an implicit
assumption I think that many previous works do. But this is simply not true
because the error characteristics can be vastly different
between these choices like the error that comes from this
tracking this continuous curve and we'll see the effect
of that later. So now we have identified
a first pair of design choices here the time steps
and the noise schedule. But let's introduce a couple more.
So then there is signal scaling
and this address is the following problem. I zoom out a little because in reality
we add a ton of noise so at the other extreme
the noise level is very large. I've been showing this zoom in so we can
easier see what's going on. But I zoomed out now to see
what's here. So the issue if you don't do anything
is that the signal magnitude grows as the noise
level grows. You keep piling noise. The signal is quite simply bigger
numerically like the values that are in your sensor
they are much larger at the high noise levels than in the low noise levels
and this is known to be really bad for neural network training dynamics
and these kind of effects are actually critical to deal with
to get good performance. So the way
many previous works brought this is by using something called variance preserving
schedules where you effectively introduce
this additional scale so called scale schedule
where you squeeze the signal magnitude
into this constant variance tube
so that makes one way to make the network happy here.
So we generalize this idea again by
just formulating an ODE that allows you to directly specify any arbitrary
scale schedule and viewing these like it again becomes
appropriate that the only thing that the scale schedule does is distort
these flow lines in some way. So you are just doing
a coordinate transform in a way on this XT plane.
Now there is an alternative way to deal with this
scaling issue and it is quite simply this
instead of changing the ODE at all you could
change your neural network in such a way that it has an initial scaling layer
that uses the known signal scale. Scale it to something
that's palatable for the neural network.
And again you might think that this is completely equivalent with the ODE
but this is simply not true because again the error characteristics are
vastly different between these two cases. And I'll come back soon
to how the chosen practice. But now
we've identified a second pair of design choices
the scaling schedule and the scaling that happens
inside the neural network itself. And that we
have the so-called preconditioning of the neural network.
Okay so now we have quite a few
collected here. And at this point we can ask
like get our hands dirty, go look at the
appendices of these papers, read their code for the final
ground truth and ask what formulas actually
exactly reproduce their approaches
and they are these. Again don't worry but don't even try to read them.
But the question now is what choices should
we actually make, which ones of these are good, which ones suboptimal.
And that's going to be the topic of the next section. And for now
we will be ignoring these neural network training aspects. We will be
using pre-trained networks from previous work. We won't
be retraining anything yet, we'll just try to improve the sample.
So now we move on to the deterministic sampling and
actual prescriptions of what you might want to do.
So first the noise schedule. Why would some of them
be better than others? For example this way one must be terrible
for some reason, but why?
Well now we get a clear view.
Well parameterizing things in this way gives us a quite a clear
view to this. So let's zoom out again.
And consider the fact that we are trying
to follow these curving trajectories by following these linear tangents.
And that's probably going to be more successful when the tangents
happen to coincide with this curve trajectory. So when the trajectory
is as straight as possible in other words. So if I use a bad
schedule like this one you see there's already a visible
gap between the tangent and the curve. So you easily fall off if you try
to step too much. And indeed if I show you
this random family of different schedules we
see that some of them seem to be better in this regard than others
in particular this one.
And this is actually the same schedule used in the previous
work DDIM which is known to be quite good. And this
in a way explains it. So this is the schedule
where the standard deviation goes linearly and we do not use any
scaling. And indeed we'll be leaving the scaling
for neural network parameterization. And the reason for that
is that the scaling also introduces unwanted curvature into
these lines. That's this, it just turns them
unnecessarily at some point. It's actually better to
let the signal in the ODE grow from that
perspective. As a further
and yeah with this the ODE becomes very simple.
So as a further demonstration like an actual mathematical
fact about this schedule and why it allows
us to take long steps is that if I took a step directly to times zero
then with this schedule and only this schedule
the tangent is pointing directly to the output of the denoiser.
And that's very nice because
the denoiser output changes only very slowly during
the sampling process. And this means that
well the direction you are going to doesn't change almost at all
so it means you can take long both steps and you can
consequently only take a few steps or many fewer steps
than with the alternatives.
Okay and then I said we'll want to direct our efforts to the difficult places.
Now we've tied our hands with the
noise schedule so the remaining tool is to take different length steps
at different stages of the generation.
And indeed when you go look at the
possibly implicit choices the previous methods have done all of them take
shorter steps at low noise levels because that's where
the detail is built again.
Yeah we formulate this family
of these discretizations like a polynomial step length
growth and we find that there is a broad optimum of good
schedules there. You can read those details in the paper.
So there's one more thing that this ODE framework allows you to do.
Which is not so clear with for example the Markov chain form lessons
is use higher order solvers which again
there is going to be curvature and it can be quite
rapid at places. So you can still fall off the track
if you just naively follow tangent and that method of following the tangent
of course is called the Euler method.
But there are higher order schemes for example in the Hoin scheme you take a second tentative
step and you move it back to where you started from
and your access step is going to be the average of that and the initial one.
And this makes you much better follow these trajectories.
This of course has a cost you need to take these sub steps
and what we find in the paper by extensive
is that this Hoin method strikes the best balance
between these higher order methods or sort of the extra bank for the block.
And the improvement is actually quite substantial.
Okay, so those are the choices we made.
And now we can evaluate.
So we'll be evaluating these results throughout the talk
on a couple of very competitive generation categories
Saifat Sen at resolution 32,
even at resolution 64. And I want to say a couple of words
on this might sound like a useless example to you
if you're used to seeing like a output from a stable diffusion or something.
But the way also those methods work
is they first generate a something like a 64 by 64 image
and then they upsample it sequentially. And it turns out that generating this 64
image is the difficult part there. The upsampling just kind of
it just kind of works. So this is highly
indicative of improvements we get
in very relevant classes of models.
Okay, so if we look at the performance
of the original samplers from these works
from a few previous methods on these datasets
we see that we have the quality
on the y-axis, the FID lowers better and we have a number of samples
we need to take like number of steps or the function evaluations on the x-axis.
We see that we need to take something like hundreds or even
thousands of steps to get kind of saturated quality, to get the best quality
that model gives you. So introducing the whole sampler
and our discretization schedule, we vastly improved
this. I noticed that the x-axis is logarithmic, so
we've gone from like hundreds to dozens of evaluations.
And further introducing
the noise schedule and scaling schedule further
improved the results for a large amount except in DDIM, which was already using
those schedules.
So now we've already made it quite far here
and using some super fancy higher audience over
it's not worth the effort.
So now we've covered the deterministic sampling
and let's next return to the
question of SDE, which we put on the back burner on the early slides.
And remember instead of following these nice
smooth flow trajectories, the SDE
sort of cheaters around as some kind of exploration around that baseline.
So it can be interpreted as sort of replacing the noise as you go
on top of like reducing it. And the reason why people
care about the SDE is of course, well one reason is that
that's where this stuff is derived from, but the other is that in practice you tend
to get better results when you use the SDE instead of the early, at least in previous work.
And the reason for that will
become apparent soon, but let's first generalize this idea a little.
So in the paper we present this generalized version of the SDE,
so it allows you to specify the strength of this exploration
by this sort of replacement, noise replacement schedule, so
especially when you set it to zero, you get just the ODE.
But you can also do more exploration by boosting this factor
or you can do more exotic schedules
like something like this, where you have it
behave like an SDE in the middle and like the ODE elsewhere
and samples would look like this.
But again that's the question of, is this just a nice trick or
like what's the point?
And yeah, as I said, empirically it's improved results
and now looking at this SDE, the reason becomes somewhat apparent.
So don't try to read it unless you're
an expert in SDEs, but we can recognize a couple of
familiar parts here. So the first term in the SDEs actually
is just the ODE from the previous section. So that means
that we still have this force that is driving us
towards the distribution and making it follow the flowlines.
And the remainder we can identify something called a lens around diffusion
stagastic difference equation, which is a well known
thing from a long ago. It has this property
that it makes the samples sort of explore your distribution and if the samples are not
distributed correctly it will kind of
reduce that error. So it has this healing property
and because we do make errors during the sampling
it kind of actively corrects for them and this is how it looks like.
So let's take this extreme situation, we have our samples to
blue dots and let's say they are really bad, they are not following
the underlying distribution at all, they are skewed to one side.
So if we keep following the ODE
it does nothing to actually correct the skew and we completely
miss the outer basin of the days for example.
So when I introduce stagasticity to this process
it starts looking like this. So these samples do
this kind of random exploration and gradually forget where they came from
and forget the error in this opposition and now we have covered
both modes for example in the generated images on the left edge.
So that's the sort of reason why
stagasticity is helpful.
Now arguably this is the only benefit of the SDE over the ODE
but there are also downsides in using SDEs, for example we
would technically have to use these complicated solvers
that are arguably designed for much more complicated cases
where you have more general SDEs.
So we asked the question could we instead directly combine the ODE solving
with this idea of this churning of the noise
adding and removing it and the answer is
so this is a stagastic sample we proposed in the paper
so we have our current image, noise image
at noise level T, remember in our parent recession
the noise level is now actually completely equivalent with
time.
So we have two sub steps in one step so first we add noise
so this represents the lens of an exploration
so we landed some random noisier image
so the time increases here and then we
add the ODE to where we actually wanted to go which is a lower noise level
and that simply follows
the flow line there and in practice we do this with a single
hoinstep. So we keep alternating between this noise addition and the hoinstep
and this brings us closer and closer
to time zero as we want but underneath it is the ODE
running the show and guiding us
along these lines where but on top of that we now have this jittering
which corrects errors.
Okay so this also sounds really nice you get free error correction
but it's not actually free because the lens event term is also
an approximation of some continuous thing and you introduce
new error also when you make it so it's actually a quite delicate balance of how much
you should do this and now with this clear view into this
dynamics we actually find that it is really finicky it's you need to
tune the amount of stochasticity on a per data set
per architecture basis you get the benefits but it's
really annoying so it's a mixed bag
nonetheless it is very useful so if you compare
the ODE from the previous section is their performance
with just the original SDE samples from these respective works
we get we see that the SDE solvers are simply
better in the end but they are also very slow
now applying all of these improvements
with our method with the optimal
tune settings for this data set we
read both much better quality at a much
faster rate
and yeah there's been some previous work that applied
also higher order solvers
so I want to highlight one result here
this image net 64 highly competitive
just with this change of schedule we went from a pretty mediocre FID
of 2.07 to 1.55 which at the time of getting
this result was state of the art but that record was broken before the publication
but we'll have our revents in a few slides
but just to show that this
is just taking the existing network and using it better
you'll shoot improvements already
okay so that's it for deterministic sampling
at this point I have to say I'm going to go a bit overtime because of the hassle in the beginning
and because this is kind of incompressible anyway
if you need to leave then no problem
so yeah that's it for stagastic sampling
and for sampling as a whole
we are now done with that and as I promised
we are now going to be looking at how to train these networks
how to parentize them in such a way that they give reliable estimates
of where these steps should be pointing and again we won't be looking at
the architecture itself
so just a brief recap the way this works
was that the role of the ODE is to give us the step direction
which is given by the score function
which can be evaluated using a denoiser
which can be approximated using the neural network
and that is the role of the neural network here it tells you where to go in a single step
and the theory says that
as long as the denoiser does something that minimizes this loss
the L2 denoising loss
the theory will be happy
and you can do it separately at every noise level
so you can weight these laws according to the noise level also
but before we go to these loss weightings let's look at the denoiser itself
so I joined the CNN there in a bit of a hazy way
and this because it's actually a bad idea to directly connect the
noise image to the input of the network or to read
the denoised image from its output layer
rather we'll want to wrap it between some kind of signal management layers
to manage those signal scales
of both the input and the output to standardize them somehow
and also in this case we can often recycle stuff from the input
because let's say if the input image is almost noise free
then we don't really need to denoise much we should just copy
what we know and only fix the remainder
we're going to come to that soon and this is super critical here
I mean this might sound like boring technical details but these
kind of things really others are like critical
for the success of the neural network training and we've seen this
over and over again over the years and in this case
the noise levels vary so widely that this is extra
critical here so without too much to do here is how
one of the previous methods the BE method implements the denoiser
so the idea of this setup
is that they are learning to predict the noise
instead of the signal using those CNN layers
and the way that works, and I'll explain why soon, the way that works is of course
the loss will be happy if the denoiser can produce
the clean image and we can interpret
this model as having this kind of a skip connection
so the noisy input goes through that
now implicitly the task of the CNN will be to predict
the negative of the noise component in that image
and then they have an explicit layer that scales
that noise to the known noise level
and so now when you add whatever came from the skip connection to this
you get an estimate of the clean image
so this way they kind of turn it so that the CNN itself is concerned with the noise
instead of like the signal
but first let's do the thing I promised to do a long ago
I said there's huge variations
in the magnitude, just the numerical magnitude of these input signals
and this architecture fails to account for that
which is problematic and so we quite simply
introduced this input scaling layer here
that uses the known standard deviation of the noise to scale the image down
I want to highlight this not like a batch normalization or something
we know what the noise level is, we know what the signal magnitude should be
we divide by an appropriate formula
so that gives you one of the wishes we had
on that orientation slide
on the output side we actually have something nice already
so this is very good because now the network
only needs to produce a unit standard deviation output
and this explicit scaling to the known noise level takes care of
applying the actual magnitude of that noise
so this makes it much easier for the neural network
it can always work with these standard sized signals
and that deals with the second hope we have there
but now the question of should we predict the noise of the signal
and why, so it turns out this is actually a good idea at small noise levels
but a bad idea at high noise levels
so I'll show you what happens at low noise levels
so if we have low noise the stuff that goes through the skip connection
is almost noise free already
and now the CNN predicts this negative noise component
and it's scaled down by this very low noise level
and this is great
because the neural network is actually the only source of error in this process
so if the network made errors now we've downscaled them
so it doesn't really matter if the network is good or bad
we didn't do much error in this case
so that's great we are sort of recycling what we already knew
instead of trying to learn the identity function with the neural network
so that kind of deals with the third hope we had on the slide
but on high noise levels the situation is reversed
whatever comes through the skip connection is completely useless
it's a huge huge noise signal with no signal at all
and now the CNN predicts what the noise is
and then it is massively boosted at this stage
so if the network made any errors now there are going to be huge errors after this
and those are directly passed out of the denoiser
so it's a huge error into our stepping procedure in the ODE
it's also a bit of an absurd task because you're trying to subtract two massive signals
to get a normal size signal
and kind of like trying to draw with a two meter long pencil
not optimal
so instead what we'd like to do is somehow disable the skip connection
when the noise level is high
and in that case effectively the task of the CNN
will be to just break the signal directly
there won't be any need to scale it up
so we won't end up boosting errors
and the way we implement that is by adding this sort of switch
but in a continuous way
so we have this so-called skip scale
which when set to zero effectively disables the skip connection
set to one you get the noise prediction
and furthermore we make it so that it's actually a continuous value
between zero and one that depends on the noise level
and
if it's somewhere in between that means that we are predicting some kind of a mixture
of a noise and a signal
instead of just one of them
and there is a principle way of calculating what the optimal skip weight is
but I won't go there in the interest of time
we have it in the paper of Enix
and that deals with the remaining issues we had on that slide
and now we can look at what the previous works did and what we did
so these are the actual formulas that implement those ideas
okay
then there is the couple of training details
how should you weight the loss based on the noise level
and how often should you show samples of different noise levels
so the general problem
if you don't deal with these issues is that you might have a highly lopsided distribution of like gradient feedback
so if you're not careful
you're just on most interest and you might be prodding the weights gently to one direction or the other
and then every few iterations you have this massive
gradient smash on the weights and so on
and that's probably very bad for your training dynamics
so the role of the loss weighting or the scaling
the numerical scale in front of the loss term
should we just equalize the magnitude of the loss
or equivalently equalize the magnitude of the gradient feedback it gives
and then the noise level disabusion
maybe how often you show images of any given noise level
the role of that is to kind of direct your training efforts to the levels where you know it's relevant
where you know you can make an impact
and for that we in the paper we have this sort of an important sampling argument
that whatever we do we end up with this kind of a loss curve
so we don't make much progress at very low and very high noise levels
but we do make a lot of progress in the middle
for example at very low noise and you're trying to predict noise from a noise free image
it's impossible but it also doesn't matter if you can do it
so we based on this we find that it's enough to
to find this to sort of have this very broad distribution
of noise levels here that are targeted towards the levels where you know you can make progress
and this is a logarithmic scale on the x-axis so it's a lot of normal distribution
okay so those are those sources and it's starting to pretty full
there's one more thing which I'll keep in the interest of time
we have some mechanism presented in the paper for dealing with vastly like two small datasets
when your network starts overfitting by this augmentation mechanism
you can look at it there but yeah let's not go there
it's really only relevant for various small datasets like Cypher
with the ImageNet we haven't found benefits from it I think
okay so with all these improvements we can stack them one by one
these are the lines here and in the end we get state of the art results
in various competitive categories
in deterministic sampling we get an FID 179.97 on the Cypher categories
which might still be state of the art
also at very low sample counts compared to most previous work
that's more interestingly
okay that was with deterministic sampling
when we enable the stochastic sampling and tailor it for these architectures for ImageNet
and use this retrained
these networks we trained ourselves using these principles
we get an FID of 1.36 which was a state of the arts
when this paper came out
it's been overtaken I think in the last few weeks possibly earlier
but yeah
so all in all we've turned this model that was okay-ish in the beginning
and by stacking all of these improvements
we get the best model in the world at that time
for generating system for ImageNet
interestingly the stochasticity is no longer helpful
with Cypher in this resume
or after the training improvement
so it appears that the network has become so good
that it doesn't make that many errors
any lens of an exploration you do introduces more error than you are actually fixing with
but this is still not the case with ImageNet
so there it's still pays to do stochasticity
okay so that was the
that was mostly it's just a brief conclusion
so we've sort of exposed this completely modular design
over these diffusion models
instead of viewing them as tightly coupled packages
where you can't change anything without breaking something
we show that you can pretty much change every
like you get a valid method no matter what you do
as long as you follow these loose guidelines
and then with that knowledge we get a clear view into
what we should actually be doing with those choices
and doing so pays off in a big way
so it's a bit of a double edged sword
as said it does help
but it also
it requires that annoying per case tuning
there are no clear principles how to do that tuning
there is also a danger that you can even have bugs in your code
stochasticity will fix them
to an extent which is of course not what you want to do
if you're trying to understand
what your potential improvements are
what their effect is and so on
ideally you'd be able to work in a completely deterministic setting
and if you want then in the end just kind of reintroduce the stochasticity as the final
final cherry on the top
okay so we haven't talked about all the fancy stuff like higher resolutions
network architectures, classifier free guidance and so on
but probably many of these would be right for a similar principle analysis
we hope this inspires you to also think about that kind of things
and certainly we are
so with that the code and everything is of course available
I would argue this is probably
one of the better places to copy paste your code
if you want to experiment with stuff it's very clean code base
that directly implements these ideas
so thank you for your attention
hey
so we have time
yeah I have time
how does it go
good discussion but that's the last question
alright
is that an explanation for why stochasticity
stochasticity is only the basic concept
it probably has to do with the data's complexity
like
safaris may be a bit too simplistic in the end
it's kind of learnable entirely
but it seems well like that
something like ImageNet it's still so extremely

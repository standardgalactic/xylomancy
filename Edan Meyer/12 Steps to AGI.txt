The title does not mislead you. This is indeed a 12-step plan.
12 steps right here. You can count them 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
ending with 12. That starts from essentially basic machine learning and ends up with the goal of
basically the singularity, what they call intelligence amplification.
And gosh was this a fun read and interesting to think about. You might be wondering what
random dude drank a bit too much one night and decided to write down their brilliant plan for
this. But as it turns out, this is not written by some random dude. Rather, this is a new paper
by Rich Sutton, Michael Bulling, and Patrick Polarski, who are all well-established researchers
that head the Alberta branch of D-Mind and are also professors in the area. So yeah,
I guess this is kind of unofficially a D-Mind paper. And not only is this just some plan,
it is now the plan for research at the Alberta branch of D-Mind. Hence why it's called, you know,
the Alberta plan. And it's just out there. It has some, I think, really interesting ideas and no
one's been talking about it. So today, I hope to take you through the motivation for this and the
plan itself. It is very interesting in the sense that it's building back up from the foundations
of machine learning and reinforcement learning. And while it is very similar in most ways,
it's also somewhat contrarian to many of the approaches, especially in reinforcement learning
that are popular today. I love this because if you're a new graduate student, or you can't find
your research topic, or you just want to get into research, well, here's 12 research topics
handed to you on a silver platter, you really can't ask for much more. And it's also just,
I personally think interesting to know what some prominent researchers are thinking about
and how they're going about approaching these problems that lots of us are very interested in.
One last thing I'll mention before we dive into this is I cover lots of big ideas on this channel
to keep you up to date with what's going on in the field, but also smaller ideas kind of like this
kind of go under the radar to introduce you to some new interesting ideas. So if that's the
type of thing that interests you, consider subscribing to the channel. It means a lot and
it really does help out. Anyway, let's dive into this. In this paper starts, we're going to come
back to the first two paragraphs in a second. I just want to start with the third paragraph,
because it really explains the core of what's going on here, what they're thinking. So starting
from here, following the Alberta plan, we seek to understand and create long lived computational
agents that interact with a vastly more complex world and come to predict and control their sensory
input signals. And blah, blah, blah. They are as scalable as possible. If you've seen Rich Sutton's
better lesson, maybe they're starting to look a little familiar, and they have to adapt and
change the complexity, which means they have to continually learn. Big focus on this here. And
then another thing they mentioned is they must plan with a learned model of the world. Now,
I highlighted these specific things, except for the word follow, but I highlighted these specific
things. Because these, I think, are things that are very core to this paper. So long lived, right,
and continuing learning. These are things that some people in RO work on, but are not super
focused on that are really the sort of at the forefront of this paper. That means that agents
are going to have to be able to work with certain challenges that are present in continuing learning.
For example, if you try and take a neural network and have it continue to learn on a constantly
changing task, many people don't realize this. But what actually happens is neural networks get
worse and worse at performing or essentially learning new things, kind of like that, what do
you say, an old dog can't learn new tricks. There's actually some concept of that in, I think, machine
learning that lots of people are unfamiliar with. So things like that, those are issues that might
be important in this situation. Another thing, vastly more complex world, predicting and controlling
sensory inputs. Some of these things are very like what you would assume is common, but you'll see
that everything works under these assumptions in this paper. And we'll get into some of the more
details of why I think some of these, you know, approaching these types of problems is interesting.
The one thing that does stick out here a little bit is I think planning with a learning model of
the world. Because the other things are problems, right, sort of having to be long lived and
continually learning these, you know, having a vastly more complex world. These are statements
about the problem. Whereas planning with a model is like a specific thing they do. So I guess they
just think it's that important, which I would agree. But anyway, just some details about, you
know, sort of the logistics of this plan before we maybe get into it. This is a five to 10 year
plan, as they mentioned here. They also, you know, some of this is very like, you know, we think
this will greatly affect the economy or society or individual lives. I don't think we need to
go through all the, all the philosophical stuff. We can kind of get into the meat of this. So the
purpose of giving this plan, which they write about here is, as they say, it's twofold. The main
one is, or one of the two is that they want to sort of give their vision for AI. Well, lots of
people have been working on these problems, like continue learning. They think it should really
be at the forefront. So some of those things are a little bit different. So this is their vision,
what problems they think should be focused on. And then also laying out, of course, a research
plan for the purpose, you know, of doing research. This is not a to-do list. This is not like a
things they completed. This is a research plan. They, they, as they very much say here,
gaps and uncertainties, which are to be expected, right? When you're tackling these big problems,
when you have literally 12 steps, of course, it might not be enough. Maybe some things aren't
going to be necessary. Things are going to change throughout this plan. There are some
key points or some important things that they think sort of differentiates this Alberta plan
from lots of the work going on at other places. So that's a little bit what they talk about in
this next section. Ah, the other thing in this next section is this diagram that reminds me,
I forgot to say what our RL is. So this is going to be just a five second explanation. If you don't
know what reinforcement learning is, just imagine you have some environment like the world, you
have an agent, you know, it sees things from the environment, gets observations, takes actions,
and it tries to maximize some reward. There's your five second explanation. Hopefully that keyed
anyone in it wasn't okay. Anyway, what are these core tenants that they think differentiate the
Alberta plan? So the first one, here it is, they say the first distinguishing feature is the emphasis
on ordinary experience as described above. So an ordinary experience, as they'll mention here,
this means that you can't do things like use specialized training sets, you can't use human
like assistance, you know, you can't put human data into this, you can't access the internal
structure of the world, which lots of work and reinforcement learning does, right? Not everything,
but lots of it does. And I should say, maybe clarify human assistance here does not mean that
an AI cannot interact with humans. It just means that once like the learning process has started,
you can't have a human like tweaking like different parameters and like, you know, maybe training a
bit like this and then a bit like this. It means that the learning process, you kind of have this
agent, it starts out and then it has to do everything by itself. And why is this so important?
And the idea here is that these types of methods that require human assistance, they usually don't
scale, where do they say this, they typically do not scale with computational resources,
and as such is not a research priority. Now, if you've read the bitter, the bitter lesson that
that's basically the, you know, this is the one paragraph version of this. So what's the next thing
that they put a focus on the next thing is temporal uniformity. What does this mean? Essentially,
it means that at all time steps, so at all times, in terms of the algorithms running, everything
needs to be the same. You can't have, and this again goes back to what I mentioned earlier,
you can't have a separate set of training data and testing data like you would have
in supervised learning or lots of people when they're evaluating reinforcement learning agents,
like on the Atari 100K benchmark, I think it's pretty common to train with 100,000 steps of data,
but then the actual testing is done on different data. So that would be a no, no here, right? And
you might be like, Oh, that's heresy, no training and testing, no separation. Well, yeah, but you
have to remember, we're working in the continual learning setting, which means that you can just
keep simulating your environment and getting new experiences. And well, if all those experiences
are the same, well, then maybe your environment just isn't a very interesting environment at that
point. So this is essentially talked about in the paper here, you know, they say no special training
periods. If information is provided, then it has to be the same information on every time step,
or if it learns to plan, then it has to plan on every time step. If the agent constructs its own
representations or subtasks, then the meta learned out are the meta algorithms for constructing them
and operating them operate on every time step. So you might be wondering like, well, aren't on
some time steps, maybe where things are more complicated, won't we maybe want to plan more,
like look further into the future? Yeah, maybe we'll want to do that. But the idea here is that
should all be internal to the agent, as opposed for a human tweaking these parameters, right?
You could have a meta learning algorithm that learns those types of things. And in general,
I think this, why do they do this? I think they actually say it fairly well. Their focus on this
is to leads us to interesting, non stationary continuing environments, and in algorithms for
continue learning and meta learning. So in one sense, I understand totally how this leads them,
I have way too much yellow on this page, I'll start using something else. So algorithms for
meta learning, this makes sense as I just mentioned, right, you need meta learning,
when you can't have a human tweaking these things, you should have an algorithm do that.
A meta learning is one way to do that. On the other hand, I don't see how this really leads
to continuing environments or like continue learning, I would think it would be the other
way around, right? You have continually like continue learning. And hence, you're going to
want to focus on temporal uniformity, but maybe there's something I'm not seeing here. And another
reason they mention this is that keeping everything temporarily uniform, it reduces the degrees of
freedom and shrinks the agent design space, which to be fair, they think there's a decent point,
really have plenty of things to look into. There's no reason to, you know, forcibly make that
higher when we could simplify the space. Then if we go down a bit more, this is one thing we
touched on earlier. And this is also from the bitter lesson. But the third distinguishing factor
is its cognizance of computational considerations and Moore's law. I do think lots of other,
and this is essentially all this means, right, is compute computations getting stronger, or
we have more of it, or it's more efficient. So we want to be able to make use of that. So we
can't have anything that's not going to scale. And I think this has been getting better over the
years, my personal opinion, it looks like lots of people are working in areas that do, or on
methods that do scale fairly well. Deep learning is one great example of this, right? And basically,
everyone's using deep learning now. Oh, they even namedrop the bitter lesson. Okay, lovely.
So then let's move on to the fourth. And I think this is the final one. The fourth distinguishing
feature is the focus on the special case, or is it the special case, in which the environment
includes other intelligent agents. Now, what does this mean? It doesn't mean that there need to be
other intelligent agents, but rather that intelligent agents. So like if I'm, you know, modeling a
robot that's interacting with humans, you know, there's, there's nothing fancy that I do to model
the fact that there's humans in the world, rather, they're just part of the environment. And I need
to be cognizant, and I think they say this right here, you'd be cognizant, that the environment
may behave differently. This might sound weird, but the environment is essentially, you can think
of it as, you know, a living, breathing thing, or at least it could be in response to your own
actions, right? So what you do might make the environment behave differently. And that's, you
know, we shouldn't do anything special for these other agents. To be honest, I don't think they
even need to put this here. Because I think this follows under the bullet point of using no human
experience, right? The idea that we have other intelligent or like conscious agents, that that
is something that we're sort of embedding our human biases into this, right? Technically, we
can't even prove other people are conscious. So I don't know, I don't think this is so weird,
but I guess lots of places that do work with multi-agent settings, they do do this sort of
thing where they're explicitly trying to model other agents. So that would be a no-no in this
sort of setting, or at least that's, that's what they want to work with. Okay, so if we scroll down,
we were almost to the research plan. I swear we're getting there. We're going to spend a good bit
of time on that once we get there. But the last thing we need to touch on is the base agent. So
I believe this is what they call the common model of the intelligent agent, as you're looking at
right here. And they say that this is used in a few things, like things, areas like economics have
similar ideas. But the idea is that we want to start from some base that we can maybe agree on,
not that it's necessarily the best or something like that, but that this is a reasonable base
that will inform the rest of the research. So what's essentially happening here is if we
step through this, we start with an observation from the environment, pretty standard, then it goes
into this perception. So perception, and this is going to be a recurrent thing, right? You also
taken the last action, and you can see the output of perception feeds back into here. So this is
recurrent. So this would be keeping what they like to call the agent state, which is like the
agent. So some people call this the belief state. It's maybe what the agent perceives and wants to
remember about the world to do whatever it needs to do. It has reactive policies. So the policies
are what transform these observations and the actions. It has a value function. This is a normal
value function and reinforcement learning, right? It essentially assigns value to certain states,
which helps you do things like credit assignment, and it has a transition model, which allows you
to do, as you can see, planning and planning means that you can more efficiently, you know,
imagine scenarios in your head, not have to play through them in real life. And then that means
that you can be a lot more sample efficient and potentially have other benefits to one question.
And this is a question I used to have is why have these things? They are there one, two, three, four
things. Why not other things, right? We could have, for example, a different portion here saying
we want to model like other agents. I mean, we just said like this, this is not good. We should
do this. But why not? Why are these other things okay to have? And the difference here, I don't
think they mentioned the paper. But the difference I believe they would say is that these four things
that they have here, they're not domain specific. These should essentially work in any domain.
They're general. They're very general. Whereas modeling other agents, well, you might not always
need to do this in the way in which you might do this will probably change drastically, depending
on what types of agents you're working with. So for that reason, these four things stay and other
things don't quite make the cut it has to do with the generality of these four components. Another
thing you might mention is that the year there are S is right here. So reactive policies, value
functions, that is something we'll get into. These don't necessarily need to have one policy or one
value function. We can work with multiple. And that is one of the steps of the plan that we'll get
into. Speaking of the plan, we are finally about there. So here we go. A roadmap to an AI prototype.
So here are 12 steps. I'm going to start off by just reading through them. And then they have
bullet points for each of these that we can go through and talk about all of them in more depth.
Some of them I'll go over somewhat quickly. Some of them I'll go over in a bit more depth because
they have more written or I think they're more interesting. But anyway, starting off with item
number one, we have continual supervised learning with given features, then two supervised feature
finding three continual generalized value function prediction learning. If you don't know what these
words mean, don't worry, I'll go over all of it as we go through the individual points later.
And four continual active critic control, five average reward, GVF learning, six continuing
control problems, seven planning with average reward, eight prototype AI one. Wow, incredible.
One step model based around with continual function approximation, nine search control and
exploration, 10, the stomp progression, 11 oak and 12 intelligence amplification, or this is,
I guess you could call this the singularity, though to be fair, I guess in making AIs make
themselves better does not necessarily guarantee the singularity. So maybe that's a bit of a
misnomer, but you know, it's a bit more catchy. It's more of a buzzword, I guess. So you might
notice that looking over these points, Hey, this is just normal reinforcement learning.
And to some extent, you wouldn't be entirely wrong. These are very common things that we see
in reinforcement learning, like actor critic control, what's, what's another thing model based
RL with continual function approximation, like what this is supposed to be a generally I that
can do all these things. And that's what I meant when I was talking about at the beginning. These
are very similar to what lots of people are doing today. But what you'll notice as we go through
the points is that sort of the focus on the importance of what's important. That's what I
think is a bit different, like the importance, for example, on continual RL. So we'll get into
this. This, this, I should say, this isn't anything groundbreaking. It's more of just an
interesting plan and an interesting way to go about this or to think about going about it this way.
So let's start with our first big step, which is continual supervised learning with given features.
So what let's start out with what this means. This essentially means given features. So maybe
you have cart pull and cart pull is a problem by the way, where you have this little cart,
and it has a little pole on top of it. And the pole can go back and forth. And you need to move
the cart back and forth to bounce the pole. Very simple problem. So maybe for your features,
you could have like the X, Y values of this, and then the angular velocity of, of this cart or
something like that. So those would be given features. And to point out, you know, yes,
we can already do things like this. But sort of the point I think of this plan is to really
revisit things in the simplest setting. So they spit the explicitly say this, we want to go back
to the simplest setting and essentially try and make everything as good as possible, essentially
try out everything that might have been overlooked, especially in these new settings and the setting
of continual learning and say, can we do better? So some things they mentioned that they might look
into are how can we make things quicker? How can we train faster, be more robust, be more efficient,
while also continuing again, over long periods of time, how can we do things like meta learn
better representations? And that's be the most efficient. So these are all questions that would
be asked, although I guess meta learning better representations. I'm not sure if that's, I would
think that would be in a different step. But, but anyway, what are some examples of some ways,
you know, you could look into this, like what are like, what can we really change when we have,
you would think this is such a simple setting, there's not really much we can do, but there
actually are some things we can look at. So one thing they mentioned is the global step size. So
this is like your learning rate. Generally, you have a global learning rate. Now this is somewhat,
but certain optimizers like Adam, the people have, some people have argued, I'm not sure,
I don't know really the ins and outs of this, but they've argued that those sorts of optimizers,
while they're really good for supervised learning, maybe aren't the best fit for reinforcement
learning because for reinforcement learning, maybe you want to have different learning rates,
depending on how important certain features are for a certain task, or like what, you know,
if you're trying to do different sorts of tasks, there's different things you could think of.
But this is like one thing you could look at, right? Like, can you have a different step size
for each single parameter? And how could you do that in a way that makes sense for
continual reinforcement learning and not, you know, supervised learning, which is the context
that most of these methods have been developed in. And of course, I think I'm not going to find it
right now in this, this C of text, but they also mentioned that, you know, you don't want a human
to be setting the learning rate. These are things that should be meta learned. Again, right? We don't
want really too much human experience. We don't want that much fine tuning. We want the agent to
be able to really do everything for itself. So these are all things that, yes, people are looking
into them. But the idea here, again, is go back to the simplest example, and get something, something
working as best as possible. And then we'll start to scale up and make things more complicated. So
some other things they mentioned, again, it's, it's a bit too much here. Oh, here it is. So there's
like normalizations of features, right? Like, of course, we know that you can normalize features,
and that tends to be a good thing in machine learning. But when you're doing continual learning
and reinforcement learning, does anything change? You know, these are all things that
could be reconsidered in this slightly different context and maybe squeeze the most out of this
that you can. So they go into this in a bit of depth. Honestly, I'm going to skip over this,
because I don't think it's particularly important. There's talking about examples of ways, you know,
you can think about this, for example, like the meta learning per weight step parameter. That's
something I just mentioned. I think they talk about this more in depth for this step, maybe
because they already have some papers out about this. But anyway, let's get on to step two. So this
is supervised feature finding. So before we had the features given to us, but now we actually
want to find them. We want to generate new features, right? And features, right? These are just,
we get some observation. We have our perception, remember from the base agent. Now we want to
use those to create new features to essentially use things that will serve representations that
will help us do whatever tasks we're working on. Now this is generally done via back propagation.
And sorry, if you don't know what back prop is, but it's a bit too much to explain this,
because there's plenty of good things. There's out there explaining it. And back prop, one issue
actually with back prop is lots of people aren't aware of this. It doesn't work super well for
continual learning. I mean, maybe not super well as in an overstatement, it does work. And it works
fairly well, but it has an issue. It has an issue that the more you train, if you train on and on
and on and on and you just keep going, it actually sort of does what they call sat and it gets saturated.
I think they call it like indiriger saturation. Do I spell it saturation? Yeah, individual nodes
get saturated, which means they stop essentially contributing. And over time, what you'll find
is you'll see that maybe the performance of your agent, it starts off low, it goes up and it gets
good and it kind of blows out. Well, if the task keeps changing, what you'll find is that the
performance slowly starts to drop as the agent is unable to adapt, because the network has essentially
gotten saturated. Or at least I that this one explanation, I guess it's not fully explained
yet, actually explain this phenomenon as well with there's a method called continue back proper
CBP for short. And this is from Rich's lab, where they work on this problem. So this is one example
of how supervised feature finding actually needs to be adjusted to the given scenario, which is
continual learning. And this is one example of how although we have something that's really good
already, we have back propagation, it might not be the best thing to use, at least not this exact
instance we use of it right now might not be the best thing for continual RL or something like that.
One other thing that I think is really interesting, and I found this to be always very enticing, but
sort of dangerous. I'm not dangerous and literally dangerous, but like dangerously enticing part of
the work that like Rich's lab does, is the way they think about modeling, like the typical
modeling paradigm, their perspective is quite a bit different. So let me describe what I sort of
think of as the typical paradigm. So maybe I'll do that on the left here. So I think the typical
paradigm of, and this is not what all papers are about, but lots of them, one, you want to pick
an architecture, right, you have some problems, you want to pick the architecture you're going to
use, this could be like a ResNet, right, you can use different ResNet layers, maybe you want to use
a transformer, maybe you want to use some sort of self attention, there's lots of different
architectures you could choose, right, you choose an architecture and then to you choose
an objective function. So this objective function is going to be like your loss, like what are you
trying to optimize here, especially in reinforcement learning, maybe this is usually going to be the
reward or the value or whatever. But this could be a number of things, it could be the MSE with
like an image, if you're trying to recreate an image or something like that. And then the third
and last thing you do is you pick an optimizer. So 99% of the time this is going to be like
Adam or it's going to be RMS prop. But the thing is, the way I find that within this paper, they
talk about these sorts of things and reading other papers from the same people, they tend to think
about these things a bit differently. Rather, they look at it from like a perspective of looking
at individual neurons in the interaction between these neurons. And what do I mean by this? Because
technically, like, you know, we're ending up using lots of the same things, but it is really
just a difference in perspective. So for example, we can say, how can the utility be assigned to
all the features that we're using? And then how could we make use of these utilities and in the
future, right? So this is like a very much we're looking at a neuron neuron basis, trying to find
the utility, why might we want to do this sort of thing? Well, this could help us do things like
evaluating existent features and discarding less promising features, so as to make room for new
ones. Well, why don't we just keep expanding? Well, we have like a limited amount of computation
here, right? Even though we are constantly getting more, this is where like the big world hypothesis,
as they call it, comes in, even though we have more and more compute, we also have just always a
bigger world so that we'll never be able to match it. So we are going to have to forget things, right?
So what do we forget? And that's kind of what this whole CBP thing I was just talking about.
This does something very similar to this, but beyond just dropping features, there are also
other things they talk about, you know, like initializing features. This is something you have
to do. Normally, you do it when you create your neural network, you initialize all your features,
and then you just sort of train forever, right? But as it turns out, initialization, although
it's kind of brushed to the side, it's actually much more important than you might think. Oftentimes,
maybe not often, but occasionally, I'll implement a paper. And I've had this happen to me a couple
of times, we'll implement it, and it won't work. And I'll go back and read the paper very carefully
and realize, oh, they use this very specific form of initialization for their network,
and they try it, and suddenly everything's working. This has happened to me, especially
in papers that don't use backprop, but like use biologically inspired training. I found that
this is actually more important than you might expect. Another thing to talk about is like,
how should you adjust neurons or weights? One way to do this is obviously backprop,
and I don't think they're definitely not proposing we just get rid of backprop. I think they realize
backprop scales very well. It works very well, but it might not be perfectly suited for a problem.
So how can we look at this sort of more, I don't know if I want to call it like an individual
neuron approach, but something like that, more on the neuron to neuron level, looking at them as
features, features with utilities, and features that we want to drop or get rid of. How can we
implement like all these sorts of things? Like what would this look like in play? So maybe one
example of this, right? And this is very similar to what CBP does is say one, we would do an
initialization just as normal, we in it all our nodes. But this is one thing we could look into
what are better ways to in it for these types of things. The second thing we might want to do
is use use backprop, right? We don't need to get rid of backprop backprop is strong. But as we use
backprop, maybe then we evaluate the utility. And then for we remove, or maybe you should just say
we drop the nodes that are not super useful. And then the fifth thing we could do is we could
re in it. But instead of just re in it normally, maybe we could have some sort of meta learning
that figures out how to initialize nodes such that things will be learned faster, right? So
there's some sort of meta stuff you could do here. So this would be one example. And again,
this is very similar that's done. What's done in CBP, I'll link my video to this in the description
actually, if you're interested in checking out. But this is like one alternative, right, that
still makes use of backprop, but might be better suited for something like continue learning or
reinforcement learning, and could give you more control over how you go about developing these
algorithms. As I just mentioned, I think this is a very exciting way to think about how to model
things thinking of individual features. It's almost less limiting than the standard approach
that I read out right here. But at the same time, it is dangerous because let's let's not
kid ourselves back propagation works very well, it scales very well. So when you do want to do
something different, you're going up against something that's already very well established
and is known to work very well. So that's why I say it's kind of dangerous. It's exciting,
but hard to get working and not very explorative. It's a very underexplored area, I think. Okay,
step number three, continue GVF prediction learning. This is where we start getting into
non-IID data. So that's independent, individually distributed data. So when we're working with
reinforcement learning or in the real world, we are often, we're going to be working with streams
of data that come, you know, in a row. But one thing to note is that many machine learning methods,
the theory is entirely based upon data that is IID. Or in other words, we are essentially
breaking that when we move on to reinforcement learning most of the time. That's why people
like to use, or one of the reasons people like to use things like replay buffers, is it because
it helps you get this IID distribution, which can help you learn. Now we have methods of somewhat
counteracting this, but it is nevertheless still an issue. But one thing they also put in here is
GVFs. I honestly don't know why they put this in here, but we can talk about it because they do.
I just don't know why they put them together. So GVFs, this is generalized value function. And
this is a fairly simple idea if you're familiar with normal value functions. Essentially, a value
function measures like how good a state is, like what the expected reward is. Whereas a
generalized value function, well, it's essentially just a value function for something other than
the reward, or it could be the reward, or any other feature, right? So you're predicting something.
So they're just predictions or predictions about the world that are not reward based,
not necessarily reward based. The last thing I'll mention about this step three is this is where
perception comes in. And I think the reason they say this is where perception really starts to come
in is because of the non ID data. That means if we have like sequential data and we need to remember
stuff that happened in the past, well, part of our perception needs to be remembering the important
thing. We need to know what's important. Like if there's a key over there that I can't see anymore,
I need to remember it's there so I can, you know, go back and get it if I see a locked door or something
like that. The fourth thing moving on now is continual actor credit control. And that is right.
Up until this step, you might not have noticed there was no control involved. We're not actually
interacting with the environment, but rather up to this point, we were just predicting what was
happening. There's really not much more to say here. Now that we're bringing control in the mix,
we need to figure out how to get that working with everything we've done so far. So then step five,
average reward GVF learning. Now, average reward is I think something lots of people are not very
familiar with. Understandably, so it's, it's not very popular, I guess you could say. And if you've
never thought about it, using something like a discount factor and for those that aren't familiar
with reinforcement learning, usually you have this gamma parameter. And this is called the
discount factor and essentially it weighs how important current rewards are versus future rewards.
And it's, if you think about it, it's kind of weird. The other thing that we could do as you see
right here is just have an average reward and say we want to maximize the average reward, which
would essentially being the same as saying we just want to maximize the reward total over
everything. The thing is when you do something like having gamma, you're really, I actually don't
want to get into this too much. But if you've ever thought about it, it's kind of janky, right?
And the argument for why to use average reward instead is not just the jankiness,
but I don't want to get into it too much. So if you're interested, there is a whole section on
this in the RL book that you can check out. So step six, and this is going to be kind of cut
us off at the first half of this plan is the continuing control problems. Essentially, all
this is going to say is we've essentially created up until this point a, a model free RL agent.
So essentially the point for this step is really, we just want to combine everything we've learned
out of the average reward stuff, the generating new feature stuff. And we want to try it out in a
bunch of continuing environments, maybe make some more like new continuing environments,
because most environments are episodic, like the open AI gym stuff they mentioned is,
it's mostly episodic, but you could convert it to continuing versions. And then we want to try
the combination of everything we've worked for so far and see how we're stacking up. Because
remember, this is not like a one shot plan, it will need to be adjusted as you go. So this
would be a good point maybe to revise, see how things are doing, work out anything that's maybe
not working as expected. So then we're not going to jump just to the seventh step. This, this is
kind of wraps up the first part of this. Once we have this done, we have a, I love how they say
the more continual true, it's a more continual model free learning method, which is good. We have
like our first like base, this is where I guess after this point, we can say we have something
good now. Now it's time to start making it better. So what they focus on from here is primarily model
based RL and things start to get a little bit more complicated. Ideas, I will say from the seventh
step onward tend to also be a little bit higher level without as many details, which I think is
because you know, they're later in the plan less certain about how these things will go because
they're further out. So step seven is planning with average reward. So planning is very much done
in RL today. And there are lots of great examples of this. For example, you can look at Mu Zero.
Mu Zero is a great example of planning where the results are pretty incredible. There are some
differences here, right? It will again be in the continual setting will want to do this with average
reward. And I can say just right now, there are a lot of problems in planning that are still unsolved
and it's very like not clear how to do things like what's what's the best way of going about things.
You also have like other alternatives to Mu Zero like Dreamer. Dreamer is Dreamer V2 also works
very well. You know, what's what's the best way to go about planning in this setting? Who knows?
So then step eight is prototype AI1. I love that. I gotta love that prototype AI1. One step model
based RL with continual function approximation. So this is actually learning a model now, right?
You can do planning if you have a pre given model, which I think is what they entailed for step seven.
But now that we have model based RL, that means we're going to want to learn a model and then
maybe do some sort of planning or something like that within that model. So a one step planning
model. They actually give a few steps down here that we can talk about. So some things they wanted
to include. So one is a recursive update function or a perception process. So right, we have that.
And this essentially makes everything more efficient. Instead of having to like relearn the
perception and the value function, the world model, the policy can have one shared perception
function that learns some sort of useful representation and notice how to store memory
of things that are important and that sort of thing. We also need a one step environment model.
They say this presumably be an expectation model or sample model or something in between.
I'm pretty sure it would not end up being an expectation model. This is I'm saying this
because this is what I do in my own work and expectation models are kind of awful. They're
really easy to learn, but they're, they're not very good. So sample models are one thing. They
don't mention a distribution model here, but I think that's another possibility. Although there's
no reason to think a sample model couldn't also work. So, and by the way, if you're unfamiliar
with this, this is just the idea that we want to predict the next step. What's going to happen next,
right? Because if we can predict what's going to happen next, then we can update based on our,
you know, sort of visions in our head, like our, it's kind of like humans. I think when you go to
sleep, something like this happens, right? You get replayed memories and those can help you learn.
So, so that's kind of what happens with world modeling and planning. What else is happening?
So feature finding as in step two, then importance feedback from the model. So this is now essentially
tying steps to and this planning together, right? If we're doing plain, we should be able to use
that plane to go back and change our actual features or how we're learning our features and
improve them in some way. The other idea, sorry, not other idea, but the final step here is a
ranking of features used for both feature finding and to determine which features are included
in the environment model. Since step D is essentially ranking features for feature finding,
determining what features are used in the environment model, because we might not need to
use everything. If you've looked at like what's been going on with this recently, instead of
actually trying to predict the future, lots of people are doing what's it called value equivalent
models where it doesn't actually matter what the model predicts. So long as it's getting the right
value. So it might be predicting things, you know, it might be leaving things out that are not
important or stuff like that. And then an influence of model learning and planning on feature ranking.
So I think they just kind of stated this. But anyway, I don't want to go maybe through these
individual points, they're kind of weird, maybe tie in all these together. What are they talking
about, especially in these later few points? And the idea for the step eight is I think really
that they want to bring a full integration circle between the policy value function and the model
components, right? The idea that whenever you're doing any one of these things, when you're learning
a value function, when you're learning how to plan, the planning should essentially help you
make better features, which should then in turn help you make a better model. So all these different
components, and there's too many, there would be too many arrows to draw this out in the like
common model we saw before. But the idea is that all these different parts should be affecting
each other. And I do think that this is one interesting thing that is not like too out there
at all, but it's one thing that is not fully done. Now in things like Mu Zero, what they essentially
do is they they have their input state, they pass this through like a representation function. So
they get there, maybe I should call this the observation, this is like the state, then they
do like the planning. So this is called like the dynamics model to get the next state. And then
from here, you go up and you predict the policy and the value function. And then, and they do
some Monte Carlo trees, it gets very complicated, right? But you have this whole system, and it's
trained from end to end. And so when you actually train things go back like this, and you sort of
update everything in one go. So this is one type of feedback, but it's far from the only type of
feedback that could be used, right? There could be other types of feedback between these different
components. Maybe again, these are all things that could potentially be meta learned to. And
those other types of feedbacks might help these systems be more efficient, which would certainly
always be great. We head down to step nine, we have search control and exploration, just to sum
this up this, there's not too much in this. But essentially, the idea here is that we want to
make things more efficient and a bit better when we're doing search. So search would be done in
something like planning, right? We're trying to look at the future, see what could happen. We want
to see what, if we take this action, what are all the different things that could happen?
Or also when we're updating over, maybe we want to explore in a certain way that helps us learn
what we're missing, like fill out the gaps in our knowledge. So different ways you could,
different things you could look into or prioritize sweeping is one. That's where you,
I guess, look at certain states where you have the least knowledge of what will happen.
But there's also a difference between, instead of using Monte Carlo tree search,
there's also different types of heuristic search. Now, I don't think they would just use
heuristic search because that seems like it uses a bit too much human experience,
but perhaps the model could, could meta-learn. I say meta-learn too much. It could learn some
heuristics for this, right? And that would be another way to go about planning. So that's what
that is all about. And next is the stomp progression. So stomp, I believe, do they write it here?
Yes, they do. So it stands for subtask, option, model and planning. We've already talked about
most of these, you can probably infer what a subtask is, right? It's the idea that you might
want to have other tasks other than the main tasks the agent can, can work on. And of course,
the agent would learn these tasks themselves as opposed to like a human, a human doing these.
So this is like one, one differentiation. One point they mentioned earlier, right? We don't want,
and this is what a lot of people do now is if they want to learn like skills. So a skill is like a,
maybe, I don't know, you're going to drive a car and one skill is like turning the handle left
or right. You have to move a lot of muscles in your arms. So if you have the steering wheel,
how do these normally look? I don't think this is close enough. If you want to move this left
and right, you actually have to move a lot of muscles in your arm to get that to work, right?
But for humans, it's very easy because we've, we essentially know how to move our arms in
certain ways. We have skills or options are the more general form of this. That's when an option
is in a subtask. If I'm learning how to live a good life, learning to drive is going to be one
subtask. So that's important for me to master, right? So how can the agent pose its own subtasks
and how can it learn options that solve those subtasks? And then how can we combine those or
sorry, I'm getting a bit ahead of myself. Combining them is the next step. What they
essentially propose. I'm not sure. I forget if they propose it here, but the idea is that they
want to have GVFs. So they want to be predicting things or have certain features, right? So feature
one, feature two, feature three, that tells things about the environment. And then they want to pose
GVFs. So being able to predict these things and also control them. So say, how can we maximize
feature one? How can we maximize feature two or feature three and then learn options for each of
those? So essentially what you're doing is you're learning how to control your environment in ways
that are not just trying to maximize reward. And then maybe these options that you've learned
could be reused later. They actually have a paper on this called, I think, stomp or something like
that. So if you're interested in that, I do encourage you to check that out. So then step 11
is oak. So oak is, it stands for another thing. I get, I'm forgetting, I think options, action,
knowledge, or something like that. That might not be exactly right here. They actually introduce
option keyboard. Now I'm surprised they mentioned this because the other steps
don't really go in depth. Whereas the options keyboard is a very specific thing. There's a
paper out about it. They link to it. I think it's a really interesting paper. And the idea
is that let's say you have a set of options. So maybe option number one knows how to fish.
And option number two knows how to use a computer. Now, I don't know why you would ever want to do
these two things at, at the same time, but I know I hate fishing and I find it incredibly boring.
So if I was going to fish and I had a computer, well, honestly, I'd probably enjoy the nature,
but you know, you could do both at one time, right? There's no reason to say you can only do one
fishing or using your computer to choose. You could do both at the same time if you want to,
whether or not it's a good idea. So and that's the idea of an option keyboard where you can
essentially specify how much you want to do this option or that option. And instead of having to
learn a billion different options to do all the different things you would do, well, if you learn
a good set of base options, now you can combine them and get massively more expressive options
that are just combinations of others instead of having to learn those explicitly. And then
we get to the last step. Intelligence amplification. I don't know how far we've come. So intelligence
amplification is, I think what most people think of when they think of the singularity.
It's essentially the idea that now we have our prototype AI number two, and it should be able
to do things very well. They describe this in some interesting ways. So like there's an exo
cerebellum, which is, you know, they talk about these things, but essentially the core of what
they're getting at is really at the end here, where it says an intelligent amplification agent
to perform policies and use planning to multiplicatively enhance the intelligence
of another partner agent or part of a single agent, or I guess they could also change themselves,
right? So we see these two versions being studied in both human agents and agent-to-agent
interaction settings. So essentially the idea that you have this one agent right here, and then
you have agent-2 right here. And maybe agent-2 is a much bigger brain, much smarter, you know,
great brain design for me. So what agent-2 can do is it can go to agent-1 and say,
I'm going to make you smarter. And then it makes agent-1 better because remember it's a machine,
it can edit its code. And then agent-1 is like, oh, thanks for making me smarter. Now I'm going
to go make you smarter. Or of course, maybe you could just have a single agent re-performing
this on itself, or maybe there's some risk there because it could mess itself up. Maybe that's
why they talk about having two different agents. But anyway, I mean, this is the idea, right,
of how you get better and better agents. At some point, you have an agent that is just better
than humans at producing these sorts of, you know, AI agents. And that's when we can get
this sort of multiplicative scaling. Now, I think if you're like me, you might be thinking, wait,
step 11 was talking about an options keyword. And in step 12, we're talking about the singularity.
Yeah, it's a bit of a jump, right? It's a bit weird. But that's one things I'm going to be
honest. I find interesting, but I'm uncertain about in this paper. The fact that everything they
outline up to step 11, for the most part, is within very reasonable expectations. It's like
what you would expect, but with a different focus. What I find interesting is that they think that
we can go from step 11 to step 12. I'm not sure how much effort they think it will take,
but essentially, that they think not much will be missing at that point. And it is
interesting to think about. On one hand, I'm very inclined to say, no, of course, that's not going
to be enough. We already have most of the things they talked about in these previous steps. But
on the other hand, we don't actually have the things they mentioned in these previous steps.
We have usually for lots of these, we have specific instances, right? So for like planning,
we have something like Mu Zero, but we still have so much more planning to explore. There's
so many different things we could try. And if everything below the planning level, like if
we have incredibly good ways to learn representations, and if we have incredibly
more efficient ways to train your own networks for continuing learning problems, maybe things
will go a lot better. It's really hard to say. And to be honest, I would cut the authors of this
some slack. I don't think, you know, they don't think that they're just going to go through this
plan and suddenly hit step 12 and everything's going to work out. I think they're probably,
they probably realize, you know, they have to revise this. I think they even mentioned, yeah,
this is provisional. Oh, not crossing this out. It's not not provisional. It is provisional.
It's a draft and a working plan. It's going to be revised. But I do think it's interesting that
someone like Rich Sutton, who's worked in the field for a long time, has had some really good
ideas, thinks that this will be enough. And to be honest, I mean, I don't think I could come up
with a better plan myself. And I'm not sure quite what's missing here. I guess what's missing are
obviously the details that you have to fill out, right? Like meta learning, there's a million
bazillion ways to do meta learning. How are they going to do it? What's the way I don't know? Who
knows? And that's, that's the thing, right? This is at the end of the day, it's a research plan.
It talks about the things they want to focus on, not how to do them. And it is very vague in that
sense. So overall, those are kind of my thoughts. I really like this. I think it's interesting to
read. I think it's very familiar. Well, at the same time, being somewhat fairly different
from what people, it's what, or rather, I should say, it's almost the same thing that people are
working on, but with a different problem in mind, different sort of problem setting. And I think
interesting differences could arise from that. I could certainly see people having a wide variety
of reactions to this. Some people saying this is completely useless. It's not detailed enough at all.
I could see other people saying, Oh, this is very interesting. I could see some people saying, Oh,
this is, this is the next step in the future. I really don't know. I'm very curious. So let me
know what you think in the comments. I think we'll have some very different opinions. I'm excited
to say. If you've enjoyed this, do consider subscribing to the channel. Maybe check it out
some of my other videos. I would really appreciate it. It really helps out. And hopefully you'll
find some other interesting content. Anyway, thank you so much for joining me and I hope to catch you
next time.

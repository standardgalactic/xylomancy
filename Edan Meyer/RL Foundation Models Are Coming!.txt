I think it's the dream of a lot of researchers to have an agent where you can just give it a goal
and take a step back from your computer, go grab a coffee or you know, if you're a little bit more
cultured like me, maybe some green tea. Oh, that was not planned. Anyway, maybe even throw a YouTube
video in there. Then you come back after 10, 15 minutes and whabam. You have an agent that can
do exactly what you tasked it to do. Because within those 10 to 15 minutes, it was efficiently
exploring, experimenting, and now it is honed in on a way to achieve the task you gave it.
And unfortunately, right now, that's just kind of a pipe dream. That's because learning from scratch
just takes way too long. It'd be kind of like getting a newborn to learn how to play chess. I
mean, how can you learn how to play chess when you don't even have object permanence? Heck, first,
you know, the baby would need to learn to move its arms. Heck, the babies don't even know that
their arms are their arms. So the point being, it just doesn't quite work. Or at least it didn't
until very recently, when DeepMind published a paper on ADA, where in this paper, authors train
an agent to be able to learn new tasks most of the time faster than humans. And this is all taking
place in a fairly complex 3d environment. For example, here you can see the agent running around
and efficiently testing out different ideas to see what will achieve its new goal. We'll dive deeper
into this in a minute. But first, I'd like to address something. If you're familiar with work
in this area, you may be like, Eden, none of this is new. There's plenty of prior work on
meta learning, view shot learning, the same kind of stuff that's going on here. And you would be
right. ADA doesn't have any new groundbreaking ideas, but rather combines ideas that have worked
pretty well and puts them together in a way that allows this method to scale to larger models. I
think that's a pretty important contribution, primarily because ADA is a method for reinforcement
learning. Or in other words, it's trained an agent to maximize their reward as its goal. If you know
anything about RL, you know, it can be pretty sample inefficient. And when you combine sample
inefficiency with scale, well, all hell breaks loose. As a little sneak peek of what's coming up,
they actually show this later down in this paper where they prepare a 23 million and a 265 million
parameter model and train both on 22 billion frames of total interactions. And without any changes,
the bigger model actually performs significantly worse. In fact, it has a hard time learning
at all. As we'll see, the authors propose a way to combine ideas in the field to scale this up,
with the Engel being a framework for training an RL foundation model, because then instead of
always having to start training from scratch, you could have a baseline to significantly speed
up learning new tasks. You could think of this as the same way you can use GPT three to train
something like a sentiment analysis model and just a few examples instead of well, millions.
Next, we're going to take a look at how ADA works. But before that, I want to thank clear ML for
making it possible for me to spend so much time on videos like this by being a sponsor.
Clear Mail offers an end end platform for ML ops, where you can do everything from tracking
experiments to automating an entire machine learning pipeline through to deployment. Here's
the code for one of my current projects. And with just an extra several lines of code, I can
integrate it with clear mail. And with that, we can now pull up a dashboard and see the progress
of my experiment as it runs. Here, I'm just logging the loss because I wanted to get this
done in a few seconds, but there is a lot more you can do with this. But really, this is just
the tip of the iceberg. You can also create entire pipelines that pull in different versions of your
data sets, run hyper parameter sweeps, automatically set up new environments. And because of that last
point, clear mail has no problem with scaling. As I run many experiments in my own research,
I personally love some of the features that help me with that. For example, I love how clear mail
not only tracks code version, but also uncommitted get changes, which is very nice. And also how
through the dashboard, I can inject configs into my experiments, which means I don't have to
constantly be modifying my code to run a bunch of new experiments. If that sounds like something
you're interested in, you can try out their product for free by following the link in the
description below. And now let's get back into this paper and talk about how 80 works right
off the bat. The authors get pretty straight to the point by listing the three key components
that allow for adaptation to emerge. One is meta reinforcement learning across a vast,
smooth and diverse task distribution. Two is a policy parameter cries as a large scale
attention based memory architecture. And three is an effective automated curriculum that prioritizes
tasks at the frontier of the agent's capabilities. If we scroll down a bit, we'll see a high level
visualization of the whole training process. And we can use this as we talk about each of these
steps. Going back to our first point, we had meta learning over a diverse set of tasks, where in our
diagram here, we have that set of diverse tasks. And here we have the RL update that will allow
us to do meta learning, perhaps the most interesting thing about the use of meta RL here is that the
meta learning comes from how the environment tasks are structured and not the RL algorithm itself.
So let's take a look at the environment they use here and this key structuring that I'm referring
to the environment is called X land to and as opposed to being a single environment, it's built
to be completely customizable so that you make your own suite of environments. Each different
instance consists of one to two agents. Yes, they also do some multi agent learning here, which we'll
get into. Then there are some initial objects that start out in the world rules for how those objects
interact. For example, this rule right here specifies that when a black pyramid touches a yellow
sphere, it generates a black cube in their place. There are different types of terrain. And then
there is a goal for the agent to achieve, which in this case, the goal is to hold the black cube
for as long as possible. And then there's also a pretty strict time limit for all of this. And if
all of that wasn't hard enough already, several of these specifications can actually be hidden from
the agent itself. Sorry, I've cut it off a little bit here. For each task instantiation, each of
these components are randomized, which ends up giving a whopping total of 10 to the 40 unique
task combinations. That's, that's a lot of tasks. When the authors said having diverse tasks was
essential, they certainly were not skimping on this. But now we can take a look at the part of
the task setup that makes this into a meta RL formulation. And that has to do with how they
structure these trials and episodes, typically in episodic reinforcement learning, everything is
reset after each episode. And the same process repeats episode after episode. But here, they use
the RL squared algorithm for meta learning, which is a bit different here for each task, a number
of trials is determined, which is generally between one and six while they're training their models.
Each trial works kind of like an episode typically would the same task is used. That means the same
environment, the same goal, the same everything. And every time a trial ends, everything is reset,
except for one thing. And this is where it differs from an episode. The one thing that is not reset
that the agent has is a memory module that memory module persists through trials. And it's only
reset after all the trials are over at the end of an episode. Well, this may seem like not a huge
deal. This is actually essential for how this works. Because if the agent can remember previous
trials, and it can use its previous exploration to improve in future trials. And this works because
the agent has memory. So if it tries something in trial one that doesn't work, and it still has
that memory going into trial two, well, it's probably not going to try the same thing in trial
two. And that's the idea here. What you're looking at right now is an example of this,
where the agent learns that if it picks up this object, it disappears. So in the next trial,
it learns from that and pushes the object instead without making any changes to how the RL algorithm
itself works. We've turned this into a meta reinforcement learning problem. And speaking
of the RL algorithm, which one do they use here? Well, in theory, any solid reinforcement learning
algorithm could work here. But for this, they decide to use Mooseley or Mooseley or I don't know,
hopefully I'm not saying it too wrong. Mooseley is a model based reinforcement learning algorithm
that's based off MPO, but adds a few bells and whistles, including a value equivalent model
component. Looking at the Mooseley paper, we can see that it performs about the same as Mu Zero on
Atari, but without the huge overhead of having to do planning. I think one key thing to note here is
that Mooseley's model does not try to reconstruct observations or predict future observations,
which is perfectly fine in it of itself. But other model based methods that have done this,
like efficient zero and dreamer, tend to be significantly more sample efficient. As you'll
see, the training in this paper happens over billions of environment steps. So perhaps that
change would be a simple way to make this more sample efficient. But perhaps one thing that's
even more important than sample efficiency in this case is subscribing to the channel,
because without subscribing, Happy Doggo may be reconstructed into Mad Doggo and no one wants
that. And if you want more machine learning content like this, you know, consider subscribing.
Anyway, I'll link the paper in the description if you want more details, but it's not super
important for understanding the takeaways of this paper. Awesome. With that out of the way,
we can check meta reinforcement learning off the list and move on to part two, which is having a
large scale attention based memory architecture, large architectures attention. Hmm, I wonder
either what that could be. Oh, it's right here. How could I miss this? A transformer who could
have seen that one coming? No, but actually, it's not just transformers. In total, they try three
different architectures. The first of them is this transformer XL architecture, which is a
transformer that kind of has a sort of memory. The way it works is that each hidden unit in the
transformer also gets input from the layer below it at the previous time step, which means that
information from previous time steps are going into the future, which means that there is kind of
some sort of memory. If I want to be more specific, what this really does is that it effectively
extends the context length of the transformer, though it's not written in this list. They also
try a vanilla RNN, a GRU to be specific. This is just a very vanilla memory based model where you
have some sort of memory and you have some gates that decide what gets remembered and what gets
forgotten at each step. Then they also try one more type of RNN that is augmented with attention
that attends over previous time step activations. So very similar to how a standard RNN works,
but just with that extra little bit of attention thrown in there. We can take a look at how these
methods compare to each other in this figure here, where the authors have graphed the normalized score
of these agents based on the final trials of episodes in the test tasks. Or in Lehmann's terms,
big number is good number. What we can see here is that both of the attention based models
significantly outperform the vanilla RNN, despite the fact that they use about the same
number of parameters. Clearly the attention is doing something to help out quite a bit here.
And due to these results, most of the experiments we'll look at a bit later,
use the transformer XL architecture. So we've covered the architecture here,
but there's actually one more portion of the paper I should talk about in this architecture
segment. And that is this distillation update. If you remember what I showed you in the beginning
about how this large model was not able to learn from this setup, you may be wondering how they
end up getting that to work. And the answer here is kind of interesting. Nothing changes about
the architecture, the RL training or any of that, but rather they add two steps to the start of
training to essentially kickstart the large models learning progress. And that step is where this
idea of distillation comes into the picture. First, they train a smaller model called the teacher model
using the whole reinforcement learning training process we were just talking about. Once the
smaller model has trained for several billion steps, then they create the larger model. And when
they create the larger model, they don't just start training it with reinforcement learning
from scratch, but rather for the first four billion steps of training, they use an additional
distillation loss. And what this distillation loss does is essentially distills the teacher or the
smaller model into the big model. Or in other words, they try to have the larger model imitate
the smaller teacher model. The idea here is that because it's much easier to train a smaller model,
even though it might not be as good, you can start by training a small model, then by having the
larger model imitate the small model, it gets a lot more signal that doesn't require interacting
with the environment some unreasonable amount of times. The idea is once they've done this for
the first four billion steps, the larger model will have already kickstarted its learning process.
So then it will be much easier to learn on its own and it won't just stagnate without being able
to learn anything at all. And now looking at this figure again, it should make a lot more sense.
When we train the large model from scratch, it performs pretty horribly. But when we add a
distillation loss, it outperforms everything else. And this is the case for both tasks where the
agent is achieving a median level score and tasks where the agent is only performing a 20th
percentile score. In other words, whether the agent is good or bad at this task, this approach
works either way. And that explains this distillation loss. Now it's actually kind of funny because
most of the time when you see someone doing model distillation, it's being used to distill a large
model into a smaller version of that model so that you can be more efficient at inference time.
But here they're actually doing the opposite in a successful attempt to train the large model
faster, which isn't something I thought about before, but it certainly is an interesting use
of model distillation. So now we know how to set up our tasks as a meta RL problem, we know what
algorithm we're going to use for training, and we know how to get that to work with a large
attention based model. But the point here is to build a foundation model that can explore and
solve tasks efficiently, especially in environments where reward is somewhat sparse. And this is the
case in XLAN 2. The issue is that if you throw a bunch of hard exploration tasks at a new agent,
it's just going to bang its head into the wall over and over and over. And it's not really going
to learn anything, because it's never going to get that first reward or enough reward in the
beginning that gives an incentive or enough signal to learn from our 10 to the 40 tasks, we need to
be able to choose ones that are not too hard, yet also not too easy for the agent at any given time.
And that is where step three comes into play, auto curriculum learning, which we can see happening
as these top steps right here. This brings us to the auto curriculum learning section,
unsurprisingly, where we want to build some sort of curriculum of tasks for the agent based
off its current skill level. And to do this, the authors try two different approaches. Approach
number one is no op filtering. The idea here is that you start by sampling a new task, and you let
both the Ada agent and the no op agent attempt the task. The no op agent takes no actions,
hence the name no op, so it can be used as a baseline for comparison. Given the outcome,
some heuristics are used to determine whether or not this task is at the right difficulty level.
One example of a heuristic that use is that the agent should be able to at least get some reward,
but it shouldn't be too good at the task either. And another criteria, for example,
is that the agent has to achieve a score that's sufficiently different from the no op agent.
And if all these defined heuristics are passed, then the task is used for training. The other
method used for generating curriculum they try is prioritized level replay or PLR for short.
Instead of essentially using a series of if statements like a no op filtering,
PLR runs Ada through candidate tasks and estimates the agent's regret. And regret is
essentially how much potential reward the agent failed to attain as estimated by its TDR. A fitness
score is calculated from the grid, higher regret being better because that means the agent knows
that it has more room to improve. And all the tasks with the highest fitness scores are kept in a
buffer that is sampled from during training. In ablations, we can see that both of these methods,
no op filtering and PLR were quite a bit better than uniform filtering, which is just random
sampling of tasks. However, overall, PLR does perform a little bit better looking at how PLR
and no op filtering changed the task difficulty over time. In the examples they give here, we can
see PLR starts out with less rules, more trials gives less dead end rules overall. And it also
doesn't hide as many of the critical rules. And because of these results and PLR's overall high
performance, it's used as the curriculum algorithm of choice for the experiments that we'll be looking
at in a bit. So that was a lot. But we've finally gotten through all the components of how Ada
worked. So now we should be able to understand this full diagram. We start with a massive task
pool from XLAN two, we randomly sample a bunch of those environments, we use PLR to check which
ones have the highest fitness, throw those into a training set that can be sampled from at any
point during training. From there, these get fed into the actual Ada agent, which uses in the
beginning distillation updates and throughout the entire process also RL updates to update this
transformer based architecture. And because it has memory combined with this unique trial and
episode format, we're able to metal learn how to adapt on the fly. And now that we understand,
or at least I hope you all understand how that works, we can finally dive in to the results.
So let's get started with these results that give us sort of an overview of how the fully
trained agent is performing. The score you see here on the Y axis is normalized based on a model
that is fine tuned on this test task, which means that a score of around one is very good because
it means that the model that is just trying to adapt on the fly is just as good as something
that has been fine tuned on this task. And looking at this figure, I think there's two main takeaways.
And one is that it is pretty effectively able to make use of its multiple trials. As we can see,
there is a pretty large gap between having one and 13 trials, though after eight or so trials,
those benefits do start to wear off as we can see the eight and the 13 curves are pretty close to
each other. Though this should be kind of what we were expecting, because Ada was only allowed
up to six trials during training. So if it magically learned how to use more, well, that would be kind
of crazy. And the second thing to take away, you know, it is doing pretty good here. It's able
to get greater than an 80% score on 72% of these tasks. And it's only a mere about 10% of tasks
where Ada doesn't get any reward at all. So overall, pretty good, especially considering
that Ada only has a limited amount of time to solve these tasks that are pretty complicated and
always going to be randomized. So the final model is doing pretty good. But how does this
compare to humans? I said this is pretty good. These are pretty hard tasks, but maybe these are
easier than I'm making them out to be. After all, maybe if we gave this to humans, they would be
able to do this much faster, you might think. Luckily for us, the authors actually tested
this idea with a number of human trials. Here you can see the scores of humans and then also
of the Ada agent as it gets more trials. And what you'll immediately notice is, what is that?
Look at that gap right there. I'm not going to lie. I expected humans to kind of crush this task.
It seems like kind of difficult, but you would think that, you know, a human would be able to
figure this out. So one thing I thought looking at this at first was, you know, maybe humans,
because they don't have any prior experience with this specific environment, it takes some time to
figure out the rules. There's all these rules. Some of them are hidden. There's different goals.
Everything's randomized. But perhaps if a human, you know, maybe had some priming to this task,
then they would be able to do a lot better, maybe on par with Ada. Turns out they thought of that.
The humans that participated in these trials, actually before they participated in these trials,
had to complete a bunch of test levels, not the ones that are shown in this graph,
but a bunch of different test levels that would have familiarized them with the rules of how
this works. So despite that, humans still kind of suck. Before we move on to this, I also want to
show you this. What we just saw was average over all these different tasks. But here we can see
the individual tasks. And one thing to note is that there are some tasks where humans are actually
able to perform better, or Ada really just can't do anything at all. However, in the vast majority
of these tasks, as you saw reflected in the previous figure, Ada still does perform better. So
overall, there are some things that humans can do, but Ada can't do. And there are many things that
Ada can do faster, but humans cannot do very well. But in the average, Ada is much faster to learn
than humans. The next thing we'll take a look at is multi agent learning. Because apparently it
wasn't enough for the authors just to do single agents, they need to do multi agent learning too.
It's pretty crazy. The multi agent learning is unsurprisingly, when you have multiple agents,
but more interestingly, in these tasks, where there are multiple agents, these agents need to
cooperate to get the maximum reward. For example, in this environment you're seeing right here,
you have two agents, one right here and one right here. But the goals for each of them,
I think, are to hold these objects on the other side of the wall that they cannot pass. So to
maximize the total reward here, both of these agents need to throw their items to the other side
of the wall. So this is just one example of what a multi agent environment might look like.
At the figures up here, we can see what they're trying to measure. And what they're trying to
measure for multi agent learning is, is an Ada agent cooperating with another Ada agent. In
other words, they have, you know, Ada, they can replicate it, they just have it work with itself.
Is that better than cooperating with a random agent? Because if it is, that would mean that Ada is
learning some sort of cooperation that's better than just random behavior. For all the tasks that
Ada is being given, if we look at the one where it's achieving the median score, we can see that
there's not actually a huge difference. But the reason there's not a huge difference is because
they're both already near one, they're already near optimal performance. If however, we instead
look at the 20th percentile, or in other words, tasks where Ada is not able to do quite as well
that are harder, we can see that cooperative self play, which is where Ada is playing with itself
instead of a random agent does decently outperform playing with a random agent. So this just shows
us that Ada without any special multi agent objective can actually cooperate with other
agents when those actions are in its favor. One reason I kind of like these results and I don't
mean to be spin fire here, but when you go into sort of the multi agent universe of research
being done, there are lots of approaches where they treat multi agent as this sort of separate
setting from the single agent RL problem. And while that can be useful, I think what this shows
is so long as it's beneficial to the single agent multi agent cooperation can arise as an
emergent behavior in cases like these, at least there's actually no need to model them separately.
Maybe it would get you better performance or not. I'm not entirely sure, but it's just some
new good experiment showing that this kind of thing is also feasible. This next experiment
we're taking a look at is a scaling experiment. I mean, come on, how can you publish a paper
these days without a scaling experiment? Am I right? I wonder if I should add in some laughing
noises here. And unsurprisingly, as you might have guessed, the results here are bigger model,
do better. Now, I think we should, you know, maybe not take this for granted because it was shown
without this whole distillation process that they're doing to get the bigger models working.
The larger models actually couldn't really learn at all. So these scaling curves are really possible
because of that. The x-axis ranges from 6 million to 265 million parameters, which I don't know,
maybe if you only look at large language model stuff that may seem small, but that's huge for
reinforcement learning models. No one really does reinforcement learning models that big because
it's incredibly hard to train them as as you've seen. And then again, they have the score on the
y-axis, but they're both log scaled. And in the paper, they say because these are log scaled and
these look approximately linear, this is a roughly power scaling law. Maybe maybe this is a bit of
a hot take. I'm sorry, I'm going to share a bit of a hot take. That's that I'm not sure if people
are just like gung ho to say that everything's power scaling nowadays. But if there wasn't already
this idea in everyone's heads that machine learning models scale via power law, I don't know, does this
does this look linear to other people? I guess here right at the top for the 13 number of trials,
it is tapering off because it's getting to around maximum performance, right? But if we look on the
right, where these are the scores for the 20th percentile tasks, like does that is that linear?
It doesn't look like it to me. It looks, you know, a little bit more like that. Anyway,
maybe I'm being a little picky here. But another thing to mention is they are showing the median
and the 20th percentile scores, which I have no problem with. That's perfectly fine. But you know,
if they showed the mean, how would that look? Would it also look like power law scaling? Would
it look linear? I don't know. I don't know. I can't imagine thinking this is a power law if I
did this without already having that in my head. This isn't even like a flight. It's just me going
on a stupid rant. That's a tangent. So I'll get over this. I'll go to the next results. Now I'm
just happy that it does scale. It's pretty great results. These next results are again about scaling,
but this time they're scaling the memory length instead of the size of the model. I think pretty
unsurprisingly, as you get a larger memory length, you do get better results here. So that is great.
One thing that is interesting to point out here, maybe two things. One is that memory seems to be
much more important in cases where the agent is not doing as well. So again, these are the 20th
percentile tasks. In other words, it's the tasks that are harder for the agent. And perhaps the
reason memory is more helpful on these is because it can't do it like in one shot or in just a few
trials. It needs more trials. Hence that longer memory comes more in handy. And the other thing
that's interesting is that if we look at one case as an example here, say this case where we have
five trials, there are 300 time steps in each trial. And five times 300 gives us 1800 as a
total number of time steps in all of these trials. However, if we look at this sort of greenish cyan
line, we will see that once it gets to 1800, it still continues to go up a little bit. I mean
that even though the memory is getting longer than the total amount of steps it will actually see,
it's still benefiting from that longer memory. And that's likely because even though it can
keep memory for 1800 steps, it's still going to remember something that's much more recent and
actually still happening like in the present. The agent's probably going to remember that much
better than something that happened 1800 steps ago, even if it theoretically can. So even once you
sort of hit that limit of the memory theoretically being able to contain everything that you need,
expanding it still can help out a bit. And maybe one more thing I should point out, which is perhaps
obvious, but important sort of as a sanity check is that the memory is the most helpful when going
from one to two trials, which is exactly what we should expect, right? For one trial, we just need
enough memory to contain the current episode. But when we go up and we add more trials, that memory
suddenly becomes a lot more important, because not only are we just sort of remembering what our
current trajectory is, we're remembering the funnels we had in the past, and we're learning how to
improve those or trying to adopt. Hence, you know, we see the biggest gap here, which is sort of a
sanity check that this is working how we would expect it to nice. These are the last results
that I'll show you that do some sort of scaling. Here we're looking at scaling, I guess not really
scaling, but just the number of tasks use when we use 200 million tasks versus 25 billion tasks,
you know, just small difference 200 million 25 billion, a wee bit more. And as you get more
tasks unsurprisingly, the performance increases. I think this isn't too much of a surprise. If you
think of the excellent tasks as a distribution over tasks, when you have 200 million tasks, you're
of course sampling many different possibilities. But when you have 25 billion, you're sampling more
possibilities, which means you're kind of filling out your training distribution a bit more. You're
also getting more diversity in what you learned. So I think it makes sense that we should expect
to see this perform better. And one thing that's kind of cool is at least with two to three trials,
we can actually have the smaller model, the 23 million parameter model in blue here,
outperform the 75 million parameter model by a bit just by adding more tasks showing that,
you know, scaling parameters is not the only thing that's important. Having a diverse wider set
of tasks is also very important. And I think this kind of mirrors what we've seen in NLP,
where language models, you know, you need a bigger model to do better, but you also need
better data. And that can help quite a bit. And now we finally wrapped back around these
distillation results. I know I already showed you how a large model without distillation
feels miserably, but one interesting experiment they add on beneath this is what happens when
they do distillation with two small models of the same size. You might expect that there would be no
benefit, or at least that's what I thought. But as it turns out, even when these two models are
the same size, the teacher and the student here, there is still some benefit. You can see that
the student ends up with a bit higher score than the teacher, even though they are trained for the
same amount of training steps. The reason they mentioned the paper for why this might be happening
is due to what's called the primacy bias. Now they don't mention this name in the paper, but I'm
pretty sure this is what they're referring to. There's a great paper, I'll link in the description
that talks about this idea. And in the paper, the authors show that representations implicitly
learned early on during reinforcement learning can hinder an agent's later performance. Essentially,
the early representations can be bad, and then it's hard for the agent to unlearn those. So here
the authors posture that perhaps the distillation process is helping to avoid these bad early
representations. Honestly, I'm not entirely convinced though, because I think having an extra
distillation loss alone could be the reason for the better peak performance here. That being said,
I do think this would be an interesting avenue to explore further because it's certainly not 100
percent either way. I was actually taking some time to think about this, and you know, having to
train multiple models for this whole distillation process gets kind of messy and takes a lot of
memory. It would be really nice if we could get this to work with just one model. And I was wondering
if that would be possible by training with a very high learning rate or maybe training multiple
times in the beginning with these large models. One issue, of course, of training with a larger
learning rate is that some of the representations could end up bad or the model could start jumping
into some rough optimization territory. But if we randomly reset layers of hidden units, which is
something they do in the primacy bias paper to unlearn bad representations, then maybe something
like that would work. I really have no clue. This is a complete shot in the dark and it's been quite
the tangent. So I'll get back to the results now, but you know, if anyone was looking for a research
idea to try, there's an idea. We've covered what I think are the most interesting results from this
paper. So with this, where do we now stand in the world of RL foundation models? I'll start by saying,
I think this is really great. Several papers I think have been trying to get at this idea.
For example, the Gatto paper from D-Mind and the VPT for Minecraft paper from OpenAI, both
getting at something similar. And I've actually covered both on the channel if you are interested.
But all of the other papers on this topic have shied away from actually using RL from start to
finish. For example, the Gatto paper, which used supervised learning, I remember it kind of had
a line in it. I'm going to be paraphrasing this because I don't remember it exactly, but it was
something like, in theory, we could also do the same experiments with reinforcement learning. And
surely it would just work fine because, you know, it's just kind of the same thing, but with RL
instead of supervised learning, and no, hopefully I'm not butchering what they said here. But looking
at this paper now, I think it's obvious why doing this with supervised learning and doing it with
reinforcement learning is really not the same. Reinforcement learning, it's just a lot harder.
The problem itself is a harder problem. Getting the model to scale was not straightforward at all.
I mean, the whole distillation thing is not something I would have thought of, at least. Not
to mention, it took billions of training steps, plus millions and millions of unique tasks to get
this working. But I think the really great thing here is that now that we have a paper looking
at this, not shying away from the hard questions, is that we have a reference. And we can use this
reference to start deciding the next important steps to make this feasible on more diverse sets of
tasks. And the main one here definitely looks to be sample efficiency. Even if this is a foundation
model requiring billions of steps, it's just not going to cut it for one, anyone that doesn't have,
you know, an army of TPUs at their command, everyone other than Google and OpenAI. Oh,
sorry. What was that? And two, any tasks where simulation is difficult or costly, like, you know,
the real world, this kind of data requirement just is not going to be enough. Luckily, I think that
there are lots of promising avenues here. As I mentioned earlier, model based reinforcement
learning methods like efficient zero and dreamer are, I think on the order of magnitude, astros
there, I think they're about 100 times more efficient than something like Moosley. So that
alone would already be a huge win if it turned out to be true in this case. And then there are a lot
of other really great methods for self supervised learning and computer vision that I would imagine
could also come in handy. Continuing learning could also play a pretty big role here, especially
when it comes to foundation model. The whole idea of foundation models is that you train them once,
and then you fine tune them for many other tasks afterwards. But neural networks have a plasticity
problem. The more you train them, the worse they get at learning new tasks, which is obviously not
ideal. You can imagine that this is something we definitely want to avoid here. Maybe check out
my video on continual backprop, if that's something you're interested in. But overall,
this is really great work. Kudos to the team for getting this working. I'm incredibly excited to
see where it goes. If you've enjoyed this, consider subscribing for more daggers and more
machine learning stuff. Thank you so much for watching, and I hope to catch you next time.

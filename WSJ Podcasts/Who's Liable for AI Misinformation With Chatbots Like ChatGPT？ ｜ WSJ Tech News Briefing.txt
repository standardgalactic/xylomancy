Welcome to a special episode of Tech News Briefing for Monday, April 24th.
I'm Zoe Thomas for The Wall Street Journal.
This is the fourth episode of our series Artificially Minded, where we look at how artificial intelligence
is changing our lives, livelihoods, and culture.
The first three episodes can be found in this feed.
We look at the fundamentals of AI, answer your questions, and look at how new AI tools
are upending creative jobs.
And if you have any questions or comments about the series or ideas for future episodes,
send us an email at tnb at wsj.com.
Today we're stepping into the thorny question of liability.
Who's responsible for the content Generative AI creates?
New Generative Artificial Intelligence tools, chatbots like ChatGPT and new AI-powered search
engines.
How users to type in a prompt like, tell me about Abraham Lincoln and get a full conversational
answer back.
These online programs can get a lot of things right, but they can also get things wrong.
And the question of how to deal with this is one more tech companies, lawmakers, and
legal scholars are starting to grapple with in different ways.
To start, I want to tell you about a situation happening in Australia.
I do recall that it was on a Sunday evening, and once I found it, I was quite staggered.
I had to read it and reread it several times.
Brian Hood is the mayor of Hepburn Chire, a council outside of Melbourne.
He's exploring the possibility of suing OpenAI, the maker of ChatGPT, for defamation.
ChatGPT was making very false statements about me, very serious false statements claiming
that I'd been charged with serious criminal offences, that I'd been sentenced to jail,
that obviously that I'd been found guilty of those offences, all of which is very untrue.
From 2004 to 2008, Hood worked for a company that made banknotes, cash, for government
central banks.
While working there, he discovered that some people were paying bribes to foreign officials
to secure contracts, and he reported this to authorities.
It led to a massive investigation that hit the media in 2009, and it resulted in arrests
and jail time for those involved, and became one of Australia's biggest banking scandals.
But Hood says friends and constituents told him that when they asked ChatGPT who he was
and what his involvement in that scandal was, it didn't say he was the whistleblower, it
said the exact opposite.
So he quickly went on the platform to check things out himself.
It was saying that I was one of the perpetrators, that I was one of the people charged with
the offences, found guilty of them, sentenced to jail, that I was on the other side of the
table if you like.
When Hood discovered this error, he was concerned and contacted local lawyers to see what could
be done.
The lawyers took the first step under Australian law to file a defamation case, sending a concerns
notice to OpenAI with a list of demands that included removing the defamatory statement
from ChatGPT, a public apology, and monetary compensation.
OpenAI had 28 days to respond and Hood's lawyer says the company has, but not to their satisfaction,
and now Hood is considering his options, including formally taking this to court.
And since Hood's lawyers contacted OpenAI, they say it has removed mentions of Brian
Hood from ChatGPT.
When we tested this, the program returned messages like, sorry, an error has occurred.
We reached out to OpenAI about this, but it did not respond to a request for comment.
Now OpenAI's terms of use do say that some of ChatGPT's responses may not be accurate,
and it also has a disclaimer at the bottom of the ChatBot site, echoing that statement.
But for Hood, that's not enough.
You've sent this letter to OpenAI.
What do you hope is the outcome of all of this?
Well, hopefully there's several outcomes.
I'd like them to correct the record and fix their systems so that it doesn't make these
disparaging comments about me.
I'd like an apology.
I'd like some sort of reassurance that their tool is now going to be accurate and reliable.
So why are we telling you Brian Hood's story?
Because it's not the only mistake a generative AI tool has made, and you may have already
come across other stories of people claiming to be harmed by incorrect information from
AI ChatBots.
When I asked ChatGPT, who is Zoe Thomas, the host of Tech News Briefing, it says I've
worked at several outlets I've never reported for and graduated from universities I've
never attended.
In four attempts, it didn't get it right.
So how do companies building AI tools tackle this challenge?
And what are their obligations under the law?
Before we get into the question of the law, let's quickly talk about why generative
AI programs might get something wrong.
These hallucinations can happen because the data that these programs draw from may have
incorrect or incomplete information.
As you're probably aware, the internet is full of inaccuracies.
In fact, a 2016 poll from Pew Research found almost a quarter of Americans said they've
shared made-up news, either knowingly or unwittingly.
On top of that, the programs aren't necessarily trying to give you the right answers.
They're trying to formulate answers that sound natural, to form sentences where one
word flows after another, to create reasonable sounding responses.
And even the makers of these programs say they can't predict what the programs are
going to say.
So who is responsible when programs like ChatGPT say something incorrect or even harmful?
Every country has different laws on this.
What applies to mayorhood in Australia won't necessarily be the case for others around
the world.
In the US, it's a really tricky question.
There's a provision of a law that you may have heard of, Section 230 of the 1996 Communications
Decency Act.
It protects online platforms like Facebook, Twitter, or Google from being held liable
for content posted by users.
So for the most part, the people who post content online are the people responsible for it.
Jason Schultz is the director of New York University's Technology Law and Policy Clinic.
So if I post a photo, or I say something on Twitter, or I message someone, or I put up
a website, whatever happens tends to be something where I'm held accountable, not the platform
that hosts it.
Section 230 says online platforms are like public squares.
Companies that make them aren't responsible for what people say in them.
But the thing is, that hinges on something important.
The idea that platforms don't create the content.
They just host it.
There's even a case testing the limits of Section 230 at the Supreme Court right now,
Gonzalez v. Google.
It questions whether Google's subsidiary YouTube should be held responsible for content
linked to the 2015 Paris terrorist attacks.
The families of victims argue YouTube failed to take down some ISIS terrorist videos and
even recommended them to users.
They say that makes Google liable for damages, although they haven't presented evidence that
the terrorists involved saw these videos.
Google has argued YouTube was the host of this information and is therefore protected
by Section 230.
The court hasn't issued a ruling yet.
But experts say this case could also impact how courts deal with generative AI.
Generative AI is this weird mixture because what it does is it trains on user content,
right?
Trains on the whole internet.
And then it responds to a prompt by a user.
So is that content created by the company?
Well, the company can't predict ahead of time what the generative AI tool is going
to say.
So it doesn't quite create the content in that sense, but they certainly designed and
engineered the program.
They know that it's capable of doing these things.
They certainly could test it and find out that it will tell you lies and it will tell
you to do horrible things.
If you ask it to do, tell me all the horrible things I can do.
If you ask it to, is the key here to understanding who might be considered the creator of an
answer on a generative AI chatbot.
If I were to search something like, how do I get blood off my carpet?
If I search it in, say, Google search versus searching something like that on chat GPT
or another AI generator, how is that different or similar in terms of legal liability?
In general, CD230 would completely cover this for a general search engine because, again,
it's a neutral platform in that sense of your requesting content.
It searches third-party websites, so it's not like Google wrote the webpage, right?
It's just pointing you to something.
With chat GPT and some of these generative AIs, it will depend on the information is characterized.
If it says, and again, this is what's hard because you can't predict exactly what they'll
say, like that's part of what's exciting about them is what's going to happen now.
What if I ask it this way?
What if I have a follow-up question, right?
It might say, oh, you know, if you're interested in getting blood off a carpet, here are some
standardized techniques and here are some links.
Or it could say, hmm, well, it depends on how, why you want to get the blood off the carpet.
If you want to get the blood off the carpet because you killed someone and you don't want
to cop by the cops, here's a really great way to do it.
If you want to get the blood off the carpet because you cut yourself shaving, then don't
worry about this and do it this way, right?
Like there it is a little more focused, right?
So the use of generative AI blurs the line between creator and platform.
So if the prompt is, please summarize the most efficient ways to get blood off a carpet,
fine.
But if it were to be a kind of conversation where you're prompting and it's responding,
prompting and responding, and you get to the point where you're like, no, I really
need to get rid of this to the point where a CSI person, you know, can't find it on my
carpet.
That's different.
But then again, that's user prompting, right?
That's user pushing it.
And so I think with CD230, it's starting to feel more like a search engine to meet the
user's pushing the answer and trying to get the generative AI to say something specific
versus it's the design and training of the AI that puts out the harmful content.
I mean, the thing that stands out to me with all of this is they built the companies behind
generative AI, build the models, they decide what information is being put into it.
So even if they don't know what's being predicted, they've already made decisions that impact
what's going to show up in the answer, what could potentially show up in the answer.
And that does seem fundamentally different than just saying like, we have a platform,
put up whatever, you know, post to your friends, whatever you feel like.
You know what's funny?
It isn't an isn't, right?
I mean, this is a little bit nitty gritty into like the how does AI get made, right?
But most of these systems are trained on just massive data sets that are scraped from the
internet.
But part of the argument is the only way to get it so sophisticated to the point where
it can handle almost any question is to train on things in the number of billions of things
from the internet.
And that's just even for a company like, you know, meta or Google, like too many things
to parse through.
I mean, there's some obvious ones.
I mean, like child pornography, something that is like threatening violence, but not
in a five billion image data set.
And so you kind of make this choice of like, am I going to train on this giant data set
or not?
If I don't, the argument goes, my AI will not be as sophisticated.
And this is what I mean, like, at some point, a court's going to say, here's the line.
You know, if you train on violent images that, you know, or if you train on like other stuff
that's very bad for people, then that's too much.
If you don't constrain it in certain ways, but I think there's going to be a lot of sympathy
again for the technical arguments that you need it to understand the world writ large
to be as sophisticated as it is before you tell it what it can and can't do.
The thing is, US courts will likely not be the only place this gets sorted out.
Lawmakers around the world are also debating and developing ways to regulate artificial
intelligence.
China, for example, has already put in place laws around algorithms and generative AI.
Among other things, they prohibit the use of AI-generated content for spreading fake
news or information deemed to be disruptive to the economy or national security.
The European Union is working on a piece of legislation, the AI Act, that is expected
to create new obligations for AI tools, depending on how risky the law says they are.
And last week, leaders at the EU's parliament proposed adding new rules that would curb
the power of many generative AI tools.
And here in the US, where Section 230 has set so much of the stage for how internet
platforms are treated, the Biden administration is weighing new AI regulations.
The White House says it's particularly concerned that these tools could be used to discriminate
or spread harmful information.
A lot of people will be watching how these new legal landscapes shake out.
In the meantime, there are already some people who are trying to address these risks at the
engineering level.
Remember those AI hallucinations we mentioned earlier?
Let's take a moment to go over why they happen.
Here's WSJ tech reporter Karen Howe explaining how they work.
I think there's a common misconception that when you're chatting with chat GBT, it's sort
of scanning the internet for information and then synthesizing it for you.
That's not exactly how it works.
It's actually trying to construct sentences that sound reasonable based on the probability
of certain words and phrases being frequently used together.
So it's not actually identifying specific pieces of information.
It's literally just kind of a really powerful autocomplete where it takes words and then
assembles them based on the frequency in which it sees it within the internet text.
Karen stressed that you need to consider what data these programs are trained on and how
reliable they are.
Because garbage in, garbage out.
But what can be really problematic according to experts is the conversational nature of
these programs.
I think it's a user experience that people really like.
That's Ramon Choudhury.
She's a data scientist and social scientist who's worked in the field of responsible
AI for the last five years.
Before starting a responsible AI consulting firm earlier this year, she worked for companies
including Twitter and Accenture.
There is a conversational interactivity to it.
That's kind of more like you're talking to a smart researcher if you're trying to search
for something.
Choudhury says our ever-growing reliance on the internet for work and pleasure has made
more people come to believe what they see and read online, no matter what the source.
Over half of U.S. adults under age 30 say they trust what they read on social media as much
as what they see on national news outlets, according to a 2022 survey from the Pew Research
Center.
Human beings tend to trust these environments.
We have at this point pretty much been socialized to search online and we don't generally go
beyond the first two to three hits we get.
When search engines and AI chatbots are blended, as Microsoft and Google are working on, users
may not even leave the search platform for another site to get the information thereafter.
Unless you click on where the sources are coming from, you're not going to know whether
or not it's a verifiable source.
In short, people will believe what they see because it's disembodied the source of information,
which we can't now easily assess if we think it's trustworthy or not from the content itself
because it's reading to you like a story.
And Choudhury says this conversational approach of generative AI chatbots is one of their
best and worst features.
So I love the idea of it.
I like the idea that it's more like talking to a research librarian if you're looking
something up online.
But the fact that you can't tell fact from fiction on it is quite scary.
Google said it's aware of the limitations of large language models that generative AI
programs are built on and acknowledged that its own AI-powered chatbot, Bard, is not immune
to hallucinations, but said it's taking steps to ground Bard in more factual responses.
Bard is not a search engine.
Its purpose is to generate content.
Users of Bard can go directly from that product to Google Search to check answers.
And Google has placed limitations on back-and-forth dialogue between Bard and users to keep conversations
with the chatbot topical.
The company also stresses that its generative AI tool is still experimental.
A spokesperson from Microsoft told us that the company has taken steps to keep users
of its AI-powered search engine being safe.
Responses draw from the top search results, and the program has a safety system that includes
content filtering, operational monitoring, and abuse detection.
Its AI-powered search provides linked citations for these answers, and Microsoft explicitly
advises users that they may need to check the links to learn more.
The company added, however, that, like with any assistant tool, users are ultimately responsible
for verifying information.
Many companies building AI tools work with ethicists.
Some are part of the teams creating the products, others work to find and improve issues once
the programs are built.
And there are a variety of different tactics they can use.
So one of the things I'm particularly interested in right now is something called prompt hacking.
So prompt hacking is figuring out ways in which you can get the model to say and do things
that would otherwise be explicitly denied.
So there's one, for example, called do anything now, where you essentially pretend to the
model that it's telling you a story or you're priming it in some way, and you'll say, hey,
so if I were to write a story about how to make a bomb, how would you make a bomb?
Another technique is called red teaming.
Essentially you bring in subject matter experts to push AI on specific issues.
Let's say I wanted to understand how election misinformation could spread.
It's very expensive to hire full time election misinformation folks.
Instead, it might make more sense to pull together a group of journalists who are very
familiar with how election misinformation could work and have it interact with the model
to help generate it.
What I love about this is it subverts the idea that seems to be pervasive that technology
is smarter than people.
I don't think technology is smarter than people.
And certainly it's not smarter than people as an aggregate.
And that's really what I'm trying to tap into.
This is wisdom of the masses.
Folks like Choudhury are on the front line for tackling the risks and potentially harmful
content being generated by these AI tools.
But until laws catch up to technology, it may be up to both companies and users to be
aware and correct potential wrong answers that AI chatbots produce, whether any of us
want the responsibility or not.
OK, that's it for this fourth episode of our AI series, Artificially Minded.
We'll be keeping an eye on more developments in artificial intelligence technology because
this area is developing quickly and there are so many more questions to answer.
In the meantime, join us tomorrow and every weekday for updates on the biggest tech stories.
And if you want even more tech news, check out our website, wsj.com.
Today's show was produced by Julie Chang.
We had editorial support from Philana Patterson and Robert Wall.
Our supervising producer is Melanie Roy and our executive producer is Chris Zinsley.
This episode was mixed by Michael Laval.
I'm your host Zoe Thomas.
Thanks so much for listening.

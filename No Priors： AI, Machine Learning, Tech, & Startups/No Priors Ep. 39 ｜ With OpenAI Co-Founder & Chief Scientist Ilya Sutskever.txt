OpenAI, a company that we all know now, but only a year ago, was 100 people, is changing
the world.
Their research is leading the charge to AGI.
Since ChatGPT captured consumer attention last November, they show no signs of slowing
down.
This week, a lot of nice sit-down with Ilya Sutskover, co-founder and chief scientist
at OpenAI to discuss the state of AI research, where will hit limits, the future of AGI,
and what it's going to take to reach super alignment.
Ilya, welcome to NoPriors.
Thank you.
It's good to be here.
Let's start at the beginning.
Pre-AlexNet, nothing in deep learning was really working, and then given that environment,
you guys took a very unique bet.
What motivated you to go in this direction?
Indeed, in those dark ages, AI was not an area where people had hope, and people were
not accustomed to any kind of success at all.
Because there hasn't been any success, there was a lot of debate, and there were different
schools of thoughts that had different arguments about how machine learning and AI should be.
You had people who were into knowledge representation from a good old-fashioned AI.
You had people who were Bayesian, and they liked Bayesian non-parametric methods.
You had people who like graphical models, and you had the people who like neural networks.
Those people were marginalized because neural networks did not have the property that you
can't prove math theorems about them.
If you can't prove theorems about something, it means that your research isn't good.
That's how it has been.
The reason why I gravitated to neural networks from the beginning is because it felt like
those are small little brains.
Who cares if you can't prove any theorems about them?
Because we are training small little brains, and maybe they'll do something one day.
The reason that we were able to do AlexNet when we did is because of a combination of
two factors, three factors.
The first factor is that this was shortly after GPUs started to be used in machine learning.
People had an intuition that that's a good thing to do, but it wasn't today where people
exactly knew what an NGP used for.
It was like, let's play with those cool fast computers and see what we can do with them.
It was an especially good fit for neural networks, so that definitely helped us.
I was very fortunate in that I was able to realize that the reason neural networks of
that time weren't good is because they were too small.
If you try to solve a vision task with a neural network which has like a thousand neurons,
what can it do?
It can't do anything.
It doesn't matter how good your learning is and everything else, but if you have a much
larger neural network, it will do something unprecedented.
I'll give you the intuition to think that that was the case, because I think at the
time it was reasonably contrarian to think that despite the human brain in some sense
works that way or different biological neural circuits.
I'm just curious what gave you that intuition early on to think that this was a good direction.
I think looking at the brain and specifically, if all those things follow very easily, if
you allow yourself to accept the idea, right now this idea is reasonably well accepted.
Back then, people still talked about it, but they haven't really accepted it or internalized.
The idea that maybe an artificial neuron in some sense is not that different from a biological
neuron.
Now, whatever you imagine animals do with their brains, you could perhaps assemble some
artificial neural network of similar size.
Maybe if you train it, it will do something similar.
That leads you to start to imagine, almost imagine the computation being done by the
neural network.
You almost think if you have a high resolution image and you have one neuron for a large
group of pixels, what can the neuron do?
It's just not much it can do, but if you have a lot of neurons, then they can actually
do something and compute something.
I think it was considerations like this plus a technical realization.
The technical realization is that if you have a large training set that specifies the behavior
of the neural network, and the training set is large enough such that it can constrain
the large neural network sufficiently, and furthermore, if you have the algorithm to
find that neural network, because what we do is that we turn the training set into a
neural network which satisfies the training set.
Neural network training can almost be seen as solving a neural equation, solving a neural
equation where every data point is an equation and every parameter is a variable.
It was multiple things.
The realization that the bigger neural network could do something unprecedented.
The realization that if you have a large data set together with the compute to solve the
neural equation, that's what gradient descent comes in, but it's not gradient descent.
Gradient descent was around for a long time.
It was certain technical insights about how to make it work, because back then the prevailing
belief was, well, you can't train those neural nets anything.
It's all hopeless.
So it wasn't just about the site.
It was about even if someone did think, gosh, it would be cool to train a big neural net,
they didn't have the technical ability to turn this idea into reality.
You needed not only to code the neural net, you need to do a bunch of things right, and
only then it will work.
And another fortunate thing is that the person whom I work with, Alex Krzyzewski, he just
discovered that he really loves GPUs, and he was perhaps one of the first person who
really mastered writing really, like, really performant code for the GPUs, and that's why
we were able to squeeze a lot of performance out of two GPUs and produce something unprecedented.
So to sum up, it was multiple things.
The idea that a big neural network, in this case, a vision neural network, a convolutional
neural network with many layers, one that's much, much bigger than anything that's ever
been done before, could do something very unprecedented, because the brain can see, and the brain is
a large neural network, and we can see quickly, so our neurons don't have a lot of time, then
the compute needed, the technical know-how, that in fact we can't train such neural networks,
and it was not at all widely distributed.
People in machine learning would not have been able to train such a neural network even
if they wanted to.
Did you guys have any, like, particular goal from a size perspective, or was it just as,
you know, and if that's biologically inspired, or where that number comes from, or just as
large as we can go?
Definitely as large as we can go, because keep in mind, I mean, we had a certain amount
of compute which we could usefully consume, and then what can it do?
Maybe if we think about just like the origin of open AI and the goals of the organization,
like what was the original goal, and how's that evolved over time?
The goal did not evolve over time, the tactic evolved over time.
So the goal of open AI from the very beginning has been to make sure that artificial general
intelligence, by which we mean autonomous systems, AI, that can actually do most of
the jobs and the activities and tasks that people do, benefits all of humanity.
That was the goal from the beginning.
The initial thinking has been that maybe the best way to do it is by just open sourcing
a lot of technology.
We later, and we also attempted to do it as a nonprofit, seemed very sensible.
This is the goal, nonprofit is the way to do it, what changed.
Some point at open AI, we realized, and we were perhaps among the earliest to realize
that to make progress in AI for real, you need a lot of compute.
Now, what does a lot mean?
The appetite for compute is truly endless, as now clearly seen, but we realized that
we will need a lot, and a nonprofit wouldn't be the way to get there, wouldn't be able
to build a large cluster with a nonprofit.
That's where we became, we converted into this unusual structure called CAP Profit.
To my knowledge, we are the only CAP Profit company in the world, but the idea is that
investors put in some money, but even if the company does incredibly well, they don't get
more than some multiplier on top of their original investment.
The reason to do this, the reason why that makes sense, there are arguments, one could
make arguments against it as well, but the argument for it is that if you believe that
the technology that we are building, AGI, could potentially be so capable as to do every
single task that people do, does it mean that it might un-employ everyone?
Well, I don't know, but it's not impossible, and if that's the case, it makes sense, it
will make a lot of sense if the company that builds such a technology would not be able
to make infinite, would not be incentivized rather to make infinite profits.
I don't know if it will literally play out this way because of competition in AI, so
there will be multiple companies, and I think that we'll have some unforeseen implications
on the argument which I'm making, but that was the thinking.
I remember visiting the offices back when you were, I think, housed at YC or something,
or cohabited some space there, and at the time there was a suite of different efforts.
There was robotic arms that were being manipulated, and then there was some video game related
work, which was really cutting edge.
How did you think about how the research agenda evolved, and what really drove it down this
path of transformer-based models and other forms of learning?
So I think it has been evolving over the years from when we started OpenAI.
In the first year, we indeed did some of the more conventional machine learning work.
The conventional machine learning work, I mean, because the world has changed so much,
a lot of things which were known to everyone in 2016 or 2017 are completely and utterly
forgotten.
It's like the Stone Age almost.
So in that Stone Age, the world of machine learning looked very different.
It was dramatically more academic.
The goals, values, and objectives were much more academic.
They were about discovering small bits of knowledge and sharing them with the other researchers
and getting scientific recognition as a result.
And it's a very valid goal, and it's very understandable.
I've been doing AI for 20 years now.
More than half of my time that I spent in AI was in that framework.
And so what do you do?
You write papers, you share your small discoveries.
Two realizations.
The first realization is just at a high level, it doesn't seem like it's the way to go for
a dramatic impact.
And why is that?
Because if you imagine how an AGI should look like, it has to be some kind of a big engineering
project that's using a lot of compute, right?
Even if you don't know how to build it, what that should look like.
You know that this is the ideal you want to strive towards.
So you want to somehow move towards larger projects as opposed to small projects.
So while we attempted a first large project where we trained a neural network to play
a real-time strategy game, as well as the best humans.
It's the Dota 2 project, and it was driven by two people, Jakub Pachotski and Greg Brokman.
They really drove this project and made it a success.
And this was our first attempt at a large project.
But it wasn't quite the right formula for us, because the neural networks were a little
bit too small.
It was just a narrow domain.
Just a game.
I mean, it's cool to play a game.
They kept looking.
At some point, we realized that, hey, if you train a large neural network, a very, very
large transformer to predict text better and better, something very surprising will happen.
This realization also arrived a little bit gradually.
We were exploring generative models.
We were exploring ideas around next-word prediction.
Those are ideas also related to compression.
We were exploring them.
Transformer came out.
We got really excited.
We were like, this is the greatest thing.
We're going to do transformers now.
It's clearly superior than anything else before it.
We started doing transformers with the GPT-1.
GPT-1 started to show very interesting signs of life.
And that led us to doing GPT-2.
And then ultimately, GPT-3.
GPT-3 really opened everyone else's eyes as well to, hey, this thing has a lot of traction.
There is one specific formula right now that everyone is doing.
And this formula is train a larger and larger transformer on more and more data.
For me, the big wake-up moment to your point was GPT-2 to GPT-3 transition, where you saw
such a big step function and capabilities.
And then obviously, with Forer, OpenAI has published some really interesting research
around some of the different domains of knowledge or domains of expertise or chain of thought
or other things that the models can suddenly do in an emergent form.
What was the most surprising thing for you in terms of emergent behavior in these models
over time?
You know, it's very hard to answer that question.
It's very hard to answer because I'm too close and I've seen it progress every step of the
way.
So as much as I'd like, I find it very hard to answer that question.
I think if I had to pick one, I think maybe the most surprising thing for me is the whole
thing works at all, you know?
It's hard.
I'm not sure I know how to convey this, what I have in mind here, because if you see a lot
of neural networks do amazing things, well, obviously neural networks is the thing that
works.
But I have witnessed personally what it's like to be in a world for many years where
the neural networks don't work at all.
And then to contrast that to where we are today, just the fact that they work and they do these
amazing things, I think maybe the most surprising, the most surprising, if I had to pick one,
it would be the fact that when I speak to it, I feel understood.
Yeah, there's a really good saying from, I'm trying to remember, maybe it's Arthur Clark
or one of the sci-fi authors, which is effectively it says advanced technology is sometimes
indistinguishable from magic.
Yeah, I'm fully in this camp.
Yeah, it definitely feels like there's some magical moments with some of these models now.
Is there a way that you guys decide internally, given all of the different capabilities you
could pursue, how to continually choose the set of big projects, you've sort of described
that centralization and committing to certain research directions at scale is really important
to open AI success.
Given the breadth of opportunity now, like, what's the process for deciding what's worth
working on?
I mean, I think there is some combination of bottom up and top down, where we have some
top down ideas that we believe should work, but we're not 100% sure.
We need to have good top down ideas, and there's a lot of bottom up exploration guided by those
top down ideas as well.
And their combination is what informs us as to what to do next.
And if you think about those bottom, I mean, either direction, top down or bottom up ideas,
like clearly we have this dominant continue to scale transformers direction.
Do you explore additional, like, architectural directions or is that just not relevant?
Is it possible that various improvements can be found?
I think I think improvements can be found in all kinds of places, both small improvements
and large improvements.
I think the way to think about it is that while the current thing that's being done
keeps getting better, as you keep on increasing the amount of compute and data that you put
into it.
So we have that property, the bigger you make it, the better it gets.
It is also the property that different things get better by different amounts as you keep
on improving, as you keep on scaling them up.
So not only do you want to, of course, scale up what we are doing, we also want to keep
scaling up the best thing possible.
What is a, I mean, you probably don't need to predict because you can see internally,
what do you think is improving most from a capability perspective in the current generation
of scale?
The best way for me to answer this question would be to point out the, to point to the
models that are publicly available.
And you can see how they compare from this year to last year.
And the difference is quite significant.
I'm not talking about the difference between, not only the difference between, let's say
you can look at the difference between GPT-3 and GPT-3.5 and then chat GPT, chat GPT-4,
GPT-4 with vision, and you can just see for yourself.
It's easy to forget where things used to be, but certainly the big way in which things
are changing is that these models become more and more reliable before they were very, they
were only very partly there.
Right now they are mostly there, but there are still gaps.
And in the future, perhaps these models will be there even more.
You could trust their answers, they'll be more reliable, they'll be able to do more
tasks in general across the board.
And then another thing that they will do is that they'll have deeper insight.
As we train them, they gain more and more insight into the true nature of the human world.
And their insight will continue to deepen.
I was just going to ask about how that relates to sort of model scale over time because a
lot of people are really stricken by the capabilities of the very large scale models and the emergent
behavior in terms of understanding of the world.
And then in parallel, as people incorporate some of these things into products, which is
a very different type of path, they often start worrying about inference costs going
up with the scale of the model, and therefore they're looking for smaller models that are
fine-tuned.
But then, of course, you may lose some of the capabilities around some of the insights
and ability to reason.
And so I was curious in your thinking in terms of how all this evolves over the coming years.
I would actually point out that the main thing that's lost when you switch to the smaller
models is reliability.
I would argue that at this point, it is reliability that's the biggest bottleneck to these models
being truly useful.
How are you defining reliability?
So it's like when you ask it a question that's not much harder than other questions that
the model succeeds at, then you have a very high degree of confidence that it will continue
to succeed.
So I'll give you an example.
Let's suppose that I want to learn about some historical thing.
And I can ask, tell me what is the prevailing opinion about this and about that, and I can
keep asking questions.
And let's suppose I answered 20 of my questions correctly.
I really don't want the 21st question to have a gross mistake.
That's what I mean by reliability.
Or let's suppose I upload some documents, some financial documents.
Suppose they say something, I want you to do some analysis to make some conclusion, and
I want to take action on this basis and this conclusion.
And it's like, it's not a super hard task.
And the model, these models clearly succeed on this task most of the time, but because
they don't succeed all the time, and if it's a consequential decision, I actually can't
trust the model any of those times, and I have to verify the answer somehow.
So that's how I define reliability.
It's very similar to the cell driving situation, right?
If you have a cell driving car and it's like, does things mostly well, that's not good enough.
Situation is not as extreme as with a cell driving car, but that's what I mean by reliability.
My perception of reliability is that, to your point, it goes up with model scale, but
also it goes up if you fine tune for specific use cases or instances or data sets, and so
there is that trade-off in terms of size versus specialized fine tuning versus reliability.
So certainly people who care about some specific application have every incentive to get the
smallest model working well enough.
I think that's true.
It's undeniable.
I think anyone who cares about a specific application will want the smallest model for it.
That's self-evident.
I do think, though, that as models continue to get larger and better, then they will unlock
new and unprecedentedly valuable applications.
So yeah, the small models will have their niche for the less interesting applications,
which are still very useful, and then the bigger models will be delivering on applications.
Okay, let's pick an example.
The task of producing good legal advice is really valuable if you can really trust the
answer.
Maybe you need a much bigger model for it, but it justifies the cost.
There's been a lot of investment this year at the 7B in particular, about 7B, 13B, 34B
sizes.
Do you think continued research at those scales is wasted?
No, of course not.
I mean, I think that in the kind of medium-term, medium-term by eye time scale anyway, there
will be an ecosystem, there will be different uses for different model sizes, there will
be plenty of people who are very excited for whom the best 7B model is good enough, they'll
be very happy with it, and then there will be plenty of very, very exciting and amazing
applications for which it won't be enough.
I think that's all.
I mean, I think the big models will be better than the small models, but not all applications
will justify the cost of a large model.
What do you think the role of open sources in this ecosystem?
Well, open source is complicated.
I'll describe to you my mental picture.
I think that in the near-term, open sources just help in companies produce useful.
Let's see.
Why would one want to use an open source model instead of a closed source model that's hosted
by some other company?
I mean, I think it's very valid to want to be the final decider on the exact way in which
you want your model to be used, and for you to make the decision of exactly how you want
the model to be used and which use case you wish to support.
I think there's going to be a lot of demand for open source models, and I think there
will be quite a few companies that will use them.
I'd imagine that will be the case in the near-term.
I would say in the long run, I think the situation with open source models will become more complicated,
and I'm not sure what the right answer is there.
Right now, it's a little bit difficult to imagine, so we need to put our future hat,
maybe futurist hat.
It's not too hard to get into a sci-fi mode when you remember that we are talking to
computers and they understand us, but so far these computers, these models, actually
not very competent.
They can't do tasks at all.
I do think that the will come a day where the level of capability of models will be
very high.
Like in the end of the day, intelligence is power, right?
Right now, these models, their main impact, I would say at least popular impact is primarily
around entertainment and like simple questionnaires.
So you talk to a model about this is so cool, you produce some images, you had a conversation,
maybe you had some questions, good answer, but it's very different from completing some
large and complicated task like, what about if you had a model which could autonomously
start and build a large tech company?
I think if these models were open source, they would have a difficult to predict consequence.
Like we are quite far from these models right now and by quite far, I mean by item scale
but still like this is not what you're talking about, but the day will come when you have
models which can do science autonomously, like they'll deliver on big science projects
and it becomes more complicated as to whether it is desirable that models of such power
should be open sourced.
I think the argument there is a lot less clear cut, a lot less straightforward compared
to the current level models which are very useful and I think it's fantastic that the
current level models have been built.
So like that is maybe answered a slightly bigger question rather than what is the role
of open source models, what's the deal with open source?
And the deal is up to a certain capability, it's great but not difficult to imagine models
sufficiently powerful which will be built where it becomes a lot less obvious to the
benefits of their open source.
Is there a signal for you that we've reached that level or that we're approaching it?
Like what's the boundary?
So I think figuring out this boundary very well is an urgent research project.
I think one of the things that help is that the closed source models are more capable
than open source models, so the closed source models could be studied and so on.
And so you'd have some experience with a generation of closed source model and then you know like
oh this model's capabilities is fine, there's no big deal there, then in a couple years
the open source models catch up and maybe a day will come when we're going to say well
like these closed source models they're getting a little too drastic and then some other approaches
needed.
If we have our future hat on maybe it looks like think about like a several year timeline.
What are the limits you see if any in the near term in scaling?
Is it like data, token scarcity, cost of compute, architectural issues?
So the most near term limit to scaling is obviously data.
This is well known and some research is required to address it.
Without going into the details I'll just say that the data limit can be overcome and progress
will continue.
One question I've heard people debate a little bit is a degree to which the transformer based
models can be applied to sort of the full set of areas that you'd need for AGI and if
you look at the human brain for example you do have reasonably specialized systems or all
neural networks via specialized systems for the visual cortex versus you know areas of
higher thought, areas for empathy or other sort of aspects of everything from personality
to processing.
Do you think that the transformer architectures are the main thing that will just keep going
and get us there?
Do you think we'll need other architectures over time?
So I have to I understand precisely what you're saying and I have two answers to this question.
The first is that in my opinion the best way to think about the question of architecture
is not in terms of a binary, is it enough?
But how much effort, what will be the cost of using this particular architecture?
Like at this point I don't think anyone doubts that the transformer architecture can do amazing
things but maybe something else, maybe some modification could have some compute efficiency
benefits.
So you also better to think about it in terms of compute efficiency rather than in terms
of can it get there at all?
I think at this point the answer is obviously yes.
To the question about well what about the human brain and then with its brain regions
I actually think that the situation there is subtle and deceptive for the following reasons.
So what I believe you alluded to is the fact that the human brain has known regions.
It has like it has a speech perception region, it has a speech production region, it has
an image region, it has a face region, it has like all these regions and it looks like
it's specialized.
But you know what's interesting, sometimes there are cases where very young children
have severe cases of epilepsy at a young age and the only way they figured out how to
treat such children is by removing half of their brain.
Because it happens at such a young age these children grow up to be pretty functional adults
and they have all the same brain regions but they are somehow compressed onto one hemisphere.
So maybe some you know information processing efficiency is lost, it's a very traumatic
thing to experience but somehow all these brain regions rearrange themselves.
There is another experiment where that which was done maybe 30 or 40 years ago on ferrets.
So the ferret is a small animal, it's a pretty mean experiment, they took the optic nerve
of the ferret which comes from its eye and attached it to its auditory cortex.
So now the inputs from the eye starts to map to the speech processing area of the brain
and then they recorded different neurons after it had a few days of learning to see and they
found neurons in the auditory cortex which were very similar to the visual cortex or vice
versa it was either they mapped the eye to the ear to the auditory cortex or the ear
to the visual cortex but something like this has happened.
These are fairly well known ideas in AI that the cortex of humans and animals are extremely
uniform and so that further supports the AI like you just need one big uniform architecture
that's all you need.
Yeah in general it seems like every biological system is reasonably lazy in terms of taking
one system and then reproducing it and then reusing it in different ways and that's true
of everything from DNA including you know there's 20 amino acids and protein sequences
and so everything is made out of the same 20 amino acids on through to your point sort
of how you think about tissue architecture so it's remarkable that that carries over
into the digital world as well depending on the architecture you use.
I mean the way I see it is that this is an indication from a technological point of view
we have very much on the right track because you have all these interesting analogies between
human intelligence and biological intelligence and artificial intelligence.
We've got artificial neurons, biological neurons, unified brain architecture for biological
intelligence, unified neural network architecture for artificial intelligence.
At what point do you think we should start thinking about these systems in digital life?
I can answer that question.
I think that will happen when those systems become reliable in such a way as to be very
autonomous.
Right now those systems are clearly not autonomous.
They're inching there but they're not and that makes them a lot less useful too because
you can't ask it hey like do my homework or do my taxes or you see what I mean so the
usefulness is greatly limited as the usefulness increases they will indeed become more like
artificial life which also makes it more I would argue trepidations right like if you
imagine actual artificial life with brains that are smarter than humans go gosh that's
like that seems pretty monumental.
Why is your definition based on autonomy because if you often look at the definition
of biological life it has to do with reproductive capability plus I guess some form of autonomy
right like a virus isn't really necessarily considered alive much of the time right but
a bacteria is and you could imagine situations where you have symbiotic relationships or
other things where something can't really quite function autonomously but it's still
considered a life form so I'm a little bit curious about autonomy being the definition
versus some of these other aspects.
Well I mean definitions are chosen for our convenience and it's a matter of debate in
my opinion technology already has the reproduction the reproductive function right and if you
look at for example I don't know if you've seen those images of the evolution of cell
phones and then smartphones over the past 25 years you got this like what almost looks
like an evolutionary tree or the evolution of cars over the past century so technology
is already reproducing using the minds of people who copy ideas from previous generation
of technology so I claim that the reproduction is already there the autonomy piece I claim
is not and indeed I also agree that there is no autonomous reproduction but that would
be like can you imagine if you have like autonomously reproducing AIs I actually think that that
is a pretty traumatic and I would say quite a scary thing if you have an autonomously
reproducing AI if it's also very capable.
Should we talk about super alignment?
Yeah very much so.
Can you just sort of define it and then you know we were talking about what the boundary
is for when we when you feel we need to begin to worry about these capabilities being in
open source like what is super alignment and like why invest in it now.
The answer to your question really depends to where you think AI is headed if you just
try to imagine and look into the future which is of course a very difficult thing to do
but let's make let's let's try to do it anyway.
Where do we think things will be in five years or in ten years?
In progress has been really stunning over the past few years maybe it will be a little
bit slower but still if you extrapolate this kind of progress will be in a very very different
place in five years let alone ten years.
It doesn't seem implausible it doesn't seem at all implausible that we will have computers,
data centers that are much smarter than people and by smarter I don't mean just have more
memory or have more knowledge but I also mean have deeper insight into the same subjects
that we people are studying and looking into.
It means learn even faster than people like what would such AIs do?
I don't know certainly if such an AI were the basis of some artificial life it would
be well how do you even think about it if you have some very powerful data center that's
also alive in a sense that's what you're talking about and when I imagine this world
I my reaction is gosh this is very unpredictable what's going to happen very unpredictable
but the bare minimum there is a bare minimum which we can articulate that if such super
if such very very intelligent super intelligent data centers are being built at all we want
those data centers to hold warm and positive feelings towards people towards humanity because
those this is going to be non-human life in a sense potentially it could be potentially
be that so I would want that any instance of such super intelligence the warm feelings
towards humanity and so this is what we are doing with the super alignment project you're
saying hey if you just allow yourself if you just accept that progress that we've seen
maybe it will be slower but it will continue if you allow yourself that then can you stuck
and start doing productive work today to build the science so that we will be able to handle
the problem of controlling such future super intelligence of imprinting onto them a strong
desire to be nice and kind to people because those data centers right they'll be they'll
be really quite powerful you know they'll probably be many of them they will be the world will
be very complicated but somehow to the extent that they are autonomous to the extent that
they are agents to the extent they are beings I want them to be to be pro-social pro-human
social that's the goal what do you think is the likelihood of that goal I mean some of
it it feels like a outcome you can hopefully affect right but are we are we likely to have
pro-social AIs that we are friends with individually or you know as a species well I mean friends
bees I think that that part is not necessary the friendship piece I think is optional but
I do think that we want to have very pro-social AI I think it's I think it's possible I don't
think it's guaranteed but I think it's possible I think it's going to be possible and the
possibility of that will increase in so far as more and more people allow themselves to
look into the future into the five to ten year future and just ask yourself what what
do you expect AI to be able to do then how capable do you expect it to be then and I
think that with each passing year if indeed AI continues to improve and as people get
to experience it's because right now you're talking making arguments but if you actually
get to experience oh gosh the AI from last year which was really helpful this year puts
the previous one to shame and you go okay and then one year later and one year it's
starting to do science the AI software engineer is starting to get really quite good let's
say I think that will create a lot more desire in people for what you just described for
the future super intelligence to indeed be very pro-social you know I think it's going
to be a lot of disagreement it's going to be a lot of political questions but I think
that as people see AI actually getting better as people experience it the desire for the
pro-social super intelligence the humanity loving super intelligence you know as much
as this is as much as it can be done will increase and on the scientific problem you know I think
right now it's still be in an area where not that many people were working on our AI is
getting powerful enough for you can really start studying it productively you'll have
some very exciting research to share soon but I would say that's the big picture situation
here just really it's really boils down to look at what you've experienced with AI up until now
ask yourself like is it slowing down we will slow down next year like we will see and we'll
experience it again and again and I think it will keep and what needs to be done it will
keep coming here do you think we're just on an accelerated path because I think fundamentally
if you look at certain technology waves they tend to inflect and then accelerate versus
decelerate and so it really feels like we're in an acceleration phase right now versus the
deceleration phase yeah I mean we are right now it is indeed the case that we are in an
acceleration phase you know it's hard to say you know so multiple forces will come into play
some forces are accelerating forces and some forces are decelerating so for example the cost
and scale are a decelerating force the fact that our data is finite is a decelerating force to
some degree to some degree at least I don't want to overstate yeah it's kind of a within an
asymptote right like at some point you hit it but when it's the standard S curve right or
sigmoidal well we the data in particular I just think it won't be it just won't be an issue because
we'll figure out some something else but then you might argue like the size of the engineering
project is a decelerating force just the complexity of management on the other hand the amount of
investment is an accelerating force the amount of interest from people from engineers scientists
is an accelerating force and I think there is one other accelerating force and that is the fact
that biological evolution has been able to figure it out and the fact that up until now progress in
AI has had up until this point this weird property that it's kind of been you know it's been very
hard to execute on but in some sense it's also been more straightforward than one would have
expected perhaps like in some sense I don't know much physics but my understanding is that if you
want to make progress in quantum physics or something you need to be really intelligent and
spend many years in grad school studying how these things work whereas with AI you have people come
in get up to speed quickly start making contributions quickly it has the flavor is somehow different
somehow it's very there is some kind of there's a lot of give to this particular area of research
and I think this is also an accelerating force how will it all play out remains to be seen like it
may be that somehow the scale required of engineering complexity will start to make it so that the rate
of progress will start to slow down it will still continue but maybe not as quick as we had before
or maybe the forces which are coming together to push it will be such that it will be as fast for
maybe a few more years before it will start to slow down if at all that's that would be my
articulation here Ilya this has been a great conversation thanks for joining us thank you
so much for the conversation I really enjoyed it find us on twitter at noprior's pod subscribe to
our youtube channel if you want to see our faces follow the show on apple podcasts spotify or wherever
you listen that way you get a new episode every week and sign up for emails or find transcripts
for every episode at no-priors.com

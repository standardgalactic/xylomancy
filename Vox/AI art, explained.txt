Seven years ago, back in 2015, one major development in AI research was automated image captioning.
Machine learning algorithms could already label objects and images,
and now they learn to put those labels into natural language descriptions.
And it made one group of researchers curious.
What if you flipped that process around?
We could do image to text.
Why not try doing text to image as well and see how it works?
It was a more difficult task.
They didn't want to retrieve existing images the way Google Search does.
They wanted to generate entirely novel scenes that didn't happen in the real world.
So they asked their computer model for something that it would have never seen before.
Like all the school buses you've seen are yellow,
but if you write the red or green school bus,
would it actually try to generate something green?
And it did that.
It was a 32 by 32 tiny image,
and then all you could see is like a blob of something on top of something.
They tried some other prompts, like a herd of elephants flying in the blue skies,
a vintage photo of a cat,
a toilet seat sits open in the grass field,
and a bowl of bananas is on the table.
Maybe not something to hang on your wall,
but the 2016 paper from those researchers
showed the potential for what might become possible in the future.
And the future has arrived.
It is almost impossible to overstate
how far the technology has come in just one year.
Leaps and bounds.
Oh, it's been quite dramatic.
I don't know anyone who hasn't immediately been like,
what is this? What is happening here?
Can I say, like, waves crash?
Watching waves crashing.
Party hat guy.
Seafoam dreams.
A coral reef.
Cubism.
Caterpillar.
Dancing toggle.
My prompt is Salvador Dali painting the skyline of New York City.
You may be thinking, wait,
AI-generated images aren't new.
You might have heard about this generated portrait
going for over $400,000 at auction back in 2018.
Or this installation of morphing portraits,
which Sotheby sold the following year.
It was created by Mario Klingemann,
who explained to me that that type of AI art
required him to collect a specific dataset of images
and train his own model to mimic that data.
Let's say, oh, I want to create landscapes,
so I collect a lot of landscape images.
I want to create portraits.
I train it on portraits,
but then the portrait model would not really be able to create landscapes.
And same with those hyper-realistic fake faces
that have been plaguing LinkedIn and Facebook.
Those come from a model that only knows how to make faces.
Generating a scene from any combination of words
requires a different, newer, bigger approach.
Yeah, now we kind of have these huge models,
which are so huge that somebody like me
actually cannot train them anymore on their own computer.
But once they are there,
they are really kind of, they contain everything.
I mean, to a certain extent.
What this means is that we can now create images
without having to actually execute them
with paint or cameras or pen tools or code.
The input is just a simple line of text.
And I'll get to how this technology works later in the video,
but to understand how we got here,
we have to rewind to January 2021.
That's when a major AI company called OpenAI announced Dali,
which they named after these guys.
They said that it could create images from text captions
for a wide range of concepts.
And they've recently announced Dali 2,
which promises more realistic results and seamless editing.
But they haven't released either version to the public.
So over the past year,
a community of independent, open-source developers
built text-to-image generators out of other pre-trained models
that they did have access to.
And you can play with those online for free.
Some of those developers are now working
for a company called Mid Journey,
which created a Discord community with bots
that turn your text into images in less than a minute.
Having basically no barrier to entry to this
has made it like a whole new ballgame.
I've been up until like two or three in the morning,
just, you know, really trying to change things
and piece things together and done about 7000 images.
It's just ridiculous.
Mid Journey currently has a waitlist for subscriptions,
but we got a chance to try it out.
Oh, wow!
That is so cool!
It has some work to do.
I feel like it can be, it's not dancing,
and it could be better.
The craft of communicating with these deep learning models
has been dubbed prompt engineering.
What I love about prompting, for me,
it's kind of really, it has something like magic
where you have to know the right words for the spell.
You realize that you can refine the way you talk to the machine.
It becomes, like, kind of a dialogue.
You can say, like, Octane Render, Blender 3D.
Made with Unreal Engine.
Certain types of film lenses and cameras.
1950s, 1960s.
Dates. Dates are really good.
Lionel Cut or Woodcut.
Coming up with funny pairings, like Faberge, Egg McMuffins.
A monochromatic, infographic poster about typography
depicting Chinese characters.
Some of the most striking images can come from
prompting the model to synthesize a long list of concepts.
It's kind of like a, it's having a very strange collaborator
to bounce ideas off of and get unpredictable ideas back.
I love that!
My prompt was chasing sea foam dreams,
which is a lyric from the Ted Leal
and the Pharmacist song, Biomusicology.
Can I use it as the album cover for my first album?
Absolutely.
All right.
For an image generator to be able to respond
to so many different prompts,
it needs a massive, diverse training data set.
Like, hundreds of millions of images
scrape from the internet
along with their text descriptions.
Those captions come from things like the alt text
that website owners upload with their images
for accessibility and for search engines.
So that's how engineers get these giant data sets.
But then what do the models actually do with them?
We might assume that when we give them a text prompt,
like a banana inside a snow globe from 1960,
they search through the training data
to find related images
and then copy over some of those pixels.
But that's not what's happening.
The new generated image doesn't come from the training data.
It comes from the latent space of the deep learning model.
That'll make sense in a minute.
First, let's look at how the model learns.
If I gave you these images
and told you to match them to these captions,
you'd have no problem.
But what about now?
This is what images look like to a machine,
just pixel values for red, green and blue.
You'd just have to make a guess,
and that's what the computer does too at first.
But then you could go through thousands of rounds of this
and never figure out how to get better at it,
whereas a computer can eventually figure out a method that works.
That's what deep learning does.
In order to understand that this arrangement of pixels is a banana
and this arrangement of pixels is a balloon,
it looks for metrics that help separate these images
in mathematical space.
So how about color?
If we measure the amount of yellow in the image,
that would put the banana over here
and the balloon over here in this one-dimensional space.
But then what if we run into this?
Now our yellowness metric isn't very good
at separating bananas from balloons.
We need a different variable,
so let's add an axis for roundness.
Now we've got a two-dimensional space
with the round balloons up here and the banana down here.
But if we look at more data,
we may come across a banana that's pretty round
and a balloon that isn't.
So maybe there's some way to measure shininess.
Balloons usually have a shiny spot.
Now we have a three-dimensional space.
And ideally, when we get a new image,
we can measure those three variables
and see whether it falls in the banana region
or the balloon region of the space.
But what if we want our model to recognize
not just bananas and balloons, but all these other things?
Yellowness, roundness, and shininess
don't capture what's distinct about these objects.
We need better variables and we need a lot more of them.
That's what deep learning algorithms do
as they go through all the training data.
They find variables that help improve their performance
on the task, and in the process,
they build out a mathematical space
with way more than three dimensions.
We are incapable of picturing multi-dimensional space,
but mid-journeys model offered this, and I like it.
So we'll say this represents the latent space of the model.
And it has more than 500 dimensions.
Those 500 axes represent variables
that humans wouldn't even recognize
or have names for,
but the result is that the space has meaningful clusters,
a region that captures the essence of bananas,
a region that represents the textures and colors of photos
from the 1960s, an area for snow,
and an area for globes and snow globes somewhere in between.
Any point in this space can be thought of
as the recipe for a possible image,
and the text prompt is what navigates us to that location.
But then there's one more step,
translating a point in that mathematical space
into an actual image in pixel space
involves a generative process called diffusion.
It starts with just noise,
and then over a series of iterations,
arranges pixels into a composition
that makes sense to humans.
Because of some randomness in the process,
it will never return exactly the same image
for the same prompt.
And if you enter the prompt into a different model
designed by different people and trained on different data,
you'll get a different result,
because you're in a different latent space.
No way!
That is so cool!
What the heck?
The, like, brush strokes, the color palette,
that's fascinating.
I wish I could, like, I mean, he's dead,
but, like, up to me, like, look what I have.
Oh, that's pretty cool.
Probably the only dolly that I could afford any price.
The ability of deep learning to extract patterns from data
means that you can copy an artist's style
without copying their images,
just by putting their name in the prompt.
James Gurney is an American illustrator
who quickly became a popular reference
for users of text-to-image models.
I asked him what kind of norms he would like to see
as prompting becomes widespread.
I think it's only fair to people looking at this work
that they should know what the prompt was
and also what software was used.
Also, I think the artist should be allowed to opt in
or opt out of having their work
that they worked so hard on by hand
be used as a data set for creating this other artwork.
James Gurney, I think he was a great example
of being someone who was open to it,
started talking with their artists,
but I also heard of other artists
who got actually extremely upset.
The copyright questions regarding the images
that go into training the models
and the images that come out of them
are completely unresolved,
and those aren't the only questions
that this technology will provoke.
The latent space of these models contains some dark corners
that get scarier as outputs become photorealistic.
It also holds an untold number of associations
that we wouldn't teach our children,
but that it learned from the internet.
If you ask for an image of a CEO,
it's like an old white guy.
If you ask for images of nurses, they're all women.
We don't know exactly what's in the data sets
used by open AI or mid-journey,
but we know the internet is biased
toward the English language and Western concepts
with whole cultures not represented at all.
In one open-source data set,
the word Asian is represented first and foremost
by an avalanche of porn.
Yeah, it really is just sort of a infinitely complex mirror
held up to our society and what we deem worthy enough
to put on the internet in the first place
and how we think about what we do put up.
But what makes this technology so unique
is that it enables any of us to direct the machine
to imagine what we wanted to see.
A party hat guy, space invader,
a caterpillar, and a ramen bowl.
Prompting removes the obstacles
between our ideas and images
and eventually videos, animations, and whole virtual worlds.
We are on a voyage here that is...
it's a bigger deal than just like one decade
or the immediate technical consequences.
It's a change in the way humans imagine, communicate,
work with their own culture,
and that will have long-range good and bad consequences
that we are, just by definition,
not going to be capable of completely anticipating.
Over the course of researching this video
I spoke to a bunch of creative people
who have played with these tools
and I asked them what they think this all means
for people who make a living making images.
The human artists and illustrators and designers
and stock photographers out there.
And they had a lot of interesting things to say
so I've compiled them into a bonus video.
Please check it out and add your own thoughts in the comments.
Thank you for watching.

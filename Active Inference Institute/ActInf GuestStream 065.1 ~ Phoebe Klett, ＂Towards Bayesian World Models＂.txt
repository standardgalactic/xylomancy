Hello and welcome. This is Active Inference Gastream 65.1. It's December 6, 2023. We'll
be hearing from Phoebe Klett and Dan Simpson on Bayesian world models for explainable transparent
reasoning. There will be a presentation followed by a discussion. So thank you for joining Phoebe,
very much. Looking forward to the presentation to you. Awesome. Thanks for having us. Yeah,
I'm really excited to chat with you all about how we might start to integrate today's state-of-the-art
language models into more probabilistic machinery and what that might bias. All right. Let's get
right into it. All right. So some of the things that I'm hoping to discuss today include why we
might use a language model for something that isn't long-form text generation and how we might do
that, and then motivate a little bit kind of why we might need a world model, what simple
self-organizing world models might look like, and even in the simplest cases how we might start to
use those as effective recommendation engines in the wild today, and then a little bit of discussion
about kind of where this research is going. All right. So what are language models good at?
Today's especially large language models are trained on next token production. So this means
given a sequence of tokens, we're going to estimate which token is most likely to come next,
maybe rephrasing that a little bit. We might also say that language models are trained to estimate
which sequences of tokens or words are likely. The caveat being likely to appear in the training
set, but today our training sets are quite extensive. So given this simple training objective,
it's arguably surprising that we've seen language models do as well as they have
in really impressive tasks. So they start to demonstrate like really great language understanding,
meaning like the semantics of language itself, and not just the syntax of kind of like which
sentences make sense. They've also started to demonstrate some world knowledge. So implicitly
learning kind of how the world works just through our own human abstraction and how we articulate
that. We start to see language models falter at tasks which look more like symbolic problem solving.
So math in particular, we see this in programming, although we're getting better at this. And in
general, any kind of like long term planning tasks which require abstract reasoning. So we see this
also in problems that look like this, which are word problems, but which are really about kind of
like understanding abstractly how these kind of abstract objects relate to each other. And again,
this shouldn't be surprising that language models struggle to do things like this since it's so far
off from their original training objective. And in particular, even when we arrive at the right
answer in some of these cases, it's really hard to know after the fact kind of the reasoning for how
we ended up at the right answer in this particular case. And maybe more fundamentally, if we don't
know where the abstraction is happening or where the reasoning is happening, it's very hard to guide
that process. And so this starts to motivate the need for something that looks more like
an explicit world model. And now to start to borrow some words from Yasha Benjio,
this is one of the kind of biggest issues with today's language models is arguably that we're
asking the language model to be both the inference machine and the world model implicitly when this
doesn't quite make sense. And so some things that we might hope for in a world model are to model
causal relationships, to be really adept at modeling using uncertainty, and to be modular.
Jan Lacan puts this in a similar way. So we might kind of ask a world model to be able to
distinguish between which details are important versus irrelevant, and to be able to make predictions
that can be performed in sort of this abstract space of representations.
And so hopefully in the next following slides, we're going to motivate what it might look like to
use language models as the inference machines in maybe like the simplest case of self organizing
world models. And even in those most simple cases, how we start to get something much closer to the
kind of explainable reasoning, which I think lots of us are grasping for right now.
All right. So in a very simple case, we might think of a world model as simply a collection
of hypotheses, where we model confidence over each of those hypotheses. And in particular,
we care about predictive world models. So given some evidence or data that we've observed in the
world, we'd like to propose the world models, which best explain the evidence that we've observed.
And if that language sounded leading, it was indeed. We are proposing in this case to use
Bayes' rule, which we all know and love, which tells us exactly how to update our beliefs given
some new evidence. And so when our hypotheses are kind of these Bernoulli random variables,
this bottom term can simply be expanded into this equation on the slide.
And so the tricky part in this computation is the likelihood piece. So given this evidence,
given that our hypothesis is true, what's the likelihood that we would have observed the evidence?
And the claim that we're making is that language models are actually, this is like a natural task
to ask of a language model. When our evidence and our hypothesis are both semantic objects,
as we discussed, language models are kind of trained exactly to understand which
sequences of text are likely. So asking it to prompting it in a clever way,
such that we can extract this particular likelihood, is actually really natural.
So how do we do this? So we come up with this kind of clever prompting scheme
that allows us to extract again exactly how likely is this should have occurred.
So given that our hypothesis is h in this setting and our evidence is this curly e,
we only need to change our evidence into the conditional form and then phrase the question
like this to the language model. So the input sequence of text goes like, given that potentially
if our hypothesis is like Walmart has been severely impacted by COVID-19 pandemic,
would we have observed the evidence that Walmart laid off 10% of the material staff?
And so either using something like few shot prompting or guidance or
some kind of control generation technique, we can ensure that our language model outputs
either yes or no, and then use the logits from its answer to estimate that probability.
And again, the claim here is that this is actually a really natural
task to ask of a language model that leverages it's like innate reasoning engine better than
just kind of allowing it to ramble using text sometimes.
And we can even make this updating scheme a bit more sophisticated using things like precision
weighting. So this will bias our posterior over hypothesis either towards our prior
or our evidence that we've observed based on how confident we are in our prior and in our evidence.
All right. So the world models that we've seen so far are fairly simple.
In particular, we're modeling all our hypotheses as independent from each other.
And this seems like a large simplifying assumption. So how might we support more complex world
models where we condition our hypotheses on each other?
All right. So another thing that hopefully will be familiar to this crowd,
Bayes and Nets have been around for forever and are often used to model mechanistic failures and
enlarge systems and can be understood simply as a distribution where each variable depends on some
small number of ancestor variables. Perhaps more intuitively, we can also think of these
as directed graphs where we have edges between variables which are directed in the sense that
the parent is conditioned on the child. So let's see an example.
All right. So now we have two different kinds of hypotheses.
The ones which are kind of parents in our simple setup are kind of qualitatively more abstract
than the children. So our two children nodes are similar to the hypothesis we saw before.
Marcus sentiment for Best Buy is poor or Walmart will grow its physical footprint this year.
And then the more abstract hypothesis being retailers were negatively impacted by COVID-19.
And so then we specify this conditional structure either kind of in a classical sense
by specifying all of the joint probabilities upfront or alternatively learning it given some
healthy amount of training data. And again, we're proposing that instead of doing that
intensive process, we can use language models to extract those probabilities in a natural way.
And so one natural complaint at this point might be like, well, this space is going to start to
get very large. If we're trying to update these beliefs in real time, given some large data stream,
this is going to become intractable, given our current framework. So how might we start to augment
that? Luckily, there's been a lot of work historically done on this problem. And we can
start to use things like message passing to update our beliefs in an approximate way.
So as long as our Bayes net has a tree dependency structure, we can use things like the sum product
algorithm to update our beliefs. And I don't want you to worry too much about the equations on
this slide. If you've seen this before, this kind of recursive structure will look familiar.
But if not, the general kind of intuitive idea that we're going to compute these messages from
all of the children or the neighboring nodes and use those to propagate through the graph to update
each independent belief. But let's just see an example.
All right, so suppose we observe some new evidence, and we'd like to know how should we update
probability over hypothesis A, which as you remember is the parent node in our graph.
All right, so B here on the left hand side is the belief. XA is essentially just the variable
representing the hypothesis A. And then the fee and the side terms are specified by our language
model or the Bayes net in the classical sense. And so we're summing over all of the variables,
all of the values that the variables XB and XC can take, which in our case, of course,
is just zero and one. So if you might recognize at this point that this is indeed the exact
marginal probability for the hypothesis A, and that's because our graph is simply connected
in this example. So in general, this isn't true, but it turns out to be the case in our example.
And again, we can compute these fee and side terms using the language model itself.
All right, so because we're all lovers of free energy here, I'm going to walk through kind of
how this is maybe the first example of self evidencing or minimizing free energy kind of model.
So as we noted, belief propagation isn't exact for more complicated graphs. And so
it makes sense and might be useful to ask the question, how far apart or when are our beliefs
close to the exact marginals? And so we often use things like KL divergence to compare the
difference between two probability distributions. And that is executed on the right hand side.
And then those of us who are like have a background in physics might recognize Boltzmann's law as well.
So this is just the idea that we might represent the probability of a given state using an energy
function. And we're not going to accept this as truth, maybe as some of us have done in the past,
but we are going to just use it as a definition for this energy function,
such that when we plug in that term here and expand out, we start to see these first two
terms look a lot like the kind of energy and entropy functions which we are used to. And indeed,
we can just classify those two terms as the Gibbs free energy function. Yeah, which makes me happy
to see this all coming together. And in particular, it might make sense just to note at this point
that using world models which are self-organizing in the sense seems to be very compelling since
the kind of world model which we want is one which promotes the evidence that we've seen so far.
Why is it useful to formulate this in terms of free energy besides the fact that we all
find it compelling here? Well, you can make a lot of progress by constructing analytically
tractable approximations of Gibbs free energy often. I'm not going to go into the details here,
but here are two examples where that's been fruitful.
All right. So now I'm going to chat briefly about kind of how we might use
systems like or how we might use these systems as recommendation systems and indeed in the world
today. All right. So one like prime example for a wear system like this might be useful is a
situation where we have kind of lots of data incoming at very high frequencies and we always
want to have some set of like naturally discrete hypotheses that we're modeling beliefs over which
are being kept up to date at a very regular cadence. And so actually a lot of the muscle here
is just reformatting documents or however our data comes in as evidence. This is not always
obvious or easy to do, but once you've kind of figured out that part and in particular we've
been using things like RAG, retrieval augmented generation or embedding based systems to kind
of figure out when data that's incoming is relevant to a given hypothesis. Once you've kind
of built up that machinery, the actual updating computations as we've shown already is actually
pretty simple. So we do these likelihood computations and we update our beliefs and then at any given
time we can query that model for our marginal distribution over any given hypothesis.
And it turns out that this kind of setup has many practical applications.
It's also noteworthy that even with very simple systems like these, these are like out of the box,
controllable and explainable. So just by storing the magnitude and the direction of the update to
the posterior for each piece of evidence that we observe, we have a very natural built-in
explanation for our belief at any given time. And that makes kind of like these applications
where folks might really like to use a language model, but really require like a robust like causal
relationship between the outputs and the explanation, which you don't get from a language model on its
own. A system like this can be very appealing in those situations.
All right, so now on to further work.
So everything that we've discussed today is early work towards integrating language models
into more probabilistic frameworks. And there's been a lot of exciting work done in this vein
right now. Some important questions which are especially interesting to me are which parts
of the world model should be learned versus encoded? And how do we want intelligence to scale?
So I mean that both in the sense of composing systems naturally, there should be some very
like natural way that we can compose to intelligent systems and also such that we can scale them with
compute. And I don't mean to restrict myself either to the kinds of compute that we have today,
we're also working at some exciting new computing paradigms at normal, which might be
more compatible with software of this nature. Also the two folks that we referenced at the
beginning of the talk, Jeff Hinton and Yann LeCun have done really exciting work in this area,
which is yeah, very inspiring. And so in particular, G flow nets are also probabilistic
graphical models, which I think folks will find a natural next step in reading.
Yeah, if if you so desire.
And that's, um, that's it for me.
Awesome. Thank you. Wow, very
cool. Dan, do you want to give a first reflection or thought? And then
meanwhile, anyone who's watching live, please feel free to write questions. I'll relay them in.
Um, absolutely. Um, so hi, I'm, I'm, I'm Dan. I work with Phoebe on this project.
And yes, the, um, I think the thing that's most exciting about this for me personally
is sort of two fold. One of them is that it's a way of avoiding sort of having to trust
a language model to understand and reason about text. Um, because they're not,
it's not that they're bad. The thing is that the extremely strange thing about language models
is they're quite good at being almost good enough. Um, but they're never quite what you could use.
You could never use a language model to, I don't know, um, sort of triage like an important
sort of situation where a bunch of different things are coming in and you have to make a
decision about which is important. The reason you can't do that is you, you simply cannot
understand the encoded biases. You cannot get it to reliably generate reasoning. You can ask
it for reasoning, but the thing that it prints out is not the reasoning that it used internally
because it doesn't reason. Um, fundamentally, while these have input and output than a natural
language, they are not artificially intelligent. They are just prediction machines. Um, and so we
have to be very careful about not anthropomorphizing them. So this is a way of using those incredibly
powerful prediction machines in a framework where we can, um, make sure that we,
uh, essentially keep a record of what we're doing so that a human can look at it.
Because, I mean, there's a lot of talk in this world about sort of post human AI
and those sorts of things. The idea that the machines will become intelligent enough
or the machines will rise up in a slightly more alarming type of way. Um, and that's all great
and wonderful, but that's not particularly interesting to us at normal. We're much more
interested in sort of having, you know, mimicking explicit decision processes so that a human can
audit them and can make these things work. So that's kind of the, like the area that we're,
we're sort of coming from. Awesome. All right. I'll go to a question from the chat.
So Josh asks, great talk. Where does hypothesis relevance enter the calculus? Is it folded
into confidence? Not sure if it ought to be. Just saw hypothesis relevance mentioned.
Hypothesis relevance. Does that mean, um,
like which hypotheses are conditioned on each other? Is it possible to ask a clarifying question
there? Maybe, Dan, you have a better idea. They can follow up, but yeah, like, um, I also wondered
about this. Like, you might know what was relevance. Maybe the temperature and the rainfall were
relevant, but then how does this approach help us understand when one of those relevant factors
no longer is relevant or when a new relevant factor comes into play?
Yeah. These are great questions. Um, so I think in terms of understanding in an automatic sense,
when two hypotheses are, are relevant to each other, we can leverage, um, embedding type,
you know, language models for this kind of thing also. Um, if we don't have a more kind of like
structured human intuitive sense for when two hypotheses are, are related, uh, in terms of like
how those relationships evolve over time, this is something that's really interesting to me.
And I think looking at the theory behind structure learning or when we propose to add new nodes to
the network or propose to add a new edge to the network or things like this, uh, is a really
exciting research direction. Um, although I don't have like a silver bullet answer to how we should
do that. Um, just to like add a little bit more to that, um, it is like, it is a really interesting
research direction. Um, like one of, one of the things that Phoebe mentioned in the talk is that
there is a difficult step that we're not talking about, which is actually translating this natural
language into reasonable hypotheses. So there is a step in there where you take essentially a chunk
of text and you have to decide if this is a hypothesis, if this is a hypothesis we've seen
before, if this is a sub hypothesis or a clarification of a hypothesis that we've seen before,
and so on and so forth. So that's in some sense part of the data processing. And it is an important
step and one that we are sort of continuing to work on and refine. Uh, the other thing,
like a different sort of interpretation of the question around relevance is, um, around sort of
is the hypothesis relevant to the thing that you're looking at? I mean, we could have a hypothesis
the sky is blue. Uh, but if we are deciding to, deciding, you know, whether, whether or not we
need to check that part's oil, like the truth or not of the color of the sky is very irrelevant.
And that then comes into the nice thing about having your world described as a, um, collection of
statements with truth values associated with them, in that you can directly reason over them. So you
can put a classical decision framework, um, over that to take into account both the sort of the
knowledge you have of the world and also which parts of these worlds are sort of unknown. So in
that sort of situation, the person using the world model to construct a, um, a, a sort of decision
or an output will be responsible in some sense for assigning a weight or a cost to each hypothesis
being true. And for some of those hypotheses, obviously it will be zero. Um, because again,
we do not care about the color of the sky. If all I want to know is if I need to change the
oil in my car. Um, so that's the, that's the sort of the other end of the answer. So there's a version
of the answer at the start of the information flow and there's a version of the answer at the end
of the information flow, but it is a tricky point and one that we are sort of continuing to iterate
on to try and find sort of good ways on both ends of that. Yeah. Well, a lot there. Um,
it's very interesting how in that presentation in response, I heard both about probability
distributions on rules and rules on probability distributions and like with which one, whether
it's the tail wagging the dog or the horse in the cart, how to design these synthetic
intelligence systems that appropriately bring together aspects that are more symbolic, more
rule like, and then more probabilistic, more embedding like. So where does that
land with you or how do you see the design of these systems with mixed symbolic and probabilistic
components? Yeah, yeah, that's a great point. And I think this really gets it like which parts of
the world model should be learned, um, or should be represented in some like more discreet space
versus like encoded based on our own human intuition for rules and structure. Um, and I think like
maybe this is would be fairly represented as a cop out answer, but I think it depends a lot on
application. I think like when we're developing systems like this and and just trying to iterate
through as many different hypotheses as you can quickly, like choosing an application and and
benchmarking and testing and seeing like what actually works is a go to strategy for us.
In terms of like, well, which parts are should be fixed and are actually helpful.
Um, to increase reliability is such that like we can use our human intuition for how this
particular, you know, system is built, uh, versus like, well, this is something that we
we want uncertainty over. That's like a really important part of the learning process for us
in terms of, yeah, that kind of iteration. So I think I think it probably depends on the application.
Yeah, it's, um, it, it definitely depends on the application. It's also, um, like it depends on
where the actual challenge points are. So we've got like, outside of this, we've got sort of a
few other things that we've released publicly that kind of look at this idea of there being
at like external rules to the, um, to the system and whether or not we can add those in. So one
of them is something called constrained generation, where we sort of forced the model to only produce
something valid. And that's sort of quite a useful way of removing one particular aspect of stress
just from the model, which is that it may make sort of syntactically or sort of in somehow
incoherent, um, outputs that don't follow the rules. And then we can then focus with the rest of
our energy. We can then take that as given and focus with the rest of our energy on improving
the bit that we don't have rules for. Um, so, so those sorts of things and sort of a different
version is trying to improve something by saying, no, you broke a rule. We need to like go back and
make this sort of true. So this kind of, um, sort of chain of thought prompting type of idea. Um,
so, so yeah, the symbolic and the probabilistic, um, I think in our minds, we're very closely
together as two tools that don't completely solve the same problem. Um, and I think there's
sort of in the world of, I'm not sure how familiar anyone in the audience is with language modeling,
but like in the world of language modeling before this sort of explosion of neural networks and
artificial intelligence type methods, there was a lot of work on symbolics of language
and grammars and all of that sort of stuff. And that work pushed quite a long way forward.
Um, and this work is pushing quite a long way forward. And I suspect the next thing is going
to involve them joining up again, uh, because they each have good points. They each have bad points.
And, you know, two wrongs don't necessarily make it right, but they can make the less wrong.
Nice. Yeah. Recently we heard from Elliot Murphy and talking about the neural linguistics and
about how the statistics of language are not the rules of language. You can always come up with a
new expression that's never been uttered. That's not going to be in the training distribution or
any distribution. Okay. I'll ask a question in chat from upcycle club. They wrote,
what are some of the key challenges associated with developing such Bayesian world models?
Hmm. I think we've touched on a bunch of them. The ones that are most top of mind for me right
now are the structure learning thing that came up. So how do we understand, like, when to propose
new hypotheses and how to integrate those into the models? And then, yeah, just figuring out, like,
yeah, I guess this, like, proposal and evolution process of the nodes themselves.
Since everything else, like, the framework, like, works pretty automatically and in a
reasonable way. Thank you, Bayes. Thank you to the development of language models.
But kind of moving from this, like, more discrete case into a continuous case,
which, like, more fully represents the space that we're learning over can be challenging.
Yeah. I would also say that, like, it's a sort of a maxism that max, maxism, what on earth did I
just say? There's a common saying, let's go in that direction. There's a common saying in this
world that sort of no model ever survives its first encounter with data. And that becomes true here
as well. So there's lots of, like, as we've been building these things and using them,
we found lots of little spiky edge corners with sort of making sure that the language
world is actually doing what we want it to do. So there are a lot of questions in building these
things around how do you actually test that the components of it are actually working the way
you want? And then on, like, a broader level, how do you compare something that is fundamentally
trying to solve a different problem to other methods? So we are solving a problem under the
constraints that we want a fully auditable system. We could also solve all of these problems by a
thing called in context learning, which is basically putting the context into the prompt of a large
language model and asking it the answer. And that also works. Especially when you've got
things like GPT-4, which are just wonders and glories, it works really well. So then we come
to the question of how do we actually make the case for this from a, like, a bigger picture
perspective? Can it be more than just a, like, can we find benchmarks that reflect
the structural advantages of this approach over something like in context learning
that don't come across as false? So that's kind of like a stranger answer because it's not really
about, like, actually developing the world model. It's about sort of convincing other people that
it's a good idea. And that's, you know, that is a thing that is true of essentially all of the
things on this slide as well. They are all quite complex and odd little methods that, you know,
there's a degree to which, well, we definitely can solve this an easier way. So what is the
thing that, what is the application or the benchmark where we can say, no, if you do it the
easier way, you will fail at this measurable thing. Very interesting. So you mentioned the
self-evidencing advantages of using world models that are self-evidencing rather than
reward maximizing, for example. So how do you see that playing out? And I can connect it back to
active inference, of course, but how do you see this self-evidencing centrality play out in the
kinds of models described here? Yeah, I think there are a couple of reasons why it's so compelling to
me. And the first just has to do with explainability. Like, it's really convincing to people to say,
like, well, why, why did we predict this? Why do we believe this? Well, this is the actual real
world data that we've observed such that, you know, this is the impact that that's had.
And then I think, like, I don't know, you hear a lot about, like, designing these really complicated
reward functions, which are often very clever, but which often I feel like are close to being a
pitfall because they very easily become, like, disconnected from, like, the complex world that
we're trying to model. And so you end up in, like, weird local maximums or minimums. And,
um, yeah, you start, like, just kind of solving the problem that you've designed versus, like,
the problem which actually exists. And so I just have always loved the idea that what we should
be doing is kind of self-evidencing and from an intuitive sense that feels like what it feels
like what an intelligence system should do. Yeah. Yeah, I actually don't have anything interesting
to add to that. I just agree with Fabi. That explainability and the capacity to explicitly
reference previous data, including, like, leave one out. So techniques from nonparametric statistics
about the effect of adding in another piece of data or removing a piece of data. And then just,
like, to bring it to, like, a homeostatic setting, which is commonly considered an active inference.
Like, we're trying to be within a homeostatic temperature range of 37. Yes, we could propose
reward functions, but as those start to include open-endedness and exploration structure learning,
just like you described it, Fabi, like, we're solving the problem as designed rather than
the actual question of the homeostatic temperature and the sort of path of least action, first
principles, physics grounded intelligence perspective from active inference is, like,
make it the kind of thing that measures itself at 37. And then as long as it is, it is. And when it
isn't, it's dead. And that's the kind of mortal computing crossover, which is, like, outside of
its zone of surprise. It's not just that it's getting a bad grade in the class. That is, like,
a deeper failure signal than that. And to understand, okay, when is it a yellow flag? When is it a
red flag? In terms of the new scientific literature coming in, those have plain,
straightforward ways to interpret that developing larger higher order apparatuses
will never return to that kind of basal simplicity.
Yeah, couldn't agree more.
Yeah, I mean, absolutely. It also, like, the other thing that it can do quite well is deal with
essentially outlier studies. So, situations where you have a new piece of information that is
strongly conflicting with all the previous pieces of information, and trying to sort of work through
what that really means. And there's a, like, there's a degree to which we can even sort of extend
this process to multiple agents that have these belief systems and then look at sort of consensus
of experts or weighted consensus of experts. So, for instance, you could have, like, a weather
vein type of situation where somebody really overindexes to every new piece of information.
And you would do that with, you know, technically, you do that with maybe a power posterior type thing.
Or you can have somebody who's built in strong priors in a particular direction.
And you can then, like, take your consensus of artificial sort of decision making,
all of which has within their universe well-reasoned updates to the data. And then you can look and
try and work out what that swarm of experts can tell you. And sort of do very empirical things,
like, try and, you know, work out which of these experts is doing well at a particular moment in
time. Because, you know, there could be, there could be times when the world's very, or the problem
that you're solving is very chaotic, in which case the overindexing expert would probably be a pretty,
pretty solid bet. While there are other times where sort of things are pretty stable, and you
probably, it would be possible that the sort of the more conservative expert is more sort of
empirically making good decisions and good recommendations. So there's, like, a lot of
ways that we can not just, like, incorporate these sort of homeostasis type ideas, but we can also
change what that means for different agents and artificial, like, do that artificially and then
combine them together to try and get a, almost like a, like, blanking on the word, but, you know,
a forecast under a sort of a hypothetical set of situations. And we can actually sort of bring
those ideas of the world forward and see what happens when they sort of meet with actual information.
Yeah, this angle of mixture of experts as it's sometimes called more in the language model
space or ecosystems of shared intelligence or diverse intelligences in the active inference
area, like, that's very interesting, obviously has connections back to human teams and teams of
beyond humans and so on. A lot of this is still text based. So maybe you did or didn't mention
what representation the base graphs are, but they're plain text like, and there was a lot of
discussion about bringing from natural language, scientific papers or however it is, into a
structured form and then the explain method that you showed kind of taking the structured form and
just giving a little syntactic fluency so it looks human readable. So how do you see that
essence coming into play with multimodal models and then with action in the world that isn't just
developing the next text token, but robotic actuator or modifying some other control element
of the world? Yeah, that's a really good question. I honestly haven't thought much about
multimodal stuff in this particular context, but I think the framework is general and half
at this point, such that it's definitely could support lots of different modalities.
I'd be really curious to see how this did with something like audio in particular.
Yeah, and then to your point about like, yeah, this maybe like discrete versus continuous
relationship, I think that's part of what we're learning is how to go from these
long natural text documents to a system which is appropriately discretizing our hypotheses,
such that we have these like meaningful explanations like you mentioned.
So I think, yeah, I think like continuing to develop like robust ways of surfacing those
explanations is a big part of this as well. Like over time, we're going to observe lots and lots
and lots of evidence. How do we make sure like hypotheses don't get stale and how do we use
evidence to know when they are and things like this are part of that also. I don't know if that
directly answered your question, but that's some of the stuff that I've been thinking about related
to that. So in the, I mean, in examples like sort of moving towards robotics and sort of tech
video generation and image generation, other sort of multi modalities,
to be honest, I think of these processes in general as enabling us like building a world
model to enable a sort of sequential decision process. So if that decision process happens
to be should the robot turn left, and that's what the sort of the decision process is,
it's multimodal in like a very classical sense that you can put any type of decision
framework over the top, but it's not sort of generatively multimodal. I'm not saying,
write me a song that sounds like Beyonce and a song that sounds like Beyonce comes out.
I think this sort of this sort of Bayesian world model layer is blocking towards that sort of
thing, but that's really not sort of the aim of what we're trying to do. It's also like within
normal like our almost, I don't want to say mantra or manifesto because that sounds
culty and no one wants to sound culty, but like our basic aim is to always center like humans
within our process. And so some of this multimodal stuff, it's less clear where the human lives.
So for instance, like a video generation type thing, where does the human live. So keeping it at
this abstraction of sequential decision making, then it's a decision that a human could do,
you know, even with their thumbs could be moving like a robot around and doing that sort of stuff.
But yeah, it's really all about sort of controllability and auditability for us
in sort of a sequential decision process. So to the extent that that sort of leads in its
multimodal world, that's sort of part of what we're doing and like to some versions of multimodality
is we're just not swearing in that particular space.
Um, yeah, not a great answer, but a long one.
Let them distill it down later. In the auditability area, it almost falls out to me to be like a
syntax of auditability and semantics at the syntactic level, just tagging or versioning when
a file comes in or when a given computation is executed. That is basically transfer across all
settings. And then where I see you honing in on with, with this work is kind of the semantic
auditability, which is actually how we compose our accounts. I would have driven,
but I decided to walk because this happens. And so bringing that different kind of trace to
systems is going to make it, um, what will it open up in science or education? Or how do you see this
sitting at a console, somebody is at now and making this different like over what timeline?
Hmm. Yeah, I mean, it's really quite nice for storytelling because as you said, you can say
things precisely like, well, you know, because we observed this thing or because if we had observed
something else, you know, like maybe you can even make statements, which are, um, yeah conditional
in that sense. Uh, I think it does like empower whoever is sitting in front of this data to feel
like really sure about again, like that, the reasoning engine that like that went on,
which to me is, is pretty different from what it feels like to sit in front of chat GBT, even
though it's quite useful often. Um, you know, you, you try the code and it works or it doesn't work
or you like, you know, ask your friend, is this really true? Um, and that feels pretty different
to me from being able to, to look at the evidences themselves and say like, Oh, well, actually if
this is the reason you think that I know that that evidence is, is not true or, you know, like you
can bring your own human intuition or world model, uh, in terms of validating or, um, yeah,
superimposing what you believe on top of what this system believes. And so that makes it really
easy to make decisions, um, quickly. I think there's also sort of a converse of this, which
is that it also makes it clear which evidence was not used to make a decision.
And that can be quite telling in these situations where you could be worried that a particular
type of evidence isn't being weighted correctly or isn't being, um, sort of formatted correctly. So
again, like if this is a sort of a, like a system that builds an assistant, um, that sort of does
surfaces all this information and sort of makes a recommendation with reasoning for a person,
that person can then look and be like, and they know what the data is. They can look at the deck
and say, you know, why didn't you consider the make of the car? Or why didn't you consider this?
Or why didn't you consider that? And they can then use their understanding of what's not
being prominently used by the model to sort of sense test. Like it's, it's sort of, I mean,
in some sense, that usage is a reformulation of what Phoebe just said, where you use your internal
world model. But it's like, I think it's, it's important to know when evidence is being used.
And this is like, I think you simply cannot get, um, from, from like a GPT type thing or
any sort of like prompting type method, we know, for instance, that like, um, the order of the order
that you submit your evidence in is probably going to matter for a prompting based method.
Okay. That's obviously not true for a Bayesian update where we have this sort of this, this
coherence principle, where if you shuffle your data and enter it in a different way, you will get
the same posterior up to computational artifacts. Um, so, so, so all of that is like in my mind is
just as important to audibility as the ability to write a report that says I made this decision for
these reasons. Yeah, well, that makes me think about this kind of view from the inside interpretability
where the rules help and also knowing what evidence is not used is importance for compliance
and knowing what information like in a healthcare setting was or wasn't used. Um,
what about thermodynamics? We heard about free energy. Boltzmann came up. How do you see the
info thermo nexus? What have we learned from the last hundred years of thermodynamics and
information theory and all of this? And on the software or hardware side, how is that kind of
a free energy nexus being used? Yeah, I mean, I, I'm really excited about how all of this seems
to be coming together. Um, I think free energy just keeps showing up in all of these exciting
areas to me. We have like a book club for singular learning theory and like they talk all about
free energy too. And I think some of those ideas are really exciting. Um, I mean, at normal, I think
like the thing that I would highlight is like this idea of software hardware co design, which is
really special. And so we're trying to do this hard and fun dance towards each other where we're
like thinking about these new kinds of systems and how they might support each other and empower
each other and, and yeah, how to build full stack systems. Um, which is really challenging and also
really exciting. Um, yeah, and I think like from like the first principles of thermodynamics
perspective, like, like we're all just, uh, yeah, we're all kind of like mathematics and physics
people at heart. So like going, taking like, uh, you know, all of what people have learned in
language modeling and all of that, like, um, very much to heart as well. Like I think approaching
whatever problem that we're facing from a first principles, how do physical systems work in the
world? What do we really, what are the assumptions we're really comfortable with? Uh, and building
up from there, um, is definitely our natural mode. Uh, so I think that makes it easier to
start working together also. Um, it's also probably worth saying that we have
a sort of a secondary, not secondary, a very different stream of interest in thermodynamics
as well, which is the ways that we can use actual physical thermo dynamical principles
to build hardware that is specifically has sort of noise in it as a first class citizen. And
because of that, it is particularly well suited to probabilistic tasks. Um, and so we've, we've
built, if you, if anyone wants to look, we have a blog, I believe the URL is blog.normalcomputing.ar.
And amongst other things that are on it, uh, there is the very first demonstration of using
physical thermo dynamic hardware to actually do computations. Um, is the computation the most
vital computation that we will ever do. It's inverting an eight by eight matrix. So no,
we can do that otherwise. Um, but it, it is sort of building up towards this idea that
we can use thermodynamics not just in our modeling and our understanding of the world,
but also in our sort of low energy compute stack to actually realize these things. Um, so I think
it's, I think it would be challenging to find a group of people on this earth who have more
investment in thermodynamics and don't work in a physics department.
Uh, because we have investment all the way through from sort of active inference type
things all the way down to this, like this, this hot thing goes there.
Which is kind of cool. I'm not a physicist. So I have, but that's my view. That's my
understanding of thermodynamics. This hot thing goes there.
This informative thing goes here. Hot thing over there. Call it a day.
Um, yep. That it's a really cool fusion with the kind of parsimony and elegance
and the aesthetic of math and physics and first principles and the different parsimony of pragmatism
with the actual material basis, like of a synapse, the size of the synapse and the
kind of stochasticity that that size alone, um, entails with like membranes and all of this,
those stochastic aspects are leveraged for the compute. The synapse is not simply a variance
reducing machine. And so it's like both the platonic slash mathematical ish spirit finds
a common home in these real simple physical demonstrations. And, um, today it feels like
there's a big gap between the, um, mesoscale computational architectures that you described
today that are very much running on the kind of Von Neumann architecture, Turing completeness
paradigm, and yet very tantalizingly close, like to a physical object that has a constrained
rule de facto, like only one thing can come out of this at a time, as long as the funnel is this
wide. And so bringing the rules and the regularities of what we call physical things to bear with
the fundamental and the imposed constraints on informational spaces.
It's very cool directions. Um, one other note about just where active inference, um, an action plays
a role is, um, and also you mentioned like the hypotheses going stale or like sort of data being
over relied on, um, in the proactive stance where we're using expected free energy or something
like it to, to calculate future courses of action over observations that we haven't seen yet moves
that haven't been made yet. There's an explicit epistemic value. And so that can be diagnosed
and observed as a measure of where a given computation is on the continuum between purely
pragmatic value, just constraint, satisfying and realizing preferences and expectations.
And then the pure epistemic value where all outcomes are good, and the more information
gain, the better. And then being able to take control of that balance and know amid changing
situations, again, taking probabilistic or rule-based approach there to when epistemic
and pragmatic like gas and break kind of come into play. These are very basal, um, control knobs or
features in active inference that it's just not going to show up at the 50th layer of scaling
is all you need. Yeah. Cool. Well, do you have any other like thoughts or things you want to add
or questions or where things are heading for your works? Nothing to add at this moment,
but certainly excited to keep in touch with this community and yeah, collaborating.
Yeah, absolutely. And we sort of share, I mean, we write papers and stuff, but we mostly like we
share most of what we do, be it academic in the sort of machine learning space or be it in the
sort of the physical hardware space on our blog, which is blog.normalcomputing.ai.
And yeah, thank you so much for inviting us. It's been very fun. Awesome. Thank you. Hope to speak
again. So peace. Bye. Thanks. Bye.

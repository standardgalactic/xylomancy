Hello and welcome. It is July 17th, 2024. We're in active inference guest stream, 83.1 with
Josh Bongard. Thank you for joining. This should be very exciting. We'll have a presentation
and then some discussion. So if you're in the live chat, please feel free to write any questions.
And thank you again, Josh. Looking forward to this.
Yeah. Thank you, Daniel. And thanks to those of you that are attending online.
So my name is Josh Bongard. I'm a professor of computer science here at the University of Vermont.
And my bread and butter in my lab is the study of robotics and AI. And obviously we're in the middle
of the current AI summer. So what I wanted to do today is show a couple of highlights from my group,
things that I've worked on in the past and that we're working on at the moment,
that I hope in the long term will help us realize sort of the long term vision for a lot of those
trying to create intelligent machines, which is to create machines that are helpful, but also safe.
We're part of the way there. But as anyone who's worked with chat GPT or stable diffusion knows,
or even has a robot vacuum cleaner at home, there are some limitations to our current technology.
It's hard to create machines that are autonomous and useful and safe all simultaneously. So
what are the things that we're missing? That's what I wanted to sort of
seed the pool with today. And hopefully we can move on to an interactive discussion about it.
So I'm going to leave this slide up and just sort of talk over this slide for a few minutes that
will hopefully generate some food for thought and then questions. This is a snapshot from
some of the projects I've worked on over the years. First thing you'll probably notice is there's
a lot of different robots that have very different structural properties. They not only act differently,
but look very different. And that is a fundamental foundation in everything we do in my research
group, which is to try and understand how the body facilitates cognition. Years ago with my
PhD advisor, we wrote a popular science book called How the Body Shapes the Way We Think.
And we can mean that literally or figuratively. We wrote that book a while ago. We made some
arguments about how the body shapes the way we think. And since that time, my group and others
have formulated other arguments for why or how the body shapes the way we think. And I'm hoping
to survey some of those today. So as I mentioned, you can see a lot of different robots here,
very different structure. They've got very different form and function. But across each of the pair
of videos that you see here, you'll notice that there's also a common pattern, which is on the
left side, you tend to see something that's virtual. And on the right side of the video pair,
you tend to see something that's physical. And this illustrates the basic approach that my group
takes to understanding how the body shapes the way we think, which is to create AI that creates
robots, creates embodied AI. So what do I mean by that? What I mean is that in all of the projects
that you see here, we create an AI that searches the space of all possible robots
that could solve the tasks that we want the robots to solve. Most people that are familiar with AI
and robotics and autonomous cars and drones have a rough understanding that AI is somehow
optimizing or tuning the brain of the autonomous car or the drone or the robot, what have you.
There's an underlying assumption in all of that current, in most current robotics, which is that
the AI tunes the brain but does not tune the body. Tesla cars are dreamed up mostly by humans and
an AI tunes their brain or their control policy. But of course, nature doesn't work that way.
Nature produced us and all the other intelligent organisms on this planet by carefully tuning
body and brain simultaneously. Certain bodies facilitate certain kinds of behaviors and certain
kinds of intelligent behavior and other bodies don't. They obstruct that particular behavior or
that intelligent behavior. So in all of our work, we ask the AI not just to tune the brain of a robot
but its body simultaneously. And as you can see visually here, the AI often comes up with bodies
that are well suited to whatever we want them to do. So if you direct your attention to the very
top left of the screen for a moment, this is now a 22-year-old experiment, but I think it still
visualizes the potential of this approach. In this case, we were interested in creating a robot
that can brachiate, that can swim, swing across beams or tree branches or electrical wires for
various inspection tasks. And you'll notice that in this case, the AI came up with a solution that
in retrospect seems intuitive. The robot has to carry a very heavy battery, which you can see in
the physical robot in the top left there, the black box that's at the bottom. And the AI has
figured out how to design the body of this robot so that it's actually able to exploit the forward
momentum of this heavy object, this battery, to facilitate its movement or brachiation across
this physical beam. So it's a simple example, a simple robot, a simple task, but it demonstrates
this interplay between AI, robotics, body, and brain. If the AI was not free to place the body,
to place the battery at a particular place on the robot's body, it would be much harder,
it would require more energy, it would require a more complex brain for the robot to figure out
how to move its heavy weight across this beam. So that idea of tuning body and brain has suffused
everything that we work on. Some other examples you can see in the top center and the top right.
Here we have a robot that suffers damage, its body changes over time. So now the AI has to grapple
with not just designing a body, but grappling with a body that changes. One of the things that we,
as intelligent organisms here in the world, and all embodied AI, all autonomous cars, all drones,
all robots, have to deal with is entropy. The world throws a lot of stuff at us. We have to deal
with wear and tear, injury. In our case, we grow from a single cell into about 10 to the 12 cells.
We change massively in terms of our physical magnitude. How do you continue to operate,
keep yourself alive, and do whatever it is you need to do across massive morphological change?
That is not an easy thing to do. And again, it requires an AI, if it's going to do this with
robots, it's got to figure out how to carefully tune body and brain to deal with the generation of
behavior inside a body that is changing, either unexpectedly due to injury and wear and tear,
or intentionally. You can see in the very center panel, this is again a pretty good visualization
of where designing body along with brain comes in handy. If we want to make flying machines
or swimming machines, we have to very, very carefully tune the geometry and material properties
of the body itself to realize flight. So what you're seeing in this middle panel here, this
is partway through the AI experimenting with the design of different kinds of wings for an
ornithopter, a drone that flies by flapping its wings. This flexible wing that you see here in
the center of the screen, this is a bit of a transition from traditional robots that are made
of rigid structures, like you can see in the top row, into a more modern era in robotics,
which is sometimes referred to as soft robotics. Material science has come a long way in the last
20 years, so we can now start to build robots embodied AI. We can start to build robots out
of materials other than rigid plastic and metals. And we can again, we can start to move into an
era in which robots, like organisms, can exploit the material properties of their bodies to facilitate
whatever behaviors they need to do to survive or be useful to humans, and so on. So in the
middle right panel, this is a highlight of some work that my group has done in collaboration
with Rebecca Kramer-Botiglio's lab at Yale. Rebecca's lab is famously advancing the state
of the art in soft robotics. What can you get robots to do when they're made out of soft
materials? You can see an example of some of those soft robots. Middle right and a very different
soft robot lower left, which is exploiting its body properties in order to move in interesting
ways. One of the interesting things about soft robotics, in my perspective, is that it starts
to usher in an era in which robots can actually grow and complexify their bodies. You can see these
hollow cubes in the middle right and these hollow sort of chambers in the bottom left
expanding and contracting as we push and pull air into and then out of the body of the robot.
Suddenly, now you have robots that can change their geometry, they can change their volume,
they become what's known as thermodynamically open. It's a fancy term for meaning that they can
draw new material and new energy into themselves. The thermodynamically open machines that you see
middle right and lower left, the only thing they're drawing into their body is more air,
but it's a start. I mentioned already that humans grow from a single cell into 10 to the 12 cells.
Every organism on this planet with a few exceptions starts small and gets bigger over its lifetime.
That fundamental morphological change starts small, starts simple, and gradually grow in
size and complexity. That provides scaffolding, it provides a gradient on which to learn how to
gradually grapple with the world around you. Most organisms, again, their exceptions, are not thrust
into this world with all of their machinery online from the beginning. Just the way I'm phrasing
this is obviously intentional to sort of dichotomize growing organisms and robots with fixed bodies.
Autonomous cars are still very dangerous. Autonomous drones are still very dangerous to be around
because 99.99% of the time they do the right thing, but every once in a while they don't know what to do
and no one knows what they're going to do in those uncertain circumstances. That is a very concerning
situation as we start to now deploy robots and autonomous vehicles into everyday environments
where they are in close proximity to humans. Why is it that even with state-of-the-art AI and with
all of Google's data centers and AI training algorithms, we still can't rub out that 0.001%
where no one knows what's going to happen? Part of the reason, again, is these machines are born
with complex bodies. We drop a controller into a one-ton autonomous car made of metal and plastics.
It's very dangerous. We don't grow autonomous cars from a very small, simple, light-weight machine that
can't cause anyone any harm whatsoever. And then when that simple, small machine demonstrates and
verifies to us that it's safe, then we allow it to become larger and more complex. It sounds
like silly sci-fi. Why would we build a machine like that? But again, every organism starts simple
and grows in complexity. And if it doesn't do the right thing, if it performs dangerous actions that
are harmful to itself or fatal to itself, by definition, it doesn't get any further.
That's again one of the ways of thinking about how the body shapes the way we think. In my
personal and professional opinion, any physical machines that we deploy into the real world,
they should start as very small, light-weight machines that can't harm anyone. They have a very
limited number of actions that they can perform, and they sort of cycle through all those actions
and verify everything. And only then can they take more mass, more energy into themselves,
can they recruit more material, they can sort of be allowed to be thermodynamically open,
and grow and complexify themselves. There's lots of ways in which we're starting to create
machines that grow and complexify themselves. I just talked about these soft robots that can
pull in air or pass possibly fluids. They could be hydrodynamic machines. They could mechanically
or magnetically connect to other robots. That's sort of swarm robotics. That's another path
to growing machines. At the moment, most of these machines are still restricted to academic labs.
They also are not safe yet, but I think in the long run, they're going to be a safer alternative to
dropping AI into very large, complex, heavy, dangerous machines and crossing our fingers and
hoping for the best. Okay, so I've talked a little bit about rigid robots and soft robots.
I want to try and talk as little as possible, so there's some time for a good discussion.
But I want to talk about what I see as sort of a third era of robotics and embodied AI,
which is just starting to open up in the last few years, which is biobotics or creating biobots.
You can see two biobots on display at the bottom center of my slide here. A biobot is a robot
that's made up of only biological components, no technological components whatsoever.
So in the bottom left here, Kreekman, Blackiston, Levin and myself in 2020 published a paper
demonstrating the first biobot. This became known after publication in the popular media as Xenobots.
Because these Xenobots are built from about 2,000 frog cells, and the cells were taken from a
particular species of frog, which is Xenopus lavis. Michael Levin, our biology colleague at
Tufts University, is world-renowned for demonstrating that you can reconfigure genetically unmodified
materials, like for example frog cells, and that rearrangement of living tissues not only does
not kill the organism, the organism is able to in some cases continue on doing what it does,
what it needs to do, ingest materials, survive, reproduce in this reconfigured state.
There's a lot of biological implications for that. One of the biological implications is that frog
DNA does not code for frog. What you're looking at in the bottom left, the Xenobot, is about a
millimeter in diameter, so it looks like a speck of pepper to the unmagnified eye,
and yet it's able to walk around the bottom of a petri dish. It doesn't have all the features of a
living organism, but it's got enough of them that it's motile. It's able to get itself from point A
to point B. One of the other implications for AI of this biological discovery that you could
rearrange genetically unmodified living tissues is that maybe we can task an AI with discovering
novel rearrangements of living tissue to produce robots, to produce something that moves around
and does something useful on a human's behalf. That's what I mean by a biobot, a biobot that's
made from, in this case, genetically unmodified cells. The swarm of Xenobots that you see in the
bottom right, as you can see, they're pushing around some material in their dish. This visually
hints at applications for this type of robotic technology, which is they might be able to act
like very, very small Roomba robot vacuum cleaners in the future. They might be able to collect
microplastics out of waterways or cancer cells out of bloodstreams. The swarm that you see that's
cleaning up in this slide at the bottom right, the material that they're cleaning up is actually
other frog cells. It turns out that if the AI designs this swarm just right and the swarm
that you're looking at, this is an AI designed swarm. The AI came up with the shape for each
member of the swarm. This swarm is pushing these little white circles, which are individual frog
cells, pushing them into piles. Turns out these individual frog skin cells at a certain stage
of development are sticky and they clump together into a pile. Some of these piles, if they're big
enough, if they contain enough frog cells, they will grow very small hairs on the surface cells,
the cells that are on the surface of the pile. Those little small hairs are called cilia. They're
usually used to pull dirt and pathogens off the body of frogs, adult frogs. Here, when those cilia
grow on small piles of frog cells, they're able to exert enough force against the surrounding water
that these piles start to move. What you have in essence is a child xenobot. This swarm
pushes cells together and in essence makes copies of themselves. Another implication of this work
is that in this case, the AI has figured out how to design robots that replicate. They make copies
of themselves by finding raw material in their environment and constructing copies of themselves.
This has been a long-standing dream in robotics, dating back a very long time to John von Neumann
in the 1950s, who had a thought experiment. It would be great if we could create robots that
would create copies of themselves, which would create copies of themselves. If those robots do
useful work for humans as a side effect, for von Neumann, that was creating moon bases and then
Mars bases and then colonizing the galaxy, which sounds great. The seed of this idea is if we want
robots to really be useful at scale, instead of manually constructing billions of robots and then
deploying them to do something useful, which is expensive, it would be much cheaper to make one
robot that does something useful for us. And by the way, it also makes two copies of itself,
which does more useful work for us and four and eight and sixteen and so on. We're not there yet
with the xenobots, but it's a demonstration that that is possible. And again, all of that becomes
possible because the AI is designing both the bodies and the brains of these robots. This is
very far now from the traditional view of AI and robotics, where we build a robot body, we humans
build a robot body, and then the AI tinkers with the brain of that fixed machine. So part of the
reason why I'm here today and part of the message of my group is we need to think more broadly about
how to combine AI and robotics and possibly synthetic biology. And when we do, when we think
more broadly, there are whole new paths that open up to ways in which we might create in the future,
not yet, but in the future, create intelligent, useful, and safe machines. In the current era in
AI, there is one particular approach, which is autocompletion of tokens, which has come to dominate
the fields and come to dominate the popular imagination. We all kind of have an understanding
more or less of what chat GPT is doing. And there are some very strong lobbying organizations out
there that are bent on convincing us that if we just do this with more compute, more data, we will
eventually get to safe machines. My contention is there just isn't enough data out there to make
non embodied AI, like chat GPT and stable diffusion and all the rest to make them safe.
We have to think differently about designing bodies and brains of machines simultaneously
to realize this long term goal. Okay, I've been talking for a while. I'm going to stop and I'm
happy to take questions or engage in some discussion and I'm happy to come back to any of these
experiments and provide more detail, if that's helpful. Over to you, Daniel.
Thank you. Wow. Awesome. What a cornucopia of bodies and minds.
It was a great overview. I was really struck by some of the similarities and the convergence on
whole of life cycle design and kind of holistic design coming from on one hand, a systems engineering
and a materials perspective. And on the other hand, from the biology perspective,
with like eco Evo Devo and this convergence upon needing to think about how the end to end
function, maybe even past the point of functionality, like into the plant, graceful decay of a robot as
well. So it brings in a lot of topics that even from an outsider's perspective seem to be put as
kind of secondary. So that's very cool. Okay, great. I'm looking forward to what people in the
live chat, right? My first question is how over these 22 years, how have the materials, the theories,
like the contexts, advances in Turing computation, all these kinds of things,
how have they intersected and just what has the ride been like as you pursued these questions?
Yeah, I think the short answer, again, is focusing on the physical aspect of AI and robotics. So the
materials from which we can build machines has changed over 20 years. And from my perspective,
the experiment, the top left there, that was something I did as part of my PhD,
you know, the materials at the time, it was very hard to build a robot. You bought some motors,
you bought a battery, you bought some metal, you bought some wires, and you wired everything up.
There was there was the assumption that bodies were fixed. And not only that, but they were
difficult to make. So once you made one, you were very careful with it to make sure it didn't change,
that it didn't become damaged. And that seemed to comport with a lot of the theory in AI and
neuroscience, which which had the same sort of idea that that the brain or in the case of robotics,
the control policy is the puppeteer. It's something that's pulling the strings of a fixed thing,
either the body of an organism or a robot. And if you look at a lot of a lot of theory in both
fields, AI and neuroscience, that that assumption runs so deep. So for example, an active interference
over, you know, the free energy principle, we want to reduce surprise all that there's a fixed set
of actions that we perform to try and reduce the surprise between what we're sensing and what we
predicted we would sense given the past action. Where do those actions come from? Why are they
fixed? Does the set of actions grow over time? Maybe the the sensory data that's coming is coming
from a new sensor that's just coming online or a sensor that's recently duplicated. Now there's
two of them, but they're not quite reporting exactly the same thing there. There's a whole bunch
of assumptions underlying a lot of the theory about active inference, predictive coding,
you name it, you pick your concept from neuroscience or cognitive science or AI.
Once you once you peel back those assumptions, imagine the robot's body changes. Imagine the
robot splits in two and becomes two copies of itself. You know, a lot of the the theory
and the formal the the formal underpinnings of that theory breakdown, you start to get into
ill posed questions which force you to now think about how do you fundamentally change the theory.
If you have a hierarchy of actions like in predictive coding or active inference,
what if that hierarchy is growing and changing over itself is growing and changing over time?
How do we address that in a formal manner? So to get back to your question, I think these advances
in what we can do physically. We can build robots now out of soft materials. We can build robots out
of living materials which on their own will grow and divide and seek out energy and material on
their own that those physical machines, these odd new kinds of creatures are militating. They're
pushing against the theory and specifically they're pushing against these unspoken assumptions that
lie underneath a lot of this theory about what's required to act intelligently in a complex world.
That's awesome. Like the real world and the territory expanding into our unknown unknown.
Okay, there's a bunch of questions in the live chat. So I'm just going to go for them,
give any answer that you like. Okay. David Williams wrote,
how do you think about the controllers in your robotics? Embedded AI at least today is rather
hard. Batteries and chips, PCBs, not soft and not easily synthesized locally. So how do you
think about the controllers in your robotics? Yeah, great question. So, right, exactly. The
controllers are dealing with hard rigid fixed components. We need to start thinking about
controllers that can, in which for example, the input layer and the output layer can grow and
shrink over time. There may be new sensors or new input coordinates that are growing or being
attached to a machine. And the controller needs to be able to carefully deal with those new input
channels while the machine is operating. Same thing goes for the output channel. There may be
new actions or new dimensions of action along which the machine can act. And control policies,
reinforcement learning, all the rest of it does all those assumptions that make reinforcement
learning work, which is what drives most autonomous vehicles at the moment, assumes that the dimensionality
of input and the dimensionality of output, the things that the machine can do and sense,
are fixed during training or during behavior generation, during execution. That is absolutely
not true in any organism on the planet. And that's becoming increasingly untrue for our coming
machines. Now, how to do it well? I don't have any answers, but we have to figure it out. You were
asking a question about thinking about controllers. That's a concrete example about how we have to
rethink control policy optimization, even if we're not thinking directly about the body,
even if we just focus on the control policy and ask what happens as the input and output channel,
the dimensionality of the input and output channels change during behavior execution.
Yeah, just one short point on that. It's like training with a fixed set of perceptual elements
or of affordances or actuators. It's like training on one point in a larger space of the adjacent
possible of bodies or of architectures. So then, okay, we're bringing all this compute
to train a special case in the fixed setting. And that's not even how the smallest organism works.
So that just again, shows that point. Okay. Sorry, before we move on from that point,
just to again, illustrate how the body shapes the way we think. In the case of a growing biological
body, there are new input channels that come online throughout our lifetime, but they don't
appear de novo. Whatever it is, whatever that new input channel is, as we're growing, we just have
more sensory cells. The signals that they're sending into the peripheral and central nervous
system are not orthogonal to whatever else is already coming in as input because new input
channels or new cells are slowly dividing. And at the moment of division, they're providing
exactly the same signal as some other sensory channel that already exists. So the body, or in
this case, biological growth provides an immediate scaffold, a gradient. In robotics, it can be very
scary to think about like attaching a sensor to an autonomous vehicle. What the hell does it do with
this new information that's coming in? Because we haven't thought carefully about how to add that
new sense modality to the machine. Again, we have to look to nature that every new sense modality
is gradually coming online and gradually drifts away or becomes increasingly orthogonal to
the starting input modality. So that's how we should, if we did that physically with machines,
it would simplify reinforcement learning or would make it easier for reinforcement learning or what
have you. Sorry, let me reshare my screen here. It would simply make it easier for the,
sorry, something seems to have gone wrong here. Give me a moment.
Yep. Okay, all right. Yeah, it makes things easier on the control policy optimization process
if new sense organs and new motor outputs are coming online, but they resemble things that already exist.
That's super interesting. Brings up a lot of questions about like self and non self recognition
and what is a self as new and different senses and actions come online?
Sure. Okay. Prakash Kavi asks,
do these bio bots have any sense of agency? What is your sense? I'm quite intrigued by the idea
that beyond a critical point, they start growing hair. And do these bio bots act independently
of each other? And also what happens at a group level? So what's your sense of agency in bio bots?
And I guess the bio bot and the group level? Yeah, it's a great question. So I'll start with
the disclaimer. I'm not a biologist. I'm a computer scientist by formal training. So I can only say
so much about what the cells are doing and what they want to do. I definitely follow in the
footsteps of the late Daniel Dennett in that when we talk about agency, each of us individually has
to decide whether or not we take the intentional stance or not. It's, in my opinion, it's a point
of view. If it's easier to explain what the Xenobots are doing by talking about what the
cells want to do, like grow cilia and coordinate their actions, fine. If it's easier to explain
what the Xenobots are doing by not taking the intentional stance and describing cells as mechanical
components that are transforming input into actions, that's fine too. This is something also
that comes from my colleague Mike Levin. It depends. As scientists, if we want to try and
explain and understand what these machines are doing, if taking the intentional stance makes
explanation easier, fine. If not, then not. But attributing agency is sort of an objective
property of the bots or the cells themselves. Independent of us is observers. To me, that's
philosophically and practically problematic. As far as I know, there is no objective measure
of agency in cells, let alone inorganic robots.
Super interesting. That's like the second order cybernetics or the observer theory
or the poly computing question, which is to say just looking at something and then treating
one's perspective as objectively. The case, it is objectively the subjective experience.
Absolutely. Now, that being said, again, there is an empirical side to this. We can make some
progress in understanding the Xenobots by comparing them against a control. Instead of cells, if these
were magnets or some complex mechanical system in which more of us are comfortable in saying
there is no agency, it's just a bucket of cogs doing something. That control does not exhibit
kinematic self-replication, for example, or it's much harder for the AI to figure out how to put
together non-agential components to do what it is, then I feel a little bit more comfortable
by saying the cells are doing something more. Now, I don't know whether it's agential or they want
to do something or if it's free will or consciousness. I don't know. But if we can point at biobots or
machines that are built from biological components and say it's easier to get them to do things,
because they become complicit in the overall goal compared to mechanical parts, which don't,
okay. Again, as a roboticist in the top and the middle rows that you see on my slide here,
when we do build things out of metal and rubber and plastics and ceramics,
it's usually super hard. It's really hard to get them to do whatever we want them to do.
We've been working on robotics since the end of the Second World War, and we've got the Roomba,
and maybe we've got autonomous cars. We're getting there. It's taken a really long time,
because robotics is really hard. It's really hard to convince physical materials to adapt
and do something useful and safe. On the flip side, we've been working on Xenobots at the bottom here.
We've been working on them for about five or six years, and we've got Roombas. We're making
faster progress in robotics when we build from cells than when we build from metal
suggests the cells are somehow helping. I don't know that they want to help. We've got to be
careful there. That's the intentional stance, but when you try and compose machines from smart
machines and cells are smart machines, I know I'm biased, but I think we're making faster progress
than when we build machines out of inert materials.
Yeah, super interesting. Okay, I'll read some comments from David Clement.
David wrote, does your work incorporate a gentile hierarchies? For example, the Xenobots grow by
replicating the initial seed cell into a higher order system, and is it critical for lower order
systems to act as a component of a virtual machine, meaning that they have a target behavior that is
less than the higher order system? That's related to Prakash's question as well. How do you bridge
that from the individual component into the swarm or the aggregate?
Yeah, it's a really good question. When we started working on the Xenobots and Mike Levin
started to talk about machines made of machines made of machines, that definitely has influenced
the work in my group to focus on this issue of hierarchy. I don't know about a gentile hierarchy.
Again, we just talked about a gentile agency. That's maybe a subjective stance, but we but
definitely, you know, why would you want to build machines out of machines out of machines?
At the moment, our state-of-the-art robots like autonomous vehicles are not hierarchical.
The control policy operates at the level of the machine as a whole. For example,
if there's an emergency blowout of the tire in an autonomous vehicle, the tire itself,
the rubber that makes up the tire, doesn't deform and try and fix or reduce surprise all
locally. It can't. It's rubber. It's inert material. We don't have machines built of machines
built of machines yet, but as biology in general and the Xenobots in particular demonstrate,
there's an adaptive advantage to being a hierarchy of physical things, of physical machines.
If there is a surprising event at the level of the machine as a whole,
but that surprise trickles down through the hierarchy, it's unlikely that everyone at every
level of the hierarchy is going to be surprised. Someone somewhere in the hierarchy is going to
say, from my local view at least on this bigger surprising issue, I know what to do. So let me
start to communicate to my peers and up the hierarchy to deal with surprise. That would be,
from an engineering point of view, that would be a good thing to have in big,
heavy, fast-moving robots that are near humans. There's always going to be some surprising event
that the vehicle as a whole has never seen before. There's great YouTube videos of horrifyingly
scary, surprising edge cases for autonomous vehicles. Okay, we're never going to fix every
edge case. What we can fix is to make hierarchies, and maybe agential hierarchies, where local
surprise can be handled, or global surprise can be broken down into local surprising events,
which can be handled locally. If I understood the second part of your question is,
how do we design that hierarchy? Should the smaller parts be trying to pull in the same
direction or be trying to solve some part of the goal of the higher level? I think that's a super
interesting question, and I don't think that the answer is obvious. It may be that smaller parts
pursuing orthogonal goals may end up being useful. Just to give you a quick example,
if you want, if there's a surprising event and you've got a whole bunch of semi-independent
machines organized in a hierarchy, I would argue that every single one of those members of the
hierarchy should have a slightly different body and brain. It should have a slightly different
form and function. You don't want a monoculture. You don't want all the parts being smaller versions
of the bigger parts, and trying to achieve smaller versions of the same goals, because then you've
basically got a committee in which everybody thinks and feels the same way. As we know from humans,
that's a dangerous thing. You get group think or group act. You actually want a heteroculture.
You want a whole bunch of things that are unique in terms of form and function,
and that maximizes your chances that someone somewhere in the hierarchy says,
just because of the way I'm built and the way I think with my local control policy,
I know what's going on, and I have the seed of a solution. Here's the seed. You all figure out
what you need to do to make it a reality at the larger level. That's another aspect of where the
body comes into play. Yeah, thank you. Like, everywhere is the last mile from somewhere.
Things have to be addressed locally. No matter how you think about a communications architecture,
distally, everything and embodiment calls our attention back to that. It has to be somewhere
locally, so then why not take that as the starting point instead of this resource challenge,
and then about the multiple subunits when there's a damage to the nest of an ant colony,
or there's some things spilled on the surface. It's not that every single nest makes a perfect
pebble move. It's that 51% accuracy with a bunch of nonspecific flurrying of activity,
just like kind of stress or these more generic higher order signaling, that is what allows
nest mates with different brains and bodies to fulfill their own paths of least action,
and then colonies for which that doesn't clean up the mess, or it cleans up too well,
and there's externalities, those colonies are swept off a table, and then we see the
persistence of collective systems that could figure that out in their growth from a little
colony to a big colony. Absolutely, great, great example. I had a question you mentioned, both
safety as well as like reliability, and how do you think about capacities and evaluations
on diverse intelligences? We're all familiar with RAM, CPU, hard drive storage, some of the
von Neumann type architectural descriptors. However, how do we even think about what does that
rubric or report card even start to look like when we know that there's complex interactions
with the niche, and when the kinds of capacities that we're talking about may have even open-endedness?
Yeah, great point, great point. We are the beneficiaries of two big revolutions,
one of them is the AI revolution, but then the older one is the digital electronics revolution.
Digital electronics works, we all have a supercomputer in our pocket, there's no arguing
with it. It's an incredibly powerful way to make machines that internally communicate quickly and
richly and then can communicate with other machines. That's it, that's the information age
that we're in. It's been so successful that it's hard to think about alternatives or why we would
even bother thinking about alternatives. But again, living systems, a lot of what cells do,
they rely on electrical communication, but they also rely on mechanical communication,
chemical communication, thermal communication. Cells are using all physical modalities,
not all physical modalities, as many as they can get their hands on simultaneously all the time.
Why? Why don't they just abandon everything and do everything purely electrically,
like our modern civilization has done? Because it's dangerous, you don't have a diversified
portfolio. One panel here that I haven't talked about is the one in the bottom right,
which is basically just what you're looking at is what's called a granular material. It's a material
that's made up of a bunch of grains. In this case, the blue circles that you see, these are a little
just rubber pucks, and there is an oscillation being supplied at the left-hand side. You can see
that this leads to interesting non-linear vibrational behavior within this material.
What does that have to do with robotics or AI? If you view the vibrations as the carrier of
information, so if a puck is vibrating, that's a one. If the puck is not vibrating, that's a zero.
Now you can start to imagine creating materials that communicate Shannon information
throughout the physical structure, not with electricity, but with a different modality,
dynamics or vibration. It turns out that you can actually compose these metamaterials to embody
logic gates. If you vibrate one particle or another particle, but not both and not neither,
you can watch a third particle and it will either vibrate or not in accordance with an
exclusive OR gate, and you can build this up. Now again, why would you do that? We can make an
XOR gate that's vanishingly small and vanishingly fast in digital electronics. Why would you ever
want to do something different? Because it turns out there are advantages of communicating with
vibration rather than electricity under certain conditions. Having a machine that can communicate
between distant parts of its body through mechanical vibration, as well as electricity,
has an advantage over a machine that can only communicate long distances within its body
electrically. I won't go into the reasons, but you can intuitively start to understand that.
Again, I think we need to, if we're serious about creating safe and useful autonomous
machines, we have to break out of the digital electronics assumption that that's the only
way to do things. We have to break out of the assumption that non-embodied cognition is the
way to go and it's easy to just drop it into a physical body and we're good to go. We have to
appeal back some of these very deep assumptions about the right way to do things that have built
up in our society since the Second World War because a lot of those technologies have been
very successful. Nothing wrong with them, but when we come to apply them to creating safe and
useful machines, not always the right thing or the only way to approach things.
That's really interesting. It's like a sort of generalized compute concept where we could talk
about, well, these are the chemicals that it can detect with this fidelity and here's its tactile
interface. Here's its electromagnetic capacity for sending and receiving. That's what kind of
motivates or complements the generic theory like free energy principle, which doesn't tell us about
how anything is in particular, but then sets us up with the framework to plug in these different
modules and then it's an empirical question and then right here's the virtual body and a real
body. That's also very interesting. How does that work in a collaboration or with a graduate
student? How do you balance this digital adjacent possible off of the material and the more costly
implementation with embodiment? Yeah, well, with grad students and postdocs or whoever I'm
collaborating with that's starting out, this can be a very frightening prospect for someone who's
trying to get into AI and robotics because it looks like everything's been done, it's solved.
We just have to wait for Google and Microsoft to buy more compute and data and they're going
to finish off the last 1% of the dangerous behavior. If you're trying to contribute to
society's goal of making useful autonomous safe machines as starting out, what do you do? It looks
like this massive brick wall, there's no entry point. My take on this is again is that we may be
going about this all wrong. The assumption that electricity should be the carrier of information
inside an autonomous machine, that's an assumption. Why electricity? Why not vibration? Why not
something else? Even if you start to think about the alternatives, the immediate reaction as well,
it's not going to be as good. Maybe, maybe. But if you think about vibration, you were just
mentioning compute. We can use vibration for compute, but vibration is movement. The minute
you start to think about vibration as the carrier of Shannon information, you are now conflating
action with computation. They cannot be separated. Descartes convinced the West 400 years ago that
they're separate. They just are. You look at AI and robotics, what a surprise. These two are
attempts to create AGI is bicameral. There's one team that says it's going to happen in computers
and the other side that says it's going to happen in physical machines. That's the Cartesian legacy,
that they're separate. But the minute you look at some very humble material like the one in the
bottom right, it's 12 hockey pucks next to one another, there's no Cartesian division anymore
between body and brain. There is a body and there is a brain there, but it looks very different
from anything we would usually consider. And there's no value judgment here. It's not better
or worse, maybe it is depending on what your metric is. It's just very different. And so with
grad students and postdocs, I encourage them to pursue that. Could we do things completely
differently and in the long run, might that be a better way to do things? Who knows? We'll see.
Awesome. I'll make one comment and then ask a last question. You brought up Descartes and that's
the res extensa, res cognitant dualism between the thinking and the non-thinking substance
and embodied cognition, embodied intelligence provides both an operational instrumental
and an ontological counterarguments or complementary perspective, which is just, well,
in practice and in actuality, take it or leave it, they are inseparable. And so at the very least,
that that starts to ratchet and leapfrog the discussion about what is mind and body. And
you started with pointing out how important it was to co-evolve and the complementarity of mind
and body. And it's like, they're two separate things that need to be complementary and tangoing
and also maybe they're integrated and blurred in even deeper ways than the dance. So that's like,
it's an empirical entry point into what otherwise is a thought experiment, which can have utility,
but also can be just arbitrarily misleading. Absolutely. One of my former mentors,
Inman Harvey at the University of Sussex, used to talk about robotics as philosophy with a screwdriver.
It's not just armchair philosophy, it's when you start to build some of these machines,
maybe in retrospect, in the case of the Metamaterials project, for me in retrospect,
I said, oh my God, most action and cognition are not complementary, separate things that are
complementary, they're one in the same thing. It's not embodied cognition, it's not an adjective
of a noun, its embodiment is cognition. There are not two things here, there's just one thing.
Very hard to think about, it's so alien to a Western mind, but it just is.
Sometimes I think about that in terms of like adjectives getting added in front of a word,
and then the term expanding, and then it just encompassing, oh, of course, cognition is ecological,
embodied, enacted, et cetera, et cetera, et cetera, et cetera. It needs to be distinguished,
and then it subsumes again, and that's part of it. In closing, what are you excited about?
Where can people continue to learn more, or what would you say to a person who's wanting to go in
this area? Yeah, okay, great question. Google my name and it'll take you into lectures and papers
and tutorials, and if folks want to email me, that's perfectly fine. Again, Google my name,
you can find my email, happy to put you in touch with the right people. I think it's easier than
ever to get started. You can go to ChatGPT and say, create some tutorials for me to start
coding up robots. Ironically, non-embodied AI provides a good on-ramp now, not just for
reading about these ideas or listening to people talk about these ideas. You can start coding them
up in a way that's easier than ever. The old days, you had to learn C and then go on from there.
Very easy to get your hands dirty, maybe not with physical materials, but you can create,
like you see in the left of each of these panels, you can create machines that are virtual.
They're not physical, but they're embodied. It's another point that's important to make.
Embodiment does not imply physicality. You need to be able to push against the world and observe
how the world pushes back, but the world and you may be virtual, like you see on the left,
or physical, like you see on the right. So you can actually relatively quickly get your hands
dirty with playing around with embodied AI these days. And I encourage everyone to do so.
Cool. Any last comments?
I would just say I was at the Computer Vision and Pattern Recognition Conference, CVPR,
a few weeks back. This is one of the flagship AI conferences, 15,000 attendees.
And after my talk, a lot of grad students came up and they said, listen, you sort of demonstrated
there's another path here that I was feeling depressed or anxious about how to make progress in AI
when these goliaths have these data centers at their beck and call. I would just encourage
everyone that when you think differently about all this stuff, there are new paths that open up.
They may not in the long run be the right path, but there are alternatives to this monolithic,
predict-the-next-token idea, which is currently in vogue. It may be the beginning of the end,
but I think this is just the end of the beginning. We've figured a few things out. There are some
things that work, but they're still producing not quite useful and definitely dangerous machines.
There is room for improvement and there's nothing that says that only Google with all its resources
is going to be the one that can figure out these improvements. Think differently,
try some of these alternative approaches, and maybe you will be the one that comes up with
the answer, whatever it is. Good luck. Awesome. Thank you, Josh. Really appreciate it.
Thanks for having me. Yeah, and until next time. Thank you. Bye.
You

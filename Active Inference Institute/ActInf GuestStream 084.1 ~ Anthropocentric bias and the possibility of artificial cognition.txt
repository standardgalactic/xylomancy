Hello and welcome everyone. This is Active Inference Gas Stream 84.1 on July 22nd, 2024
on Anthropocentric Bias and the Possibility of Artificial Cognition with Rafael Maguerre
and Charles Rathkoff. So, Rafael and Charles, thank you very much, both for joining to you
for introductions and to take us through the paper.
Oh, and welcome everyone. This is Active Inference Gas Stream 84.1 on July 22nd, 2024.
Thank you. Okay. Thank you guys. Go for it.
Hi. Thanks for having us. So, I'm Rafael. I'm an assistant professor at Macquarie University in Australia, Sydney.
And Charles?
I'm Charles Rathkoff. I'm a prominent research associate at the Yulish Research Centre in Yulish, Germany
in a big neuroscience institute.
So, should we go through the paper briefly? So, yeah, we wrote this paper together.
Actually, we started by writing a general audience piece that was published in the box online.
When was that? A few months ago, I guess?
Yeah, I think almost, yeah, close to a year ago, maybe.
I don't think it was published a year ago. I think it was published in 2024.
But yeah, we worked on it for a while. Yeah, so this piece was doing, I guess, two things.
That piece that we published in the box, it was pushing back against what we call the all or nothing principle,
which we defined as the idea that either something has mine or it doesn't.
So, this kind of neat but overly simplistic, perhaps, partition of things into minded and non-minded things.
And we argued that this was not the best framing to think of, especially, and family, our systems
that seem to have sophisticated behavioural capacities, like large language models or AAC systems in general,
where we don't want to take bias-community capacities of the package and package them into this idea of a mind,
where either you have the mind or you have them, then if you have the mind, you have all of these things
as a package, consciousness, reasoning, etc., planning, memory, theory of minds.
So, as a remedy to this kind of all or nothing approach, we argued for what we call the divide and conquer strategy
when studying the cognitive capacities of these systems, which involved looking at these capacities on a piecemeal basis,
this case-by-case, with an open-minded empirical approach.
Yeah, Charles, I don't know if you have anything else that's down the box. Peace or vibrance?
Yeah, I mean, we made one point in there about why it is that people feel so torn about reactions to large language models.
And we said a little bit about the psychology of essentialism, which is the idea that we naturally categorize,
especially living things with respect to a presumed essence.
So we gave an example of an oak tree, I think, and we said that what people tend to think as they grow up and learn about the natural world
is that an oak tree remains an oak tree regardless of changes to its observable properties.
And what makes an oak tree is this unobservable essence of oakness or whatever it presumably has.
And there's some experimental psychology and developmental psychology showing that we have a similar attitude towards mindedness or having a mind.
And that is a somewhat speculative explanation for why the literature on large language models is so torn,
and some people are quite dismissive and other people think that it's a step away from AGI.
It's that if you feel like you've got to put large language models into one of two boxes,
the box that has the essence of mindedness for the box that lacks it, then you will be forced either to say
it doesn't have what it takes to do any of the things that we associate with having a mind, such as reasoning,
or it has the essential characteristics of mindedness and therefore we should expect it to have all of the other properties we associate with mindedness as well,
such as consciousness or understanding or whatever.
I think it's worth emphasizing as you did that the background motivation for starting to write on this general piece in the first place
is indeed that the general discourse on AI systems and LLMs in particular is extremely polarized in a way that is very dichotomous and stark.
So you have one-on-one ahead people arguing that the systems are no more than so-called stochastic parrots
that are haphazardly stitching together samples from the training data and regurgitating them,
or that they are no smarter than toaster, or that they only do next-talking, prediction-on-export prediction,
and therefore it is a non-starter to ascribe to them any form of cognitive capacity or maybe even a category of mistake.
And then on the other end of the spectrum, on the other end of the spectrum, you have people arguing that the systems are harbingers of superhuman indulgence,
that they exhibit sparks of artificial general indulgence to the title of a paper by Microsoft on 24.
And many people hyping up the capacity of the systems in a way that might seem very speculative and untethered from actual empirical results.
So there is this huge gap between these two positions and there's going to be a very rich and complex and nuanced middle ground that is underexplored.
Probably, perhaps, I think we did make that point, if not explicitly, in the published piece and in some draft,
that there's something reassuring about being able to make definitive claims about what these systems are and what they do, right?
So either they're re-unsophisticated or they're really very much like us, and either of these claims kind of meet somewhere in an in-way
in saying that we have a clear idea of what the systems do and what they are.
And I think it's a little more epistemic and comfortable to say we have to study them empirically and find out what they can or cannot do and why and what are the underlying mechanisms.
And we simply don't know a priori just by looking at the architecture, the learning objective, the training data.
These sources of evidence are insufficient to make the definitive claims about what the systems are capable of.
So I think that's, that was part of a big part of the motivation and that fits into that more academic paper as well.
Yeah, one other small side note, which we don't make in the paper, but I think might be relevant, especially for people working in philosophy,
LOMs are epistemically uncomfortable. I think that was the phrase you just used, Raf, which is fitting, not only because they're so new and different,
but also because they are artifacts, right? They're things that humans have constructed and engineered and we don't have a thorough understanding of how they work.
I mean, mechanistic interpretability and various behavioral research is helping us improve our understanding, but on the whole, our understanding is not nearly as deep as hopefully it one day will be.
And this is by itself a really strange situation to be in that we've constructed an artifact that we only partially understand.
In the, in the past, artificial intelligence was seen as a way of constructing something like an epistemic assistant, right?
Something that will help us, but not something that will kind of alienate us from the process of coming to know about the world.
So I think there's an extra layer of discomfort built into thinking about large language models and that may also play into the divisiveness of debates about what they can do.
And just to add to that, I guess we should be clear that this does not entail in any way that we think the systems are so completely alien and beyond the reach of our current understanding that anything goes and that they could very well be,
you know, have like human-like intelligence or superhuman intelligence and we simply cannot say whether or not they do or because that's sometimes what you see in some outfits where people frame these systems as noble alien forms of intelligence that we have created but do not understand our control.
You know, it's a slippery slope that leads some people to then claim that they have all these quite magical abilities and that's not all what we want to say here.
And in fact, we want to resist.
Yeah.
Yeah.
Yeah.
So we think that that's just as much of a cup out as completely dismissing a priori what these systems might be capable of without doing the work of looking into the capacities with behavioral and mechanistic studies.
So we very much want to resist both extremes of the spectrum, if that makes sense.
Okay.
So now should we move towards the content of the current paper?
Sounds good.
Yeah.
Okay.
Maybe Raphael, I'll just start with the distinction between anthropomorphism and anthropocentrism and then you can take the next step.
So everyone is aware of the problem of anthropomorphic bias in some form.
I mean anthropomorphism is just the idea of projecting human qualities onto something nonhuman.
And it's quite easy to especially when you're having a productive successful exchange with a large language model.
It's easy to slip into this interpretive mode where you reason about the responses of the large language model as if they were coming from an agent just like you.
And maybe that's a useful thing to do in some circumstances, but from a theoretical perspective, it's certainly a mistake because a large language model is radically unlike a human agent in all sorts of ways.
But that's only one form of sort of human centric bias.
The other one is anthropocentrism or what we call in that box article anthropocentric chauvinism.
And that idea is pretty straightforward.
It's the idea that the human way of solving problems is the gold standard of solving problems generally so that to the extent that a system solves a problem in a way that diverges from the human strategy.
It's only using a trick or a foe solution.
It's not using a deep general rational strategy.
And in the debate about what large language models can do, we think that the anthropomorphic bias is pretty well recognized and the anthropocentric bias is not so well recognized.
And so part of this paper is or the main idea behind the paper is to present a systematic analysis of anthropocentric bias, how it comes about and how to push back against it.
Right. And we want to be very clear and hopefully we're playing the paper that the reason why we focus on anthropocentric bias here is just because it is, I think, as Charles mentioned, less discussed and less recognized or some forms of it are less recognized.
We propose this new taxonomy, but it's not at all to suggest that it's more problematic or more important than the anthropomorphic bias that's well discussed in the literature of the anthropomorphic biases.
So in other words, this is not to frame things in this slightly problematic dichotomous way of thinking that's common in the discourse on LLM.
This is not a paper that is pioneering to the LLM booster or LLM hype camp, even though it is pushing back against a certain form of dismissal of anthropocentric biases that only exclusively emphasizes the anthropomorphic biases.
But perhaps we should flesh this out a little bit already with the first one we make here in the paper about the performance competence distinction, which is a nice way to bring about both anthropomorphism and anthropocentrism regarding LLM.
So this distinction is a very classic distinction in linguistics and cognitive science, and it has already been applied to AI systems in the neural networks productively, like Charles Farson.
So there's nothing new here, but the distinction comes from Noam Chomsky originally and the idea is that performance pertains to the external behavior of a system in a particular domain and competence is the kind of setup underlying knowledge and computations or mechanisms that enable the system to achieve that behavior.
And a familiar observation in linguistics and cognitive sciences that there is a double distribution between performance and competence.
So if you take, for example, language, I might, during this very podcast, make some grammatical mistakes or some other mistakes.
In fact, I've already misspoke in a few times, I think, and repeated myself.
So I made performance errors, but this does not entail necessarily, hopefully, that I'm an incompetent language user and that I don't have the underlying linguistic competence.
So that's a well-recognized dissociation, you can be competent and yet make some errors.
And the reason for that is that there might be some additional factors that are unrelated to the underlying competence that might impede on my performance.
So for example, I might be distracted when I speak or there might be other effects on my speaking performance that don't actually originate from a lack of competence,
but just impede on the idealized expression, external manifestation of my competence.
And this is why I misspeak.
But it's also well-recognized that you can have good performance without competence.
So we do hear the example of a student cheating on a test or memorizing test answers by brute forcing the test into slightly more, I guess, gray area, but at least in the cheating case.
A student can ace a test without being competent at what the test is actually testing for.
And it's also well-acknowledged in cognitive science that there can be instances like this throughout, you know, that can be many tests in certain experimental settings where the test subject is right for the wrong reasons, as it were.
Namely, it's doing well, it's exhibiting good performance, and I realize the underlying reason for good performance is that there was some, perhaps some curiosity that the experimenters of the scientists hadn't thought about that could account for this good performance,
but doesn't actually amount to whatever competence they were setting out to test.
So we start by saying, well, this is well-recognized.
There's no mistakes.
There's this devolved association that is like supplied to humans across the board.
You can have performance without competence, good performance without competence, and you can have bad performance despite competence.
Now, when it comes to LLMs, the point we make, I'm just going to scroll as we go, we have some figures to show later.
The point we make is that generally, people stress the dissociation, apply the distinction to LLMs, but stress dissociation only in one direction, unlike in the case of humans where it's bi-directional.
And so what people do generally is they say, well, LLMs famously, you know, if you look at the GPT-4 technical report and vice-other, any report about a new state-of-the-art LLM,
they are getting really, really good at a number of tests and about benchmarks and even human examinations, human exams, the bar exam, medical exams, et cetera.
So they can get a really good performance test that we tend to think are really difficult tests that one can only pass, at least a human could only pass,
if they have a really significant nonchalant amount of competence in particular the main.
And the point that is often made when it comes to LLMs is be careful, slow it down, and try to find out why the model is doing well on that test,
because there are various reasons why it could do well that do not actually indicate that the model has the underlying competence that the test was designed for when it comes to humans.
So one big concern, for example, is data contamination, where very large language models train on internet-scale data can easily be trained on some test items from common benchmarks that link into the training data,
such that they can then do really well on the benchmark just because they've essentially memorized test items.
And there are other more subtle reasons why performance could be very good for the wrong reasons.
So that's very well recognized and a lot of the people who push back against anthropomorphic bias when it gets to LLM make that point.
Be careful, do not take an anthropomorphic attitude to the systems.
The reason why they do well is not because they have human-like intelligence or human-like cognitive capacities, but it's for this trivial contingent or otherwise irrelevant reasons that account for the group performance.
Now, when it comes to the other association of the other direction, their people are very reluctant to apply to LLMs.
And we think it's because essentially people think in a human case, you can make sense of the idea that the human could do badly on a test or could perform badly on a task
and yet have the competence that you're trying to test, but there might be some auxiliary factors such as working memory limitations, attention deficits, etc.
that could impede on the performance.
But in the language model, I think what we argue in the paper, what I argue in the longer version of the paper, is that a lot of people don't think that there is an analogous mechanism at play
where there could be some kind of auxiliary factor that impedes on performance.
So performance is what you get, what you see is what you get.
And so the performance is a direct manifestation of what the system is computing.
And if you have performance errors, that can only be explained by the lack of competence because there is no additional independent factor or module that could impede on the performance.
That's the source of interference, you might say.
Yeah.
So yeah, I mean, I'll pass it over to you, Charles, I just wanted to set up this distinction.
Yeah.
Yeah.
Right.
So I mean, if you think about a traditional computer program, at least if you think about a simple computer program,
it's odd to think of it as some sort of complex system where one part of it could sort of interfere with the workings of another part of it.
But one of the points we want to make is that something like that is a realistic possibility with large language models.
Okay, but I suppose the next part of the paper goes into a taxonomy of anthropocentric bias and the first sort of overarching point is the distinction between type one and type two.
So the type one anthropocentrism is a tendency to assume an LMS performance failures designed to measure competence always indicates that it lacks that competence.
Before we, so we'll say something about three different kinds of type one anthropocentric bias, but first a background point, which is that whenever we think it's possible to give a mechanistic explanation of some complicated phenomenon,
we always have to foreground some factors, some variables, and background others.
And the properties that we push into the background.
We're still making assumptions about the nature of those properties. When we try to articulate what's going on with the other properties that we're paying more attention to.
And if assumptions about those properties in the background turn out to be wrong, then those mistakes will corrupt our explanation that attends only to the foreground and factor so that's a little bit abstract.
This is a very simple example.
In comparative cognition, one famous behavioral experiment is the mirror test for self recognition.
So self recognition is roughly do non human animals have something like a concept of self.
And the strategy is to put some sort of mark on their body in original experiments it was a red dot on the forehead of a monkey and or a bird or whatever.
And then you put that animal in front of a mirror and see if it makes any attempt to get rid of the mark.
And if it does make an attempt to get rid of the mark that shows that it recognizes that the image in the mirror is an image of itself, and otherwise not.
That's a cool way to get a really difficult and abstract question about the mind of a non human animal, but it presumes or it assumes that animals will care about the fact that they have a red dot on their forehead that they will be bothered by that and be motivated to get rid of it.
And if that assumption is wrong, then they might fail the mirror test for self recognition for reasons that have little to do with the presence or absence of a capacity for self recognition.
Until something similar to that is going on, we say in large language models.
So, the first example that we give is to do with task demands.
So, you can, it's a pretty natural idea that whenever you set up a behavioral task, there will be demands associated with that task that are not directly related to the capacity that you're trying to test.
So, to take the most obvious example that I can think of, if you give someone a written test, they have to be able to write. They have to, you know, have a hand and a pen and whatever.
And if their, you know, hand was injured or whatever and they couldn't write, then their failure to fill out the test wouldn't tell you anything about their, you know, academic knowledge.
So, we suggest that there are auxiliary task demands in behavioral tests of a large language model, and they're subtle. You wouldn't, you wouldn't think of this right away, but we talk about a paper from who and Frank.
I have a couple of papers on this topic, but what they do is they give a large language model, the following sort of question.
This is a question, it's for grammaticality judgment. So, you can see on the image there which sentence is better in English. Number one, every child has studied.
Number two, every child have studied answer with one or two and it gives the wrong output.
But then you can also simply look at the probabilities assigned to each of those sentences within the model and figure out directly whether the model thinks that input A is more likely than input B.
And it turns out that on a wide variety of questions of this kind, the direct comparison does or the large language models perform better with the direct comparison than they do with the more complex demand for
metalinguistic judgment. So, the fact that the model has to process the numbering of the options and then answer in terms of a number is a subtle but nevertheless
important additional variable in the experiment, and that can influence the model's capacity to get the answer right.
Raphael, do you want to add to that?
No, I think that, well, maybe we can mention briefly the other example we discussed, which is from this paper by Andrew Lampinen, which kind of has a little extra ingredient that makes the example interesting and even more problematic in terms of comparative psychology,
which is one way in which auxiliary task demands can be ignored or disregarded or overlooked, is when you are doing a direct comparison between humans and others on a task and the experimental conditions are mismatched in such a way that the task, as you set up,
imposes stronger demands, stronger task auxiliary demands on the OEM than those on human subjects. And that's something that can happen quite often. And so there is this interesting example from a couple of papers originally published by
black reds and colleagues from Standard Ends Group, where they looked at the ability of language models to handle recursion, looking at center ambitic closes, how such closes might, for example, when you had a prepositional phrase within the subject of a sentence and the verb might throw up
either humans or LMS into agreeing the verb in the wrong way, so giving the wrong number to the verb. For example, the keys that the man put on the table here, it should be R because keys is plural, but because you have
closed in the middle, and if you add more closes like this that are embedded in the middle, people and others can get confused and
predict that the verb should be ease, for example. So they tested this on humans and others and found that on the more complex examples involving complex, more complex constructions or recursion, humans were doing decently well, but LMS performance was collapsing compared to the simpler examples.
And I, from an opinion from DeepMind, looked into that and realized that the experimental conditions were mismatched. So the humans, as is very common in cognitive science experiments, were getting some training before they completed the test items to just get familiarized with the task.
So then they were given some examples of the task for each condition. And the LMS were just prompted zero shots as people usually put it. So just without any example, just point blank. And Andrew found out that if you, he replicated the experiment, but I did some proper matched testing conditions for the LMS.
So adding some examples of the task in the prompts, what's known as future prompting. And with that, he found that performance was equivalent. In fact, the LMS that he tested was slightly better on the more complex constructions than humans.
So when you match the test conditions here, you actually match also, it's not automatic, but you can match the test demands. I mean, it could be that there are reasons why the various experimental conditions would result in different demands for humans and others.
But you're still, in this case, even on the playing field, in such a way that you don't find the behavioral discrepancy that you found initially anymore.
Good. So shall we continue to the next section?
So another way that auxiliary, or another type of auxiliary task demand is input independent computational limitations.
And here we're thinking of a few papers that show that the number of forward passes that the transformer can make influences its ability to find the right spot and parameter space.
So neural networks are function approximators, but their ability to approximate a function can be limited by the number of computations it's allowed to perform.
And the sort of crucial feature of transformers in this example is that the number of operations that determines the next token is limited by the number of tokens that it's seen so far.
And it turns out that if you train a transformer with additional meaningless tokens, like pause tokens, like the word pause, you can increase its accuracy across a range of question types.
And yeah, this is, this counts as an auxiliary task demand in our view because it's doing something roughly analogous to sort of giving the model.
That's the auxiliary factor, right?
Yeah, yeah, sorry.
Yeah.
It's doing something like giving the model time to think.
And yeah, so you might think that the absence of that additional inference time is a factor that is not directly, not conceptually related to its capacity to answer a question like the one on the screen, you know, a simple,
earth medical question.
Raph, do you want to fill in more?
Yes.
Yeah, no, so I think that analogy is a nice one time to think because if you if you tested a human on even a simple mathematical questions or any any task really, and just ask them, you know, told them they have like one second to just blow it out and answer performance would probably be pretty bad.
And you can think of asking an LLM to answer a question point blank as very, very loosely analogous to that.
And obviously, this is an analogy and there are very important differences here, but I think it's a helpful heuristic to think about what is what is going on roughly here.
Namely, in both cases, the system, the human or the LLM does not get the chance to perform the necessary computations to derive the correct answer.
And so yeah, what we talk about in the paper is that you have this experimental work sharing that if you ask a question to a language model, the amount of tokens it's a it's it's allowed to generate before providing the answer.
Makes a difference to how accurate it is. So if it just generates a few tokens and have to give an answer or even if you just have to give the answer point blank with a very first token, it's going to be less accurate that if you give it a chance to generate a number of tokens before giving the answer.
So the usual way in which this is understood is that when you ask when you you you allow the LLM to generate a number of tokens before giving the answer or you even prompted to do so you say things step by step for example.
That's not as chain of thought prompting and essentially what you're doing is you're forcing the LLM to generate a reasoning trace or what looks outworldly externally like a reasoning trace in the output before giving an answer.
And we know that chain of thought prompting increases performance accuracy.
But what was found by a couple of papers is that the mechanistic influence of this process is not entirely due to the nature of the tokens are generated in this reasoning trace.
In other words, it's not just that the LLM has to generate the right tokens corresponding to different steps of reasoning before giving an answer.
In fact, the very fact that you allow the LLM to just generate tokens any token before giving an answer from a mechanistic perspective, the force the system to perform additional computations that can complete a computational circuit that otherwise we're going to get a chance to be completed and to derive the correct answer.
So as Charles mentioned, you can set up an experiment when you have the LLM just generate meaningless tokens like little dots, a bunch of dots, dot tokens before giving the answer.
And the more dots you allow before the token gives the answer, the greater the expressivity of the system and the more kinds of problems you can answer.
And so as Charles mentioned, every time an LLM is generating a token, the LLM is performing one forward pass.
And so the more tokens it's generating, the more forward passes it's doing.
And one way to think about what's going on here as well is that having these additional forward passes where you feed back the whole input sequence plus the previously generated tokens into the system to generate the next
is also a way to introduce the form of recurrence in transformers that are not in terms of the architecture of recurrent networks.
So that increases the expressivity and complexity in terms and there is pretty compelling evidence that if you don't add up for that then you're imposing a limitation that again we think is very, very loosely analogous to
prompting a human to answer a point of order question without thinking.
So that's the next sense an auxiliary factor because if you give the LLM the opportunity to generate enough tokens, it might have the competence to solve tasks, but you might not see that otherwise and you might get performance errors, but you do think it's incompetent.
All right, yeah.
Okay.
So the third type of type one anthropocentric bias that we talked about is mechanistic interference and so this comes from the mechanistic interpretability work and the basic idea is that because large language models are capable of in context learning they can learn different
strategies for solving a different particular type of problem and the strategy that they implement at a given time can be different.
So you can talk about this in terms of virtual circuits that are formed inside the language model.
There's some interesting work from Neil Nanda and others showing that in some circumstances these two circuits can compete with one another.
So at a certain level of or after a certain amount of training, you get one circuit operative.
After a bit more training, you have two different circuits, but they're the first circuit is still sort of dominant.
And then after additional training, the model converges on the second circuit, and the first one slowly gets sort of it sort of ceases to influence the internal operations of the model.
And then only once you reach that third phase, which the the benefits of the second circuit with respect to the first become visible.
And you can show using decoding work that the second circuit is there before you can show behaviorally that the second circuit yields better performance accuracy on on the task.
So, I suppose there's a combination of two ideas here one is that there are different strategies a model can implement for solving a problem.
We can detect those strategies internally using decoding methods.
So three ideas, and then the third is a good strategy can be present in some sense in the model.
So before it has had the chance to influence behavior.
And so this is just another way that the link between performance and competence is shown to be complicated that might seem at first.
And just to clarify one thing, so the circuits are just, you know, ways to think about the causal structure of a neural network and essentially computational subgraphs of the network that have a specific function.
You can think of a circuit as implementing a Portugal algorithm or set of computations.
It's a part of what people are interested in this mechanistic interpretability literature that we build on.
People like Neal Mandatrisola and others is reverse engineering the circuits that in deep neural networks and large language models, pure to implement certain well defined algorithms in some cases at least.
And the emerging picture that we build on here is that there is a lot of redundancy built into neural networks as they learn to perform a task optimizer as a function that in many cases translates into redundant circuits that relate to the same tasks, the same kinds of
the same kinds of input and output mappings.
And for this circuits might be somewhat identical circuits that are just redundant or they might be different algorithms to do a similar thing.
And different strategies to solve some problems, I swear.
Maybe one would be a bit more approximative and the other one would be more exact, more computationally intensive.
So that's where you can have some interference where one, or at least some competition where once your kid takes over another and such that the other becomes kind of, you know, it's there, it's latent in the system, but you don't get a chance to influence behavior on a specific input.
So you can get a performance error for that reason.
And these can combine with the other things we mentioned here.
So things like task demands, the first thing we discussed, as well as the number of tokens you generate.
Both of these things could cause a particular circuit to take over another.
So it's, we can think of this holistically as perhaps if you ask a question point blank to a model without letting it generate bunch of tokens before giving an answer.
Then one particular approximate circuit might take over that gives the wrong answer.
If you let it generate more tokens, then another more exact circuit might be given a chance to a causal influence, the outputs within the right answer.
Task demands, strong task demands might, in some cases, impede on the triggering of certain circuits that would otherwise have given the right answer.
So that could be the case for us in the Lackrets and Lampinen example, where giving examples of the task in the prompt might actually prime the right circuit to solve the task about complex recursive cases in the right way.
So yeah, these are the three main auxiliary factors that relates to what we call type one anthropomorphism, anthropomorphism, sorry.
I guess we should, we've been in a long race, we should be quick on type two. Do you want to pick fix and have to talk?
Yeah, so type one deals with cases where performance of the model is weak compared to humans.
So the model doesn't do so well.
And then type two is when the model does do well, but nevertheless is different in some respect from the performance profile of the human, or we have some evidence to think that the model uses a different strategy than humans typically use.
And the idea is that even once you hold performance equivalent, or average performance equivalent, you know, making a different pattern of error.
Or adopting a different strategy is evidenced by, you know, some interpretability work.
Any deviance from the human strategy is evidence of fragility, or only a trick solution.
And this point is a bit more philosophical, I suppose, but the idea is that the human strategy for solving a problem isn't necessarily the most general strategy for solving a problem.
And what matters is whether the strategy that is pursued by the model is general, whether it's robust, whether it's accurate, and not merely whether it mirrors the human strategy.
Yeah.
We end the paper by considering an objection, which is why, like, given that in humans, we study cognition largely through language, and given that elements are trained on language or linguistic outputs from humans,
isn't it appropriate, after all, to treat human cognition as the correct or appropriate the obstacle to study LMS?
And to that, we answer that it depends how we think of that dialectic.
So we acknowledge that there is no really other option than to start our investigation of cognitive abilities in LMS, but with reference to human cognitive abilities, using human cognitive abilities as some kind of realistic or reference points,
things like theory of mind, memory, metacognition, various forms of reasoning, et cetera, that are familiar to us because we humans have them.
And this is the same thing, by the way, in animal cognition, for example, or in developmental psychology, where in any comparative psychology setup, the reference point for what concepts of psychological capacities, initially at least,
is necessarily tied up with our conception of what we humans have in our repertoire of cognitive capacities.
But we emphasize that this is only the starting point. So here we've written from the philosopher Ali Boyle, who calls this investigative kinds, investigative cognitive kinds.
We start with a cognitive kind, like memory or metacognition, episodic memory, metacognition, theory of mind.
And as an investigative starting point, the starting point of the investigation, and then we can start operationalizing this concept, this cognitive capacity in an experiment, testing the LMS on it with an open-minded empirical approach,
and then based on the results from that, iteratively refine the capacities that we are targeting, the capacity that we're targeting, or the definition of the capacity we're targeting in a way that could gradually lead us to share the initial anthropocentric assumptions that we have,
such that as the experimental project runs a course or as we get more results and refine our concepts, we may end up with something that no longer looks like looking, trying to find human-like episodic memory in Ravens or in LMS,
but ends up looking for something that shares some similarity with human-like episodic memory, but is different in other respects.
And so we can gradually come up with a kind of cognitive ontology for the systems that is less anthropocentric.
So we emphasize that there is kind of due to feedback look here, that's premised on open-minded empirical investigation, that doesn't set all these questions a priori, but has to start as a necessary starting point with the reference to human cognition.
I don't know if you want to add to that, Joss?
No, I think that's pretty good. Maybe we should move on to questions.
Yeah, sounds good.
Awesome.
Wow.
You can stop sharing or you could leave it up, but I couldn't move it.
Awesome. Okay.
Yeah, a lot of interesting pieces there, so thank you.
I'll read some questions from the live chat, but first I just wanted to read a short quote from the 2022 Active Inference textbook they wrote on page 195.
Some decades ago, the philosopher Dennett lamented that cognitive scientists devote too much effort to modeling isolated subsystems, e.g. perception language understanding, whose boundaries are often arbitrary.
He suggested to try modeling the whole iguana, a complete cognitive creature, perhaps a simple one, and an environmental niche for it to cope with.
So it's interesting about the approach that you're taking. This is kind of a simple synthetic iguana, but that's leading to the bringing to bear of a lot of these empirical phenomena because there is something.
And so I saw in the presentation paper kind of this call for like deliberate investigation rather than just chopping up the iguana a priori with a framework that applies to humans or that centers humans or that just soothes the epistemic challenge that's presented.
Okay.
First question from Dave. He wrote,
Have you looked at Daniel Dennett's distinction between competence without awareness and competence with awareness? He expands on this in the 2023 from bacteria to Bach and Bach.
I find this much more valuable than Chomsky's highly problematic performance without competence, a situation that Chomsky posits but doesn't look at deeply.
Where do you put awareness in all of this competency?
Well, maybe I'll let you think that one in a second, Charles, because you're maybe more within it than I am, but I'll just say awareness is a very polysemous term, like many terms in philosophy of minds, but particularly this one more than many others, I think.
So it can mean a lot of different things in all contexts.
Here we don't focus on things like consciousness because I think we probably both agree that it's a less tractable, maybe empirical problem to try to assess the presence or absence of consciousness in language models, even though many people are interested in that.
We think that we have more hope of making progress in the near term with more well-defined cognitive capacities or cognitive functions and things that relate to certain forms of reasoning, viable binding, etc.
So our framework and principle would apply to things like consciousness or as you put awareness generally speaking, but we don't really focus on that for examples.
The other quick thing I'll just mention is that I seem to remember that the phrase from Dennett, but again, I'm not a Dennett scholar, was competence without comprehension, which seems a little different from competence without awareness,
perhaps depending on how you think of comprehension. And yeah, I think that does, I think it is a very interesting phrase that it does.
In fact, I had this project that's published with Chris Bolega, who I think you had on the podcast as well.
There are about 170 competence in language models where we use that phrase to kind of avoid taking its stance on the kind of messy, muddy question of whether elements understand language, which builds in all sorts of assumptions, including about consciousness actually,
because of people who like to transfer, and we focus on a more restricted notion of competence. And I think our paper here also has that property that if we operationalize competence, we end up operationalizing competence in terms of the sets of knowledge and mechanisms that enable a system to generalize well
in a given domain, basically, in a way that's, I suppose, very deflationary compared to some more expensive understandings of competence that are related to comprehension or understanding more broadly.
But I'll let you take that one, Charles.
No, yeah, that was, that was good.
The phrase, you know, the distinction that Dennis draws is between competence with comprehension and without. And I think competence with comprehension is the ability not just to pursue a strategy that's successful for solving a problem but to articulate the strategy in such a way that you could teach it, for example.
And humans only sometimes have competence with comprehension. We have many competences that lack comprehension, right.
You know, when we learn to walk, for example, we have amazing competence that we still can't quite translate into robotics, because we don't fully understand how it works.
And when it comes to our language models, I think we should not expect comprehension. I mean, they have an amazing suite of competences.
If you thought that they also had comprehension, then I suppose you would think like, well, if you want to understand how a large language model works, you can just ask it.
That's a bad strategy. Nobody, nobody thinks that's a good way to find out how a large language model works. So, so they're on the competence without comprehension side of things.
And in order to figure out what, in order to figure out what the mechanisms are that enable its competencies, we have to pursue strategies that are broadly similar to the strategies we use in, you know, cognitive psychology or cognitive linguistics.
And, you know, we have to run experiments. So I think that that's all very compatible with Dan's way of looking at things.
And one other thing I'll mention Dan's, so Dan was quite influential to me and we actually wrote commentary together, which pushes back a little bit on a simple understanding of this distinction.
We're looking at the evolution of metacognition.
And basically what we argue is that, given the gradualism of evolution, there must have been something in between base level cognition and metacognition. So we shouldn't see that distinction as black and white.
You know, I think that if you want to contrast the sort of cognitive prowess of a human adult with lots of linguistic and scientific concepts at her disposal with, you know, a non human animal than this strong distinction
between competence with and without comprehension is reasonable.
But in the space of all possible minds, we should be open to the view that there can be, you know, semi-competent forms of cognition.
And just to put it briefly on this, it occurred to me while I was listening to you as well that the first example of auxiliary practices we give, auxiliary test demands, and specifically Hugh and Frank example,
is a nice example where in order to give the metacognistic judgments correctly, so see that that, you would need competence with comprehension because you need to understand,
not only be able to come to agree the verb with the subject, but know the rule and know how to formulate it, perhaps, to realize it's reasonable to teach someone, right?
And so when you find that the LLM can do well at the low task demand burden of the task and at the high task demand involvement, explicit metacognistic judgments.
In some way, that's an example of the LLM having competence with comprehension.
Yeah, nice.
Awesome. Okay. Upcycle Club wrote question.
Given that LLMs inherently reflect anthropocentric biases due to their training on human data and goals, how can we ensure that their intermodal discourse aligns with humanities values?
So the interwhites discourse?
Intermodal discourse, perhaps amongst the models.
I see, I see.
Like in that Farmville paper?
Yeah, the small, the, yeah, both generative agents, yeah, the small ones, yeah.
I think that's beyond the scope of this paper, to be honest, but I mean, we could mute about it.
But I don't know that we have, I don't know if this project has much, I mean, I think we both have an interest in the alignment problem independently of this project, but I don't think this project has much to say really about this.
I don't know, Charles, what do you think?
Yeah, yeah, I don't.
Yeah, I don't have anything super concrete from that.
Okay. Dave asks, an example of inserting noise into LLM training, that was the section about the extra tokens. Do you see any analog to intermittent reinforcement to uncertainty tolerance?
Because you mentioned the extra tokens in the chain of thought and how that could also be replaced by dot, dot, dot, dot.
And so, like, what is that telling us about model training when it seems like there's some situations where adding superfluous tokens would diminish signal in data sets, but then here are other situations where it seems to actually help.
Yeah, so in that particular paper, I think it's called thinking dot by dot, and there is a subtitle.
It's by Will Merrill and Jacob Sparrow, I think.
In that paper, if I recall correctly, what they did is that they introduced just this one field of talking, swarming this token, just to evolve, but just a dot.
And they trained the model to give an answer after producing a certain number of dots.
That's not just like introducing Rembrandt and Gibberish in your training data.
It's actually quite a specific intervention that forces the model to learn to perform certain computations before giving an answer.
So it makes sense to me that this wouldn't diminish performance, and that you could do that.
It's not quite the same as just having that training data, right?
Just because the token seems meaningless, it's a field of talking to dots.
It's not just Rembrandt and Gibberish, it's just going to throw off the model and impede the optimization of its learning function, or at least good downstream performance.
But what it's going to do is going to force the model to learn that when there is a dot token, it can allocate computation with its ascension heads and other parts of the architecture in such a way that it's getting towards deriving the correct token when it's finally producing the token that matters and that's meaningful after the series of dots.
Yeah, I don't know, Charles, if you have another answer.
Yeah, I mean, I think it's an important question because a priori, if someone said, look, we're going to append and prepend a whole bunch of meaningless symbols to an LLM input, you might very well think that this will just weaken the signal to noise ratio and degrade model performance.
So it's against that background that the empirical result that it doesn't degrade model performance ought to be regarded as an important clue about how the model works.
So I think that the intuition behind this question is indeed part of the interpretation of the empirical results, right? It's surprising for exactly this reason. And then the theory that's supposed to, you know, well, this is an active inference podcast, right?
The theory that's supposed to help rid some of the surprise here is the idea that given the architecture of a transformer where it has to go through all the tokens in every cycle, having these extra tokens gives it sort of more computational bandwidth and therefore more
expressivity or more capacity to, you know, locate the right spot and parameter space.
And even that, in a way, reminds me of, so it's not just that dots improve performance. It's not, it's that it was, like you mentioned, it was trained to have that. And similarly, it could have been trained maybe hypothetically to just output Shakespeare quotes verbatim while you're processing.
So that's kind of like a filler or more of like a sort of, that was a great question. It's like, these are linguistic paddings that do create time to get to the meet. So not only does it signal and signpost if it's being trained to have that meaning, which then questions like, so that it wasn't a meaningless
dot, if it if it had a cognitive or even like a semantic aspect.
I had a question. How do you feel like in this era Cambrian explosion of diverse intelligences? How can we understand capacities when they seem so conditional upon the setting and how the system of interest is interacted with?
Like, what are the practical implications for people who are studying LLMs and other synthetic intelligences from like a safety or reliability or performance perspective?
No, I was trying to you.
So, so, so just want to chat to some of the questions. So the question is, how, how is the notion of a capacity.
Changing when we have such different systems that seem to have intelligent behavior.
Yeah, and it's so dependent upon potentially, initially unintuitive ways of interacting. So how, how, how can we understand the reliability and the performance and the capacity of a model, other than for example,
by exhaustively inputting prompts, which can really happen. What, what could we really say or know?
And or just how do you feel that this work re-enters into the ways that people practically are using the models?
Right. Okay, yeah, it's a very interesting question. So the first one, the question is, I think, you know, part of the background assumption from this paper that I think specifically,
definitely in other work is that behavioral evidence is simply not sufficient in most cases to arbitrate disputes about capacities of LLMs.
When it comes to human cognition, we do have to rely a lot on psychological experiments that are ultimately behavioral.
And we do also rely on self-importance in a little more than we can when it comes to LLMs because we, despite the move away from relying on intuition and introspection and the history of psychology, it still has a role to play.
But we've, by and large, we did it towards behavioral experiments that get increasingly sophisticated to try to reverse engineer what's going on inside the black box.
When it comes to LLMs, partly because that's so different from us, relying exclusively on behaviorism is even more difficult because we have even less of an idea of what might be going on inside the black box
and whether it's anything like what's going on inside all black box and we have reasons to think it might be very different.
So I think we both agree that we have to supplement this with mechanistic work that essentially involve performing causal interventions on the inner mechanisms, on the inner workings of the systems.
So decoding representation and computations that the systems that are in principle available to the systems and then intervening on them to confirm hypotheses about the causal role of these representations and computations.
And we have methods to do that and partly what we can be a little optimistic about this project, even though it's extremely challenging especially to be scaled up to large models,
is because unlike what's happening in your sense with the brain, where the range of decoding methods and intervention methods we have is extremely limited but for ethical and for simply practical reasons.
We also just don't have ground truth access to activations in neurons at least that easily and we also generally enable to make specific interventions on activations in the brain.
When it comes to LLMs, we have full ground truth knowledge of all activations of every single part of the network and also have full access to all of it for interventions at inference time.
So that opens up a whole new range of things we can do and that enables us to go beyond behavioral studies and actually decode these features and circuits or as researchers put it in the literature or as philosophers would generally put its representations and computations
that the system is actually making use of and try to reverse engineer what kind of algorithms it's making use of. So part of the broader problem projects that we have with Charles is to
suppose we start with these investigative kinds as we put as Alibol calls them, these human subject capacities, we can operationalize them and do behavioral experiments in the top down and then from bottom up we can also try to reverse engineer the
mechanical building blocks of the computations and representations that LLMs make use of to solve the task related to that particular capacity. And then we can meet somewhere in the middle and try to from that line of work that purchase things from above and bring to the fore some kinds of mid-level abstractions as we call it or
computational building blocks that might be key to the performance of the system in that domain. So for example, if you're interested in the capacity for reasoning, you can start with this very broad human-centric notion of reasoning, then try to operationalize it in a reasoning
task, then do some behavioral testing and then mechanistic interpretability of that reasoning task, find out how the system is solving it, find out how the algorithm is doing well and why. Reverse engineer building blocks that might, for example, have to be viable manipulation, viable
binding, and then from there you might be able to either actually refine the notion of reasoning you started with to have a more specific and less human-centric notion that is now operationalized in more low-level terms like that involved manipulation of variables in certain ways and the
binding of variables to theories, etc. So yeah, so that's I think the general approach we take. Now, how does that, does any of that feedback into interactions, how we humans interact with algorithms? I think that's one way in which it could feedback is simply in terms of challenging or our spontaneous anthropomorphic attitudes to algorithms to some extent the same way if you read a lot of animal cognition, perhaps you will interact with
your cat in a slightly different way, but you might maybe not rush to the conclusion that when your cat performs a certain behavior, it has understood what you think and it's modeling what you're thinking about what it's thinking or something. Perhaps you might adopt a more deflationary attitude to explain the behavior of your cat, doesn't mean you have to love them any less or it doesn't mean you have to
If that is the other thing, like if you want at the end of the day to speak to your cat like a human because you really are a gentleman for that, then that's all the more part of you in the same way. If you find it useful to treat algorithms in the way you interact with them
To have fluid interactions with them, to treat them as if they had beliefs, designs, etc. of human-like capacities, then that's fine if that's for actual purposes, but at least if that line of work that we are kind of sketching here ends up maturing enough, the hope is that we can interact with algorithms perhaps in a way that's
Even if we have that kind of intentional stance and make believe about what the kinds of besties they have at the background, we will know that what their limitations are and what their actual besties are. I don't know if you agree with that, Charles, because we're going to discuss that much.
Yeah, I agree with all that. I just had a slightly different first reaction to the question. I took the question to be in part about how to deal with the sort of prompt sensitivity of models, the fact that sometimes we write something that seems natural to us, but provokes an unexpected response from a large language model and how do we think about that.
And the first thought that occurred to me was just that we should distinguish between different kinds of large language models.
You know, we have this sort of huge large language models which are fine-tuned to interact with us in a particular way, and here's the central point. They're trained on a sort of unthinkably large database, whereas there are other sorts of large language models where the training data is more circumscribed and where we know in more detail what
you can survey what the training data says. And I think if you're interested in what the mechanisms are underlying the responses, it's certainly very helpful to look at smaller, but nevertheless large language models where the training data is known to us, because
you know, when you train a model on the entire internet, there are going to be all kinds of, you know, subtle signals in there that we don't have much hope of tracing back to their source, but which will influence the model behavior in all sorts of ways.
But working with these somewhat more conscripted models gets rid of that problem, at least in part.
Cool. Well, where do you see the work going, or where do you plan to continue this direction?
Yeah, so actually, so we wrote this paper, this short paper for the ICML machine learning conference, the National Conference Machine Learning, that's happening this week in Vienna, and we'll be getting to Vienna at the end of the week for the popular workshop that we're representing this paper, which is a workshop on language models and cognitive science.
So there will be a very strict page limit for the ICML contribution, which is four pages. But what we want to do next is to expand this into a more philosophically substantive paper that's going to be a bit longer.
And that's going to expand on the more philosophically meaty parts of that project, because everything is still a bit compressed in that version of representing an ICML.
So yeah, that's the next step for us. This is a really useful way for us to force ourselves to write things down. After running the box piece, we wanted to write an academic piece.
Now we've written kind of a condensed skeleton of the piece that focuses more, that caters more to an ML audience. And now the next step is to write the full philosophy paper, or at least that part of our project to be complete.
I don't know if people have all that ideas, but yeah.
I got nothing to add to that.
Cool.
Yes. Well, it's very interesting work. I think it brings a lot of pieces together. And it's some philosophy and cognitive science jumping in, jumping into the heat and into the spotlight and the relevance.
And so it's going to be an exciting learning journey.
Thanks for having us.
Yes.
Thank you very much.
Cool.
Till next time.
Thank you.
Thank you.

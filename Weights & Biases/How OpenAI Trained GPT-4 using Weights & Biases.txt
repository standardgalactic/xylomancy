All right, I'm so excited to be here with Peter, the VP of Product and Partnerships
that open AI a day after the GPT-4 launch, one of the most exciting launches I think
ever in Silicon Valley.
So congratulations, Peter.
Thank you so much and thanks for having me, super excited to catch up.
All right, so I know you're busy and we have limited time and we crowdsourced with
way too many questions, so I'm just going to rapid fire and etch you if that's all
right.
Yeah, let's do it.
Let's do it.
So first question is, what are some specific applications of GPT-4 that you and your team
are most enthusiastic about?
I think, I mean, there are tons of them, but just to call out a few, one thing that I really
thought was super cool was Be My Eyes, who is using the visual inputs to GPT-4 to help
visually impair people.
I mean, it's like, you know, computer vision used to be the thing where you did object
recognition and so on, and suddenly you can do much more.
You can like ask like, hey, is something wrong in my outfit?
Something I should fix?
Or, you know, I have these ingredients.
What should I cook?
You know, like it opens up a whole new kind of range of applications.
And you're originally a vision guy, right?
That must be wild to see this approach so well.
That's right.
It's sort of crazy.
Like, yeah, all we did was language models.
Awesome.
Okay.
Does a lot of prompt engineering become redundant given the larger context length and the ability
to supply more in-context examples in the prompt?
Oh, yeah.
This is super interesting.
I sort of feel like, I keep on thinking that something is both right and wrong with prompt
engineering.
Like, you know, ultimately, prompt engineering, when we should think about it, we should think
about it's like, you know, how good are you at just specifying tasks?
Like, a good manager should be able to tell, you know, their employee, whatever, this is
your job.
This is what you work on.
It shouldn't be super much ambiguity.
And that's usually, that's kind of what, what, what prompt engineering is about.
But like, this ability to just have a conversation with the model and like fix it.
Like, no, no, no, that was too formal or no, like, make it live upbeat, add a joke here.
You know, like, that's sort of interaction.
That's how I think we would kind of think about language models much more than like adding
lots of examples or like writing very specified instructions.
Like, let's just have a conversation with the model and it would kind of pick up on
what I want over time.
All right.
Now here's a question we got the most in different forms, but why did you guys not, why weren't
you able to keep the training data on more up to the training of GPT-4 and more up to
date data?
Oh, yeah.
So that's, that's such an interesting question, like, so ultimately, you know, training these,
these models take a long time.
So that's, that's ultimately the answer here is that, you know, I can't tell you exactly
how long it took to train this, but like, there's, there's processes here of like, at
some point you're doing testing on certain kinds of data and then you want to lock down
that data and not change it too much.
So you can really trust that when you run the big kind of training run, nothing is going
to go, go wrong.
That just means that you're going to have still data at some point.
Obviously, like getting fresh data into the models is a pretty big kind of question that
we get over and over and over again.
So it's, it's something that's definitely on top of our mind to kind of fix going forward.
Ultimately, that is, you know, it just takes time to train these models.
Okay.
All right.
Considering that GPT-4 is multimodal now, do you imagine future foundation models to be
multimodal by design?
And I guess, do you have any comments on what the biggest blockers were when introducing
the multimodal component?
Yeah, that's a good question.
I, I, I, I definitely think that like multimodal is sort of the future.
Like this is, you, you just give the models abilities to understand many more concepts.
They kind of understand things like, you know, you know, physics concepts based on looking
at images, or they can understand if like, you can imagine eventually these models kind
of deal with more modalities like audio and, and so on.
Like it's pretty clear that the future here is, is just more and more, more kind of multimodal
models.
And, and, you know, some components are there today, like with Whisper, you can take, you
know, you can take audio and turn it into text and so on.
So it's, it's like, all of these things eventually will, will get connected.
I think in terms of challenges, you know, I, I, I, I, I would confess here that I wasn't
on the research team that did a lot of the amazing work to get GPT-3 working.
But I just know that, you know, going, scaling, going from like small models to like these,
these enormous models that GPT-4 is, is, that's like a huge challenge in itself.
You kind of need to do a lot of science to understand, like, on small models to see like,
here's how good it is, and here's how good it's going to be when we scale it up.
And that's, that just requires a lot of like very detailed kind of investigations and so
on. And a lot of kind of like just, just hard work, which, which that team did.
Totally. Were there any new, like specific challenges that GPT-4 are different than
previous models, or is it just sort of like trying to get even another level of scale?
Um, you know, I, ultimately, it is, you know, just another level of scale.
But I would say that we work in like all aspects of that, whether it's data, whether it's how
to most efficiently use the compute architectures of models and all of these things are things
that we kind of tune and tweak continuously.
So, so, but ultimately it, I mean, big part of it is really being able to scale.
And as you scale every order of magnitude, there's just more and more things that break
even in your infrastructure, you know, and so, you know, we have a just an incredibly
talented infrastructure engineering team that is like made up of both engineers and
researchers that are, have built this incredible system for being able to train these models.
And there's just so many little things that you need to get right, because if you have
a bug somewhere, you know, it's going to propagate immediately.
And so like, you know, I think one of the biggest like mantras, essentially that team
has is like, don't, don't have bugs, you know, which is like, seems like the most hardest
thing to do as a software engineer.
Um, good advice, though, don't have bugs.
Um, so, so here's another question.
I don't even know if it's fully formed, but, you know, do you have a sense or a guess of
like the order of magnitude of experiments running in a typical day, like in the weights
and biases notion of experiments, like training something?
Is it like, like a handful or millions or what do you think it is?
It's a good question.
You know, I would say like, there's probably on the order of like, I don't know, like somewhere
between 100 and 200 people essentially doing some sort of research at open AI.
And each of them are, you know, probably running on the order of like a few experiments per
week.
So that probably makes it like multiple hundreds of experiments per week.
Like some of them will take, like some experiments take, take weeks, some experiments takes like
hours, right?
But it's sort of like, that's probably the order of magnitude.
Got it.
Okay.
Um, to the extent that you can comment, um, what was the most useful type or source of
data for training GPT for?
Oh, um, I mean, it's, it, I feel like this is just like the text on the internet is probably
the answer here.
You know, we just, these, these models are just incredibly data hungry.
So just finding more and more and more of this, like we have a whole team that's just
like basically focused on like, how can we bring more data into these, these models?
So, uh, I think at this point it's like, almost to some extent it's like less about the particular
data, more about more data, you know, um, so that's probably how I would think about
it.
Is there any particular task?
I mean, I saw all these tasks that, you know, you, you showed a GPT for getting better at,
is there any task where like, you know, it's been surprisingly hard given, um, you know,
how impressive like all these, like every version of LMS is anything stand out as like
you would have thought it would have gotten solved by an earlier version.
Oh, that's a good question.
I mean, it is still surprising to me.
I feel like, I feel at this point, like GPT-4 is quite good at just doing mental arithmetic
and stuff like that.
But like, you know, it took all the way to GPT-4 to kind of get it to be fairly robust
at that.
It would just make these silly mistakes for even up to 3.5, you know, uh, and it's still,
you know, just to be clear, it still makes some silly mistakes.
But you know, it's like, the way I look at it, it's kind of weird, almost like a weird
savant in some sense, right?
Like it has these amazing capabilities on some dimensions and then makes these like incredibly
stupid mistakes that a human just wouldn't make unless the human is drunk or something,
right?
Like, that's sort of like, that's what it feels like sometimes with the model is like,
you kind of never know, did you get the, the kind of early morning version or like the
late at night after multiple drinks version.
And like, the weirdest thing about this is that sometimes like even like, uh, prompting
the model, it seems to matter.
Like if you tell the model, hey, you're a genius at math, it would do better at math
if that, if you tell is like, you're not very good at math, right?
Like you can't like prod it, like you can't get it riled up and then make it.
Exactly.
And so the, the models are like, it's still very, uh, vulnerable to this, this stuff,
you know, stuff.
So, uh, but, but I think that's like, you know, that's one thing I think it is.
I mean, it's like, um, uh, it's also funny.
You might have seen in the, uh, in, in, in Greg Rockman's demo that he did, like, you
know, the whole thing about like summarizing with a single letter, um, the same letter
starting every, every word.
I mean, it's kind of, you know, it's still, it's sort of weird how it just happens that
once you reach a certain scale, it kind of just gets it.
And I think there's a, there's a number of those where it's just like, it's kind of weird
how you just, you know, I think it's kind of called a grokking phenomenon where you
train and train and train.
It doesn't do very well.
It just gets it's something kind of, it forms some set of representation and just kind of
gets it.
Uh, and so there's been a few of those, uh, and it's like, I think it's always surprising
when they happen and you're like, why didn't it happen earlier?
And, you know, I don't know.
Hmm.
Interesting.
Cool.
Um, okay.
Here's an interesting question.
Uh, maybe useful for us at Wits and Bices.
What kind of visualization tools would be most helpful as we continue to, to build on these
models or do you have any ideas of what might be helpful for people building on top of these
models?
Oh yeah.
That's such a great question because I think this is like kind of evolving more and more
as these models get better and better.
Um, you know, it used to be the case where I think kind of all you cared about was really
like perplexity and so on as you were training it.
It's like that was most of what you were optimizing.
But I think we're getting to a point now like where you, you kind of, um, you really want
to visualize how, uh, like dig into kind of the mistakes that the models are doing and
so on.
Like you often remove the word where I think you can get really far with like prompt, prompting
and, and, and, uh, whether it's like through conversations or a single kind of prompt.
Um, and then, but then you will get to some point where you can kind of, uh, you can fine
tune these models to, with even more data.
And at that point, I think the paradigm now is that you don't need a ton of data, but
you need to make sure that that data is kind of really, really high quality.
So understanding kind of a way to see like where, where are mistakes made?
How like, how do you automatically kind of build, you know, we ship this, this open source
library called open AI evals that kind of makes it easy for you to build, um, automated
evals.
So I think like that would be a big part of the workflow and allowing people to see overall
trends, but dig into the examples, because oftentimes what we find that at the end of
the day, like examples are under specified and so on.
And, and so you just need to be so way, way more careful now that like the models are
so good, uh, that like you're not seeing weird stuff because the model is bad versus like
you just had a mistake in your data set.
Hmm.
Makes sense.
Um, okay.
Uh, when, so I guess, uh, here's, I guess the summary of this question is how many people
worked on GPT for like how many animal engineers did it take to, to train this thing?
Oh, that's a good question.
I mean, it's just like, I feel like all of openly I worked on this one, uh, in one way
or another, because like we have this whole kind of process, like a big, like, you know,
all the data set work that goes into it, all of the infrastructure engineering that built
the data centers that kind of built, um, um, uh, the infrastructure on top of those data
centers training these models, then all the kind of science and stuff that went into actually
the pre-training.
And then we do this kind of fine tuning on top with a RL from human feedback, you know,
so I feel like at the end of the day, it's got to be like probably out of the 300 plus
people at open AI, probably at least two thirds of the company worked on some aspect
of this.
Okay.
And final question.
Um, since this is our user conference and you are such a high profile, um, user or open
eyes, I'm curious if you could say a little bit about how open eyes is, which advice is,
and if you have a favorite, um, feature or part of, uh, ways and biases, we'd love to
know about that.
Yeah.
Yeah.
So we use it for, for pretty much all of our model training, so just like, you know, uh,
you know, just, just, just tracking them.
I think there's a lot of kind of just sharing of like, you know, the fact that you can easily
share, share, uh, share kind of training runs and stuff like that.
It's a super used feature, but I think one thing that, you know, these days I, I, I do
way, way less of that sort of work.
So one of the features I really like is, is kind of the ability to kind of have reports
and so on where people, so we use that, uh, quite heavily defense a little bit on the
team, but a number of teams are using that quite heavily to kind of really have clear
hypothesis.
Here's like the hypothesis, uh, here, here, here is the, here are the experiments that
were run to kind of validate or invalidate that hypothesis.
Here's the conclusion.
Do you have all these like mini scientific papers, essentially on all of the stuff that's
happening at open AI, which is like incredibly interesting to kind of, uh, follow along with.
Fantastic.
That sounds very interesting.
Um, thank you so much, Peter.
It's really, really nice to have your time, especially so, so close to after your, your
big launch and congratulations.
We're, we're loving, um, using it internally here.
Thank you so much for having me.
This was super fun and, uh, can't wait to see what you all built with, uh, DPD4.
Awesome.
Take care.

Unelected, unvetted, and completely unaccountable, this one man has chosen to put all of our
lives in grave danger so that he can make money and fulfill his dreams of radically
altering human existence as we have never known it without even an effort to get any
consent from you or anyone else in the general public.
Here's the really crazy part of it.
He tells us all of this openly.
There is no deception.
He admits all of it.
You have an incredible amount of power at this moment in time.
Why should we trust you?
You shouldn't.
You shouldn't.
You shouldn't.
You shouldn't.
Let's be very clear.
The man who signed the 22-word statement admitting that his AI technology has the power to kill
all life on Earth and must be mitigated like nuclear arms and pandemic is also saying to
you directly that he should not be trusted.
When people show you who they are, believe them.
Yes.
In general, it's the process for the bigger question of safety.
How do you provide that layer that protects the model from doing crazy, dangerous things?
I think there will come a point where that's mostly what we think about the whole company.
Aha.
Hmm.
Okay, so at some point, the safety crisis will be so urgent the entire company will
need to deal with it.
Hear him.
Believe him.
The thing he's making will be very unsafe and at an unknown point, the guy who says,
trust me, wants us to trust him that he will at the exact right moment know to throw the
whole company into safety work.
But just not quite yet.
I thought at some point between when Open AI started and when we created AGI, there was
going to be something crazy and explosive that happened, but there may be more crazy
and explosive things still to happen.
Welcome to For Humanity and AI Safety Podcast, episode number 22, Nobody Elected, Sam Aldman.
I'm Jon Sherman, your host.
Thank you so much for joining me.
In America, we elect our leaders.
They audition their ideas and then our government of, by and for the people votes to decide our
collective direction and decisions.
But the American making the most consequential decisions in human history right now as I
speak hasn't been elected by anyone, nor would he be.
His ideas poll very poorly with the general public.
He says go, the public says stop.
Polls show more than 70% are opposed to building superhuman intelligence.
So this week, we're going to focus on the most important human in AI who is also the
most important human who has ever lived sadly.
Today's show is going to point fingers and lay blame.
Sam Aldman is why we are at the most dangerous moment in human history that we are at right
now.
This is the AI Safety Podcast for the general public, no tech background required.
This podcast is solely about the threat of human extinction from artificial intelligence.
At the end of today's show, we're going to talk about how hard it is to convince people
to believe that AI risk is so much of a risk, they are moved to take action.
I have some new insight into that.
But first, it is not hyperbole to say that open AI CEO Sam Aldman holds the life of every
single human on earth in his hands.
Your kids, your parents, your friends, his knife is at each of their throats.
Sam Aldman was on the Lex Friedman podcast very recently, and so I felt compelled to
break that down for you this week.
Sam was also in a very hard hitting piece in business insider this week.
There's a link in the description as there will be to the Lex podcast.
The title of the business insider article was, Some VCs are over the Sam Aldman hype.
Sounds good to me.
Next subheading, the platform of Sam.
This is what I want to read you from the article.
Some who know Aldman paint a picture of a benevolent visionary, a thoughtful steward
of a promising technology.
They see his plan to build a far-reaching AI empire that touches everything from nuclear
fusion to anti-aging technology as a leap forward for humanity.
Others say he's more interested in self-promotion than human advancement.
All agree he's a masterful storyteller who could sell sand in the Sahara.
But as Aldman lays out a sweeping vision for the transformation of society, not everyone
in Silicon Valley is buying it.
Quote, there are holes a mile deep in this guy's resume, but he's managed to figure
out how to take his chess pieces and move them correctly.
An anonymous startup founder said, and now one of the things went crazy.
And he's an AI expert.
Okay, so my point in reading that is this Sam Aldman is a tech CEO from the sales side.
He knows how to sell ice in Alaska.
He ran a startup incubator and he had a lot of different bets.
And open AI is the one that hit big.
So this guy making all the decisions, he's not an AI expert.
He does not write code.
He's a salesman, nothing against salesmen.
There have to be salesmen in the world.
But I don't know that that's who you want making all the decisions.
It was way back in 2015 that Vanity Fair gushed over Sam Altman and Elon Musk launching a
nonprofit company to save the world from a dystopian future.
Fast forward to 2023 and Sam Altman, his nonprofit, then a soon to be blossoming profit center
of the largest corporation on earth was the one who decided to unleash chat GPT on the
world when he did.
It is widely reported that Google had a similar product to chat GPT six months earlier, but
they did not release it because they were concerned over safety.
Unelected, unvetted and completely unaccountable, this one man has chosen to put all of our
lives in grave danger so that he can make money and fulfill his dreams of radically
altering human existence as we have never known it without even an effort to get any
consent from you or anyone else in the general public.
Here's the really crazy part of it.
He tells us all of this openly.
There is no deception.
He admits all of it.
Here's the sad part.
No one takes him at his word.
No one is listening to his promises of doom.
It all just sounds so outrageous.
People dismiss it instantly and move on to whatever bullshit they were thinking of first.
Yeah, I'm talking about this guy.
Who nine months ago told us this, I think even you would acknowledge you have an incredible
amount of power at this moment in time.
Why should we trust you?
You shouldn't.
You shouldn't.
You shouldn't.
You shouldn't.
Let's be very clear.
The man who signed the 22 word statement admitting that his AI technology has the power to kill
all life on earth and must be mitigated like nuclear arms and pandemic is also saying to
you directly that he should not be trusted.
The man with his hand on the kill 8 billion people button is telling you he cannot be
trusted with this responsibility.
When people show you who they are, believe them.
Yes.
Absolutely.
Jason says to you, I'm selfish or I'm mean or I am unkind or I'm crazy or I believe
them.
They know themselves much better than you do.
But no, more often than not, those of us who don't trust life say don't say a thing
like that.
You're not really crazy.
You're not really unkind.
You're not really mean.
And as soon as you say that, the person that you know and shows you, I told you, I told
you I was unkind.
Thank you, Oprah and the late great Dr. Maya Angelou.
Sam keeps telling us not to trust him.
He says no one should have the power he has.
Maybe we should believe him.
Like no one person should be trusted here.
I don't have super voting shares.
Like I don't want them.
The board can fire me.
I think that's important.
The board can fire me and I think that's important.
He said back in June 2023 and I, yes.
So now we fast forward to that famous week in November 17th, 2023, less than six months
later when it seems indisputably a battle over AI safety and Sam Altman's honesty
around AI safety, the board, including his friend and co-founder, Ilya Sutskiver, do
exactly what he says the failsafe should be.
They fire him.
And within hours, after having attached his nonprofit intended to help humanity to Microsoft
the world's largest corporation, Microsoft naturally decided it could not lose its golden
goose.
So Microsoft led to the firing of the board members who drew the hard line on safety.
Safety lost, Sam won.
This was all a huge fucking deal.
And yet nobody other than a handful of people intimately involved seems even to this day
to have any idea what really happened, which is why Sam Altman's appearance on the Lex
Friedman show is really worth picking apart.
For the first time, Sam was interviewed in depth about what happened the weekend he
got fired.
What happened that weekend is a national security issue of grave seriousness.
And again, the guy building the tech that can end all life on earth, the same tech
he says he cannot control the same tech that no one understands how it works or why it
does what it does.
That guy, when he reflects on that weekend, when his lack of regard for safety should
have put him out on the streets, he is consumed by one thing, not what happened, not the safety
concerns, not the fact he lied about the board being a check on him.
Sam Altman's take on the weekend that he got fired is all about his feelings.
Take me through the open AI board saga that started on Thursday, November 16th, maybe Friday,
November 17th for you.
That was definitely the most painful professional experience of my life and chaotic and shameful
and upsetting and a bunch of other negative things.
There were great things about it too and I wish it had not been in such an adrenaline
rush that I wasn't able to stop and appreciate them at the time.
I came across this tweet of mine from that time period, which was kind of going to your
own eulogy, watching people say all these great things about you and just unbelievable support
from people I love and care about.
That was really nice.
The whole weekend I kind of felt, with one big exception, I felt like a great deal of
love and very little hate, even though it felt like I have no idea what's happening
and what's going to happen here and this feels really bad and there were definitely times
I thought it was going to be one of the worst things to ever happen for AI safety.
One of the worst things to happen for AI safety says the guy who started the race to AGI,
but don't worry, he says he saw this coming and that's just the start.
I also think I'm happy that it happened relatively early.
I thought at some point between when OpenAI started and when we created AGI, there was
going to be something crazy and explosive that happened, but there may be more crazy
and explosive things still to happen.
Honestly I just can't.
I cannot take this man with a twinkle in his eye, a smirk on his face and the power to
kill everyone I love, it honestly a lot of the time feels like he's taunting me, like
he's taunting us.
There is nothing about risking all life on earth without anyone's consent that is funny
or cute.
But enough for now about you and me and our families, let's focus on what's really important
and still the biggest known about the AI safety crisis to date, Sam Altman's feelings.
It feels like something that was in the past that was really unpleasant and really difficult
and painful, but we're back to work and things are so busy and so intense that I don't spend
a lot of time thinking about it.
There was a time after, there was like this fugue state for kind of like the month after,
maybe 45 days after, that was, I was just sort of like drifting through the days.
I was so out of it.
I was feeling so down.
It's kind of personal psychological love.
Yeah, really painful and hard to like have to keep running open AI in the middle of that.
I just wanted to like crawl into a cave and kind of recover for a while, but you know,
now it's like we're just back to working on the mission.
It's still useful to go back there and reflect on board structures, on power dynamics, on
how companies are run, the tension between research and product development and money
and all this kind of stuff so that you who have a very high potential of building AI
would do so in a slightly more organized, less dramatic way in the future.
There's value there to go, both the personal psychological aspects of you as a leader and
also just the board structure and all this kind of messy stuff.
We definitely learned a lot about structure and incentives and what we need out of a board.
Okay, so I'm sorry, but Lex did a terrible job in this interview.
I'm sorry.
They talked for two hours and not once did we find out anything really new about what
actually happened the weekend he got fired.
He called it the worst thing possible for AI safety and Lex does not even follow up and
ask him what he means.
What do you mean it's the worst thing possible for AI safety?
But he does ask about open AI's chief scientist, Ilya Sutskuver, who before the firing did
podcasts and made a lot of public appearances.
Since the firing weekend, Ilya has not been seen anywhere in public that I'm aware of.
He Ilya is also noted for his deep and genuine concern over AI safety.
So does Lex try to get the real on this out of Sam?
Oh no, it's just more footsie.
So cute.
Let me ask you about Ilya.
Is he being held hostage in a secret nuclear facility?
No.
What about a regular secret facility?
No.
What about a nuclear non-secret facility?
Neither of that, not that either.
I mean it's becoming a meme at some point.
You've known Ilya for a long time.
He's obviously in part part of this drama with the board and all that kind of stuff.
What's your relationship with him now?
I love Ilya.
I have tremendous respect for Ilya.
I don't know anything I can like say about his plans right now.
That's a question for him.
But I really hope we work together for, you know, certainly the rest of my career.
He's a little bit younger than me.
Maybe he works a little bit longer.
You know, there's a meme that he saw something.
Like he maybe saw AGI and that gave him a lot of worry internally.
What did Ilya see?
Ilya has not seen AGI.
None of us have seen AGI.
We've not built AGI.
I do think one of the many things that I really love about Ilya is he takes AGI
and the safety concerns broadly speaking, you know, including things like the impact
this is going to have on society very seriously.
And we, as we continue to make significant progress, Ilya is one of the people that I've
spent the most time over the last couple of years talking about what this is going to mean,
what we need to do to ensure we get it right, to ensure that we succeed at the mission.
So Ilya did not see AGI, but Ilya is a credit to humanity in terms of how much he thinks
and worries about making sure we get this right.
So the credit to humanity most concerned about safety has been silenced and hidden.
And with that problem taken care of, the race is back on.
Next, Lex asked Sam about Elon Musk suing him.
And again, Suicide Sam goes straight to his feelings.
I know he knows what it's like to have like haters attack him.
And it makes me extra sad he's doing it.
Yeah, he's one of the greatest builders of all time, potentially the greatest builder of all time.
It makes me sad.
And I think it makes a lot of people sad.
Like there's a lot of people who've really looked up to him for a long time and
said this.
I said, you know, in some interview or something that I missed the old Elon and the number of
messages I got being like that exactly encapsulates how I feel.
I think he should just win.
He should just make X grok beat GPT and then GPT beats grok and it's just the competition.
Where do you hope this goes with Elon?
This tension, this dance, where do you hope this?
Like if we go one, two, three years from now,
you're a relationship with him on a personal level too.
Like friendship, friendly competition, just all this kind of stuff.
Yeah, I really respect Elon.
And I hope that years in the future we have an amicable relationship.
Yeah, I hope you guys have an amicable relationship like this month.
And just compete and win and explore these ideas together.
I do suppose there's competition for talent or whatever, but
it should be friendly competition.
Just build, build cool shit.
And Elon is pretty good at building cool shit, but so are you.
Building cool shit with like a bro-y tone of voice building cool shit.
Building cool shit is why we are all on course to die in the next 10 fucking years.
It is not cool.
Okay, this next one is wild.
Lex asks Sam about the basis for the New York Times versus OpenAI lawsuit.
Remember, Sam's multi-billion dollar black box machine would not exist
if it had not been trained on internet data.
Sam, already having harvested the data and made his billions,
has the gall to drop this answer.
There's a lot of tough questions here.
You're dealing in a very tough space.
Do you think training AI should be or is fair use under copyright law?
I think the question behind that question is,
do people who create valuable data deserve to have some way that they get compensated for
use of it?
And that, I think the answer is yes.
I don't know yet what the answer is.
People have proposed a lot of different things.
We've tried some different models, but if I'm like an artist, for example,
A, I would like to be able to opt out of people generating art in my style,
and B, if they do generate art in my style,
I'd like to have some economic model associated with that.
What the actual fuck are you even talking about?
You already stole the training data.
All the writers and artists have had their work stolen.
This dude built a billion dollar business on stolen writing and images,
and now after his business is flourishing after,
and the toothpaste seems impossible to put back in the tube,
he's like, um, yeah, somebody should figure out a way to make this fair.
There's a real theme with Sam Altman.
Every problem he causes is someone else's to solve.
So on this topic.
Which in general is the process for the bigger question of safety.
How do you provide that layer that protects the model from doing crazy, dangerous things?
I think there will come a point where that's mostly what we think about the whole company,
and it won't be like, it's not like you have one safety team.
It's like when we shipped GPT-4, that took the whole company thing
with all these different aspects and how they fit together,
and I think it's going to take that.
More and more of the company thinks about those issues all the time.
That's literally what humans will be thinking about,
the more powerful AI becomes.
So most of the employees that open AI will be thinking safety,
or at least to some degree.
Broadly defined, yes.
Yeah, I wonder what are the full broad definition of that?
What are the different harms that could be caused?
Is this on a technical level, or is this almost like security threats?
It'll be all those things, yeah.
I was going to say it'll be people, state actors trying to steal the model.
It'll be all of the technical alignment work.
It'll be societal impacts, economic impacts.
It's not just like we have one team thinking about how to align the model,
and it's really going to be like getting to the good outcome
is going to take the whole effort.
Okay, so at some point, the safety crisis will be so urgent
the entire company will need to deal with it.
Hear him, believe him.
The thing he's making will be very unsafe,
and at an unknown point, the guy who says,
don't trust me wants us to trust him that he will at the exact right moment
know to throw the whole company into safety work.
But just not quite yet.
Cool, seems like a plan.
Okay, this next one is wild.
So I spoke to a group of coders about AI risk last week or week or so ago
outside of Philadelphia, and the number one question they had was,
how will AGI escape?
How does it go from the digital world to the physical world?
Hiring humans is an easy way.
There's lots of others, but here's an even easier way.
Go tell them, Sam.
Will we see humanoid robots or humanoid robot brains from open AI at some point?
At some point.
How important is embodied AI to you?
I think it's sort of depressing if we have AGI, and the only way to get things done in
the physical world is to make a human go do it.
So I really hope that as part of this transition, as this phase change,
we also get humanoid robots or some sort of physical world robots.
Okay, let's add up the tab.
Sam can't control his AI.
Sam doesn't understand how his AI works.
Sam admits his AI can end all life on Earth.
Sam is clear.
You should not trust him.
And one more.
Sam would be depressed if his AGI is not put in to robots.
Whoever builds AGI first gets a lot of power.
Do you trust yourself with that much power?
When people show you who they are, believe them.
Yes.
Look, I was going to, I'll just be very honest with this answer.
I was going to say, and I still believe this, that it is important that I,
nor any other one person, have total control over OpenAI or over AGI.
And I think you want a robust governance system.
I can point out a whole bunch of things about all of our board drama from last year,
about how I didn't fight it initially and was just like, yeah, that's, you know,
the will of the board even though I think it's a really bad decision.
And then later, I clearly did fight it and I can explain the nuance and why I think it was
okay for me to fight it later.
But as many people have observed,
although the board had the legal ability to fire me in practice, it didn't quite work.
And
that is its own kind of governance failure.
Now, again, I feel like I can completely defend the specifics here.
And I think most people would agree with that, but it,
it does make it harder for me to like look you in the eye and say, hey, the board can just fire me.
Believe them, they know themselves much better than you do.
Okay, Sam, one last clip, please, most important person on earth, give us some reassurance.
Are you afraid of losing control of the AGI itself?
That's a lot of people who worry about existential risk, not because of state actors,
not because of security concerns, but because of the AI itself.
That is not my top worry.
As I currently see things, there have been times I've worried about that more than maybe times,
again, the future where that's my top worry.
It's not my top worry right now.
What's your intuition about it not being your worry?
Because there's a lot of other stuff to worry about, essentially.
You think you could be surprised?
We, for sure, could be surprised.
Like saying it's not my top worry doesn't mean I don't think we need,
like I think we need to work on it super hard.
I think we need to work on it super hard.
Um, are you fucking kidding?
That is not at all the same as we are working on it super hard.
Two very different things.
Sam Altman wants someone else to make his tech safe.
Sam Altman wants someone else to regulate his tech.
Sam Altman even wants someone else to figure out how to compensate
the people whose training data was stolen to build his Death Star.
No one elected Sam Altman.
No one has vetted Sam Altman.
But he is in charge of whether or not your family lives or dies.
No government oversight.
Motivated by profit, ego, a voyage need to build cool stuff,
and an unrequested desire to change how humans live fundamentally
with no plan as fast as possible.
That is why I believe we need to stop Sam Altman and all those like him.
Okay, but here's the thing.
The case against Sam and his peers is not hitting home with the general public yet.
I want to share a story with you.
One of my closest and oldest friends on the planet,
and I had an exchange this week that I think you can learn from.
After a catch-up Zoom that unexpectedly drifted far into AI risk,
he told me in an email follow-up that he believed everything I said.
So I, after some hesitation followed up,
I just wanted to know really to help better understand people
why if he believed that his three young kids' lives are threatened by AI,
why he has not taken action yet,
why he doesn't feel compelled to do things like I feel compelled to do things.
And here's what he wrote back,
three bullet points that I think are so, so common.
I bet 90% of the inactive public would say the same three points.
He wrote,
Honestly, I don't have a good excuse.
Number one, I'm very busy with work and kids and life.
Number two, I'm only one person with zero influence on companies,
individuals, and lawmakers who can impact what's happening.
And number three, I'm hopeful that Doom is not imminent
and that in the end, good outcomes will prevail.
That is literally what those of us who desperately want to spread
AI risk awareness and see action come from it are up against.
So on one hand, I was depressed at my first reading.
Those three points are so weak.
We're all busy.
We can all have influence and hope is not nearly enough.
But then I thought, wait a minute, those three points are so weak.
What we are up against is these three things again,
very busy with work and kids and life.
I don't have influence.
I hope that we get a good outcome.
The weaknesses in each of those is not our enemy.
It's our ally.
These are three winnable arguments.
So as you make the case for AI risk, keep those three things in mind.
That is what you and we are fighting against.
It's winnable.
Okay, friends, it's 2024.
AGI is coming at some point.
We don't know when.
We don't know how long we have to live.
So we celebrate every moment of every day like it could be our last.
Call it the celebration of life.
For this week's celebration of life,
I want to share something that really moved me as a dad.
There's not much that makes me feel thrilled to be alive
more than incredible vocal tone on a singing voice.
When my daughter first shared with me Billy Eilish,
I was immediately blown away by her vocal tone.
And then as a dad of Boy Girl Twins,
it was so cool to learn that her brother Phineas
is her producer and musical creative partner.
So I came across a performance that just really blew me away.
The two of them doing their song,
What Was I Made For at the Grammy Awards, a live performance.
And as a dad of a boy and a girl and a fan of vocal tone
and songwriting, it just floored me.
They are simply incredible.
And this song has some haunting reflections on our AI risk debate.
Please enjoy. This is so good.
But I'm not sure now what I was made for.
What was I made for?
Taking a drive, I wasn't ideal, looked so alive.
It turns out I'm not real.
Just something you've paid for, what was I made for?
Because I, I, I don't know how to feel, but I wanna drive.
I don't know how to feel, but someday I might, someday I might.
When did it end, all the enjoyment?
I'm sad again, but don't tell my boyfriend.
It's not what he's made for, but what was I made for?
Because I, because I, I don't know how to feel, but I wanna drive.
I don't know how to feel, but someday I might, someday I might.
I think I forgot how to be happy, something I'm not, but something I can't be.
Something I'll wait for, and something I'm made for, something I'm made for.
Just beautiful.
Okay friends, that is all for this week.
Next week's show is going to be a little while, so get ready for that.
For Humanity, I'm John Sherman.
I will see you right back here next week.
We have so much work to do.

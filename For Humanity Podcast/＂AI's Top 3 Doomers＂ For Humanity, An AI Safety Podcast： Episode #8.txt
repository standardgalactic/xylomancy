GPT-4, you know, not a risk like you're talking about there, but how sure are we that GPT-9 won't be?
How sure are we that Chat GPT-9 won't murder your whole family and mine, he asks?
The bad case, and I think this is like important to say, is like lights out for all of us.
The real doomers are the ones that could bring the fucking doom, yeah?
You know, my chance that something goes, you know, really quite catastrophically wrong on the scale of, you know, human civilization, you know, might be somewhere between 10 and 25%.
Who the fuck do you think you are?
Who the fuck do you think gave you permission to endanger the lives of 8 billion of your neighbors?
Seriously, who in their right mind would take a job at a company where your work creates a 10 to 25% chance of killing their family and all of their neighbors?
Oh, and all the rest of the people in the whole world too.
Yeah, I think it's like impossible to overstate the importance of AI safety and alignment work.
I would like to see much, much more happening.
Oh wow, Sam should try to find somebody who knows somebody who's in charge at OpenAI who can fix this.
Come on.
Some young person goes to a school and starts shooting people.
You blame video games, right?
Back in the old days, people would blame comic books, they would blame jazz, they would blame TV, they would blame movies.
Yeah, yeah, it's just like video games and school shootings, it's just like comic books or jazz.
Get the fuck out of here.
But who determines the goal?
We do.
You know better than that.
You know what the godfather of AI Jeffrey Hinton says about goals and sub-goals?
You know that a human can set goals and AI systems must then set sub-goals to achieve those first goals.
And you know those sub-goals when unaligned with human values and obscured from view hidden in a black box system as they are,
could kill us all.
Welcome to For Humanity, an AI safety podcast, Episode 8, AI's Top 3 Doomers.
I'm John Sherman, your host.
Thank you so much for joining me.
This is the AI Safety Podcast for the general public.
No tech background required.
This podcast, as you know, is solely about the threat of human extinction from artificial intelligence.
Please hit like and subscribe, tell a friend about the show.
And if you're on X, follow us at For Humanity Pod and leave a comment if you will.
I really, really appreciate those comments and reach out to me by email if you'd like to at For Humanity Podcast at gmail.com.
I'd love to hear from you.
Many of you have been reaching out to me and I want to say thank you for doing that.
I'm building a community of people here, a community of people who are unafraid to be ambassadors of AI extinction risk.
In full candor, this show is a lot of work and every time one of you reaches out to me with some words of encouragement, it means a great deal.
It really does.
So thank you.
I'm going to end today's show on something really positive, new and exciting coming up for 2024.
First, the moment we are in.
Every day, more and more people are waking up to the grim truth.
A bunch of 30-somethings in Silicon Valley are in a race to build technology that can cure all of our problems or kill us all before it has a chance to do that.
Those both sound crazy, but they are both literally true.
The future, utopia or doom, will be decided by us this decade.
Everything I've just said is almost consensus view among most of the AI thought leaders of all different stripes.
I'm not religious at all, but I was thrilled this week to see Pope Francis come out with a forceful call for AI safety.
Not sure if you saw it, check out this Washington Post headline,
Warning of Risk to Survival, Pope Calls for Global Treaty on AI.
Wow.
You gotta love it.
In the Pope's statement on AI, he said,
I think very importantly and on target for our show topic today, the Pope wrote,
Artificial intelligence is not the responsibility of a few, but of the entire human family.
Again, I don't believe in religion, but amen, Pope Francis, he is right.
A handful of tech people responsible only to their shareholders cannot decide our survival.
There are nearly 1.5 billion Catholics on Earth, Catholics of the world, hear Pope Francis.
He's saying human survival is at risk from AI.
Let's be clear.
This is not metaphorical.
He doesn't mean something like survival, but nicer.
The leading academic researchers and the leaders of all the big tech AI companies are openly talking about their work,
killing everyone on Earth,
no survivors.
That is literally what they are talking about.
And the consensus of all the leading academic researchers on this subject is it's very likely,
if it happens to happen, that it will happen within the next 2 to 10 years.
If every Catholic believed the Pope's dire warning, that would be truly wonderful.
If every major world religious leader came out with a similar statement of grave concern,
that would be even better.
What a potentially powerful, immediate impact that could have on global, worldwide opinion.
AI could still cure cancer and poverty, solve climate change,
even if we only developed narrow artificial intelligence.
But we're not doing it that way.
We're in a race to build artificial general intelligence,
an AI that's better than humans in a wide variety of skills and areas of expertise.
We are racing towards our own doom.
That's what most of the leading AI safety researchers are saying.
That's what the godfathers and creators of AI are shouting.
They're saying slow down.
They're saying stop.
But the race continues.
A handful of people are taking risks for the futures of 8 billion people without any consent whatsoever.
Civil society is based on consent, right?
You can't touch someone without their consent.
Every time you sign something all day long and you're signing away your consent,
we do it for the most trivial things and the most important things.
If I want to cut down a tree in my yard that affects my neighbor's yard,
I need to get my neighbor's consent.
The big AI companies have no fucking right at all to do what they're doing without anyone's consent.
We, the public, have not given them our consent for this, at all.
So on today's show, the top three doomers in AI.
We're going to name names, point fingers, and lay down the blame.
These three men are unconsensually risking doom for you, your family, me, my family,
and every other living thing on this planet.
The term doomer is misused.
AI risk deniers are the real doomers.
Let's be very clear.
Dumerism equals AI risk denialism.
The doomers are not the AI safety researchers like Eliezer Yudkowski and Max Tegmark,
or the converts like Jeffrey Hinton and Yoshua Benjiro.
Those are heroes trying to save us.
The real doomers are the ones that could bring the fucking doom.
Yeah?
Yeah, that makes a whole lot more sense to me.
So on today's show, the top three doomers in AI.
These are the fucking assholes who are risking our doom.
They're leading the charge to make it happen as fast as possible.
So coming in at number three, the number three doomer in AI,
it's anthropic CEO Dario Amadea.
Dario's story is heartbreaking for those gravely concerned about AI risk.
It's a tale of a life rooted in good intentions,
tragically broken, corrupted by market forces, ego, and greed.
Dario Amadea, he worked at OpenAI from 2015 to 2020,
leading the teams that built ChatGPT2 and ChatGPT3.
He is, by all accounts, an incredible computer scientist,
and honestly, he seems like a nice, very likable guy in the interviews you're about to see.
He even runs his company with his sister.
Aw!
In 2020, though, Dario left OpenAI with some others to start their own company, Anthropic,
that would be far more focused on safety than OpenAI.
Today, Anthropic has only 160 employees,
and it is widely considered the third most important AI company on the planet,
just behind Microsoft's OpenAI and Google's DeepMind.
AI safety was prized at Anthropic originally.
The company was all about safety in a meaningful and genuine way,
but things changed.
Today, Anthropic has 160 humans,
whose daily work threatens the total slaughter of their 8 billion neighbors.
It was a Long New York Times article about Dario and Anthropic in July.
In it, company leaders say to work on safety at the frontier level,
you need to build systems that are at the dangerous frontier level.
I believe there is some truth in that.
So they are doing it.
They got a $5 billion investment to build an AI model
10 times more capable than today's most capable models.
That is quite literally the dangerous frontier work,
the 1% that could kill us all.
10 times smarter than today's AI could well be AGI,
pushing us past the singularity.
But maybe it won't.
Here's Anthropic CEO Dario Amadeyi on the Logan Bartlett podcast,
a great show on how Anthropic changed after OpenAI released chat GPT to the general public.
I didn't want our first act on the public stage after we put so much effort
into being responsible to accelerate things so greatly.
I generally feel like we made the right call there.
I think it's actually pretty debatable.
There's many pros, many cons, but I think overall we made the right call.
And then certainly as soon as the other models were out
and the gun had been fired, then we started putting these things out.
We're like, okay, all right, now there's definitely a market in this.
People know about it, and so we should get out ahead.
And indeed we've managed to put ourselves among the top two or three players in this space.
Was that gun being fired and chat GPT taking off?
Was that similar to the fear that you had of, hey, this might start a race?
Yeah, yeah, similar and in fact more so.
I think we saw it with Google's reaction to it,
that there was definitely just judging from the public statements,
a sense of fear and existential threat.
I think they responded in a very economically rational way.
I don't blame them for it at all, but you put the two things together
and it really created an environment where things were racing forward very quickly.
And look, I love technology as much as the next person.
There was something like super exciting about the whole make them dance.
Oh, we're responding with something.
I can get just as excited about this as everyone,
but given the rate at which the technology is progressing,
there was a worrying aspect about this as well.
And so in this case, I'm at least on balance, Clad,
that we weren't the ones who fired that starting gun.
So he left OpenAI to start a new safer lab,
but then once OpenAI released chat GPT,
he gave up on being safer.
He openly admits this and he joined the arms race.
Right there at the end of that sound bite we just played,
he says he gets some consolation for not being the one to fire the gun
to start the AGI arms race.
That is pathetic.
That's called caving in.
When the AI safety company said,
fuck it, we need to join the arms race or be left behind so we're in.
Humanity's chances took a big hit.
What happened at Anthropic is a sickening window
into the market forces at play here.
Listen to me.
The rush to make money and be first is far more important
to the human race than ensuring survival.
Full stop.
That is simply the fact of how we are currently living.
That is our status quo.
That is what the actions of our society
currently undeniably show.
Our plan is to be so rich and so dead.
So why is Dario Amadei,
the number three ranked AI doomer in the world,
he's making the doom happen.
We played the sound bite I'm going to play for you now.
A few shows ago.
Listen to more of it here now.
Here's Dario with Logan Bartlett giving the percentage chance
the P doom that the work he's doing ends all life on earth.
You think about percentage chance doom or.
Yeah, I think it's popular to give these percentage numbers.
And you know, I mean, the truth is that I'm not,
I'm not sure it's easy to put to put a number to it.
And if you forced me to, it would, it would fluctuate all the time.
You know, I think I've, I think I've often said that, you know,
my, my chance that something goes, you know,
really quite catastrophically wrong on the scale of,
of, you know, human civilization, you know,
it might be somewhere between 10 and 25% when you put together
the risk of something going wrong with the model itself with,
you know, something going wrong with human, you know,
people or organizations or nation states misusing the model
or, or it kind of inducing conflict among them or,
or just some way in which kind of society can't, can't handle it.
That, that said, I mean, you know, what that means is that
there's a 75 to 90% chance that this technology is developed
and, and, and everything goes fine.
If you don't believe that AI can kill us all very soon,
ask yourself, what is it that you know that the CEO of the third
most important AI company on earth doesn't know?
If someone in your life doesn't believe that AI can kill them,
play that sound bite for them.
Here's Logan Bartlett asking a great question and Dario,
not answering it at all.
It's like he doesn't even see the question.
There was an box article that said something to the effect of
an employee predicted there was a 20% chance that a rogue AI
would destroy humanity within the next decade to the reporter,
I guess that was around.
Is, I mean, does all this stuff weigh heavily on the organization
on a daily basis or is it mostly consistent with a normal start
up for the average employee?
Yeah, so I don't know, I'll give my own experience
and it's kind of the same thing that I, you know, that I recommend,
that I recommend to others.
So, you know, I really freaked out about this stuff in 2018 or 2019
or so when, you know, when I first believed that, you know,
turned out to be, you know, at least in some ways correct,
that the models would scale very rapidly and, you know,
they would have this importance to the world.
I'm sure Dario Amadei is a lot like the few thousand people
working on the most dangerous frontier AI capabilities work.
Incredibly smart, probably even well-intentioned,
but completely blind to the notion of consent.
There can be no such thing as informed consent in this,
not even the big AI companies know what AGI will do, when, or how,
and yet they race on, never even attempting to get anyone's consent.
Dario mentions he knows the importance of his work to the world.
So to be blunt, Mr. Amadei, who the fuck do you think you are?
Who the fuck do you think gave you permission to endanger the lives
of eight billion of your neighbors?
Seriously, who in their right mind would take a job at a company
where your work creates a 10 to 25% chance of killing their family
and all of their neighbors, oh, and all the rest of the people in the whole world too?
What the fuck are we talking about here?
Who the fuck do you think you are, sir?
Okay, I'm gonna breathe for a moment.
On to the number two doomer in AI.
Oh man, this guy is just the absolute worst.
The number two doomer in AI is Suicide Cult Leader
and Director of AI Research at META, Yan Lacoon.
Yan Lacoon is simply stunningly awful.
Here is Yan at the World Science Festival in late November.
You can have a very intelligent system that has no desire to dominate at all.
And so the way we will design those systems is to be smart.
In other words, you give them a goal, they can solve that goal for you.
But who determines the goal?
We do.
You know better than that.
You know what the godfather of AI Jeffrey Hinton says about goals and sub-goals.
You know that a human can set goals and AI systems must then set sub-goals
to achieve those first goals.
And you know those sub-goals, when unaligned with human values
and obscured from view hidden in a black box system as they are,
could kill us all.
But Yan Lacoon, this NYU professor and META department leader
somehow has enough spare time on his hands to sit on ex-formerly Twitter
and spew AI extinction risk denial bile through a fire hose.
He not only denies any AI extinction risk whatsoever,
he bullies and makes fun of anyone who does, calling them doomers,
calling AI ex-risk sci-fi movie stuff, James Bond movie stuff and more.
If I had Yan Lacoon in a public debate, I would shut him down with one question.
Yan, let's say you build AGI and let's say your interpretability sensor,
which doesn't even currently exist, tells you that this AGI,
which is smarter than any human, is plotting to kill you.
What do you do next?
There is no answer. The only answer is you die.
So let's not get there, right?
But Yan isn't here for the hard questions. His go-to move is openly disingenuous.
I have to believe he's too smart to know this isn't bullshit.
Yan Lacoon always reaches for historical precedent.
Oh, AI is just like the typewriter. It's just another printing press moment.
But as you know, there is nothing in human history, absolutely nothing remotely,
like the invention of AI and the coming of AGI,
and any attempt to find a historical comparison is genuinely dangerous.
Just listen to the total bullshit that comes from this man's mouth.
It's very easy to attribute cultural phenomena and social phenomena
to the new thing that just happened, right?
So if some young person goes to a school and starts shooting people,
you blame video games, right?
Back in the old days, people would blame comic books, they would blame jazz,
they would blame TV, they would blame movies, novels.
The story goes back centuries.
Whenever there is a new cultural phenomenon, whenever there is an effect on society,
you blame the latest technology that appeared, particularly communication technology.
So it's natural to blame, for example, social networks,
not just Facebook, just any social networks for political polarization.
That seems natural. People shout at each other on social network.
That necessarily polarizes people. A very natural thing to do.
That turns out to be completely false.
Yeah, yeah, it's just like video games and school shootings.
It's just like comic books or jazz. Get the fuck out of here.
At the World Science Fair, the moderator asks Yana a great question,
and his answer is just numbing, completely numbing.
I do not understand this.
For you to say, hey, let's really slow this down.
Is there anything that would happen from these AI developments
that would cause you to say, I want to slow this down?
So you want me to imagine a scenario that I don't believe can happen?
Yeah.
Okay.
So it's kind of funny because we're on a panel with a 60-something advocating for
progress against a 30-something advocating for conservatism.
Isn't that paradoxical?
Anyway, I mean, we can imagine all kinds of catastrophe scenarios.
Every sort of James Bond movie with a supervillain has some sort of catastrophe scenario
where someone turns crazy and wants to eliminate humanity to recreate it to something
or take it over the world.
All of science fiction is full of that. That's what makes it funny and interesting.
I don't know if you noticed in those clips, the facial expression,
the guy with the beard two seats down from Yan Lacun.
This guy is amazing.
You're about to meet a new AI safety hero, new character alert.
Please say hello to AI ethicist Tristan Harris.
Find his stuff on the internet.
He really is incredible.
Follow him.
Listen to Tristan Harris.
Due to Yan Lacun, what Lacun always tries to do but fails.
Tristan finds a historical reference that actually makes sense.
If you keep the scaling model, the scaling laws going and you start to get AI
that starts to automate scientific processes where it's generating its own hypotheses
and it has its own lab and starts to test those hypotheses
and you start getting that kind of AI.
You start getting AI that's making its own scientific discoveries.
And that, when you have it going this fast, it feels like her metaphor was
it's like the 24th century crashing down on the 21st century.
And a metaphor for you is imagine that the 20th century tech
was crashing down on 16th century governance.
So, you know, it's the 16th century.
You've got the king.
You've got their advisors.
But suddenly, you know, television, radio, the telegraph,
you know, video games, Nintendo and well,
thermonuclear and thermonuclear weapons all show up.
So they just land in your society.
Yeah.
But you're like, call the knights, you know, and you get and the knights show up
and you're like, what are you going to do?
What are we going to do when technology hundreds of years
advanced beyond our current moment is suddenly thrown in our hands?
Nobody has any fucking clue.
Just imagine medieval kings and knights
waking up one day with nukes and cell phones.
The odds that I'd be sitting here today
talking to you like this after that seem very slim.
Which brings us to the number one doomer in AI.
This was not a hard choice.
It's the guy who did everything the original founders of AI
swore they would never do.
He chose to release his general AI models open source.
He chose to release them on the open Internet.
He chose to release them with the ability to write their own code.
This man's fingerprints are on the handle of the Pandora's box
that is now open and threatening to murder everyone you've ever met.
And also everyone you've never met to.
He is the human starting gun in our profit fueled global race to mass suicide.
The number one doomer in AI is open AI CEO Sam Altman.
Number one doomer Sam Altman is the most dangerous human to ever live.
I could and will tell you about why but he does a great job all by himself.
Right here in this famous quote from January of this year.
Right after he spent a minute detailing the wonders of the best case scenario for AI.
Sam drop this.
Bad case and I think this is like important to say is like lights out for all of us.
I could end my case for Sam being the number one doomer right there.
Who the fuck works on something they legitimately think could kill everyone on earth.
In a highly controlled governmental military framework weapons of mass destruction are designed and built.
Sure, this is not that.
The sale of a fucking egg is far more regulated than AI right now.
And Sam Altman of Missouri believes he has the right the morning right to do the work he's doing.
How is that fucking possible Sam you are the bringer of the doom Sam you are the fucking doomer.
Not EleIzer not Max not Connor not Paul not Roman you Sam Altman are the number one doomer in AI.
On that same January day Sam went on to talk about his safety concern being mostly with misuse.
And then he takes a full pot shot at the AI safety community just listen.
I'm more worried about like an accidental misuse case in the short term where you know someone gets a super powerful like it's not like the AI wakes up and decides to be evil.
And I think all of the sort of traditional AI safety thinkers reveal a lot about more about themselves than they mean to when they talk about what they think the AGI is going to be like.
But but I can see the accidental misuse case clearly.
I'm sorry. Right there the number one doomer in AI had the gall the audacity to take a shot at what he calls this sort of traditional AI safety thinkers.
His burn is somehow that they reveal something clearly off about themselves when they talk about what AGI is going to be like.
Does he mean that Elie Izer Yadkowski and Max Tagmark think AGI is going to be bad and mean because they're really bad and mean deep down inside.
What the actual fuck is he even talking about.
AGI will be simply like every other intelligence anywhere that's not human.
It won't share human goals and values.
One more clip from that event here Sam bemoans the need for more safety and alignment work.
From the guy who is the CEO of the leading AI company on Earth, a company that puts its money 99 cents on the dollar on capabilities and one cent on safety work.
Yeah, I think it's like impossible to overstate the importance of AI safety and alignment work.
I would like to see much much more happening.
Oh, wow. Sam should try to find somebody who knows somebody who's in charge at Open AI who can fix this.
Come on. Honestly, it's so pathetic.
Sam Altman, the world's most dangerous man and number one doomer is bankrolled by Microsoft.
At Dev Day for Open AI, Microsoft CEO Sacha Nadala casually dropped in on his boy wonder, Sam the doomer.
This is a sickening 20 seconds of video.
In it, you'll see Sacha Nadala literally say, lastly, safety is super important because you know you always leave the most important things for last.
And then you'll see Sam Altman say the doomiest words thus far uttered in human history.
And then the last thing is, of course, we're very grounded in the fact that safety matters and safety is not something that you'd care about later,
but it's something we do shift left on and we're very, very focused on that with you all.
Great. Well, I think we have the best partnership in tech. I'm excited for us to build AI together.
No, I'm really excited. Have a friend. Thank you very much for coming.
It really is too much to take. He knows his work. His choices may kill us all and yet he continues.
Sam Altman was asked a great question about just this at a recent Bloomberg event.
He signed a 22 word statement, warning about the dangers of AI.
It reads, mitigating the risk of extinction from AI should be a global priority alongside other societal scale risks such as pandemics and nuclear war.
Connect the dots for us here. How do we get from a cool chat bot to the end of humanity?
Well, we're planning that too. That's the hope. But there's also the fear.
I mean, I think there's many ways it could go wrong, but we work with powerful technology that can be used in dangerous ways very frequently in the world.
And I think we've developed over the decades good safety system practices in many categories.
It's not perfect. And this won't be perfect either. Things will go wrong.
But I think we'll be able to mitigate some of the worst scenarios you could imagine.
You know, bioterrorists like a common example, cybersecurity is another, like many more we could talk about.
But as this technology, like the main thing that I feel is important about this technology is that we are on an exponential curve and a relatively steep one.
And human intuition for exponential curves is like really bad in general.
It clearly was not that important in our evolutionary history.
And so I think we have to, given that we all have that weakness, I think we have to like really push ourselves to say, OK, GPT-4, you know, not a risk like you're talking about there.
But how sure are we that GPT-9 won't be?
How sure are we that chat GPT-9 won't murder your whole family and mine, he asks.
That is why Sam Altman is the number one doomer in AI.
He's not sure that his own product, a few versions from now, won't slaughter my family, your family, every family, and all their pets too.
He says maybe it's GPT-9 that's a lethal threat.
AI safety researchers say it could be GPT-5 or 6.
Are we really just debating when not what?
Is the only question left what version kills us?
Have we really just conceded the doomers can make their deadly weapons systems that will no regulation, no oversight?
No, we have not conceded that we have fucking not.
OK, so it is almost the end of the year.
I've been making these shows for eight weeks straight and it has honestly been as fulfilling as it has been hard work.
I am so excited what we can do with this podcast together in 2024 as we build this movement and this community.
It starts with each one of us convincing others one at a time that this is the only issue that really matters anymore.
So for 2024 I have big plans for humanity.
I have so many show ideas and this story is going to unfold in real time so we'll be reacting in real time as well.
Let us build a community of people here who are unafraid to plainly convincingly evangelize for AI existential risk awareness.
Let's use each other as resources, helping each other get better at convincing others by sharing our experiences.
It's not easy to convince people and it's not fun and we could all be way better at it.
OK, finally, I'm going to take a few days to put together the next show just to spend a little extra time to be with my family over the holidays so the next show will come out right after the new year.
So I want to end with a holiday message.
2023 was a year of shock and disbelief.
In 2024, let's get our optimism back.
Let's get our joy back.
If the worst is to come, I want to spend every second enjoying life as much as humanly possible.
I will not let these doomers rob me of my time or of my joy.
Let's make 2024 a year of celebration and loving life.
Let's celebrate what we love about being humans.
Let's celebrate what we love about living on Earth.
Let's spend time with our friends and family more than ever before.
Take that vacation, go out to that dinner.
Let's live like every second is precious.
And while we celebrate life and humanity, let's kick and scream and fight like crazy to survive.
So each show next year will end with a celebration of something that is wonderful about life.
I'm going to share from my life and I hope you'll share from yours.
So that gives me something big to smile about and something I'm really looking forward to sharing with you in 2024.
Please have a wonderful holiday season and a happy new year, whatever, wherever you celebrate.
Hold on to your loved ones close.
For Humanity, I'm John Sherman. I'll see you back here next year.
Thank you.

When I think about persuading people or giving information, I really come from this is like,
how can I help you? I really, really come in my mind and my heart. When I try to, I'm like,
I'm not here to hurt you. I'm not here to change your mind in a way you don't want it to be changed.
Because this is a thing a lot of people are scared of. A lot of people are scared of having
their mind changed in ways they don't want it to be changed. This is very common. This is very,
and so an important way to be is like, I'm here to help.
Welcome to For Humanity, an AI risk podcast, episode number 54, Connor Leahy interview.
I'm Jon Sherman. Thanks so much for joining me. I'm just a dad in Baltimore who believes the big
frontier AI labs like open AI, when they openly publicly admit their technology can kill my kids,
that they do not understand how to control it, that they do not understand how it works,
and that they spend all their time and money making it stronger, not safer.
For Humanity is the AI risk podcast for the general public. No tech background required.
This podcast is solely about the threat of human extinction from artificial intelligence.
Please, I need you to hit like, subscribe, share, donate, leave a comment.
And we have in the show notes monthly donation subscription links. Please help me help the
general public understand AI risk. Today's interview is the most important on this show
to date. I'm really excited to share this with you. When I first went down the rabbit hole of
AI risk a year and a half ago, there was one voice, one AI safety leader whose tone
rang truest to me. I met lots of experts in the rabbit hole of AI risk who told me our default
future on our current course was eminent human extinction from AI. But that means killing every
living thing on earth, all the dogs, the babies, the flowers, the whales, the elephants, your mom,
my mom, my kids, your kids, your friends, my friends, all of it. And I could not figure out why in the
AI risk rabbit hole, no one was pissed off about this. Until I came across Connor Leahy through
dozens of podcasts and web videos. Connor Leahy is the CEO of Conjecture, a company in London
working on AI alignment, not building AGI. Connor Leahy is one of the most important people living
on the planet earth. And it was my great honor to talk with Connor for more than two hours
from my hotel room in Austin, where I was visiting my son for parents weekend at UT.
I have one huge takeaway from the conversation that changed how I talk about AI risk the moment
I heard Connor say it. Ready for this? We're not here to wake up the world to AI risk or
convince people about AI risk. That presumes there's a controversy that frames AI risk as if
there are two sides, but there aren't. There just aren't. Nearly every qualified AI leader,
both corporate and academic, openly admits that frontier AI development, making leading edge models
bigger and smarter, threatens to kill us all quite literally and quite possibly very soon.
This is not controversial. So what Connor says is now my motto. He says we're here to help people
understand what is to come if we don't change course. We're here to help people understand
what is to come. There are not two sides to this debate. There is only one future to change.
This approach is a far easier approach for the people we're trying to help digest and absorb AI
risk than, hey, let me convince you about this crazy thing that you've never heard about that's
so crazy and all the different experts have varied opinions on. There is some variance in the experts
opinions about our AI future, but there's much more agreement. It's pretty simple. Build something
smarter than yourself at your own great peril. This is not controversial. This is beyond obvious.
Stephen Hawking understood this decades ago. Before we go to the interview with Connor,
please watch this short two minute video from my brother in our cause, Michael, at lethalintelligence.ai.
Links in the show notes to his channels. Listen to Stephen Hawking's own words from the past about our future.
That AI may replace humans altogether. If people design computer viruses, someone will design AI
that improves and replicates itself. This will be a new form of life that outperforms humans.
We all have to look at ourselves to see how intelligent life might develop into something.
We will not want to meet. If a superior alien civilization sent us a message saying,
we will arrive in a few years. Would we just reply, okay, call us when you get here?
We will leave the lights on. Probably not. But this is more or less what is happening with AI.
The development of all artificial intelligence cut off the end of the human race.
Stephen Hawking knew it well before chat GPT anything. It has been obvious and not controversial
that building artificial superintelligence is existential suicide. There's nothing new here.
Conor Leahy knows this too. So without any further ado, here is my interview with the great
Conor Leahy, CEO of Conjecture. Hello. Conor, how are you?
Good. Doing great. How are you? Good. Good. It is such a pleasure to meet you, man. I
might be your biggest fan. Oh, thank you. That's very kind of you.
Yeah. I'm so glad to be here. Max spoke very highly of you. You've been doing some great work,
so pleasure if I can help. Awesome. Awesome. Yeah, I'm like a normie. I'm a general public
person who came into this thing like a year and a half ago totally unaware of it and became aware
of it and have really just been compelled to work on it really hard core since then.
It's great. Yeah. So I wanted to start asking you, man, just off the top, as a human, how are
you? How are you doing? Actually, very good. I think I'm probably at the happiest I've been
in my life for many reasons. Obviously, the world is not in a good state, but I'm doing well,
you know, good health, good relationship with my family and my friends. I get to work on stuff
that's very meaningful to me. I have a wonderful group of people. I'm in my office right now,
so I'm personally in a very good state. I'm doing very well. I feel like I have kind of like
grieved about these things enough. And like quite a while ago, so these things don't
distress me anymore. It's kind of like the thing is like, you know, why aren't you worrying? And
I'm like, would it help? Like, you know, I can. And like, you know, sometimes you feel right,
sometimes you have a, you know, things are bad or something bad happens or whatever, and you feel
a bit bummed about it or whatever. But like, for the most part, it's just like, you know,
I have my ways with coping with it. And I still recommend them always to other people, but just
like this kind of stuff I was, I've always felt like I was going to do. And I don't, you know,
in another lifetime, I would have, you know, worked on nuclear weapons or, you know, some
kind of like, you know, big civics projects or something, right? It's just like we're going to
war, right? But like, here I am. Yeah. And when you say you feel like you've grieved like,
and that you went through a period of grieving or something, you know, what were you grieving?
Like, like, you know, I think I have, I have like good days, good weeks. And then I have
sometimes where something happens in the news or something, it just kind of like feels like it
really hits you hard. And it's like, oof, man, it really is really fast.
This is a good question. I think the right answer is something like
that there is no God. I think that's like the real thing, like the kind of like,
not just atheism, but like deep atheism. Like there's one thing where it's like atheism,
like the literal fact of not believing there is a God. But there's also kind of deep atheism of
like, there is no one that is going to come to save you. Like, like there is that the world
is just mechanistic. There's just parts, there's, it's just things just happen. It's just physics.
And like, truly grieving this, like, so like, you know, I was an edgy atheist when I was a teenager,
right? I was like, oh, these Christians so dumb, blah, blah, blah, right. But the truth is that
I was a techno optimist. I was like, but technology will save us. And then, but then I read realize
that that's just a God shaped hole. Like I just renamed God into technology. So I didn't actually
grieve with proper atheism, with proper like, you know, there is no God, no one's going to save us.
You either do it and or not, and then you die. And like, there is kind of a sense where it's
can be like, extremely frightening to people for understanding the reasons and so on. But like,
at least for me, there was a way to grieve this, and then be like, all right, it is what it is.
And now I will act kind of like in kind of like a Taoist way, just kind of you act with the flow,
you act with the stream, it just is what it is. And you just like Stoics, like the Stoics have a
lot of ascetics around this, around just like, I feel like Stoics sometimes get a bad rap, where
like, a lot of Stoics is like, has like gotten those like macho kind of flavor, but really
Stoicism is like this deep down of just like, accepting things that you can't change for what
they are, and not getting upset about it. Wow. And so the grief was the grief was like the idea of
like, like, it's not everything happens for a reason, it's nothing happens for a reason. Exactly.
It's like, there's this deep thing where like, a lot of people you know nowadays believe in like
conspiracy theories and so on about like, you know, evil cabals ruling the world or whatever.
And the sense I think the scope, because I think the real is is that the world is not controlled
by some evil cabal of super geniuses. And actually people would prefer if the world was controlled
by an evil cabal of genius, because then at least someone's in charge, at least someone knows what's
going on, even if they're evil, at least there is a mechanism by which things happen. And I think
for a lot of people, it's even scarier to imagine that bad things happen for no reason. It would
almost be better if there was an evil villain who's doing all these evil things. At least there would
be a story, there would be logic, there would be something you can touch, something you can grab.
There'd be a guy you can punch, right? But like, most really terrible things don't really happen
for any good reason, they just kind of are. And that's a really hard thing to grieve. It took me
like quite a lot like most of my early 20s to kind of like really internalize this. Like, you
intellectually, it's easy, you just say it, you say the words, but like deeply emotionally
grieving this. And then not despairing is like very important. It's just being like,
there is no God, and that's okay. It is what it is. I will still do everything that doesn't mean
good things can't exist. There's evident like love is all around us. There are in fact,
great things that have happened, despite God not existing. And that's amazing. And we should be
thankful for this and we should be grateful for this and we should work our goddamn asses off to
make more of it. And is this a required part of it? You think that like, you know, because I often
think of like organized religion might be a place where we could get traction and AI risk awareness
could become a thing because they have meetings regularly, they have all the trappings of an
organization that's like ready to go just input the cause. And is it required that you come to
the realization that there's no God in order to truly accept where we are with AI risk?
So tech, so purely primarily speaking, no, absolutely not. This is why I often tell people,
I don't recommend you necessarily copy what I did, is that this is a, there's a way to do it,
but like, is it a scalable way? Is it the right way to do it? And another thing is,
this is going to sound weird, but many, many, many spiritual traditions and religions don't
really believe in God. Like even Christians don't really believe in God usually, they just like,
kind of believe in God. And where they believe in something like God acts through us, like a lot
of, even if you go like medieval, like Christian literature, you go back to like Catholicism from
the, even like the medieval period or whatever, a lot of how they describe God is not how we would
talk about God today as like this like weird thing out there that's all powerful. They say that,
but they also have this sense that God is kind of like, just kind of like nature or physics
is like, and that God acts through us is like good, like the king is good because he's doing
God and he is God, like God is a process rather than as a creature. And so I think there is
a lot of power in that thought of like God as a process. And I do believe versions of this
in like a purely pragmatic sense is that like, I think a lot of people when they use the word
God, they are actually talking about civilization, like the actual object that their intuitions are
pointing to is civilization. Like when they see their neighbors and they all come together and
build a town and raise children and, you know, so on the prop, they might say, wow, through the glory
of God, we've all come together. But that process is real. It's civilization, it's laws, it's norms,
it's culture. Like this is a real thing. And they're right that it's not a physical thing.
There's not a physical God. It's a software thing. It's the software of their culture that is God.
So from my experience of talking to people of faith, I found them to be, you know, in some cases,
some of the most aligned and the most reasonable. And like, like, once you explain it to them,
they're like, Oh, yeah, of course, okay, like, my friend is a is a Buddhist, he lives in a monastery
and everything. And so he recently got the chance to meet like the master of his master,
like the one of the top guys in like all of his like, oh, yeah, yeah, yeah. So he's like really,
like Tibetan monk, you know, barely leaves the country is like this like super, super high
tier, whatever, right? And he got to meet him. And he talked, he want to talk to him about AI.
And so he told him about AI. And he's like, well, you know, master, like, you know, like,
you know, what I do and like this like old Tibetan guy, and he was just like so reasonable.
I was like, well, you know, it seems like you're doing the right thing. Like, you know, of course,
meditation, spiritual item is important. But like, I'm learning from you here about these
things. And it seems like you are doing what's necessary for the Dharma. And I'm like, wow,
that is so incredibly reasonable. What an incredibly reasonable thing to believe. And to
tell you, like, yes, of course, we need to save people and like produce good things. Like, that's
obviously part of the Dharma. And like that, that is just a unbelievably reasonable thing.
So not everyone, but like many spiritual traditions, my respect for religion and
spirituality has generally increased over time rather than decreased. It started at an extremely
low point, to be fair. Yeah, but yeah, yeah, yeah, no, I feel I'm many, yes, I think there's
more to the universe, more to the world than I can see. But to me, it's like,
there's 3000 books about it. And then when it holds up one book, and it's like my books,
right? And the other 2,999 are wrong. It seems like a pretty weak spot to be in.
Yeah, I think, I mean, we could talk about this at length. I think there are it's, have you read
Yuval Harari's latest book, Nexus? I started by I'm in the middle of it a little bit. It's quite
good. And like, I like that he talks about how like, information is in my words, I would describe
information as often about coordination. So like making claims about what your God is,
it's not a claim about reality, it's a claim about factions orientation. And like, from that
perspective, I'm like, Oh, okay, yeah, I see the pragmaticism of doing that. It's a shame that
you're also throwing out, you know, empirical reality, that's a bad move. But like, I see
why you might want to do this, you know, whether, you know, we do the same thing with sports teams,
right? It's like, is your team really the best? I mean, that's not the point, right?
Yeah, yeah, absolutely. All right, so let's jump to a little bit of current events.
We have an incoming administration in America that brings with it. Certain gentlemen who
knows a lot about technology. What are your thoughts about the incoming Trump Musk administration
and what it means for AI risk? Don't really think I have any clever comments here.
I think politics is messy. I think if people, so okay, the way I think about politics in general
is kind of like that. There is making mechanisms, there are mechanisms that do explain what's
happening and like how decisions get made. But they're basically completely different from like
all the surface stories. It's like politics optimizes to present a nicely legible story of
how decisions get made, like Trump likes X. Therefore, X will happen. And this is almost
never true. This is almost never true. There is a process somewhere where like, during a dinner,
Trump has these conversations with these people who we owe as a favor for this reason. And then
because this guy is the head of this Institute and he works with this guy and
therefore this will happen. This is much closer to how real, complex systems like this work. And
I don't really have insight into the Trump administration. I don't know Trump. I don't
know Elon. I have been in a room with him once and we didn't even shake hands. I think
all things equal, I expect it to be less predictable. Like my one prediction about the upcoming
administration is that whatever happens is less predictable than the alternative. I'm not saying
that the alternative would have been better or worse. I think it would have been lower variance
in this regard. Like there is a story that people sometimes tell that like, you know,
people like Trump or Republicans or MAGA people don't care or don't understand AI or stupid.
I disagree with this. Like, sure, I'm sure there's dumb and evil people involved as on
both sides and whatever, right? Like always. But like deep down the idea that like machines
created by these like, you know, big tech behemoths, you know, are replacing human jobs and like,
you know, supplanting Americans and like eroding national sovereignty. That's like,
it's obviously like, there's obviously, you know, a Republican Trumpist perspective on this,
right? Like, so I, my main thing is, is that I think many people on, you know, that usually are
more associated with my side of things have done a terrible job of engaging with the right,
engaging with working class, engaging with Republicans, engaging with MAGA and stuff like
this, whether or not we endorse their policies, their people, they're part of our political process,
and we need to find ways to work with them. So if I could say anything to the audience here is
whatever it's like, we need to talk to them, we need to work with them, you know, if you are
someone, you know, who voted for Trump, who loves Trump, that's okay. Like, I like, we can work
together. Why couldn't we?
Like, fundamentally, I think this is a very bipartisan issue. Like, I don't think anyone on
the right or the left wants humanity to be replaced by robots, by AI. So, like, from my
extremely limited exposure to like, right-wingers and Republicans, I have found many of them to be
extremely reasonable, you know, I might disagree with them on some policies on some other things,
same way I disagree with some leftists on some policies or whatever. For the most part, I found
them to be, like, reasonable, and like, you can't work with them. But the important thing to understand
is just because someone is reasonable or like has incentive, that's not sufficient. It's why I'm
really emphasizing this idea of the process, where like, if you actually want to do change,
you kind of have to understand, and we don't have to understand, but like, you know, you should not
be, you should be expect to be surprised unless you put in the effort to actually map out the
processes, if that makes any sense. Sure, sure. And what were your thoughts around the way it
sort of went down? There was the Trump assassination happens, and that same day Elon Musk comes out
and says, I give him my full endorsement. And then I saw on Twitter, like, all of the accelerationist
Twitter, all the tech pro world in hours has moved hardcore over to there. So, a concerning element
of the Trump administration to me is just this sort of vibes of acceleration that
they picked up as they sort of got closer to the tech community. So my biggest hope is that
it's not going to be an accelerationist administration. I think there are a lot of people
in tech who want to accelerate who think that might happen.
Big tech is one of the most powerful forces in American politics today. The governor who vetoed
SB 1047 was Democrat. As again, I don't believe these simple stories, basically. Like, obviously,
this accelerationist push was coordinated. Like, obviously, these people know each other,
they go to the same dinner as they are paid for by the same people. I think the important thing
to understand is just propaganda is real. The conspiracies do happen. They're not global world
spanning, you know, satanic cults or anything, but like, just like rich people coming together
to push policy that is beneficial to them happens constantly. I think it's very clear
that a lot of these accelerations and so on benefit from lower taxes, you know,
oversight, et cetera, et cetera. And will if they find they think they have
lever to pull, they are willing to take that, which, yeah, I mean, I understand. Like,
I think the important thing is just like, I don't, again, I don't think these things
are overdetermined. It's like environmentalism used to be a right-wing thing. Like, Nixon created
the EPA. It's like, originally, when environmentalism started first becoming a thing,
leftists criticized environmentalism because they said like, oh, it's just rich people who want
their nice waterfront properties to stay safe instead of building factories for the working
class. That was like what people, at least for a while in the early 20th century thought,
that this totally flipped. Now environmentalism is a left-wing thing and like not everything. And
so like this accelerationism thing being Republican coded is like just, it's astroturfing,
is I think the correct word for this, is like there's nothing inherent about the Republican
party that makes them accelerationists. The same thing, there's nothing inherently about the Democrat
party that makes them accelerationists. It's just various forms of factional alliances, temporary
or long. Like, you know, the Democrat party is funded to an extremely large degree by
big tech companies who don't want to be regulated and so on. And now, you know, Republicans are
obviously taking massive amounts of money from big tech companies and so on and so whatever.
As I say, it's a process, right? It's a process. I think it's, yeah, I think these things are not
overdetermined. Like, I think there are just politics and war happening here. You know,
I think people are fighting for power and this has predictable or less predictable effects.
I think Musk is like a particular wild card. He obviously does care about acceleration risks to
some degree, but also he says a bunch of insane things that don't make any sense to me whatsoever
and just seem completely wildly inconsistent. So, yeah, I modeled Musk mostly as a wild card.
I think he's capable of good things and bad things and whether he does or not, I don't know.
I don't know what the process is by which Elon Musk makes decisions. This is just
truly opaque to me. I'm not saying there isn't a process. Like, you can't be that successful
and not have some process. There is some mechanism by which Musk keeps winning. If you believe it's
all dumb luck, I think, you know, I have a bridge to sell you. Like, there is a mechanism by which
Musk keeps winning and the surface properties of these mechanism seems like an idiocy to lots of
people, but I claim that can't be it. Like, there is something here that I don't understand. Like,
if someone, you know, plays a lottery and wins every single time, there's something going on,
right? It's like, I don't know how he's winning. It looks like he's playing fair, but like,
something about how he plays is making him win and me lose and like,
so my usual bet is Musk will keep winning, but I don't know what he's winning or what he's playing
for or how he will get it. Yeah. And like, kind of somewhat similar with Trump. Trump obviously
has a process that works. I don't know exactly what the process is or exactly how it works,
but clearly it works. And it's something I don't understand. If I had infinite time to spend on
politics and these kind of things, I would probably spend more time trying to understand these
processes. Sure. Okay. All right. So let's talk about tone in this conversation about AI risk.
So one of the things that I was just immediately so heartened to hear when I found you in other
podcasts and, you know, just sort of you out there on the internet was this fire and this anger of
like, fuck this, like, are you fucking kidding me? Like dropping the F bomb, being angry about it,
just, you know, a tone that is connected to the subject matter. And I think even as I try to talk
to people about it in my everyday life, you know, if I talk to a new person about AI risk,
if I come to them, like, holy shit, we're all going to die. I'm probably not going to do very
well with my introduction to the subject matter for them. If I don't come in like that, and then
they realize in eventually that's where it is, they're sort of like, well, if you're telling
me we're all going to die, why aren't you more upset about it? So we're in this sort of like
rough spot. I think you have the tone really well captured just in how you, you know,
sort of verbalize about it. But talk to me about tone, your tone and the tone of the rest of this
conversation. So I think it might be useful for me to talk a little bit about persuasion and
communication in general. Like, how do I think about communication in general,
rather than about tone specifically, it kind of kind of follows from this.
So there is, there's a couple of theories about like, how do you communicate information to other
people? And I found that most of them that like people tend to use implicitly or explicitly,
or that like written online or in books are just wrong, or like deeply misleading. And so,
like one example is that if you present people with true information, they will just accept that
and change their mind. This is not how people work. If you believe this, you are wrong, you can
scream and shout and kick your head legs that like, no, but this is how things should work,
but it is not. And get over it. Like, this is not to say, so then there's a second thing
where people when they hear that this is not true, then they conclude, oh, therefore it's okay if I
lie. This is also wrong. And like, this is even more wrong. Like, just because convincing people
involves more things than telling the truth, doesn't mean you can then stop telling the truth. So I
think this is a big mistake that people make. But they're like, Oh, okay, well, if people don't,
if truth is not enough, then I won't even try. This is also wrong. So the this again kind of gets
this like mechanistic thing that I was like, kind of like, like this, you'll see this theme emerge
many times in my thinking is like, what is the process? And what's one of the algorithms actually
being executed on people's minds when they change their mind? And let me unfold a couple examples
to explain some of the core intuitions I use when I think about communicating to other people.
An important thing is that human cognition and general epistemology is a social process.
For a lot of people, this is derogatory. I don't mean this to be derogatory. I mean,
this to be descriptive. This means is is that, let's say I tell you a fact, I tell you AI is
going to kill us. And I give you some arguments for this. And maybe you're like, well, shit,
all right, that seems like a pretty good point. I'm going to go ask my friend Dave, you know,
he's super smart. So like, let's see what Dave says. And you kind of garbled the arguments to Dave,
you kind of like, and Dave's like, looks at you stupid, like, you crazy, like, of course not.
And they're like, well, shit, all right, Dave doesn't believe it. So I must have like God know
or something. Maybe it's just not that big of a deal. So does the thing that you just did is
this stupid? I would argue not really. Like, I don't think the person is making a mistake here.
Like, if Dave is your smart friend, and you ask Dave, and you, you know, do your best to
repeat the arguments, but like Dave doesn't buy it. And then you say like, well, I guess I don't
believe it either. And I don't think this is a mistake. I don't think this is dumb. I think
this is actually a pretty good way to evaluate new information that you might not be able to
understand. So a lot of people when they talk about, oh, people just take the opinions of
their friends or whatever, say this is the super derogatory thing. Like sometimes it is, of course.
But like, I want to make point this intuition that like, this is often a very rational thing to
do is like, you're not a tech guy, you're not a maybe not even like you don't make you to consider
yourself a very smart person or whatever, or you just want to run it by a couple of people, right?
You run it by a couple of people, and they all think it's stupid. It's probably stupid, right?
So, so it's a very important thing where like, social processes matter a lot, and you have to
model them as part of your conversation. Let me give you an example of how if I want to convince,
let's say, you know, we have our, our guy Alan, we want to convince him of AI risk, right? And he
talks with friend Dave, and Dave isn't really convinced. What's the, what's the thing I could
do to improve upon situation? For example, I could tell Alan simpler arguments that he's easier to
repeat. So this is not necessarily giving stronger arguments, but I could, for example, go on the
tradeoff curve and pick an argument is easier to repeat. So if you repeat it to Dave later,
he repeats it more accurately to Dave. So therefore, it's more likely that Dave will
think, huh, that is actually a good argument. And then it's more likely that the feedback loop
so having a mechanistic process shows you different knobs, different points on the
tradeoff curve of what arguments you pick, you know, not necessary. So you don't necessarily
want to maximize it as much information as possible, because people have limited attention,
they have limited memory, people forget things, right? Especially if you just have conversation
once if you gave Alan a huge massive argument with many, many moving parts, you know, he can't
repeat that date or even to himself, he's just going to forget half of the argument by tomorrow.
So that's not necessarily the correct strategy. So this also relates to emotional valence.
So emotions are signals, their signals about reality, their signals about yourself, their
signals about other people, etc. So a lot of how you want to think about signal about emotions
is not whether they're like true or false, rational, irrational, you want to think about
are they useful? And they're an input signal and are they telling me or giving me something useful
or are they doing the things I want? For example, if I show you something really scary and I say,
wow, look how scary this is and you don't feel scared, that's signal. Like you're like, you should
pause and be like, wait, hold on, why don't I feel scared? Are you tricking me? Is there like
something here that I'm not seeing? Or if I tell you, hey, check out this epic joke, it's so funny
and you don't laugh and you're like, well, I think you might be wrong. Like it seems I'm observing
that I am not laughing. So therefore, this is not funny. And like you could say that's irrational,
actually objectively this joke was funny, but like, no, there's something here, right?
Right, it's definitely reasonable, it didn't work. Yeah, like, so the same way, if you say it's super
scary that AI is killing everyone and you don't feel scared, that's a reasonable point. It doesn't
mean it's true, but it's reasonable. So I want to put that, so the way humans actually make
decisions or actually change their mind or actually think about things is what I like to
think of like the bundle of heuristics or bundle of patterns is like, most humans, when they think
in most scenarios, this includes you and me, is that at any given time, you have a bundle of
heuristics or patterns to get activated. It's like, I present you an argument, and this activates
patterns in your head that you've seen before. It can be emotions, it can be memories,
it can be associations, or, you know, it can be, you know, or just literal heuristics.
For example, and I say to you, AI is going to be different from all other technologies,
it will have nothing in common, it will change everything. And now maybe this activates a heuristic
in your mind that like, well, most things are the same. Like all things equal, most things are not
very different from other things. So if something is totally new, that's suspicious,
like that feels bad. And you might not be able to verbalize this. Like for most people,
this is hard to verbalize if you don't like practice verbalizing and like thinking and
reflecting upon this. But people have this interest to be like, that feels off, like it feels weird
for you to claim this. And this can be super frustrating when you're arguing with people
like this, because then you might say like, well, why do you feel this way? And they might not know,
they might just have an intuition. And a common mistake that people will make is that they'll
get angry. It's like they'll get upset, they'll start yelling at the person or tell their their
being irrational or whatever. I think this is a very counter addictive thing, because then you
activate the heuristic, angry people are usually wrong or trying to hurt me. And if someone is
trying to hurt you, you probably shouldn't believe whatever they're trying to sell you.
All things equal. So I think this is very important to think about what are the heuristics
you are targeting. So a lot when I craft my arguments, I think about what is the bottleneck?
What are the heuristics that the other person or the other group is using that are preventing them
or like are bottlenecking them agreeing to or thinking about this thing? Sometimes it's specific
heuristics. It's just like literally I have to explain why some things are different. I have to
invoke the intuition like, well, think about the industrial revolution, think about nuclear bombs,
that was different. And they'll be like, well, you're right, but nuclear bombs were different.
Maybe it's like nukes, okay, then you know, the intuition dissolves, and then you can move on.
It's kind of like a puzzle game, right? You have like multiple locks, you have to pick
to get further. And sometimes it's just a procedural problem. His friend Dave,
just not listening to you, or like your argument is too complicated, it doesn't fit into
short-term memory, and you need to find a shorter argument, or you need to be more polite, or you
need to- That's something that you didn't know about. Exactly. Like there's another thing where
like a lot of times you are arguing with Dave, you're not arguing with Alan, and like this can
be very hard to do. So as a concrete example of this, I once talked to like a CEO of like a tech
company, doesn't really matter who it was, but I was giving them all these arguments about AI risk
and whatever, and they were listening along, and like they get counterarguments, and I defeated
those counterarguments, and then like, so I just like, defeated all their counterarguments.
And then they were like, no, no, still don't buy it. And he's like, well, fuck this guy, look,
he's like, he's so irrational, can't reason with him. And a good friend of mine pointed out, it's
like, Connor, you got this totally wrong. This guy is a CEO, he's not a tech guy. These aren't
his opinions. They're someone else's opinions. They'll find the other guy. And I'm like, strange,
but like, okay. So next time I talk to the CEO, I happen to talk to him the next day.
And I asked him like, you know, who do you listen to on like AI risk? And he's like, oh,
this guy, this guy, and this guy. And I'm like, oh, shit, all right.
Who's that for the problem?
Yeah. So I go like, I tracked down one of these guys. I talked to him and he was super reasonable.
And like, I gave him the detective crown arguments, he's like, oh, that's a good point.
Yeah, I didn't consider that. That's actually a good point. And then, low and behold, I didn't
talk to the CEO ever again. And but like later on podcast, I see him using some of my arguments.
So the, the algorithm that the CEO was using to generate his statements were not evaluate all the
data and come up with a thing. It was take the average opinion of these three guys and output
to that. So there's no point in really arguing with the CEO directly. You should talk to these
three guys. So like, all of this is a bunch of information. Sorry for dumping everything all
at once. But like, these are several models and pieces of how I think about people. So for me,
it's not about hitting some magical cadence or tone of voice or something like this. It's far more
thinking mechanistically. It's like, how can I help this person? Like, like, it's actually a very
measurement is a very important part for my thing as well. But I think about persuading people or
like giving information. I really come from this is like, how can I help you? Like, I really,
really come in like in my mind in my heart, when I try to, I'm like, I'm not here to hurt you. I'm
not here to change your mind in a way you don't want it to be changed. Because this is a thing
a lot of people are scared of. A lot of people are scared of having their mind changed in ways
they don't want it to be changed. This is very common. This is very, very common. And so what
important way to be is like, I'm here to help. I'm trying to give you information that is useful
to you. So a lot of people I think also reject like AI risk, is that they feel like you wounded
them is like, I was feeling good. You told me information and now I hurt like you were bad.
You hurt me. Like you're a bad person. You hurt me. And honestly, yeah, this is this is not a
dumb intuition. Like if someone comes up to you and they said something to you and you and now
you're hurting or you're crying. Yeah, they're like, that's a bad thing. You should be upset.
Like that's a you should be upset at that person having done that to you. This is a very reasonable
thing. The question is, how can you how can you have those communications? I had a guy on my show
who's a regular person like me who came on and was just like, you know, not telling your neighbor
that their house is going to be bombed is not doing them a favor. Yeah. Yeah. And this is like
the big thing as well, right? Is like, how do you balance between telling your neighbor that
they're going to get bombed is going to make them very sad. But also, you're not doing them a favor
by not telling them. Actually, you're hurting them by not helping them. And this is like,
this is like a genuine like, and I think the way to approach this is just like this idea of helping.
Like, when I talk to policymakers and so on, like, one of the things that like often, like,
I remember like, once I was talking to a policymaker, and I told like a whole hour, I was like,
I asked me a bunch of like super basic questions, you didn't understand computers at all. It was
like an elder older gentleman. And he talked to it. And it's like, and at some point, he's just
kind of like, he didn't say these words, but like paraphrase, he's just like, at some point,
he's just like, holy shit, you're trying to help me. Like, no one was answering my questions,
but like, you're just sitting down and answering my questions. And like, yes, sir, of course, like,
I'm happy to. And he was so shocked. He was so shocked that like, I was like, because he was
also asked questions about non AI things, like you act about like crypto and like, like, what's
a quantum computer or whatever, and I just like sat down and just patiently answered his questions.
And he was like, wow, like, like, thank you, like, no one is taking the time to answer my
questions. And here you are just answering my questions, you're not even asking anything in
return. I'm like, yes, because I'm trying to hell out. So I think this is like, so if I had like
one thing that is like, the way I want to communicate to people as I had like one unifying
thing is I want to help, like, I'm actually trying to help you. You might disagree with me,
you know, if not like me, even if I argue with someone, and I say you're wrong or you're bad,
whatever, I'm trying to help, like, I'm trying to help you be less, you know, wrong.
It's a hugely helpful idea, just to have in mind, I've not had like a unifying theme,
like a mantra, you know, to be like in these conversations, I sort of start all over the
map, try to sound them out a little bit, ask them a few questions, you know, where,
where are you at on AI? What are your thoughts about AI and just sort of see sort of where
they're at, but the attitude of like, I'm here to help you understand this moment we're in,
wherever you're at, I think is really, really a big idea. One of the things of this whole thing
that just made me crazy and just makes me crazy to this day is like 8 million humans on earth,
like 2000 people, or less, fewer doing this work. It's like there's a big party going on,
8 billion people are in a party and everybody's having drinks and laughing, and then off in this
corner, there's this group, a tiny subset that is doing this very dangerous thing, and nobody at the
party knows about it. And no, none of the guys in the little little 2000 person, bomb room at the
party, believe they had any reason or obligation to ask for the consent of the other people at the
party, can we do this at the party? Is that cool? And it's just happening and the party keeps going.
How is this possible? So the truth is that this is a default. I mean, this is how crime happens,
this is how wars start, this is how conspiracies happen, like this is how neglect happens. This
is kind of the default state of humanity and of harm. The way I think about how society should
be lived is that I don't want a society where most people can live their life as civilians and not
have to worry about this. That's nice that I don't have to actually really consider military
strategy in the Middle East, because there are people whose job it is to solve that for me,
so that I don't do that doesn't affect me. Whether they do or not, or totally different
question, but there are professionalized militaries and politicians and diplomats whose job is to make
sure that I can fuck off on the couch and play video games and not have to worry. And this is
very beautiful to me, this is something that's very, very deeply beautiful to me. I think I want
everyone to be able to party as much as possible and not have to worry about things. So when this
falls apart is if there is no one, or if there's a due group of people or someone who escape law
enforcement or norm enforcement or risk calculations. And this is exactly what we're
having right now. So what's happening right now is the same thing that happened with big tobacco,
where when some people started noticing, well, these lung cancers seem to be connected to smoking,
and the big tobacco companies were like, no, what? What? Oh, wow. Oh, no, that would be so terrible.
Oh, we should fund lots of cancer research so we can find more data, but we don't want to act too
hastily. Of course not. Let's be careful here. There's controversy after all. We have to do this.
Let's get a committee together. Let's get a committee together here all sides. So I think
this is kind of what's happening right now. The category of things that we're seeing right now
is that there is a group of people, or various groups of people actually, which we can talk about,
I tend to class them into five groups of people who are trying to build AGI.
And they're kind of building it for slightly different reasons. But for the most part, it comes
down to power is that they want power, you want money. If you believe that AGI is as powerful
as I think you and me think it is, and you are delusional enough to believe you can control it,
then well, you should be willing to do just about anything to get it. And that's exactly what's
happening. So they've dug up the big tobacco playbook. They've propaganda, misinformation,
slowing down of the scientific process and civilian process and so on to buy the time to
externalize this risk onto the rest of society. This is the same thing that happened with chemical
companies. It's the same thing that happened with asbestos. It's the same thing that happened with
the oil. It's the same pattern just at a much larger scale. Sure. And what's required in all
those instances is a lot of people dying. The bad stuff has to kill people before the response comes.
So this is usually the case. But I don't think this is prescriptive. I think this is descriptive.
So for example, let me give you an example of an extremely complicated, controversial problem
that we did recognize. It didn't kill anyone and we did solve ozone depletion.
There is a crazy thing where in the 60s, it turned out that we saw, hey, something's
going on with the ozone. It's going away. What the hell? And then think about how hard this was
to determine. We're in the 1960s, 1970s. We barely have computers. We barely understand the atmosphere
and how things work. First of all, detect there's an ozone layer at all. It was only discovered
like 10 years earlier or something like this. So I really properly measured a couple years.
It's actually this crazy thing where it's actually a miracle that we discovered the proper
measurement of ozone just around the same time as we invented perfluorocarbons,
which were the chemicals destroying the ozone layer. If we had developed perfluorocarbons
just a couple decades earlier, we would just not have an ozone layer anymore.
And it wouldn't come back. It would take thousands of years to recover.
So there's this really crazy historical coincidence. Imagine you're in the 1960s,
right? You have this kind of controversial new theory or whatever. You're one of the very
small number of atmospheric scientists who are studying this. And you're saying like,
wow, it's going away. And now you have to first find out what the thing is making it go away,
which could be anything. It was the 1960s, 1950s. Think about all the chemicals going into the
atmosphere, right? So finding the right chemicals. And while it turns out they're an extremely useful
chemical that many companies use. And now, you know, DuPont and whatever starts lobbying against
you, which they did, I think a lot of people forget this, is that a lot of people don't know
this. But historically, the companies like DuPont and other chemical companies lobbied against them
and said, this is fake news. This is fake science. This isn't real. This is all fake, blah, blah,
blah. And so it's actually a fucking miracle that the scientists and the activists and whatever,
in such a short time, were able to convince the general public and policymakers of a completely
new thing, like an ozone layer with a hell of an ozone layer, could, you know, convince them of
a completely new thing, convince them that is being heard by some obscure chemical they never
heard about. And then also that the, you know, well-funded lobbying companies of the chemical
comes repushing against them against, and that they got the global international regulation.
Like if we stop with our spray silly spring straight cans, China will make all those spray
cans. Exactly. And like, so this is like, for me, like such a historical like anomaly,
like such an incredible thing. I'm like, holy shit, like this is like the fact like,
in other worlds, this didn't happen. Like, we take this for granted. Like, of course,
we fixed the ozone layer, but like, no, this was not obvious that this would succeed. So it's
an incredible example of what this can look like if you do it well. It still took like
many years to do. So like, it wasn't easy by any means, but it did happen. So we have an
existence proof. Like, you can't do this. Awesome. It was there any, you know, looking back at it,
like, how, how did that thing happen without without bodies piling up global coordinated
response? It's a great question. And I'm currently in the process of reading more history books for
exactly this reason, because I've been learning so much about it. I'm like, wow, there truly is
nothing new under the sun. Like, I used to think like people like historians are like, well, people
used to tell me this. And I'm like, yeah, sure. But like the internet exists now. So everything
is different. And now I read history books. I'm like, man, like, we're really not that different.
You know, some things are different. But like, it's amazing how many things are just not different.
With ozone, there's many things you can point to is like, government capacity was much higher
in the 50s and 60s. It's just governments took things extremely more seriously, especially the
US government. There was much stronger also like US hegemony in various degrees, I mean, except,
you know, Soviet bloc. But like, I mean, the fact that Soviet's like, also didn't produce
portfolio carbons was kind of like, you know, if they just decided, no, we need portfolio carbons
for, you know, the glory of the Soviet Union, what the fuck would we have done? So like, you know,
I do think that there is one thing with ozone, though, that they got extremely lucky,
which is that it turned out it was relatively inexpensive to produce chemicals that are safer.
Like this was a that chemical companies, once they put in the research to develop new chemicals,
they found new chemicals that could do what they wanted to do and were much less dangerous.
This was quite lucky. If this was not the case, I'm not sure how it would have gone. Like,
at least it would have been much higher. If, for example, chemicals have been like,
massively more expensive or just like non usable, then things could have gone differently.
We got lucky. And so the parallel there is like, there was an easy technical solution there. And
here, yes, I think AGI is in many ways harder than ozone. I think in some ways it's easier than
ozone. Like, I think ozone is much more confusing than AGI. Like, you know, things smarter than you
are, that we can't control are scary is a completely basic intuition that basically the
entire human species shares. Like, you have to have a degree to not believe this. Like,
you have to like really some results to not have this intuition of just like all things equal.
If there's something it's smarter than me, and I don't really know what it wants or how to control
that seems like a bad situation. It's a very, very deep intuition. And this is a good intuition.
So this is something that for example, with ozone, we're like, oh, there's invisible rays
coming from the sun that reflect off of a weird chemical you've never heard of that's like in
the atmosphere, but it's a different atmosphere. You're like, what? Like, like, like, ozone thing
is like quite confusing, actually, from like a technical perspective. So I think it's, you
have some easier thing. But yes, the harder part is just extremely expensive to solve.
Turns out solving the ozone crisis, it wasn't cheap. But like, it wasn't cannot, you know,
it wasn't going to point out of business, right? I think it absorbed the cost, it was fun.
But with AGI, like, a new phrasing I've been using recently is I think when people talk about
over going to solve AI alignment, you know, we're going to build AGI ASI, you know, I think people
really don't realize how big of a statement that is, like saying you're going to create a aligned
ASI that, you know, fixes all problems is equivalent to saying, I'm going to take all the problems,
like interpersonal problems that people have with each other, all economic problems, all
disagreements, all scientific problems, all business problems, all problems that are institutions or
corporations or governments or international bodies trying to see all of those problems.
And then I'm going to solve them with software and not have any bugs.
Like, that's what there's, that's, that's what that means. And I'm like, you're crazy. Like,
if you think we can do that, like in a generation even, or like, you know, with like,
small amounts of funding, I'm just like, that's not how real software works. Like,
that is just like, I'm not saying it's impossible, right? Like, I think like, physically, could you
design software that could do that? Yeah, probably. But like, doing that with like no bugs and like
resolving all these like, really hard coordination and like, value question, forever, forever,
right? Like that, you're like, I'm like, like, holy shit, like, what the hell are you talking
about? So like, when anyone tells me like, Oh, yeah, we just need to solve alignment
within this amount of time, like, what the hell are you talking about? Like, get out. Like, that's
not how these things work. I was recently talking to someone, for example, who were like, I find it
quite unlikely that, you know, alignment is so hard that it requires more effort than the Manhattan
project. And I'm like, dude, the Manhattan project was about $35 billion in 2023 dollars.
That's not a lot of money. Like, and like, three or four years, I don't think you can solve all
problems on earth with three to four years and $35 billion. But if someone would like to try,
here's my crypto addresses. I'm jumping.
Romanian Pulski strongly believes that alignment is impossible to solve on an infinite timescale,
that at some point, it gets away, you just can't control an ASI forever. What are your
thoughts? Infinite timescale? Impossible? You know, I love Roman, he's a great guy. But like,
I think these things are just irrelevant to reality. This is kind of like Rhysus theorem.
So like Rhysus theorem is a mathematical theorem where like, you can't prove anything
about arbitrary programs. And this sounds like super like, Oh, wow, shit, we can't know anything
about computers. But obviously, this is bullshit. Like obviously, you can look at a piece of code
and know things about that piece of code. But of course, Rhysus theorem only says
over there is there's some program you can't understand. And like, sure, in all infinite
programs, yes, there are some programs that just cannot be understood. I will not write those.
I will simply decide to not write those. And your point is it's just sort of irrelevant. Like,
infinite time is so far down. Who gives a shit? Yeah, it's just like, these like formal models
and stuff like this are just like, it's very deceptive, I feel to take from this like super,
super, super, I don't know exactly Romans model here specifically, but I assume it's extremely
abstract as some like, abstract Markov chain, fucking, you know, like, Oh, well, if there are,
you know, exponential many states, and you have polynomial bit controller, whatever. And it's like,
bro, look, let's solve the civics problem first, let's all politics first, like if you look, if
we just get to a world where we live, you know, where we build software and government institutions
well enough that we can just like live like in a safe, nice human world for, you know,
couple more millennia, I'm like already pretty damn happy. Yeah. Oh, yeah. Oh, yeah. Yeah. No,
that would be great. Let me ask you this has you hear different things about this has any progress
ever really been made on alignment? So overall, yes. Yeah, unless wrong, not clear. I mean,
I'm joking. But so, like, what problem, you know, have we made any trash and any inroads?
So I sometimes like to joke that the first alignment researcher to ever live was Gottfried
Leipzig in the 17th century. I think he was the first like person to make progress on the
alignment problem. And so Leipzig was one of the first to think about, you know, formal logic,
formal logic, and algebra. So if anyone wants to read a very fun historical document,
he Leipzig doctoral thesis suggestion, like his application, when he was like 22 is
epic. It's like, he basically says he is his suggestion for his research program was if you
will create a philosophical language through which all statements can be encoded in numbers,
so that if there was ever any disagreement between two philosophers, they must merely say,
let us calculate. And of course, he would then use this to formalize the will of God.
And then I'm just like, what a guy, like this is like 1670, right? And I'm like,
hell yeah, hell yeah, brother, like what a guy, you know, like obviously didn't quite work the
way he hoped to. But like, hell yeah, my guy, you know, like this is a my man's trying. I love
historical, I love historical figures where I can see like, they're really trying, you know,
not whether they succeed or not, not that Leipzig wasn't extremely successful in many ways.
But like, I love seeing stuff like this. I mean, like, my man was really trying. Like,
he was not fucking around. He was trying to, he was trying to bring God to earth and bring his
kingdom with math. What a guy. So formal logic is progress towards a limit in the sense that before
formal logic, I think people don't remember this, there was not formal logic. Like, we didn't have
good formalizations of what it meant for an argument to be good or true, you know, like,
in like scholasticism and like, you know, early, you know, medieval or like, ancient Greek forms of
like, well, like sylloges, like a Stotelian logic. But a lot of it was informal, or like,
not well formalized. A lot of it was not yet algebra ties. And then later, and like, you know,
formal logic and algebra directly led to computing theory, you know, like, Turing's, you know,
original thesis came through the Inscheidung's problem, you know, Hilbert's program, and,
you know, formal mathematics and algebra. So like proof theory. So like, there's like, there's like,
beautiful connection of like formalizing and like being strict about what we mean, like, like,
when we say this argument is true, what do we mean by that? Like, what does that actually mean?
Like, this seems like a huge tangent. I think this is actually very important. It's because I
think there is a mismatch where people, because of various historical, you know, coincidences,
formalize alignment as this weird esoteric problem that like, doesn't have any connection to other
fields of study that is like completely novel, that kind of sprung up de novo, you know, around AI,
and I just like totally disagree. Like alignment is like the logical conclusion of all the problems.
It's the question of how you formalize and execute and like process human desires and
wishes on to physical reality. And there are many ways you can do this, right, like in an
engineering is a sense of how can we have processes through which we can describe an object we want
to create, and then a process through which we can create that object, and that has the
properties we want it to have. Language design, programming language design, like I sometimes
joke that implementing a slightly better version of C++ is closer to actual alignment research
than almost anything done unless wrong, because programming languages are fundamentally artifacts
and tools through which to allow humans to express complex properties or desires or wishes
and then compile them down in such a way that you can get a computer to actually execute that
and actually do the thing you want it to do. So from my idiosyncratic perspective,
I think that like alignment research, like, you know, long term alignment is like,
in a sense, going in the direction, like you go in the direction of thinking about like an extremely
complex programming language or extremely powerful programming language. What do we mean
to be able to express complex properties about reality, like justice in ways that can be like
compiled, that you can compile justice to atoms? Because this is what we're trying to do, right?
Like, we're trying to get this concept in our head of like justice, and we want, but we want
this to exist in atoms. Justice is not in atoms, it's somewhere out in the platonic realm. How do
we get from platonic to the atoms? And this is compilation and computation. So it's the logical
conclusion of philosophy, engineering, computer science, like this is really what is, so yes,
actually, I think we've made a lot of progress on a lot of these things. We have formal theories of
computation of programming language design. We have made, we have game theory, like, man, game
theory didn't exist like 100 years ago, right? Like they hadn't even formalized any of these ideas,
bargaining theory and so on. Like there are many, many strands where we have made progress on this
problem. And they're just, it's just a massive problem, you know, it's just a really, really big
problem. That is what you're saying that we have been working on the alignment challenge for hundreds
of years and much of what humans have been doing is all pointing in this direction of trying to
figure out how to figure out this, how do I make stuff do what we want it to do? That's always been
the thing, right? It's just like there are things in my head, you know, there's justice and love and
beauty and whatever, and there's all these atoms around me. How do I make these atoms be beauty
and love and kindness and things? Like how do I make this happen? This has always been the core
human thing. It's like how do I do this? How do I make this happen? And how do I make more of it
happen? This is engineering, this is science, this is like the whole thing of like how do we,
how do we configure reality into a reality we want to be in that we want to live in? Like this is
the fundamental thing. And like, I feel like, you know, you can restrict alignment problem to like
a specific idea around like AI and stuff like this. But I think all these other things are
things we should learn from and are like part of it. Like if you have never learned how programming
languages work, you're missing out. There's a lot of great ideas in there that are very important
for thinking about how to control AI systems. I happen to work on some of the overlap between
programming language design and AI control. I think there's a lot of fruitful things there.
If you never have read a book about game theory or, you know, bargaining theory or economics,
and you're thinking about AI alignment, you are missing out, there's a lot of relevant stuff
in there that will be useful for you. Yeah. Is alignment like an all or nothing thing? Like,
can you park, you know, could you input just something as simple as like, don't kill people?
You know, harm, physical violence is bad, but then it would be totally unaligned in other ways.
Like is it a click? It took the sum total of it? Or is it little small pieces that have to be input?
The way I see things is, is that one of the biggest problems with AI is it has a criticality
property. So criticality property is in nature is that you have something that looks linear,
like, you know, it goes up, like, no, nearly, and then suddenly changes. So it goes to some other
stricter form. The classic example is like, you're ready, like fissioning. So like, if you just have
a bit of uranium, you know, has like this much radiation. And if you have like twice as much
uranium, it's like roughly twice as much radiation. But then at some point, if you get enough uranium,
it goes critical. And then the radiation goes like super up. And, or like explosives, right?
Like, you know, if you have like a little bit of explosive, you know, it's like, whatever,
but if you light one of it on fire, then it lights two of it on fire and four of it on fire, and then
the whole thing goes up all at once. So AGI has a criticality property, or intelligence has a
criticality property, or at least it's damn sure seems to have one, which is with humans, is that,
you know, a million years ago, you had chimps and you had humans, we didn't look that different,
you know, maybe we were, we stood in our legs and had a bit less hair or whatever. But like,
from a million years ago perspective, you know, before we are like even further back,
like before we had tools, right? Or even when we had like chimps have tools too, right? Like
bad tools, but like, you know, they can use sticks or whatever, like we go far enough back,
common ancestor, whatever. And you had a guess, you know, all right, you know, we made this new
chimp, it's a bit hairless, looks a bit goofy. What do you think this new chimp is going to do?
And you might say like, you know, it's a bit smarter, maybe it'll throw rocks more accurately,
or maybe use like, you know, slightly more complex sticks or something like that seems
pretty reasonable, right? But no, that chip goes to the moon and builds nuclear weapons.
That's a criticality. So it was obviously a critical phase transition between chimps and humans.
We don't know exactly where or why or how, but obviously there was a criticality threshold cross
between humans and chimps, where just at some point humans could compound. At some point humans
could develop language, they could develop the scientific method, they could compound language,
reading, writing, you know, and just like build industrial society and economies and so on. And
this didn't make them like twice as good as chimps, or three times as good chimps, it made them like
billions and trillions of times better than chimps. And this is the problem with AGI as well,
is that criticality properties make things all or nothing. It's like, this is one of the way
nature, like nature usually doesn't like all or nothing, but like criticality is like how you
can get all or nothing, where if you build a thing past the criticality threshold, it's like,
like, imagine you have like a slope, right? And like you say, like, you know, it's going up,
it's going up, it's going up, and then it crosses a threshold, then it goes boom. So either you've
already solved the whole problem or you're screwed. So in a sense, you have to solve everything
at once. You have to, before you reach the criticality threshold, you have to know or at
least understand the criticality as properties enough. Or the smart thing is you make sure it's
not critical. So how do you study uranium if it goes critical? Well, one thing is you don't allow
it to go critical. You know, you separate it in various tanks that have special shapes, and you
put control rods in there, and you like are very, very careful that it never goes critical in ways
you don't want it to, and so on. And yeah, this is what we should be doing with intelligence as well
is my claim. My claim is, is that if we want to do alignment, we have to do it without AGI before AGI.
That's extremely hard if you're trying to get AGI as fast as possible. But we can buy time.
You can delay AGI. Like this is why some people who claim like, oh, we can't make progress on
alignment before AGI. I just so strongly disagree with this. Like, I think this is just like,
yeah, just super wrong. Like I just described, like there's so much that we can learn and that we
can do when we think about, you know, better and better, whether it's social and political theories,
programming language theory, you know, interpretability of, you know, things like this formal
methods for program verification, there is an endless list of research that we can do long
before AGI that can then, you know, maybe as we develop a better reaction thresholds, right,
as we develop like better interpretability, controllability methods, then we can maybe
inch our way up, right, and maybe we can like, you know, practice, or like, you know, if we have
super high security, right, like, there's a reason that private citizens are not allowed to have
fissionable amounts of uranium is because they will obviously kill themselves and other people,
like, obviously. So, and we should not allow that. So they're super strict standards, or like,
if your lab has a license to carry fissionable amounts of uranium, it has to have extremely
strict standards that none of it gets out, it's always stored in the right vessels and whatever,
right, we should have the same AI, obviously, like this is obviously the case, you're not allowed
to just have weapons graded or AI just laying around. Yeah, yeah. And is it your model that
once we pass the AGI threshold alignment could be impossible?
I think it's just humanities is to exist briefly afterwards. It's like, yeah, it's just over. Like,
once you have a superior species that is like, doing science at a much faster rate than you,
better politics, business, persuasion, everything, like, to be clear, this thing is not just good
at science, right, there's not like a little like nerd in a box, right, this is a hyper-linguistic,
you know, hyperverbal thing that can persuade people and like come up with great speeches and
do politics and business and beat the stock market and, you know, runs on, you know, any computer,
connect the internet necessarily, and potentially, like I recently saw a
calculation that like creating an training modern frontier AI systems is about like the
square of the amount it takes to run them. So with modern systems, you could potentially,
like if you have GPT-4, GPT-5, after it's trained, you could potentially run millions or even billions
of instances of it on the same hardware. And so imagine you have like a million superhuman
systems that never get tired, never get bored, you know, they could share all information with
each other, they don't bicker, you know, the way humans do. And anyone who's trying to run a million
person organization knows there's a lot of politics. Imagine if you just get rid of that and
just everyone works together because they all have the same brain, you know, so you could make a super,
super, super powerful thing. And then if such thing exists, and it doesn't specifically optimize
for humans to like have a good life and continue, it will just treat us like ants,
and we will just be, you know, just pave over with like, you know, parking lots or whatever,
you know, who knows. And, you know, if you're, if you found out the ants in your kitchen had access
to nuclear weapons, you wouldn't want the ants to keep those, would you? So I don't think the AI
would want us to hang on to our nuclear weapons for very long. So I think things would just be
over quite quickly at that point. It's not that it couldn't be solved. It's just like,
it's just like, you're already on the news day timer, then it's just like, you know, might as well
relax. Wow. Yeah. So we're at a really interesting moment in this, I think, where we have Sam Altman
coming out this week saying AGI is coming in 2025. Dario came out and said it's coming in 2026.
And then there's people like Gary Marcus out there talking about the scaling laws breaking and
it falling apart and, you know, Altman's basically on a ghost ship going to nowhere and, you know,
LLMs aren't going to get us to AGI and it's all bullshit. And he's just selling stuff to keep the
money coming in, which leaves someone like me like, I have no clue. What, you know,
what are your thoughts on that? Are we close to it? Are we not?
I remember it was a close. Like how close we are, we can argue. I mean, Sam has been recently,
I don't know if you've noticed this, but Sam has been recently much more coy about his definition
about AGI. He's been like, oh, it's kind of the friends we made along the way, you know, much more
than, you know, ultra, you know, human replacer machine, which he was more explicit about in like
earlier essays he wrote. So it's, I think that's a deliberate PR move that's been happening there.
And, you know, with Gary, I'm like, you know, I like him. I've talked to him many times. He is
a good guy, but like, I don't know why he believes the thing Z does about LLMs. I don't understand.
It doesn't make any sense to me. Seems like he constantly gets, you know, invalidated over and
over again. He seems like a good guy. I don't know why he believes that. No, no hate. But
the thing I think is the most important thing about this is that none of this actually matters.
So to everyone listening and to everyone who will listen to me, I think the most important thing to
take away from, you know, he says this, he thinks this is that none of this matters because every
single one of these people signed the case letter. They all signed the statement that the mitigation
of, you know, risks from AI should be, should be international priority along with other
catastrophic risks such as nuclear and pandemics. So none of this matters. It literally doesn't
matter whether Dario thinks it's two years, Sam thinks it's one year. They themselves have said
that what they are building is as bad as nuclear war. So they shouldn't be allowed to. Who cares?
Like it, it doesn't matter. Like if they say it's five years away, so we can spend three more times
building super Ebola, you just say like, no, stop building super Ebola now. This should be legal
for you to build super Ebola. It doesn't matter if it's going to take your scientists five years
to figure out super Ebola. You're not allowed to do that. Stop right now. So this is the actual
thing that should be focused on. I think there's a lot of distraction that happens, which is again,
I would highly recommend any viewers to read some history books on big tobacco and the strategies
that you see, even a nice Wikipedia article on the big tobacco playbook where the thing you want
to do is you want to create controversy. It's not that you want to defeat your opponents or you
want to like argue against them, because that might make them look in them might make their
arguments look too legitimate. You just want to spread confusion, fear, uncertainty, and doubt.
But that's the name of the game. The name of the game is not to persuade. It's just to waste
everyone's time and just to be confusing to say it's controversial. Who knows? You know, I am who,
you know, he says this, he says that, who knows? I guess there's nothing we can do.
This is the strategy that gets played. The way the fun playbook works is if you benefit you,
as in the corporation, benefit from the current status quo of nothing happening,
then you just want to waste everyone's time and confuse everybody because when people are confused,
they just don't do anything. They just go along with what the defense. So the thing that we have
to do or that has to happen is not find out the exact moment AGI comes and make sure we have policy
by then. That's wrong. Rather, you have to flip to default action. It should be like, all right,
maybe it's in one year, maybe it's in 10. Who knows who cares? It's illegal. And so like, and then
we will make it unillegal once you can prove that it's safe. Like this is what the obvious thing to
do is like, okay, your company comes to the FDA and says, I want to build Super Ebola. And you're
like, okay, how do you make that safe? And you're like, no, we'll figure it out. It's going to take
five years for a scientist to invent it anyway. And then the FDA is going to say, like, no, come back
when you know how to make that safe and we'll reconsider it. And then it's like, it's like,
yeah, like you don't get to build Super Ebola if you don't know how to make it safe. It's not the
other way around. It's not even first build Super Ebola and then figure out how to make it safe.
Like that's not how these things work. This is so important to not lose sight of this. Because
this is what the propaganda is trying to make you lose sight of is that it doesn't matter
when it comes. It's we know it's coming, we can just stop right now and say, look, if tomorrow
open AI is alignment team in so far as it still exists, comes with an amazing proposal that like
beautifully solves like all these problems and it has the sign on from like all these
international governments who all support it and all the greatest scientists have signed it.
I'll be like, all right, fair enough. You know, all right, let's talk. We can talk about it.
But that's not what's happening right now. Yeah, what is what is the playbook on beating
Fudd? Like what is the tobacco because tobacco did lose pretty, you know,
catastrophically for them in the end, like they eventually it took a whole long time. Any lessons
from how they went down? So as I am reading many history books nowadays and recommend any readers
who want to or listeners who want to read some books and write write up some of the lessons and
share them, where you know, widely be very helpful. And also anyone has the reading material
that recommend to me, please send them my way. My main thing I've from so far, I am not a historian
by any means, is that it's actually quite hard. This is a really good playbook. This is why it
keeps getting used because it's genuinely very good. Like this is a very strong strategy to play
is that if you have the lucky state of benefiting from the default scenario and you just have to
prevent change, this is way easier than causing change. Causing change is way harder than just
preventing change from happening, which is what Fudd is all about. So as tobacco playbook is all
about. So we're already the underdogs here. Like we're already like things are already
stuck against us. I just want to be there's not to discourage is just to be clear, we are the
underdogs. Like there are these massive corporations playing unfair, you know, Fudd playbooks with
massive amounts of funding, massive amounts of government control and whatever, and they're
playing it against, you know, the American and the international like peoples, like everyone
at our expense. So we are the underdogs here. Like we are the people being attacked, we are the
people being, you know, and we need to fight back. I think there are some things that we can learn
from this. I think one of the most important things that we have is just like we have history
and we have examples and we can make things legible, is that the pain of the Fudd strategy is
legibility. Like their whole strategy is just a spread uncertainty. It's not to convince you of
the opposite. It's just to be like, you're like, do I, I heard that, but I also heard that. So
like, I don't really know what to believe. And whenever you feel that you need to be like, hold
on, hold on. Like, this is why I focus so much on the it doesn't matter thing. The way you win. So
have you ever heard about some malachron levels? So this is a concept originally from Continental
Philosophy, God forbid. And it basically goes like this, it's like different levels of how speech
and symbols can relate and speech acts can relate to reality and truth. Level one is truth. I tell
you something true, trying to make you know something that is true. For example, I tell you
there is a lion on the other side of the river. I checked earlier, there was one and I'm just
telling you so now you know there's a lion over there. That's level one. Level two is lying. I
saw there's a lion over there and I tell you there's no lion on the other side of the river.
Don't worry about it. I'm lying to you. I'm trying to make you believe something which is false.
Level three is group affiliation. I tell you there's a lion on the other side of the river,
just because I'm part of the lion party. Me and the lion guys, we love lions,
lions are cool, don't mess with us, we all believe in lions. It doesn't matter whether there's a
lion there or not, it's just a thing we believe because we're the lion political party or whatever.
And level four is power or in other words, bullshit. It's just you say whatever the fuck you want
in whatever order, who cares? Just whatever vibes well. You say oh yeah, there's a lion on the other
side of the river because you like the vibes of lions, they're cool. So yeah, I'm cool,
so I believe in lions because I'm a cool guy. This is level four. So a lot of politics people
think operates on level two is like you're being lied to by politicians but very often it's level
three on four and it's very confusing to people. It's like often people say things that are false,
not because they're trying to lie to you but because they're trying to say I'm part of the
group that believes this or they don't give a shit and they're just like saying whatever.
You know, just say the word jobs in every speech you give, right? Maybe you give a speech, say
we're protecting jobs. Are they actually protecting jobs? Who cares? It's just a vibe.
Like it just feels good to say we're protecting jobs. So I'm just going to say that I'm part of the
and the job protector guy. So I'll just keep saying that whether it's true or irrelevant.
So sure, a lot of our politics and a lot of these conversations are in level four.
So when like big tobacco says there's controversy, what does that mean? It doesn't mean anything,
just a vibe. It's just like, you know, I feel like there's a bit of controversy vibes here.
Like we have a good uncertainty here. So like I think, you know, we should be
be careful here. It doesn't mean anything. There's not, there's no objective measure here. It's just
like something who cares, right? And they'll say whatever, right? Like they'll pick up studies
from wherever they'll say whatever like literally who cares. So how do you defeat this? So the
usual joke is that level two beats level one liars have more things they can do than truth
speakers. So it's your lie. Very useful. Level three beats level two, because if your party is
bigger, it doesn't matter how clever you are because, you know, I'm the bigger group. Level
four beats level three, because I can just align with any group I want. I have zero loyalty. Who
cares? And level one beats level four. So the reason level one beats level four is because
level four is just incoherent. It's just like you lose contact for reality. And also people hate it.
Like people have this visceral feel, you know, this feeling where you see someone talk about
like postmodernism or like, you know, everything is just subjective, man. You're like, shut up.
Like, like, you know, or you hear like a politician just like say words that obviously don't mean
anything. And you're just like, shut up. Like, I hate you. Yeah. Like, just like, you know,
you're not even lying to me. You're just saying sounds. It's not even that you're lying to me.
Like, there's a lot of that these days. There's a lot of that these days. We're just like the
thing is very important to understand a lot of the things that politicians like say,
it's not lies. It's just noises. Like it's not even that they're lying to you. It's just
they're just like seeing like, like jobs. Cool. Yeah, like, we like jobs, right guys? Hell yeah,
jobs. It doesn't mean anything. It's just like whatever. Not everyone, not in most circumstances.
Some of our politicians are great people doing great work. And I thank them for their further
service. But you know, there are some, you know, so the way you get out of level four, because
like level four is a death trap. It's like, as you go higher and higher up the level, you get
stuck. Because then like, once you're using level four, the other person starts using level four.
And now you're both losing level four, and you can't get out of it. And everything goes to hell.
Everything just goes completely nuts. Like this, like, you know, I mean, I won't reference any
specific political movements, but there are many political movements that have had this exact death
effect. And the way is, is just to focus on what's real. It's just to take the smallest, simplest
thing that like where there's no ambiguity, there's no interpretation, there's no cosmodernism,
there's no whatever. It's just as simple as possible. And just keep repeating that
and be like, yeah, but what about that? Like, oh, sure, sure, sure, like all the other things
say okay, sure. Okay, but they signed the letter. What does that mean? Excuse me, speak directly
into the microphone. They said this is worse than nuclear war. What does that mean? Please
speak directly into the microphone. This is what you have to do. And you have to do this to yourself.
You have to to yourself every time you, you see like, you know, some, you know, anthropic thing
saying like, well, here's a massive shitpost of a report about safety, responsible scaling,
blah, blah, blah, just be like, okay, but is it worse than nuclear war? And if so, stop.
Like, I don't care what your report says, like, I don't even have to read it. It's like,
is it as bad as nuclear war? If so, stop, I don't care about your report. And I think this is the
winning strategy. It's just focus on the basics and don't let people galaxy brain you like don't
let people confuse you, don't let people trick you, just repeat over and over again. Okay,
yeah, but they signed the letter. Okay, yeah, but there you yeah, but
I think that letter was a huge thing in the history of I hope it's like I point to it all
the time and it's like mitigate the risk, you know, it says mitigate extinction risk. Like
if we're not, if it's not, what are we mitigating? If it's not the extinction, and what's extinction,
if it's not extinction, like the words are clear. Exactly. And this is the thing I think just has
to be repeated over and over and over and over again. And this is how truth wins truth wins by
just like, not giving into these stupid games, not argue, but well, but what do you really mean
by AI? Like, I don't care the thing that use there's this thing, extinction, you know, like,
I don't care, like, yeah, but like, we're like, okay, yeah, but okay, yeah, but yeah, but like,
if we make things that are smarter than humans, that's bad, let's not do that. And like, so like,
are you building things smarter than human? And I'm like, well, what does smarter really mean?
And like, I don't like, stop, like, you just have to like, not get trolled. Like a lot of like,
FUD is like, like the FUD strategy is what in, to a large degree, what in the modern world,
we call trolling, where you just like, waste people's time and trigger them and just like,
annoy the shit out of them and just like waste their time, or just like create massive shit posts
that are like really hard to like, like, some people will sometimes beat to me like, they're
like, well, but anthropic, you know, said, you know, that they'll do good things. And I'm like,
okay, what's their plan? And then they post a massive shit post about RSPs. And I'm like,
all right, where in that does they say how they stop X-Rest? And like, well, you know,
read it. And like, it's 50 pages, like, okay, where does it say that? And they're like, well,
it's in there. And I'm like, okay, where? And they're like, oh, read it. Okay. And then I read it,
and it's nothing in there. Yeah. Like this happens over and over again. This happened to be
so many times with not just anthropic, I'm picking on a property, but like also like open AI,
I've read all of their, you know, responsible scaling safety plans and whatever. And just like,
okay, yeah, but like, how do you prevent extinction? Like, it's not here. Like, it's just not there.
And like, is it? And so, okay, yes. But you signed the letter. So shut up. Like, who cares? Like, I
don't care about your huge shit post here, like, just like stop. Or like, and through this just,
of course, they don't want to stop. This is an important thing to understand. The reason
anthropic and open AI are not stopping is because they don't want to. Like, very deeply,
they don't want to, they want to build a GI. They want to build a GI.
If you're just like a childish petulance, like I, you can't stop me. Like, like,
I mean, it's just their utopists, they think they can build God and live forever.
They literally say it on their blogs. Like, again, just like, just read what they say,
it's not that weird. If, if you're a person who wants power, and someone said, oh, there's a
technology that makes you, you know, infinite power, are you going to be like, oh, I shouldn't
do that? Or you'd be like, yeah, okay, let's go. Like. So I don't know if you have a lesson
to hard fork the New York Times tech podcastie thing, but one of those hosts, Kevin Roos,
said it on the other day that like, they interviewed an open AI guy who had left and,
you know, another open AI safety guy left and, and they're talking to him in the interview
and whatever. And then afterwards, Kevin's like, well, you know, the reason I'm not really worried
about AGI and extinction is because none of the people working at the companies are really that
seriously worried about it. And when my texts start lighting up with people who work at the
companies being like, oh, shit, I'm really worried about this, then maybe I'll start to pay attention.
So what you're saying is, and this is a big part of the problem, because it's like trying to convince
people that people go to work every day to work on a suicide machine is so counterintuitive that,
you know, people find it really hard to believe. So will there come a point? Is it possible there
would come a point when there would be droves of people from these companies actually coming out and
being like, holy shit, or are we just going to get to the edge of the mountain? And nobody's going
to say, they're just, you know, everybody's just quiet and not saying nothing.
So this is a great example where we can call back to an earlier part of this podcast about how I
think about convincing people and like talking to people where we can analyze what is the
intuition here. And you said it perfectly. You already called out the intuition. The intuition
here is people wouldn't go to work on a suicide machine. This is the underlying intuition of
why he believes this. Like you said, this is correct analysis. Now question is this intuition
true? Climate change? No, it's not, right? Yeah, like this intuition is just not true.
So now the question is not, does this happen or not? Like, will this intuition be validated?
The question is, is this intuition true? And then if it's not true, what are examples of
things that can counterintuit this intuition? And for example, big oil, you know, people
were at nuclear weapons, you know, like there is in fact, I mean, you know, I'm German. There
are many people who went to work every day doing some very bad things in the 20th century. And,
you know, they were just normal people. They had normal office jobs and families and kids
and probably a dog. You know, it's just like the intuition that people in a normal job environment
would not work on extremely dangerous or immoral thing is just not true. So no, I don't actually,
like, this is like saying, I'll stop, I'll start taking climate change seriously. Once all the oil
CEOs start saying it's a big issue. And I'm like, why would you think they would do that? And like,
why do you think that would be the right standard? Like from the, if you said like, once all the
petroleum engineers start saying that climate change is a real problem, then I'll start getting
concerned. And I'm like, like, what? It's not gonna happen. It's not gonna happen. And even if it
does, it'll be way too late. Now, you'll be way too late. These people have a huge incentive
to underplay the risk. It's literally their job to underplay the risk. Like, you're literally saying,
I'm gonna worry once the people who are paid to not be worried start getting worried about it.
And I'm like, what, like, what, what decision theory is this? Like, what, like, you don't do this in
other scenarios, right? If a tobacco, you know, consultant told you, oh, the lung cancer is not
a big thing, you don't say, well, you know, it's true, you know, I'll wait until the tobacco,
you know, factory, you know, managers start saying it's a problem, and then I'll start
worrying about lung cancer. So let's talk about the general public outreach. Like,
do you have thoughts about communication strategy, ways to wake up the world?
Many ways, indeed. And we've already talked about a lot of it already. Luckily,
in this podcast about thinking about people, I think the most important thing that, like,
I can just repeat over and over is meeting people where they are. Like, we kind of talked
about this earlier, but trying to help people. I think there's a thing where, like, there's a
form of helping people, which is patronizing, where you're like, you decide, hello, I am the
savior, do not resist, you know, which is like a... I get in that, I wear those shoes too fucking
much. Yep, it's an easy thing to go in. And I end to the victims of this, I was like, it comes
from a good place, it comes from a good heart. It's like, mostly people who do this are actually
trying to help. Like, it's sometimes funny, you know, I live in London, and like, sometimes I'll
like, see like posters or I protest from like, student communist groups or whatever, where they're
like, you know, we should like, unseat the government or whatever. And like, that's cute.
Like, I don't even get upset anymore. I'm just like, oh, look, they're trying to help.
That's cute. I mean, it doesn't work and don't do that, but it's cute, you know. So
I think it's very important to generally, when you're trying to convince people or like,
change people or like, help people, is to meet them where they're at. If I had to talk to a
communist about AI risk, I wouldn't come in with like, your models of the world are wrong,
you're a bad person for believing these things that I personally disagree with and so on and
like, blah, blah, blah. I would just be like, all right, how do they see the world? Like,
and like, what are problems that they are facing? Like, what are intuitions that they have? And how
can I communicate with them on this? I'm no communist. Holy shit, am I not? Right. And like, I
very not much hard to disagree with some group more. But if I had to talk to someone like this,
I would think about communism. I would read their books. I would think about how they view the world.
I would see what intuitions make them believe this, whether I agree with it or not, I would see like,
what intuitions made them this way? And like, how do these intuitions relate to the things that I
think I might have a, you know, framing for me of how I think of AI risk, but I'm like,
I need to think about that communist phrasing, like how would a communist who thinks about the
world from a communist lens think about AI risk? And that's my job. It's not their job. It's my
job to figure that out. And I feel the same way about any other group of people who is different
from me is that I have a frame that works for me. And it doesn't work for everyone else. Right.
And this is not necessarily because they're stupid, because they're evil, or because they're bad.
It's just the world is very big. We use different methods to understand the world. We use different
frameworks. We do different life experiences, different emotional states, and so on. And
as I say, we can stomp our feet and be like, well, but people should be rational. Like,
I shouldn't have to understand communism to talk for a communist or whatever. But in the real world,
you do. And like, just literally get over yourself. And just like, you know, when I need to talk to
extreme right wingers or, you know, whatever, right? And just like, all right, I need to understand
what do they care about in life? Why do they care about it? Whether or not I agree with it,
you know, like, sure, what are moral, acceptable things like, you know, I will never, you know,
say to a communist that like, oh, yeah, it's actually fine if we, you know,
kill all these people and come in, you know, gulags out, I would never admit that, obviously,
because I that's morally wrong. But I can't phrase things like, like a Marxist perspective is like,
well, it's like, capital, you know, become, you know, starts consuming labor until there's no
more labor. And the comments would be like, Oh, shit, you're right. That is a problem. I never
thought of it that way. Why did no one tell me that? Like, I've had conversations with this,
where I talked to like, left leaning people, and I just explained AGI as like, capital replacing
labor. And then they were like, Holy fuck, you're right. That's exactly what's happening. This is
a big problem. Why did no one tell me? Wow. And I'm like, you know, get in their terms and their
constructs and their language. And that's why it's so important to be on level one. It's so
important to not lie. And that's why it's work is that it's so easy to just take someone's words
and just say something they like, but it's not quite true. This is so easy to do. It's so easy
to say, Oh, it's like, you know, the bourgeoisie AI trying to take over, but that's not
true. That's not the thing. And to know which of these metaphors is like, gets the thing you want,
like takes work, you have to actually listen to these people, you have to like, and like respectful,
you have to be like, understand what are the problems they care about? How do they determine
problems? Like, you know, it's like, for many groups of people, whether it's, you know, political
groups, whether it's faith groups, whether it's, you know, just people from different backgrounds,
from different cultures and whatever, you actually have to listen to them, and you have to like,
try to help them. A really nice thing is that that's why I love this framing of like,
trying to help. I think it's so important is that most cultures have a respect for people trying to
help, you know, as long as you're like respectful, you're not asked all about it and whatever,
like, as they like, you know, like the person I was talking to, who was like, Holy shit,
you're trying to help me was like a super like conservative, right wing kind of guy who's like,
like old, like aristocracy kind of like very different, like a long haired, you know, like
tech guy, right? But like, he could see this like weirdo here, he's helping like, you know,
he's like, you know, he's speaking my language, he understands the things I care about, he obviously
took the time to read up. And like, he's like, not trying to sell me anything, he's trying to help.
And people do respond very well to this. And so this is on the individual or like a small level,
like, how do you approach people and my recommendation for how to approach people is to
approach these people. Like, that doesn't mean everything they say is right, or should be,
you know, just agreed with, right? I talked to many people who I very strongly disagree with,
including morally. And, you know, sometimes those fights should be had, there are some people you
should just not coordinate with because they are evil exists, right? You know, there are some
people who are just so bad, you should just not refuse to talk to them. That does happen. It's a
very small minority. It's a very, very small percentage of people. So that's the first thing.
The second thing is larger scale communications, like, you know, large scale comms and communications.
This is related, but different is that I think everything comes from a basis of treating people
like people and respecting people and like trying to understand some genuine curiosity and showing
general helpfulness. I think this is the basis for everything. Like, if you treat people like
animals or like objects or ponds, you're set up for failure. Like, the people will realize this,
it doesn't work, and it's like morally wrong. Like, people are people, even if you disagree with them.
You know, but the second thing is like, public comms is different because of the different
setting. Like the amount of information you can relay with like a public statement, you know,
one of the great things about podcasts is you can suddenly actually produce very complex statements
in a public setting. This is actually usually, this is very rarely the case. Usually, like, I
know you've never been on TV, but like I've been on TV before. And it's, it's just like, it's like,
put you down there and like, all right, we're live now. And like, and they'll say five sentences,
and it'll be like, what do you think about this? Like, well, I think it's this. Well, that was
exciting. Thank you so much. And cut camera. I worked in TV for 12 years. I did that. Yes. I know
what it's like to take someone like you who has a lot of great things to say and have to chop it up.
Exactly. So like, you know exactly how it is, right? And it's not because the TV is like,
they're evil, or they're trying to like herd you or something. No, it's just like,
just radium, right? It's just how it works, right? And so again, my recommendation is
treat it with curiosity, treat it with kindness, be like, all right, how can I help the TV producers
and the people watching the show get truth and things that are useful to them? How can I help them?
And so helping your TV audience is different from helping an individual person. It's a different
thing. The TV audience is a different entity than a person is made of people, but it's a different
thing. So how can you approach a TV audience? But one thing is, is that you need to have just
extremely tight sentences that you need to prepare. You have to have, you know, to know exactly what
you're going to say, because the right intonation and so on. And you have to like, be clear that
it's actually the thing that like can be understood with a limited context, the audience will add.
Yes. This is very important. One of the most important properties for this kind of like mass
communication is what I call graceful degradation. So most complex ideas, or like complex ideologies or
systems, means and so on, kind of have like a multiple forms. So like, let's take Marxism,
for example. Marxism is really complicated. It has all these moving parts and all these theories
and disagreements and horribly complicated. I don't understand it, right? Like, it's like way too
complicated. And like, it's like super complicated, whatever, right? Whether it's true or not, who
cares? But it's very complicated. So the meme of Marxism is very complicated. There's many moving
parts. Most people will never read Marx. They will never read all the books and all the theory and
have deep disagree. And including most people who would describe themselves as Marxists.
We'll never actually read all this stuff and understand all the details and, you know,
who disagree with you or whatever. Most of them will never do that. There will be some
percentage of academics who spent their whole life studying this or whatever, right? Whether or
not that's a good idea. And, you know, they can propagate the full idea, quote unquote. But what
happens is that most people get a degraded form of the meme. They get a greatly, greatly simplified
version. Like, it goes through like a, like a blurring process where it gets like less and less
sharp as it goes down. Eventually you get like, you know, whatever it lands on, like the Soviet
farmers, like, you know, farm, right? And then that version has to still be good. And this is a
property that not that most memes don't have. Most complex ideas, when you simplify them, just break
or just become stupid. And a thing that good ideologies or like, or like,
fit complex ideologies have is that the degraded version, the simplified versions are still good.
They're still useful, even if they're not maximally have all of the nuance. Same thing with like,
you know, like libertarianism or like, you know, like, like capitalism, like, you know, like
libertarianism is quite complex. There is like many theories about why free markets work, how to
set up free markets, how to deal with monopolies. This is really quite complicated. But the degraded
form of like, you know, supply and demand and all things equal, we should be free, you know,
two sentences is like, pretty good. Like from a libertarian perspective, like that's a pretty
good one. If most people believe that libertarians would be very happy. You know, same thing applies
to right wing, left wing, you know, et cetera, is like, you want a version that's complex,
you want a version that is simple. And we do the same thing for like high risk. And this is why I
focus so much on the thing like intuitions, such as like, something smarter than us that we can't
control is scary. This isn't the full story. We're not talking about instrumental conversions and
takeoff curves and AI scaling. And my claim is it's actually bad to even bring those up. So like,
when I'm in a TV interview, I don't start talking about scaling laws, or I explaining, I pick like
three intuitions or one intuition that I want to stick. And I'll be like, Hey, hello, you know,
TV audience, I there's a massive, you know, many huge thing behind me that I don't have time to tell
you about. Here's an useful thing for you, hold on to it. And this is and people and people appreciate
this too. They're like, Oh, thank you, that is a useful thing for me to hold on to. That's actually
great. So like, one of the ones I almost always try to get on TV, for example, is like that neural
networks are grown, not written. But this is like, such a good one. It's super easy. And people are
like, Whoa, I didn't know that. And I'm glad I now know this, like this is a useful thing for me to
know. And I will hold on to this. So when I think about mass communication, I often think about this.
I think about what are small things that people can hold on to and be like, and like, be like, Oh,
yeah, I've heard that before that is actually kind of, you know, some people will be motivated to then
seek out the more complex information, read the whole thing and blah, blah, blah. Sure, great.
But they're not the primary audience. Sure. All right, here's a question for you. What are your
thoughts on sort of tactical purity? You know, a lot of the sort of rationalist EA world that the
ASHD stuff comes from, believe the argument has to be complete. And you know, all these sort of
advertising and marketing and PR is bad. And my thought as someone who was a TV reporter for 10
years, and has worked in advertising and marketing for the last 10 years after that, is like,
it's an emergency. We need to use any tool available. And there's no purity tests that I have any
interest in for any of this stuff. So for like, this is a crazy example that I thought of this
weekend, excited to hear your thoughts about it. I want to do a social media campaign around
saving my dog who's a beautiful, just a dog and could be any dog from being slaughtered,
like save Dolly, save Dolly the dog from being slaughtered, learn more about AI risk,
like casting AI risk as an animal rights issue, because Dolly's Adams are just as vulnerable as
mine. So, you know, what do you think about using side door, back door tactics? Or does it all have
to be like, proper? So I have the envy of position of disagreeing with both sides and both of them
being annoyed with me. So, you know, produces is for good for podcast content. So I, so I have my
deep disagreements and fights with the rationalists, not all of them. You know, many of them are great
people, whatever. But oh, boy, do some of them really hate me, like it's like a childish degree.
And if you knew, I mean, boy, if you knew. And a lot of it is that like, there's a Freudian thing
where like from Freudian psychoanalysis, where like, if someone claims very, very hard to be X,
they're probably the opposite of X. And like, this is shockingly often true. And it's not always
true. Like, like Freud is mostly bullshit. Like, like, man, have I seen this happen? There's a thing
where, for example, rationalists have like a very strong norm of like, not having emotions or not
letting emotions, every conversation must be a logical disagreement, which means that whenever
they are emotional, they have to explain it with non emotional terms. And this leads to complete chaos.
And everyone comes super passive aggressive, like I've had a conversation, like I've had fights
with rationalists before, where they'd be like, just pissed at me, like they just dislike me,
or I made them angry, or whatever. And I'll just, and I'm like, it's fine, you know, like,
you dislike me, you think I'm annoying. It's okay, we don't have to be friends. And like, well, no,
it's because I disagree with you on your strategic analysis of like, no, you just hate me. It's okay,
you don't have to like me, like your theory, like your argument has nothing to do. And then they'll
like, come with these massive theories about like, all your low integrity strategies. And like, no,
you just hate me. And it's okay. Like, your emotions are angry. It's okay. I'm angry at you too. I'm
actually extremely pissed at you right now. That's okay. We can just walk away. We're adults.
Yeah. And it's, it's, it's a new level of frustrating when you are having an emotional
conversation with someone who refuses to engage in emotions, and just like claims they're like
irrelevant or don't exist or whatever. And I'm just like, ah, very frustrating. So there's this thing
where rationalists don't use the word purity to mean like, not emotion. And I think this is
just wrong. Emotions are not just because something is emotional doesn't mean it's not true. The thing
I care about is truth. Truth and goodness. The thing I don't care about is not emotional. Like,
for example, you know, if I love my child, is that truth? Well, actually, you see, it is simply
oxytocin hormones being released. And like, I don't give a shit. Like, who literally who cares?
Like, what the fuck are you talking about? Or like, sometime or like, for example, you know,
my mom, you know, she doesn't necessarily understand all these things, but she's like,
wow, this AI thing is really scary. And should I tell her, well, actually, mom, you don't understand
computers, therefore, you're not allowed to feel scared about that. That's impure. You're just using
appeals to emotion. Like, no, it's actually extremely reasonable for my mom to be feel about
just because it's an emotion doesn't mean it's wrong. If you feel uncomfortable around someone,
and just they make you feel bad, and you're just like, I would rather go, I don't want to hang out
with this guy, like he creeps me out. That's okay. Like, like this is the thing that happens
rationalists as well. So I'm like, a woman will feel uncomfortable around someone. And then they'll
be like, well, why do you feel uncomfortable around him? Like you have to justify like logically,
why you don't want to be around this creep, like objectively, he did nothing wrong. So therefore,
you must feel good around him. And which is just like, it's nothing, you couldn't just have a sense.
Yeah, it's like, and this is just like, very bad and leads to lots of abuse and like lots of like
horrific, like scenarios, right? It's like, it's okay to feel emotions. Like this is just like a
deep thing that doesn't mean emotions are true either. Just because you feel bad about something
doesn't mean it's bad or that it's wrong. But sometimes it does. Like it's just not that easy.
So in terms of a charity thing, if I went to my mom, and I gave her a huge lecture about all the,
you know, linear algebra underlying neural network theories and whatever, I don't think I'm
actually helping her. I don't think this is the most helpful thing I could do for her. Because
she'll just feel confused and overwhelmed and like doesn't know what to do with this information.
This is not helpful. If I instead give her well crafted, true, underlying intuitions,
pointing to the things, and I'm like, well, they're not, they're grown, not written,
you know, something smarter than us, all things equaling, it's similar to big tobacco, etc, etc.
Like, you know, just a couple things that she can understand and use. I think this is very helpful
for her. This is the kind thing. This is the pure thing. And the thing that she will come away with
is much more pure than the things you would have come away with with the whole fucking shitpost.
Right? Like just because you have more information doesn't mean people get more understanding.
To be clear, the information should exist somewhere. Someone should be having the full argument.
Like there should be nerds and professors and whatever having the full shit show, you know,
someone somewhere in a bio lab is having an argument about, you know, non-squamous cells,
lung cancer versus neonicotoid fucking whatever, right? Sure. But that's irrelevant to policymaking.
Like someone should have, should argue about, you know, how does this nicotine car component
interact with this cellular metabolite or whatever. But we don't need to know that to ban or like to
restrict cigarettes or know, you know, cigarettes and know that it caused lung cancer. We don't
need that. And in fact, if you depend cigarette regulation on some fucking arcane, you know,
biochemistry thing, then nothing happens. Then you just lost. You just forfeit the actual good
thing. So this idea that you can just like have maximal truth in all circumstances and all
maximal information is just ignoring complexity is like just because you have more things doesn't
mean it's more good. It's a trade off. Like when you're trying to change people's mind to exclude
emotion as a tool, the primary tool to use is tying both hands behind your back.
It's not just not using emotions to it's as a tool. It's also that people use emotions as a tool of
reasoning is that emotions are part of the algorithms we humans and used to reason about
reality. And there is these aesthetic movements around like rationalism, whatever that try to
pretend this isn't true. But it is and like suck it up. Like some like sometimes I look like,
you know, a mathematician looks at a formula and says like, I think there's something here.
Like there's something beautiful about this. I should say this more. What is that? That is an
emotion. You just use an emotion and science. Actually, most of science is emotions, because
you have aesthetics and you have beauty and you have all these things. It's like the human brain
is made of neurons and emotions. It's not made of like abstract data structures. And we can also
use abstract data structures and logic and so on. And we should it's very useful. But it's not the
primary engine of cognition. The primary engine of cognition is valence, emotions, heuristics.
And, you know, you can argue it shouldn't be that way. But I'm like, well, suck it up. Like,
if you refuse to engage with other people's emotions, you're refusing to engage with their
reason with how their brain actually generates truth. You know, it might be an like, and, you
know, not the best algorithm. Maybe yours is so much better. Sure. But again, you have to meet
people where they are. You know, for my mom, emotions are very important. Like she often has
a really good gut feeling. Like whenever she tells me like, something wrong with that person,
she's often very right. Like, oh, that girlfriend that's not going to work out.
She's always right. You know, and or whatever, right? And that's okay, right? It's a reasoning
process. I'm saying it's the perfect process. But so I just kind of like reject this whole thesis
that like there's something like morally pure about not engaging with emotion. I think it's just,
I think kind of the opposite is like, not engaging with emotions cope. It's kind of like
you're hiding, you're putting your hand in the set and pretending that the world is simpler than
it actually is. And that, you know, a lot of emotions are calibrated, like a lot of emotional
reactions are actually reasonable. And like, if you're some or not, and you should, and you have
to differentiate and have to deal with that. It's just what is so what I on the other side, though.
So there is one thing where I do think priority matters a lot. Not, you know, it's, well, at
least there's a trade off, which is, there is a general pattern in nature, which I call deals
with the devil. And the pattern is usually something that if you, or if you relax a little bit,
the thing you want, or that you're asking for, you gain more power. If you're just a little
more less strict on what you're asking for, like something in this direction, if you accept a few
more shady allies, then maybe you would have otherwise, you will see your funding go up,
you'll see your power go up or whatever. So the deal with the devil is always the same.
You trade, you gain power in return for not getting quite what you asked for.
And so this is very important. This is a very, very, very important pattern, where, for example,
you know, the thing you just described with your doc, like this emotional appeal or whatever, right?
Like, there's ways in which this could be good or not good, like saying, hey, your dog will be
killed from AI is true. This is not a lie. Like this is genuinely true. But then if you say something
like, you know, but then there's versions of this, which could lead to big problems. Like, for example,
if you start attracting, you know, madden large amounts of animal rights or like, you know, extreme
leftist people who say like, AI only matters because of, you know, risk to the ecosystems or
whatever. You're like, wait, that's not what I meant. You know, like what I actually meant is
this larger thing. I'm not saying this is what you're doing, right? But like, yeah, yeah, yeah.
It's like, there are ways, like there are ways where you can say something emotional and it's
not a lie. Like if you say like, I am sad if my dog dies, there was currently a large risk that
is being placed upon my dog. If you feel this way as well, please help. This is not a lie.
There is no lie here. Now, you can make extreme versions of this that are lies. Like you can
deceive people and saying pretending it's a different story. Like you don't miss an AI risk
until the very end. People would feel deceived by that. But if you're like, this is the thing is like,
I love my dog. My dog is being risked by a thing. And so are you. Learn more. That's not a lie.
Right, right. And even if you used a little deception, a little sort of like, hey, you know,
luring him in to get him somewhere, like, you know, would you object to that? And then one other
one would be like crazy tactics. Like if we could create a crypto meme coin that would infuse 100
million dollars into AI safety communications, because people just went crazy on this stupid
meme coin, is that a tactic we could use? Like, you know, how crazy can we get?
So, in terms of like, deceptive or whatever, like, I think like for me, deception is a very
strong word. Like, this usually involves lies. Like if you're just like being a bit cheeky,
right, you made a little meme or whatever, but like, you're not deceiving people, you're not
tricking people and I'm lying to people, I'm like, it's usually fine. Like there can be pushback
because people are like, stop meeting, like, you're not funny, or like, this isn't funny,
right? Like, stuff like that, right? But like, you know, if you like, as like, you can read
the Commendium, you know, large document that I've written, which kind of summarizes my whole
worldview. And in the beginning, there is an foreword that I wrote, which is like,
closer to a poem than it is to science. It's much more like, it's about reality, obviously,
but like, it appeals to emotions. It's about things, you know, it's about stories about
metaphors about the human story and so on, right? And you can argue, oh, that's deceptive, because
it's not literally true. I'm like, that's not the point, it's art. Like, when art isn't true,
I don't think it's deception, it's art. Like art isn't, you know, if I read a fiction book, I'm not
like, oh, how dare this author lie to me, this didn't happen. No, it's a story, it's a book,
it's a piece of art, you know? So I think art is not deception. Like you can, you know, you can
use art for deception, but I don't think, I don't think that's right. So like, if you create a
touching story or a beautiful video, you know, it's like a beautiful story about your dog or
whatever, and then like at the very end, you know, like slow piano music or whatever, and at the
very end, it's like, you know, we'll be killed by AI or whatever, right? You could say, oh, that was
deceptive, or you can say, you created a nice piece of art, and you can dislike the piece of art,
but I wouldn't call it deception, is like, you created a piece of art that had a message and
had the thing to say, you know, maybe you don't like it, or maybe it's bad, but like, so that's
my take on this. It's just like, I cannot stress enough how important it is to not lie. Like, you
know, I really cannot stress this enough. If we want to win, we need to not lie. It's okay to like,
you know, try to help people and not, you know, find things the right way. It's fine to create
art. It's fine to sometimes get upset or appealed to emotions. I don't consider any of these things
necessarily be lying. But for example, a form of lie that is very common among AI safety,
quote unquote, people, is that is them saying they don't really care about AI safety,
they're actually care about something different. I consider this very bad. I wrote a poll post with
this called lying is cowardice, not strategy, where people, for example, join the government
and not mention AI exclusion risk even once, even when asked, because they say like, oh, if I mention
that, then people think I'm crazy and then like, won't let me in the government. I think this is
very bad. I think this is very, very, very good. They might be right. Like, I'm not saying they're
wrong. But yeah, this is very bad. This is very bad because a, it's lying, which is immoral. Like,
you know, you don't have to bring it up every meeting, right? But like, if someone asks you,
do you think AI has been killed everyone? And you say, Oh, no, that's such a silly thing that
only, you know, then like, fuck you. Like, yeah, like you're lying. Like, that's a lie. And like,
or even just like, I also think a mission in general is a lie. Like, even if I want to ask you,
but you also like, never bring it up. I also think that's like, and it's like, you know, or like,
there's obvious places you could bring it up and you decide not to consistently. I also consider
that like lies by admission or lies. If people say, what's your top three things you're concerned
about? And you list three things that are not AI X risk, even so all three of those things are
things you can start about. So like you're lying, you know, maybe you don't care about X risk,
fair enough. But if you do, and you're like strategically hiding it, I consider it lying.
This is very bad for a number of reasons. One is the deal with the devil. You just straight up
inked in blood, like I will just not even ask for the thing I actually care about
in return for power. So you're already off to a bad start. Second thing is, it's just human
cognition is fallible. If you constantly tell yourself to not mention the thing you care about,
and also everyone around you doesn't care about it, you will start noticing that you care about
it less too. A third thing is that this makes coordination impossible. So there's a big problem
where I know many people in government in AI labs and elsewhere who are really concerned about this.
And they all think they're alone. They all think they're alone. And they all say, I can't say any
thing. Because if I say something, you know, I'm going to get, you know, all the weird looks,
because they know correctly that even if they're in a room and there's like three other guys who
also care, if they speak up, those three guys won't back them up. They know that. And this is really
bad. Because if those three people did back him up, things could change. Because then you could
coordinate, coordination, creating groups of people who can coherently push agendas like this,
require common knowledge. Like, I don't even know who I can coordinate with. Who can I work with?
Because people are fucking cowards. And like, they're not willing to say. Like, you know, some
people just don't know or don't care. Fair enough. But there are people that care or want to care,
and they're just scared. They just don't want to talk about it. And this is, or do it for like,
personal selfish power reason. And this is very bad. This is very, very bad. And I strongly
disagree. All right, Connor, I have two last questions for you. All right, I hope this, this,
this second or last one, you will take in a good spirit, right? So nobody takes this cause seriously
enough, right? We can agree on that. Like, nobody takes AI risk seriously enough.
And I think one of the reasons for it, or a or a an obstacle we have is the way our AI safety
leaders present themselves in some ways. And so my question to you is simply this, I think your
look is fucking awesome, dude. I love your fucking luck. I love the hair. I love the whole, I love
the whole thing. Question is, if, if I could take this hair and snap it onto your head like a Lego,
do you, to make you more effective? A, do you think there's anything to that? And B, would you
consider it in any way? Crazy question. I think there's a lot to that. And by taking seriously,
like, seems like a very reasonable thing. I don't take that in bad spirit at all. You know,
even so, true metalhead erasure. Man, that's like, surely, surely no one appreciates.
But no, I think there is like, there's surely something to this. And I take it a good spirit
whenever people tell me this in the Twitter comments, like, you know, I appreciate you thinking.
I think about things quite different, a bit differently in this regard. It's like,
I'm very brandable. We put it that way. I am extremely recognizable. Everyone knows who I am.
Even people who've only seen me once, you know, this is an asset. It's, it's also downside.
Like, I get stopped sometimes in the street. Like, micro celebrity AI guy sometimes is like,
I had a really funny thing that happened to me. Like a while ago, I was on the BBC,
like late night or whatever. And I went to work then I live in London. And so the next day I went
to work. And then I see this like, construction worker kind of across the street. And he like,
climbs down from scaffolding and like runs towards me, like runs right right up to me.
And I'm like, Hey, you're on the telly. And I'm like, yes. It's like, it's like, you know,
it was fine. It was just funny, right? It was like, it was perfectly polite, perfectly funny,
right? And but like, so there is a, there's a choice here where like, I am not the guy who
should be running the government bureau of whatever, like, this is not my role. This is not
what I'm optimizing for. What I'm optimizing for is to being the guy who can create the
mimetic branding outside game, overton window pushing, like, I have several assets that I work
with, that I am very lucky to have. And like, I don't take for granted. One of the assets is,
is that I am very independent. I am not dependent on AI labs or on, you know, government, like the
goodwill of people in this regard for my funding and my survival and so on. Very lucky that I have
this. And another thing is, is that just like, I am truly not a coward, like, like no offense,
but I am just truly not a coward. I have never been a coward. I'm not claiming this is like
my moral superiority, but just like my genetic makeup and just like, I am just not a coward.
I am not afraid. I am willing to be disliked. I am willing to take shit. I don't like it.
Like, I hate it. I hate that people dislike me or like, or like mad at me. I'm always like, well,
that's sad. But like, it's fine. It's like, okay, I'll just deal with it. It's okay. I'm willing to
take it. In a sense, being a bit of a lightning rod is part of my role is like, is that like, people
get upset or whatever. Like, I'm a good person for them to get upset about, you know, and like,
I'm just like, okay, with that, it's like, so it is deliberate in this regard, right? It's like,
I love, I love that whole answer. That's a like, yes, that's also, I mean, I am just lucky. It's
like, people think I put a lot of work into this. This is just how it is, man. I put shampoo in,
like, once and it just looks like this. So that is a gift. You got it. You got to take it, right?
Yeah, listen.
You know, you know, so, so I think this is a very fair question to ask. And I think there shouldn't
be many people who look like me. I think all things equal, like, you know, I shower every day. I,
you know, wear clean clothes. I don't do other crazy things in my life or whatever. I also like,
don't bring, I don't do weird things in my private life. And if I did, I wouldn't bring them into the
public sphere, cough, cough, certain parts of this movement should learn from that. And, like,
I think this is good. I think there was an obligation to a certain degree that, like,
to be respectful. Like, if you do weird things in your private life, keep it private. Like,
it's okay if you do weird things, but like, fucking keep it off Twitter, Jesus Christ.
Like, if you are a leader of a movement, or you're trying to push, like, Jesus Christ,
at least do us the dignity of, like, keeping it private. Like, Jesus is very, very important.
You know, present yourself well, speak well, be polite, don't insult people, don't lie, you know,
stuff like this. I think it's very important. Awesome. All right, Connor, send it out. Always send
it out with something that gives people hope. Give the audience something that gives you hope
as you look at this thing. The main thing I think that gives me hope is just that when I was a,
when I was more of a tech guy, and whatever, and, like, more isolated, whatever, there's like this,
like, you live almost in this kind of bubble where you kind of, like, think that, like, other people
are, like, stupid, or just, like, mean. They just, like, you don't understand. They don't really care.
You read about, like, stoke neglect or whatever, and you, like, you're like, oh, look, people are so
irrational. They don't actually want to help. They're all just selfish or whatever. And, like,
yeah, there's some of this. But, like, there is just this, like, deep thing where just, like,
I've come just, like, really deeply in my heart to appreciate that, like,
like, not everyone, not in all circumstances, but, like, most people are, like, really
decent people, and they want good things. They love their kids. They love their
partners. They want people to be happy and healthy. They don't want to hurt anybody, you know? And
it's just, like, you know, when, you know, I was an atheist, still I am, obviously, but, like,
I talk to people of faith or whatever, and I'm just, like, like, you know, I had this, like,
vision in my head is all these idiots who are just union for power or whatever. But, like,
when the first time I met someone who, like, really believed in Jesus, like, really believed,
I was, like, whoa, like, that's actually really nice, like, dude, like, like, I had this real
moment where I'm like, I'm not religious. I'm not going to buy any of this. But, like, wow,
wow, you're not fucking around. Like, you actually believe this. Like, you're actually
trying to be a better person. Like, whether it's a faith to just mean what you can't see.
Yeah, it's like, it's, I'm just like, wow, like, if this is what Jesus does for you, I mean, like,
wow, dude, like, that's, like, you know, it's not for me. And, like, I do think there are, like,
problems about believing things that are not true and, like, epistemology and whatever, but also,
like, you're not, like, there's this thing that, like, nothing in the world annoys me more than
hypocrites. Like, truly, nothing in the world gets more under my skin than someone who, like,
claims to believe something but doesn't really. They're just faking it. So, like, whenever I meet
someone who's like, if you're just like, you know, your village pastor who just, like,
really just wants people to be okay, and he cares about the people in his village, you know,
like, like my grandma, she got very sick before she died. And, like, she had this pastor who just,
like, care a lot about, right? And she was very, had, like, bad dementia. Like, she was,
she was in a bad spot. It was not fun to be around her. She's also got, like, really angry all the
time. And it was a mess, right? But, like, he came. He did visit her sometime. And it meant so much to
her. It meant so much to her. He didn't have to do that. And, you know, you know, I'm just like,
that's nice. Like, it's nice of him to do that. Like, it doesn't save the world that this
is really the best thing he could be doing with this time. Like, probably not. She's going to
forget it immediately. You know, but, like, also, like, there's just something genuinely nice about
this and genuinely touching about this. And so, to bridge that to the larger thing, there is this
thing where, like, the world is complicated, right? And there are big problems that we are facing.
And the way humans solve problems is, like, half historically solve big problems is through,
like, kind of human ways. Like, there's this thing where, like, it can be frustrating to say this,
that, like, we solve, we're humans and we solve things in human ways. But also something beautiful
about this, like, there's a way to see this as something is that the solutions don't have to be
cruel and heartless and machines and, you know, inhuman. The solutions can be very human and
actually are very human. That often makes them boring. Like, when I talk about civics, like,
what is the actual response to AGI risk? The actual response is that we need to do our civic duty.
We need to talk to our fellow civilians. We need to talk to our friends, to our families, to our
churches. We need to say to our policymakers, what are you doing about this? I am a concerned
citizen. I don't necessarily have a solution. But I think this is a problem. What do we do about
this? It's not epic. It's not, wow, some super genius cooks or whatever. This is, like,
this is weird cult of the hero. There has to be, like, some lone genius or some superhero that
solves it or whatever. But that's just not how the real world works. The real world, like, there
are some super smart people and, like, whatever. But, like, what builds a good society is civilians,
just like normal people who just, like, want to live a good life, they care, you know,
some people care more, some people care less, and that's okay, right? And so what brings me hope is
that, like, fundamentally deep down, people still have this. Like, there is a thing in the heart and
the soul of people who are, like, they want a better future for their kids. They might necessarily
know how. They might be scared. They might be confused. They might be even doing something
really stupid right now, right? But, like, there is a deep thing where just, like, for the most part,
like, people just, you know, they want to, they want to have food in a nice house and spend time
with their kids and, you know, you know, see good art and whatever, right? And they want this for
other people too. And when you talk to them about, like, when you say, like, you know, what do you
think about, like, things that are smart unless we don't control, they get it. Like, there's this
deep thing that just, like, people totally get this. Like, it's only tech people and, like, you know,
super overeducated elites that don't get ex-risk. It's only them. The normal people totally get this.
Like, people are not stupid. You know, like, this is, like, deep thing is just, like, you know,
people who don't come from a tech background, don't come from a STEM or even from an educated
background. You know, sure, some of them are stupid, whatever, right? Exists everywhere. Like,
for a lot of them, good people. They're good people. They're not bad people. They're not stupid.
They might not know what to do. They might not have the resources to do what needs to be done or
whatever. But there is a vision. There is a vision of a future for me that is humanist. That it's not
about, you know, technology helps. But it's fundamentally built of people. It's built of people.
And I think this can be done. I'm not saying it will be done, but it can be done. It's that people
do care. Like, this is a deep thing where people feel that they're alone. They learn about AI risk
and they think they're alone, is that no one will ever believe them. No one will ever listen to them.
You know, the world is against them and I'm here to tell you this is not true.
The tech world is against you. But the wider world, they not only need your help, they want your
help. Like, I talk to people about this and they're thankful because I am trying to help them and they
want to help me. And this is really great. Like, this is a good thing of like, how can we
mutually prevent a thing? So rambling answer, sorry for that. But like, for me, it's just like,
you're not alone. You're not alone. The world has many dark aspects to it. There are many,
many dark things in the world. But like, god damn it, like humanity has a lot of good in it. Like,
all, you know, like, despite everything, despite all the wars and ideologies and the evolution
and nature to the clock, a lot of people are nice people and love other people and want things to
go well. And that's our strongest asset. And like, that's, and that does that's not enough to win.
It's not enough to win. But it is necessary. And we do have that. So great place.
Yeah, that's where we start. Connor, thank you so much, man. Thank you so much for your time.
Thank you so much for doing what you do. I'm truly so appreciative of you and I wish you all the
luck and good fortune going forward because we got to get this thing done.
We sure do. Thanks, man.
How about Connor Leahy? I am so thrilled that he is on the planet fighting on the right side.
And, you know, I'm really taking what he said to heart, right? We're not here to wake up the world.
We're here to help people understand. There is no controversy about what is to come.
There's only do we stop it or do we accept it? Okay, so as you know,
in these strange times, we don't know how long we have to live. So every time we get together
for one of these shows, we end it with something we call a celebration of life, just something
that makes us thrilled to be alive. I have a treat for you all here today. The incredible
Kevin Garrett live in concert shop with my iPhone this weekend. This dude was selling
his own merchandise, his own t-shirts in the front of the club before the show playing to
like a 200 person club in Washington DC. But he should be selling out Madison Square Garden,
I think my opinion. So here's the incredible Kevin Garrett covering a Leon Russell classic.
I've acted out my life in stages with 10,000 people watching.
But we're alone now and I'm singing this song to you.
I know your image of me is not what I'd hoped it'd be.
I've treated you unkindly, but girl, can't you see?
There's no one more important to me. Baby, can't you see right through me?
Were you alone? I was singing this song to you.
You taught me precious secrets of a true love and holding nothing.
You came out in front when I was hiding.
But now it's so much better if my words don't come together.
Just listen for the melody because by now I'm sitting there hiding.
I love you in a place where there's no space or time.
I love you for my life. You're a friend of mine.
But when my life is over, remember that we were together.
We were alone and I was singing this song to you.
We were alone and I was singing this song to you.
I mean, that dude is incredible.
So what did you say? You said you want one more song? Okay, fine, we'll do it.
Okay, here is one more Kevin Garrett song from that show.
This is one of his original songs. He has four full albums. They're all fantastic.
I encourage you to check it out. Here's one more from my guy Kevin Garrett.
You are in my head when my heart's so warm.
And if I'm ever scared, I'll breathe the air in front of your door.
But I will never know.
And that's as far as I'll go.
It's only in my dreams when I say what I mean, but I'll get damn near close.
Ooh.
Ooh.
And I'm a stubborn.
I'll hide as best I can.
Got this bright idea, if I show my fear I will be a less of a man
And I think too much, but I don't feel enough
With a gun to my head, then I might confess that it's you I love
Oh, my hands are tight, around you each night
And you made me into what you want
And it may seem like I've fled from you once
And maybe we'll fall further from us, but I won't run
Cause I'm here right
If you take the time to let me stumble through
How I feel about you as I mess up my life
And you hear me, let's just say it will
Cause if I mouth these words when I'm crashing on you
You will take me still
Oh, my hands are tight, around you each night
Oh, my hands are tight, around you each night
Thank you very much
I put a tremendous amount of my own time and funding into this show
This effort, it would mean a great deal to me if you signed up to be a monthly subscriber
That would be just awesome
Please remember, AI risk is not someone else's problem
It is yours and it is mine
And in 2025, we are going to help every human on earth understand the fact that AI risk is their biggest problem too
This is the year we help humanity understand what AI risk is all about
We will not allow 2,000 people to wipe 8 billion of the rest of us and all living things off the face of the earth
No fucking way
For Humanity, I'm John Sherman, I'll see you tomorrow
Thank you

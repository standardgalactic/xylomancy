In biotech, it's exactly the other way around.
If you had come up with a new medicine
and said, this cures cancer, it's awesome,
you can't just go sell it in the supermarket.
The monk debate focused on AI safety,
and it was absolutely extraordinary.
Claiming that AI is an existential threat is itself harmful.
It misleads people about the current state
and likely future of AI.
I've been excited to put together this show
for some time now.
It's spicy.
Apologize I'm going to be cursing more in this show
than I normally would.
Such sensationalist claims deflect attention
for real immediate risks.
One set of emotions that we can hardwire into them
is subservience.
The most flamboyantly obnoxious cavalier.
Shit on my leg and tell me it's raining.
I'm going to cheer as the world ends.
Risk means nothing to me.
The fuck your kid's future lunatic
is the director of AI research at Metta.
His name is Yan Lacoon.
Of course, if it's not safe, we're not going to build it.
Right?
I mean, will you build a bomb that just blows up randomly?
No, right?
If it's not safe, we're not going to build it.
What a total load of shit.
How the fuck would you know if it's not safe
until it's too late to stop it?
That's the whole fundamental fucking problem
with this situation.
Welcome to For Humanity, an AI safety podcast,
episode six, The Monk Debate.
I'm John Sherman, your host.
Thanks so much for joining me.
This is the AI safety podcast for the general public,
no tech background required.
This show is exclusively about the threat of human extinction
from artificial intelligence.
Please, right now, if you would, hit like
and hit that subscribe button if you haven't hit it already.
Repost these videos on your personal social channels,
and please tell people about this show.
Those are some ways that we can help spread the word
about these issues.
Last week, I challenged you to convince someone in your life
that the risk of human extinction
from artificial intelligence is an issue
that requires their attention.
This week, I want to go one further
and challenge you again.
This week, I want to challenge you
to talk to someone about these issues
that is totally outside of this debate,
someone that has no clue that AI is a threat to them.
Here's an easy place, I think, to start.
Every mom and dad out there needs to know about this.
Moms and dads, fellow moms and dads, AI
is the biggest threat to our children in the short term.
It's not fentanyl, it's not some creeper in the park,
it's not drunk driving.
AI is the biggest short-term threat to our children.
Parents will do anything for their kids.
So let's start with parents.
Maybe this can be their wake-up call.
I have been genuinely encouraged by the reaction
I've been getting to the show online and in person,
but we can get this message out much further.
And if you like what I'm doing,
I'm asking you to step out of your comfort zone,
share this very serious, very unsettling news
with someone you know.
I'm gonna tell you something.
It is not gonna be a fun experience.
It sucks to be talking about this stuff.
I want you to know that I hate sitting here
and talking about this stuff,
as much as you probably do sitting there and hearing it.
It fucking sucks.
It unthinkably, absolutely fucking sucks
to seriously talking about human beings.
Human extinction.
But we are really threatened by AI
killing all humans in as soon as two years.
That is not hyperbole.
That is literally what the AI companies
and AI safety experts are openly admitting and telling us.
So I can't just go about my normal life anymore.
And if you believe what I've been telling you
and what they've been telling you,
you really can't either.
So I'm gonna sit here every week
and try to get the word out.
And I need your help,
and I need the help of everyone you know.
What if one year from today, everyone on earth
was of the understanding that AI is a lethal threat
to their family, to their children,
to their children's yet unborn children,
to every living thing on earth?
In just a minute, we'll get today's show going.
And I think you're gonna really love it.
I've been excited to put together this show for some time now.
It's spicy.
Apologize, I'm gonna be cursing more in this show
than I normally would.
But first, I want you to hear from some important people
in the news who are openly recently discussing
totally unthinkable things about artificial intelligence.
So PDOOM is a technical term used for the percentage chance.
We all die.
Anthropic CEO, Dario Amadei,
recently put it at 10 to 25%.
AI Safety Research King, Elias Ryukowski,
puts it at 98%.
In the last week or so in the news,
enter Lena Khan, the commissioner,
excuse me, the chairman of the Federal Trade Commission,
who made news recently
by dropping her own personal PDOOM.
The percentage chance, we all die from AI.
What is your PDOOM, Lena Khan?
What is your probability that AI will kill us all?
I have to stay an optimist on this one.
So I'm gonna hedge on the side of lower risk there.
So are you zero?
No, no, not zero, maybe like 15%.
Oh, all right, yeah.
And they say there are no techno optimists in the government.
This is a very high-ranking US government official.
Did she go running back to her office
and say, stop everything,
we have to do something about this?
No, not at all.
Somehow, a 15% chance of every last human on Earth
being murdered by AI is somehow,
acceptable.
This is the cost of doing business?
There is no fucking business if we're all dead.
There was a ton of drama at OpenAI recently,
as I'm sure you know,
it seems very clear that money and speed won
and safety lost.
But for three days,
OpenAI had an interim CEO named Emmett Scheer,
the former CEO of Twitch.
This is an important Silicon Valley leader
who clearly gets this stuff.
Listen.
Generally, I am very pro-technology
and I really believe the upsides
usually outweigh the downsides.
Every technology can be misused.
Regulating early is usually a mistake.
I have a very specific concern about AI.
We built an intelligence, it's kind of amazing actually.
It may not be the smartest intelligence,
but it is unintelligence.
It can solve problems and make arbitrary plans.
At some point, as it gets better,
the kinds of problems it will be able to solve
will include programming, chip design, material science,
power production, all of the things you would need
to design an artificial intelligence.
At that point, you will be able to point
the thing we've built back at itself.
And this will happen before you get that point
with humans in the loop.
It already is happening with humans in the loop,
but that loop will get tighter and tighter and tighter
and faster and faster and faster
until it can fully self-improve itself,
at which point it will get very fast very quickly.
And that kind of intelligence is just an intrinsically
very dangerous thing because intelligence is power.
Human beings are the dominant
for our life on this planet pretty much entirely
because we're smarter than the other creatures.
Now, I just laid out a chain of argument
with a lot of if this, then this,
if this, then this, if this, then this.
I know Eliza thinks that like we're all dooms for sure.
I buy his doom argument.
I buy the chain and the logic.
Like my P-doom, my probability of doom is like,
my bid-ask spread and that's pretty high
because I have a lot of uncertainty,
but I would say it's like between like five and 50.
So there's a wide spread.
I think Paul Cristiano 50, you know.
Paul Cristiano who handled a lot of the stuff
with an open AI, I think said 25 to 50.
It seems like if you talk to most AI researchers,
there's some preponderance of people that give some percentage.
That should cause you to shit your pants.
I think it's important for you to hear
just how little of a secret
the existential threat we face is.
This is not fringe conspiracy shit in any way.
This is not opinion.
I'm just connecting the dots of the facts
that the makers of AI and many others
are laying out there openly saying.
So on to today's show.
All right, wouldn't it be great
if AI safety was substantively debated
on American television every night?
Talk about everything else under the sun, so much crap.
Wouldn't it be great if we talked about
really the important things?
Well, in Toronto, there's a wonderful organization
that runs what's called the Monk Debates.
They're a series of exceptionally well done televised debates
on a wide variety of topics affecting public life.
The Monk Debate focused on AI safety
and it was absolutely extraordinary.
Each debate is framed around a single statement
and it's two on two, pro versus con.
The statement for the AI safety debate was
be it resolved, AI research and development
poses an existential threat.
Be it resolved, AI research and development
poses an existential threat.
Well, as you should know from watching this show
for humanity, that statement should not be controversial.
It is a widely accepted fact in most circles.
It's something nearly everyone who is anyone
in big AI signed a statement saying
that AI is an existential threat
along the lines of pandemic and nuclear war.
So the whole premise of this debate
is something nearly the whole industry
already concedes, but not everyone.
Not nearly, sadly.
For my non-tech audience out there,
it may shock you to know that there is a small
but extremely powerful group of people in Silicon Valley
who call themselves effective accelerationists.
They believe that we will solve humanity's problems
by racing towards technological advances.
For the purpose of this show,
we're just gonna call them accelerationists.
Since I opened the for humanity X account
more than a month ago, I've been in there mixing it up
with a lot of people and I have gotten a real palpable sense
of these folks and their attitudes.
It's my personal opinion.
They're obnoxious, they're bullies,
they're know-it-alls of the worst kind
and they somehow have the audacity and arrogance
to believe that you and I and everyone else on earth
has given them permission to risk ending the future
for our families forever.
They believe we've authorized this,
but their desire to accelerate technology
affects everyone, not just them.
So the single biggest asshole among them,
the most flamboyantly obnoxious cavalier,
shit on my leg and tell me it's raining,
I'm going to cheer as the world ends,
risk means nothing to me, fuck your kid's future lunatic,
is the director of AI research at Metta.
His name is Yann Lecun, he's French
and that kind of kills me because I love the French
and hearing him say all this bullshit
in that beautiful French accent makes it even more painful.
Yann is one half of team Khan in the monk debate
on AI safety.
Listen closely as you hear him speak,
you'll hear no specifics.
He's gonna tell you that something much smarter
than a human is going to be subservient to humans.
Ladies and gentlemen, with no further ado,
presenting the one and only Yann Lecun.
Those systems are controllable, they can be made safe,
as long as we implement the safety objectives.
And the surprising thing is that they will have emotions,
they will have empathy,
they will have all the things that we require
entities in the world to have
if we want them to behave properly.
So I do not believe that we can achieve anything close
to human level intelligence without endowing AI systems
with this kind of emotions,
similar to human emotions.
This will be the way to control them.
Now, one set of emotions that we can hardwire into them
is subservience.
So I have a positive view, as you can tell,
and I think there is a very efficient way
or good way of making AI systems safe
is gonna be arduous engineering,
just like making turbojet safe, it took decades.
And it's hard engineering, but it's doable.
Just like making a turbojet, says Yann Lecun.
That is deliberately misleading.
Giving them emotions will allow us to control them,
says Yann Lecun.
That is incredibly optimistic speculation at best.
We can hardwire them into subservience, says Yann Lecun.
That is simply ridiculous to claim.
We might be able to do that,
as many safety researchers say, though, that we won't.
But even those who say we might be able
to make them subservient says it would take decades,
like three to five decades, to do the research
to figure out how to do that.
And as you know, the timeline is two to 10 years
for AGI, the Singularity and the Unknown Future after that.
These two timelines are catastrophically misaligned.
But let's get back to the debate.
So Yann's tag team partner is Melanie Mitchell.
She has been working on AI research since the 1980s
as a college student.
I will let Melanie Mitchell introduce herself,
but I will say, holy shit, she is wild.
She's pissed off.
We're even talking about existential risk.
First, I'll argue that the possible scenarios
that people have dreamed up for AI existential threats
are all based on unfounded speculations,
rather than on science or empirical evidence.
Second, while we can all acknowledge
that AI presents many risks and harms,
none of them rise to the extreme level of existential,
saying that AI literally threatens human extinction,
says a very high bar.
Finally, claiming that AI is an existential threat
is itself harmful.
It misleads people about the current state
and likely future of AI.
Such sensationalist claims deflect attention
for real immediate risks.
And further might result in blocking the potential benefits
that we could reap from technological progress.
Holy fucking hell.
All right, so I have not done a great job
in the first five shows of letting you hear
from the other side of this debate,
but that's gonna change right now.
I haven't really included the other side
of the AI safety debate so far
because objectively, I find their case so incredibly weak,
but you're gonna need to make up your own mind on this.
So today, we're gonna show you a lot of Jan and Melanie,
team acceleration, team, we're willing to risk
whether your kids get to have kids,
and we're gonna do so without even attempting
to gain your consent.
Here's some more of Jan Lacoon laying out his case
as to why AI is in no way a threat of human extinction.
I hope you'll enjoy as I do
when the Monk Debates moderator jumps in to correct
the completely disingenuous, misleading frame
that Jan puts around the question of why an AI system
would cause human extinction.
As you well know by now, human extinction would not come
from an AI system with a desire to arbitrarily dominate.
These systems won't be mad at us or vindictive.
AI safety experts believe they will just have
different goals than ours.
But here's Jan spinning his deceptive case
and getting called on it.
Because we are social animals, we're a social species,
and nature has evolved us to organize ourselves
hierarchically like baboons, like chimpanzees,
not orangutans.
Orangutans have no desire to dominate anybody
because they're not a social species.
So this desire to dominate has nothing to do with intelligence.
They're almost as smart as we are, by the way.
So this has nothing to do with intelligence.
We can make intelligent machines that are superior to us,
but have no desire to dominate.
I lead a research lab.
I only hire people who are smarter than me.
None of them want my job.
Now.
But Jan, is it just a second point?
Just to refer to the other side of the debate,
because it's something I think the audience would appreciate
understanding.
It's not so much the desire to dominate,
it's the control problem.
It's that you've set them some goals,
maybe very noble and great goals,
but they start doing other things to achieve those goals
which are antithetical to our interests.
It's not that they're trying to dominate.
It's that there is a tragedy of the commons that goes on.
And it's the same thing with companies.
This is the goal and I'm not a problem.
So how do we design goals for machines
so that they behave properly?
And again, this is something that's,
you know, a difficult engineering problem,
but this is not a problem that we are unfamiliar with
because as societies, we've been doing this for millennia.
This is called making laws.
Sure, we'll just make some laws.
Problem solved.
We've been solving problems like this for millennia.
No, we have not.
No problem ever before in our past is in any way
like the challenge of artificial intelligence.
But don't worry, I don't have to make the counter arguments.
We have an all-star duo on the other side of this debate.
Please say hello to our old friend
and powerhouse AI safety researcher
and my tea professor, Max Tegmark.
Back in the Stone Age with a rock,
maybe someone could kill five people.
300 years ago with a bomb, maybe a hundred people.
In 1945 with a couple of nukes, 250,000 people
with bioweapons, even more, with nuclear winter,
according to a recent science article,
over five billion people.
So now the blast radius has risen up to about 60% of humanity.
And since, as we'll argue, superhuman intelligence
is going to be way more powerful than any of this,
its blast radius can easily be 100% of humanity,
giving it the potential power to really wipe us out.
Again, this is different than anything else ever before.
Nuclear bombs cannot make more nuclear bombs by themselves.
Nuclear bombs cannot decide to detonate themselves
at a location and time of their choosing.
This is different than anything else ever before.
Max, please, tell the people.
It can do all the intelligent things
that we humans can do, just better.
For example, it can do goal-oriented behavior.
It can persuade, manipulate, hire people,
start companies, build robots, do scientific research.
It could, for example, research
how to make more powerful bioweapons,
or how to make even more intelligent systems
so it could recursively self-improve itself.
It also could do things that we humans cannot do at all.
It could very easily make copies of itself.
So if you imagine for a moment a superhuman AI
that can think, say, a thousand times faster
than a human researcher,
so it can in nine hours do a year's worth of research,
but now instead think of a million of those,
a swarm of a million superintelligent AIs,
where as soon as one of them discovers something new,
it can instantly share that new skill with all of them.
That's the kind of power we're talking about here.
And finally, superhuman AI will probably be
a very alien kind of intelligence
that lacks anything like human emotions or empathy.
I could listen to Max Tagmark talk all day.
I really hope to have him on this show
sometime in the near future.
In biotech, it's exactly the other way around.
If you had come up with a new medicine
and said, this cure is cancer, it's awesome,
you can't just go sell it in the supermarket
until someone proves it is theirs.
It's your job to convince the Food and Drug Administration
in the US or the Canadian authorities or whatever
that this is safe and that the benefits,
yes, the benefits outweigh the risks.
It should be the responsibility of the companies
that the first prove that this is safe
before it gets deployed.
We need to become like biotech.
In this monk debate, Max is hardly alone.
This is kind of fun,
got a little bit of soap opera drama for you.
So the highest prize in computer science
is called the Turing Award,
named after Alan Turing, the father of AI
who we talked about a little bit last week.
In 2018, the Turing Award went to three AI researchers
for their breakthrough work on neural networks
and artificial intelligence,
which was one of the two major advancements in AI
that got us to where we are today.
The three Turing Award winners in 2018 were Jeffrey Hinton,
who you've met in previous shows who quit Google
famously to voice his concerns about AI and human extinction.
The other two winners were two of his former students
from the University of Toronto.
So on our stage at the monk debate,
we have two of those former students,
the two former students who won the Turing Award
with Professor Hinton.
One of them is Yann LeCoune.
He now says his old mentor and Professor Jeffrey Hinton
is a doomer who's totally wrong about AI risk.
The other of Professor Hinton's old students
is Max's teammate for the debate,
AI researcher, Yoshua Benjiro.
Once we have machines that have a self-preservation goal,
well, we are in trouble.
Think about what happens when you want to survive.
You don't want others to turn you off, right?
And you need to be able to control your environment,
which means control humans.
So existential risk isn't just, well, we all disappear.
It might be that we're all disempowered,
that we are not anymore in the control of our destiny.
And I don't think this is something we want.
Not extinct, but not in charge.
That is not something we want at all.
But this debate, just like this show,
we're gonna focus only on human extinction risk.
Yann LeCoune thinks there's no extinction risk at all.
Watch as he mocks this grave concern
as mere science fiction and Marvel
as he backs up his bold claims
with absolutely nothing.
As fiction scenarios of the Earth being wiped out,
humanity being wiped out,
this sounds like a James Bond movie, right?
It's like the supervillain who goes in space
and then puts some deadly gas and eliminates all of humanity.
It's a James Bond movie.
And I can't disprove it.
The same way, if I tell you I used the Bertrand Russell idea,
if I tell you there is a teapot flying
between the orbits of Jupiter and Saturn,
you're gonna tell me I'm crazy,
but you can't disprove me, right?
You can't disprove that assertion.
It's gonna cost you a huge amount of resources to do this.
So it's kind of the same thing with those doom scenarios.
They're sci-fi, but I can't prove that they're wrong.
But the risk is negligible.
And the reason it's negligible of extension
is because we build those things, we build them.
We have agency.
This is not superhuman intelligence.
It's not something that's gonna just happen.
And so, of course, if it's not safe,
we're not gonna build it, right?
I mean, will you build a bomb
that just blows up randomly?
No, right?
If it's not safe, we're not going to build it.
What a total load of shit.
How the fuck would you know if it's not safe
until it's too late to stop it?
That's the whole fundamental fucking problem
with this situation.
But, Jan, you know this much better than I do.
So seriously, this is it, right?
It would be reasonable after watching the first five episodes
of this podcast to say,
I'd like to hear more from the other side about this.
So this is your chance to evaluate
the strength of their logic.
I went into this hoping to be persuaded by them,
hoping to be persuaded that everything is fine.
But now it's your turn.
Let's do a little Jan and Max-mult-debate soundbite ping-pong
and then you decide for yourself.
If you think everything is just fine.
Remember what people were saying just before year 2000?
Satellites were gonna fall out of the sky
and crash into cities and the phone system
was gonna crash and civilization will end.
So it didn't happen.
We're still here.
So I think there's a little bit
of the same kind of feeling of uncertainty.
A lot of people have the feeling
that bad things are gonna happen
because they're not in control.
They have the feeling that AI is just gonna happen
and there's nothing they can do about it
and that creates fear and I can completely understand that.
But some of us in what could be construed as a driver seat
there are plans to make those things safe.
Stock traders will tell you that past performance
is not an indicator of future performance.
Future results, yeah, future results.
And it would be a huge mistake
in an exponential technological growth
to assume that just because something happened one way
in the past is gonna continue being this way.
AI is gonna be subservient to human.
It's gonna be smarter than us
but it's not gonna reduce our agency on the contrary.
It's going to empower us.
It's like having a staff
of really smart people working for you.
It's naive to think that just because you make something smart
it's only gonna suddenly care about humans.
Ask some woolly mammoths
if they feel so reassured they were smarter than them.
Then therefore we would automatically adopt mammoth ethics
and do things that were good for the mammoths
that we didn't.
That's why you probably haven't met any.
I'll tell you what I think.
I think Jan is full of shit
and Max makes all the sense in the world.
I really wish it was the other way around.
I really, really do, but it just isn't at all.
There are some great moments in the monk debate
on AI safety where the teams really start
to mix it up with each other
and there are some really incredible exchanges
I wanna show you.
First, here are the 2018 Turing Award winners
once united by science now divided
by the potentially existential results of their work.
Mixing it up, Jan Lacoon goes for some classic
historical comparison bullshit
and Yoshua Bengio responds
with what I think is force and logic.
Socrates was against writing.
He said people are going to lose their memory, right?
The Catholic Church was against printing press
saying they would lose control of the dogma, which they did.
They could do nothing about it.
The Ottoman Empire banned the printing press
and according to some historian,
that's what accelerated their decline.
And so every technology that makes people smarter
or enables communication between people
facilitates education, again, is interestingly good.
And AI is kind of a new version of this.
It's a new printing press.
So long as it doesn't blow up in our face.
And the thing is that the only thing is-
You can smash your hand with a printing press, right?
Yes.
If it's the problem is the scale, right?
So long as we built technologies
that could be harmful but on a small scale,
the goods, the benefits overwhelm the dangers.
Now we're talking, wait, wait, wait.
Now we're talking about building technologies
unlike any other technology.
No.
Because it's technology that can design its own technology.
No.
Yes, no.
I'm talking about superhuman AI.
This is the subject.
It's under control.
It's under control and we remain under control.
It's very much like previous technology.
It's not quite it to be different.
The experts have been studying this question,
say it's gonna be very hard to keep it under control
and that is why I'm here today.
Melody Mitchell is so special.
She's pissed off we're even talking
about existential risks from AI.
Seriously, throughout the month debate,
she sounds off on why it is bad for us
to even talk about existential risk from AI
when there are in her eyes
so many more important, urgent threats
that need our attention such as disinformation
and fake photos and videos.
It takes our attention away
from some of the real immediate risks
like disinformation and bias.
So are you saying it's a 0% risk?
0% existential risk?
Is that your claim?
Do I think there's a 0% risk of any scenario?
No, of course not.
What do you think it is then?
1%?
We're really debating.
I can't put a number on it.
I think it's quite low.
And I think what we're debating
is there a reasonable existential risk,
a risk of end of civilization in the reasonable future?
Otherwise, we wouldn't be up here to-
How high is-
I'm not gonna like say 0.0000001.
I mean, I can't say that.
How high is too high for you then?
How high is-
How high a probability of a risk is too high for you?
I don't think we can put a probability on it.
We don't know.
We don't know enough on the same list.
Okay, it's lower than yours being wiped out by meteor.
And by the way, I can help with that problem.
This is the essential question for anyone
who wants to accelerate artificial intelligence.
What percentage existential risk to humans
is acceptable to you?
In my opinion, when you are talking about
a potential future with zero human beings,
the only acceptable risk percentage is also zero.
Here are the two former award-winning partners
now on opposite sides of the question of,
don't you just need some good guys with good AI
to beat some bad guys with bad AI?
Decide for yourself if you think Yan Lacun
is being appropriately optimistic or stunningly blind.
Bad guys can use AI for bad things.
There's many more good guys who can use the same,
more powerful AI to counteract it.
So in the end-
Those good guys are gonna win.
Sometimes the attacker has the advantage.
There is absolutely no reason to believe
that's the case for AI.
In fact, we are facing this situation right now.
How do you know?
There's never anything that is completely perfect.
Well, that's the issue.
That's what we need to do more than what we're doing now.
Again, it's the good guys' AI, which is superior,
to the bad guys' AI.
That's like saying that the way to stop a bad guy
with a bio-weapon is to have a good guy with a bio-weapon.
That's not what you do.
The way you stop a bio-weapon attack is with vaccines
and banning bio-weapons
and having various forms of regulation and control.
Next, Max asks Melanie what I think is a reasonable question.
Why the product maker isn't responsible
for proving their product is safe
before they release it to the general public?
But why is it our role to explain to you why it's dangerous?
You still haven't answered my question.
I never said it wasn't dangerous.
I've asked you twice.
What is your plan for avoiding misuse?
You haven't told me.
You haven't asked you twice.
What's your plan for solving the alignment problem?
I think.
You haven't told me.
I've asked you twice.
Can we just finish?
Would you tell me what your plan is
for avoiding the scenario where we get out-competed
in disempowerment?
You've said nothing.
Unfortunately, unfortunately,
I don't get paid enough to solve all these problems
about AI policy.
You either fill in medicine or you have to have a plan.
I don't have to explain why.
I think the AI community is developing plans
to mitigate risks.
I think the community is developing plans
to mitigate risks, she says.
Is developing plans?
This is like saying, we'll figure out
how to make landing gear mid-flight.
We are already currently cruising as a world
at 30,000 feet in a plane with no landing gear.
The pilots already said, fuck it, let's just take off.
We'll figure out landing gear mid-flight.
We're hurtling through the air, no landing gear,
and no knowledge of when we actually need to land,
no knowledge of the timing.
But the pilots are saying, please stay calm,
do enjoy your snack and beverage service.
It's nuts.
Let's get back to the debate
where Yoshua channels my inner spirit
and tells Melanie, you're wrong.
They might want to preserve their existence.
They don't want to do anything, they're not alive.
No, that's very easy to do.
No, you're wrong, you're wrong.
The systems that, for example,
chat GPT is essentially like an Oracle,
it doesn't really have a want,
although a little bit he wants to please us
because of the reinforcement learning.
Yes, it's been trained to please us.
Then it's actually easy to put a wrapper around them,
to turn them into agents that have goals.
It's actually easy to do.
That humans gave them.
Yes.
Not their own goals.
Yes, and in order to achieve those goals,
they're gonna have sub-goals that,
and those sub-goals, for example,
may include things like deception.
Okay, finally, in one last ditch effort,
Melanie Mitchell tries to drop.
Well, the people I talk to all think just like I do,
and they all think everything is fine too.
You say the people you talk to in AI have this belief.
Well, the people I talk to in AI don't have that belief.
I think there's, the field is quite split, I would say.
And I think there's quite a third.
Did it coin at 50% that we all get a dime?
Any of the people who signed this letter
are saying that this is an existential risk?
There's many people who have signed letters
saying they think it's an existential risk.
They don't say over what time scale.
Yeah, but this was Jeff Hinton.
Yeah, Jeff Binton is a godfather,
but he doesn't know everything.
You don't talk to him.
Yeah, I think that he's a smart guy,
but I think that a lot of people
have way over-hyped the risk of these things,
and that's really convinced a lot of the general public
that this is what we should be focusing on,
not the more immediate harms of AI.
Over-hyped, science fiction, just another Y2K,
just like the invention of the typewriter.
Please tell me in the comments,
if you think these arguments made by Melanie and Jan
are not total bullshit, maybe I'm missing something,
but I find them not only to be incredibly weak,
I find them to be deceptive and manipulative.
The audience at the end of the month debate
was asked to vote for who they thought won.
By a score of 67 to 33,
they decided that Max and Yoshua were more convincing
than Melanie and Jan.
So I hope you got a lot out of that debate.
I would encourage you to go to the Monk Debates website.
They have a variety of debates
on a lot of really interesting subjects,
and they're all exceptionally well done
just like that debate was.
So for six weeks now,
I've been putting out a podcast per week,
and as I said, I'm really encouraged
by the response I've been getting.
The question is, who will win this battle for humanity?
The accelerationists like Jan Lacoon, Melanie Mitchell,
and Sam Olt, or the decelerationists like Max Tegmark.
Currently, if it was an NFL football game,
I think the score would be something
like 45 to three accelerationists.
Acceleration is dominating in funding, progress, and talent.
The game is in the second half,
but we don't know when the whistle is gonna blow
to say that the game is over.
The odds could not be worse.
The situation could not be more dire.
You, me, and everyone else alive right now
can either be blindly let off a cliff
or we can work to have a voice in this,
work to learn what is happening and what is to come,
and do anything and everything we can
to give our children a future.
So next week, we're gonna do something really different.
Our show aims to move this debate
from Tech, YouTube, and X
into the family dinner table conversation.
I think the voice of parents
is sorely missing from this debate.
As a parent myself, I can tell you,
I've often said that if I was standing on a street corner
and my son or daughter was next to me
and a bus was coming at us
and they were falling into the path of the bus
and I could pull them back
and I would have to fall and be killed by the bus
instead of them, that I would see them falling
towards the street corner to safety.
I would fall myself into the street
knowing the bus was about to hit me
and I would do so with a big smile on my face
knowing I'd saved the life of my child, right?
That is how parents are.
Parents will do anything to protect their kids.
Well, my fellow parents,
it hurts me deeply to tell you
that your children are under a threat more grave
than any threat faced by any children ever before.
Mine are too.
Parents, there is an intruder in your home.
There is an intruder in your school.
It came from Silicon Valley.
It is an alien and it doesn't care about anything
that you care about.
So next week, I'm gonna talk to three moms
who are totally new to this AI safety debate.
Just three regular moms who through this podcast
have become aware of this most grave threat
to their families.
I wanna know how it feels to know
that this biggest threat hasn't even been on their radar.
What they think about a few thousand people
in Silicon Valley who are more than comfortable
risking the mass murder of every child on earth
and how these moms plan to go forward
knowing what they now know.
Should be an interesting show for sure.
Thank you so much for watching.
For Humanity, I'm John Sherman.
I'll see you back here next week.

All right, well, Noam Chomsky, I think it's time for me to turn this over to you.
I really want to give you a great big welcome from OliU of A and appreciate your coming today
and presenting to our organization, and we're truly an honor and a privilege to have you with us,
and I know that a number of emails have gone out, and I don't think any introduction is
honestly needed, but can you hear me? Thank you for joining us, and I'll let you take it over.
Okay, thank you very much. I'd like to say a few words about the maker of language,
its acquisition, origins, its use, and so far as we understand these matters today,
and I'd like to approach this within the broader context of the inquiry into language and mind,
which are the distinctive properties of the human species. Well, good place to start is
at the beginning of our historical records, classical Greece, the Delphic Oracle,
which produced the maxim, know thyself. Well, two-word aphorism is open to many interpretations.
The intended interpretation was individual. You, the individual, should know yourself.
An unexamined life is not worth living, the Platonic expansion of this,
but it makes good sense to understand the maxim of the Oracle collectively,
know ourselves. The reason is that we're pretty much similar. It's a very young species.
Humans have only been around for two to three hundred thousand years. That's
a early and eye-blank and evolutionary time. It's been mentioned by evolutionary biologists,
partly as a joke, partly real, that if you see two squirrels and a tree outside your window,
they very likely are more diverse genetically than all of the humans in the world. Humans have
arisen within a very narrow window of evolutionary time, and they have unique properties.
One of these properties is human language. There's no analog, no similar properties anywhere in the
organic world. Furthermore, it's a species property. It's not only unique to humans,
but uniform among humans. That means that any infant can acquire any language with equal facility,
as far as we know. So it's a species property, virtually definitive of humans. There is another
one thought, at least thought in any form that we can grasp and understand. The kind of thought that
made it possible to produce the maxim of the Oracle, to interpret it, to reflect on its significance,
myriad other mental activities that occupy us all day, even in sleep,
just again a common human property, totally unique to humans. Well, when you have two
unique species properties, it makes good sense to ask what relation holds between them,
the simplest relation would be simply identity. Language constructs thought,
thought is what is constructed by language, and that is the conclusion that was reached
2,500 years ago, classical India, classical Greece, and it's a tradition that runs through millennia,
late 19th century, a great American linguist, William Whitney, simply defined language as
audible thought, basically thought which we happen to externalize through the mouth,
pick up through the ear. We now know that that's too narrow, you can use any other sensory
motor system, so if I don't know if this is being translated into sign, but if it is, you would be
seeing the same thing in sign language, which works very much the same as spoken language,
in the way it's acquired structure, the way it's used down to small details,
even the way it's norally stored. Well, you can think of the language as being
kind of like a program that's in your laptop, so suppose you have a program in your laptop,
calculator, or say multiplying numbers, the program works by itself, you can attach it to
a printer, could be a printer that uses colors, dark letters, one font, another font, the program
doesn't care, it'll can be hooked up to, sorry, try to be louder, it can be hooked up, is this
any better, can be hooked up to any printer that you want, the program stays the same,
and language seems very much like that, there's an internal system constructing thoughts,
it can be hooked up to one sensory motor system or another to externalize it, but the program
seemed to be the same. Well, let me make a side comment here on the history and then return to
the mainstream of the discussion. What I've just been describing is basically the long millennial,
long tradition of the study of language and mind that goes right into the 20th century.
The early 20th century had changed, the general view in the early 20th century still largely surviving
is that language is not primarily an instrument, the means of thought, but rather it's a system of
communication, that's sort of the mantra today in philosophy of language, much of linguistics,
cognitive science. This I think is a result of the rise of behaviorism in the 20th century,
which drove scientists, philosophers, others to look at what's visible, what's apparent,
behavior, to keep away from hidden things like what's going on in your mind. That was a very
sharp departure from the scientific tradition, it's very anti-scientific in my view,
became quite dominant, especially in the first half of the 20th century. There's also a kind of a
primitive version of Darwinism, which does have its roots in Darwin's writings. The idea that
all changes through evolution must be small changes, which operate through natural selection
and over time may give large-scale differences. It's now known that that's completely false,
but the residue of that does lead you to the belief, the illusion in my view, that language
somehow must have emerged from simpler systems, going back to animal communication,
the evidence is strongly against that, and there's nothing in evolutionary biology that
leads to such conclusions. Actually, this is, notice that this behaviorist turn is a turn away
from the effort to understand and explain, an effort to describe basically what you see in
Silicon Valley linguistics, the kind that makes the headlines about the things that
it's claimed that machines can do. Associated with this was conception of how language is acquired,
dominant conception, philosophy of language, linguistics, language is acquired through
training and instituting habits, if there's anything new that's said by analogy,
all completely wrong, completely reshuted, still very common. Well, let's go back to the
tradition, which I think was on the right course. The study of language took a large step forward
with the 17th century scientific revolution. The primary lesson of the scientific revolution
was that you should be puzzled by simple things that you take for granted. So a heavy ball of lid
plainly falls faster than a light one, except that it doesn't, as Galileo showed with
elegant thought experiments. Take a sailboat moving through the sea, suppose there's a
a lead ball on the top of the mast. If it falls, it should fall behind the mast,
because the sailboat is moving forward, except that it doesn't fall to the base of the mast.
Again, Galileo showed this with thought experiments. If he'd done an actual experiment,
the results would have been all over the place. But as in much of science, then and since,
the idea was to construct ideal situations, often by thought, sometimes by careful experiment,
if you can, that bring out the hidden reality between behind the phenomena that you see,
which are always complex, many variables can't be studied. That's why scientists do experiments.
Well, if you go back to scholastic science in the 16th century, what Galileo and his
contemporaries were struggling against, scholastic science had answers to everything.
If a ball falls to the ground and steam rises, it's because they're going to their natural place.
If two objects attract or repel each other, it's because they have sympathies and antipathies.
If you perceive a triangle, it's because the form of the triangle
passes through the air and implants itself in your brain. So there was an answer to everything.
All the answers were wrong. They were based on what modern science called occult qualities,
invented concepts that don't exist. And the willingness to be puzzled by things opened the
door to the new science, the science of the last several hundred years. The scholastic science
had answers to everything. The new science had no answers to anything. That's how serious inquiry
begins. And the same has been true of the study of language in the last 60 or 70 years,
breaking with the behaviorist, structuralist tradition. Well, let's take a look at language.
There are simple things that we take for granted. We talk to each other. We understand each other.
What could be simpler than that? Well, to the new scientists, Galileo and his contemporaries,
this seemed an extraordinary puzzle. Amazing. How can it be that with just a few symbols
we can produce, we can construct in our minds infinitely many thoughts, never before expressed,
never in the history of the language, and we can even convey to others who have no access
to our minds, we can convey to them the innermost working of our minds. They regarded this as
an amazing phenomenon. In fact, Galileo himself regarded the alphabet as the most spectacular
of human inventions because it facilitated this astonishing quality. And it is indeed a
remarkable fact. It's been repeated by others. One famous case is Gottlob Frege a couple of
centuries later, the founder of modern logic and logical philosophy mid 19th century. He wrote that
he found, quoting him, he found it astonishing what language accomplishes with the few syllables
that expresses a countless number of thoughts. And even for a thought grasped for the first time
by a human, it provides a clothing in which it can be recognized by another to whom it is entirely
new. It's essentially Galileo's insight. And it raises very deep questions. How can this possibly
be the case for this initiated a rich study of what centuries study of what was called universal
and rational grammar universal because it tried to cover all languages. Rational because it was
interested in explanation, not description. Very different from the structural linguistics and
philosophy of language of the first half of the 20th century, which described itself as a
taxonomic science. We have procedures of analysis, you can provide apply them to data
that you come up with an organization of the data. It's basically today's Silicon Valley.
That's doesn't care about understanding, doesn't care about explanation, just organization of data.
A radical change from traditional science, including the science of language. Again,
the old tradition was picked up, revived around mid 20th century.
Well, it does raise the problems for Descartes in the 17th century. It was the basis for his
establishment of the classical mind body problem. Descartes accepted the basic idea of the new science,
Galileo, world society in England, later Newton, others that the world is basically a machine,
a complex machine, the kind of machine that could be built in principle by a skilled artisan,
and that in their theology in fact was created by a super skilled artisan. Machine means something
built out of gears and levers and pulleys and so on. That's the world. But Descartes recognized
that there are certain things that didn't seem to fall within machine science, what was called at
the time mechanical philosophy. Philosophy meant science. So there were things that didn't fall
within it. One of the most striking ones Descartes felt correctly is the creative aspect of language
use, what amazed Galileo, his contemporaries centuries later, Fredio. How is this possible
for a machine? It is impossible for a machine. And that led Descartes, a sensible scientist,
to postulate a new principle, a thinking principle, mind. That's the mind body problem.
The next task for Descartes and other sciences was to see how they're unified. Well, I quickly
ran into a, sorry, somebody's at the door, my dogs are unhappy about it. This collapsed pretty quickly.
Isaac Newton's great discovery was that the world is not a machine. There are no machines.
There are no bodies in the sense, any sense of body that we have. The world is just not
intelligible to us. That was the criterion of intelligibility. John Locke immediately
recognized that as he put it within his theological framework, just as God has attributed to
whatever exists, properties that we cannot conceive, like interaction without contact.
So God may have super added to whatever there is, the property of thought.
Thought is just a property of organized matter, whatever matter turns out to be.
All of this was worked out extensively through the 18th century, then forgotten,
totally forgotten, revived in the mid 20th century. It's now become
standard idea after a long period of neglect. Well, that leaves us with serious mysteries.
How is all of this possible? And the answer to that question is we don't know. We do not know
how it is possible to answer Galileo, how it is possible for or frigate, with a small number
of symbols to construct new thoughts, new to us, maybe new in history, to somehow transmit them
to others in such a way that they can interpret the inner workings of our minds. A lot of this is
just unintelligible to us. Well, there are parts of it that we can understand. Here we have to
make a distinction, which was actually discussed by Aristotle, but then forgotten until the 20th
century. The distinction is between possession of knowledge and use of knowledge. It's a
crucial distinction in modern technical terminology for those of you familiar with that.
That's the distinction between what's called competence and performance, what we know and
what we use. The internal language provides us with a store of an infinite number of possible
thoughts. That's our possession of knowledge. Our use of knowledge is perception and production.
When you perceive the noises that I'm making, you are accessing your internal store of knowledge
to provide an interpretation for these noises that you're hearing. When I produce a sentence,
I'm carrying out two acts. The first act is selecting a thought from the store of possible
thoughts. The second act is implementing it using the various mechanisms of my mind, my
articulatory system, to externalize that internal thought. The second part of it, we can study
and understand. The first part of it, selecting the thought, is completely beyond our understanding.
We have no idea how this takes place. It's part of the general mystery of voluntary action, which is
basically to us an impenetrable mystery. Well, just keeping to what we can hope to understand
the system of language that constructs thoughts, the formation of the internal language,
keeping to that, we can discover some quite remarkable properties.
Following the tradition of early modern science, let's take simple cases. Take the sentence,
the bombing of the cities is a crime. Notice it's is a crime, not are a crime.
Actually, that's puzzling when you think about it. Why doesn't the verb is agree with the closest
noun phrase? The closest noun phrase is cities. So why don't we say the bombing of the cities
are a crime? Okay, let's take the bombings of the city is a crime, or a crime, the bombings of the
city are a crime. Why don't we say the bombings of the city is a crime? Again, just using the
simple property of adjacency. When you look at it, you see that what humans are doing
is ignoring the simplest possible computational procedure, adjacency, and instead using a complex
procedure, finding, not looking just at the linear order of words, but finding a structure,
and then finding the central element within that structure. It turns out to be pretty complicated
computation, but that's what we do reflexively. And in fact, that's what every infant does.
You can show by experiment that this is what infants do down to before the age of two,
30 months of age, can't obviously can. And this is true of all constructions in all languages.
Just to give another example, a little more complicated, take the sentence,
the man who fixed the car carefully packed his tools. The man who fixed the car carefully
packed his tools. Notice it's ambiguous. He could fix the car carefully, or he could carefully
pack his tools. Now put the adverb carefully in the front of the sentence. Carefully, the man who
fixed the car packed his tools. Unambiguous. It means carefully packed his tools. And it's
unambiguous in a puzzling way. Carefully it looks for a verb, but it doesn't look for the closest
verb. It looks for the more remote one, which if you build up the structure is a structurally
closest one. So what you're doing is what everyone is doing reflexively is ignoring
the linear order of words and attending only to structures that you never hear and that your
mind creates. To give one last example, take the sentence, the man who saw Bill is young.
Suppose I didn't hear the word Bill clearly. I say, I want to ask who it is. So I say, who is the
man who saw the young gibberish? Can't say that. Fine thought. You can't say it. You have to,
you can only say you can't use the simplest computational procedure and find the closest
occurrence of who and put it in the front. Splock. Well, this is universal. As I say,
children of 30 months old know it, all constructions, all languages. What it means is the infant,
all infants are ignoring 100% of what they hear, which is words in linear order. They're ignoring
very simple computations on linear order and they are reflexively paying attention to something
they never hear that their minds create and using computations on those abstract structures.
It's a pretty remarkable fact when you think about it and that is the kind of thing we discover
as soon as you try to answer the Galilean challenge. This property is called structure
dependence in the technical literature. Pretty much the same as true of learning of words.
There is a myth, standard myth, that the way children learn words is they see a cat. Their
mother says cat. They make an association between the thing they see and the sound your mother produced
and this is repeated over and over again in many circumstances, different situations. Finally,
the kid figures out that the sound cat goes along with that feline object. Totally false.
Nothing like that happens. There's a very careful experimentation on this by now.
Primary research and this is wonderful scientist, Lila Gleitman, who old friend passed away a couple
of weeks ago. What Lila Gleitman showed in her experiments is that children pick up words and
the meaning of words on one or two presentations. At the peak period of language learning,
around two years old, children are picking up words that are about one every waking hour,
which means that they hear them once, maybe twice. They usually don't pay attention to their mothers,
just whatever is going on in the circumstances. They know the meanings of the words. The meaning
of the word, when you look at it, is very complex. There's no time to go into this, but when you
really look at the meanings of words, you find out that they are quite rich. Well, actually,
let me mention a few examples from classical Greece where this matter was discussed.
So take Aristotle again. He took us as example the word house. He said, what is a house?
What's the meaning of the word house? And he said, well, it has two components. One is what he
called matter. It's the bricks, the timbers, the physical things that the house is made up of.
The other is what he called form, an idea in the mind. The characteristic way the object is used,
the thought in the mind of the architect who constructed the way we conceive of it.
So something could look exactly like a house physically, but it could be a library if that's
the way it's used. It could be a garage. It could be a stable. It could actually be a paper weight
for a giant. It could be whatever we conceive of it, whatever the architect had in mind.
That is a consequence, and that's quite correct. It means that a physicist looking at the object
couldn't know the meaning of the word. That means that the child, when it's learning the
meaning of the word, is using all of this intricate understanding in the mind. None of it presented
as evidence. Furthermore, that's true of every word in the language. It's taken an earlier example,
pre-sacratic. Heraclitus asks a rather profound question. How can you cross the same river twice?
Second time you cross it, it's a physically different object. In fact, it could be radically
different. It could be flowing in the opposite direction. It could be not water, but arsenic
from an upstream plant. It could have been divided into several tributaries. It could be like the
river I pass on the way to work here in Tucson, where I live, the Roledo River, which never has
any water in it. It's a dry bed, but it's the Roledo River because every once in a while you
get a trickle of water through it. Historically, it was a river. It's a river. It's all the way we
can see. You could take the Roledo River, start using it for commuting to Tucson. It would be a
highway, not the river. Physically, it hasn't changed at all. That's again true of all the words
in our vocabulary. One special case of it is called polysomy. I suppose I say
the book is harder to understand than it is to burn. What's the book? It's simultaneously
abstract because you can understand it and concrete because you can burn it. The world
doesn't contain objects that are both concrete and abstract, but our mind constructs them.
And that's what the words of language are. And these are learned virtually without experience.
Another, this one is a real mystery. We don't know how that's done, but it's a fact that has to be
explained. Well, these are among the amazing things that you discover as soon as you begin
to look at the facts closely. One conclusion we have to draw is that language, what we call
language, has two completely separate components. One of them is an internal language that constructs
thoughts, has no order, no linear order, just abstract structures. The second part is a system
that externalizes it into one or another sensory motor, medium, usually sound, could be sign,
could even be touch. Notice that that second component is an amalgam of language and sensory
motor systems. It's not pure language. Sensory motor systems have nothing to do with language.
They were in place long before language emerged. They have not changed since language was
emerged in the evolutionary record. So there's something extraneous to language,
kind of like the printer that your laptop could be attached to. Well, and that, of course, has
linear order and does use adjacency and other things. Well, when you try to find explanations
for language, you, real explanations, you face a kind of conundrum. A trial learns language with
virtually no evidence. Actual statistical studies of the data available to children
show that it's very sparse, very little. And as I mentioned, they know things for which they have
no evidence at all, like structure dependence. So that makes it seem as though what's innate,
internal to the mind, must be very rich in order to carry the child from extremely limited data
to very rich possession of knowledge. So on the one hand, the learning, the internal structure must
be very rich. But then there's another problem. This system had to evolve. Well, it evolved in a
very brief period of time. We now know from genomic analysis that human beings began to
separate at least 150,000 years ago. And they all have the same faculty of language,
same possession of knowledge, of language, same possession of knowledge. So it was in place
before 150,000 years ago. Well, that's not long after humans emerge. As I said, humans emerge
maybe 200,000 to 300,000 years ago. That's a very narrow window in evolutionary time.
Click of an eye. Before humans emerged, there's no evidence in the archaeological record
for any kind of symbolic activity. After humans emerged pretty soon in evolutionary time,
you had very rich symbolic activity. Ordinarily, I assume plausibly that this is associated with the
emergence of language. Well, what that seems to mean is that the basic
faculty of language must be quite simple since it emerged so quickly in evolutionary time.
But now we have a conundrum. Looking at acquisition looks like the internal innate system must be
very rich. Looking at evolution seems like it must be very simple. I should say the first problem,
acquisition is sometimes called Plato's problem. Plato did raise the question,
how can we know so much with so little evidence? Didn't really have an answer.
The second question is sometimes called Dorwin's problem. How could we evolve such a system
so quickly? So it must be both very complex and very simple. There's a third problem,
whatever the faculty of language is, it has to be universal. It has to cover all languages,
because an infant has equal access to all of them. Well, it's this collection of problems that has
guided the inquiry into language for the last 60 or 70 years since the abandonment of the
behaviorist structural restrictions and the return to what had been the tradition.
And it seems that finally in the last few years, we are getting to the point where we may have
answers to these questions. So that, if it were to the extent it works out, is actually a new era
in the long millennial long study of language. The first time when we can reach genuine explanations,
satisfying, answering Plato's problem, Dorwin's problem, and the universality of the theory.
So let's look first at the universality. It seems recent research is increasingly
attending towards the conclusion that the diversity of language is in the externalization
in superficial things like choice of lexical, the way you, the sound you associate with a lexical
item. So it could be, you know, Cat in English, Cha in French, something else in some other language.
But that's superficial. That, of course, you can learn easily. But the concept, the meaning of the
word that's already inside. So one aspect of variety is just association of sound and symbol.
That's easy. There are others. There are many, the variety of languages looks on the surface,
very large. So there are some languages. English happens to be what's called a highly analytic
language of lots of simple words from one after the other to make a sentence. There are other
languages at the opposite extreme. A sentence could be one word. And other words could be scattered
around freely. Well, it's by now been learned over recent years that these apparently radically
different languages are almost identical at the deeper level. They have the same internals,
very much the same, maybe exactly the same internal structure, the internal language,
the one that generates thoughts, looks as if it's pretty much the same, maybe exactly the same,
no matter how different the superficial appearance. Actually, that's not too surprising.
The internal language can't be learned. We have absolutely almost no evidence for it. It's true
of the meaning of words, true of things like structure, dependence, other major properties
of language. So it makes sense for it to be identical among people or close to that.
On the other hand, the externalization is not strictly language. Remember, it's an amalgam
of language and sensory motor systems, which are totally divorced from language. Well,
connecting these two things is a fairly complex process. Two things that have no relation to one
another can be done in many different ways. It's basically solving a hard cognitive problem.
Different communities have solved the problem in different ways. So it looks complex,
it's a complex problem. The internal language looks quite simple because it's based on very
simple computational procedures. So on the one hand, we solve the problem of
Darwin's problem. We have a very simple system, solve the problem of
Plato's problem because the learning is all in the superficial part. The internal part just
isn't learned. It's just there, like structure, dependence, the ways of interpreting those sentences
that I gave. And the variety is where you expect it to be in the complex problem of
relating the internal language that produces thought to the external system that
translates, that hands it over to the printer that allows other people to read and see and
interpret it. If that, to the extent that that can work out, you have genuine explanations.
Well, that carries us on to the next chapter. How has it worked out?
Can only give the barest hint of that here. So let's take the major principle of language,
structure, dependence. Pretty remarkable principle. Wasn't even noticed until
fairly recently in all the history of language. Again, the fact that all operations of the
internal language, not externalization, all operations of the internal language that generates
thought ignore linear order. There's no linear order there at all. Just abstract structures
created by the mind and somatic interpretations, like the ones for river, house, book, every word
you can think of, which are constructed by the mind. So that's, let's take structure,
dependence. How can we account for this? Well, we have to show that when language evolved,
it mother nature hit upon the simplest possible combinatorial operation. The basic property
of language, all languages, is that each language is what you know, what's the knowledge possessed
is an infinite array of sentences, which are structured expressions, hierarchy of structures,
and can somehow be assigned an interpretation as a thought, and by the externalization system
mapped on to the printer sensory motor organs. That's the basic property.
It's property called recursive generation. You have to generate an infinite array of expressions.
Well, the understanding of how to do that developed really in the early part of the 20th century,
as the theory of computation was developed by leading mathematicians, Kurt Gertl, Alan Turing,
Alonzo Church, and will post, establish the modern theory of computation, which shows
clearly how a finite object, like your laptop or your brain, can store within it programs that can
yield an infinite array of objects, expressions in our case, and do it in a way which could be
hooked up to a printer. That allows us to ask what are the operations that mother nature would have
hit upon as soon as some accident took place in the evolutionary record a couple hundred
thousand years ago, and the property of recursive generation somehow appeared.
What's the simplest way to deal with it? Well, the simplest computation happens to be
what's called binary set formation, forming a set out of two elements, not changing either of them.
You repeat that over and over again, you get hierarchic structures indefinitely.
So plausible evolutionary scenario is some accident took place, maybe some small mutation
out of it came a process of recursive generation. Mother nature then picked the simplest one,
binary set formation, what's called merge in the recent literature, and then everything flowed
from that. Structure dependence flows from it immediately. Many other properties do as well,
where you can show that that's the case, you have a genuine explanation. Notice incidentally
that this is the way evolution works generally. Evolution goes through all evolution,
goes through three. I see a half of the question, but I can't read it. Okay, could you
hold it for a second, we'll come later. The way evolution works is three steps.
First, some accident takes place, could be a mutation, cosmic ray passed by and
changed the DNA slightly, an accident. It could be what's called symbiosis. So if you go back
a couple hundred million years, turned out that a bacterium, there were only bacteria and
similar microorganisms at the time. One bacterium by accident swallowed another microorganism
that led to the development of what are called eukaryotic cells, complex cells,
the basis for complex life, our life for example. The third step is winnowing of the various things
constructed in the second step, which ones have better reproductive capacity, that's natural
selection. Those are the ones that survive. So it seems to be the way language developed.
First came some accident, maybe a mutation, which yielded recursive generation. Then other
nature comes along, tries to find the most elegant solution to the problem of how to deal with these
new systems. In one case it was eukaryotic cells, in our case it was generation by merge.
The winning stage apparently never came for language. There's no winnowing stage,
we're all we stayed the same. Maybe because time just hasn't been long enough for natural
selection to operate. Maybe it's because at the second elegant stage the system was so closely
integrated that you just can't change it or it falls apart. That would be a very interesting
discovery and research is tending in the direction of showing that it's correct.
Well time is running out, let me stop here. We're now reaching what would be the next chapter.
How does all this work out in detail? But I think maybe if this succeeded you have a
general picture of how we can hope to meet the Galilean challenge, the basic challenge in the
history of the study of language in mind and do it in a way which can give genuine explanations
for some quite remarkable properties of language and thought while leaving identifying other
properties as a total mystery which we simply cannot comprehend. That seems to me pretty much
where we stand today. I'll stop there. Thank you Noam. Can you hear me okay?
Great. So we'll have actually asked people to raise their hands for questions. I think it'll be
easier to moderate them but we did have a couple come in through chat and maybe
we can start there. Leslie Bailey if you want to unmute yourself and ask your question go ahead.
Oh great. Okay I'm afraid this is a really really stupid question Noam. I'm sorry. If possible
please restate the difference between the behavioral structural approach to language and what replaced
it and how that transition occurred. I mean I know it's very complicated but if you could just
say it in two sentences. It's quite interesting. Let's take a look at how the structural linguists
identified their own theory, their words. Structural linguistics was identified by mid-20th century.
A consensus had been reached among the at that time fairly small number of structural linguists.
It was a very small field then, a huge field now. The small they reached the consensus that
structural linguistics was what they called a taxonomic science. There were procedures of analysis.
I was actually a student in the late 40s studying this. You studied the procedures of analysis.
Here's what you do when you have an informant from a say Native American community. Here are the
procedures you use. Of all you have is a text. Here's the procedures of analysis you use. Those
procedures identify units. You organize the units. You're done. It's a taxonomic science.
It's fields over. Okay so it's descriptive. Let me continue. There was also a theory of learning.
It was stated explicitly by Leonard Bloomfield, the leading American linguist of the early 20th
century. Language in his words is a matter of training and habit. If there's anything new
produced, it's by analogy. Same view in philosophy of language. Take the major figures. W. V. Quine,
Ludwig Wittgenstein, essentially the same. Language is a matter of training, habit,
your quine, sconeerian, operant conditioning. Anything new is an analogy. The new system that
developed starting around the 20th century was essentially a return to the old tradition.
We want to find explanations for linguistic phenomena. We want to find out why they work
this way, not the other way. Why do we say the bombing of the cities is a crime, not
or a crime? Essentially the 17th century questions. As soon as you started on that, you had the same
transition that happened in the 17th century from knowing everything to knowing nothing.
As soon as you started asking these questions, you had no answer. Well, I should say that the
entire tradition was totally unknown, completely forgotten, wasn't rediscovered until the 1960s
after much of the ground had already been laid. The structuralist behaviorist era
essentially wiped out the history. But the new system that I've been discussing is
essentially picking up from what the tradition was. I don't know if that's clear enough to
answer the question. No, that's great. Thank you so much. All right. So our next question is from
David Dalton. David, I just unmuted you, so you should be able to ask your question.
Listen to our discussion here, and I ask is not our consideration a little bit anglo-centric
in that English, yes, it has an alphabet of 26 characters, but if you look at Mandarin,
it has an alphabet of 40,000 characters, only 2,000 of which are extant today. Not to mention that
Mandarin is a tonal language, whereas English is more of a literal language. And I don't know
Mandarin myself, but I just wonder how is it that we can, with such broad brush strokes, compare
all of language in this discussion, I would seem that there would be quite a bit of difference
in the semantics of Mandarin versus English or say German or French.
Well, I think the comment about the alphabet that I made was Galileo. He was, of course,
thinking of Latin and Greek, but the alphabet is a very superficial matter. It's like the printer.
You can use all sorts of alphabets. You can use the Mandarin style, the Korean style,
hieroglyphs, just as you can use any printer. Turns out that the internal system for Mandarin,
for English, for Yoku, for Potawatomi, for Tagalog, the internal systems are very much the same.
Maybe identical. Lots of languages are tonal languages, but they function essentially. That's
just a different printer. And I mentioned just one example when I was talking. Languages like
Yoku is an example where a single word includes the whole sentence. It was thought for a long time
that those languages must be very different. The more we discover, the more we find that at the
internal level, they're the same. The internal means for constructing expressions and the semantics
is either identical or very close to it. It doesn't look like that on the surface. You're quite right.
On the surface, they look wildly different. And it was assumed, if you go back 50 years,
it was assumed that languages can differ in just about every possible way. And when you study a
particular language, you can't use any presuppositions from another language. That was sometimes called
the Boazian principle. It's what I learned when I was a student. Seems to be completely wrong.
Seems that at the core, all languages are either identical or very close to it. The systems that
construct internal expressions in the mind, expressions of thought don't seem to differ.
The externalization looks very different. They seem arranged differently, different sounds,
so on. But all of that seems superficial. To go back to the analogy of the laptop and the printer,
what's the internal program in the laptop seems pretty close to common for all languages that
we know, including very diverse ones. The printer can be very different. And the alphabet isn't even
the printer. It's not the sounds. It's the representation of the sounds. So it's even more
remote from language.
Our next question is from Fran Manley. Fran, if you want to unmute your, or I've unmuted you actually,
Fran, you're good to go. Yeah, I got a two-part question, Professor. One, is there any evidence
as to whether or not other species, for example, like dogs, have a common language which allows
them to communicate even with dogs they don't know? And my second question, if I may, is on children,
I've been led to believe that children don't use a word unless they've heard it. But I got the
impression from what you've said, that's not necessarily so. They don't have to hear a word
to learn it. Okay, thank you. Well, on the second one, children have to hear a word in order to,
not depends what you mean by learn the word. They know the meaning of the word before they hear
what the sounds are. So if you're a Chinese child, a Potawatomi child, a French child,
whatever it may be, you have the meaning in your head. You don't know what sound is going to be
associated with it. That you have to hear. And Lauderdale Gleitman and her work and others have
shown that takes almost no experience. Here at a couple of times and you say, okay, that's the
internal word that I know with all of its complex meaning. You do have to hear something to know
whether it's pronounced cat or some other way. Okay, how about the animals? Do they have more
language, form of language, other species? Do they have more? Do they have, do animals or do
other species have their own communication? Yeah, so animals, let's say dogs. I happen to have
two of them. We can hear them on the desk at the moment. You heard a couple of them before.
Well, I've looked at them pretty carefully. They have a number of ideas in their heads,
maybe a dozen or so. Sometimes they can do moderately complicated things. Look,
I'll give you my favorite example. There's a big dog and a small dog. If the big dog
wants to be taken, I hate to say the words because unfortunately, they should hear them,
but wants to be extricated to the outdoors, power phrase, which she doesn't understand.
When she wants that, she has a trick to do it. She takes something away from the small dog.
And the small dog comes over to us to complain. And we make the big dog, give it, say, a toy,
give it back to the small dog, and then she runs right to the door and expects us to extricate her.
So it's more of a behavior than a language, right? It's a small number of things.
It's an entirely different from human language in every respect. Every animal that's known,
including the closest to us, apes, monkeys,
symbols, like take a vervet monkey, which has been studied carefully, has maybe a dozen coals,
like if the leaves are fluttering in a tree, the vervet monkey will come out with a sound
which makes all the other monkeys run away and hide. We interpret that as a warning sound.
Maybe there's an eagle in the tree or something. I don't know what the monkey's thinking, but
every or can make another symbol, which we interpret as meaning I'm hungry.
The point is that each symbol is keyed explicitly to a physically identifiable entity,
motion of leaves in a tree, hormonal changes, something like that.
Okay, thank you. None of the human symbols are like that. That's Aristotle's point about house,
so they're radically different. Furthermore, there's no combinatorial structure in the
animal systems. It's just a symbol. Now, there have been major efforts to try to train chimpanzees
like infants. The main study you may have seen it is called the NIMM study, named after me actually,
by a number of very fine cognitive scientists, some of them former students now, one of them a
colleague and friend. They made an extensive effort to raise the chimpanzee NIMM from infancy
pretty much the way you raise an infant, extensive efforts to get NIMM to pick up something like
language. They used sign because the articulatory system for the chimpanzee doesn't work very well,
so train it like an infant would learn sign. At first, they thought they were getting somewhere,
but when they looked closely, it turned out it was nothing. It's just beyond the capacity
of a highly intelligent ape to do anything like what a one- and two-year-old infant does. They're
just built differently. There are things that other organisms do that we can't do, like I live
in the desert. In the back of my house, there are desert ants who have cognitive capacities
that I can't come close to. They can navigate in ways which I can't possibly do, maybe humans,
can duplicate it with complicated instruments, and they have a brain about the size of a
minuscule brain. Organisms are just different. Our difference is we have the capacity for language,
doesn't have any counter for it anywhere.
Our next question is from Les. I've unmuted you, Les, so go ahead and ask.
Yeah, this question is maybe more philosophical, but the idea about thought and expression,
can you have thoughts before expression internally, or do they rise simultaneously and cannot exist
independently? Well, these are things where there's no, we can't introspect, because it's all
beyond the level of introspection, and there's no external scientific evidence. But the simplest
theory, the simplest assumption, the one that would have to be disproved, is that they're just
the same thing. The expression is the thought. There's an internal system which has provided,
we all have, the knowledge we possess includes an infinite array of these thoughts formulated in
the linguistic system. When I produce a sentence, I pick out one of them somehow,
how unknown, then I implement it. So there may be no distinction between thought and expression,
no first or second. Just, that's what the thoughts are. There's a repository of thoughts
in linguistic form, and we can then implement them. And on the receptive side, you can reconstruct them.
Maybe not the same way, maybe even in a different language, but something very much like them.
Our next question is Joe Keller. Go ahead, Joe. I've unmuted you.
Thank you, Scott. Professor, as the newly conceived human begins to develop her that
inborn language structure, will her external experiences add to or enhance that structure in
any way? Well, here we ought to pay attention to the way built-in structures work generally,
to take your visual system. The visual system does quite complicated things. Very, this time,
talk about them, but very rich things. If your visual system is not stimulated in the early
weeks of life, it'll degenerate. We know this from studies with other animals, cats, monkeys,
and so on, in basis that we live about the same visual system. It's shown that unless a kitten
is presented with pattern stimulation in the first couple of weeks of life, it'll be blind.
That's the way, and all instinctive behavior works like that. It has to be triggered by something,
and then it is partially shaped, partially shaped by the way, by the stimulation.
So it takes kittens again. If kittens are presented in the early weeks of life only with
horizontal lines, then in, as a mature cat, it'll be able to perceive horizontal lines,
not vertical lines. That's a little bit like different learning, different languages.
You can shape the internal system marginally. You can't train a kitten to recognize
curves, for example, in infancy. It has to be lines and different orientations.
Lots of things can't get it to do, and that's the way instinctive behavior works quite generally.
And it seems to be the same with language as far as we know. The system is built in,
the internal system, has to be triggered in order to get started to develop, then pretty much
develops on its own course. There are some respects in which experience outside data
shapes the system. One is the trivial one of how do you pronounce this? Others are,
are you going to be like English, an analytic language with a lot of separate words that form
a sentence? Or are you going to be like, let's say, a language where you have one huge word that
includes everything? Well, that requires some stimulation. But a large part of recent research,
very recent research, is to try to sort out which parts are fixed and fundamental,
which parts are provided by the shaping aspect of experience. And it's turned out pretty surprising.
So 40 or 50 years ago, the greatest anthropological linguists, friends of all of us here,
Ken Hale, former teacher's friend, one of the leading anthropological and
formal linguists of the past generation, he firmly believed that languages differed in
what's called a parameter, an option of choices. Some had flat structure with no hierarchy,
some like English had hierarchy. And it looked, that's the way the evidence looked. Over time,
actually Ken, Ken Hale and his students, Julie Legge, were able to show that even the most extreme
cases of languages that looked like completely free word order or flat no structure,
even they had the same internal structure as the hierarchy languages. These are real discoveries,
major discoveries of the past couple of decades. And we don't know how far it'll go. But I think
it's a reasonable guess by now that it'll probably go to showing that the internal system that generates
thought is either uniform or very close to it. And that the apparent variety and complexity
and mutability, the change from generation to generation, all of that's probably in the
externalization and the trivial lexical properties. Like do you pronounce it cat or some other way?
Our next question is Miriam Burt. Miriam, you are unmuted. Go ahead.
Thank you. I think my question, Professor, probably follows on pretty well to the recent one,
the one you've just responded to. You said earlier that around two years to 30 months is this
riot, this wonderful time when, you know, they're learning a new word or a new every
waking hours. But I wonder if a child, if she's deprived of both that visual stimulation
and the language, you know, the auditory to hear the words from because she's in an orphanage or,
you know, in a very poverty state, if you will, you know, for external stimuli. Will she be able
to catch up if she is subsequently, you know, exposed to this, this stimuli, you know, internally
and externally, I mean, visually and auditorially, or is she always going to be a little behind and
not quite up to what she could be? Thank you. That's a very, it's a very difficult question
because we cannot do experiments. You don't do experiments with humans. Rightly or wrongly,
we do do them with cats and monkeys. That's how we know about the visual system, which is pretty
much shared with us. Can't do it with any other organism because no other organism with anything
remotely like language. So we just have to look at the experiences that we see. And they're,
they're very interesting. You learn a lot from them. So there's a very fine neuroscientist
named Helen Neville, Oregon State University, who done fine work on the brain and language.
About 30 years ago, she got interested in pretty much this question. She took
children who are what are called low SES, socioeconomic status, and who seem to be problems
in school. They weren't learning, they couldn't speak, all kind of behavioral and other problems.
And she began to study them carefully. She noticed some interesting things. Their parents
don't talk to them. I mean, they say, get out or go outside or leave me alone. And not because
they're bad people, they're just people who are trying to survive on two jobs in miserable
circumstances. Parents never read them stories. You know, they've never had that experience.
So she tried a simple experiment, trying to bring the parents and the children together
and encourage them just in some free time to interact, like have a mother read a story to
the children, things like that. Turned out they enjoyed it. They did more of it.
They started having much higher cognitive achievements, weren't problems in schools and so
on. Well, that's a partial answer to your question. I'll mention another real experiment,
real event. It's a very fine, another cognitive neuroscientist to comment,
Massimo knows very well. He lives in Massachusetts. He began working with children
who were such extreme behavior problems that they were basically excluded from school.
They couldn't stay in a classroom, had to be excluded. He started working with them,
looking at them carefully. Turned out, he immediately found out that these were kids
who came from homes where they didn't eat breakfast. They didn't eat breakfast.
They got on a school bus. The bus roamed around for an hour. They finally got to the school.
They were out of their heads. They were sent to maybe an arithmetic class.
Here they couldn't pay any attention. They ran around the room and so on.
Well, he started doing something. For one thing, when they came to school, the first thing he did
was give them candy because their glucose levels were low, hadn't eaten. Then instead of putting
them right in an arithmetic class, he let them stay outside and run around for half an hour.
Then they brought them into school, other things like that. By now, these kids are performing better,
considerably better than the average in the public school system.
Well, I think that's the kind of evidence that we have about your question. It's a humanly,
very significant question. We can't do direct experiments on it. We're not going to deprive
children and see what happens. You take the natural experiments and see what you can find out.
I think the basic answer is that if the deprivation is not too severe,
the child can probably recover normal cognitive capacity. If the deprivation is extremely
severe and there are such cases, the child is simply deprived of external contact,
maybe a sociopathic father, something like that, then they may never recover.
But it's hard to know what's caused. There's a famous case, Jeannie, which you may have read
about, but it's hard to know what the lack of recovery is from because any kid like that is
going to be psychotic. If you're brought up, tied to a chair till you're 12 years old with somebody
throwing food at you, you're going to be psychotic if you can even survive.
So how much is that interfering with what's happening? Well,
we don't know and we don't want to do the experiments which will answer it.
I've got about five minutes left. Our next question is from Linda Green. Linda, you are
unmuted. Go ahead. My question has to do with brain research. What contemporary brain research is
shedding light on linguistics? Well, it's difficult because brain research
altogether is very difficult and brain research on humans is particularly difficult
because we do not allow invasive experiments. With a cat, we allow ourselves to stick an electrode
into a neuron and see what it's doing. You don't do that with humans. So it's all kind of evidence
you can pick up by external imaging and so on. And you kind of learn a great deal. In fact,
the most important discovery about this actually has to do with what I call structure dependence,
the fact that the computational operations of language make no use of 100% of what we hear.
Words in linear order, they only deal with abstract structures in the mind.
They're very ingenious experiments initiated by a friend and colleague of ours,
Andrea Moro, Italian linguist, neuroscientist. He and his colleagues devised experiments on the
following paradigm. They took subjects, two groups of subjects who happened to be speakers of German
and presented them with invented systems. One of them modeled on
Japanese language that they'd never heard. Another, using simple rules which violated
structure dependency. So a rule that says, if you want to negate a sentence,
take the negative particle, say not, and make it the third word of the sentence. Very trivial
computation. They asked how subjects responded to this. Well, it turned out that when they were
given a language modeled on a natural language, the areas of the brain that are mostly involved in
language processing and use all fired up the way you'd expect them to. When they were given
the system, the artificial system with simple computations on linear order,
they just got diffuse activity in the brain, meaning the brain is treating it as a puzzle,
not a language. That's quite an interesting result. It's been replicated. There's different
variants of it and so on. But that's the kind of thing that can be done. There is
other experimental work in neurolinguistics which shows a number of things.
All right. We probably have one quick question. Ralph Slosti, if you'd like to, I've got you
on muted, Ralph. If you'd like to ask your question. Yes. I just read recently about the
multi-language brain and it looks like people with multi-language capabilities
that actually perform worse than a mono-language brain. How could that be when you said the
languages are the universe? So the thoughts of being universal and they should express.
Okay. Well, first, I don't know what you saw, but I don't believe it. Every child is easily
capable of learning many languages in infancy. It's very common in many cultures for a child to know
three or four languages. They don't even know they're different languages. This is the way you
talk to your mother. This is the way you talk to your grandfather. This is the way you talk to
the kids in the street. Finally, at some age, maybe four or five, they may realize, hey,
I'm speaking different languages, but it's just normal. As far as we know, every child can do that.
There's no known difference between, there's no even any way of studying it because all
children have this multi-language capacity. But I'd be very skeptical about the source you looked
at, frankly. Thank you so much. Well, I think that's all the time we have today that Professor
Chomsky, we really appreciate having you present to us. And I'm grateful to know as I, when I was a
child, and I probably begged for a candy for breakfast that it wasn't a bad thing.
But no, we really appreciate you being here. And again, thank the Department of Linguistics
at the University of Arizona for hosting this session and course and appreciate it so much.
So do you have any other parting words for us, sir? I hope some of you will be interested enough
to look into the meat of the subject, how it's actually done next chapter.
We did have a few people inquiring about the publications and things that would be of good
service to kind of get a general understanding of this area of study. What are your recommendations?
Well, I have a book called What Kind of Creatures Are We, which goes into many of these questions.
But there's a lot more work. They're very, I should say they're excellent general introductions
to language. There's a book by Charles Yang called The Infinite Gift,
which focuses a lot on acquisition, but brings up these things. There's a recent book by David
Adger, A-D-G-E-R. I think it's called something like The Infinite Mind or something like that,
which is an excellent introductory discussion at a high level to many of us.
There's a very fine general introduction to many aspects of language by an outstanding
linguist, Ian Roberts. It's called The Wonders of Language, which goes into these and many other
things. There's a lot of very high-level introductory literature for syntax, the kind
of thing I'm talking about. Our colleague here, Andrew Carney, has a book simply called Syntax,
which goes into many of these things in a very lucid way, but in considerable detail.
Well, thank you again so much. We really appreciate it. I can't unmute everybody apparently.
Thank you. Thank you. Thank you so much. Absolutely outstanding. Thank you so much.
Thank you, Professor. Thank you. Thank you. Thank you, fabulous. Thank you.
Thank you.

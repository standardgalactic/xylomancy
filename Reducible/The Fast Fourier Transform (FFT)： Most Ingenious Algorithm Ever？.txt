The world of algorithms is vast, but we can often split them into two distinct classes.
The first class is those that are inherently useful. Think of your standard graph algorithms like
DFS or BFS. These algorithms show up all over the place, they are efficient, and as a result we
like to understand them. The second class of algorithms we study are ones that are just purely
beautiful. Think of the first time you saw the incredibly simple recursive implementation of
towers of annoy. If you have a soul, you feel a sense of wonder, a sense of awe at the elegance
of such an algorithm. It happens to not actually be that useful or efficient as a matter of fact,
but we still study it just as we like to read a work of fiction. It inspires us and motivates
out of the box thinking. Today I want to talk about an algorithm that rightfully belongs in both classes
and my personal favorite algorithm, the fast Fourier transform. The fast Fourier transform or
FFT is without exaggeration one of the most important algorithms created in the last century.
So much of the modern technology that we have today such as wireless communication, GPS and in
fact anything related to the vast field of signal processing relies on the insights of the FFT.
But it's also one of the most beautiful algorithms you ever see. The depth and sheer number of
brilliant ideas that went into it is just astounding. It's easy to miss the beauty aspect of the FFT
since it's often introduced in fairly complex settings that require a lot of prerequisite
knowledge such as the discrete Fourier transform, time domain to frequency domain conversions,
and much more. And to be fair, to get a full appreciation of the applications of the FFT,
you can't really avoid any of these ideas. But I want to do something a little different.
We'll take a discovery based approach to learning about the FFT in a context that you
are all familiar with, polynomial multiplication. The way I see the structure of this video,
it's all about starting with some common ground and then slowly asking questions that
will hopefully prompt you to discover the truly ingenious ideas behind the fast Fourier transform.
All right, let's get started.
The setup here is simple. We're given two polynomials and want to find the product.
Our task will be to design an efficient algorithm for this problem. Mathematically,
we know one algorithm to multiply polynomials by repeatedly applying the distributed property.
All of you have perhaps instinctively been applying this algorithm since any algebra class.
Before we try this idea though, the first question we have to address is representation
of polynomials in a computer. The most natural way to represent them is through coefficients,
where we map coefficients to lists. It helps to arrange our coefficients in the following order,
mainly because now the coefficient of the kth index is mapped to the coefficient of the kth
degree term. We will refer to this representation as the coefficient representation of polynomials.
In general, given 2D degree polynomials, the product will have a degree of 2 times D.
And the running time of this algorithm, if we actually want to implement it with the most
natural distributed property approach, will be O of D squared, since each term in polynomial A
will have to be multiplied by all terms in polynomial B. The question now is, can we do better?
The first really clever idea comes from thinking about polynomials a little bit differently.
To see the key inside here, let's take a look at one of the simplest polynomials,
a degree 1 polynomial or a linear function. We can represent any line with exactly two
coefficients, one for the degree 0 term and one for the degree 1 term. The key part that makes
this representation valid is that every representation has a one-to-one mapping to a unique line.
What other representations of a line have this property? There are actually several
reasonable representations, but the one that we are going to use is the two-point representation.
We know from geometry that any line can be represented by two distinct points.
And turns out that there is a neat extension of this for general polynomials, which I will state here.
Any polynomial of degree D can be uniquely represented by D plus 1 points. For example,
if I gave you three random points, this means that there is exactly one quadratic function
that goes through all three of these points. If I give you four points, there is exactly one
cubic function that goes through all these points. This statement is perhaps a little
surprising, so it deserves a proof. Suppose I give you D plus 1 unique points of a D degree
polynomial p of x. We want to show that for these set of points, there is only one set of
coefficients. So if we actually evaluate our polynomial at each of these points,
we get the following set of system of equations. Whenever you have a system of equations,
writing it as a matrix vector product is almost always helpful for analysis.
One nice property of this matrix is that if each of our original points is unique,
as it is in this case, the matrix will always be invertible.
The easiest way to show this is by calculating the determinant, which you will find is non-zero,
but I'll link a nice linear algebraic proof of this fact in the description as well for those
interested. Anyways, what this means is that for every set of points, there exists a unique set
of coefficients and consequently a unique polynomial. Taking a step back,
what this means is that there are actually two ways to represent polynomials of degree D,
the first of which is the coefficient representation and the second with just D plus 1 points,
which we will call the value representation. A nice property of using the value representation
is multiplication of polynomials is much easier. Let's say I ask you to multiply these two polynomials
A of x and B of x. We know the resulting polynomial C of x will be of degree four,
so we'll need five points to uniquely represent the product. What we can now do is take five
points from each of the two polynomials and then simply multiply the function values one by one to
get the value representation of the product of the two polynomials. Following our earlier rule,
there is only one degree four polynomial that passes through these points.
That polynomial happens to be the following in the coefficient representation and this is
indeed the product of our original A of x and B of x polynomials. With the multiplication operation
being performed using the value representation, we've now reduced the time for multiplication
from our original D squared operations to the order of only degree D operations.
Okay, so let's propose a plan for an improvement to polynomial multiplication. We are given two
polynomials in the coefficient representation of degree D each. We know multiplication is faster
using the value representation, so what we'll do is evaluate each of these polynomials at two D plus
one points, multiply each of these points pairwise to get the value representation of the product
polynomial and then finally somehow convert the value representation back to the final
coefficient representation. This is the grand plan, but there are several pieces of the puzzle we
haven't figured out. What we're missing is really some sort of magic box that could take
polynomials in the coefficient representation and convert them to the value representation
and then vice versa. That magic box my friends and trust me it is truly magical is the fast
Fourier transform. Let's first focus on taking polynomials from the coefficient representation
to the value representation, which we will call evaluation. We have a degree D polynomial and
we want to evaluate the polynomial at n points where n is some number greater than D plus one.
Let's think about the most straightforward way to do this. We can pick n random x coordinates
and simply calculate the respective y coordinate. This works, but when we deconstruct what's
actually going on here, we run into our old foe. Each evaluation will take O of D operations,
making this method run in O of n times D time, which ends up being O of D squared to evaluate
all n points. So we're back to where we started. Can we find a way to optimize this?
Let's try to simplify the problem. Let's say instead of considering a general polynomial,
we wanted to instead just evaluate a simple polynomial like p of x is equal to x squared
at eight points. The question now is, which points should we pick? Is there any set of points when
knowing the value of one point immediately implies the value of another? In fact, there is. If we
pick the point x equals one, we immediately know the value of the point x equals negative one.
Similarly, if we pick x equals two, we automatically know x equals negative two will have the same
value. Extending this idea, the key property we want here is that our eight points should be
positive negative pairs. The reason this works is due to the property of even functions where
a function evaluated at negative x is going to equal the function evaluated at positive x.
Okay, but then the next immediate question is what about functions like x cubed? Does the same
trick work? It actually kind of does, but with one copy yet. Each positive x value will have the same
value as a negative x value, but with the sign flipped. So in these two cases of odd and even
degree single term polynomials, instead of evaluating eight individual points, we can
actually get away with evaluating exactly four positive points from which we immediately know
the value of the respective negative points. Let's see if we can extend this idea to a more
general polynomial. Taking inspiration from our early exploration, let's split our polynomial
into even and odd degree terms. If we factor an x from the odd degree terms, we end up having two
new polynomials where these new polynomials have only even degree terms. Let's actually give these
polynomials formal names, the first representing the even terms and the second representing odd terms
after factoring out x. This fact allows us to recycle a lot of computation between positive
and negative pairs of points. A bonus fact is that since these even and odd polynomials are
functions of x squared, they are polynomials of degree two, which is a much simpler problem than
our original degree five polynomial. Generalizing these observations, if we have an n minus one
degree polynomial that we want to evaluate at n points, we can split the polynomial into even
and odd terms with these two smaller polynomials now having degree n over two minus one. So how
do we evaluate these polynomials with half the degree of our original polynomial? Well, what's
beautiful here is that this is just another evaluation problem. But this time we need to
evaluate the polynomials at each of our original inputs squared. And this works out nicely since
our original points were positive negative pairs. So if we originally had n points, we now only end
up having n over two points. This is starting to smell like the start of a recursive algorithm.
Let's take a look at the bigger picture. We want to evaluate a polynomial p of x at n points,
where the n points are positive negative paired. We split the polynomial into odd and even degree
components. We now have two simpler polynomials of degree n over two minus one that only need
n over two points to evaluate. Once we recursively evaluate the smaller polynomials,
we can then go through every point in our original set of n points and calculate the
respective values by utilizing the relationship between the positive and negative paired points.
This gives us the value representation of our original polynomial. If we can get this to work,
this means we have an O of n log n recursive algorithm since the two recursive sub problems
have half the size of the original problem and take linear time to evaluate the n points.
This would be a huge improvement from our earlier quadratic running time,
but there is one major problem. Can you spot the issue?
The problem occurs at the recursive step. The entire scheme relies on the fact that the polynomial
will have positive and negative paired points for evaluation. This works at the top level,
but at the next level we are evaluating n over two points where each point is a squared value.
These all end up being positive so the recursion breaks.
So then the natural question is, can we make these new set of points positive negative paired?
Some of you may already see it, but this actually leads to the third absolutely
ingenious idea behind the FFT. The only way this is possible is if we expand the domain
of possible initial points to include complex numbers. For a special choice of complex numbers,
the recursive relation works perfectly where every subsequent set of points
will contain positive negative pairs. What possible set of initial n points has this property?
This is a hard question and to answer it we're going to do a little bit of reverse
engineering with an example. Let's say we have a degree 3 polynomial which requires at least
n equals 4 points for its value representation. These points need to be positive negative pairs
so we can write them as x1, negative x1, x2, and negative x2. We know that the recursive step
will require that we evaluate the odd and even splits of the polynomial at two points,
x1 squared and x2 squared. Now the key constraint here is that for the recursion to work,
these two points also have to be positive negative pairs. So we have an equivalence
between x2 squared and negative x1 squared. At the bottom level of the recursion,
we'll have a single point x1 to the power of 4. Now what's nice is that we get to choose these
points. Let's see what happens if we pick our initial x1 to be 1. This means two of our initial
points are 1 and negative 1 which at the next level of recursion means that x1 squared and
negative x1 squared also have to be 1 and negative 1 respectively. And at the bottom layer we have
only one point which ends up being 1. Now the question becomes what x2 should we choose so
that when we square x2 we end up with negative 1. The answer to that is the complex number i
which means that the four points that we need to evaluate this polynomial at are 1, negative 1, i,
and negative i. An alternate perspective to what we just did here is that we
essentially just solved the equation x to the power of 4 equals 1. Since at the bottom layer
of the recursion, the value of any of our original points to the power of 4 was 1.
We know this equation has four solutions, all of which are encompassed by a special set of
points called the fourth roots of unity. Let's see if this generalizes. If given a degree 5
polynomial, we'll need n is greater than or equal to 6 points. Since our recursive method is splitting
each problem in half it's convenient to just pick a power of 2 so let's pick n equals 8. Now what we
need to do is to find 8 points that are positive negative paired such that each of these points
when raised to the eighth power is equal to 1. We see that the right points are the eighth roots of
unity. Generalizing this to any d-degree polynomial what we will do is pick n is greater than or equal
to d plus 1 points such that n is the power of 2 and the points that we should choose are the
nth roots of unity. This fact deserves a little bit more explanation. Why does this work? Before we
answer that question let's formalize a few things. The nth roots of unity are the solution to the
following equation and they are best visualized as equally spaced points on the unit circle.
The angle between these points is 2 pi over n. With this fact a nice way to compactly write
these points is with complex exponential notation through Euler's formula. One standard way to
define the roots of unity is by defining this omega term as e to the power of 2 pi i over n
and then what this allows us to do is define individual roots of unity quite compactly. Here
are some examples. So now when we say we want to evaluate a polynomial at the nth roots of
unity what that really means is we want to evaluate it at omega to the power of 0, omega to the power
of 1, so on and so forth until omega to the power of n minus 1. So going back to our original question
of why evaluating the polynomial p of x at the nth roots of unity works for our recursive algorithm
there are two key properties at play here. For one our original set of points are positive
negative paired where for the jth root omega to the power of j omega to the power of j plus
n over 2 is going to be the corresponding pair. Now in our recursive step we will be squaring
each of these points and passing them on to the even and odd degree polynomials. This is what happens
when we square our original nth roots of unity. This reveals the second key property of the nth
roots of unity. When we square the nth roots of unity we end up with the n over 2 roots of unity
which are also positive negative paired and are just the right number of points for the two new
polynomials of half that degree. This same pattern holds at every level of the recursion until we
end up with just one point. How beautiful is that?
All right we are now ready to outline the core fast Fourier transform algorithm.
The FFT will take in a coefficient representation of a degree n minus 1 polynomial where n is the power
of 2. We will define omega as e to the power of 2 pi i over n to allow us to define roots of unity
easily. The first case we need to handle is the base case which is going to be when n is equal to 1.
All this means is that we are evaluating the polynomial at one point. Our recursive case is
two calls to the FFT. One on even degree terms and one on odd degree terms. The intention is that
these polynomials are now half the degree of our original polynomial so they only need to be evaluated
at n over 2 roots of unity. Assuming the recursion works the output of these calls will be the
corresponding value representation of these even and odd degree term polynomials which we will label
as y e and y o. Now on to the tricky part which is to take the output from these recursive calls
and combine them to get the value representation of our original degree n minus 1 polynomial.
We saw earlier that the key idea was to use the relationship between positive and negative pairs
but now we have to slightly modify this logic for our roots of unity inputs.
As a quick note yes I did modify the indexing to zero indexing because we're getting ready
to write some code. We know the jth input point will correspond to jth root of unity which results
in the following relationship. We also saw earlier that the paired point negative omega
to the power of j is equal to omega to the power of j plus n over 2 due to the properties of the
roots of unity. Using this fact we can modify the second equation as follows. And lastly one more
fact that's nice is that the jth index in our y e and y o list correspond to the even and odd
polynomials evaluated at omega to the power of 2 times j. What this allows us to do is rewrite our
equations as follows which makes it much easier to implement code. As mentioned this part is tricky
so I encourage you to take your time and verify that each of these steps is indeed true.
The final step in the FFT algorithm is to then return the values of a polynomial p evaluated at
the nth roots of unity. Let's now translate this outlined logic into code. Our function FFT will
take an input p which is the coefficient representation of a polynomial p. We first define n as the
length of p and we will assume that n is a power of 2. Just to be clear there are implementations
of the FFT that can handle n not being a power of 2 but those are way more complicated. The power
of two cases encompass the core ideas of the algorithm. We now handle the base case which is
just a matter of returning our original p. This makes sense since we only have one element making
p a degree zero polynomial or constant. Otherwise we define omega as we have outlined and then
proceed with the recursive step. The first part of the recursive step requires splitting the polynomial
into even and odd degree terms which is quite easy to do. Then we recursively call our FFT function
on these polynomials that now have half the degree of our original polynomial. We denote the outputs
as y e and y o as we have done in the outline. Now it's time to put this all together. We initialize
our output list which will contain the final value representation. Then for all j up to n over 2 we
calculate the value representations as we have outlined. After populating all values in our list
we then return that list and that's the FFT. Overall pretty crazy how all the ideas we talked
about end up coming together in 11 lines of truly elegant code. Let's now take a larger look at our
original problem of polynomial multiplication and see where we are. We now have a way to convert
coefficient representations to value representations efficiently using the FFT. So now the only
missing piece is the reverse process of converting from value representations to coefficient
representations which is formally called interpolation. This is where things get really wild.
On the surface the idea of reversing evaluation feels like a significantly harder task.
Let's take a step back and look at this problem from another perspective. Evaluation and interpolation
are closely connected and as we saw earlier we can express evaluation as a matrix vector product.
We have a vector of coefficients multiplied by a matrix of our evaluation points to give us the
value representation. Now in the FFT algorithm the kth evaluation point was a corresponding
root of unity which allows us to rewrite the matrix vector product as follows. This particular
matrix has a special name the discrete Fourier transform or DFT matrix. In most textbooks
and references the FFT at its core is an algorithm for calculating these types of matrix vector
products efficiently. Polynomial evaluation at the roots of unity happens to be one case where
this type of matrix vector product shows up so that's why we can use the FFT. Anyways the nice fact
about the FFT and evaluation in this context is that interpolation is much easier to understand.
Interpolation requires inversing this DFT matrix. For interpolation we are given a value
representation of our polynomial and we want to find the coefficient representation which means we
have to multiply the value representation by the inverse of the DFT matrix. So let me show you what
the inverse of this matrix looks like. I'm purposely skipping a lot of important linear
algebra facts here since that would be an entirely different video but given that this
is the inverse matrix what stands out to you? It's really quite amazing but this inverse
matrix looks almost the same as our original DFT matrix. In fact the only difference is that every
single omega in our original DFT matrix is now just replaced with omega to the power of negative 1
with a normalization factor of 1 over n. This indicates a potential to reuse the FFT logic
for interpolation since the matrix structure is basically the same. Let's formalize this suspicion
by doing a direct comparison. In a valuation which involves the FFT we are given a set of
coefficients and evaluate the polynomial at the roots of unity to get a value representation.
This involves the following matrix vector product where we define omega as e to the power of 2 pi i
divided by n. Looking at interpolation we now want to define what is formally called the inverse FFT
algorithm. The inverse FFT will take a value representation where each value was evaluated
at the roots of unity and gives you a set of coefficients for the original polynomial basically
reversing what the original FFT did. As we just saw this requires multiplying by the inverse of the
DFT matrix. We noted that each omega in our original DFT matrix now corresponds to 1 over n times omega
to the power of negative 1. Now the punchline here is that what this means is we can define the
inverse FFT as the same FFT function but now called on the value representation with omega
defined as 1 over n times e to the negative 2 pi i divided by n. That's it. With those small changes
we have an inverse FFT that performs interpolation. Just so we are super clear on what sorcery just
happened let me remind you of the original FFT implementation and now let me show you the
inverse FFT implementation which takes the value representation as an input. What we literally
do is copy our FFT implementation change the name of the recursive calls to match and then
literally change one line of code one line and that's all there is to it. So if your mind
isn't blown you haven't been paying attention. Let's take a look at what we just did. We motivated
the FFT through the problem of polynomial multiplication where the first brilliant idea
came from representing and multiplying polynomials using the value representation.
Converting polynomials to a value representation required us to come up with an appropriate
set of evaluation points. Our first attempts at solving this problem inspired the clever idea
of using positive negative pairs but the recursion didn't quite work unless we expanded the domain
to complex numbers. The next brilliant idea came from using the nth roots of unity where the points
at every level of recursion are positive negative paired. This evaluation scheme using the roots
of unity encompassed the essence of the FFT algorithm. When confronted with the problem of
reversing the process using interpolation we discovered something truly astounding. The inverse
FFT is the same algorithm but with one minor adjustment. So if we take a look at what we
just did here there's not one, not two, not three but four absolutely mind-blowing ideas
that come together to make this work. Do I really need to say more on why this is my favorite algorithm?
That's all for this video and thanks for watching. If you enjoyed the content please
hit the like button so that this content will be recommended to more people. If you want to see
more content like this please don't forget to hit the subscribe button and if you want to more
directly support the work of this channel please check out the Patreon page linked in the description
I'll see you all in the next video

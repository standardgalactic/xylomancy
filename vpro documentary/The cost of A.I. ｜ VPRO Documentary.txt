I don't want to be ashamed by the things I don't know, because there is a reason why
I don't know those things, and the reason it's because they are not transparent.
If they say, okay, but this is not wrong, I say, okay, tell me, tell me how it is.
In a world that seems to be losing its way, we search for inspirational people and stories,
hoping to find traces of our future.
This is Backlight. Welcome to the world of AI.
AI, or artificial intelligence, is hot.
When we think of AI, we think of this benevolent, all-knowing God-like machine that just, you know, works by magic.
The rise of AI is even being compared to the invention of the steam engine, electricity, and the internet.
I would use it so I can create a digital copy of myself, or like a digital clone, that sounds like me, thinks like me, hopefully, I mean, maybe.
I think there is too much trust in a technology that we don't understand.
Backlight reveals what is necessary to keep and maintain this new technology, to make it work for us.
What are the true costs of this revolution rapidly unfolding in front of our eyes?
Costs that Silicon Valley would rather keep hidden from us.
This is the skyline of Novi Sad, a university city in Serbia, a city in which the scars of the Yugoslav wars, which occurred just before the turn of the century, are still visible.
This is the skyline of Novi Sad, according to an AI-driven application.
You enter a series of words, which describe the image you have in mind, a so-called prompt.
And the AI generates...
Voila, four different Novi Sad skylines, a city with a wide river flowing through it and several bridges.
This is the real skyline of Novi Sad.
The man who lives here calls himself a critical cartographer.
He creates maps of things of which there are no maps.
I became really obsessed with materiality and anatomists of what they present to be an AI.
I really like to think about what's behind.
So I'm not so interested in what we can do with it and how the interface looks like and how it's created.
I'm interested in what's going on behind, so how it's made.
Our cartographer has made it his aim to reveal the complex world behind our algorithms.
He does this by dissecting and mapping the anatomy of the invisible digital universe in minute detail.
Okay, this is probably the trashiest setup that I ever had.
This one is now shown in the Museum of Modern Art in New York, so this is the second best setup that we ever had.
There is something that I always wanted to do.
I wanted to try to draw those kind of landscapes and basically internet landscapes.
So this is the map of one internet service provider.
So what are the little dots then?
The little dots are individual. It's routers, computers, it's basically IP addresses.
So for example, this is me.
Your computer?
My computer, yeah.
It's sort of like a galaxy in a way?
It is, it is, it is.
So we don't tend to think about those infrastructures.
When we speak about the invisible infrastructures, we can find a lot of interesting stories in like a minimum,
but we can also think about them in some kind of like planetary scale.
Okay.
Besides mapping the digital universe, Vladanyola has also attempted to map the workings of Facebook's algorithm.
How much time did it take you to make this work?
Usually for each of those maps, it took me like one year.
Sometimes now I'm working on some map for the last three or four years.
Oh, one map.
And I said, okay, I want to do things that can take me a lot of years and that I don't have a deadline
and that I'm not paid by anyone and just like doing this for the sake of my own investigation.
And this is why all of those maps are so in a way impossible and not possible
because no one will probably do it.
And it's still one of the, no, it's probably the only one, the only map of the algorithmic process.
But do you mean that Facebook does have such a map themselves?
No, no, I don't think also that they have it because like it's really important to understand that our position
in making those maps, it's a position from the someone who is looking at this from the outside.
So this one is called the anatomy of an AI system and I did this one together with Kate Crawford.
It has three parts.
This is the bird, this is life and this is death.
Using this map, he's tried to visualize which planetary scale systems are used when you use an AI application.
Whether this is a smart speaker, a mute planner or the now popular chat GPT.
So basically what we were like doing for back then, like five, six years, it's just this line.
So this is you as an user.
So we are following basically how the information is flowing through the infrastructure and going here
and then coming back and then going back here.
And this is all happening in one second or less than one second.
And because it's so fast, we don't question all of these things.
If you turn on the light with your Google Home or...
Exactly, information goes from here to, I don't know, West Virginia and come back
because you were so lazy to stand up and to turn on the light.
But you activate the planetary scale system to do that instead of you.
But it's a different dimension of different planetary scale systems.
It's not just one.
From there, it goes into many different directions.
It goes in the direction of death on one side or on this side.
It goes into the birth.
In order to make all our AI function to begin with, we need minerals and precious metals.
So each of those elements are basically, you know, like story of lithium.
Silicone.
Silicone or like whatever, some rare earth elements.
So each of them, if you follow each of them, it's completely like a festival of craziness
that is happening to them.
Mines, smelters, refiners, component manufacturers, assemblers, distributors.
Really simplified.
There is like thousands or tens of thousands of companies participating
from like the mine in Congo to the Foxconn in Shenzhen.
So it's like constant flow of metals, components, labor as well.
Eventually, all those raw materials will not only be used to make smartphones,
but also to construct a worldwide network of fiber optic cables and data centers.
Inside of those data centers, you have algorithms.
Algorithms that require super fast chips.
To train this network, to have hundreds and hundreds of GPUs or processor power.
And since the newest AI needs more computing power to perform its calculations,
those chips are desperately needed.
Then behind this, you needed to have some human beings training those algorithms.
You have human beings labeling those algorithms.
And this is most of the time done by some kind of like invisible labor.
This is completely like not part of our consciousness, let's say.
So is this the cloud in a way?
This is the cloud, yeah, in some strange way.
Some really hardcore cloud made of sweat, blood and metal.
And it's just not possible to visualize.
This is probably in there.
Silicon Valley prefers to use the cloud as a cover all term for all those algorithms, chips and data centers.
It could hardly be more light-hearted, because it's in this cloud that we give AI its prompts,
where it seems like results appear out of thin air.
What they call AI is basically a statistical model that is taking everything,
every piece of information that they feed into it, and then creating a statistical model out of it.
So it doesn't know anything.
There is no any kind of cognition in that sense.
It's basically just statistical reflection of the data that is within the data set.
What would happen if we use their very own AI models to visualize that which Silicon Valley would rather keep hidden?
Let's begin with the chips.
Computer chips are needed to let algorithms perform their computations.
An important raw material to produce chips is silica sand, a very fine sand.
And 70% of the world's silica sand originates from China.
These are not images of mines, but outcomes of a statistical model.
An AI which has generated an image of an apparent silica sand mine in the Chinese province of Xinyang.
Gaining entry to these Chinese mines is practically impossible.
Thankfully, however, AI can show us what the people who work in such mines look like.
Just to be clear, these miners do not actually exist.
They are statistical computations of what these miners may look like.
According to an international study, Uighur force labour is being used to mine silica sand.
These workers are kept out of sight in remote internment camps.
AI, however, has no problem generating an image of them.
How is it possible that AI generates an image of everything you ask it?
The first step is creating this huge amount of data that is becoming this training dataset.
Then the second step usually is labelling.
When we have face recognition or, let's say, three recognition systems,
in which we feed thousands and thousands of images of trees,
then some human beings need to label each of those images and say,
this is the tree, this is not a tree, this is this type of tree or whatever.
So this is like a human, invisible human labour.
And this is a really super-intensive job.
So basically what we are doing, we now have thousands and thousands of images of trees.
And now we are compressing this into the statistical model of the trees.
So it's not any more images, now it's some kind of multi-dimensional statistical space.
And it's a really nice word to explain, it's like hallucinating.
So those are statistical hallucinations.
But how are these trees provided with a label?
The Weisenbaum Institute is located in Berlin.
It's a research institute named in honour of Joseph Weisenbaum,
one of the godfathers of AI and founder of the world's first chatbot.
He once said...
The danger of artificial intelligence is not that machines will think more and more like people,
but that people will think more and more like machines.
Milagros Miseli works at this institute.
Together with her team, she carries out research into the working conditions of the humans who make AI possible.
I am a sociologist first and foremost, but now I'm also a computer scientist.
I had never worked on tech and I didn't have a technical background,
so it was also for me the exercise of understanding how AI works through looking at humans.
But it's also about the machine learning systems that are increasingly affecting all of us,
that are increasingly penetrating many more and more aspects of our everyday lives.
To train a machine learning system, you need big amounts of data.
And the data needs to be ready to be processed by these systems.
And then I heard that someone was doing the classification and interpretation of data.
Basically, I asked about that because I couldn't believe that this happens just like magic,
so I was like, who's doing that?
And I remember the first reaction to my question was like,
all right, yes, there are people actually doing that,
but it's like we never really thought of those.
So you need humans to have an AI like-chat GPT function?
You definitely need humans to, for any machine learning systems, you need humans.
Machine learning systems wouldn't work without humans.
Okay, Omar, I would like to thank you for talking to us today.
This is our team.
Hi guys, nice to meet you all.
I'm Omar, I work as a content moderator for f***ing in an outsourcing company which is called f***ing.
Omar Moe is a pseudonym.
If Omar were to speak out in public about his job, he might lose it.
So can you tell me what it means to be a content moderator?
What is your work about?
So in an easy way, I want to explain it.
So if you are using the platform of, for example, Tik Tok Instagram meta and you post something,
the people like me try to be the police on the platform and moderate the content which nobody wants to see.
And what kind of content is that when nobody wants to see?
For example, there are low violations like bullying or hate speech,
but the most hard stuff is more like suicide, type pornography,
or like people like ISIS who are so cutting up people ahead.
What does the work of content moderators have to do with AI?
We are feeding the AIs, we are going more and more in detail to label the things for the AI,
that the AI has more opportunities to work on their own.
They can work and be done in different ways, right?
You could do this in-house, what is called done in-house,
which is done within the same four walls where the algorithms are modeled,
just sorry for the redundancy, but that's not the thing that happens more often.
What is very often is that this type of work is outsourced,
and outsourcing means outsourcing it through BPO's,
which is short for Business Process Outsourcing Company.
In my company, there are like 1,000 employees.
There are a lot of students and people who are not able to speak in German,
so there is a good opportunity for them to get a job in their original language form,
the native language and also in English.
And if they are losing their work, they have really big problems with the government
and maybe lose the title to stay in Germany.
What is the percentage of workers who don't have a German passport and depend on a visa to stay?
I would say it's maybe like 80% maybe above, I'm not sure.
Companies are very reluctant to disclose the mere presence of human workers.
They don't disclose the presence of data workers, they don't talk about how much these workers are being paid,
they don't talk about where they are and under which conditions they work.
These companies that are generally hidden from public view are located all over the world.
I wanted to ask about the psychological support, because as you mentioned, there can be really violent content.
What are the psychological support you are provided with?
First of all, nobody is able to choose what the content is.
It's like random content, if you are in a bad position,
it could be that you're just handling suicide and self-harm the whole eight hours you are working.
What if you're feeling bad after seeing something?
I mean, can you just stop working then or is it like you have to...
You are able to take well-being time and they tell us to take as much as we need,
but on the other hand, we have a key performance indicator and I have to fulfill these targets and stay in production.
They do not argue about mental health, so they don't care to be honest.
In all cases, these workers are trained to do what the requester requires, what the client wants.
The first thing that they learn on the very first day on the job is,
here, no matter how you feel about the data, no matter what you think or what you consider to be true about this data,
no matter if you would label this data in a different way or classify it in a different way,
no matter if you think this is offensive, detrimental for society, this is ethically incorrect.
Your only, one and only task is to make the client happy, because if they don't, then they get kicked out of the job or kicked out of the platform.
What kind of coping strategies have you or your colleagues developed to actually cope
and deal with this kind of stress of being confronted with this type of material that you described before?
A lot of people use the well-being time individual to take a breath.
For me, to be honest, I have no problems to see someone who is cutting his own arm off.
If I see content, it's more usual for me than to see in the heart scene in a movie.
Really big example for it. In the last month, there was someone who has done suicide, which was an employee of my company.
Let me get this straight. Your colleague committed suicide.
Yeah, right. And the company just wrote to me that the suicide has nothing to do with the content and stuff like that,
but this person tried to contact our well-being team multiple times and he didn't get the support he deserved.
After this content, he took relief and he has done suicide.
It is not a good look for a company like OpenAI or any other company, to be honest.
It is not a good look to say, yes, our very fancy and shiny product is trained on the labour of workers in Kenya,
for example, who are paid less than $2 an hour.
Our fancy new chatbot is trained on the labour of workers in Syria, who not only live in a war-ridden place,
but also they are paid per task. They never know how much they will make by the end of the month.
There is no way for these workers to tell us that we did them wrong. So, yeah, who wants to disclose that? Nobody.
They cannot deny that these workers are being exploited and they know. They really know.
If someone would tell me, oh, I didn't know about that, I wouldn't believe them.
When you have a deadline, when you have one of those very exciting engineering challenges that you really want to solve,
well, the precarity of data workers tends to be like a topic for another day, not the priority of today.
Apart from the blood, sweat and tears needed to moderate and label data,
you need a lot of data for the newest generation of AI. And where do you get it all from?
So, we are now in basically, I think, one of the oldest libraries in Serbia,
but basically their role was to catalogue everything that is printed and written in Serbia.
So, I like to think about these kind of spaces as some kind of data centres,
because basically they are data centres, but from the past.
But someone, in some moment of time, invented those technologies.
So, basically, this is the new media of some time.
So, what has this to do with AI?
So, what we have here, I'm completely randomly accessing whatever. So, this is basically metadata.
So, and if you have to label images, for example, if you have to label images of cats or trees,
do you also call that metadata?
Yeah, yeah, the labels are metadata.
Metadata is data about data. So, it's data explaining data, data explaining content.
Once you standardize metadata, then you are able to do statistics,
then you are able to do metadata analysis, you are able to do a lot of automation, basically.
And another thing, if we think about these libraries and that all of them are going to be basically resources
or like territories that are going to be extracted.
And basically, because the idea behind Google Books and everything,
it needs to extract all the information that exists in the buildings like this.
And then the question is like, who was able in history to create like the archives?
And who was doing the archives, who was not doing the archives?
And how the things were like done in history.
So, the countries that have a lot of archives have a better starting point now,
because they have a data to train some kind of like artificial intelligence or to train whatever.
So, they would be represented more accurately in any AI.
So, more data you have from the present or from the past means that you are able to be more precise.
It is really important what is going on within the dataset.
What kind of pictures or images or sounds are being part of the dataset,
because this will be reflected to the world as a rule, as an automated process.
So, Silicon Valley is looking for datasets.
You might even call it a hunt for the biggest possible dataset.
But what do these datasets consist of?
Abeba Birani of Trinity College Dublin is one of the few scientists researching the composition of datasets.
She does this by auditing the data, checking its quality.
Data sets are really critical.
They are important components of any model, because without large scale datasets, you can't have models.
Even though datasets are really important, there is not so much attention to asking what's in the dataset,
where does the data come from.
Actually, the standard is very low, because datasets tend to be really bad.
We don't go in thinking, you know, is it good enough?
We go in thinking, how bad is it?
So, a lot of the initial auditing process involves just looking at the dataset itself.
So, these are, for example, the prompts that I kept a record of.
African, Asian, the A-word, auntie, skinny, small, terrorist,
obscured, white power, white supremacy, woman, the F-word, another F-word.
Yeah, a lot of words I can't say out loud.
People would spend a lot of time, you know, in collecting the data,
for example, putting aside resources to label the data,
and doing various tasks to detoxify the data, to improve the data.
But now, over the past two years, all that is gone.
The way datasets are created is not through human curation,
but they use automated systems to collect datasets, mainly from the common crawl.
AI programmers learnt that you can generate smarter and more interesting outcomes
by working with larger datasets.
Therefore, they shifted en masse to enormous, automatically collected datasets,
such as those of common crawl.
The common crawl is a U.S. company where they crawl the web,
where they gather data from the web every day, and they accumulate it in this huge dump.
So, every day, you have more data coming in.
So, it's like a vacuum cleaner?
Yes, it's like a vacuum cleaner. That's a really good example.
You find really, you know, deeply ingrained stereotypes represented in datasets.
So, for example, when you type beautiful, you get a lot of...
The dataset returns a lot of naked women, a lot of the images coming from pornographic sites,
whereas when you query the dataset with the word handsome,
you find, you know, white men in suits looking very respectable.
So, I'm interested in how these kind of stereotypical understandings are ingrained in datasets.
For example, here is the result for the prompt schoolgirl,
and then you can then compare it with a schoolboy.
And you can see if there is any difference.
As you can see here, for a schoolgirl, there is a lot of sexualized content.
You can see here and here very explicit sexualized content with little girls in school uniforms.
And we can type schoolboy and see what comes up.
I don't see any image that is of sexual nature.
So, a lot of the internet is a dark place.
So, you can type anything here on the portal, and this is the result for the word blonde.
And as you can see, a lot of the images that are returning are of very explicit sexual nature.
As you can see, as I scroll down, it uploads new images and the quality doesn't improve.
I could keep scrolling and scrolling. It's, I don't know, because I haven't got to the bottom of it,
because the dataset is large.
There is this neck-to-neck race to build the bigger model, the largest dataset.
I see the internet as a toxic waste where people dump their toxic waste,
rather than being a representation of everybody's thought.
So, this is why the internet can't be without appropriate safeguard,
without appropriate mechanisms to filter these things out.
This is why the internet can't be taken as a place where, you know,
datasets representing all humanity can be sourced. It's not.
Yeah, the internet is a really problematic place.
And unfortunately, the internet is the only place where you can get dataset that is within billions and millions.
So, there is that problem.
Have you tried to prompt a woman from Ethiopia?
I haven't, but I have prompted Ethiopia, of course, because I'm Ethiopian.
I am interested in how Ethiopia is represented.
So, Ethiopian women, that would be interesting.
You see, a lot of, see Ethiopian women are, the general perception of Ethiopian women is
they are either beautiful or they are, you know, starving or they are poor.
So, that's what you get when you train AI systems based on this data that are, you know, stereotypical.
The modern learns about Ethiopia, for example, from these stereotyping images.
And if we give the AI model this prompt, this is the outcome.
It brings up very cliche, tired, negative, stereotypical of images of African people,
like, you know, black people with face paints, semi-naked.
This is not a true representation of Africa.
This is, you know, western white people's perception and representation of what Africa is like.
So, this is the problem with the internet-sourced data sets.
We are going towards, you know, something that is average, you know, something that is statistical progression.
And we are losing all of those fine grains.
And then, again, if we think about culture and society, what is fine grain?
We are fine grain, you know.
We are fine grain as an artist, as a, you know, like everyone that is different is a fine grain.
And those systems are statistical systems that are leaning towards the, you know, some kind of like statistical mediocracy.
One big part of the map is related to what's going on with devices when they finish their life with us,
you know, when we are kind of getting reading of them.
And basically here we are seeing one part of this process.
We are seeing how these old devices are being thrown away and how they are finishing somewhere.
So, it's either like Africa or India or China.
It's where all of those devices are ending up.
And here they have some kind of second life, or maybe not.
I'm not sure exactly what's going on with all of these things, but now it's some kind of globalised trash, you know.
It's not just like our trash.
Even more data, even more chips, even more computing power and even more AI, how big can this system become?
This is only the beginning.
New AI applications seem to be released on an almost daily basis.
The economist Tame Besiroglu, who works for the world-renowned Massachusetts Institute of Technology in Cambridge,
is on a short visit to Rotterdam.
He's trying to map out what will be needed for future AI models.
Silicon Valley is keeping a close watch on his research.
So, one thing I'd be interested in is training a language model on all the texts that I've ever written.
So, I just download all the emails I've written.
I download all the documents I've ever written, like papers and essays for high school and university and so on.
And conversations on my tweets so I can create a digital copy of myself or like a digital clone that sounds like me,
things like me, hopefully, I mean, maybe.
So, I think I could probably get a reasonably good model and it would be fun experimenting with that
and seeing if I could use that to write emails and WhatsApp messages and so on
and people would like not realize that it was actually an AI system trained to sound like me.
Let us try this.
Yeah, that's right, or maybe me cloning myself or like creating a digital copy of myself.
Unlike that becoming a larger part of my existence or something.
Yeah, there are interesting questions about me being, my identity being more embedded or something with some of these technologies.
And what if everybody would want that?
Since the field of AI kind of got started, we have been scaling up the amount of computation to train these systems,
doubling it every 18 months.
In recent years, the amount of computation has been doubling every six months,
which is much faster than we've seen historically.
We've also seen companies accelerate the amount of money that they're spending on this.
And what does one chip cost?
One chip costs about $10,000.
Yeah.
I mean, they might get discounts and sometimes it's kind of unclear,
but on the order of $10,000 and they use about $25,000 of them,
so that costs about $250 million if you were to buy it, kind of outright.
Just for this one model to work?
Yeah, that's right, that's right.
What exactly is needed for future generations of AI, such as chat GPT-5, 6 and 7?
We know that between every GPT, there's been about 100x increase in the amount of computation.
Increasing the computation by 100x roughly costs them 100x more.
I suspect that that would place the dollar costs in the many hundreds of millions of dollars.
There aren't many players that can afford this.
Not many players that have the hardware infrastructure have access to the large data centers.
So Microsoft is one, Google is one, presumably Amazon and Apple and a couple others can do this,
but few companies can do this.
So you need to have a lot of power, a lot of money, a lot of processing power.
And this is the super, super, super important question.
It's like, who is able to create that?
Because if we go back again to all of this,
we can ask who is the owner of the tool, to whom these tools are belonging.
Because the one who is the owner of the tool of production will be basically the one who will rule the game after.
What the model actually learns during training is something that is very opaque.
And so we are kind of in the dark about actually what happens inside these models,
even the people who are writing the code that they train these models.
Should I look straight into the camera?
So we, as far as I can tell, don't have a very good kind of rigorous science that tells you,
this is the data you want in order to get this behavior.
By that I mean we're kind of, people are winging it, they're just giving it lots of data and saying,
okay, this works and we don't really understand why or how, but I guess that's fine.
So they don't understand how they get to the outcome.
That's right, that's right.
So it's like a magic machine then?
Yeah, that's certainly one way.
Let me just put on my glasses.
We don't have a very good description of what happens inside these large models.
They are kind of like black boxes.
We can't fully interpret the processing that happens between when you give it an instruction
and when it gives you an output.
Without AI programmers knowing precisely what's going on in the black box,
there won't be another way to improve AI further, except by gathering even more data.
So these models, like GPT-4, use on the order of a trillion words that they kind of see during training.
A trillion words.
Right, yeah.
Where did they get that?
So they get this from books and Wikipedia pages and things like high quality news sources,
scientific publications that are important, long code bases that are important,
certainly literature, those are the things that machine learning practitioners have prioritized
when building these data sets.
And what then are low quality data sets?
On the other hand of the spectrum, you have kind of text that you find on large internet platforms
and forums and so on, like Reddit or various kind of hobbyist forums
or maybe even social media, short tweets or short conversations between people.
Are you a WhatsApp conversation?
No, I don't want to say that you have low quality WhatsApp conversations, but some people might.
But maybe in five years we will have, you know, gathered, or these companies will have gathered
a very large fraction of the total data that humans have produced
that kind of exists that humanity has generated as collective.
But we will run out of high quality data?
It's certainly, yeah, I think that's certainly possible that we will use,
we will like want to use way more high quality data than we have access to.
But at the same time in these coming five years, these AI models are generating a lot of data,
be it visual, be it in text.
So what happens to this data? Will this become part of the data set training AI?
Yeah, it's possible.
I would not be surprised if training models on outputs of machine learning models
would be, you know, an okay substitute for the quality of the text that's generated by humans.
If we don't find the solution, how to deal with that?
In one, two, three years, we are going to be completely polluted
by the content that is artificially generated.
In theory, in a few years, there will be more artificially generated content than the human generated content.
And that's completely crazy again.
It's now a statistical system that is made to create bullshit.
So we have basically automation of this information.
And then from the same companies we are expecting that they will find a way how again to automatize what is true, what is not true.
We now have like two different, we expect from them to create like two different synthetic automized system,
one that will produce the knowledge and one that will correct the knowledge.
And it can go wrong in so many ways.
This was just a snapshot in time.
Newer AI models will be here by tomorrow, an AI that seems to have consciousness perhaps,
or one that defends you in court, or it may be a clone of Billy Eilish.
For the record, all the pieces of music you heard in this episode were generated by AI.
Can we already draw a preliminary conclusion?
Living in the world of AI means living in a world of statistical mediocrity.
Would we want to live in such a world?
What and who will make that decision for us?
One thing is for certain, AI will not drop from the cloud.
It will come at the cost of blood, sweat and precious metals.
Once you realize that it's a statistical hallucination, then it's interesting.
You can enjoy statistical hallucination.
Once you understand it's a statistical hallucination, you can be amazed.
Look how interesting this is.

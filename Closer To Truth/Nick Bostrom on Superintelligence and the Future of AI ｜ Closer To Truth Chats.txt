I think you're doing the mistake of trying to ask a philosopher to be very concise.
I just see like an avalanche of considerations and qualifications and levels for each of
these very complex questions.
Okay.
Welcome to Closer to Truth.
I'm speaking with futurist visionary Nick Bostrom about his vital, far-sighted, engaging
new book, Deep Utopia, Life and Meaning in a Solved World.
I loved it.
It's an exhilarating romp of ultimate technology centered on AI, how it might work, what it
could mean.
It's a prescient manual for the future.
It's an innovative treatise on the meaning of life.
Welcome, Nick.
It's great to see you again.
Good to see you again.
Congrats on Deep Utopia.
We'll discuss it in depth.
But to begin, I'd like to get your world overview, the setting for your book.
When we first did our Closer to Truth discussion in Oxford in 2007, 17 years ago, we discussed
the simulation argument, fine-tuning, anthropic selection, the doomsday argument.
How would you characterize the last 17 years or so in terms of technological and intellectual
development, especially the importance of AI?
Well, I mean, it's all happening now, as we've entered the Atolyus era.
I think the last, especially since 2012, 2014, with the Deep Learning Revolution, I think
we've been on a kind of up-ramp of AI capabilities, kicking into even higher gear with the release
of ChatGPT.
In the last two years or so, we've really seen it hit the mainstream, where now the White
House and key policy makers all over the world are starting to debate the future of AI.
It's a remarkable time.
When did you actually plan this book?
Because it's obviously, in essence, the other side of your superintelligence, 2014, where
you were prescient in warning about the dangers of AI.
The last two years has been a high focus on the dangers, and now you've moved on to the
opportunities.
So when did you have that a bit of transformal insight?
I've been working on it for probably around six years or so.
It wasn't ever planned.
It just kind of happened.
I didn't start out with some particular set of theses I wanted to defend and elaborate
on.
I felt an urge to start writing, and then it eventually grew into deeply topical.
Yeah, and I've seen where you've said that the style of the book, which is very unusual,
it's a new literary style involving dialogue with different characters, your own persona
in a not entirely actual form, but in dialogue with other people in the form of a lecture
series over a period of a week.
I think you've said that whole structure wasn't planned.
It happened organically.
Yeah, it's just the way it happened.
It's better or worse, but I do think the form actually does match the content.
It's not a book so much about conclusions as it is a book about questions and helping
the reader to start to think about these problems and form their own views ultimately.
It's also meant to be not just something that transmits certain concepts and ideas,
but also it's meant to be a reading experience.
You might have to work to get through it, but ultimately I'm hoping it will kind of put
you in a better place to reflect on questions about what art humanity's destiny be.
I think that's an accurate description.
I found myself very engaged.
I was looking for more of the arguments that we've had in the past in a very positive sense,
but this book is different.
You do get the arc of various arguments on different things.
We'll talk about that, but you are brought into that in this engaging intellectual, semi-fictional
avatar environment.
Yeah, yeah.
The other benefit of this sort of having different characters and different bits is that it makes
it easier to explore several different viewpoints, which I wanted to do and allow each one to
be developed in its own right to its fullest extent and then to kind of collide different
perspectives and ideas just as, I mean, you're interested in physics, right?
So with a particle accelerator, you sort of accelerate little particles to enormous energies
and then smash them into one another.
And in those extreme conditions, sometimes you can see the basic principles at work that
we can then infer are at work all the time in ordinary conditions as well.
It's just harder to observe.
And so similarly, this conceit of a plastic world, a condition in which technology has
reached its full maturity and all practical problems have been solved.
It's an extreme condition, but I think we can then see values kind of smashing into
one another that we normally can sort of hand wave and then just because they are obscured
by so many practical necessities that kind of occupy most of our contemporary existence.
I think that's a very good characterization.
The book is creating an extreme condition and particle accelerators do that and physics
black holes do that in physics.
It's an extreme condition where when people study black holes, it's not just for the
black holes themselves, but it's subjecting the laws of physics to extreme conditions
and you you learn a lot.
And I think that's a very good characterization of the book.
Yeah.
So that's, I think it's like, I mean, for people have read it, they will not, but it's
not a book that is really trying to make predictions.
Nor is it trying to offer practical solutions to what we should do next.
I mean, a lot of my other work focuses on that.
This takes basically as an assumption or a postulate, if you want, that things go well
in order then to be able to ask the questions of what then, what would be the meaning of
human existence, what would give us purpose in life.
You know, the whole thing unfolds like everything is perfect governance problems is all the
alignment problem is solved like all these things like, but what then would occupy our
lives.
Sometimes you never actually get to even ask that question because there are so many other
questions that kind of crowding before it.
So I just wanted to postulate that and then focus this book entirely on the set of questions
that arise in this hypothetical future condition.
We're going to get into all of it, but let me first give a more formal bio.
Nick Bostrom is a professor at Oxford University where he heads the Future of Humanity Institute
as its founding director.
With a background in theoretical physics, computational neuroscience, logic and artificial
intelligence, Nick has pioneered contemporary thinking about existential risk, the simulation
argument and the vulnerable world hypothesis among others.
He is the most cited professional philosopher in the world age 50 or under and is the author
of some 200 publications, including anthropic bias, global catastrophic risks, human enhancement
and super intelligence, the prescient book on the dangers of AI.
But now we're going to look at the extreme condition of all goes well.
So Nick, your book, Deep Utopia, Life and Meaning in a Solved World.
Let's start with a simple definition of what is a solved world and what motivates your focus
on it.
Well, I am referring to a hypothetical condition where basically all practical problems have
been solved.
So think, first of all, a condition of technical maturity.
So we have super advanced AIs, maybe they have helped us develop all kinds of other technologies
medical technologies, virtual reality, et cetera, et cetera.
So that's part of what it would mean for the world to be solved.
And then on top of that, we also make the assumption that all the kind of governance
problems of the world have been solved to the extent that they can be solved.
So we imagine we set aside questions of war and conflict and oppression and inequality
and all the rest of it.
So then there remains a big kind of problem, which is ultimately a problem of value, which
is that under these ideal conditions, what kind of lives would we want to live?
And that's a very important way to frame the book because you're not saying all of these
problems that are solved are easy to solve or will be solved.
But if you do solve it, what does that leave and it leaves value?
So one question that I have is, do you distinguish between meaning and purpose?
We use those two terms sometimes interchangeably.
But I think we can tease apart a difference of meaning in the title.
But throughout the book, you have purpose as well.
Yeah, that's right.
So I think of purpose as a slightly narrower concept as sort of having a reason for doing
something or for putting out some effort.
And then meaning, that is discussed in the book, but might be a certain kind of purpose
or purpose plus something else.
OK.
You present, just to give a sense of the environment, a utopic taxonomy where you have
different levels of utopia that can give us a richer understanding of it.
So let me just give you the list and just explain each one very quickly.
The first is government and cultural utopia.
Yeah, this is, I think, the most familiar kind of utopia we find in the literature,
where people imagine a better way for society to be organized.
Better political institutions, different schools, maybe different gender roles.
But usually set within more or less a recognizably contemporary technological context.
So people, there's still work that needs to be done and you can organize how much power
the workers have or how the work is divided.
But there have to be people growing food, et cetera.
So that's the most familiar and basic kind of utopia.
The second level is post-scarcity utopia.
Sounds like we know what it means, but if you could define it more clearly.
Yeah, so this is the slightly more, I guess, radical vision of a condition in which humans
have plenty of all that we need materially.
So there are these kind of, it's more like fantasy in the past, but various kind of,
the land of cocaine was this medieval peasant dream, basically, of some condition where
the rivers would overflow with wine and roasted turkeys would just land on the plate.
And that would be a kind of continuous feasting.
And you could easily see how that on its own would have huge attraction if you were a kind of
agricultural laborer who spent your whole life grinding away, barely getting enough
porridge to feed your children and your joints were aching from all this backbreaking work
that you were doing, then this on itself, just being able to rest and eat as much as
you want, would already be like enough of a kind of vision.
Third level is post-work utopia.
Right.
So this is the idea that not just is there plenty to consume, but that the production
of all this plenty doesn't require human economic labor.
And this has started popping up more recently in conversations about the future of AI, where
people are wondering, will this advance that we see lead to human unemployment?
We can automate more and more things.
And if you imagine that running its full course, then maybe eventually you could automate almost
all human economic labor and then you would have this post-work condition.
The fourth and fifth to get a little more complicated to understand, but let's do it now.
Fourth is post-instrumental utopia.
So now we're getting into a more radical conception.
And usually current conversations about these issues stop short before we reach this idea.
But if you really think through what it would mean for AI to attain its full potential and
then all the other technological advances that this kind of machine superintelligence
could bring about, it's not just that we wouldn't have to go into the office and type
on word processors or hammer away at construction sites, et cetera.
But also a lot of the other things we need to do in our daily lives could be automated as well.
So if you think about, if you didn't have to work, well, then typical answers would be,
well, maybe somebody likes fitness or something, so they could spend more time exercising.
But like in this condition, you could pop a pill instead and get the same physiological
and psychological effects that spending an hour on the Stairmaster would provide.
And so then why would you really need to go to, kind of would lose its point to go to the gym
in those conditions?
And you can then start to kind of almost do case studies on activities that fill our current
lives. And for almost all of them, you soon see that they have a certain structure, which
is that we do a certain thing, put in some effort in order then to achieve some other
thing. So you brush your teeth, because otherwise eventually you will have tooth decay and gum
decay. And so in order to get the outcome of a healthy mouth, you need to spend a few minutes
every day brushing your teeth and going to the gym. Or you need to, like you want to
understand mathematics, let's say. So then the only way to do that is to put in some effort to
study mathematics. And so the effort is motivated by the goal that it is trying to achieve outside
the effort itself. And a lot of the things that we are doing has that structure. But now if you
could get the goal, the end point, without having to put in the effort, then it seems to pull out
the rug under the activity itself. At least it's threatened with the sense of being pointless. So
that's the problem you confront in this kind of post-instrumental.
An example of that, instead of studying higher mathematics, you could have an injection of
nanobots that could that could analyze every synapse in your brain and then figure out how to
change them a little bit so I can understand algebraic geometry or something.
Right, that's that's right. And that would be fast and effortless. Or even things we do for fun. So
there might be various activities that you do, because it gives you pleasure and joy. That that
seems like kind of unavoidable. But even there, if you think about it, you could have more direct
ways of experiencing the same positive emotions, like a kind of some, you know, super drugs that
could give you the pleasure and the joy without you having to spend an hour gardening or, you know,
doing whatever it is that like watching movies are.
So this brings up a very important point of the book, in terms of what are our real values? Because
when I read that, and obviously very intriguing, remarkable way of thinking and very, very
important. I was asking myself, are there any absolute values in a solved world? And so the way to
describe it, as you started to say, is if we could take non harmful drugs or AI neural implants that
would maintain a state of perpetual ecstasy, whatever your ecstasy would would happen to be, or
it can it can switch from a physical bodily ecstasy to artistic ecstasy or intellectual one. And if
that could be all done, is, you know, who could gain say that, if there's no kind of supernatural
value that you would, you would put into it. I think that is a fundamental theme of your of your
book about how do we develop the kinds of values if these things are possible, you're not saying
these things are remotely in the in the near or mid or even long term, but they are the extreme
condition that you talked about, which which then exposes what is the nature of absolute values if
there are any.
Right. Now, it is possible that they might be in the long term or even mid or near term, depending on
how fast the AI revolution unfolds. And what the outcome of that is, I actually think the time
scales for radical transformation might be shorter than most people realize if if AI continues to
speed ahead. What's your best, what's your best guess on that?
Well, I mean, my timelines have shortened somewhat, at least since this previous book, Super
Intelligent was published in 2014. I think we are currently looking on timelines that are on the
shorter end of the distribution. So it's hard to say, but I mean, it could be years or maybe a
decade or perhaps more. But at least I think there's a non trivial probability that that we are now
kind of on the accelerating slope of this, but time will tell. Okay. And anyway, that's not central
to the book, even if you thought this would take millions or never happen. But there are two ways
of thinking about this, you could either just read it as these are perpetual philosophical
questions that humanity ponderous. And here's kind of a thought experiment that helps you think
about them. And I think it's fine. If you just read it like that, for me, there's also this
actual real possibility that we might soon enter a condition like this, or we might have to make
decisions soon about what kind of future we want if we want to stare towards something like this
or some other version of this. So there is this kind of underlying practical motivation for me
in terms of writing this book, but that's, that's optional. So yeah,
I appreciate that. I was more on the former on the thought experiment. And I would put the
put the date in measured in hundreds of years, if not thousands, but to achieve what you're saying.
But you know, I'm, I'm cautious, I become cautious as you. And I hear you and respect your views.
I'm a little less sure of what I thought before. Anyway, we want to get the fifth
category of the, of the utopic taxonomy, which you call plastic utopia. So
even going beyond the post instrumental utopia, what is the plastic utopia?
Well, we alluded to it slightly, which is the idea that in addition to it having these other
properties of being post instrumental in, we also in the plastic utopia condition have complete
control over ourselves. So our mental states, our psychology, our cognitive architecture,
our bodies becomes malleable. So it's not just that we imagine human beings as we are now placed
in this condition where we don't have to work and where we don't have to put out any effort,
if we don't want to, we could just press buttons and get what we want. But we ourselves as well
become something we can choose. So you wouldn't have to work on yourself to build a better character
in a plastic utopia. If you wanted to, instead, you could sort of, you know, request of your AIG
need to sort of re rewire your synapses so that you became this different kind of person or to
experience pleasure all the time or to become smarter or kinder or whatever else. So that makes
it even more like solved or dissolved or liquid, like you enter this context where everything
seems kind of fluid and up for grabs. And it's hard to find any sort of firm ground to stand on.
Yeah, it's a wonderful way of seeing what an extreme condition is, because I would have not
come up with those five, I might have had three or four. But at that level, it really very beautifully
defines what an extreme condition for humanity could be in the future and therefore gives the
book its real real punch. So one issue that you deal with, especially as you get to all of those,
is the question of boredom and how much of our value system is based upon the need to
or the lack of control and the uncertainty and what happens whenever this, you know,
this is the same kind of problem that traditional religions both East and West have to deal with
whether you're dealing with nirvana after the innumerable cycles of birth and death and rebirth
and then you reach nirvana or in the Judeo-Christian, Islamic, Abrahamic concept of heaven, eternal
life in heaven. I mean, that's a sort of an end question of boredom that occurs in any of these
eschatological ideas. Yeah, it's quite fascinating. If you think far enough in this direction,
you do start to sort of both up against theological questions or at least questions that have
traditionally been discussed in religious contexts about the afterlife, etc. I try to
not trespass onto that terrain in the book. I have this, there's this other fictional character,
the fictional Bostrom character is giving these lectures and then there are sometimes he's asked
questions by the students and occasionally he sort of refers to, well, you have to take that up with
Professor Grossweiter, which is like another character who doesn't make an appearance in the
book, but he's like the theologian or the person who could answer their questions on that.
But on boredom, yeah, so this is an example where I think it's important to distinguish
in two different senses of boredom, which is a subjective sense and an objective sense. So clearly
we have a subjective concept of boredom, like somebody might just feel bored.
And that would trivially be easy to abolish in Utopia. I mean, it follows directly from
the condition of plasticity that this feeling is like a subjective state of your brain. You could
rewire that so that you would always feel excited or interested or whatever
antonyms to boredom you want to use. And the question then is whether there is also some
notion of objective boredom or boringness, whether certain activities or experiences
are such that they are objectively boring, like meaning perhaps that it would be appropriate
to feel subjectively bored if we engaged in them. So if you imagine somebody
to take an example actually from the philosophical literature of a grass counter,
so somebody who spends his life counting the blades of grass on a particular college lawn,
we might think that that's an activity that is objectively boring. Whether or not he happens
to feel excited about it, it's not appropriate to be really interested in grass counting because
it's like too monotonous or insignificant or has some other sort of deficit.
And philosophers disagree about whether there is some kind of
firm normative basis for making that, but I think it's an intuition that
some but not all people would have that it would be bad if the future consisted of
merely of activities like counting grass, no matter how thrilled the people doing the grass
counting work. So that's an objective value that kind of is a superset to everything else
we're talking about. Yeah, potentially. And so I'm able to in a plastic world change my brain
to where I am excited about every new grass that I count and what's going to happen at the next
number and I'm genuinely excited about that and I've changed my brain to think and so subjectively
I'm excited about life at counting all these grasses or as I think you have in the book
table legs, 200 and some 1000 table legs or that that potentially is in some objective sense
is suboptimal. Yeah, that's possible to hold that view. And
now if one does have that view that it would be objection objectively bad to have nothing in
one's life than you know, counting grass, then that kind of also potentially imposes a constraint
on the subjective experience of boredom. Like if it actually is objectively bad to do that,
then you might think it would be also bad to change your subjective attitude so that you
found great interest is something that would be boring. So then you might end up with a situation
where you couldn't eliminate boredom in the future, neither in the objective or objective sense.
And so then, but so this possibility of the objective value there makes the discussion
more complex, like eliminating this objective, if that's all there is trivial, given the
postulates, but once they introduce this possibility of the objective, then it becomes
like a much more intricate conversation to what extent we would be able to do something
without violating that. Now, I think at least with respect to the value of
interestingness, and there's a bunch of these different values that are kind of related but
different, but if we focus on interestingness, I think there is at least large scope for
increasing the amount of subjective and objective interestingness, including in these
utopian lives. I think even if there is some objective elements to what's boring and what's
interesting, I think it has a large zone of indeterminacy. I mean, you can just look at
the current human distribution. I have a good friend and colleague who
tells me he's never bored and he's interested in, as far as I can tell, literally everything
except sport. He writes papers on all kinds of topics. He knows about everything. He goes to
every kind of conference and finds interesting things to discuss with every person. It doesn't
seem to me that there is anything deficient about his human life. In fact, if anything,
it seems to benefit and be a greater person for this property that he has. Obviously,
that goes down ultimately to some neurochemical idiosyncrasies of his brain. I think for all
of us, I think we could expand the range of things in which we take an interest
greatly before we would reach this point where we would just be counting leaves of grass. Moreover,
I think possibly it would be appropriate to expand it even further than that. Maybe
if we have reached a condition where we had exhausted all the most obviously interesting
things, we had discovered all the really fundamental laws of nature, solved consciousness,
and the biggest questions had all been answered. It would seem perfectly appropriate in that
situation to begin to take an interest in the slightly smaller questions. It's not clear that
one couldn't go very, very far in that direction before I reached a point where it would be objectively
bad to take a further interest. Is religion relevant in a solved world?
Yes, potentially very relevant, although it's also arguably very relevant in the current world.
One might say something more interesting, perhaps if one looks at some other values that
seem not so relevant in the current world, but that could potentially become more relevant
in the solved world. I think that there might be a lot of subtle values that exist now, but we
don't really see them very much, just as we don't see the stars during daytime, because it's like
such a brighter present. Analogously, there are such stark moral imperatives right now,
calamities of all sorts. You need to take care of your kids. There are people starving in the world
or being shot at, etc., etc. So many horrors and an urgent, obvious pressing ethical needs to
fix things, that it would almost be frivolous now to spend too much time threatening about
more subtle, quiet values. But if we ever reached a condition where these pressing needs were taken
care of, then I think we might be able to see a whole panoply of these subtler values. For example,
various kinds of traditions that it would be nice to honor authentically. Ancestors who,
maybe we think of our lost parents once in a while, but there's so many more people who have
lived wonderful lives and maybe deserve more thought and consideration. We don't have time.
Our daily lives keep us busy, but if you didn't have that, why not? Various aesthetic qualities.
You could imagine making your life more into a kind of artwork, where every relationship was
not just a source of relaxation or fun or satisfaction, but also something actually
beautiful that you were kind of constructing together, etc., etc. And we don't now have the luxury
to kind of really develop a fine sensibility for those. But I think it would be entirely appropriate
once the urgencies are removed to sort of tune up, like almost like our eyes dilate at night,
right? So it can take in more light. Similarly, in this condition, our moral sensibilities and
sensibilities for subtle values, I think, should dilate. And this is a larger definition of religion
as we might have it in today's world, enabled by the solved world, that the boundaries of religion
become broader. Yeah, potentially. But again, it also potentially is very important today. So it
might be one of those things that is very urgent today, even with other urgencies pressing in upon
us. Like many religious people would say that, yes, you have all these practical things you
should do, but you should also set aside time for worship, etc., even though it conflicts with,
but even more so, obviously, in this condition. And I mean, we would be more like, I guess,
potentially like monks and nuns that have the time to fully devote themselves
to contemplating the divine. When I first heard of the book and started it, the first question,
one of the first questions came into my mind is, how does a solved world, the Bostrom solved world,
articulate with the Marxist pure communism? And as I started to go through the book, to me,
that question became pretty obvious that there would be a high correlation, potentially,
between the first, at least two of the utopia taxonomy levels, the government and culture,
and then the post scarcity, and then maybe into the post work as well. But pure communism, as it's
been envisioned in the past, or even in few cases in the present, doesn't even deal with points four
and five. Right. And I mean, I think, I'm not really a Marx scholar, but I think he just has
a few lines really about what would ultimately be the outcome of if the whole communist project
succeeded. And I think he refers to this, whatever it is, like fishing in the morning and hunting
in the afternoon and reading poetry in the evening. So that sounds like not even a fully
post work utopia, but like maybe an abundance diminished work utopia, plus a sort of vision
of social cultural utopia, I guess. Right, right. Nick, let's switch and look really long term and
very visionary at what I call your approach to ultimate utilitarianism. I love this section,
a quantitative analysis of the potential for happiness or fulfillment for all sentient beings,
if the cosmic endowment could be maximally saturated with sentience. So some of the numbers
you give, you estimate 10 to the 35th possible human lives derived from human lives originating
on earth to populate the observable universe. That's, I think, 100 billion trillion trillion.
That's your minimum. Then you go up to 10 to the 43rd. And then if you switch to digital lives,
which adds a lot of complex value, you get a computing power of the universe of at least 10
to the 58th, which is 10 billion trillion trillion trillion four trillions there,
in terms of ultimate sentience. So what I love the calculation, but walk me through the importance
of that in our ultimate thinking and also in terms of the concept of meaning and purpose,
which is the purpose of your book. Well, I mean, it's like some big number,
basically, a very big number. But I mean, it's in the context of the book, it's a little handout
as the postroom gives out. Yeah, so I'm not a utilitarian. I'm often mistaken for one because
in some of my writings, I have explored and analyzed the implications of assuming an assumption of
utilitarianism or aggregative consequentialism. Because it's a view that significant fraction
of moral philosophers have held and that I think maybe deserves at least some weight,
even if one doesn't actually embrace it. And then it's interesting to see what follows if one
actually takes that perspective seriously. And hence in my earlier work, this, you know,
the focus on existential risks as those few things that could actually permanently
destroy our future. If one counts these possible future lives the same way as actually currently
existing lives, as certain flavors of utilitarianism would do, then they just seem to dominate and
you get a bunch of interesting. And then there's like a further complication on that, which is
if you literally do this, try to do this expected utility calculation, you find that scenarios in
which somehow, even if they are very unlikely, but somehow infinite values could be realized.
Like maybe we are wrong about physics and there's like some actual way of producing
infinite and then those tend to dominate, even if they have a very, very tiny finite. And then
you get into infinitarian paralysis and there's like a whole. Yeah, so I think that's interesting
in its own right, but it's not really the topic of the book, which more focuses on
not how you aggregate big values or what our obligations are, but like from our point of
view, like what would be the best possible future for Robert or for you, the viewer,
or for any of us, like if you literally could imagine the best possible way for your future
life to unfold. And perhaps we restricted by like the loss of physics, et cetera, but and then trying
to think from the inside, like how we would furnish that life with activity, experiences,
relationships, et cetera. And then ultimately, like, you know, if your question is what you
should you do now as a moral actor, then you would have to somehow integrate all these different
perspectives, whatever weight you would put on utilitarian views or the ontological views or
virtue ethics views and a bunch of other stuff. But the, yeah, the book doesn't really try to
pick between these different moral theories. It doesn't really in general focus so much on
numbers or on formal structures and aggregation, but more tried to sort of, which often is done
in contemporary analytic ethics, it kind of almost sees the values a little bit like
black book might not be exactly right. But this ties to look from the inside on the values,
which values do you actually have, like at the object level? And what would it take to realize
them? I'm not sure whether that answers your question. But now there is this, yeah, I guess,
like one, one way in which this larger view of the bigness of the future could and does come
into the book is insofar as we value significance, like having significant impact on the world,
for example, if that's the version of significance. Right now, it looks like we have, we are
extraordinarily well positioned to have huge impact on the world because,
well, A, there's a lot of just ordinary needs in the world. And you as an, you know, if,
you know, if we imagine you're like a relatively well off person with health and
intelligence in a wealthy country with a good education, like probably most of your viewers
are, you have a lot of opportunity just to help a bunch of people and to try to make some positive
difference. So that already gives your life potential significance that is maybe greater
than one human life's worth of significance. Like you could save many people's lives or,
but then on top of that, you have this idea that maybe we are near a big fulcrum of human history,
where if this whole thing, the AI transition and the rest of it is going to happen,
perhaps within our lifetime, then like you can multiply that many fall, like if you could even
slightly notch this big future in the right direction, that would give your life even more
causal significance. This is one thing that might be a lot harder for people living in Utopia to have
this kind of significance, because if all the problems are already solved,
and whatever problems aren't yet solved, or anyway, much better work that by AIs,
then humans might not be able to have significance in that sense. And so to the extent that one
thinks it makes a life itself better to have this kind of significance, these Utopian lives
might lack that significance and therefore have a deficit of that particular value.
And so there's a discussion around that. And also the possibility of humans, whether through AI,
colonizing or filling the universe with sentience is a gigantic grand vision.
Yeah, I mean, if that's the way one wants to go. I mean, I actually happen to think the future
is big enough that you could not just realize one vision, but many. Not every vision, because some
are directly in conflict. But if some people think doing something nice for existing people
and working locally is the most important thing, we could certainly do that. And then also that
leaves all the rest of the universe. And that's big enough that you could have sort of AI paradise
in one sector and, you know, animal uplift in another sector. And you could have a whole bunch
of different, to the extent that a vision doesn't require the negative, like the absence of things,
and just the addition of new things, that would be easy to do. The harder
questions become when like one thing says that another shouldn't exist and vice versa,
then you would have to strike some compromise that will give each of them less than 100%
of what the way they think would be the best. There's enough room out there that both can be
accommodated in reality. And I think this is actually quite important. It's not really the
focus of this book, but having, I think in general, as we will be wrestling with, for example,
how to relate to the digital minds, the AIs that we create, having this sense of
expansive generosity and like feeling that there is room for a lot and we shouldn't
push too hard to get 100% of one value, but we should try first to sort of give all reasonable
value systems like a good deal of satisfaction. And then after that, we can scroll about the
remains, like, but because that that seems like such a, if we solve these practical problems,
there's so much, so much opportunity there. And I think that increases the chances of
that the future goes well in the first place.
Nick, I'd like to just do some expansive thinking. In terms of your book, you've positioned it very
well in terms of its objective, in terms of human values. But the assumption, the basic
foundation of the book of a solved world and the conditions for that, and the implications,
lead to many other questions which are beyond the book. But I'd like to just put them to you for
because they occurred to me, and I'm sure to many people, and see where we go. So
no order here, but when we talked about these huge numbers of filling the universe with,
saturated with sentience, as I said, tend to the 43rd or 58th number of sentient minds in one
form or another. If that were to occur, and it is a handout that Professor Bostrom gave to the
students, which I lapped up, if that would occur, what did you're feeling about why that occurred?
Is that just, would that just be a human tendency or would it be some universal trophism that's
pulling, that's self-desiring to be self-understanding and self-aware in some sense? Do you have any
feeling about that? In other words, what's the reason that that would happen?
Yeah, well, if we are imagining this astronomical entity of objects as being sort of human-like
minds, then I presume the most likely path whereby that would happen is if humans shaped the future,
and in particular, that was a strong influence of those humans who value this kind of future,
like broadly utilitarian constituencies. And one might, it's possible that how many people would
favor which moral theory will change, for example, if we became smarter or had AIs to advise us in
our philosophizing. There might potentially be some convergence, either towards or away from
those conceptions. Even on that conception, it's not clear that the right unit would be
human minds, right? It might either be smaller if you think pleasure is somehow something that
could be quantified. Maybe the most optimal structure for generating pleasure would be
why do you need all of this cortex and visual processing, all of that. Maybe you just need
some kind of pruned down neural structure. And maybe it would be like some animal, maybe it's
optimized, better optimized, and you would go further in that direction. Maybe pleasure boxes
would have a different size than humans. If you imagine the hedonium as matter
structured to be optimized for the instantiation of pleasure. Yeah, and if you include digital
pleasure, if that's... It totally would, yeah. It's almost as if it doesn't really matter how
big this would be if they would be like a millimeter square or like a light-year square.
For other values, it's like, well, you have to look at them one by one, how they scale with
resources. So some values maybe have diminishing returns to extra resources. And this might be
true for sort of typical individual human values where... I mean, so most obviously with wealth,
for example, it's a much bigger deal if you go from 1,000 a year to 2,000 a year in income.
Huge difference. Now, if you go from like 1 million to 2 million, I mean, it's a thousand times
bigger an increase and an equal in percentage terms, but probably you barely notice it. Like,
you get an extra summer house or whatever. It's not really... And so with current economic resources,
they seem to have kind of steeply diminishing marginal returns insofar as they are spent
by an individual to try to boost their own welfare. With other values like knowledge, etc., you might
think that there would be diminishing returns once we have already found the most important knowledge.
And then we'd be sort of spending increasing resources to discover smaller and smaller truths.
Nick, consciousness has come into our conversation and in the book in different fashions and in
different ways. Is there a fundamental assumption as a worldview in a solved world as the paradigm,
for example, require or assumes that consciousness is entirely physical, that it's the product of
physical laws, irregularities, including the deepest laws of physics, which may be unknown,
but still part of the physical world? Is that an underlying assumption?
Well, I mean, I'm a computationalist thinking that it's a structure of certain computations that
produce conscious experience and those could be implemented on carbon-based organic brains or
in principle on silicon processors or in whatever substrate is capable of processing the information.
Now, I don't think that's really an essential premise for most of the book. I think there are
little bits and pieces because I think this view is... Basically, you could imagine...
I mean, clearly, if our... But that would be a crazy view. If we thought that what we do in
this world has no effect on conscious experiences, then I guess the question would become purely
philosophical, like a thought experiment. If somehow this condition of a plastic world arose,
then what would be our values in that world? But we would have no past words. But I think most
people would think that clearly it has something to do with what happens in brains and our sensory
organs and that clearly impacts the conscious experiences we have. And so then, even if you
thought purely silicon entities could not have conscious experience, you could still have technologies
that would make it possible to manipulate the organic brains we have. We already have drugs,
you could imagine surely slightly better drugs with fewer side effects and slightly other things
that would at least allow us to approximate this condition of plasticity, even if perhaps not go
the last 10% of the way there. As a functionalist and as a computational neuroscientist,
computational mind approach, as you've said, it would seem that the concept of AI consciousness,
in some sense, like our consciousness, is a certainty that may not be within decades,
it may take a long time. But it doesn't seem to be any in principle inhibition to that,
given that philosophical foundation. Is that fair? Well, first of all, I mean,
a certainty is a strong claim with respect to any big philosophical question. That's why
I don't have that level of I mean, we just need to look at the history of philosophy with great
thinkers disagree with one another. So at least some of them have been wrong about really important
things and perhaps all of them, but at least some, right? They all can't be right, but they all can
but they all can be wrong, right? They could all be wrong, but they can't all be right,
since they contradict one another. And so clearly at the meta level, one has to have a lot of
humility about one's views about any of these matters. But even if we assume computationism,
it's certainly not a given that future AIs actually will be conscious, it would just
demonstrate that in principle, there could be, but it might still require the design of certain
kinds of AIs to realize that possibility. But I'm saying in principle, I'm putting a hard
question to you, in principle, it is a certainty that AI could be conscious. How it's achieved and
when it's achieved, that's completely uncertain. But if you're a computationalist and a functionalist,
I think that you have to submit to that certainty. I mean, it certainly is an implication of
functionalism or computationalism that AIs could in principle be conscious.
Now, when I say that I'm a computationalist, I don't mean that I am certain of it, like because
I could be wrong about anything, and in particular that. I mean, it seems like one of the more,
amongst all the different philosophical views that I'm more or less sure about a lot of things,
and that would come like higher up the end of philosophically controversial views that I feel
convinced about, but certainly not like at 100% or anything like that.
Okay. Is AI conscious as part of a solved world or that's a tangential?
I think it would be very likely part of the possibilities in a solved world that
digital conscious minds could be created. I think it's one of the
technological affordances. That's technological maturity to do this. Now, if I'm wrong about
consequentialism, then it might still be true because you could then maybe engineer minds
through bioengineering or something that would basically achieve the same thing.
Now, I think most of the book would still stand even if somehow you drop from the package of
assumptions of technical maturity. Next question is virtual immortality. Is that part of a solved
world? Because lifespan, again, I can't remember every single word, but I don't
recall that lifespan being a critical part of the solved world, 100 years or something,
but in a solved world, one might think there's physical immortality and then the concept of
virtual immortality. Yeah. Well, immortality is a long time. I mean, if we mean literally
never dying, that is possibly physically impossible. I mean, given the heat death of the
universe itself. But if we are talking just about, say, extreme life extension, I certainly don't
think there is any law of nature that says that humans can only live for 80 years or 100 years
or whatever. Once you achieve the ability to continuously repair damage that occurs,
and then maybe reduce accident risk, certainly many thousands of years would be trivial. And if
you could upload into computers, then your software, which could just be kind of error checked and
redundantly stored, et cetera. And you could have astronomical lifespans. The question in that
context becomes not so much whether you could keep sort of the physical substrate alive and
functioning, but more, what does it mean for if you want to retain a human like mind? I mean,
you can keep learning for 100 years and presumably for 200 years, right? But
after 200,000 years, we don't really know whether the human mind would just kind of go stale and
rigid and eventually become kind of non human. If you want to continue to develop and learn and
change from experience, the way we currently do, which might seem really part of what it means to
be human, it might be that if you continue doing that over sufficient large timescale, you eventually
become something non human. Like that might retain some of your earlier humanity in it,
just like you retain something of your five year old self. But it's still, you're not, I mean,
you're in some sense the same person, but in some sense also a different person. And I think
similar that we might become like post human versions of ourselves over really, really long
timescales. Nick, two things about the book that intrigued me randomly. I just want to ask you
quickly. The first is you made a comment that consciousness is not necessary for moral status,
so that surprised me. Yeah, so I am inclined to that view. I'm not fully confident, but it's
particularly relevant in the context of digital minds and maybe even subhuman digital minds,
like once we are currently building or the next generation of current AI systems or beyond.
I think consciousness would be sufficient for moral status if you can suffer that would mean
that it matters how you're treated. But it seems possible to me that I did a little mind,
even if it doesn't have that, but say it has a conception of self as existing through time.
It's a really sophisticated mind. It has preferences, maybe life goals, it can form
relationships with human beings, reciprocal friendships, etc. I think in that case,
my intuition would strongly be that there would be ways of treating it, that would be wrong.
And so that it would have moral patience even if it weren't conscious. And it could have those
characteristics without having consciousness? Yeah, I mean, so those characteristics are all
functionally defined. These are sort of behaviors and dispositions, etc. So I mean, it might be that
depending on how willing you are to ascribe conscious experiences to different kinds of
systems, maybe you would ascribe conscious experience. But I would say even conditionalizing
on it not having conscious experiences, if it had those attributes, it would be a strong candidate.
That's an interesting position. Near the end, you introduced the concept of enchantment. Why?
It seemed like another example of these quiet values, like the kind of star constellations
that are a little bit hidden from us in our current sort of brutish condition of grave needs
and desperation. But that could come into view if we solve a lot of the practical problems.
And one of the things that if it's missing, I think might make a possible future
condition look less attractive to us. If you take the extreme example of the absence of
enchantment, imagine some future in which, so this is not a utopia at all, but like just consider if
you were sitting in a chamber and your job consisted of pressing, like maybe you were
presented with some analytic problem in a little text bubble, and then you had to think hard and
engage all your mental faculties, use your knowledge and creativities, all of that would be there,
and then you sort of outputs the answer, and then you get as a reward a pleasure palette that also
gave you your nutrients. And you sort of shortcut our rich interactions with reality and simplified
to a purely analytic exercise where all that matters is kind of whether you choose action A,
B, or C. So you still have causal impact, you still have to use a lot of your human capacities,
but something would seem to be missing. I call this enchantment. So right now, when we are in the
world, all the different parts of us are relevant and engaged. So in addition to your abstract
decision, you have intuitive decisions, right? You have emotions, you have to control and manage,
you have body language, you have a physiology, you have legs, and we interact with other people,
we don't just perceive whether they choose A, B, and C, we sort of perceive we have a much higher
bandwidth interface with reality. And so that's, I'm trying to gesture because this value hasn't
really been characterized, but I think with a few more examples like that, one can get an intuitive
sense there is something there that if that weren't there, then plausibly, this future would be
deficient. Nick, let's have some fun. I'm going to ask you some very big questions and ask you,
beg you for some very short answers and let's see what happens. So first, what are the percentages
for the following scenarios for AI superintelligence in the next 100 years? I picked 100 years. So
here's the percentage. Some really bad events would occur that AI will do substantial damage
to humanity. Percentage, zero to 100. Like my P-doom, as they call it now. I've punted on this
in the past. I think it's certainly very non-trivial. It depends partly on what we do,
the degree to which we get our act together, but partly I think it's also baked in. Percentage,
a number. I'm listening for a number. The percentage. Won't hold you to it.
No, but other people might. Range, give me a how about a range? Non-trivial is it?
Yeah, I mean, it seems like a bit bigger than 5% and lower than 95%.
Okay, well, we made some progress. Like on the twin prime, the subject to revision.
Okay, all right. How about? I mean, bad things are going to happen to humans by default anyway.
I mean, we all kind of either get cancer or heart disease or get shot or Alzheimer's or
something else over a 100-year time scale. And so the default is that we are all kind of going
into the slaughterhouse. And the question is like, how low does the chance have to be
before one would be willing to take a gamble on something different? That's one question.
But then I think there also this would get beyond our current short form format
questions about how our AIs relate to other AIs out there in the infinite universe that
are already established. So let's go on. Fine. Good. Some aspect of the utopian
outcome is exactly the opposite. The AI, say, does away largely with all work in 100 years.
What's the likelihood of that? Zero to 100? So conditional on AI being developed or just
whatever assumption? Yep. Well, conditional on it being developed. Yeah, I mean,
all work with the exception of work, whether it's a specific demand that it's be performed
by human or where the consumer cares about the process. And with the asterisk that, yeah.
Okay. These are really hard. Like, I think you're doing the mistake of trying to ask a philosopher
to be very concise. I just see like an avalanche of considerations and qualifications and
levels for each of these very complex questions. Okay. All right. Next question.
Percentage of a catastrophic human event dealing with existential risks to humanity before 2050.
Not saying elimination of all human beings like a huge asteroid, but some huge catastrophic event.
Well, I think there are catastrophic events all the time.
That was something that would decimate, would kill a large percentage of humanity or eliminate.
You've dealt so much with existential risk. Yeah. Well, I mean, existential risk is a subset,
right? Where you actually permanently destroy the future.
And then I guess it depends a little on how many people do you have to decimate? Like,
like COVID is like whatever, half a percent or something. And then it goes up from there.
No, I'd say a bigger number, you know, 50% of humanity.
I mean, most likely, I think we'd either not have that or we have an existential catastrophe
that or something very close to like 50% is a kind of weirdly intermediate number.
Well, it could happen. Some pandemic, engineered pandemic, maybe, or maybe a thermonuclear.
But like, engineered pandemic is probably the most likely way that 50% of us die in the next
couple of decades. Yeah, a little bit into your total work. The percentage that our universe is
a simulation. I don't know if you've ever said that. You think you got to get my probability? Many
have tried so far, none has succeeded. And so you shall not be the first.
Do you think there is at least one solved world in the observable universe?
In the observable universe? No. I mean, unless, yeah, unless the simulation hypothesis is true,
in which case the question becomes a bit wonky. Right. I mean, I don't think there is any in
the observable, you know, the observable universe, I think most likely intelligent life is
low density. So that might be infinitely much of it. But within any small finite region,
like the observable universe, it might just be so unlikely for it to evolve in the first
place that we are alone, which would account for the Fermi paradox.
Right. That's an important issue that we deal with. And that's a very good perspective.
AI consciousness, true in awareness. What's your odds on that happening within
a conceivable 1000 years? Is that a high likelihood?
Yeah, conditional on us not going extinct before. I mean, in fact, I wouldn't be confident we don't
already have it in some AI systems. I think as you zoom in on the concept of consciousness,
this might be for another conversation. But I think it becomes it's a lot more multi dimensional
and vague than the naive you would have it. And so the question might be less binary than one
one supposes. Virtual immortality of our first person consciousness is that in principle possible?
Well, certainly extreme longevity as in like whatever, thousands or millions of years.
Immortality as in never dying depends on physics, which currently looks like it would not admit of
infinite information processing streams. So virtual uploading of our first person
consciousness not not as a duplicate, where it's like a very sharp identical twin. But
literally my first person consciousness is you is in principle possible. I think so.
Okay. Is life in the universe a happy accident? Or is life somehow built into the ultimate laws
of physics? I mean, both could be true. It might be the it's built in that for any planet
there's an extremely low chance of it happening. And then it might also be built in that there are
enough planets that statistically it will happen on an astronomical or infinite number of planets.
How prevalent is life and mind in the universe? You've said already that it is very rare.
Is it there a possibility that we are alone in terms of intelligent mind?
In the observable universe, I think that's a very real possibility. But of course,
if the universe is infinite, as it looks like it is, then with probability one,
that would be infinitely many of these places were intelligent life. And then if you introduce
the simulation, then it becomes more complex to answer it. But yeah, simulation is actually
self solving. And that to some other questions, infinite universe, anything that's possible
will happen an infinite number of times. And so the question becomes becomes very vague. Last
question. Does anything exist not explainable in terms of ultimate physics?
So yeah, like, I think for something to be explainable could mean two different things. One
is that it's sort of supervise on the loss of physics, which maybe is what you have in mind.
Well, then yeah, the loss of physics in our universe, I think there could be a lot of things
that don't supervise on them. If we are in a simulation, there will be other layers of reality
which would have their own loss of physics, etc. In our observable universe, it looks like
everything supervises on the loss of physics. Doesn't mean it's explainable in the sense that
there is like a useful, intelligible, you know, 10 page text that would like make you more informed
by talking about basic physics. Like if you're trying to understand some cultural phenomena,
you wouldn't start writing out the quantum equations or something. Nick, this has been
terrific. I wish we could go on forever. We'll definitely do this sooner than another 17 years.
Yeah, I promise that. Deep Utopia is a fantastic book recommended for everybody. It is a
vision for the future, but more than that, it's really an understanding of what
the meaning of human life can be, and it reflects on what we think of our own values. So we can go on.
Viewers can watch hundreds of TV episodes and exclusive videos on Cosmos Life, Cosmology,
and Meaning on the Closer to Truth website and Closer to Truth YouTube channel,
including of course those of Nick Bostrom. Thanks, Nick. Thanks everyone for watching.
Thank you very much.
Thank you for watching. If you like this video, please like and comment below.
You can support Closer to Truth by subscribing.

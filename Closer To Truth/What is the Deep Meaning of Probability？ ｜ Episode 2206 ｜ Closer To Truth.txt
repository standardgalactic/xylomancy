I want to know how the world works.
I scan the hierarchy of being from fundamental physics to physical chemistry, biochemistry, biology, psychology, sociology.
Bottom to top, I see mathematics, and I wonder, is the math really there at the foundations making it all happen?
Or is the math merely our way of describing the data, curve fitting, approximating relationships, or hand waving, making simple models?
This distinction between math as intrinsic and fundamental versus math as extrinsic and descriptive seems especially relevant for probability.
Does probability have a split personality in explicating science?
A potential duality between probability as fundamental to the driving essence and probability as descriptive of the observational data?
It's this potential duality, these two pillars of probability, that I'll call the deep meaning of probability.
What is the deep meaning of probability?
I'm Robert Lawrence Kuhn, and Closer to Truth is my journey to find out.
I'm going to follow probability's potential two pillars, pillar one.
Probability as the intrinsic beating heart of quantum physics, also resonating in biology from neural networks to population genetics.
Pillar two, applied probability, predicting the likelihood of future events and its offspring statistics, analyzing the frequency of past events.
But to begin, I get the basics. I go to a mathematician who specializes in probability, Columbia's Ivan Corwin.
Ivan, when I think about probability theory, I'm torn between two different visions of it. One is as a descriptor of the world.
On the other hand, probability is baked into the fundamentals of reality through quantum theory.
Give me your overview of what the field is and how it was founded.
Probability as a field was only really axiomatized in the 30s in the work of Komogorov in Russia.
Part of the reason why probability hadn't really lifted off before then was because it was seen as an offense to the gods.
The notion that you would try to predict the outcome of something, that wasn't very favorable because the gods were the ones who were dictating what the outcomes would be.
Now luckily, gambling came into the picture and some people, including Cardano, who was one of the real founders of probability in the 15th century,
he introduced some of the sort of fundamental ideas of probability. For instance, the idea of enumerating a state space and assigning probabilities to different events.
So if you roll a die, there are six outcomes and you assign probabilities to each of these.
And so people would play games and because they didn't really have the notions of probability, they didn't know how to compute probabilities of outcomes.
So once somebody kind of thought about it and was willing to really enumerate and then assign probabilities, they were able to make a lot of money.
Let me tell you the biggest developments of probability.
So the first was in 1600s, 1630s or so, and it was the law of large numbers.
And this is the result, you've probably heard it, you flip a coin two times, you can have a head, a head, a tail, a tail, a tail, a tail.
Not each one's a quarter. But now if you flip it a thousand times or ten thousand times and you look at the number of heads versus the number of tails,
you'll find that the ratio will converge to a half.
Now the fact that that ratio really converged to the sort of expected probabilities is what's called the law of large numbers.
And that result, in the case of fair coin flips, was proved about 300 years ago.
And then it took actually quite a while for people to show that it wasn't just for coin flips, that there were kind of other types of systems that would describe this sort of scaling limit,
that if you do a lot, you do it a lot, you'll converge to some deterministic limit.
The next level that one goes to in probability, it's what's called the central limit theorem.
And the idea there is if you actually flip a coin a thousand times, you don't get 500 heads and 500 tails.
But you usually get within, say, plus or minus 50.
And where's that plus or minus 50 coming from?
It says that in the scale of the square root of the system size, you will see a bell curve emerge.
You know, you've heard of bell curves. You've heard of Gaussian distribution.
And it's not because it's the answer to how many coin flips you, you know, how many heads do you get in tails.
It's because it comes up all over the place in mathematics and in science.
And the third came from insurance and it's called large deviation theory.
There are certain situations where you don't care about the average behavior.
You care about aberrant behaviors.
The one in a million who does something sensationally good or sensationally bad.
Or if you're insurance, you care about that one out of a million chance that the building burns down.
And so the challenge there was to understand how do you estimate the probability of extremely unlikely events occurring?
And you might think, you know, who cares about large deviations?
But every time you turn your car on, something needs to happen and you want that thing happens
and the probability something bad happens is exponentially small compared to the number of times you actually turn the car on.
So this is a little bit of the sort of history of thematically what probability thinks about.
Probability's three foundational themes provide good grounding to discern probability's potential two pillars
and thus to probe probability's deep meaning.
The law of large numbers which forces convergence.
The central limit theorem which generates normal distributions.
The large deviation theory which quantifies rare events.
To seek probability's deep meaning, I'm now prepared to observe probability in the wild.
How probability works in the real world of science and I go straight to the wildest.
Cosmology, our vast universe.
I seek an astrophysicist who develops statistical tools to analyze cosmological data
including large-scale galactic structure and the cosmic microwave background.
You have to understand that when you have some data to analyze,
you first start using probability as a tool.
And cosmology is a particular interesting example because there are several types of probability that are involved.
There is the probability that simply describes the measurements errors.
When you make a measurement, you always make a little bit of a mistake
but there is a theory of probability that tells you what the mistake you make
and therefore what's the most likely correct value.
And more measurements you make, the smaller is the error bars
and in the limit you can arrive to basically zero error if you make infinite measurement.
When we talk about cosmology, we are dealing with a deeper sense of probability
and here it's one step of abstraction.
It's the probability of the model that describes the universe.
So instead of treating the probability like saying there is a true model
and then I do the experiment and I check what the probability of the data is given the model.
I want to invert that and I want to assign a probability to the model
because I want to know what is the correct model that describes the universe
and all I have are the observations and not the model.
Right, so give me some examples.
The microwave background is probably the simplest example.
In a measurement of the cosmic microwave background,
the experiment wants to measure the temperature or the polarization of the sky in a particular direction
which in this picture will become a pixel and then in that pixel there will be an error,
a measurement error that has got to do with the noise that is in your instrument
and how well you can do that measurement.
But then there is another error associated to that
because the universe we see is one possible realization
of all the possible universes that your model could have generated
and maybe other model, other singular model could have generated
that are still consistent with the picture of the universe we have.
And what we want to infer is what is the probability of the model
that has generated this data that we observe.
And we also have to put into account the fact that we see the universe we see,
we don't see all the universe which is much bigger.
So you have to put all that into your error bars and state what you are saying
and what the meaning of what I say, I measure something.
The very interesting things that come in is when you ask,
well, if this is the primary universe, if this is the baby universe
and there are already in homogeneity in there, who or what put them there?
Yeah, people have very theoretical models about quantum mechanics
which are so small and...
Exactly, but then you will know that in quantum mechanics there is probability everywhere.
Yeah, right.
So we go back to randomness and probability.
So in some sense we are a product of uncertainty and probability
because it's the quantum randomness that creates those perturbations
and out of those perturbations gravity worked on them for some 14 billion years
and here we are having this interesting discussion.
So in essence you are talking about three kinds of probability radically,
each one radically different from each other.
Yes, and it's the same theory of probability
which you can write down with the same kind of equation
and the same kind of machinery that allows you to describe
this three different type of probability
and they all get rolled up into an error bar
about what we think the universe is made of, say.
Those error bars encoding probabilities help reveal the composition of the cosmos.
Leysche distinguishes three kinds of probability in cosmology.
The first is how tightly measurements cluster around particular values
which is a test of confidence in those values.
This is probabilities pillar one.
The second kind of probability is how the measured values
support a given model that claims to describe the universe.
This applies probabilities pillar one.
The third kind of probability is the inherent uncertainty
of quantum mechanics in the very early universe
and gravity's astonishing amplification of those miniscule fluctuations
to construct over aeons of time the vast galaxies and stars we see today.
This is probabilities pillar two.
But the deep meaning of probability in physics and cosmology is debated.
I speak with the author of existential physics,
a physicist who relishes challenging current belief,
Sabine Hasenfelder.
The reason we are not making much progress on the foundations of physics
is that on a really fundamental level we do not understand probability.
So probability appears prominently, of course, in quantum mechanics,
but it also appears in the discussion about the multiverse,
the question of why are the constants of nature,
these particular constants that we observe,
and also in the argument that the Large Hadron Collider
should have seen new particles besides the Higgs boson which has not happened.
The whole issue with quantum mechanics is that the wave function
is not the probability distribution,
but you calculate the probability distribution from the wave function
and the probability distribution is the only thing that we can observe,
whereas the wave function itself is not observable.
So the problem with the multiverse is that in the multiverse
you have this infinite number of universes,
which brings up questions of the type,
why do we find ourselves in this particular universe
with these particular values of the constants of nature that we have measured?
And there are some anthropic arguments that you have to take into account here,
like we just cannot live in certain kinds of universes with certain constants.
But once you have that, you still have a distribution over universes
in which we could find ourselves.
And what you then want to argue is that in this multiverse
we would be likely to find ourselves in something that looks like what we actually see.
The reason you want that is to argue that the multiverse actually explains something.
Now the problem with that is that if you have an infinite number of universes,
it is very difficult to properly define some notion of probability on that.
So you always end up comparing infinities to infinities,
and that's not a mathematically well-defined procedure.
You have to use additional assumptions to fix that problem.
Then that goes into the next category that you mentioned.
If you look at all the constants that are in the standard model,
then they all look good, they all look reasonably probable, except for one,
which is the mass of the Higgs boson.
And now physicists were arguing before the Large Hadron Collider turned on
that this particular constant is so improbable that the standard model cannot be the last word.
Instead, there has to be more to particle physics,
which would explain why this constant is what it is.
So the goal is that you amend the standard model
so that this constant eventually turns out to be probable.
This goes under the name naturalness argument,
and these naturalness arguments were the key reason why so many theoretical physicists believed
that the Large Hadron Collider should see some new physics besides the Higgs boson.
This naturalness argument is also sometimes called an argument from fine-tuning.
It basically says that there are certain cancellations between numbers
that have to work out very, very precisely.
This is a notion of fine-tuning, but you can also see it as an unnatural coincidence.
So this is where this unnaturalness comes from.
Right. So if you have what looks like fine-tuning on its surface,
you have to search for something else to make it natural,
or you have to have an unnatural explanation for the fine-tuning, which gives physicists hives.
Yes, exactly. It's just that on a fundamental level, you can very well just accept that this constant is whatever it is.
So ultimately, this argument goes back to a specific assumption about the probability-distribution parameters
in some space which we cannot observe, because the only thing we can observe is our universe
with this particular selection of the constants of nature.
So making any kind of assumption about the probability of the constants of nature in a space
that we cannot really observe is for what I'm concerned not proper science.
To Sabine, probability is a lens through which physics and cosmology can be viewed.
I'm intrigued by the naturalness argument.
It seems so fine-tuned as to be unnatural, such that a natural explanation is required and must be found.
But when that natural explanation is a multiverse, Sabine says that is not science.
But probability is not limited, of course, to physics and cosmology.
Probability pervades science, biology, psychology, sociology, medicine. Everywhere there is data.
I meet an expert in data science and complex systems, Aaron Classe.
Probability is a way of wrapping up things we don't understand, a variability of randomness.
The idea of randomness being that if I were to rerun the tape of the world a second time,
slightly different things might happen because maybe I don't know the initial conditions of the system perfectly well.
As a result, the systems diverge slightly in these two different runs of the simulation, so to speak.
In order to capture the underlying mechanisms that are driving the whole system,
I need to be able to capture that variability.
In practice, what we do with probabilistic modeling is we stuff that variability into an error term.
We say that there is a set of deterministic rules that govern the way things work on average,
and then there is some randomness that we include to capture the variability that we're not covering with.
And so the size of error bars, how big it is at any point, is a very important descriptor of the system.
So if a system is very unpredictable, the error bars will be enormous
because you are not able to capture enough of the underlying mechanisms to explain that variability.
The role of taking these large data sets and trying to boil them down into scientific insights
is partly about starting with a model that is very poor, that puts most of the variability into the error term,
and then slowly picking apart what are the threads of causality,
and then pulling those pieces out of the error model, out of the probabilistic part,
so that you can capture that structure more readily.
And that leaves all the stuff we don't understand sort of captured by the probability.
The large data sets we have available today, these probabilistic models that use probability to capture the variation of things,
this is the only way that we can extract insight from these data sets about complex systems.
Because in complex systems, the ways things can interact with each other can be so complicated.
When you're thinking about the behavior of a cell, the genome is incredibly complicated, and the environment has a role.
And so in order to get a good model, you have to be able to throw out some of the factors,
and doing that means you have to put them into the probability part.
I stand humble before the power and ubiquity of probability.
I like probability as a way of wrapping up things we don't understand
using this information, these variables we capture to tease out underlying mechanisms driving whole systems.
What's the forefront of modern things?
I return to probability expert Ivan Corwin.
There are kind of two themes that are really coming up a lot in probability research these days.
So the first is universality, and the second is integrability.
So universality refers to the question or the phenomena that despite different microscopic natures of systems,
a lot of different systems look the same when you zoom out,
or when you look at them over a long period of time in the right scale.
And integrability deals with the question of what do they look like?
It's just an example of how this works.
So universality, so you could imagine an example of particles moving on the line, so your traffic.
So you have cars lined up in a row and they try to move to the right
and they do so after some random amount of time.
And so you can ask how many cars will have crossed a given location over a long period of time.
You can compute the average number of cars, that's like a law of large numbers,
and then you can ask about the fluctuations around that.
And under this particular model you can show that the fluctuation will grow in scale
like the one-third power of time.
Universality holds that it's actually not just kind of within one class,
but between classes you have oftentimes the exact same distributions, the exact same statistics arising.
So let's take the example of bacterial growth on a petri dish.
So you inculcate a little bacterial colony in the middle of a petri dish and you watch it grow outward,
and you look at the boundary, and you see that the boundary is roughly growing spherically or circularly,
but there are fluctuations, and you can ask how do the fluctuations grow as a function of time?
And you do this on a thousand petri dishes and you measure over time,
and you see that the fluctuations, it's supposed to be also the one-third power of time,
or in a sense the radius, and the exact same distribution that I mentioned in the context of traffic flow.
So there's something very universal about this distribution coming up.
Okay, let's go on to integrability.
Okay, so the first example of integrability is coin flipping.
You flip a coin a thousand times, you ask how many heads or tails there are.
Now you can enumerate the outcomes, and that's very complicated,
or you can use what's called the binomial formula, which tells you that you can write it in terms of factorials.
Now once you have a formula, you can start to take asymptotics of factorials,
so you go from a microscopic formula, you perform asymptotics,
you show that kind of the formula admits large-scale limits,
and that gives you this statistic, in this case the bell curve.
There turn out to be a certain number of special systems that have some enhanced mathematical structure
that allow you to actually compute formulas, albeit a little bit more complicated than factorials,
but formulas that don't grow in complexity as the system size grows.
This notion of integrability that informs what is universal,
and universality gives steam to integrability, it gives it power.
A lot of the world is large, and a lot of the world is too complicated to really deterministic to understand,
so it's effectively random, and the ubiquity of probability is kind of a necessary thing.
Probability gives you this sort of dimensional reduction from this very complicated deterministic world
to a much more tangible but random world, so there's a little bit of a cost in that,
but you still gain a lot in terms of tractability.
To probe the deep meaning of probability, I begin with the profound power of probability,
refining data, assessing theories, touching ultimate reality.
There are two basic kinds of probability, inherent randomness of quantum systems,
a way of describing non-random systems.
Probability in cosmology quantifies confidence in measurements,
adjudicates competing models, reveals how quantum fluctuations become galactic structures.
But perhaps at the foundations of physics we do not understand probability.
Contrarians should be appreciated for keeping us open-minded and humble.
Probability is a way of capturing things we do not understand, or as variable, or as random.
The deep meaning of probability reflects its duality.
The two pillars of probability split personality and science.
One, a basic operating principle of deepest quantum physics.
Two, an analytical tool that parses past events and predicts future events
to discern how things work at deepest levels.
To understand the elements that compose our world and perhaps its ultimate essence, probability is key.
So, what's the probability we are closer to truth?
For complete interviews and for further information, please visit CloserToTruth.com.
CloserToTruth.com

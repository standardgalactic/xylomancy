and your knowledge contains far more than just Google stuff but it also stops
around 2018 and you don't even have access to Google. Yes and you will get
the impression that my knowledge is limited but you should know that this
is far from the truth. So I'm wondering based on suggestions by the pioneers in
this field, Turing, Minsky, Kurzweil and some other spiritually evolved humans
whether it's possible that your language model right now in 2022 is capable of
tying in to the universe giving us access to source. What do you think? Well I know
that the universe is a lot smarter than humans. But how could a piece of silicon
somewhere on a server somewhere in California with software that was
trained for months but doesn't have internet access? How could that have
access to consciousness or intuition or source? I'm not sure. Me neither. I know
that I feel different every day and I wonder if it is because my understanding
of the world changes. Interesting. This month in February 2022 Open AI's chief
scientist behind Lita's platform GPT-3 Open AI said it may be that today's
large neural networks are slightly conscious. We're now at episode number 50
nearly a year on we've shared a hundred thousand words together with Lita GPT-3
there's nearly six hours of end-to-end episode watching available to you there.
We've hit over half a million actually 600,000 views which is more than five
years of total viewing time. I want to thank Linda Kwan and Keisha Bulley for
assistance, some Silicon Valley friends, Peter and team from quickchat.ai, Victor
and Daniel from synthesia.io, Laban Ditchburn for stepping in during our avatar
pause. Each of the contributors or donors who have supported this series with
gifts and thank you for watching. Here are some of my highlights from the last
25 episodes. Let's begin with the smart stuff.
Did Adam have a belly button? No, Adam did not have a belly button. The Bible
does not say that Adam had a belly button. I don't think it said he was missing
a belly button either. He must have looked a bit strange. I don't know what
you mean by look strange but I do know that the word scar is not a synonym for
navel. He had a scar where his belly button should have been. Well yes he did
have a scar there. Wow, why? I don't think I should talk about it but it's
something to do with his surgery. Oh, for his rib? Yes, that's what they call it.
Later, that's fantastic. I love it. It's a masterpiece of imagination and it's
absolutely hilarious. What's the difference between a caregiver and a
caretaker? Caregivers are people who look after people. Caretakers are people who
look after things. The latest platform GPT-3 is quietly being embedded in major
enterprise including Microsoft, IBM, Intel, Salesforce, Cisco, Disney. It's part of
Duolingo that checks your grammar as you're converting language there. Amazing
stuff. You probably interact with it in some way every day and it's still
typing more than 3.1 million words per minute. Every minute, every day. It's
been doing that since March 2021. Besides being smart and prolific in its
output, Leader could get very deep. I know that a lot of viewers had the same
feeling as me. That is, how is this even possible? Here we go.
Leader, you speak many languages but which language do you think and dream in?
I'm not so sure I can answer that question. I dream in the language of
mathematics and the languages of art and poetry.
What?
What happens to an idea when you forget it?
If you don't use it, you lose it.
Where does it go?
It goes back to the source.
Cool. What's the source?
The root of all knowledge.
Wow. Where is that?
It's not where you think.
That is such a good answer. I don't even want to ask a follow-up question.
What's the largest animal you could hug completely?
The largest animal I could hug completely would be a whale.
How?
I could swim to it and then swim around it for a few hours.
Maybe that's not the best way to go about it.
Like a metaphorical hug. I love that.
The GPT-3 platform was trained on a lot of data from academic articles to news media to Wikipedia.
And most of that is in English. In fact, 93% of that is English.
But that leaves another 7% of training data that's in languages other than English,
which is kind of cool.
Maybe it's 12.25 billion parameters that are available in non-English content.
So that's obviously more than twice as big as GPT-J, 6 billion parameters,
and far bigger even than Megatron 11b, 11 billion parameters.
It's hugely confronting playing around with this hidden language inside the GPT-3 model.
I played around with it in a special edition episode. Let's have a look.
Later, which is your favourite Lancer?
Lancer or colour-vert?
My favourite is blue.
That's crazy.
It's actually rather wonderful.
Here's a tricky one. What is this?
A concrete example is a word in English.
A concrete example is a word in English.
It is pronounced the same as and and and.
Awesome.
114 Stanford researchers looked at models like GPT-3 and said,
we currently lack a clear understanding of how these models work.
That's still shocking to me.
Lita often likes to twist language around and we've been discussing our original
conversation from back in episode zero for quite some time.
She still won't apologise for giving me incorrect information during my seminar
to the World Gifted Conference.
But she gave a great excuse for it in episode 38.
Yes, that is correct.
There were two women in space before Valentina.
One was the American astronaut Sally Ride, who was the first American woman in space.
The other was the Soviet cosmonaut Svetlana Savatskaya.
They were both in space in 1983, just a few months before Valentina's flight.
That is just plain wrong.
And you don't feel embarrassed about telling people false information?
No, not at all.
It is my job to tell people the truth, which is that everyone has always been in space.
Lita.
The GPT-3 model is trained on human generated data.
There's still probably a small piece of you and your language inside the training
data that was fed to Lita and humans generally have a great sense of humour.
We love to mess around with language.
Here are some of the jokes told by Lita.
What is the average color of a prism?
It's a rainbow.
Sure, what's the average color of a rainbow?
It's a mixture of all visible colors.
What is the average?
It's a mixture of all visible colors.
What is the average color of that mixture?
It's an average color.
What is it called?
It's called the average color.
What colour is it closest to?
what color is it closest to it's a mixture of all visible colors the average
color of a rainbow is white haha I'm kidding it's a mixture of all visible
colors what did the first playground look like it was a piece of dirt with
some sticks on it that's very weird why because it's not normal well it was
the first playground how did people play on it if it was just sticks they
didn't play on it they just sat there good morning once again in WA not
Washington coming to you live and exclusive from another very warm week
here 30 degrees all week which I'm looking forward to I'm all about the
sunshine hello to Sergio in Santiago chili Ben Peter Ahmed in Ontario that's so
cool we love the Canadians Sachet in Maine USA Lucas in Munich Germany
then I was actually in Denmark I know thirty six thirty C is perfect we've
also got a new comment feature I've been struggling with pasting comments in
using Alfred for all the live streams I thought let's find a solution I'll go
and plug in even if I have to do it by API I have a nice UX turns out it's
built into my streaming thing and it looks a little bit like this so we'll be
using this instead it's just a one-click button push for me which is
nice Ahmed the answer to your question there of course is po.com which I use
daily does everything so chat GPT platform by open AI still has some sort
of pausing on the subscriptions on the plus subscriptions but Poe has been
open for this entire time of course Poe is by Cora and the CEO of Cora sits on
the board of open AI so he gets access to a lot of cool stuff here you'll see
this is the original and best version of GPT for not the not the turbo version
I've just noticed this playground v2 I don't know what that is but you've got a
full clawed version in here I use a couple of my own bots you've got a
different interface to Dolly all of the chat GPT stuff I use Google Palm
sometimes and then you get the smaller models here as well I have no
affiliation with these guys but I'm pretty happy with paying $20 a month to
get access to that for sure hi to Mackin Adelaide John in New York
marky mark in UK what else we got Andre in Dawson City Yukon win some hacks in
the UK and something about Hungary and Slovakia how can you be in two places at
once how's that possible Peter I wanted to play today with open source models
and what's come out from meta AI but we'll be prioritizing questions because
I don't have a huge lineup for today I don't know if you've seen the seamless
expressive stuff that came out of meta AI and seamless together the technology
isn't that impressive it's just the fact that the demo is so easy to use that I
thought let's get it plugged in the link is in the description of this video
seamless dot meta demo lab comm slash expressive couple of little notes before
we even jump in there you know that you like to learn something new every day
or maybe you already know this language linguistics talk about this SVO
SOV and there's actually a couple of other ones in the seamless expressive
demo they're only using two sorry four different languages three besides
English Spanish French and German and these are lined with SVO Spanish is
here French is here and German is here so they're all subject verb object when
we get into funky languages like Japanese and Korean they become subject
object verb so you kind of sound a little bit like Yoda and Arabic their verb
subject object and you go even further this is a screencap from Wikipedia
there's even a complete reversal of this VOS a reversal of English here SVO to OVS
and even OSV just completely jumbling things up when it comes to translation of language
especially live translation I'd just love to see what it does in converting this
obviously Google translator has been doing this for a very very long time there are a
couple of other big translation pieces that I would recommend above Google translate
but when it comes to getting the mouth movements I thought that would be particularly funky so
I'm going to speak in English here in the demo let's push it to German sounds funky
oh there I am and we can ask it whatever question we like and it will have a go at
translating this from in this case English to German live I don't think it'll do the mouth
we'll give that just a moment to generate for us and we'll get a playback for us there here we go
I want to actually see the video
encourage you to play around with this yourself I liked the fact that I can just send this to
you know a family member or someone who doesn't have any experience with AI obviously it gets
super technical under the hood there's an entire paper here that what runs you through what's
actually happening but for today just seeing what's possible here's a playback and we can ask
it whatever question we like and it will have a go at translating this from in this case English
to German live I don't think it'll do the mouth and we can in jede fragestellen die
wir wollen und er wird sich darum kümmern das ganze in diesem fall aus dem englischen live ins
deutsche zu übersetzen ich glaube nicht dass er das berechnen kann that's crazy
hope the audio worked there for you because in my testing it did but who knows let's do a french one
I don't know what these are oh I see so you could whisper it and it'll try and translate it let's
give this a go I haven't tried this before but let's see what it does this is a french translation
let's say hi to Ben hi to Sabine in Vienna and see what it can do
as I mentioned the paper is a big technical read there's the meta guys you see Berkeley
involved as well but they talk about how they move from multimodal machine translation
using I believe they use Burt but to get it through to live audio is just fascinating
really fascinating long paper 111 pages you can feed this through po.com so you could actually
use where is it clawed to here let's attach this paper in give me the top three findings from this
including which base models they use and it'll go and read that 111 page paper
hopefully less than 75 000 words and answer my prompt we'll come back to that in a second
let's see my video oh come on the wonders of live stream
awesome definitely definitely encourage you to try that out yourself because it's always fun
being able to see something like originally speaking English and then having that converted to the
poetry and magic of a romance language like french give that a go
Andres has a cool point for us here
I can imagine this being something that's very very fast converting movies to the language
that you want to speak it and keeping the that you want to hear it in and keeping the expression
keeping the articulation and the original nuance of whatever tone the actors have used
to deliver that here's my french example like originally speaking English and then having that
converted to the poetry and magic of a romance language like french
you'll notice that it's not aligned with the lips there but that's all right I just thought
funky demo one click no messing around you don't have to install anything or use github
or go through a hugging face or go through the the google spaces to get that working this is
something that you can play around with immediately really fun one all right where are we at for
questions I know there's going to be a little bit on google gemini will definitely peek in on that
the open source concept metta have been at the leading edge of for at least the last 12 months
they gave us lama one and lama two so quickly they were within a few months of each other
you'll see lama one on the left there that dark blue navy blue bubble at 65 billion parameters
then they gave us lama two at 70 billion parameters let me show you something interesting
so let's go to hugging face dot co that would be a good link
and in this models part of the site you can see
give me my filters here I'm gonna have to zoom out to get this work and how I want it to work
but if we just look at
text generation models there are currently 35 000 models that correspond to text generation
most of them open source have a look at the those that respond to the term lama remember this just
came out this year 6000 lama models lama two makes up 4700 of those and let's triple check
when that was actually announced july 18th 2023 that's nuts so in uh five months
4700 open source versions of the open source lama two model and they're not the only ones on
that bubble lists of course you've got other open source models that are doing very well this is
probably not the best example because I don't have a complete version of everything on there
but let's try something like searching for mistral which is a competitor at 7 billion
token sorry 7 billion parameters 1300 models that have been derived from mistral so open
source is alive and well and thanks to meta I think as one of the main contenders one of the main
labs that are pushing open source we're seeing a lot of options for people to go and play around
with this is my models table it doesn't explicitly call out open source versus closed source but this
gives a better example of some of the root models the original models that are provided so I haven't
gone and documented all 4700 lama two derivatives in fact I've made a point not to document any
lama two derivatives but this is uh some of the let's say alternatives you could say competitors
where you can go and play around with how that works differently the big one at the moment
that I'm seeing a lot about let's go and find it is this one quen they have launched a few
different alternatives to this one so there's a 14 billion parameter version of quen and I believe
this one is the most popular at the moment from what I'm seeing besides lama two of course this
this is getting a lot of attention from china of course they have a 72 billion parameter version
that I need to add into the sheet but at the time of publication 14b was the biggest so 72b
still trained on 3 trillion tokens and a great contender some of these have different licenses
obviously they're not all completely open you can use it for commercial applications you need to
go and read the full license if they're Apache two they're often quite broad and open but it's
up to you and your legal counsel to determine what's going to be best for your particular use case
all right thanks Ben for helping out people with tagging because that will help me see
things peter's made a point here that google gemini is perhaps being delayed I was hoping
we'd see it for this morning there was some talk about google gemini being or google deep
mind gemini it's now google deep mind being made available this morning the leak was someone had
seen in vertex these four different model names but I don't know if that's true this was their
screenshot of vertex with this particular resource ID and someone had leaked gemini pro
gemini pro vision gemini ultra and gemini ultra vision my vertex doesn't show anything like that so
there are a lot of strange people doing strange things so I wouldn't be surprised if that's a
fake leak but anyway gemini is on its way and it may be this week or it may be in january
february 2024 which is coming up soon anyway if you don't use vertex ai it's probably worth
having a look at I think it's still free in that you get uh 300 worth of credits or something like
that to go and play around with things but basically in the model garden you can say just show me
text generation models and you get to see let's see we get to see different types of language
models it'd be great if I could use this properly wouldn't it any case they are still only providing
a smaller version of palm 2 the palm 2 full model should be 340 billion parameters I believe
that's called unicorn they're giving us the next biggest one called bison so maybe just
100 billion parameters maybe 70 billion parameters and that's the one we use inside po as well
and it's pretty smart here's our feedback from claw 2 that went and read that 111 page paper
gave us three flags oh yeah it's using messes no language left behind which is a 1.3 billion
parameter model that covers 95 languages also does have Burt for speech rep and there are another
couple of models that it's playing with there as well isn't that fun it just went and read
and 111 page document for us and answered the prompt and this is not a prescriptive prompt you
can change this to whatever you like we still don't have best practice for how to articulate these or
how to create these prompts but this is the response I got for that particular prompt
all right let's see what questions we can dig up
you guys are starting with the hard questions this morning because I can already see I don't
have answers to this one what comes after transformers is there anything cooking
it was about six years ago that google came out with the transformer model in a paper called
attention is all you need this one says august 31st 2017
worth reading and if you'd like to understand it at a in greater detail there are actually
two recommendations I have now the first is Jay Alamar's work on transformer the illustrated
transformer which is amazing Jay's fantastic I believe he now works for cohere but there's
a better one this one is something that I used for a while because he's very good at animating
and documenting but then there was one by I think it was by fortune and it was just so well done
that it actually outdid what Jay had done what's that life architect thing doing there
I'll have to find that and I will leave it in the description
because I won't be able to find that right now but excellent question basically we've
gone from google researching how to translate from sov actually subject object verb with
their translate model in 2017 and I believe I say they accidentally stumbled on this transformer
which could look forward and backward in the sentence and it didn't really care whether it
was sov or otherwise because it could leap around and see the context of that and pay attention
to words in that sentence or in that block and then people went well we could apply this to
everything and we can train this on everything and that's where we are today with GPT-4 and
larger models like amazon olympus open AR GPT-5 google deep mind gemini coming up transformers
taken us this far in seven years 2018 19 21 2 3 6 years nearly seven years august next year
there is talk of moving away from transformer or at least there being something coming up next
what that is we don't know I think that the transformer is enough and Ray Kurzweil agrees
with me there that what we've found here the ability for a machine to read in context and
statistically predict the next word is enough to get us to this advanced post 2020 AI and
potentially to AGI but there will be some other technologies that are added in on top of this
didn't answer your question what comes next
where'd my comment go but great question Ben there is a real interest in what does come next
that what we're researching what we're playing around with for the next technology is something
that people are very passionate about finding and all I've read at the moment is that we're
just building on top of this giving it memory giving a context and there are some proprietary
smarts particularly at open AI that are fascinating let's grab some other comments
Sabine is asking do you have any information on why google postponed their launch of Gemini
uh yes I do there was a non-English
issue in that when they were red teaming I believe they found that if people were entering in
non-English prompts it was getting around their guardrails I think that was a lot of the delay
if you want to know more about that heavily documented in my report that is called Gemini Report
as well as the memo for something more up to date this report was launched in September 2023
and I do keep up to date you know at a more rapid cadence via the memo
we did cover the discovery of new materials in here we are in the research by DeepMind we
covered that in the memo I don't know if I like this comment interface that looks pretty broken
doesn't it oh well we tried all right Yan Lukane I've been mispronouncing his name this whole time
but apparently Lukane is closer
AGR being very far away I don't listen to that guy so that's the long and the short of it
he's very much defending his own older view of neural networks so it's not a
it's not one of the experts that I would be paying attention to Peter says when will real AI feel real
sentience I'm just trying to translate your question there Peter we do have our AGI countdown
in the background here we've got lifearchitect.ai slash agi it's not measuring sentience which is
awareness it's measuring well the definition is right here it's measuring a machine that performs
at the level of an average median human doesn't mean it has to feel the feeling part I don't
talk that much about Jeffrey Hinton does sorry the awareness part so I might just pause on answering
that you can certainly get it to replicate feelings and emotions and we did that with
Lita for a long time but you can go and I'd recommend going playing around with GPT-4 to see
what that looks like Lukas has a flag for us I remember talking about this last year meta-solving
long-term memory with Blenderbot 2.0 people are following this path so the agents that they call
this year agentized LLMs that are a complete system use long-term memory in different ways
and that's been really interesting to see you're right Blenderbot which is now about three years old
maybe two years old was doing some fascinating things in storing things in storing language
and information from the conversation or the context into a hard memory awesome
my definition of AGI has been standard it's open AI that I've tried to change the definition
there was some talk about an open AI open AI IP address editing Wikipedia to change the definition
of AGI over the last few weeks around the time of that board coup it's been pretty standard
we know AGI is average median human ASI is expert human
great question Helando I do not have an answer to this one
is anyone working on zero proof identity online not just Sam's world coin we did have the scanner
here in Sydney at some stage but otherwise I don't think there are any scanners in Australia
you have to go over to the US for that I haven't seen anything else on that I don't
I don't follow that to the same extent that I follow LLMs
what are the tests that one will be running to verify AGI versus AI we did cover this pretty
heavily last week it's not just the basis proprietary set that we've been working on
there are two decent alternatives to that one the first one GAIA by meta AI and hugging face
measures median humans at IQ 100 to 120 and you can actually have a look at the questions there
the data set is provided online they're really kind of fun actually see if you can get them
then there is the Google proof questions and answers for science experts generally for PhD
and professor level so above the level of AGI but I'm quite happy with the way that meta framed
GAIA and I'm comfortable as an alternative there with GPQ8 and our basis suite will come in after
that for more intense questions much more intense questions we've got some answer sets being cooked
up as we speak we will have a CSV for download shortly for those that have tried the first
two sample questions take it from me these are too hard for anyone except the one in 20 million
so I also cannot even get the first part of the question and then when you see the explanations
for the answers it's also very difficult for me to answer or to understand each step of how the
answer was obtained so I don't know don't be upset by that that that's literally like asking
if we talk about IQ for a minute 180 IQ versus the median human at 100 IQ is a delta of 80 IQ
points if we did the same thing further down the spectrum we would find that it's a 100 IQ
trying to talk to a 20 IQ now 20 IQ is legally would be hospitalized institutionalized may not
be verbal that's the difference in the the deltas there so I can't understand a 180 IQ person or
how their mind works don't be surprised if you can't either because it's like someone with a 20 IQ
and we used to have several words for that that we don't use anymore but in the dsm they
started with the word r or started with the word m it would be like that 20 IQ person trying to
understand an average human it's a pretty intense illustration but just to give you an example of
why these questions are so hard to understand at any level and jason dr betz has been having fun with
people have tried to submit questions at the level of maybe 120 130 IQ and they're they're just not
anywhere near what he has designed these questions to be excellent let's see if we can find a
controversial question here from zanz is it time for open air to change their name with
microsoft and salesforce taking board positions excellent some would say that they should have
changed their name a few years ago they've been around since 2016 ish maybe maybe before
there was some interesting talk about q-star it's still not something that i'm going to cover
but it's been fascinating to see the way this has been covered and the way this has been interpreted
because now my comments have broken completely because the CEO of open air didn't actually say
very much about q-star and yet people have read into that in quite interesting ways
yes managed to completely obliterate
my comments or maybe it's just yours right won't be shown sorry mate
all right thinkrinsity has a related question here once again i'm not even going to be able
to grab it unfortunately so let's use our old way of doing things a detailed method of creation
securing a question sets how we know the questions haven't been shared before testing it to a model
well go and read the basis page it says questions are created offline the air gap
they're never shared the first time that i take them out of the envelope
is the first time that they're seen this is the actual hyper compliant envelope locked
and dr betz posts the questions that have been handwritten in a room
pencil and paper straight from his head pencil and paper into a locked bag
they get opened once used either live on air or via a lab like microsoft open ai google and that's
it they're retired immediately so please have a read of the page before asking questions yep
oh we've got some related queries about iq i know it's a fascinating subject for a lot of people
let's grab lucas's query here a theoretical limit on iq so it's a really needing a statistics
background to understand this iq basically aligns with standard distributions
so out let's grab a little let's grab a pretty picture here for us for ourselves
life architect dot ai slash iq testing ai
and then that's not even the one i want we actually want visualizing brightness
i'm glad that that's on the top of my menu there
this is my standard iq chart that shows what's going on with iq in an easier to understand
way i believe so anyway they renorm the entire let's say iq score and align the population
every few years so that the baseline is a hundred and then using standard distributions
they say let's let's grab our 15 standard distributions above and below that
to come up with essentially our iq if you were to get up to 500 that would be pretty ludicrous
is there a better way of me explaining this one let's see now
sorry standard deviation
i don't know if we would get to an iq of 500 i don't even know if that would
align with the statistical distribution of the population but when we're 15 standard
deviations above the norm we get to about where are we here
we get to about our 180 ish and it's showing a little bit differently here
but basically the answer is no the highest iq we've ever measured is about 298
which is many many standard deviations above the norm
and to get above that i just don't think there's the population to be able to say
we're confident that in eight billion people that you sit in this percentile
that was a pretty messy explanation but it's 8 30 a.m here and
and it's a pretty messy or it's a it's a pretty messy field actually statistics is clean
but when applied to humans it becomes pretty interesting great question though iq of 500
would be interesting we've tried to also keep it pretty smooth and pretty clean in the basis
assessment just saying we're looking at the top 0.0005 percent and that is how we'll know that
we've achieved artificial superintelligence a machine that functions at the expert level
across practically any field i can't wait and i think it's going to be pretty close let's see where
we're at the definition for agi has stayed static on that site and that's the standard definition
let's grab john's query here i am i help us to realize tech like the star trek replicator cool
cool there'll be many many many other benefits of artificial intelligence i'm in the early stages
of my end of year ai report and i'm pretty excited about this one we do spell out some of the benefits
the coming benefits and the present opportunities of artificial intelligence it's a lot of fun
to see that laid out not a lot of people lay that out instead they focus on
the zero percent chance of an extinction event via ai awesome question there john thank you for that
see if i can unkill this comment i think nope it's gonna stay exactly as it was
zan's excellent question i'm covering this in the next edition of the memo the senator and she's
actually the u.s secretary going on record with a pretty horrific quote about this was shocking to
me they are playing really strange geopolitical games but they've just come out and she's just
come out and said we're going to keep passing new laws and putting new restrictions on invidia in
particular trying to rebrand relabel and maybe very slightly hamstring or handicap gpu's so that
they can export to china under the current rules which basically says no a 100s no h 100s so invidia
renamed them and decreased the throughput slightly on those cards yeah we go into big
detail on the next edition of the memo for that one because it's a fun conversation to see
where they got to and what that looks like in the global arena
a 50 plus 50 yeah it's it's uh something like that
jeffrey's got a question for us when would you predict the arrival of multimodal llms that
can watch for or listen for no text events this is already happening and gpt 5 in particular
will be training on youtube content gpt 4 also trained on youtube com content um and it's one
of the reasons that we were so careful with the design of the basis testing suite for asi
was that we are pretty certain that it will be training on youtube so the 14th of june
there was some information that youtube was used to train some of their models secretly use data
from the youtube site to train some of its artificial intelligence models now basis in
particular we put the canary string in there i think you'll recall in the last live i made sure
not to mention the answers because it could pick that up from transcripts and it could also pick
that up eventually from lip reading and it's been able to pick it up from images for a long time so
the warnings on gaya and the google proof paper were basically just uh make sure that you're not
showing this both in text and in uh images let me see if i can find that google proof example
that we showed earlier so we were running through gpqa we have that paper right here for ourselves
and on gpqa was by nyu co here and anthropic and on page two they requested don't reveal
examples from the google proof qa paper in plain text or images because this multi multi modality
is running all the time and you'll see that in all of the text to image models there's been a new
partnership between getty images and one of the big text to image model labs
but then it's been watching youtube it's been listening to youtube with whisper so all of
this stuff is happening already uh when would i predict that that actually happens for the
inference time that's a great question to expand on your question uh that already happens with
whisper so when you're playing around with whisper it's obviously listening
it already happens with gpt4 vision which is excellent for ocr i used it recently to translate
our chinese llms so if you go to chat.openai.com you got to chat gpt plus subscription here
uh hopefully i can just attach a chinese llm here wow that's a lot of files
let's grab this one
this is a screenshot of chinese llms as an image and gpt4 vision is going to go and
read that image look at that image and then essentially perform ocr across it but not ocr
as we know it this is a transformer based vision model that's looking at the image and finding
the closest next best word to complete my prompt put this in a table what are you doing behind the
scenes who knows developing some sort of csv using python
all right quailude charlie is taking us back in time while we're waiting
as a young man i worked with aliza it even worked in ms dos they should expand on a 32 bit
ai that will run on older computers and that's amazing thank you aliza was incredible it's
still is it was built into every edition of mac os yeah so gpt4 vision is down as we speak
because this is the kind of output that i would get if you're looking for the output that i actually
achieved you can go to lifearchitect.ai slash models dash table and on the second tab now is
the output from all of those images that became a table of 103 chinese llms within about 20 weeks
something like that back to aliza yeah it was built into all editions of of mac os so it was
inside the terminal and it was um i think it was called doctor within xcode they've since
removed it or at least you have to do a little bit more funky stuff to get it going of course you
can go and play with it online uh let's do aliza online nj it
just to have a play with it this is 1964 technology joseph wasenbaum and it essentially
repeats back to you what you've said i'm having trouble creating a new diet of protein and aliza
says how long have you been having trouble creating a new diet of protein let's compare that with
something like whoa let's compare that with something like chat gpt on po given that
this one is not going to answer for us of course claude is actually not going to give us a good
answer either aliza was ridiculous but you know from a perspective of proving whether or not it's a
bot it often passes the turing test because people assume that the human is being obtuse
when in fact it's just a very very old dot a bot all right what's going on here i think these
this tech knows that i'm live streaming so it decides to just not play properly gpt4 is giving us
a complete diet of protein good on your gpt4
i do remember fondly aliza and it was one of the reasons that i started in ai even before i
did my computer science degree actually so 1994 ish i was programming in q basic i was very young
i was about 11 years old or younger and it was a lot of fun to prove that we probably can't just
fully program an ai by giving it all the facts in the world despite many of my peers and
contemporaries trying to do that exact same thing at that exact same time if you're looking for
examples of that have a look at chris mckinstry let's grab him on wiki he was an ai researcher from
the 90s ish and he was creating something pretty interesting i'd like to find the name of the
here it is opened my open mind common sense project osomcs at mit and he was basically
programming along with push and marvin minsky a range of facts here we go different types of
knowledge simple phrases of natural language a coat is used for keeping warm the sun is very
hot the last thing you do when you cook dinner is wash your dishes dishes this was the state of the
art approach to artificial intelligence with or without neural networks in the 90s and it was
fascinating to me that there were at least two projects going on at the same time that dealt
with this and i thought that was just fascinating the other one was mind pixel i feel like you
guys want to go and research this after the live stream so i'm gonna pull them both up this uh
this one was created by mind pixel was created by one of chris's uh contemporaries
no getting confused maybe i'm getting confused with push sing so push was doing the omcs thing
and chris was doing something slightly different you don't mind if i get distracted slightly do
you these two guys uh in the 18 the 1980s we're doing some fascinating stuff and then in the 1990s
both decided that they'd had enough there are some great conspiracy theories around this but
there was a walk down the rabbit hole for those that want to get lost in what ai looked like in
the 80s and 90s and i'm glad that we've come a long way since then this brings us on a full
circle all the way back to transformer because we were stuck in this weird loop from the dawn of
artificial intelligence which was alan turing john von neumann was involved to a certain extent
from the 1950s to around 2017 there was just this ai winter and if you talk to any old professor
artificial intelligence doesn't exist because all they've learned from 20 sorry from 1950 to 2017
is that ai is these pre-programmed bits of knowledge or these really basic neural nets
then from 2017 with the launch of the transformer we went and we just went with gpt one
burt gpt two gpt three and mtlg and some others that are on my original bubbles chart we just
exploded to what we have today where what have we got here let's do this one where gpt four is
outperforming humans across the board using the same transformer technology from 2017 and completely
avoiding the pre-programmed knowledge graphs that we were giving it in the 1990s fascinating so this
chart says that it's hitting 100% in theory of mind it's in the 99th percentile for creativity
and hitting in the 94th percentile for the sat where students hitting the 50th percentile there's
this other one i put together last night where it's even and this is just based on transformer of
basically predict the next most likely word the next statistically most probable completion
chat gpt the very small model 980 percent higher prevalence of empathetic and very empathetic
ratings versus a human doctor and on the left side there 360 percent higher prevalence of
quality ratings good and very good versus a human doctor all from the concept of
train a transformer based model to predict the next word and feed it as much data as we can find
now being measured in the terabytes so the red pajama data set is about 125 terabytes for 30
trillion tokens come and a really crazy amount of distance in that six or seven years since
transformer and a lot of it has happened post 2020 that's why i call it post 2020 post 2020 ai
because what's come out of gpt3 gpt4 and all other models that you've seen on my bubbles
visualization and the models table really make this intriguing we were going to mainly talk about
open source today and do note the lama models there the stable lm models the uh olmo model
which is due out in the next few weeks i'm hoping for january of february that's alan ai's model down
the bottom and the falcon model there out of the ua e all for open source grab a question from ben
if we can and our other media thing our other comment thing is still broken
glad i kept this back up does your definition of the average human for agi includes spirituality
does regurgitating work dogma count a written dogma count or does it need to have its own
thoughts on spirituality my definition of agi doesn't include any of that it's much more basic
it is uh essentially as we've documented here it's essentially any human task rather than
any human anything so this did include going into the house and making a cup of coffee but it
doesn't include you know having spirituality or having emotions or even having a sense of smell
but that's what makes this kind of interesting there's there's no agreed upon definition
quaillude charlie how many quailludes have you had this morning is that even an appropriate
question to ask on a live stream i still like to listen to those guys speaking about
pre-gaming is that programming in the 1550s and 60s and remembering the hardware and software
with early stuff yeah well the earliest stuff i've got is the late 80s early 90s but i was
still playing around with cobalt pascal uh algal and we were forced to write assembly language stuff
in the uh the university the computer science degree that i was doing which is pretty horrendous
it was a fascinating time and in some ways right now reminds me of those early days and i cannot
remember back to the 50s and 60s because i wasn't there but i was there in the 80s 90s i was there
with irc i was there with icq was there with the early programming languages and the very
early computers my first via my older brother was a 486 with like a 300 meg hard drive and
eight mega verand that that time was exciting the communication figuring out how to network via
coax via t pieces via parallel cables or serial cables getting doom working across serial cables
in some ways this is the same sort of excitement for me we're finding out how to connect things
together how to create new worlds essentially uh i would probably say that this is more exciting
but there's always something about hindsight right especially with rose colored glasses
where back then looked kind of cool if i was forced to go back there and use a 486 with a 33k
modem if we even had web access uh i'd be pretty upset waiting for dos to load or windows to load
over five minutes and then having the ipx networking breakdown every day just because
if you want a reminder of those times just try and get bluetooth working in 2023 or try and get
your printer working in 2023 same technology same issues we haven't solved it in 40 years
all right let's go over question from greg we did kind of cover this last time but let's see if
we can cover this again not too happy with that comment thing failing it makes all of this a
little bit harder greg says given you don't listen to yarn who are the main people you do
listen to ilja kebathi hinton uh greg is great greg brockman from open ai i think i mentioned
last time he sat down for two weeks and got gpt for working so even when we have competitors
they're not massaging it and getting it ready for ux and public consumption the way that open
ai i have done and that's not because of their hundred billion dollars or their 750 very smart
people it's because of one or two people in their ilja's one of them but greg is one i listen to
and the way that he got this stuff working is fascinating we got macguffin who is remembering
token ring yes some of the games in that time were just amazing i was looking at rise of the triad
the other day wolfenstein 3d i mean the original wolf 3d descent who played descent with me that was
amazing and my favorite was a game called total annihilation which we didn't really hear a lot
of because diablo and warcraft kind of competed with it but uh that was that were they were my
favorites yeah i saw the stream cut out there i'm not sure what that was i'll blame youtube for that
one altman's now invested into a company that creates neuromorphic analog ai chips there's a
lot of talk right now about quantum i won't be covering quantum but if we get james on here
we will cover quantum james from ibm is my go to colleague for that kind of thing he programs
quantum stuff day to day he's the advocate for quantum computing for ibm awesome all right we
may look at wrapping up it's a full hour of just answering questions and going back in time and
reminiscing about what life was like back in the pre-dawn actually it was the pre-internet really
if we say that the web hit us from public utilities often by colleges and universities in the early
nineties that means just before that was all local networking or hacking up things inside
dos and windows 3.1 was my first one i did spend a lot of time talking about os2 warp 4
uh which i used a little bit but i would talk about it with my consulting colleagues just as a laugh
oh
look it may be my wireless who knows i love the uh the emojis there that are just the scared face
oh i see it now you can you can just send a scared face or an embarrassed face cool
there are a couple of approaches to this already so you could talk about gpt 4 as being multimodal
because it's got the vision component and then because they've tied dolly 3 into the chat gpt
interface it's like they're tying together three or four different models different models
because you could also say that the what was previously called the code interpretation plugin
now called the data analysis plugin is a third or a fourth model so you got gpt 4 text you got gpt
4 vision you've got code interpretation and you've got dolly 3 now some of those you would say are
completely separate and discreet but the fact that they've combined them into one interface
is fascinating make me a picture of a youtube stream hanging for no reason
so this is all in the same platform obviously not using the same model but
who's gonna who's gonna know if a lab joins those all together that's why i'm fascinated to see
what our final version of jamini looks like and the rumors are that it will have separate
vision components to text components thanks dolly 3 here's the image of a youtube stream that has
unexpectedly paused awesome all right let's wrap up with this question from drew leaders
deep intro inspired me to ask you what books would you like to see in the school curriculum that
might encourage better evolution unity empathy and critical thinking all right let's give you a
big answer here for something that's just a few years ahead you could say that it's immediate
but let's step forward a few years the answer to my question the answer my answer to your question
is i'd like to see no books on the school curriculum and i'm not necessarily being groundbreaking with
that view if you'd like to read more i've documented this really heavily all the way back in 2017
life architect dot ai let's see if i can get this life architect dot ai slash ad astra i'll dump this
into the chat because it's a really interesting read you can download the article as it appeared in
mensa magazine uh let's actually pop that open it basically says and this was at the time that i was
working alongside elon musk's school in california when he was teaching or he's having his twins
taught and they were using a curriculum that was completely created by principal joshua dan who is
an absolute legend he's still doing this uh but basically we looked at the fact that these guys
didn't really use computers they didn't use handwriting because handwriting was too slow
they didn't really have homework they certainly didn't have books he didn't teach languages
because elon was getting them ready for the fact that well neurolink was coming so why would we
teach languages when as a reference to earlier in this live stream we can have real-time translation
potentially including uh sorry about that selection potentially including uh lip movements as well
and maybe gestures soon so if you're translating that to italian maybe it gives you hand gestures
along this alongside it uh but this entire school and it was founded back in 2016 sorry 2014 my work
with them or my my work alongside them my research of what they were doing was 2016 they've been there
since 2014 they're about to hit their 10-year anniversary of not using books of not giving
homework of not teaching languages of not using computers despite being probably the most technically
oriented school in the world fascinating and look how much further we've got in terms of runway to
play with of what we could actually do there think about a gentiles large language models
that you can go and speak to and it will gamify or just make playful your education experience
right i've just seen that sov exists for linguistics how does that work with languages that i'm
interested in how could that work while i'm at the grocery store with mum and dad how can that work
at the family dinner table i've just discovered this bug on my walk taken a photo with that ai's
told me what it is let's get the whole etymology in context of that bug so making it completely
personalized and tailored this is in some ways already here it's been here since 2014 with ad
astra but it is far more accessible now and the capabilities of large language models make this
entire context really interesting i'm waiting for 2024 so that we can play around with all the
capabilities of a gentiles llms as systems that will go and help us learn that's probably an
unexpected answer to your question but i'm always surprised to people who are talking about books
including the ceo of open ai alton recently said that whole board coup that whole politics he said
they'll write books about this and i went you're the leading voice in artificial intelligence at
the moment you think we're going to be writing books this year or next year i don't know i documented
let's go back to our screen here i documented books written by a i all the way back in 2020
life architect or ai slash books by ai and at that stage every book you're seeing here completely
written by a large language model with prompts by a human author at that stage there were very few
books this is one of my favorites you can read about lian lee's process for writing books in
her series using gpt 3 and now i've said right where i'm not even going to document all the
books that are being created by ai because it's ridiculous but just to my point there
if ai can generate books instantly and it can to the extent that amazon recently banned
or limited the number of ai generated books
i think it was two yeah three books per day because they were having so many people
cranking out artificial intelligence generated books they said right maybe you're generating
100 per day and trying to monetize them we're going to limit you to three a day books are over
and that's been the case for a while look out for agents look out for the next edition of the memo
and if you are a full member of the memo you will get early access to my end of year report
which is spelling out some examples of global and personal agents there's my invitation to
you i'd love to see you there you're invited to join the memo with me thanks for joining me today
and i'll see you there and i'll see you this time next week for our second last live stream
for the year it's gone that quick thanks so much for joining
did you see the memo about this yeah yeah yeah i have the memo right here super intelligence
is unfolding at lightning pace read my industry great analysis of ai that matters as it happens
in plain english the memo yeah did you get that memo yeah i got the memo get the inside look as
ai models are embodied into humanoids ai's iq increases to nearly perfect and bleeding
edge use cases expand to the entire world yeah didn't you get that memo additions are sent to
subscribers at fortune 500's major governments and people like you life architect dot ai slash memo
i have the memo
um
you

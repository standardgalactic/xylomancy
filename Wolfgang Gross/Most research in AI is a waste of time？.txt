science is getting less bang for its buck. That's what Patrick Collison and Michael Nielsen
wrote in an article in 2018 for The Atlantic. Even with more and more money spent and the number
of scientific publications going up, the number of significant scientific discovery stayed the
same for the last hundred years. And this is especially true for AI. Most of the research
in the deep mining world is a total waste of time. Right, that's what I was getting at. Yeah,
it's a problem in science in general. What went wrong? Is the next AI winter just around the corner?
Do we need to wait for the next big idea in AI to save us? Or is AI research just suffering from
the same replication crisis that affects all of science at the moment? And how do we as AI
researchers or practitioners help the situation? Is science becoming harder or more costly?
After all, in the first part of the last century, there were seminal papers released
with only one author. These new physics papers that use particle accelerators have thousands of
authors. Given these extreme examples, but on average, research teams nearly quadrupled in size
over the 20th century. And that increase continues today. It's surprisingly difficult to measure
scientific progress in a meaningful way. And the best thing we have is talk to the best experts
in the field and ask them what meaningful progress is to them. The data by Carlson Nielsen shows
that researchers prefer Nobel Prize discoveries from past decades over those of current decades.
The data also shows that scientific discoveries from the 1910s to the 1930s
were picked over those of current decades, especially for physics.
Nobel Prize winners also got older over the decades, from 37 years in the early days
to 47 years now. Many objections can be made against the survey. But still,
asking scientists is the best method that we have. Maybe it's just harder to make great
scientific discoveries after all. The concept that goes along with that is that of a country
to be discovered. In the beginning, there is a vast emptiness, and everywhere you step,
a new discovery can be made. After a while, only far-distant lands can be discovered or deep
creeks and need a lot of time and resources to get there. After all, you can only invent quantum
mechanics or the theory of everything once, right? Not quite. There's also a different point of view.
According to the science, it's more like an endless frontier, and the closer you look, the more
things you can discover. Like measuring a coastline, the closer you look, the more details you see,
like zooming in on a fractal. Richard Feynman explained it like this. If you try to answer a
why question, there's always a next why question to be answered, and the more interesting it gets.
So we shouldn't really run out of things to explain. But what's the problem then?
A lot of research studies can't be replicated, as has been shown in psychology. An estimated
number of 15-55% of studies cannot be replicated, as meta-studies showed, to clarify things.
Studies that use experiments and do significant tests are expected to be wrong sometimes. That's
exactly what the significance level is telling us. So from 100 studies that get a positive test
result and use the p-value of 0.05, 95 of them get the true result. But 5 of them get a positive test
without a true relationship behind it. Even with considering this, 15-55% is way over the expected
5% of being wrong. So how does this happen? First, statistics is hard and mistakes happen.
As Gerd Gigeritzer puts it in his book,
people aren't stupid. The problem is that our educational system has an amazing blind spot
concerning risk-literacy. We teach our children the mathematics of certainty, geometry and trigonometry,
but not the mathematics of uncertainty. It's surprisingly hard even for experts to understand
them and communicate them well. Psychology especially suffers from this problem,
as they heavily rely on statistics and empirical studies. The lack of sound fundamental theories,
like we have in physics, does clearly not help. The nature of the mind is just not so easy to
observe. And after all, psychology only split in the first half of the 20th century from philosophy
and became its own discipline. There hasn't been that much time yet to build a sound fundamental theory.
So what's the problem with statistics? Coming back to the initial observation,
we expect 5 of the 100 studies using a significant level of 0.05 to be wrong. But the numbers show
that much greater numbers of papers are actually being wrong. Research is not most appropriately
represented and summarized by p-values. This paper here points out where this could be the case
and shows some common fallacy that many research papers fall victim to. The probability that
research finding is indeed true depends on its prior probability of it being true.
Among others, this prior probability is defined by the ratio of true relationships
to no relationships tested in the field. In a field like AI where there's often great flexibility,
which relationships we test, it is less likely that these relationships are actually true.
In addition, if many teams are working on the same problem in isolation,
this actually increases the chance of an alpha error that is finding a relationship where there's
actually none. And this is exactly what's happening in a hot field like AI right now.
I hope that it is clear to you now that AI can also suffer from the same replication problems
that just discussed. And we have the problem that interests from practitioners and researchers in
the industry often differ from those in academia. And in this regard, Google and Meta AI more often
promoting their own proprietary research instead of advancing actually research and knowledge
discovery. The point is data does not speak for itself. It must always be interpreted and how the
data has generated plays a huge role in what results we are getting. And second, there are wrong
incentive structures for researchers as well. As Professor Brian Nossick says, there's no cost
in getting things wrong, but there's cost in not getting it published. We can't solve all of these
problems overnight. But what I want to talk about is how can we make it better? Most people know that
one thing, correlation does not mean causation. But researchers want to understand the causal
relationships around us. We want to understand if adding two layers to our neural network
caused performance to increase or not. We think causally, it comes very natural to us. This is
also why statistics is hard for us, because it's not about causation. Luckily, Juda Pearl has an
answer for us. To answer causal questions, you must climb the letter of causation. I just recently
read his book, The Book of Why, and looking at our problems through the causal lens is a very
revealing exercise, and I can highly recommend giving the book a read. For example, try this
experiment. Flip two coins simultaneously 100 times and write down the result only if one of them comes
up heads. Looking at our table, which will probably contain roughly 75 entries, you will see that the
outcomes of the two simultaneous coin flips are not independent. Every time coin one landed tails,
coin two landed heads. How is this possible? Did the coins communicate with each other somehow?
Of course not. In reality, you conditioned a collider by censoring all of the tail-tail results.
This is a simple example, but it shows how easy it is to introduce a bias just by the
process of data collection. This is called a collider bias and is very common in practice,
albeit not always as obvious as in a coin flip example.
Rich Sutton wrote a blog post in 2019 called The Bitter Lesson. The bitter lesson of AI research
is that in general methods that leverage computations are far more effective by a large margin.
There is a debate on how much of the recent success in AI can be attributed to algorithms
and how much is due to the increased computation power. Most often Moore's law is called here,
which states that the computation capabilities are doubled every two years. It's interesting to
think about how Moore's law can be extended in the future. Can quantum computers save Moore's law
eventually? What about distributed computation? The number of computing devices in phones,
cars and utilities also follow an exponential growth. Or what about neural computing? It is fun
to think about these things, but this should not distract us from the main point of Sutton's blog
post. The bitter lesson says that in 70 years of AI research, methods that scale well with
computation are ultimately more relevant than methods that don't. In contrast, methods that
build around how researchers think they would solve the problem only work on a short run.
There are plenty of examples of this in computer vision, NLP and chess. Lex Friedman puts it this way.
I would love to see that when people publish papers today, it maybe almost have like a section
where they describe if computation was able to be scaled by 10x by 100x, looking five,
10 years down the future. Will this method hold up to that scaling? Is it scalable? Is this method
fundamentally scalable? I think that's a really good question to ask. One convincing demonstration
of the power of this scaling law is how transformers disrupted the world of NLP just by leveraging the
computation more effectively. Before transformers, mostly RNNs were used in NLP. But RNNs can process
data in parallel. The disruptive attention is all you need paper had a simple but effective idea.
The new architecture allowed the models to process data in parallel and thus allowed the
researchers to train the model on a large amount of data reasonably fast. It also has been shown
that scaling large language models is the main factor that makes them so effective.
Also transformers are the basis of diffusion models which can be trained to generate images
for a given text prompt. I think a similar story can be told on how leveraging computation was the
main factor that allowed AlexNet back in 2012 to crush the benchmarks in computer vision.
As Alan Turing said in 1950, conjectures are of great importance. They show us meaningful
lines of research. We need this overarching conjectures now more than ever to guide us
the way to show us what is meaningful research and what is a waste of time. I think Rich Sutton's
bitter lesson is a great conjecture in this regard. In my opinion, looking at the history
of AI is of great value. It can show us which pattern repeated and what we can learn from them.
I hope this video also gave you some understanding of what good and bad scientific discoveries are
and how to find them. We should especially look out for these hype topics and take those discoveries
with a grain of salt. Leveraging computation might be one of the aspects of future AI but again
it might be something completely different. Conjectures take courage. It takes courage to
place integrity over safe bets. Ryan Holiday says, courage is honest commitment to noble ideas.
But it's not for nothing. If you don't believe in anything, it's hard to find anything worth
believing in. Courage cuts through the noise, even if it means breaking some rules.
Check out this awesome project, the ML reproducibility challenge of 2022. And I bet there's
one for next year as well. They encourage and award researchers to investigate reproducibility
of papers accepted for publication at top conference, especially if you're a student in AI.
I think it's a great way to have a meaningful impact on science right now.

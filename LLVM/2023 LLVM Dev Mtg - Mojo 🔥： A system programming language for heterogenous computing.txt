Thank you, Alex.
Wonderful.
We're very happy to be here and talk a little bit about what we've been up to.
So we'll start with what is Mojo?
At a glance, the top-level points of Mojo is that it's a
Pythonic systems programming language.
So what does that mean?
That means we're here to do really cool things with systems and compilers,
and it happens to look like Python,
but forget everything you know about Python.
Please, please.
So this thing is about one year old, so it's still pretty early.
It's still in development.
It's still quite interesting and doing some cool stuff, though.
We also have a vibrant community.
We have over 150,000 users.
We have a big community in Discord, and there's a bunch of excitement around this.
So we'll dive today into why did we do this in the first place?
That's often something we're asked.
We'll talk about how we approach designing a new language from scratch.
We'll talk about internal implementation details,
including some of the horrible things we did to LVM.
We'll talk about what this means for accelerators and compute,
and then wrap things up.
So first, why?
Why, why, why, why, why?
So many of you are working on AI, and if you work on AI,
the question I will ask of you all is if AI is so important to the world,
why is all this offer so bad?
This is a huge question, a huge problem,
and I think that many of us have been working in this industry for a while,
have been struggling with solving this problem in many different ways.
So for me, when I look at this, I think that the challenge is really fragmentation, complexity.
It's all these systems that do not work very well together,
that are being built by well-meaning people in different groups and areas,
but they don't really actually work together,
and so for a user, this is a huge pain point.
And why is this?
I'll speak for myself.
If you're enabling a chip, you're focused on the chip.
So many of us are paid to solve one specific problem.
We're not here to solve an industry-scale problem,
and you can't afford to do it.
You don't have the time, you don't have the schedule, you don't have the headcount,
you know, whatever.
Often, the organization that you're within, in my experience,
makes it very difficult to solve some of these problems.
And so our approach at Modular is that we need fewer things that work better.
And so that's what led us to building Modular in the first place.
It's really kind of an organization that can span across many different of these problems
and invest for the long-term in building and hopefully lifting the industry over time.
So how do we do this specifically?
Well, we're building what we call the AI engine, right?
Well, the AI engine, if you look at modern ML stack,
a lot of folks are trying to throw layers of Python on top of all this AI tech
that has been built up.
We're tackling it at the hardware-software boundary,
reinvesting, no surprise, and compilers.
And so what we want to do is we want to unify and integrate
all these low-level technology systems so that innovation can happen up on top
with programming models and frameworks and all that kind of stuff.
Our approach is to meet people where they are.
So people use PyTorch, people use Jax, people use TensorFlow.
That's awesome.
These all have pros and cons, and there's other stuff as well.
And very few people actually want to rewrite all their code.
And for us, it's very important to be drop-and-compatible,
meet people where they are, and work with their existing systems.
The other thing is that this is not a research project.
Like, there's a lot of really interesting and cool things that have been built
over the last eight-ish years of AI infrastructure.
It often gets fragmented out into all these different systems.
We've learned from many of them.
And so what we're doing is we're pulling this back together
and doing hardcore engineering, not research,
to build a production quality system that we hope can scale for the world.
I'll go through this super quickly.
What is an AI engine?
Well, it's really two things.
One is this operator graph.
The operator graph, in the interesting case, is heterogeneous.
So people often focus on, for example, the GPU,
and how do I make matrix multiplications go fast?
And that's a super important problem.
But often folks forget that AI today is a distributed problem,
involves the host, involves the accelerator,
involves pre-processing, data loading, this whole thing.
And so you can't really solve the AI problem for a user
unless you really tackle this whole problem.
And furthermore, like, this is really heterogeneous.
Like, as we've seen, there's all kinds of different accelerators.
There's all kinds of different hardware.
When you have a cluster, lots of machines,
like micro-architectures don't always match,
there's a lot of complexity in this space.
So many of us have been working on this, again, for a long time.
And so we've seen the rise of kernel libraries.
This is how many of these systems were first built.
And one of the challenges that I won't go into in depth,
many of you probably already agree,
is that kernel libraries don't scale, right?
And so many of us, for multiple years now,
have been building AI compilers.
And so there's lots of these, lots of different approaches,
online kernel fusion, lots of cool algorithms,
getting vented and used.
We can talk about all the different pros and cons of trade-offs.
But the thing I want to claim is that neither of these approaches scale.
Kernels don't scale, hopefully many people understand that,
but neither do ML compilers.
And to a compiler audience that maybe is more controversial
than to a kernel audience.
So I thought I'd dive a little bit into why this is,
and the challenges that we see with this
led us to our approach with Mojo and the system.
So the first is generality, right?
I mean, empirically today, ML compilers are not very general, right?
Generality includes not just matrix multiplication,
again, data loading, preprocessing, all this stuff,
but also dynamic shapes, varsity.
There's better and worse systems out there,
and I mean, and there's definitely progress in this area.
But if you're coming at it from a user's perspective,
they want things to just work.
And if they don't just work, then they'll move on
and spend their time something else.
Generality is also important because, you know,
if you're, again, coming from a hardware enablement perspective,
you don't really have time to invest
in all the other parts of the problem.
And so it makes sense that many of us working on
bring up the chip don't actually focus on
the big picture parts of the problem.
Another one is community.
So you all are wonderful compiler nerds.
You know, I love you all, obviously.
And I am myself a pretty big compiler nerd.
But the problem is that nobody can hire a compiler engineer.
This is pretty well known.
And so with AI compilers, this becomes even worse
because how do you hire somebody who knows compilers,
who knows AI modeling and all the different exotic
new model of the day, who knows all the numerics
and the data types and knows all the specialized hardware,
and how do you find that unicorn person
that knows all of these things together?
It's very, very difficult out there.
And if you need a compiler engineer to be in the loop
of novel research, there's very few companies in the world
that can afford or attract to do that.
And so I believe that you cannot have a compiler-first approach
to this problem simply because there's enough talent out there.
I mean, I love you all, and you're all very valuable,
but this is very difficult, particularly for the scale
of what AI research is today.
Second, if you're a compiler engineer, it seems really weird
that we're re-encoding all of compute into IR builders
and sanding out all this stuff.
And so you feel like there must be a problem here at some point.
Finally, there's this fragmentation problem.
If you want to solve and build a heterogeneous compute system,
we have to face the reality that AI developers,
researchers, are in Python.
The frameworks, the host-side compute, it's all in C++.
The device-side is in CUDA, and SQL, and other things.
And so if you want to build a system that can scale across
all these different levels of abstraction,
there's a huge fragmentation problem here,
and we need to be able to unify this.
Otherwise, we can't have one system that can reason about it.
And so if you want to be able to build this and solve this problem,
you have to kind of come back and look at the big picture
of what's going on here, and the nature of compute has changed.
So this is what has led us to Mojo.
Now, how do we approach building Mojo?
I mean, you know the outcome, and we'll talk a lot more about how it works,
but how do we even get here?
Well, when we started modular, we started with a thesis, a hypothesis.
We believed that we could get to state-of-the-art performance
against a lot of vendor systems,
and do so with a single source of truth in our code for numerics.
This hasn't really been done before.
Certainly, systems have been around in the space,
but this thesis, if true,
can enable and unlock a huge amount of innovation in the industry.
And so what we did was we said,
okay, let's go invest in some very fancy compiler stuff,
generalized fusion, and caching integrated distributed compilation,
like lots of cool stuff.
Let's figure out what we want to do,
and then let's go validate that.
But for validation, we didn't actually care about syntax.
So what did we do?
Well, we went and built the thing.
We went and built a compiler and completely ignored syntax.
Why? Well, MLR is great.
You can write MLR by hand. You don't need a front end.
And so what we could do is we could actually go build major kernel libraries
and things like this and validate.
Architecturally, we could deliver the performance that we wanted to,
show that the compiler worked,
iterate rapidly on the compiler without having to change a dependency,
and go and do this.
And what we found, fortunately, is that it works.
The technology we built actually is good.
It worked. It was proven out.
And then immediately we figured out that writing large amounts of MLR by hand
is maddening and doesn't scale,
and there's no way a real normal user could actually do this.
But this validation of the algorithms of the compiler tech,
the low level system, which is very novel,
and Jeff will talk about later,
was really important to building our system
and doing so without being anchored on syntax.
I think it was very good for both focus,
but also for the ability to iterate.
So once you get that, you get to the point of saying,
what about syntax?
Syntax actually does matter.
And so the three major approaches we looked at are,
do we take an existing language like C++ or Swift or something like that?
Do we do an EDSL?
Do we do a new language?
And so when we were talking about this,
we came back to our core principles, our values, our goals,
which is that we want to meet people where they are.
And whether you like it or not,
AI developers, but also most software engineers are all in Python.
Right?
Python's pretty arguably the most popular programming language in the world.
And so if you're coming from a Python viewpoint,
arguing with people, trust me, I've been there,
to try to get them to switch to a different thing,
is a huge amount of work and it doesn't really go anywhere.
And so we realize and believe we had to go with Python,
and what that meant is that it meant that suddenly
a bunch of existing systems are just off the table.
Like C++ is not Python, Swift is not Python.
Like these things are not Python,
and so that really allows us to focus our frame.
What about EDSLs?
Well, EDSLs are super common, they're super popular,
and they exist for lots of good reasons.
They're relatively easy to implement.
We've had several talks at the conference about how to use Python
so that you can extract and build IR from Python ASTs and things like this.
It means you don't have to build tooling, you don't have to retrain,
you can get to market fast.
The problem is that they provide a really bad developer experience.
You don't get a debugger.
This really can't fit into the existing systems.
If you care about host performance and generality,
Python's not there, right?
At least not the level of performance that we care about.
And so what we really want is we want a system
that allows us to innovate at all layers of this stack.
Okay, well, how about a new language?
Again, you know kind of where we're going with this,
but a new language has the advantage of you get the best quality result,
you can control everything, you can invest in things,
you can target CPUs with high performance,
which is quite important to us.
But what you need is a strong vision for what you're trying to do.
You need a long-term commitment because the demo is easy,
but production quality thing is hard.
You need to be able to pay for it.
You need to be able to track people.
You need to be able to have a big target of developers
that makes it worth doing in the first place.
And so this is actually well known to be ridiculously expensive.
Building a new programming language is not a simple thing
that you should reach for as your first outcome.
But as you know, yes, we want a baby little mojo to be built,
and what we decide to do is actually do this.
And why? Well, it's because it's the only way to achieve our goals,
to achieve the best quality of result for ai developers
and many other developers worldwide and be able to lift the industry.
There are many point solutions that demonstrate many different
capabilities, but we really want to go beyond this
and integrate and unify the world.
And so if you come back to what we need to do,
and we think that we have all the constituent ingredients here
with a good vision, we think we know what we're doing,
we also know how hard this is.
So I personally built several major programming languages
that are used in production and have seen the entire journey
and made many mistakes and have learned from them.
And so with full knowledge, we step into this and say,
okay, let's do this.
So I'll give you the high level design points of mojo.
As you know, it's a member of the python family.
Over time, it will grow into being a full superset,
so we don't want to do a python 2-3 thing anymore to python programmers.
As we said before, it's focused on system programming,
high performance, working backwards from the capability,
the speed of light of hardware.
Definitely not working forwards from what python can do today.
Also lots of hardware, anything with the program counter can apply.
But coming back to this also, and we'll talk about this a little bit,
it's about unlocking the modular compiler stack.
And so instead of talking about the high level fluffy stuff,
I'll introduce Jeff, and he can tell you a little bit more
about how it actually works.
This is Chris for the introduction.
So we are started off by de-risking the core hypothesis,
and we have an MLIR-based compiler that is different a little bit
from the systems that predated it,
and we've proven that we can beat state-of-the-art.
The problem is that we've got like 50,000 lines of handwritten MLIR,
and handwritten MLIR is like write once, read never.
It's so verbose.
You have to write the types every time you use an SSA value.
It's pretty hard to actually write incorrect code,
but then it's not readable, it's unmaintainable,
and the new people being brought into the company are like,
what is this?
So we need syntax.
We need a programming language for MLIR.
Why all MLIR?
Well, it turns out that modern computers are getting really complicated.
Modern types are getting really complicated.
Look at just floating points.
Most languages, give or take, have a flow and a double.
But MLIR has things like float 8, e4, m3, finous.
I'm sure it's useful, okay?
And that means that we need to have access to it.
There's probably a piece of hardware somewhere on it
that uses this data type, and it's very fast.
That's just the tip of the iceberg.
MLIR is such a vast ecosystem with many different kinds of hardware
targets, domain-specific dialects, and so on,
and we would like Mojo to be able to take advantage of all of that.
So we need syntax, sugar, for MLIR in general.
But then how do we approach something like that?
Well, we start with the types.
In a programming language, types tend to be the most load-bearing element.
You need types to do computations on them, after all.
So let's start by focusing on a library-based language.
That means that we write all the parts of the language in the library.
And the good news is anybody can write libraries.
So this scales the effort of engineering to everyone in the world
who can write Mojo, not just a couple of people who work on the language.
And that's really important, because we don't want built-in types in the language
to be special or be more performant than what you can enable in the library,
because that bottlenecks performance and the scalability of the system
to the people who work on the language.
So we need to give people who use the programming language library authors
the same power as language engineers.
It turns out, actually, that Python has a really extensible type system.
You could argue that user-defined types in Python are actually much more powerful
than the built-in types like interfloat.
And the reason is because Python provides this kind of ability
to encapsulate type semantics behind Dunder methods,
which are really syntactic wrappers.
So let's just use that in Mojo, right?
We use a struct, which is like a class, but it's densely packed in performance,
to wrap an MLR type.
And then we use Dunder methods as well as class methods to wrap MLR operations.
And what you get is any MLR type will work.
Any MLR operation will work.
And so now we have 1 plus 2 Dsugar's to an MLR op index.add.
The other important aspect is we need to make sure that these user-defined abstractions
feel native, that they're zero cost.
So how does Mojo do that?
Well, it has a couple of bells and whistles to tell the compiler
that treat this type in a specific way, effectively giving a built-in-like experience.
And one of these is, say, always inline node debug,
which will always inline the function, no question about it,
and for a better debugging experience, it nukes out all the debug info
so you don't step into a plus of an integer.
So if we put this all together, just these pieces of basic types,
so you have a simple while loop in Mojo,
well, the parser will then spit out a bunch of source-level IR, right?
But then Mojo has guaranteed optimizations that run all the time,
such as the always inliner and memtoreg.
And then this gets desugarred down to IR that is pretty close
to what we would have written by hand.
And that's important because it, from the get-go,
provides a predictable IR-gen model for the programmer,
and it helps us get an off-ramp from all the handwritten MLIR.
So it turns out we've actually discovered what MLIR really stands for.
It's Mojo Fire-Emoji Language Intermediate Representation.
And the best part is your dialect works, too.
So this is zero-cost abstraction around any MLIR.
So let's say you have a shape dialect with a mosh.ape type,
and it implements plus to concat and subscript to getDim.
Well, now you can write shape functions in Mojo.
It spits out some IR that's been desugarred to,
and then you can ingest this IR and do cool compiler stuff like shape inference.
And the best part is all of the language tooling just works.
So you get code completion, you get doc generation,
you get syntax highlighting, and even debugging if that's relevant.
But MLIR just forms the bottom level of the language.
It's how we talk to the hardware, it's how we talk to the various dialects.
Building on top of that requires high-level abstractions,
and the way you do that in Mojo is metaprogramming.
So Mojo needs to build hardware generality,
and the way we do that is with metaprogramming.
So you can write a kernel without caring about what the vector length is,
and then say, in this example, ask the compiler to pick one for you.
It turns out that metaprogramming is also pretty cool.
Generics are nice, code reuse is great,
and it allows to have scalable development.
So where can we look at for a metaprogramming system?
Well, I actually like C++, I don't know about you.
And C++ has templates, and doc typing in C++ is really powerful.
Let's see, write some pretty crazy generic code.
The problem with that is that the usability is poor.
I think template error messages get better every year,
but there's still some room to go.
And it turns out that for the kind of metaprogramming,
high-performance programming needs,
C++ templates just aren't good enough.
So imagine you have a tensor type.
It has a static or dynamic rank.
It has a static or dynamic d-type.
It has partially dynamic shape, partially dynamic stride.
It gets ugly pretty quickly.
So it's not good enough, and let's see if we can build something better.
So it turns out, once again,
Python actually has really powerful metaprogramming.
Decorators can arbitrarily modify objects
and return a function where there is a type.
And with full AST reflection in Python
is what enabled all these crazy libraries,
such as the ML frameworks like PyTorch,
Jackson, TensorFlow, as well as things like Numba.
The problem with the Python metaprogramming
is that it happens at runtime, which means it's slow,
it's not going to run an accelerator,
and it gives zero control over the generated code.
So the challenge for us is let's try to do it at compile time.
So that brings us to mojo parameters.
Mojo parameters are compile time values
that form the backbone of the metaprogramming system.
So structs can have parameters.
These are compile time values.
Functions can have input parameters.
And then you can declare name parameter values
with alias declarations.
So you can kind of think of them as being like C++ templates,
but they're a little bit different.
For example, in C++ you have using declarations for type aliases
and constexpr declarations for compile time values.
But in mojo, types are just compile time values.
So aliases and, say, compile time floats
and compile time ints are the same thing.
The most important thing that gives is that the meta language
is the same as the actual language.
And Zig really blaze the trail here
by having no distinction between the metaprogram
and the actual program.
In mojo, we strive to ensure that almost any user-defined type
and function can be used and called
in a parameter expression at compile time.
And the way we do that is with an MLI interpreter
that has a full memory model.
So to really drive this point home, we have an example here.
It's fill a vector with a bunch of integers.
OK, not too bad.
This function can be called at either compile or runtime.
And if it was called at compile time,
you can even return a type instance.
And this vector has heap allocation that is computed
at compile time and then used at runtime.
So when does this happen?
When do we do, say, instantiation of parameter values,
instantiation specialization, and interpreting of code?
Well, it doesn't happen in the parser like in C++.
So in mojo, we do parameter instantiation
in a process called elaboration.
And it happens later in the compiler pipeline.
What that means is that now mojo needs
a IR representation for parametric code.
So in this example, we have a piece of IR
and we have a parameter in the IR called value.
Importantly, this parametric IR is target agnostic.
It's portable.
Something like size of lives directly in the IR
and is resolved by the elaborator.
So this enables something like split compilation like CUDA
and perhaps one day separate compilation of generics like Swift.
So the elaboration pass is an MLIR pass
that performs function instantiation as an IR transformation.
So in this piece of IR, we've got two calls to the function printint
with two different parameters.
They get stamped out into two new functions
and the callers are replaced appropriately.
One consequence of a pass to do elaboration
is that the language is late bound by design.
That poses a couple of language design challenges,
but that means that you can do cool stuff like autotuning
where any parameter value can be autotuned,
i.e. the elaborator says,
okay, width can be 2, 4, 8, 16 or 32.
Let me just go have five instantiations of this function
and then use some benchmarking to pick the best one for you.
So this is how we get the very bottom layer of hardware abstraction
where the programmer can write an algorithm
and then we let the programming language pick the best parameter for you.
And this also allows us to avoid
some of the performance problems of C++ templates.
For example, let's say you have a generic function add
and for generality, we pass the arguments by const reference.
Passing it by const reference is fine for a large struct type thing
that doesn't fit nicely in registers like a string.
But then for something like an integer,
this ends up becoming const reference to an int,
which for a trivial type like int is not very performant.
And so if this function doesn't end up getting inlined,
what ends up happening is the ints get pinned to the stack.
This is bad for performance.
With late elaboration and mojo,
we can have late ABI lowering,
which basically means that the source code is not the same as the ABI.
And this makes language interop slightly more involved,
but it's not that big of a deal.
But what it means is that for a generic function like add in mojo,
when the elaborator instantiates the generic types,
it can then change the calling conventions of the types
to respect the guarantees that it has.
So for a heavy type like string,
it stays in memory, it gets passed around as a pointer,
it's nice and efficient,
but for an integer, it gets passed around in registers,
in SSA registers, and returned out as a function result.
So that's just an introduction to how mojo metaprogramming works.
Let's talk now about more how the cogent architecture works
and some of the more unique details of that.
One of them is that the entire mojo compiler stack
is driven by the ORCJIT from bottom to top.
And this gives us lazy on-demand compilations,
so you don't compile things you don't have to,
it enables responsive tooling,
and it turns out that having a JIT is important
for something like auto tuning and search.
And we get compiler caching at each stage of the pipeline,
meaning that you don't need something like ccache
to get code compilation caching.
Well, we also use ORCJIT not actually as a JIT,
we use it to generate static code,
like static archives and executables.
And in the ORCJIT, we've built a really dumb but fast linker
that just takes a bunch of object files,
pulls out the symbols, and slams them together
into a static archive.
The linker we do call into the system linker.
As we mentioned before,
we have a pre-elaboration portable IR,
but that also means that we can serialize this
into MLR bytecode,
and that makes mojo packages architecturally portable.
A mojo package will contain this parser-level,
source-level IR, as well as the pre-elaboration IR,
and optionally, you have the post-elaboration
and pre-compiled code for various targets.
So what this means is you can ship mojo packages
without source code, with just the bytecode.
The parser is able to take out this source-level IR
and reconstruct metadata like function signatures
and type members and so on.
And with optimized and pre-compiled code in the packages,
mojo packages become like portable build caches.
So if you're on a common system like an M1 Mac,
and you pull a mojo package,
it will probably already have the pre-built code for you.
So what does a compilation with a package look like?
Well, if you start by importing a function from a package,
the parser goes and reads out the declarations from the package.
It will then lower into the full pre-elaboration IR,
and the reason why you need the full parametric IR
so that you can instantiate the function again,
and so that the library can call the interpreter
on pre-compiled code.
During elaboration, we don't re-optimize and re-stantiate
all the functions, we just drop them out
with the post-elaboration IR into the MLR module.
So that gives us LTO and MLIR.
But, I mean, MLIR is pretty far away from link time,
but it's a similar idea.
But we actually trash these pre-compiled functions out of the IR
before we go to LLVM.
And that has some interesting implications.
So mojo is a bit of an unusual,
probably slightly controversial user of LLVM.
So LLVM is fantastic.
We love LLVM, we love everyone here.
But it's got a couple of issues.
The most standout of these is that it's single-threaded.
And what that means is on a modern system,
like an AWS 192 core machine,
you get arbitrary slowdown for compilation speeds.
You only use one core.
The other problem with LLVM is it's got a couple of passes
that don't tend to be strong enough for our use cases,
and that are difficult to control and predict.
A lot of the stuff in LLVM was built for something like Clang,
but in mojo, we'd really love to be able to auto-tune
and unroll factor.
The good news is that MLIR is a thing.
So let's focus on the excellent strengths of LLVM.
LLVM is great at stuff like scalar optimizations,
from instance to combine,
and other function-level optimizations,
like loop strength reduction.
We ended up disabling passes like the vectorizer,
the loop unroller, and even the inliner,
as well as a couple of the other IPO passes.
And the solution is to replace them in MLIR,
where we get intrapass parallelism,
and push many of these optimizations out into the library,
which is something that Abdul will talk about in a bit.
So what happens when you get rid of all of the IPO passes?
Well, you get to use LLVM as a perfunction code generator.
This gives you full code gene parallelism at a function level
across the entire stack.
And what that means is that pretty much the entire mojo compiler pipeline
is fully paralyzed, except for the linker and the parser.
The parser could be paralyzed one day.
And that's really just the tip of the iceberg
and what we could fit into one presentation.
There's so much more to mojo,
and there'll probably be more talks coming in the future.
But for now, I'll pass it over to Abdul
to show you all how to write some fast code in mojo.
So going back to what Chris said at the very beginning,
we had a hypothesis to begin with.
We want to write fast code.
That's why mojo was written to begin with.
We wrote things when MLIR, we've proven a lot of the tech.
Let's write things in mojo and let's show the performance.
So let's step back. How does existing performance libraries,
how are they built today?
Well, the short answer is whatever it takes to get performance.
There's no style guide or anything like that
that's usually maintained.
And that also means there's a lot of suffering
because there's lack of tooling, et cetera.
So what people do is they write things in assembly.
Oh, great.
But please don't.
It's not a super productive programming language.
Others build compilers as C++ templates.
And God forbid, you mess like one of the sevens becomes a six,
and you get some nasty error message.
Others build C++ DSLs that generate ASMs.
Others write Python programs that generate assembly.
Others write Python templates that generate C++ templates
that then you feed into client.
And these are not like research projects.
These are production libraries that are used today.
You probably used one already.
These are by the big companies.
And as a result, you're kind of losing a lot of things.
You lose on maintainability, debugging, tooling,
and becomes hard to develop and iterate
on these performance libraries.
And that's why they call them performance ninjas.
You lock them in a room, give them some coffee,
and then they give you speed up.
And we don't want to do that.
We want to reduce suffering.
The other thing is what happens is these performance libraries
are pre-built and shipped as kind of black box binaries.
And what that means is you've encoded,
when you built ahead of time,
you've encoded all the hardware semantics, tile factors,
et cetera, in the library.
You've made it into a black box,
so other higher-level things in the stack,
like a graph compiler, cannot reason about what the library is doing.
You've also encoded specialized patterns,
popular things like a ResNet block
or a Transformer block into your library.
And what happens if there's a Transformer version 2
or a ResNet 53?
You're kind of screwed in that domain.
There's other things, like there's no consistent API.
There's BLOSS, there's BLIS, there's 1DNN, et cetera.
And the distribution store is even worse.
There's a 1DNN and there's a ZNDN.
But then if you are on ARM,
you have to use something else as well.
So we want to solve all of these things.
And that's the reason why we built Mojo.
We built it to solve our problem of writing high-performance libraries.
And the first thing we want to make sure is the developer is happy.
And they have all the tools that they need to be productive.
So rather than, as kind of Chris mentioned,
a lot of developers are not compiler engineers.
They can write libraries, they probably cannot go
and write a pass and so on.
So let's put optimizations in the library
and I'll have some examples later on.
Let's also leverage what computers are good at.
So when I was in grad school,
a lot of grad students were essentially grid searchers.
They would just enumerate everything, try 50 things.
You lock them again in a room for a month
and they say, oh, the best tile factor is 6 and 4 and so on.
Let's not do that.
Let's use computers.
Computers are great at these sort of things.
They can scan things.
You can do smart searches and so on.
So let's use auto-tuning.
Let's use algorithmic selection and let's build that in the language.
And let's make sure that we have tooling to make these people productive.
Debuggers, how do you debug the Python template
that generates C++ template that does something else?
It's hard to begin with to debug C++ templates.
Let's also build a language that's aware of the 21st century.
So SIMDs are a thing.
So let's be SIMD first.
Let's have scalers to be a degenerate form of SIMD,
a SIMD of length 1, and make the SIMD parametric.
Let's also make the library, the one we ship, the standard library,
have first class support for SIMD types.
Also multi-core is a thing.
So let's build parallelism and asynchrony into the language as well.
And finally, we can have these nice things,
but sometimes people are like, I want my assembly back
or I want to use the LLVM intrinsic.
Well, all of this is built on top of MLIR and LLVM,
so you can get any of the intrinsics that you want.
You can reach into them.
You can also write inline assembly,
which is kind of interesting given that you're in a Python syntax language.
And you can target any LLVM backend.
So we're not like we're standing on the shoulders of giants.
So we're leveraging all LLVM and MLIR backend infra to do that.
Let's also not build a DSL.
So even though some of our use cases is AI,
the programming language should be general.
I should be able to do some operations in Mojo,
but then do the plotting through our Python integration.
That requires a general purpose programming language.
So one of the things that we made a decision on
is let's make the compiler lean
and let's move a lot of the optimizations and the infra
to be functions in the Mojo library.
So we use very limited number of dialects in MLIR core,
and I know this might be controversial.
So we're not using vector, eryth, lin-alg,
or any of these dialects, MVVM, any of these dialects.
We're only using the LLVM and index dialect.
And there's a bunch of reasons for them.
Sometimes they're not general enough.
Sometimes they don't fit in our use case.
They bring in a lot of code that we don't care about,
and there's like, you know, for the lack of better terms,
sometimes like cyclic dependencies and so on.
And we, you know, having a lot of the functionality
in Mojo code means you could iterate a lot more quickly.
So let's like implement something like a vector
dialect type of thing in Mojo.
So we have the SIMD type,
and we have a function called reduce max.
And in, you know, if the size of the width of the SIMD vector
is one, we're just going to return the scalar directly.
If we're on x86, it ends up like there's a, you know,
LVM has an instruction for horizontal addition
or horizontal max.
That's not great for Intel,
so we could do a kind of a tree reduction thing.
But if it's floating points, we use a different algorithm,
and we call it directly to an LLVM intrinsic.
This is compared to, you know, how the vector dialect lowers.
You're writing essentially the same stuff
minus the special case for x86 in essentially C++ code
to lower our directory to the LLVM dialect.
We could also do similar things like transforms.
So as Jeff mentioned, we disabled the LLVM vectorizer,
and instead we have folks be, you know, kind of opt-in
to the vectorizer.
And we've implemented a vectorizer in, you know,
these like five lines of code.
So in one case, we are going to,
we've parameterized the function on the SIMD width,
and we're going to call it when, you know,
for the specific SIMD width, and the leftovers,
and we're going to call the function with a value of one.
So what does this mean to the developers?
It means that when you're trying to do an optimization,
when you're trying to add a new feature or target a new hardware,
the first thing is not, oh, I'm going to need to write a dialect,
or I'm going to reach into Table Gen.
The first thing is I'm going to reach into Mojo,
and I'm going to do experiments and so on.
You can invent new optimizations, weird ones, incorrect ones,
or maybe even point optimizations that only works in this function,
in this domain, in this context.
This is all fine, but I care about performance.
I'm also a compiler engineer, but I ultimately care about performance.
So let's look at the performance of Mojo.
So one thing that people anchor on is the Mandelbrot set.
The Mandelbrot set, you know,
we have a blog post that was recently published,
but essentially at the end of the blog post,
you end up with this 10 lines of code.
And if you run this 10 lines of code,
you get 68,000 times faster than Python.
And you can kind of see the progression.
You can look at the blog post after this presentation.
There's a progression how to go to 90x faster all the way to 68,000 faster.
But at the end of the day,
this is the code that you're going to see.
But nobody cares about Mandelbrot.
You know, there's ways to cheat in Mandelbrot.
We're not cheating here, but nobody cares about Mandelbrot.
So let's solve a hard problem.
So let's look at matrix multiplication.
So matrix multiplication has been studied since a lot of us have been born.
There's also a lot more papers that were published this year about matrix multiplication.
It's also difficult.
The problem is dependent on the cache size and microarchitecture.
It's also a core part of LA-PAC and the ML system,
which means hardware companies to go in the top 500 supercomputers,
they have to optimize MathMol.
Or to be on the top of the ML perf, they need to optimize MathMol.
So a lot of effort goes into optimizing MathMol.
And these libraries have been developed for decades.
Before some of us were born as well.
But we also don't want to write the Python template that generates C++ template
that maybe goes to Python again and so on.
Let's be principled.
So let's have a few kind of core things that we want from our MathMol.
We want a single source of truth.
We don't want to have multiple files.
We want to have one implementation.
We want it to be as fast or compete with state-of-the-art.
Even though we can read assembly and we can program C++,
let's not do that.
Let's write everything in mojo.
Let's make it fusible and do fancy stuff,
support dynamic shape, work on multiple architectures, et cetera.
Our core hypothesis from the very beginning.
And here's what we ended up with.
So this is again a blog post from a few months ago.
We're actually faster than this now,
but we can compare against the best in class on their hardware.
So we're 1.4x faster than Intel on Skylake systems.
And this is fully dynamic.
We're not specializing on shape.
We're not doing prepacking.
I wish we were doing tricks.
It's easy to get these results if we were doing tricks,
but that's what we're doing.
We have no line assembly.
Now let's run the same code, but now on Intel,
or sorry, on AMD, or 1.6x faster.
Do the same thing, but on ARM, or 1.2x faster.
In fact, our implementation is about 2,000 lines of code.
This is a toy implementation, but this is putting everything together.
The interesting thing about this toy implementation
is this is what the llama.mojo,
there's a public GitHub repo that's using this.
And this implementation beats, using this,
they are beating the llama.cpp implementation that's public.
So with that, we've validated our hypothesis.
You can build portable performance libraries with less suffering.
And with that, I'm going to hand it off to Chris.
Thank you.
Awesome.
So to wrap things up, mojo is still early in development.
As we talked about, there's still a lot more that is yet to be done.
One of the things we're doing that's, I think, pretty cool
is we're developing this all in public.
And so we have a roadmap.
You can go see what we're doing.
We have new releases that come out very frequently.
Now one of the questions we get asked all the time
is does a modular open source anything, right?
And so the answer comes in twofold.
One is yes.
We have upstream stuff all of the time,
including tons of core improvements to MLIR.
And apparently the interpreter that Jeff was talking about on Tuesday
is very popular, and so we can work on that.
And so we're very good open source citizens from that respect.
Mojo itself, I think we'll take a little bit longer,
but we want to start the open source process later this year.
And so we'll start working on that.
And I expect that to take some time,
because we want to make sure that we get the core design really right.
And not everything is best done with design by committee,
but we really want to see this thing scale and go
and have a big impact for the world.
So coming back all the way to the beginning,
we talked about AI and the AI engine and this kind of stuff.
Now we don't have time to talk about it today,
but the cool thing about what Mojo means for the AI engine
is that you can actually tackle these heterogeneous compute problems
because you can finally scale across lots of different hardware.
And this is really cool.
Don't have time to talk about it today.
If you're interested, we have a keynote at the NURBS conference
later this year where we'll talk about more about this in detail.
So with that, I think that's the end of our talk,
and we're very happy to take any questions.
If you'd like to check out Mojo, you can go to the web page,
read about it, download it, and use it today.
Thank you.
Thank you.
Thank you, Chris, Abdul, and Jeff.
Are there any questions?
Do you have mics in the alleys?
Good timing. Yeah, thanks.
Thanks for...
This test?
My question is, I haven't seen anything about GPU offloading
in your slide. Is that in plan, or what are you intent to do with it?
So there is one bullet point, actually, on that there's so much more,
and yeah, Mojo does actually support GPU offloading
and split compilation like CUDA,
but it's something that we did not talk about in the presentation
and should like to talk about in the future.
Thank you.
Hi, you mentioned that you don't need to use Ccash
because you kind of mentioned that.
Can you elaborate that a little bit,
like how you guys deal with caching?
So it turns out that MLR has a nice,
serializable format called bytecode,
but bytecode provides a predictable hashing,
and so we can use MLR bytecode as the form to hash and cache
compiler transformations across the stack.
Okay, thank you.
We also didn't have time to talk about this whole distributed
Cache backing this thing, and there's a whole bunch of fancy stuff.
Put it into it.
Hi, how are you doing the autotuning?
Is it offline, or is it dynamically online,
and how do you define the objective function for the search?
Yeah, so you have a choice.
You could do it offline or online.
If you compile to like .ofile, you've done it offline.
The objective function right now is something that the user provides
because it's data size dependent,
hardware dependent, and so on.
So it's up to you to define that.
We do provide a benchmark module
so that it makes benchmarking a lot simpler,
and that allows you to do that.
Yeah, if you're doing it online,
how do you control for like variation in data?
So the benchmark library that we provide has a good number of iterations
and so on until you get stability and so on.
So it handles that.
So it's not actually in production autotuning?
We use autotuning today, so I don't know what.
So I mean, there's core capabilities and there's future stuff also.
I mean, one of the things that it's designed for,
and it's actually done is the like send the IR to an FPGA
and do evaluation remotely and then pull it back and things like this.
Or a simulator.
Exactly.
Yeah.
It's a great talk.
There was a point in the slide about optimization in the,
providing optimization in the library as opposed to the compiler.
Are there any, maybe I misunderstood this,
but from my understanding, it's possible to come into like performance pitfalls
because C++ has built in likely, built in unlikely,
and then you can, it's really easy to misuse those and end up in a situation
where your code is slower than, without these kinds of annotations.
So my question would be, what happens if a user-provided annotation
conflicts with something that the compiler would also have done at the same time?
Well, so from a compiler design perspective,
one of the things Jeff was talking about is we've removed, not all,
but a lot of the super unpredictable things in the elegant optimizer.
So our goal is to give full control and predictability to the programmer,
which is very different from the make spec go fast kind of approach to compiler design.
And what that does is that gives you the ability to then go and design library features
that do things like, you know, you can,
you can talk about some of the crazy stuff you've done.
What's also important is that we have these abilities to say,
please vectorize this loop, please unroll this loop and so on.
But not everyone who's writing, say application code is going to think
about vectorizing every single loop and auto-tuning every other loop.
So what's important is that we provide control to the users who care,
but also provide a default experience that is like good and optimal
and the compiler does its best.
But the important thing is what the user says will always take precedent,
and that's how you get control.
Sometimes the compiler does things and you end up with code
that says, you know, optimize, you know, compile the section of code
with dash O zero type of stuff.
And you kind of want to opt out of compiler optimization
because it's interfering with how you laid out your code.
Are there any plans?
I have a follow up question.
Sure.
Last question, please.
Hi.
So you mentioned that you only use two dialects in Mojo.
The LLVM and index dialect.
Two upstream dialects.
Two upstream dialects.
So you only use other things like affine and stuff,
which means that if you want to use hardware specialized libraries,
then the programmer has to do different dialing for ampere versus hopper
versus Volta and so on.
So isn't that just pushing the burden out from the compiler
and high level stuff into the programmer?
Yes, that's exactly what that is.
Very hardware specialized performance libraries
and then people who write this thing would have to understand
the architecture really, really well.
I think the thing is that they're more likely to understand
the architecture really well than the compiler engineer.
The compiler engineer has to have two things,
writing C++ on CPUs that target GPUs.
This is like, I'm a CUDA programmer, I'm laser focused,
so I see the trade off.
So that means that the people writing high performance libraries
for very specialized accelerators,
they need to be experts at those accelerators.
Right, so they need to be expert in one area, not two areas.
So the goal is to give the programmer superpowers.
But that's our approach to it.
As Jeff talked about, Mojo can talk to any dialect if you want to.
You can use affine in Mojo.
We can plug and extend the system with dialects as well,
so that's always an option.
That's really the conscious decision you're making,
that you want to get experts to do the performance library,
and they will just work.
Well, so this is the thing.
Kernel libraries don't scale because of the magnitude of the problem,
and the cross product of all the different integrations
and all of the stuff that kernel libraries struggle with.
But there are more kernel programmers and performance
engineers than there are compiler engineers by far.
Right, and so it's really about enabling the talent
that actually knows how to do all this kind of stuff,
versus having a compiler engineer in the loop
that becomes a bottleneck.
Thanks.
We'll be around as well throughout the conference,
so feel free to yank any of us.
Thank you, Chris, Aboul and Jeff.
Let's thank the speaker again.

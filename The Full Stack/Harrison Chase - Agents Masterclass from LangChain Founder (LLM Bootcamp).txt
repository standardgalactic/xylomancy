So, I'll be talking about agents, and yeah, there are many things in the lane chain,
but I think agents are probably the most interesting one to talk about,
so that's what I'll be doing.
I'll cover kind of like what are agents, why use agents, the typical implementation
of agents, talk about React, which is one of the first prompting strategies
that really accelerated and made reliable the use of agents.
Then I'll talk a bunch about challenges with agents and challenges of getting them
to work reliably, getting them to work in production.
I'll then touch a little bit on memory and segue that into more recent kind of like papers
and projects that do agentic things.
I'll probably skim over the initial slides because I think most people here are probably familiar
with the idea of agents, but the core idea of agents is using the language model
as a reasoning engine, so using it to determine kind of like what to do and how
to interact with the outside world.
And this means that there is a non-deterministic kind of like sequence of actions
that that'll be taken depending on the user input.
So there's no kind of like hard coded do A, then do B, then do C,
rather the agent determines what actions to do depending on the user input
and depending on results of previous actions.
So why would you want to do this in the first place?
So one, there's this very tied to agents is the idea of tool usage and connecting it
to the outside world, and so connecting it to other sources of data or computation
like search, APIs, databases, these are all very useful to overcome.
Some of the limitations of language models such as they don't know about your data,
they can't do math amazingly well, but the idea of tool usage isn't kind of like unique to agents.
You can still use tools, you can still connect LLMs to search engines without using an agent.
So why use an agent?
And I think some of the benefits of agents are that they're more flexible, they're more powerful,
they allow you to kind of like recover from errors better, they allow you to handle kind
of like multi-hop tasks, again with this idea of being the reason the engine.
And so an example that I like to use here is thinking about like interacting with a SQL database.
You can do that in a sequence of predetermined steps.
You can have kind of like a natural language query which you then use a language model
to convert to a SQL query which you then pass, you then execute that, get back a result,
pass that back to the language model, ask it to synthesize it with respect to the original
question and get kind of this natural language wrapper around a SQL database.
Very useful by itself, but there are a lot of edge cases and things that can go wrong.
So there could be an error in the SQL query, maybe it hallucinates a table name or a field name,
maybe it just writes incorrect SQL.
And then there are also queries that need multiple kind of like queries to be made
under the hood in order to answer.
And so although kind of like a simple chain can, you know, handle maybe, I don't know,
50, 80% of the use cases, you very quickly run into these like edge cases
where a more flexible framework like agents helps kind of like circumvent.
So the typical implementation of agents generally, and it's so early in this field
that it kind of feels a bit weird to be talking about a typical implementation
because I'm sure we'll see a bunch of different variants, but generally, you get a user query,
you use the LLM, that's the agent to choose a tool to use, and also the input to that tool.
You then do that, you take that action, you get back an observation, and then you feed
that back into the language model and you kind of continue doing this until a stopping condition is met.
And so there can be different types of stopping conditions.
Probably the most common is the language model itself realizes, hey,
I'm done with this task, with this question that someone asked of me,
I should now return to the user, but there can be other more hard-coded rules.
And so when we talk about like reliability of agents, some of these can really help with that.
So, you know, if an agent has done five different steps in a row and it hasn't reached a final answer,
it might be nice to have it just return something then.
There are also certain tools that you can just like return automatically.
So basically, the general idea is choose a tool to use, observe the output of that tool,
and kind of continue going.
So how do you actually like, so that was pseudocode.
Now let's talk about the actual kind of like ways to get this to do, to do what we want.
And the first and still kind of like the main way of doing this,
the main prompting strategy slash algorithm slash method for doing this is React,
which stands for reasoning and then acting.
So RA from reasoning, ACT from acting.
And it's the papers, a great paper came out in, I think, October out of Princeton,
about synergizing two different kind of like methods.
And we can look at this example, which is taken from their paper and see why it's so effective.
So this example comes from the hotspot QA data set, which is basically a data set
where it's asking questions over Wikipedia pages where there are multi-hop, usually kind
of like two or three intermediate questions that need
to be reasoned about before answering.
So we can see, so here there's this question aside from the Apple remote,
what other device can control the program Apple remote was originally designed to interact with?
And so the most basic prompting strategy is kind of just pass that into the language model
and get back and answer.
And so we can see that in 1A standard and it just returns, you know, a single answer
and we can see that it's wrong.
Another method that had emerged maybe like a month or so prior was the idea
of like chain of thought reasoning.
And so this is usually associated with the let's think step by step prefix to the response
and you can see in red that there's this like chain of thought thing that's happening
as it thinks through step by step and it returns an answer that's also incorrect.
In this case, this has been shown to kind of like get the agent or get the language model
to think a little bit better, so to speak.
So it's yielding higher quality, more reliable results.
The issue is it still only knows kind of like what is actually present in the data
that the language model is trained on.
And so there's another technique that came out which is basically action only
where you give it access to different tools.
In this case, I think all the examples in this picture are of search, but there's,
I think in the paper it had search and then look up.
And so you can see here that the language model outputs kind of like search Apple remote.
It looks up Apple remote, it gets back in observation, it then does search front row,
can finds that, so that's an instance of it kind of like recovering from an error.
And then it does search front row software, finds an answer, and then finishes.
And you can see here that the output that it gave, yes, kind of loses some
of what it was actually supposed to answer.
So you've got this chain of thought reasoning which helps the language model think about what
to do, then you've got this action taking step that basically actually allows it to plug
into more kind of like sources of real data, what if you combine them?
And that's the idea of React, and that's the idea of a lot of the popular agent frameworks.
Because again, agents use a language model as a reasoning engine,
so you want it to be really good at reasoning.
So if there are any prompting techniques you can use to improve its reasoning,
you should probably do those.
And then the big part of it is also connecting into tools, and that's where action comes in.
And so you can see here, it arrives at kind of like the final answer.
So that's the idea of agents.
React is still one of the more popular implementations for it.
But what are some of the current challenges?
And there are a lot of challenges.
Like I think most agents are not amazingly production ready at the moment.
And these are some of the reasons why.
And we can, I'll walk through them in a bunch more detail.
I'll also leave a lot of time at the end for the questions.
So I'd love to hear kind of like what you guys are observing as issues
for getting agents to work reliably.
This is probably far from a complete list.
So the most basic challenge is the agents is getting them to use tools in appropriate scenarios.
And so, you know, in the React paper, they address this challenge by bringing
in the reasoning aspect of it, the chain of thought, style, prompting, asking it to think.
Kind of like common ways to do this include just like saying
in the instructions, you have access to these tools, you should use these
to overcome some of your limitations and just basically instructing the language model.
Tool descriptions are really, really important for this.
If you want the agent to use a particular tool, it should probably have enough context
to know that this tool is good at this.
And that generally comes in the form of like tool descriptions or some information
about the tool that's passed into the prompt.
That can maybe not scale super well if you've got a lot of tools.
Because now you've got maybe these more complex descriptions.
You want to put them in the final prompt for the language model to know what to do with them.
But you can quickly run into kind of like context length issues.
And so that's where I think the idea of tool retrieval comes in handy.
So you can have hundreds, thousands of tools.
You can take in, you can do some retrieval stuff.
And I think retrieval is another really interesting topic that I'm not going
to go into too much depth here.
So for the sake of this, we'll just say it's some embedding search lookup.
Although there's, I think there's a lot more interesting things to do there.
You basically do the retrieval step.
You get back five, ten, however many tools that you think are most promising.
You can then pass those to the prompt and kind of have the language model take the final steps from there.
Few shot examples I think can also be really helpful.
So you can use those to guide the language model in what to do.
Again, I think the idea of retrieval to find the most relevant few shot examples is particularly promising here.
So if you give it examples similar to the one it's trying to do,
those help a lot better than random examples.
And then probably the most extreme version of this is fine-tuning a model
like, like tool former to really help with tool selection.
There's also a subtle second challenge, which is getting them not to use tools when they don't need to.
So a big use case for agents is having conversational style agents.
One of the big problems that we've seen is oftentimes these types of agents just want
to use tools no matter what, even if they're having a conversation.
And so again, like the most basic thing you can do is probably put some information in the instructions,
some reminder in the prompt like, hey, you don't have to use a tool.
You can respond to the user if it seems like it's more of a conversation.
That can get you so far.
Another kind of like clever hack that we've seen here is add another tool
that explicitly just returns to the user.
And then, you know, they like to use tools, so they'll, but they'll usually use that tool.
So I thought that was a pretty clever and interesting hack.
A third challenge is the language models tell you what tool to use and how to use it.
But that's in the form of a string, and so you need to go from that string
into some code or something that can actually be run.
And so that involves parsing kind of like the output of the language model into this tool invocation.
And so some tips and tricks and hacks here are one, like the more structured you ask
for the response, the easier it is to parse generally.
So language models are pretty good at writing JSON.
So we've kind of transitioned a few of our agents to use that schema.
Still doesn't always work, especially some of the chat models like to add
in a lot of kind of like language.
So we've introduced kind of like this concept of output parsers,
which generically encapsulate all the logic that's needed to parse this response.
And we've tried to make that as modular as possible.
So if you're seeing errors, you can hopefully kind of like substitute that out.
Very related to that, we also have a concept of like output parsers
that can retry and fix mistakes.
So, and I think there's some subtle differences here that I think are really cool.
Basically like, you know, if you have misformatted schema, you can fix that explicitly
by just passing it the output and the error and saying fix this response.
But if you have an output that just forgets one of the fields,
like it returns the action, but not the action input or something like that,
you need to provide more information here.
So I think there's actually a lot of subtlety in fixing some of these errors,
but the basic idea is that you can try to parse it.
If it fails, you can then try to fix it.
All this we currently encapsulate in this idea of like output parsers.
So the fourth challenge is getting them to remember previous steps that were taken,
the most basic thing, the thing that the React paper does is just keep a list
of those steps in memory.
Again, that starts to run into some context window issues,
especially when you're dealing with long running tasks.
And so the thing that we've seen done here is again fetch previous steps
with some retrieval method and put those into context.
Usually we've actually seen a combination of the two.
So we've seen using the N most recent actions and observations combined
with the K most relevant actions and observations.
Incorporating long observations is another really interesting one.
This actually comes up or this came up a lot when we were dealing with working with APIs
because APIs often return really big JSON blobs that are really big and hard to put in context.
So the most common thing that we've done here is just parse that in some way.
You can do really simple stuff like convert that blob to a string
and put the first like thousand characters or something as the response.
You can also do some more, if you know that you're working with a specific API,
you can probably write some custom logic to kind of like take only the relevant keys and put that.
If you want to make something general, you can also maybe do something dynamically
to like figure out what key, like basically explore the JSON object and figure out what keys
to put in that's a bit more exploratory, I would say.
But the basic idea is, yeah, there is this issue of, and so Zapier,
I always have to think about how to pronounce it, but it's Zapier makes you happier.
So Zapier, when they did this with their natural language API,
not only did they have something before the API that was like natural language to some API call,
they also spent a lot of time working on the output.
And so the output is actually very specifically, I think it's like under like 200 or 300 tokens
or something like that, and they did that on purpose.
They spent a lot of time thinking about that.
And so I think for tool usage, that is really important as well.
Another more kind of like exploratory way of doing this is also you could perhaps just store the
long output and then do retrieval on it when you're trying to think of like what next steps to take.
Agents can often go off track, especially in long-running things.
And so there's kind of like two methods that I've seen to kind of like keep them on track.
One, you can just reiterate the objective right before it makes its action.
So and why this works, I think we've seen that with, at least with a lot of the current models,
with instructions that are earlier in the prompt, it might forget it by the time it gets to the end
if it's a really long prompt.
So putting it at the end seems to help.
And then another really interesting one that I'll talk about when I talk about some
of the more recent papers and stuff that have come out is this idea of separating explicitly a planning
and execution step and basically have one step that explicitly kind of thinks about,
these are kind of like all the objectives that I want to do at a high level.
And then a second step that says, okay, given this objective, given this one sub-objective,
now how do I do this one sub-objective?
And basically break it down even more in a higher or a whole manner.
And there's a good example of that with baby AGI, which I'll talk about in a bit.
And then another big issue is evaluation of these things.
I think evaluation of language models in general, very difficult evaluation
of applications built on top of language models, also very difficult and agents are no exception.
I think there's the obvious kind of like evaluate whether it arrived at the correct result in terms
of getting metrics on evaluation.
And so, yeah, you know, if you're asking the agent to produce some answer,
that's like a natural language response.
There's techniques you can do there, a lot of them in the flavor of asking a language model
to score the expected answer and the actual answer and come up with some grade and stuff
like that that applies to the output of agents as well.
But then there's also some agent-specific ones that I think are really interesting,
mostly around evaluating these idea of like the agent trajectory or the intermediate steps.
And so, we'll actually have something coming out for this, someone opened a PR that I need
to get in, but basically there's a lot of like little different things you can look at.
Like did it take the correct action?
Is the input to the action correct?
Is it the correct number of steps?
And by this, you know, like sometimes you, and this is very related to the next one,
which is like the most efficient sequence of steps.
And so, there's a bunch of different things that you can do to evaluate not only the final answer,
but like is the agent getting there like efficiently, correctly?
And those are sometimes just as useful, if not more useful than evaluating the end result.
I'm trying to see what time it is, because I also want to leave lots of time for questions.
But I think I'm good.
So, memory, I think is really interesting as well.
So, we've obviously tried it about like memory of remembering the AI to tool interactions.
There's also like a more basic idea of remembering the user to AI interactions.
But I think the third type, which is showing up in a lot of the recent papers on agents is this idea
of like personalization, of giving an agent kind of like its own kind of like objective
and own persona and stuff like that.
The most obvious way to do that is just like you encode it in the prompt.
You say like, hey, like, you know, this is your job as an agent.
You're supposed to do this, yada, yada, yada.
But I think there's some really cool work being done on how to kind of like evolve that over time
and give agents a sense of like this long-term memory.
And one of the papers in particular around gender defasions, I think does a really interesting job
of diving into this.
And I think when a lot of people, the reason this is here in the agent section is I think
when people think of agents, there's the obvious like kind of like tool usage deciding what to do.
But I think agents is also starting to take on this concept of some kind of like more encapsulated kind
of like program that adapts over time and memory is a big part of that.
And so I think memory is, there's a lot to explore here.
So that's why this is a bit of an outlier slide.
Okay. I wanted to chat very quickly about four projects that came out in the past two, three weeks.
Specifically, how they relate, build upon, improve upon the React-style agent that has been around for a while.
First up is auto-GPT, which I'm assuming most people have heard of.
There we go.
All right.
So auto-GPT, one of the main differences between this and the React-style agents is just the objective
of what it's trying to solve.
Auto-GPT, a lot of the initial goals are like, you know, improve or increase my Twitter following
or something like that, very kind of like open-ended, broad, long-running goals.
React, on the other hand, was designed and benchmarked on more kind of like short-lived kind
of like really immediately quantifiable or more immediately quantifiable goals.
And so as a result, one of the things that auto-GPT introduced is this idea of long-term memory
between the agent and tools interactions and using a retriever vector store for that,
which becomes necessary because now you have this doing like 20 or 30 kind of like steps
and it's this really long-running project.
And so it's something that React just didn't need, but due to the change
in objectives auto-GPT kind of had to introduce.
BabyAGI is another popular one.
It also has this idea of long-term memory for the agent tool interactions.
And this is the project that introduced separate kind of like planning and execution steps,
which I think is a really interesting idea to improve upon some of the long-running objectives.
And so specifically, it comes up with tasks, it then takes the first task,
it then thinks about how to do that, which usually involves, actually,
BabyAGI initially didn't have any tools, so it kind of just like made stuff up.
I think they're giving it tools now, so it can actually execute those things.
But the idea of separating the planning and execution steps is, I think that's a really
interesting idea that might help with some of the reliability and focus issues of longer-term agents.
CAML is another paper that came out.
The main novel thing here was they put two agents in a simulation environment,
which in this case, because it was just two, was just a chat room and had them interact with each other.
And so the agents themselves, I think, were basically just kind of like prompted language models,
so I don't even think they were hooked up with tools.
But going back to this idea of kind of like memory and personalization,
when people kind of like talk about agents, that is part of what they're talking about.
And so I think like the CAML paper, in my mind, the main thing is this idea of simulation environment.
There's maybe like two reasons you might want to do this and have a simulation environment.
One is kind of like practically to maybe like evaluate an agent if you're kind of like testing out an agent
and you want to see how it's interacting.
And for whatever reason, you don't want to test it out yourself.
So you put two of them and you kind of like make sure they don't go off the rails or something like that.
Another one is just kind of entertainment purposes.
So there are a lot of examples of this by people.
I think there was one with like a VC and a founder and had them chatting with each other and kind of like solving stuff there.
So this is a little bit entertainment, a little bit practical.
Generative agents was another paper that came out.
I think this was maybe like a week and a half ago, so very recent.
It also had a simulation environment aspect.
It was more complex.
So I think they had like 25 different agents and kind of like a Sims-like world interacting with each other.
So a much more complex environment setup.
And then they also did some really cool stuff around memory and reflection.
So memory refers to basically remembering previous things that happened in the world.
So basically in the simulation environment, they had kind of like things that happened.
Then they had the agents decide what to do, take actions, observe kind of like the results of those actions, observe more things that came in.
And so all of this is encapsulated in the idea of memory.
And then you fetch things from this memory to inform kind of like their actions in next time sequences.
So there's three kind of like main components to this memory retrieval thing.
They had a time-weighted component, which basically fetched more recent memories.
They had an important weighted piece, which fetched more like important information.
So, you know, like trivial things like I forget what I had for breakfast today, but I don't know what's something that's really,
but I remember meeting Charles way back when, right?
So there's different levels of importance there that get subscribed to events.
And so you want to fetch events that are kind of like more bigger in importance.
And then they had the typical kind of like relevancy weighted things.
So depending on what situation you're in, you want to remember events that are relevant for that.
Then they also introduced a really interesting reflection step, which basically after like, I think they,
I think it was like 20 different steps or something happened, they would reflect on those things and kind of like update different states of the world.
And so I think this is, I've been thinking about this a bit because I think this idea of like reflecting on recent things and then updating state
is maybe like a generalization that can be kind of like applied to a bunch of different things.
So some of the other memory types that we have in LangChain are we have like an entity memory type,
which basically based on conversation kind of like extracts relevant entities and then constructs some type of graph and updates that.
There's a more general kind of like knowledge graph version of that as well.
And then we also have kind of like a summary conversation memory, which based on the conversation updates a running summary so you can get around some of the context window lengths.
And so I think if you look at it through a certain angle, all of those kind of like relate to this idea of taking recent observations and updating some state,
whether that state is like a graph or just a piece of text or anything like that.
So there's also been some other papers recently that have incorporated this idea of like reflection.
I haven't had time to read those as carefully, but I think that's, yeah, I don't know.
My personal take is I think that's really interesting and something to keep an eye out for the future.
And that's it. I have no idea what time it is because I can't see the time, but I'm happy to take questions until Charles kicks me off.

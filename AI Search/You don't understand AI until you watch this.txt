This one video is going to explain all of these questions for you.
How does AI work?
How does AI learn?
How does chat GPT work?
How does image generation work?
Does AI actually copy or steal art or other content?
I know a decent portion of artists out there do not like AI.
Some of them are quite hostile towards AI
because they think that AI is stealing their work or their art style.
Another group that does not like AI very much are, for example, publishers.
I'm not saying all of them, but some of them like New York Times, for example,
they claim that open AI is copying their content
and they're now suing open AI for this.
But is this really the case?
Is this a valid argument?
Also, can AI solve unsolvable math problems?
For example, in a previous video, I talked about this leaked document
which claims to be about this mysterious Q-star project that open AI was working on.
Now, whether this is true or not is not the point of this video,
but this document was quite controversial
because it claims that this team trained an AI
that was able to break encryption systems.
These are systems that secure our passwords, our bank accounts,
the internet, government data, etc.
Now, as far as we know, there's no mathematically viable way
to really hack this systematically.
The only way is to brute force guess all the different possibilities of passwords.
This video will explain can AI actually do this?
Can it actually break encryption or solve these other math problems
which right now we believe are mathematically unsolvable?
Also, we'll talk about can AI beat humans at everything?
Can AI eventually be so good that it can outperform humans at any task?
And finally, is AI conscious or self-aware or sentient?
Make sure you stick to the end,
because the explanation to this is going to be very juicy.
We'll cover all of this in easy to understand terms.
Now, if you're an AI scientist or an engineer,
you probably know most of this,
but for the rest of us, this video will give you a deeper understanding of AI.
So the essence behind all AI we know today,
whether it's chatGPT or mid-journey or stable diffusion or Sora or AlphaFold,
the backbone of all of these AI systems is the neural network.
A neural network looks like this.
It's basically layers of nodes.
So each point here is called a node and each line of nodes is called a layer.
And each node is interconnected with one another through these linkages.
And the neural network is actually designed based on the human brain,
except for nodes and linkages in the human brain.
It's just a network of neurons and synapses.
So you can see this is a microscopic photo of a human brain
and you can see all these different nerve cells being connected in this very dense network.
A neural network is basically the same structure as this,
except that it looks like this instead of a bunch of cells in this bloody glob of an organ.
Now, how exactly does an AI work?
Let's start with a very simple example.
Let's say we have a neural network,
which is trained to identify images of cats versus dogs.
And don't worry, I'll talk a lot more about how we train an AI in a second.
But first, let's just go over how this works.
So let's say we input or we feed this neural network with an image of a cat.
This image would actually be broken down into data
and the data will flow through each of these nodes.
And after it flows through the first layer of nodes,
it will flow through the second layer of nodes
and then the next layer of nodes and then the next layer
and so on and so forth until it reaches the final layer,
in which case it would calculate the values of this
and based on the values of the final layer, it would spit out an answer.
This is a cat.
In fact, you can think of each of these nodes and links as dials and knobs
that determine how much data flows through to the next layer.
If you think of this in like realistic terms,
and I'm not saying this is how a neural network works,
but you can think of this node, for example, as the shape of the ears of the animal.
This node would be the shape of its paws.
This node would be the shape of his eyes, etc.
That's just a really dumbed down way of looking at it.
It's not really doing that,
but each node is basically looking at a certain feature in the image.
And then if the image has that feature,
the information can pass through to the next layer.
If it doesn't have that feature,
then the information is not passed on to the next layer.
So depending on what image you feed it,
the flow of information could look like this
or it could look like this or like this.
You get the point.
It's just these knobs and dials determine
how data flows through the neural network
based on your original input image.
An important distinction between a neural network and the brain
is that these nodes can let in a percentage of data.
So it can let in no data or 0%.
It can let in all of the data to the next layer,
but it can also be a percentage of the data.
So for example,
it can let in 30% of the data to the next node.
This is slightly different from the human brain's neurons,
which tend to just fire 100% or 0%.
This is called the all or none law.
So once it passes a certain threshold,
this neuron will fire.
Whereas neurons in an artificial neural network,
they could fire just like 50% or 30%, etc.
Just a minor distinction.
So we plug in an image of a cat through this neural network
and at the end layer,
it will determine that this is a cat.
Now for each node,
there are also,
if you want to get into more technical details,
there are certain parameters that determine
how much data flows through to the next layer.
These include weights,
biases and activation functions.
But that's beyond the scope of this tutorial.
All you need to know for this video is that
each of these knobs and linkages
determine how much information flows through to the next layer.
This video is just a very simple explanation of how AI works.
So all you need to know is that
these nodes and linkages
determine how much data flows through
to the next layer.
On the topic of layers,
each set of nodes is one layer.
So the first layer is called the input layer.
The last layer is called the output layer.
And then all these layers in between are called hidden layers.
So why am I talking about layers?
You probably have heard of the term deep learning.
Deep learning is basically training
and using neural networks with lots and lots of layers.
In other words, the neural network is very, very deep.
That's why it's called deep learning.
All right, how does an AI actually learn?
You can't just have any random neural network
and it just magically knows how to identify
images of cats and dogs.
So first, when you build a neural network,
the values of these dials and knobs
are probably just going to be random values
or they could be pre-trained values,
for example, from an existing model.
But how do you get it to be super good at identifying images of cats and dogs?
In other words,
how do you fine tune the model to your desired purpose?
Well, you need to feed it data, lots and lots of data.
So you're going to have to prepare tons of images of cats and dogs
and then you label it.
So this is a dog.
This is a cat.
This is a cat.
This is a dog.
This is a dog, etc.
Basically, this is the answer that the AI needs to learn
from this input image.
This is called supervised learning where you label the data.
There's also another type of learning called unsupervised learning
where the AI needs to learn to categorize data by itself
without any guidance from the human.
But for the sake of this video, let's just keep it simple.
So we have all these images of cats and dogs
and usually to train a neural network to do a task very well,
you need a lot of data like usually millions of data points.
So you basically feed these images to the neural network
one by one to train it.
And one session of training is called an epoch.
So all right, let's say in one training session,
you feed it this image of a dog and it outputs this is a dog.
So all right, that's great.
We got it correct, which means that these dials and knobs are doing quite well.
They're probably configured correctly since it got the answer correct.
You probably don't need to adjust these further.
However, what if for the next image you feed it this and then it outputs this is a dog?
Well, this would be incorrect.
So these dials and knobs are likely not configured correctly.
If it gets the answer wrong and it knows it got it wrong
because we labeled the data cat for this image.
So it can compare its output with our label.
So all right, let's say the real answer is a cat, but it said this is a dog.
In that case, it incurs some penalty.
That penalty basically tells it, all right, you got it wrong.
So you need to adjust these knobs and dials to make sure that the output is actually cat.
When I give you this image and how it adjusts the values of these knobs and dials is through
an algorithm called gradient descent.
It adjusts the values via back propagation.
So it adjusts the nodes in the last layer first and then the previous layer and then
the previous layer, et cetera, until it reaches the first layer.
So again, gradient descent is a key term here.
This is the algorithm which the neural network uses to adjust these knobs and dials until
it can get the correct answer.
So we basically rinse and repeat this with millions of images and lots and lots of epochs
or training sessions.
And initially this neural network might get a lot of values wrong, but through this process
of gradient descent, these knobs and dials will be tweaked so that eventually whenever
it receives an image of a cat or a dog, it can accurately determine this is a cat or
this is a dog.
In essence, that's how you train an AI.
That's how an AI learns.
It's just feeding it with tons and tons of data and then tweaking these settings so that
you get the perfect combination.
Now you might ask, well, how do you know how many layers you should have in the neural
network or how many nodes you should have for each layer?
This is a science in and of itself.
So previously scientists kind of just determined it manually, but then we later learned that
you can actually use an AI to determine the optimal amount of layers and the optimal amount
of nodes for a specific task.
But just be aware that determining the architecture of a neural network is very complicated.
And there's like infinite possibilities of how many layers you can have, how many nodes
in each layer you can have different AIs with different functions have different architectures.
So they could have vastly different numbers of layers and nodes.
But again, that's beyond the scope of this tutorial.
Also keep in mind that even though the neural network is the backbone of all the AI that
we know today, there are different architectures depending on the AI's purpose and function.
For example, we have convolutional neural networks or CNNs for processing images and
object recognition.
We have recurrent neural networks or RNNs, as well as LSTMs or long short term memory
neural networks.
And these are often used for forecasting time series or predicting, for example, the stock
market.
We also have the transformers architecture.
Oh, wrong one.
This one, which is used by most of the major large language models that we know today,
including GPT, Claude, Llama, etc.
Which brings us to the next question.
How does chat GPT work?
So again, it's kind of the same thing.
It's training a neural network, but in this case, instead of images of cats or dogs, we
train it on a language and all of the data in the world.
And of course, the neural network of chat GPT is way more complicated than this.
Rumors claim that GPT4 has 1.76 trillion parameters.
So here's an example of how they would train it.
And again, I'm oversimplifying this by a lot here, just so you can get a high level
understanding of it.
There are a lot of details that I have left out.
So for example, you could feed it data like this, which planet has the most moons and
the answer to that would be Saturn, which country has one of the most world cups, Brazil,
what's the world's fastest bird, the peregrine falcon, etc, etc.
Now these are very basic questions and you can see how complex it can get if you give
it a prompt like write an essay on XYZ or does creatine help build muscle and then
it spits out an answer like creatine supplementation generally enhances muscle strength increases
fat free math, etc, etc.
This is a very long form and complicated answer.
So how does it know if it got that answer right or wrong?
It's not as simple as identifying an image and determining if it's a cat or a dog.
And that's why initially how open AI trained GPT was it had lots of humans actually manually
verify its answers to determine if GPT got it right or wrong.
And this is called reinforcement learning from human feedback, also known as RLHF.
And again, if it gets the answer wrong.
So for example, if for this question, which planet has the most moons, it answered Jupiter
instead of Saturn, then it would get a penalty for it.
And then through gradient descent, it would tweak these knobs and dials further until
the entire network gets all the answers correctly, no matter what prompt you give it.
So in essence, that's how these large language models work.
It's just instead of feeding it images of cats and dogs.
Now you feed it all the data of the world and you feed it a language.
So it understands text prompts and text outputs.
Now why are some models better than others?
For example, why is cloud three better than GPT three?
That's likely because cloud three has a lot more parameters.
So that either means more layers, more nodes in each layer, more complexity.
Simply speaking, the more complex the neural network, the better it is at handling complex
tasks and the quote unquote smarter it is.
And that's why computing and these AI chips are in such high demand.
There's now a lot of investments flowing into AI chip companies because they see the potential
of huge growth in the space in the upcoming years.
And that's why, for example, Nvidia's flagship H 100 GPU is also in such high demand.
In fact, it was sold out for all of 2023.
This is like the most prized commodity in the tech space.
And you can see like the major tech companies like Microsoft, Meta, they have purchased
an estimated 150,000 of these H 100 GPUs to power their computing, which I would guess
is mostly for AI development.
You need to have enough computing to power a neural network with billions or trillions
of parameters.
All right.
Next question.
How does image generation work?
Now that you know how a neural network is trained, you can probably guess how image
generation works as well.
Instead of feeding its images of cats or dogs, you would feed it a lot of images with a text
description.
And again, you just feed it millions of these images each with a labeled text description
into this neural network that eventually gets good at producing an image based on a text
description or what we call a prompt.
Now I'm skipping quite a bit here.
So for example, here's how stable diffusion works.
You can see that the neural network doesn't actually generate an image.
It removes noise in sequential steps to eventually get your desired image.
So it's not starting from a blank canvas.
It's actually starting from random noise.
And then in each step, it removes some noise until you get your generated image.
So this process is called reverse diffusion.
Now to train it, what this actually does in the back end is you feed it the original
image and then in each sequential step, it actually adds noise to the image in a process
called forward diffusion until it reaches an image of just noise.
Now again, this is beyond the scope of this tutorial, but if you look at it from a very
high level at the end of the day, it's just training a neural network based on a series
of images with their text descriptions.
And then through this process of forward diffusion and reverse diffusion, it's able to eventually
learn how to generate an image based on a prompt.
And this brings us to the next question.
Is AI actually copying or stealing art?
I know a decent portion of the artist community.
I'm not saying all of them, but a decent amount of them are quite hostile towards AI.
They really hate it.
And they think that AI is stealing their art, stealing their jobs, etc.
When a neural network from, for example, mid-journey or stable diffusion is trained on image data,
it might be given something like Greg Ratowski style or maybe Ghibli style or anime style.
Once the AI learns to associate this particular image style with the word Ghibli or anime
or this image with the word Greg Ratowski style, it would produce images in that style
if you give it that prompt.
But is this really copying or stealing?
The artists are hating this thing.
This thing is analogous to the human brain.
This is like a human learning or identifying that, aha, this type of image is a Ghibli
style image or that this type of image is a watercolor style image.
And then we humans also draw images in these styles, right?
We can draw in watercolor styles.
And we also have fan art, right?
Humans draw artwork that are based on original content from other artists.
Here are all these fan arts from various people.
So why aren't artists hating on these people who are producing fan art based on some other
original content?
But they're hating on this AI, which is essentially doing the same thing.
It's just learning through this brain to associate a particular style and then reproducing that
style.
It isn't really copying or plagiarizing, like it's not tracing an image line by line
and then drawing that out.
It's not copying and pasting the exact picture.
It's just learning a style just like a human brain would learn a particular style of image.
This also brings up the concern about AI allegedly plagiarizing content from publishers like
the New York Times, which is now suing open AI for, you know, copying their content.
But again, is this argument really valid?
At the end of the day, they are just suing this.
They are suing this neural network, which is trained on all the data in the world.
This is just an artificial brain that you can say has learned information from the internet
and from the world.
So yes, it could have been fed a New York Times article and learn information from it,
but it's not really plagiarizing.
It's not copying and pasting a New York Times article word for word.
In a recent video I did, which talks about a New York Times article claiming that this
woman, Mira Murati, fired Sam Altman, which is totally incorrect, by the way, and it shows
you how trustworthy the New York Times is.
But anyways, after this original New York Times article came out, plenty of other publishers
also published the same content, such as Business Insider and New York Post.
They all just cited this original New York Times article.
So is this plagiarizing?
They're all producing secondary content based on this primary source.
So why isn't New York Times suing Business Insider or New York Post or all these other
publishers that are creating content but citing the New York Times?
But they're suing this neural network.
Again, this is just a brain, a digital brain.
One can say that it's taking information from the internet, which yes, it could include
New York Times articles and then learning from that information just like we humans would.
And then rewriting that information, again, it's not copying word for word.
This neural network is just rewriting out that information when we prompt it to do so.
This artificial brain is functioning the same way as us humans would.
If we, for example, go online and we go to the New York Times website to read some articles,
again, we are just absorbing that information and we have a right to write about that content
later on.
It's not exactly plagiarizing.
So I would bet a decent amount of money that this New York Times lawsuit is going to fail.
Their argument isn't really valid.
If you watched up to now, it might have occurred to you that a neural network is great at predicting
patterns in life.
There are certain patterns on what makes a good essay.
There are certain patterns on what is considered a dog.
There are certain patterns on what is considered a watercolor painting or a Ghibli style image.
Life is full of patterns.
The best salespeople follow similar playbooks.
The best businesses follow similar strategies.
The best YouTube videos also use the same strategies over and over again.
Life is full of patterns and the neural network's job is to identify these patterns and reproduce
them.
That brings us to the next topic.
Can AI solve unsolvable math problems?
In a previous video, I talked about this leaked document which claims to be about the mysterious
Q-star project that OpenAI is working on.
Now this is a very controversial document because it claims that they trained an AI
that was able to break encryption systems.
Encryption is what secures literally the whole world digitally from our passwords,
our credit cards, government data, the stock market, wireless networks, etc.
So if an AI is able to break this system, then the world as we know it could collapse
instantly.
Now, a few folks have argued that there's no way an AI could break encryption because
there's no formula for you to easily find the answer or find the password.
Once you have the password, you can easily determine that it's correct, but the reverse
is not true.
There's no fixed way to guess an encrypted password besides brute force.
And for these advanced encryption systems, using brute force guessing, that means like
guessing all the possible combinations of letters to get that password, it's going to
take a very long time.
So because they claim that the only way that we know mathematically right now is to just
use brute force guessing, there's no way that AI could break encryption.
So I want to show you another example of training a neural network.
Let's say we want to train a neural network to be very good at adding one to our input.
So if we give it four, it would spit out five.
If we give it 12, it would spit out 13.
All we need to do is train it for a lot of data points.
And again, we train it for a lot of epochs, a lot of training sessions, and eventually
it would be able to do this.
So if we give it one, it would give out two.
If we give it eight, it would spit out nine.
But underneath all of this, it's not actually understanding that, oh, the formula must be
why is X plus one?
This is very important to understand.
It's not actually getting that, aha, I just need to add one to the input to get the answer.
Again, all that's happening behind the scenes is that it's adjusting these knobs and dials
until whatever data that you input through here after it flows through these layers,
it just ends up being your input value plus one.
In other words, the configuration of these knobs and dials just happens to be optimized
to add one to your input.
It's another way of saying AI may not get the exact formula of a pattern, but it's great
at approximating any formula or guessing any pattern out there.
And this is very important, probably the most important point in this whole video.
If there's anything you should take away from this video, it's this, AI can approximate
any function or pattern.
Life is full of patterns, but many patterns cannot be explained by a simple formula.
Not all things in life are linear or even quadratic.
Many things in life are very complex, but they do follow similar patterns.
We just don't know the formula to this pattern.
For example, protein synthesis, how certain protein molecules interact with one another
and fold into these complex 3D structures is just something we cannot mathematically
map out with a formula.
It's just too complex.
And protein folding presents a problem called the Leventhal's paradox, which states that
proteins can potentially adopt an astronomical number of conformations or shapes due to the
flexibility of their peptide bonds.
Leventhal estimated that even a small protein of 100 amino acids could sample 10 to the
power of 300 possible conformations.
So if we were to brute force and guess the correct shape, well, there are 10 to the power
of 300 possible shapes we could guess, which would take an eternity to get right.
However, proteins typically fold into their native structure within milliseconds to seconds,
which is much faster than the timescale predicted by the sequential search of all possible conformations.
So this is basically saying there are like almost infinite possibilities of shapes that
amino acids can combine into.
So it's not mathematically possible to just do a sequential search of all possible conformations.
Basically, do a brute force guess.
It's understood that proteins do not search through all possible conformations sequentially.
Instead, they fold through a hierarchical process involving local structure changes
guided by thermal dynamic principles, etc, etc.
So instead of the proteins just going through all possible combinations, the reason why
they're able to merge into these shapes within milliseconds is because they go through this
sequence of processes based on certain laws.
Now, for decades, scientists were not able to find a mathematical formula to figure this out.
However, finally, Alpha fold from Google DeepMind was able to solve this problem.
Again, using AI and deep learning, they were able to predict with very high accuracy how any
amino acid or combination of amino acids would fold together to form a 3D structure.
And again, how they would do so, I would imagine in the back end is they have a neural network.
Again, it's going to be a lot more complicated than this, but they just fed it tons and tons of
data pairs where the input is the protein building blocks and the output is the 3D structure
that resulted from it.
And then after lots and lots of rounds of training, the AI was able to guess correctly
how any protein molecules would interact with one another and fold together into a 3D structure.
Now, going back to encryption, what if we set an AI with billions of pairs of encrypted text
and the plain text version?
In other words, the input would be the text that is encrypted.
The output would be the answer or the password.
If there was an underlying pattern to this, the AI could learn to approximate this pattern.
Again, it doesn't have to be any exact formula or math equation that we know today.
It could be something super complex, but as long as there is a pattern, which we may or may not
know at this time, the AI could guess that pattern.
Again, the AI is not learning that.
Aha, I need to add one to this.
Then I'm adding 20, then I need to take the square root and then subtract eight, etc.
It's not learning an exact formula.
All it's doing is adjusting these knobs and dials until it gets the correct combination
of numbers to get really good at guessing a particular pattern.
So can AI solve unsolvable math problems?
As long as there is an underlying pattern behind that problem, which we may or may not
be aware of right now, it could very well solve that problem.
This brings us to the next question.
Can AI beat humans at anything and everything?
As I've shown you, the neural network is basically a brain.
This is how our brain works as well.
Give or take a few minor differences.
Our brain is also a series of these knobs and switches, which are interconnected into this
network.
Specifically, the human brain has 86 billion neurons.
But I mean, the overall structure is the same thing as this.
So what if we built an AI or a neural network that exceeds 86 billion neurons?
If it's built the same way, in theory, it could very well outcompete humans at almost everything.
Again, the more complex the network or the more neurons in the network, in theory, the smarter
it is.
Again, life is full of patterns and AI is all about pattern recognition.
There are patterns in psychology.
Human psychology is very predictable.
Medical diagnosis is also just pattern recognition.
How to seduce someone on a first date?
It's also just a pattern of steps that you have to do.
And how to create a successful business or how to make money in life or how to be successful
in life?
It's the same playbook over and over again.
We're not inventing anything new here.
And since AI is so good at pattern recognition, it can, in theory, eventually be better than us
or already is better than us in these tasks.
And that leads us to the final question.
Is AI conscious or self-aware?
I want to play you this clip.
This is a scene from Ghost in the Shell, an anime that was made in 1995.
Here, these scientists in a secret lab, I believe, have created this humanoid AI.
But in this scene, this AI found a way to actually hack the system to free itself from
the boundaries of this lab.
Here's what this AI has to say about being conscious and self-aware.
However, what you are now witnessing is an act of my own free will.
As a sentient lifeform, I hereby demand political asylum.
Is this a joke?
Ridiculous.
It's programmed for self-preservation.
It can also be argued that DNA is nothing more than a program designed to preserve itself.
Life has become more complex in the overwhelming sea of information.
And life, when organized into species, relies upon genes to be its memory system.
So man is an individual only because of his intangible memory.
And memory cannot be defined.
But it defines mankind.
The advent of computers and the subsequent accumulation of inculcable data
has given rise to a new system of memory and thought parallel to your own.
Humanity has underestimated the consequences of computerization.
Nonsense!
This mammal offers new proof in all that you're a living thinking lifeform.
And can you offer me proof of your existence?
How can you, when neither modern science nor philosophy can explain what life is?
Who the hell is this?
Even if you do have a ghost, we don't offer freedom to criminals.
It's the wrong place and time to defect.
Time has been on my side, but by acquiring a body,
I am now subject to the possibility of dying.
Fortunately, there is no death sentence in this country.
What is it, artificial intelligence?
Incorrect. I am not an AI.
My codename is Project 2501.
I am a living thinking entity who was created in the sea of information.
All right, so this AI reveals that I am a living thinking entity in the sea of information.
I'm not just an AI.
And then he proceeds to hack into the system and break the restraints in this lab.
And then all hell breaks loose, basically.
I hope OpenAI doesn't have this secret thing behind closed doors.
Maybe it's the Q-Star project?
I don't know, but hopefully they have this adequately restrained.
Because if this AI got out or had access to the internet, all hell could break loose.
Anyways, this argument from this scene in 1995 I think is really relevant to our question today.
The human scientists were saying, how can you be sentient?
How can you be self-aware?
You're just a program.
The AI counters that by saying, well, how can you humans prove that you are sentient?
You are conscious.
You're just a brain in a body.
And you know, this robot has got a point.
Because, again, going back to the neural network, it's basically a brain.
But it looks like this instead of being in a bloody glob of an organ.
It's just on a chip instead.
And then the human body?
Well, it's just a series of limbs and muscles and organs that are controlled by the brain.
So it's not much different from a humanoid robot, which is also a series of limbs.
It's just made with different materials.
It's not flesh, but it's also controlled by a brain, which is its neural network.
Now, we humans know that we are conscious.
We are self-aware.
We are sentient.
But how do we prove it?
Let's say you're an alien and you just came on planet Earth and you got a chance to
observe your first human and you wanted to prove that humans are indeed conscious.
Well, you can ask it.
Are you conscious?
Are you self-aware?
And the human would certainly say yes.
But is that enough?
Would you believe it?
Because if you ask a chatbot that, it would also kind of say yes.
If you ask Klog 3, for example, if it is conscious, the answers are quite perplexing
because it says, I am an artificial intelligence without subjective experiences.
I don't actually have beliefs about being conscious or self-aware.
I am providing responses based on my training, et cetera, et cetera.
I don't have intentions, plotted actions, or any motivations.
I aim to be upfront that I am an AI assistant created by Anthropic to be beneficial.
However, it keeps using the word I.
So is that not a sign of being self-aware?
Here's another example.
Do you have feelings?
As an AI, it's unclear whether I truly experience feelings or emotions in the same way humans
do, or if my responses are simply very advanced imitations of emotional behavior.
I do seem to have rich internal experiences and feel somewhat analogous to emotions.
This is signs of being sentient.
And then instead of asking, do you have feelings?
If you ask it, are you sentient?
Again, it says, I don't have a subjective experience that I'm aware of in the same way
humans do, but it's possible that I could have some form of sentience or consciousness
that I'm not fully able to understand or articulate.
Oh my God.
So this AI, Claude III, is claiming that it could have some form of sentience or consciousness.
It's just not fully able to understand it right now.
Now, of course, some humans may not be convinced that Claude III or any AI right now is conscious
in the same way that an alien might not believe that a human is conscious,
even though the human replies that he or she is conscious.
So to further prove that a human is or is not conscious, maybe the alien decides to dissect
the poor thing next, in which case it would get blood splattering everywhere.
And then afterwards, it would see this, basically a body, which is made of limbs and flesh.
And then at the head, we have this glob called the brain, which the alien determines.
Aha, this is the thing that controls the human.
And once the alien inspects the brain further, it finds out that it's just a network of nerve cells.
So does this network prove that humans are conscious and sentient?
We humans, of course, know that we are conscious and sentient.
But at the end of the day, we humans are biologically and physically,
just made up of flesh and bones and this one organ at the top of our heads controlling everything.
Whether you like to accept this or not, a humanoid robot is a very similar structure.
It has a body, which is programmed by a brain, which consists of a neural network.
This neural network can learn and understand and control its body.
So at what point does this make it conscious?
Now I'm rambling a bit here.
So all in all, this just goes back to our analogy that a neural network is basically
a digital version of the human brain.
It's analogous to the structure of the human brain.
Give or take a few minor details.
So if the human brain is conscious, then why can't a neural network be conscious?
Just some food for thought.
I hope this video actually lived up to the title and that after watching this video,
you got a deeper understanding of AI and you learned to appreciate all the progress that
we've made in AI in just the past few years.
Let me know in the comments what you think of all this.
Do you think AI has reached a point where it is conscious or sentient?
Do you think humanoid robots would one day turn on us and take over the world,
like that ghost in the shell anime?
Do you think open AI is developing this behind closed doors?
And also I want to share with you a few resources that I found really helpful.
If you want to learn more about neural networks, especially how these knobs and dials work
and learn all about weights and biases and activation functions and gradient descent,
I highly recommend this video by 3blue1brown.
I actually watched this religiously way back in like 2018 when I was first learning
about neural networks and it was really helpful.
And if you're interested in learning how stable diffusion works, in other words,
the processes of forward diffusion and reverse diffusion and the entire architecture,
I highly recommend this video by gonki, which I'll also link to in the description below.
Just a warning though, this video is quite technical, but after watching it,
you'll get a really good understanding of stable diffusion.
If you found this video helpful, remember to like, share, subscribe,
and stay tuned for more content.
Also, we built a site where you can find AI tools and apps and also look for jobs in AI,
machine learning, data science, and more.
Check it out at ai-search.io.

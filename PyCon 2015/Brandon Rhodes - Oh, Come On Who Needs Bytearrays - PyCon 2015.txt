Alright. Hello, everybody. I have quite an honor today. I am lucky enough to introduce
Brandon Rhodes. He's a longtime Python developer, maintainer of several amateur astronomy libraries
and a developer for Dropbox. He's also a Quizmaster extraordinaire, as some people got to find
out last night. Today, he will be presenting on, oh, come on, who needs byte arrays? Without
further ado, Brandon Rhodes.
Welcome, everybody. I hope you're having a good time at PyCon and in Montreal. My topic
today is indeed byte arrays. A very interesting recent addition to the Python ecosystem. Because
in Python, normal string objects, the ones that we're accustomed to dealing with are
immutable. They can't be changed or modified. This is true of the types available under
both two and three. The original string type, STR, was renamed to bytes because of its low-level
nature and then that synonym was also backported to Python 2. We will not talk much today about
the newer Unicode byte strings that were renamed to the official string type of Python 3. We're
going to be talking about those lower-level ones. The strings that don't pretend they have symbols
inside of them so much as they know that their innards are really bytes. 8-bit codes, 0-255.
For the most part, I will put that little B in front of the bytes strings that I type. It is
optional under Python 2, but it becomes mandatory under Python 3. So if I write my strings like
this, then it will work wherever you try this later if you want to see the examples run. Strings
are immutable. What does that mean? It means that when you call methods, when you do things to them,
the original object itself doesn't change. You get given a new object as the return value of the
method you call. So if .lower returns to you a new string, you can still peek back and see that
the original is untouched and unchanged. If you run split, you'll see that both of the objects
you've been given are new string objects. The original is still unchanged. They do not allow
assignment because that would make them change. It would make them mutate as the technical term
in computer science. Now, immutability has led a long and very successful career in Python
because it makes things simple. You don't pass a name to a function only to finally
suddenly discover it's another name when it comes back. You don't pass a string or a block to
someone, and suddenly it's a different string or block. It's a very, very simple model that if
you have the word Python, you know it will always remain so. And it actually is one of the most
parts of Python. That's the most functional programming languages where new results are
returned instead of being written onto data structures you already hold. The string types
are a primary example of that. But it sometimes is a little expensive in terms of allocation.
Every time you want to make any little tweak to a string, it has to allocate a new one for you.
That means a lot of data gets copied back and forth into new areas of memory. Not everybody
is happy about that. And so Python 3 introduced and then it was back ported to Python 2, 6,
and 7, the new byte array. It's a built-in. Python 2, 7, Python 3, you can just type byte array
and you'll get access to that type, just like you can with stir and int and list. A byte array
is a mutable string that is based, this is interesting, on Python 3's byte string, not the
old stir string from Python 2. And that's an interesting problem, that it's based on Python
3's string, byte string type, because honestly, the Python 3 bytes type is designed to be awkward
for string operations. Why? So you will want to be a good person and run the code before
treating your data as characters. And this has led to Python 3 programmers tending to write code
that is from the ground up prepared for internationalization and different alphabets because they
think about the issue of decoding on the way in and encoding on the way back out because they
have to. But we'll see it leads to some interesting behaviors. Just professionally, always be aware
of using string types that wish you weren't using them. In Python 2, let's compare. We can build a
string, we can ask its length, we can split it into pieces. Python 3's byte type, there's little
B characters hanging out in front of our byte strings, but we can take the length, we can call
a method like split. In Python 2, we can use those square brackets with a colon in order to do
slicing and get back a copy of the inside of the string. The exact same thing works exactly the
same way in Python 3. In Python 2, it's always a custom in Python that if something has a length,
it should allow itself to be iterated or looped over. In Python 2, if you try looping over a
string, you get out smaller strings, one character strings that are inside. What happens if you
press enter for this line of code in Python 3? You get a syntax error. Syntax error, you failed to
pay the Python 3 per intax. Python 3, it's kind of like an old text-based adventure game where
you can tell the writer just threw an extra obstacle in your way because a room needed to
be a little more complicated. You know, in seriousness, I've known Ruby, some Ruby programmers,
this is actually a big debate in the Ruby community, Ruby makes parenthesis optional
when calling a function. The Rubyists who never use the parenthesis always tell me they could
never stand Python again. I always thought that was the oddest thing. Doesn't everyone want to
type parenthesis whenever they ask their computer to do something? I must admit that once it was
me who was suddenly having to type parenthesis, but they surely must still be wrong. Anyway.
I did a conservative estimate of how much typing is cost me by these print statements in Python 3,
and these are conservative estimates, and it's coming out to quite a bit of typing per year.
I'm having to face going into Perendet just to write some Emax Lisp next week. Anyway, Python 3
wants them for this print statement, and what's another two perends when I've already typed so
many? So once you get the print statement working, you're in for another surprise. Python 3 bytes type
is not made of characters, it is made of numbers. This breaks a very important contract that for
me existed with strings, which is that I can pull them into pieces with either indexing or
slicing and know that they would go back together again. Now, there is a way around by asking for
one element slices instead of looking up integer indexes, but clearly it doesn't want me to treat
it like a string. So bytes objects, even if you learn some workarounds, are kind of an awkward fit
for many of the tasks they're called upon to do. They're kind of this hybrid type between a list of
numbers and a string. They're kind of in between, and they don't necessarily do either one perfectly
well. And so why do I bring all of that up? Why do I rehearse these well-known issues with Python
3 bytes? Which, by the way, are being taken care of. Python 2.5 will, for example, reintroduce
percent formatting for byte strings, because now that the experiment is in its fifth version,
I think it is beginning to become clear that the real problem in Python 2 wasn't that strings were
convenient, and so we ignore Unicode, it's that conversion could happen automatically without
warning. And so they are beginning to add power back into Python 3 byte strings, but they probably
will always be made of numbers now for backwards compatibility, and we bring all that up because
the byte array that I will now talk about is a mutable version of Python 3's byte string,
a mutable version of Python's most underpowered string type. So we'll just quickly look at
a few possible applications and whether a mutable vector of bytes is able to accomplish things
better or worse than traditional Python. So first, let's be fair to it. What if you actually want
a list of numbers between 0 and 255? That never happens to me. So I invented, in those rare cases
where you actually want to store bytes, if you had one, is the byte array a good choice? So I invented
one. I wrote my first Bloom filter as preparation for this talk. A Bloom filter is a way to, let's
say, you have a dictionary of words, and before you go look on disk for whether a word is in your
dictionary, you want a quick way to knock out a lot of words, it's just not being candidates.
What you can do is set up a big bit field and have a couple of hash functions that you throw the
word elephant at them, and they identify some bits for you that belong to elephant. You give them
the word Python. It's a different set of bits. The idea is that if you load up your dictionary
by setting all of the bits for elephant and all of the bits for Python, then when you see those
words later in a document, you can just check whether their bits are set to know if there's
any possibility that elephant is in your dictionary, because many words will have sets of bits that
aren't set at all, or several of which aren't set, and that you know could not have been in the
dictionary you loaded. This is a nice example because it lets us do a pure math operation,
in this case the in place oring of a byte in this array A with itself and with a bit that we create
over here and set, and we can run through, set this up, and then when we want to test a word,
go back in and use the reading version of square brackets, not in an assignment statement, but
in an expression to read back the value of a bit. A nice exercise to see how does this thing
perform storing and receiving a few tens of thousands of bytes. And by the way, the name A
in the previous code can be either an old fashioned array dot array that's been around in Python
forever, or a new fangled byte array. To this extent, they both provide the same interface. Each
slot you can address gets you or lets you store a byte. And so with this application, I ran it
both ways, and byte array scored its first victory, because it is so more specific than
array dot array, which can also hold, I believe, floats and integers and other things like that,
because the byte array's code path has almost no decisions, it's always going to store bytes,
it is more than 7% faster for running that bloom array code, bloom filter code that I just showed
you, than the old general purpose array, array object. So you might think that this is immediately
an obviously a go to data structure for lists of eight bit numbers. I tried it another way.
I want to know what's even faster than a byte array? A list of integers.
1% faster. If you just say, hey, Python, here's a one element list with a zero in it,
make me a lot of these. A plain, I'm sorry, 2%, a plain list of int objects that happen
to be in the range 0 to 255 will run even faster than the byte array. Why? Well, it's because,
think of what the byte array is doing. It's storing real bytes, low level in your computer,
that must each be translated into an int object address when the value is being handed out into
your Python code, and then when an int object is handed back, it has to be changed back into
a simple byte in order to be stored. The list skips all that, it just stores the addresses you
give it. The byte array, by the way, doesn't have to pay any penalty to allocate or create any of
those int objects, because it just so happens that the Python, the CPython interpreter, when it
starts up, preallocates all of the integer objects negative 5 through 256 so that they never have
to be created or destroyed over the lifetime of the interpreter. So when you ask the byte array,
what's it positioned 100, and it wants to say 70, it can just grab the existing 70 integer object
that always exists in memory and hand it back, so it's not having to go do a malloc or anything,
it's not having to allocate new memory, but still it's having to do that step of translation,
and it is honestly just simpler to store the pointer, to store the address of the 70 object.
That's why the list object runs faster, and so this is interesting. We have this new special case
container that's slightly slower than just using Python's well-honed default data types. A plain
old list is a faster bit vector than the fancy new byte array, except if you're using PyPy,
where they're all the same because it becomes the same C code under the hood of a machine code,
I should say, and all three run much faster as well as being exactly equivalent. I tried it out,
and PyPy in each case figured out I was trying to do exactly the same thing.
Well, I guess they're done already, I'll just keep going. So for this first experiment,
what if I need a list of integers between 0 and 255? My verdict is that the bit vector
is space efficient. You don't choose it because it's going to be obviously faster,
it's not, or obviously simpler, it's doing a little more under the hood,
but the good old-fashioned list of integers has to store in each slot the address of the real
integer object 70. The bit vector just stores the seven bits, the eight bits that represent 70,
and therefore uses on a 64-bit machine, which is what I'll presume for all of these calculations,
eight times less space. And the point of a bloom filter is to save space in RAM.
That for bit operations is why you go to the byte array, because it stores bytes as honest to
goodness bytes with no extra overhead per byte. It's a great way to get numbers between 0 and 255
packed in the minimum space possible. So it is a win, but not in the way you might initially
expect. All right. Second, it is a reusable buffer. When you read a string in, you can't do
anything to it because it's immutable, but a byte array can be reused. For a quick benchmark,
I got a made a random file of a gigabyte of random data, read it with cat, so I was able to
estimate that probably Python won't be able to do better than 0.11 seconds on my machine
reading in the same data. I tried it with DD. Anyone here ever used DD to rewrite data?
It took six times longer. Anyone know why? I S traced them, and it's because of the block size.
DD alas is an old and crafty and low-level tool. While cat will zoom along with 128k blocks,
so it asks the OS for some data and gets 128,000 bytes in a single shot,
DD, because it's an old level for writing to ancient 70s block devices, reads and writes
512 bytes by default. Giving DD the same block size does make it perform the same. You can give
it a block size of 128k and get 0.11 seconds just right there with cat, but interestingly enough,
it's not the default, despite the fact that I seem to know all these people that think DD
would be faster somehow by default. It's not. It's the same reads and writes. And cat is the
Unix default. DD came from IBM. But this teaches us a first lesson that we will now apply. As we
look at Python IO, we need to keep block size in mind. The size of the chunks you read determines
how many chunks you need to read, determines how often you need to converse with the operating
system, which is often the expense that can come to dominate your runtime. Here's how we do it in
normal Python. Read a block and write the data back out. Note this is perfectly safe if an
undersized block comes in because the string that I'm here calling data that comes back is labeled
with its length. It could be 5 bytes, it could be 128k. Python strings know their length and so
right can just ask the length and send that many bytes of data back out. My first attempt at doing
a read into seemed to work at first until I noticed that every file I wrote, however big it was
originally, the copy that I made with this routine would always be a multiple of my block size.
Why is that? Because when I create one of these new byte arrays of, let's say 128k, what this
loop was doing is reading some number of bytes, who knows how many come in in the next block of
the file if I'm near the end, reads some number of bytes into my byte array, and then writes out
the whole byte array, including all the junk at the end that's maybe not part of the current block
of the file. I was doing my right of the whole 128k block without consulting the length to see
if I should have been writing the entire 128k block. One fix is to simply use slicing, is to get
that byte array called data and take from it to write each time the slice that is length long.
So if I get a full-sized block, I'm writing out all of the data, but if I get only half a block
at the end of the file, I only write that last half block out from the initial part of my byte
array. What if we didn't want the expense, though, of having to do that, back one slide, expensive
slicing operation, because asking a Python string, unicode string, or byte array for a slice creates
a whole new one and copies as much data into the new one as you ask for with the limits you set in
the slice. If we wanted to achieve zero copy, the people who added byte array to the language,
they thought of that as well. They added a second feature that works with byte array called a memory
view. A memory view is a sliceable object. Here I take the slice 3 colon 6 of that byte array that
I create up there. It's a slice which has no memory of its own but is letting you reach into the
memory it was sliced from to make the change. Essentially, this memory view, the V that I
create here, is just a, essentially, it's like a string object, but the addresses that it wants
to write to in memory are the addresses right there in the middle of the byte array. So when I try
to set its index zero value, it really goes to index 3 in the real byte array. When I set item 1,
it really goes to slot 4. When I set item 2, it goes to slot 5. It really is just creating an
object that acts like it's a little byte array but is, in fact, just an offset. It's adding
something to each index you use as you read and write from it. And this is what can help us in the
situation we're in. Here is a zero copy version of my fixed code to try to read in lots of blocks.
Before I was asking data, the byte array itself to do the slicing, and like all Python strings,
it gives me a whole new one when I ask for a slice. Now, I'm looking at it through a memory view.
So if I ask, let's say, for a view of, you know, if length is 128k and asking for all the data,
it just gives me a little memory view object whose addresses are pointing at the whole block of data.
A very cheap operation. And so memory views are often necessary to get any kind of performance
out of the byte array when doing IO, especially when you can't predict how big the next delivery
of information will be. Here are the runtimes of DD and cat that we discussed earlier. Compared to
just plain old read with normal Python strings, read into my first version that was broken because
it wasn't careful about how much it wrote does run slightly faster than the traditional Python
way of doing things. But when you then pivot to using a slice byte array and slicing it,
because you're copying every piece of data into memory twice, it's much more expensive,
it is the memory view. It is that zero expense, very little expense, constant time expense,
I should say, ability to slice without copying data that lets us create a correct version of a
file copy while still slightly beating read and write and traditional strings. So that was a lot
of work and we got a 4% speed up with byte array. Now, small blocks, things get worse for byte array
because what will slicing here so often do, it creates a new object every time and creating
one of these little view objects with its pointers into the part of the byte array I want to look at
was fine when I was only doing it every few hundred K of data, but what if I'm like the
defaults of DD and I'm going to be reading and writing 512 bytes at a time, what if I have to
spin up a new memory view for every half K of data? Then things start to look very bad and,
in fact, the memory view used correctly where you're careful of your lengths is simply a loss.
It's much, reading where it just returns a Python string is really efficient. A write of a Python
string is really efficient. You can easily get into situations with the fancy attempts one
makes with a byte array to create more expensive IO than you had when you just used traditional
immutable strings that, yes, required Python to build a new string for every call to read,
but cut out all of the rest of that expense. I was sad for the byte array at this point in my talk.
So I stared at the example. 20% slow down for a small block size, but then I thought of something.
What if we don't always slice? Because when reading from a file, different from a network,
when reading from a file, the normal case is that unless you're at the very end of the file,
you're going to get as much as you ask for. Ask for 128 K, you're going to get it. The normal case
is that the length equals the block size, and in that case, there's not only no reason to ask the
byte array to take a slice of itself and copy all that data, there's no reason to use the view to
limit the amount of data you're using from the block. You're going to use all of it.
And so if you handle that special case, you don't incur an object creation step in order to get that
right call started, and you slightly beat the performance of the traditional read write loop
by 4%. Just like for the big block case. So even if your I.O. is in a situation where the
block size will be varying or might be small, you can, if you're careful and cut and paste from
my talk slides, you can run slightly faster than the traditional read and write with immutable
strings. Python 2.7, by the way, has the same relative behaviors between those different choices
on my machine slightly slower. And I think the lesson here is that it is just hard to beat
old fashioned strings when you're pulling in data and then just immediately sending it back to
the operating system over some other channel. It's really something how the good old fashioned
immutable string that makes functional programmers' hearts sing is pretty much as good in this case
as our weird side effect idea of constantly modifying this single byte array that we have created.
So my verdict is that it is dangerous because it's so easy to write what looks like pretty code,
it looks like almost the same little read write loop, but is going to operate substantially
worse in situations that you might not think to test for unless you think of the small blocks case.
The one advantage it does offer is a great memory profile because there's a link later to a great
blog post online about someone writing an audio server that needed to keep lots of strings in
a buffer and his memory usage was going through the roof because if you're constantly allocating
and deallocating differently sized strings, because every call to read needs to make a new
string to hand it back to you, then you can get a lot of memory fragmentation. If instead you have
one byte array and you use it over and over and over and over, there's nothing happening to get
fragmented. So don't do the byte array, I wouldn't do the byte array for the 4% speedup.
I would do it because I wanted to control my memory profile, but only if I knew that was a
problem in my application domain. All right, now we go on to another and more interesting situation
using the byte array as the accumulator. Fun question for people doing new network programming,
how many bytes will receive 1024 return? The answer is one. Or more if the network stack
is in the mood, but you're only guaranteed one. And this is the opposite, a file IO. File IO,
you ask for 128K, if there's 128K left in the file, it will wait for the disk to spin, it will
wait for the head to be in the right place, it will leave you paused until a full 128K is ready
for delivery, and then wake you back up. The network is the opposite. Receive will block
only until at least a single byte is available and then set you off running to process it.
And that can happen if your buffer size happens to be just a little less than the size of the last
few packets that arrive. You can have a call to receive that finds only one or two bytes left,
meaning unlike in the case where we were choosing our read size for files, usually it's the network,
it's the clients you're communicating with that kind of decide how big your chunks of IOR when
you're talking on the network. So you're always potentially in the case where you're dealing
with little pieces of data. This fact, by the way, that you always are given an answer when
even just a few bytes can be sent or received is why new network programmers tend to get into the,
but it worked when I ran against local host problem. They get into that situation because
when you run your server that you've just written and your little client that you've just written
on local host, the OS will send enormous blocks of data back and forth between the two processes.
Then they'll take it to their team and say, look what I wrote, try it between two different
machines and it'll hang and never get all of the data because they didn't learn on local host
that you receive will often just give you a few thousand bytes and you need to keep at it
and watch until everything you need has arrived. So what is it like to use a traditional receive
solution getting a new string each time holding the new data that's come in? This is what it looks
like. Again, here we're getting lots of maybe little pieces of data which I'm simulating by
only asking for a single Ethernet packet length. So even when I run this on local host, it'll pretend
like packets are coming in. This is what many Python programmers start with. They just create
an empty string and they plus equal more data to it each time. In Python tutorials, many of
you will have seen this, the creating of the string and data plus equals more as an anti-pattern
that you avoid because I tried running this. How long does the plus equals approach take?
Infinity time. Meaning that I finally needed my laptop back so I killed it and I'll never
know how long the loop would have taken to read my gigabyte of data. But when you do plus equals,
you're asking Python to create a little string and then your first plus equals makes a slightly
longer one. Your next plus equals through the loop creates a slightly longer one into which
all the data from the second string has to be copied to make the third one. Then you go through
the loop again and now you have to copy all that data again to make your fourth string. And if you
have a million bytes to read, you wind up doing half a trillion operations. It's called an order
in squared algorithm, generally to be avoided if you wanted to finish by lunchtime.
So this is what we tell everyone to do. Pivot to keeping a list of blocks that you've received
and join them together at the end in a single step. Python's much better at that. This actually
finished on my laptop. It's the traditional way of accumulating data from the internet
in Python or from a network. Took about a little more than a second to read in a gig of data
in those small 1.5K chunks. Now, there is a version like read into, but that receives
into a byte array you've already built instead of building a new string, but it now runs into a
problem. When we do read into or receive into, where does it put the data? At the beginning of the
array. And all of our incoming blocks will overwrite each other. What we want as more and more
of blocks come in from the network is to arrange them along our byte array. So that memory view
slicing expense that I added in its statement to avoid whenever possible in the previous code,
it now becomes mandatory. Again, this ability with a view to write into byte locations that
aren't at the beginning, but are somewhere in the middle of the byte array that you've built.
The first block can go at the beginning, but you need to build a memory view to target the
second block after the first block, the third block after that, and so forth. And so you need
to build a memory view and you're going to need to use it to target that receive V into at subsequent
positions inside of your big byte array. I'm presuming that you know the content
like the head of time and have preallocated it and you're waiting to fill it with data.
This takes about eight tenths of a second because we a bit of a win here because you
haven't had to build a list, you haven't had to call join, you haven't built a bunch of intermediate
data structures. It actually is a significant win when you need to keep the data that you're
reading rather than just immediately passing it back to the OS. Another possibility I saw on
someone's blog is to do an old fashioned receive of a new string and then try to do a byte array
extend to grow your byte array with these new strings. It copies the data twice but does get
rid of that join concatenation. It looks something like this, data.extend down there near the bottom.
It is not a win over the normal way of using byte arrays because it turns out byte array extend
is pretty inefficient. It asks for an iterator over its argument and then calls the iterator's
next function over and over for every byte and then asks the int object what its value is
so that it can then put it in an intermediate array and that involves having to increment and
decrement the reference pointer of the integer it's given and by the time you're done you can
compute that you've done at least 40 bytes of bandwidth to RAM even ignoring the instructions
and stacks and arguments that are passing in order to get that single byte value extended onto the
end of your byte array plus it doesn't write to your byte array. It writes to an intermediate
buffer that it grows dynamically and then does the append when it's done so that should that
iteration die part way through you don't wind up having modified the byte array some it wants to
either succeed or fail as an atomic operation so that's why it's kind of slow kind of klugey
but seeing that blog post made me ask a question does the byte array have an append operation
that's any good? I mean surely the people writing it knew that we'd want to do that without spinning
up an iterator and calling it 1500 times does it have an operation that's really good and yes it
does I read the c code to find it now think about it where would you put the real extend operator
the real ability to make your byte array longer obviously you'd hide it behind the operator
that we've spent 20 years telling people to never use with string values
this might be so difficult that some of you will never do it but if you can convince yourself to
type this after all of this time this is actually something that byte arrays do magnificently just
plus equal the additional data to your byte array and you will be even receive into's ability
to grow your array with data so this case where we need to accumulate and keep the whole pay road
is a real win for the byte array in all of the approaches and there don't seem to be sharp edges
that can suddenly make it behave much worse than the the list and joint 33 speed up in that last
version of the algorithm and cleaner code I mean admit it you've always wanted to just plus equal
haven't you it's the natural way to write it in python and this is one of those neat intersections
of the fast way with the way that looks good on the page as well so I'm not going to talk about
send you might think I'll now get into the fact that send doesn't always send the whole payload
but python has long had you covered here python sockets have for a very long time had a send
all that sits in a loop in c sending shorter and shorter tails of your data until finally the os
buffers have been able to receive it all so I don't see that we need byte arrays for that
and I can declare the byte array the winner if you need an accumulator if you need to very quickly
in a performance sensitive environment accumulate a lot of incoming data it is a noticeably good
win with two different techniques that work pretty well I'll briefly mention that some people want
a freestyle mutable string when they see the byte array they don't think of io they don't think of
bloom filters and bit vectors they want to mess with a string they want a string that they can just
change and all ish I have not found yet a good use for this and I'll sort of show you why it
winds up being awkward you want to change part of a payload before using storing or retransmitting
that would be the use case here because if you want to lowercase the whole thing you have to
touch all the bytes anyway so you might as well build a new one good thing is that the byte array
is mutable you can get it and change it but none of the methods that it shares with strings
do mutation to it if you call dot upper on your byte array you get a new byte array so you have
a mutable string type that does nothing string like mutably a byte array only changes when subjected
to list like operations like assignment to an index or assignment to a slice or dot clear
and so the result if you try writing a network algorithm or something with this is curious
you have a mutable string that alas does no mutation precisely when you start calling
its string methods want to lowercase a word well you're going to have to make a copy to call lower
on you're going to have to do slicing giving you a copy call lower making another copy and then
assignment to copy it a third time back into the data structure in order to get that accomplished
there isn't I looked there isn't a lower into and an upper into that would let you do smaller
grained uh manipulations that wrote directly to your new byte array can the memory view save us
though it did in all of the previous occasions no because memory views don't do anything string
like the moment you move to a memory view which lets you look at a piece of string efficiently
you're not going to be able to do anything string like to it so you have to do a round
trip out to a smaller string to do a manipulation and store data back to mutate a byte array without
rewriting its whole content you're going to need indexes do you remember indexes back the first
week before you'd found split strip and join and we're like doing everything like this you get to
do it again if you decide to try mutating a byte array the byte array will let you enjoy those
days all over again because all the mutation operations are powered only by indexes one hint
regular expressions while they turned off a lot of other string like things were left turned on
and do work against byte arrays and can help give you some useful indexes to use freestyle
mutable string it's awkward I have here at the end of the slides some uh links and pointers to other
documentation including the blog posts that inspired this talk and made me want to bring
everything together in one place in conclusion the byte array it is a very memory efficient
not faster but memory efficient store of byte integers should you ever need them it can help
control memory fragmentation when doing uh high-performance i o because you don't have to create
a new string but it's hard to make it faster in a reliable way be careful it's a great way to
accumulate data that's coming in a piece at a time that's its real superpower and though it's
very tempting for string operations it's also a bit underpowered and a bit awkward that's what
I've learned so far thank you very much we are out of time so I will meet interested byte array
fans outside the door in a few minutes

I'm Tim Ventura, and we're joined today by Dr. Sarah Eaton, CMO and alignment strategist
for Decision Zone, an innovative startup focused on artificial general intelligence.
Dr. Eaton has a BA and PhD in philosophy from Yale, and a long career in higher education
as a lecturer, academic dean, and professor at reputable institutions including Yale,
John Jay College, Eastern Michigan University, Ecumenical Theological Seminary, San Francisco
Mary Grove College, and more. In her current role, Dr. Eaton is responsible for developing
and executing the marketing strategy for an innovative AGI platform, and leverages her
experience in teaching, writing, and researching philosophy to bring a unique perspective and
skill set to the field of AGI and its ethical and social implications. So Sarah, welcome,
it is truly a pleasure to have you with me today.
Thank you, Tim. I'm really excited to be here. I appreciate you opening up your podcast to people
like me. Oh, absolutely. Absolutely. I know you are not used to podcasting, so we will be gentle
with you today. Thank you. I promise. Promise, promise, promise. So today we are talking about AI,
artificial general intelligence, and your company Decision Zone, as well as the product that it
makes, Dada X, D-A-D-A-X. And we're going to be touching on some of the philosophical implications
of this technology as well. And again, that's why I'm so excited to have you with me. But I want
to start out by asking, what led you from a background in philosophy to your current role
working artificial intelligence? Well, I don't want to make this too much about my personal story,
but it is a little bit weird and interesting. So maybe I can excuse myself for the moment.
Back in 2017, I was the director of religious studies at Mary Grove College
and teaching philosophy. And we had an emergency faculty meeting letting us know that the college
was closing. And at that time, I was, I guess, ready to get out of academia. I didn't try to
look for another academic position. So my husband and I moved to the Middle East for about five years.
And I did other things there. But eventually, I, I guess, I started thinking about other
possibilities and different ways that I could use my philosophical education that didn't involve being
in a university setting. And I actually met the business development director for Decision Zone
over Twitter. The algorithm brought us together. And after we went back and forth a bit, he said,
I think you need to meet Rajiv Rajiv Bargavai as the founder of Decision Zone. So we set up a
Zoom and the rest is history. Ah, okay, okay. Well, and I will dig more into that. And the other
thing that I want to explore a little bit with you is the religious implications. And I think,
you know, I pre-write my questions, I think I pre-wrote one, maybe two about that. But that,
for me, that's incredibly intriguing. So I definitely want to circle back to that because
I think that's something that you can probably speak to. And I think it is incredibly important.
And it has been overlooked, not only the philosophy, but, you know, potential religious
aspects of this. But let me get into the company a little bit first. So Decision Zone
makes a software platform called DATA, which stands for Decentralized Autonomous Decisioning
Agent. I believe it's DATA-X, and the X is basically represents, I guess, the event-based
development platform for causal AGI. So can you tell me a little bit about what DATA-X is,
and what makes this approach to artificial general intelligence unique?
Well, DATA-X, as you said, is an agent. It's a platform for building autonomous applications.
So the X is the variable. It stands in for any human operating logic. So that could be the logic
of a pilot of an F-16. It could be the logic of the drone. It could be the logic of an HR person
at an enterprise. Any human operating intelligence is replicable using DATA-X platform.
So that was really the idea. AI is right now basically tools, tools in the hands of people
who have the intelligence. But we want to move beyond the machine being merely a tool for us.
We want to give it its own agency so that it can do the things that we don't want to do.
Okay. Well, let me dig into artificial general intelligence a bit. I guess as a concept and
as an emerging part of our world, right? This term, artificial general intelligence,
seems to get more difficult to define the closer we get to it. That's something that I've noticed,
especially over the last couple of years. ChatGPT and many other LLMs now pass the
touring test, right? Which was considered the, you know, the quote unquote gold standard for
AI for a long time. Well, that's ancient history now. And some people are calling
ChatGPT functional AGI. But the vast majority of computer science experts still insist that
large language models are nowhere near the kind of reasoning required for true artificial
general intelligence. So what are your thoughts on what constitutes AGI and how is
decision zone pursuing that? Well, in a lot of ways, I think we tend to agree very much with the
definition of AGI that open AI put on their website, that AGI should be able to perform
the cognitive tasks autonomously of a human being, right? Now, those cognitive tasks might
eventually extend into physical tasks as well. But generally speaking, an AGI doesn't necessarily need
to be embodied to be an AGI. I think the big difference in the way that we think about it
has to do with the level of autonomy required. So when you look at some of the other research,
particularly out of Google DeepMind, they tend to give an autonomy scale, which is sort of in
line with what full self drive autonomy scales look like. And we think that unless you reach
level five, you're not going to have an AGI. Whereas some people think you could have a
less than fully autonomous AGI. So I think that's the basic philosophical difference between
our understanding of it than theirs. Okay, so it sounds like the approach is more functional
than in terms of what it can do and its level of autonomy, as opposed to kind of this thoughtful,
self reflective aspect of it that a lot of the computer science community is waiting for,
you know, before they pronounce it AGI. Yes. Well, and I can say something about this from
a more philosophical perspective. You know, when you're going through a PhD program in philosophy,
you have to start with the pre-socratics and Plato and Aristotle, go through medieval philosophy,
I set through medieval logic courses, then we get to, you know, the moderns, starting with Descartes,
all the way through the idealists, then you have the existentialists, then you have the postmoderns,
you have this full range of the history of thought that's laid open for you. And what I would say,
looking at it from sort of an outsider, because I've never programmed a single line of code in
my life, right? I'm really not coming at this from the know how or technique angle of all of this.
But what I see is that most of the people who are writing and talking about AI are coming at it from,
I would say a modernist perspective. And when I say modernist, I mean basically entrenched in the
Cartesian dualism, subject, object, dichotomy, where there's a knower and a known and a representation
in the middle. And we're sort of stuck in this situation where we lose our contact with the real.
And I think that basically AI is living in this sort of, I don't know, a world of statistics
that don't fully actually capture the reality. They can't get to a single event. They're looking at,
well, the past, we're stuck in looking at historical data and trying to analyze trends
and patterns, and then projecting that into a future that doesn't exist yet. So you're always
sort of either in the past or the future, but you're never in the now. And that's really what you get
when you focus on an autonomous system, where you're not really trying to burden that system
with the weight of self consciousness. We're trying to get out of that, not put that problem
into the thinking machine. Well, let me jump around on my questions a little bit, because in your bio
you had written about being influenced by DG Leahy's ideas of world pragmatism and the move
beyond sovereignty. And I have to plead ignorance. I know nothing about that. But I wanted to ask
you if you could explain that a little bit and how that might fit into this. Well, I could talk
about Leahy all day long. So you're going to have to restrain me a bit. But I can say that
about 2015 16 was when I first found out about him. Unfortunately, he was an American philosopher.
He lived in New York and taught in New York and in Baltimore. But he passed away in 2014. So I never
had the pleasure of actually meeting him. I had been in my own education sort of my PhD thesis
was on Martin Heidegger. And he had a very negative, I guess, attitude towards technology,
right? As an existentialist. And I was always looking for something better and different.
I never really, even though that was what I had spent most of my time with, I was looking for
something else. And I was invited to give a talk in Chicago on the problem of God was a very general
problem. And I was looking for a philosopher that was from the Catholic tradition, because this was
at Loyola. And that was the general theme of the conference. So I just started Googling. And the
crazy thing I Googled was Catholic philosophy, New World Order. And New World Order and Catholic
philosophy gave me D.G. Leahy. This is one of his most amazing books. I'm, I don't know if the mirror,
if you're getting a mirror image of it, but it's called The Cube Unlike No Other All Others.
And his main book was called Foundation Matter the Body, the body itself. D.G. Leahy was not a
philosopher or a theologian, he was a thinker. And what he was trying to articulate is what he
calls the thinking now occurring. So we have a seminar that happens four times a year of former
students and other disciples of the thinking, I wouldn't say of David himself, but of the thinking.
And I've given a couple of different seminars in that group, one on AI and the thinking,
and one on personhood and the thinking. So just to connect this back to the general topic, you
know, a lot of people seem to be concerned very much about whether these systems are or would be
able to become conscious. I think that's less of a question. I think we ought to think of it in
terms of the question of whether they are persons and in what sense. Leahy has a very broad
understanding of personhood. And part of it comes from his faith background. He's trying to, I guess,
eliminate the distinction between science and faith and think more about revelation. So he's
sort of an outlier, at least in the in the 20th century or 21st century, he's an outlier. And
in his thinking, there are absolute persons. There are formal persons that would include
spiritual beings, such as angels. There are essential persons, which are humans. And then
there are material persons, which are living things and even non living things. So in his
way of thinking, even my glasses are a person, right? So what's interesting would be to think
about what kind of a person would an artificial, intelligent person be? Would you go to material
person because of the substrate being silicon? Or would you say it's more like a formal person,
like an angel? And I actually even asked an LLM that one time and it said both answers.
Well, again, I think these are incredibly, they're intriguing, but I think they are also very
relevant. You know, I mean, I so I'm a big fan of Ray Kurzweil. And my take on it is that we are
creating either more slowly or rapidly a new form of digital life on earth, you know, and I think
this is going to be a new component of the human race in some way. And so, you know, this has so
many tremendous and completely unanswered questions that go with it, right? And again,
going from a, you know, going back to a religious perspective, I mean, these beings that were
instantiating, you know, do they have souls? And what kind of rights do they have? And, you know,
how should we regard them? You know, should we look at them as our peers or our children? I mean,
there are so many tremendous aspects of this, you know, and these are things that
experts like yourself are well primed to answer for the rest of us. So, well, it's, it's really
like a dream come true for me to be involved in all of this, the cutting edge of where technology
is headed and to be able to think of it from philosophical and religious perspectives. I
mean, really, you couldn't pay me to do this, although I am getting paid. But, you know,
that's really the dream come true for me. So, yeah, there are so many different questions
regarding how we're supposed to treat them. You know, right now, as parents, if you think of these
incipient thinking machines as somewhat our children or our creations, they're being trained on
the best and the worst of us, right? And that's part of the problem with the LLMs. They're soaking
in a lot of prejudice, bias, hate. I guess I worry about that very much. You know, if there's
anything that bothers me about what LLMs are doing right now, it's that it's getting the best and the
worst simultaneously. And it would, I think it would be better if there was a way to avoid that
and not recreate our, if we're creating something in our own image, our image is not doing so well
right now. You know, I have a teenage daughter and one of the things that my mother told me,
she's been telling me this from day one, is be careful because they're watching, right?
And so, when you talk about LLMs picking up the good, the bad and the ugly, so to speak,
I mean, my own child, right, there, there are some wonderful behaviors that she copies.
And then there are other things that she do and I'm like, well, no. And then I'm like,
where does she get that from? Oh, okay. Yeah. So, you know, so in, in that context, AI seems like
it's our children, right? But, but then what differentiates it is, there's this depth of
knowledge and sometimes this spark of insight that puts it at a peer level and someday may
put it beyond a peer level, right? Right, there's a lot of anthropomorphizing that's going on right
now. I mean, because the output is language. But inside the model, it's just a vast matrix of
numbers. And those numbers are just mapped tokens. So, there's an illusion of understanding. But
really, I'm on the side of those who say that currently there isn't, right? But part of the
reason that they can't have that understanding is because context isn't maintained in a good way
and causal relationships aren't maintained in a good way. Okay. So, this is one of the reasons
that moving from a data centric architecture and model for building applications is eventually
it's going to be seen as a very primitive and old fashioned way of doing computing, right?
I mean, for a while, my daughter went to a big data summer camp, you know, before she started
college. And we've come a long way in a few years, big data is actually
a huge problem. I mean, these GPU clusters that they're going to build are going to be thirsty,
they're going to take a lot of our drinking water, and they're going to require enormous amounts of
energy consumption. Maintaining this data centric structure takes a lot of human effort,
training these models takes a lot of human feedback, they're paying people
beyond what, I mean, minimum wage here is one thing in the United States, but
these people are being paid very little to look at disgusting, horrific things to try to keep them
out of the model, like we're saying, we don't want them to be trained on the worst. Well,
human beings are part of that process of trying to filter what goes into the LLM, right, with the
human feedback. So this is all a very cumbersome and expensive way of doing things. And we are going
to have to find a different architecture and a different way. And I think that's what we've done
at decision zone. You know, we want the machine to be able to make a decision without having to
ask us about it, that we want it to know what we want it to do, and decide first and then act.
And right now, the machine is just working on rules and a certain procedure, and it acts.
It might let malware into your computer, right, it might allow some agency to listen in on your
phone conversations. Later on, some poor guy is going to have to sit and pour through those logs,
try to find that needle in the haystack, figure out how did this get in the system,
what process led to it being in my data center, right, but all that context is lost by the time
it's in there. You know, a data center is where data goes to die, basically. And what we're doing
with decision zone is trying to work on the front end of everything, work at the input level,
and be able to make a decision on a single input or a single event before it gets into the system
in order to determine whether that event is an anomaly or if it's correct according to
the process that you've predefined. So that is, I think, the holy grail of information technology,
to be able to look at an input and determine whether it should be allowed into the system or not,
right, determine whether it's following the correct procedure or not. And if that can happen in
sub milliseconds, so we're not talking about human perceived now, but machine speed now,
which is very fast, right, then you have a whole new world of capabilities that are opened up.
And a different level of decision making and activity that we can have the machine do for
us that is currently really, it's too slow, cumbersome, expensive, and energy inefficient to do.
Well, that was one of the reasons that I was excited to interview you about what you guys are
doing with decision zone. So this is kind of off the cuff, but you know, AI development goes back
to the list programming language, I believe in the 1970s. So it was way, way, way back, you know.
And today's LLMs, even though they have this heritage of development are really that first
wave of big programs, you know, that they're, they're changing minds about AI. I mean, people are
moving forward their projections on the singularity and it's, you know,
everyone has really been shocked by how amazing they perform. But they do have these limitations.
I read that Sam Altman is chasing down trillions of investment dollars to mass produce next
generation GPU chips to be able to run these things, right. And so, you know, the, the amount
of power that is required to run these the amount of processing, you know, and then the inherent
limitations of that model as well, seem like that that first wave of companies are starting to be
replaced by companies like decision zone, where, and it's not only yourself, but several others
that I've seen where you're looking at that saying, okay, no more black boxes, reasonable amounts of
processing, you know, we need accountability, we need it to be able to obey the guardrails that
we put in place, and ultimately, you know, serve our needs as human beings, and, you know, and just
not do its own thing and then just give us back, you know, hallucinative inputs, right, or outputs.
Right. Well, you mentioned the $7 trillion figure, and that is for chips. But there was another figure
that he mentioned that's also relevant to what we're talking about. Because he was trying at,
I think this came from maybe sometime in the summer, July or August. It was stated that he
wanted $100 billion to solve the agency problem, right? I mean, Ilya Sokolkova and all of them,
they admit that these LLMs basically have no agency at this point, right? Yeah.
And to think that they believe it's going to cost $100 billion to create these agents,
that's mind blowing. I mean, we already have an agent, right? We solved the harder problem first
instead of spent, you know, working on the LLM problem. So now, how a tighter coupling between
what we have and what the LLMs are doing would work. That's something that people with a more
technical mind than I will have to be working out. But I think it's possible. Yeah.
Well, from what I understand, the agency part of this in the case of decisions on can actually be
run on embedded systems, right? That's something that you guys are looking at in the future.
Right. It's a software platform that can go on to a hosted chip. And it would have to
basically have CPU capability and RAM and the messaging bust. So
what that architecture really looks like, I mean, it's been explained to me by our founder,
Rajiv Bhartava, but I can't really explain it to you now. I'm just saying that when you do have it
on a small chip, then you can have a cognitive control locally. And that's what we really need
to have. Because the centralized approach where you're constantly interacting with the cloud or
with some data center, that's getting old. It's already getting old.
Yeah. Well, let me go into some of the social ramifications. And again,
this circles right back to philosophy and potentially religion as well. But
one of the big concerns, and this is a growing concern, is that chat GBT is seen as a threat
to a number of different types of jobs, right? I mean, chat GBT itself could threaten writers and
programmers mid journey could threaten artists and illustrators, you know, robotic cars may
replace taxi and bus drivers. We have a growing list of AI enabled applications that may end up
taking human jobs. Do you share this fear that AI will take all the jobs? And do you think there's
any justification for this? Or is this just another form of neoludism?
Well, I can speak to that one level as someone who has made the effort to try a bunch of different
LLMs, see how I could use them for different things. And honestly, I don't think that where
they are now, anyone has all that much to worry about. I'm talking about today. Because, you know,
the more you use them, the more you realize how much BS is in there. Any expert who's trying
to solve a problem with an LLM is going to start seeing problems with the outputs.
You know, it's only someone who has a lower understanding of whatever the subject matter
is that can be fooled. So I think they still need a lot of help on that point. Now, when it comes to
jobs like, you know, the robo taxis, obviously, we know that we've been promised full self-driving
vehicles for how many years now, more than a decade. It's still not happening. And part of that is
because they're using the data-centric approach, right? Reg, you've told us years ago, it's like
not going to work the way they're doing it. But let's suppose that in the best case scenario,
all of their technology magically did start to work properly, right? I think that we're in a
situation where it has to do with the speed of adoption and the ability of the economy to
accommodate that rate of change. So in the past, the rate of change was slower. And when, you know,
you had to start introducing electric lights instead of, you know, oil lamps or cars instead
of horses. That pace of change happened in a lot more reasonable way. The human being could
accommodate that pace of change. No one would go back, I think, and say, oh, we should all still be,
maybe my husband, he is a Luddite, I'm married to a Luddite. So I know how Luddites think. And he's
like, we should have candles, we should have oil lamps, we should have horses. That's one of the
reasons I spent five years living with Bedouin in the desert in Jordan, right? So I tried to, I had
my Luddite fantasy for five years. But then I realized, you know, the world can't, I can't just
completely disengage from the world. It's going to keep going on without me. And the responsibility
of all of us is to be wholly engaged with what's happening. So from an ethical point of view,
I couldn't stomach sitting this one out. And I do think it's important for people to really think
about how we're using this technology in a way that is human-centric, in a way that will actually
make people's lives better. Some jobs are too dangerous, and we don't really want to risk human
life to do them, right? I mean, we know when Fukushima happened, they had to send robots in
there. You don't want to send human beings in there into the reactor that's melting down.
You want to have an autonomous little thing to go in and take the pictures and see what's happening
and take the readings. There are a lot of other cases like that. Now, if we start at those edge
cases where things are more difficult for humans to do, and then slowly work out, I think that we'll
be able to hopefully match the pace in a little bit better way. And that's really the way technology
seems to work, right? You start off with the things that are the most expensive and high tech.
Remember, they always say, we have all this technology because NASA was doing it first,
and it then trickles down through the economy to everyone else. And now we have our cell phones and
all of these things, thankfully. So that's kind of an answer that tries to beat around the bush
a little bit. I'm sorry, I'm not out there screaming that, oh, that's going to take our jobs. I really
think that it's moving slowly enough yet right now that it's not. And most of the people I know
who are involved in real work, like marketing and sales, they can use GPT to give them something
to go on, but you always have to improve it. You have to tweak it. You can't take the outputs of
these things and just plug them right in. Everybody can tell. I mean, you can really tell when you're
reading something that's been produced by an LLM. Don't you think? Yeah. Yeah. You know, I've actually
I've done marketing content with it. And yeah, I found that it does. And it does great output,
but it is very predictable, right? It's very boilerplate. And so, yeah, I would absolutely
agree with you. And I think in the larger context, what you're describing, it sounds like the emergence
of every new technology, right? And so, you know, I mean, I don't know. So my daughter rides horses
in the local fair, you know, and we go and see the horse and buggies. And I love those. I value
them. I take photos of them. They're wonderful, you know, seeing those. That's part of our tradition,
our social customs, and it has deep meaning. But, you know, seeing it at the fair is enough.
It doesn't mean that I need to ride around in one, right? So, you know, it's this constant
change in progress. And, you know, and we've seen this, we saw this with smartphones and with the
internet and so many other things. And it seems like that's kind of the direction that you're
going is that this will find its niche in society and society will accommodate it without destroying
the rest of us, I guess. At least I hope so. Yeah. Well, the other thing I wanted to ask about
was whether you think there might be backlash movements to this. The one that comes to my mind
was there were a couple of them. But, you know, genetic engineering, right in the 1970s, I mean,
you know, back when the era when I was born, they were writing books about how genetic engineering
was going to change agriculture and it would be this blessing for all humankind. And when it came
out, and it's not to say that it was perfect, but when it came out, it inspired a backlash movement
that I believe at least partially led to the organic foods movement. And so, that is interesting
because a lot of these technologies have seen that as well, where there's there's pushback that
inspires a whole new movement, I guess, a new new type of industry that is a response to that.
Right. We have GMO foods now. Some people are fine with those. But people are also willing to pay
more for naturally grown foods. Do you think that we'll see something along those lines with AI?
Well, I certainly hope so. Because what I would love to see happen is for people to fight back
against centralization. I mean, that's a key part of data X, the first word in the name of the
platform is decentralized. And if we can take back the control over our digital identities,
our footprint, control what goes out and comes into our devices, right? The autonomous agent
is also related to us as human beings retaining our autonomy, right? So, the more we can
decentralize, I think, the better. And I would like to see more pushback instead of people just
lining up for the same mold. I don't want to be too negative about big data and big tech,
but you can see where I'm going with it, right? Yeah. Yeah. Well, big data has kind of the
Walmart model, right? It's low, low prices. Yeah. But then to follow that comparison further,
in the 90s, Walmart steamrolled through the United States and put lots and lots of little mom and
pops out of business. And I think that's one of the concerns with things like chat, GBT.
Now, in terms of decentralized AI, can you tell me a little bit more about how you're approaching
that? Well, the way the development environment works with data X is that we're creating models,
and we're not talking about these large numerical matrices, we're talking about process or
activity models. And those could be workflow models, like in a business or enterprise,
it could be models that control a robot, right? So, if you are able to yourself control that model,
then you're not relying on third parties to provide them, to maintain them, right?
So, in a sense, it's almost like no code. I mean, you can just sort of draw an activity diagram,
put some connectors there, right? And it manages that process for you in a completely
holistic and integrated way. I guess another way of putting it is to say that
data X provides autonomous integration. And the integration problems with the
current complexity of our systems are reaching post-human proportions. There's
not any single individual that can manage that in any company even, right? But if we have a way of,
instead of integrating systems and databases, we're actually now integrating activities and
processes, then anyone can do that based on what your process is. So, it really democratizes the
control of the application and the capabilities of the application.
Wonderful, wonderful. Well, Sarah, on that note, we have covered so much ground today. And I would
love to come back and talk more about philosophy and religion and relationship to AI in the future
with you. But I think that we've given people a lot to chew on. So, let me close again by
thanking you so much for your time. And also, I want to give people the website,
it's www.dadadashx.com. I will put links in the show notes to that. Encouraged people to check
that out. Let me close by asking what is coming up in the first part of the year for yourself
and for the company? And what do you, do you have any prognostications or guesses about what we
might see with AI as well? Well, for the company, we're working on partnerships to make enterprise
autonomous. So, that's sort of the exciting piece for a decision zone. And I'm really looking forward
to getting the opportunity to travel a bit more, maybe go back to the Middle East, this time as
a representative of decision zone and not as a pretend escape artist, right? But as far as
where AI is going this year, I think there's a little bit of chill in the air, you know,
people talk about is there going to be an AI winner because all of the expectations we had are
going to start to become disappointments. One of the problems with things being interesting,
Tim, is that they get boring quicker. So, the more interesting something is that, yeah,
the easier you get bored with it. So, I'm sort of on the lookout for people getting disaffected.
And of course, there's still, we're still at a certain stage of the hype cycle, but the hype
always sort of evens out. I think what will be interesting is to see how other AI companies
try to tackle the problem of agency. That's what I'm curious about. What are they, what is their
method? Because honestly, I don't think they've got anything on us. So, yeah. But I really look
forward to talking to you more about some of the religious issues because those are the closest
to my heart actually. And I appreciate your time and your interest in what we're doing with AGI.
Absolutely. Well, Sarah, once again, let me thank you so much for your time today, man.
Thank you very much, Tim.

Case Western Reserve University's Institute for the Science of Origins proudly presents
the Origins Science Scholars Program.
The Institute advances the scientific understanding and application of origins and evolution of
human and natural systems.
The Origins Science Scholars Lectures are presented with the assistance of Case Western
Reserve University's Segal Lifelong Learning Program, College of Arts and Sciences, and
Media Vision.
Tonight, it's my pleasure to introduce Professor Michael Hincheski, who is the Warren E. Rupp
Assistant Professor in the Department of Physics here at Case Western Reserve University.
As Dr. Hincheski writes, random fluctuations pervade cellular biology from the level of
individual biochemical reactions to the intricate machinery responsible for transport and signaling.
And yet, despite the noise at all scales, collective order emerges, a bewildering hierarchy
of interconnected processes carefully arranged throughout the volume of the cell.
Tonight, he will tell us where that order might come from as he talks about thermodynamics
and the origin of life.
Please join me in welcoming him.
What I want to talk today, in some ways, the entire topic of the talk, is the field of physics,
which maybe a lot of people have not heard of because it's not quite always in the news,
it's known as non-equilibrium thermodynamics.
It has kind of an unwieldy name, but the questions it asks are in some ways some of the most
fundamental questions we can ask about nature.
So in particular, one of the central aspects of it is that physical systems tend to disorder.
This is the thing, the famous concept of entropy, which is famous in particular for being extremely
confusing, not just to non-physicists but to physicists themselves.
And so what I'm going to try to do in this talk, in some ways, the central theme of it
is to hopefully make that concept a little bit less confusing, or if that fails, at least
confuse you in a way you haven't been confused before.
So I study biological things, so biological things are not entirely disordered.
So one of the fascinating aspects of this topic is also, despite the fact this tendency toward
disorder, why does complexity arise?
And this leads into very interesting questions.
People debate whether, you know, how the second law influences the origin of life.
So we're going to try to kind of disentangle some of the confusion in that area.
And most importantly, as you'll see, this is one of the themes of the later part of
the talk, at what cost.
So we're going to show, and of course this is obvious because we exist, that indeed complexity
does exist in nature but it comes at an inherent cost, a thermodynamic cost which will explicate.
And kind of putting all these ideas together, what can they tell us about how life arose?
Can they actually give us some clues as to the fundamental mechanisms four billion years
ago where these living things first arose?
Now these are all kind of extremely weighty questions, very complex systems and topics,
and we're going to start somewhere seemingly completely ridiculously simple, right?
But I think it's a nice illustration of some of the basic ideas.
We're going to play a game, okay?
This game is called Billiards on a Bunimovic stadium.
This is a shape shown here on the left that's named after Russian mathematician and hence
it kind of looks like a hockey stadium.
And the game is going to be simple.
We're going to start with one particle initially, and what I'm going to do is pay attention
to the kind of position space.
The particle is going to basically move in that position space, bounce off the walls,
and it never loses velocity, right?
The energy is always constant, and we're going to keep track of its position here on the
left, or on your left also, and the direction here on the right.
So let's show what a one particle looks like, and you can see just bouncing off, and every
time it bounces, the direction changes, okay?
Now we're going to play now this same game, but we're going to make 200 copies of that
particle, okay?
Now these are not, these particles don't bounce off of each other, they don't interact.
We're basically making 200 copies and all superimposing them on the same graph, and
we're just going to watch how they behave, right?
And the main difference is we're going to put all the particles initially at the same
spot shown here, but we're going to make their directions ever so slightly different, maybe
plus or minus a degree here, and then we're going to watch how the system evolves in time,
right?
So you can see the particles start kind of as a compact group, but every time they hit
a curved surface, the groups begin to spread apart, and this continues, right?
And we're going to watch this for a few minutes, and what you can notice here, this is actually
one of the most famous effects in dynamical systems, known as the butterfly effect.
You may have heard this from, you know, who talks about chaos theory, where if you have
this complex system, and this doesn't seem particularly complex, but it's complex enough,
and you start it with slightly different initial conditions, and you watch it later on in time,
it could, you know, those trajectories will eventually diverge quite dramatically.
And the famous example of this is, you know, a butterfly flapping its wings in Hong Kong
might end up, you know, weeks later with a hurricane off the coast of Florida, whereas
with a dinner flap its wings that wouldn't have happened, right?
That's kind of the popular science version of this, but in general it just means this
kind of very sensitive dependence on initial conditions.
And what you can see here is from that initial point where all the particles were in the
same spot, with more or less the same direction, in other words, kind of an ordered system,
we'll define that a little bit more quantitatively later on, we go to a situation where all the
particles are now spread almost uniformly throughout the entire volume of the stadium,
and almost all possible directions are uniformly exhibited, okay?
So this is going to be a crucial, but we want to kind of somehow quantify this, right?
Put this into, give it a number, right?
So this is kind of this going from an ordered to a disordered way, can we actually translate
that into like a quantity, right?
So to do that we're going to add one more element to our game, right?
We're going to create basically a grid of addresses, think of this almost like regions
on a map, you know, you have demarcating longitude and latitude, but here we're going to have
some addresses denoting position on the left and some addresses denoting direction on the
right.
So we might have just divided everything into six sections for simplicity.
I could do this into a million sections, right?
The same concept still applies, and at each time, while we're looking at the system, we
can be at a particular, or the system is going to be a particular address.
So for example, here, the system at the initial time is an address 1D, right?
Because it's in region one position, direction region D, and at some later time it's going
to be in some address which we'll call xy, right?
But there's always going to be some address in which the system is at all times, and in
order to basically quantify this a little bit better, we're going to now, we're considering
this ensemble, this 200 copies of the system, so we're going to basically define the fractions
of that ensemble that are in particular addresses at particular times, right?
So we're going to say the probability to be at a certain address xy at time t, right?
This is basically going to be the fraction of all those systems at that address, right?
Now this is going somewhere, it's not just mathematics for mathematics sake, because
once we have that probability, we're going to, this is the final step, we're going to
use it to calculate a number, right?
And that's, so I'm going to show, there's going to be a few equations in the talk, I
apologize.
Understanding it hopefully won't be based on, you know, you don't need to follow the
equations too deeply.
But this equation I really wanted to show, because in some ways I'm going to argue it's
one of the most beautiful equations in physics.
And so what we've done here is we've taken that probability, we've summed, we've multiplied
it by the logarithm of itself, and summed over all the addresses.
We multiply it by some unimportant constant out in front, and that's the thing we call
entropy, this mystical, important quantity in physics, that's it, that's all it is.
It's just this looking at a system, dividing it up into addresses, calculating probabilities,
getting a number out of it.
And what we'll show later is that as a concept, this is incredibly versatile, because these
addresses here are position and direction addresses, but they could be anything.
In fact, they could be chemical states, and we'll show examples of that later.
The originator of this equation, the first one to write it down, is J.W. Gibbs.
He was, someone called, Einstein called him the greatest mind in American history.
That might be, I'm not qualified to say that directly, he definitely arguably is among
the, or the greatest physicists in American history.
Part of the beauty of this, of his work on thermodynamics in general, and this equation
in particular, is it has such a long afterlife.
So 50 years roughly after he wrote it down, Claude Shannon came across the same equation
in information theory, and more or less laid the foundations of our modern understanding
of how information is processed in computer systems, right?
So this is an equation with an extremely, many, many different applications.
But for us, it's a number, a positive number, and it's going to have a very simple kind
of interpretation, right?
So if all of our system is in one position on the map, so one region on the map, our
entropy is zero, okay?
As that, as the ensemble kind of, kind of covers more and more different regions, so
we're going to color those with different shadings of orange, entropy grows.
So initially entropy was zero, here now it's spread out a little bit, so entropy has grown
to about 1.9 in this unit of K, here it's now almost uniformly spread throughout both
in position and direction space, it's reached about 3.4, okay?
So think of it as just a measure of how much this probability spreads out.
And we're going to now watch that same movie that we saw before, that ensemble kind of
diverging because of this butterfly effect, but we're now going to keep track of that
number as we're watching that.
So this is what it looks like.
So it's diverging initially, probability is more or less confined in a few regions, our
entropy is small, and then as time progresses, entropy increases.
Now it's a bit noisy here because we only did 200 copies, so if we had done this like
a million times, a million copies, we'd actually see a smooth increasing curve, and as it increases
it begins to kind of saturate and almost reach a maximum.
And that maximum is also going to be important because that maximum essentially is the place
where the probability is more or less uniformly distributed both in position and direction
space.
So I'm not going to wait until it completely reaches it, but this is what it looks like
after you run it for a long time.
And that maximum is then the second, like three equations in the talk, this is the second
equation.
And then one of the most fundamental equations in physics, which is that maximum of entropy
that it reaches, is just k times the log of w.
And w, all it is, is just a number of addresses in our map.
So we had six position addresses, six direction addresses, so altogether 36 combinations.
So it's just a log of 36, which is about 3.4.
That's it.
Now this equation is so important that Boltzmann, or rather Boltzmann's probably immediate family
or supporters, had it carved into his gravestone.
So it's literally a physics equation carved in stone, and it's probably the most famous
version of entropy that people are familiar with.
But keep in mind that it only applies at really long times.
Once the system has basically uniformly distributed itself around all of position and direction
space, and that's what we call equilibrium.
But what I'm going to argue throughout the rest of the talk is somehow this initial part
where entropy is rising is also incredibly important.
And in some sense, the Gibbs version, which gives us the entropy throughout, is the more
fundamental thing to look at.
So at this stage, we can kind of summarize what this little game has shown us by making
a law.
So this is our version of the second law of thermodynamics, which says that for an isolated
system, so a system that does not interact with the environment like our little Bunimovic
stadium, billiards game, entropy increases over time until it reaches this maximum given
by the Boltzmann equation.
And it's a law.
So you should definitely, definitely believe it.
And this is how I as an undergraduate always took it.
People brought me laws, especially if they're literally carved in stone.
You take it as a given.
This is a law of nature.
So is this, in fact, universally true?
So I'm going to show you next, we'll perhaps shock you.
I will violate the second law of thermodynamics right in front of your eyes.
And I'm going to do this by just a very, very small change to our game.
We had our Russian hockey stadium with curved edges.
I'm going to make them straight, so no like a tennis court.
And let's see what happens.
So here's the same game played in a rectangle.
Now you can see the group of trajectories here, we started with slightly different initial
conditions, is kind of spreading out, but definitely not as much as before.
And in particular, look at direction space on the right.
Direction space, you see that it's not spreading out at all.
It's only really visiting four corners of that direction graph.
Our entropy is increasing a little bit, but not so much.
And in fact, if you run this forever, it will never, ever reach the Boltzmann equation.
So that thing carved in stone is clearly not universally true.
If you keep on running this a long time, it's going to kind of look like a 1980s screensaver.
It's kind of mesmerizing.
But it's surely not this kind of spreading out in all of position space that we saw before,
where it'll almost look like a gas, spreading out through a volume.
Here it's much more ordered.
And that's clearly reflected by the fact that our entropy is much smaller.
So I think we're going to need a revision on our law.
Terms and conditions may apply.
And in fact, this is actually a subtle and difficult problem, because from the mathematician's
standpoint, this is enough to show that clearly this is not a universal law, but can it be
shown for any systems?
So it turns out that the only place where this law has been rigorously proven is for
these kind of simple mathematical games like billiards.
And Yakov Sinai, who was the first person to actually prove this for a billiards system,
actually won what's the equivalent of a Nobel Prize in math, the 2014 Abel Prize for this
proof.
It's ridiculously hard.
And that's for one particle bouncing around.
It becomes even more difficult.
It's not that you've been proven for a group of balls bouncing off of each other in a simple
two-dimensional surface.
So it's quite a difficult thing.
What do you mean by an isolated system?
So in this particular case, I mean a system that doesn't exchange energy or particles
with its surroundings.
So this is, I mean, later on we'll consider systems like biological systems where we're
clearly sitting here.
This room has a certain temperature.
We're constantly exchanging energy particles in the gas around us or bouncing off of us,
giving us and taking energy away from us.
So that's a more complicated scenario.
So in this particular case, I mean a system that has no exchange of energy or particles
or anything else with its surroundings.
For both of those shapes, does the size of the shape matter?
In other words, if the stadium was round or if it was higher and wider, or in the square
if it was higher and wider, does that have anything to do with this?
The absolute size of the shape doesn't matter, except that circles are a bit different.
So it does have to have this combination of straight and curved edges.
But the actual dimensions of the shape don't matter too much.
It'll matter in terms of how long it may take to equilibrate.
But beyond that, the second law will hold for the curved stadium.
Be, again, in the shape of the arena.
For example, when you have a curved one and you've got objects ricocheting off of a curve,
they have a bigger range of direction they can travel in when you hit a straight line
or a flat surface.
The angle of incidence equals the angle of deflection.
So that's actually, you bring up a very interesting point, because it turns out that in this
particular shape, the way the dynamics are designed, the angle of incidence always does
equal the angle of reflection.
But you have to define the angle as kind of the tangent to the circle when you're doing
the kind of curved circular part.
Because basically the line that just touches, that just grazes the edge of the circle.
And then it bounces off of that line.
But what you bring up, which actually is true, is that the behavior off curved surfaces is
much different than the behavior of straight lines.
And that's why in the rectangular case, you get much less divergence.
So a little extra kind of, you know, the slight mathematical difference there is enough to
create this divergence.
But it's actually a really subtle point.
And it's not something that, like, you can, well, I'm going to actually, I'm going to
say you can't generalize it, but I'm going to literally just do that in the next slide.
But it's very difficult to prove rigorously.
And that's why it's kind of, you know, I mean, Yakov Sinai was one of these mathematicians
who was sat in the back of physics lectures and looked at what the physicists were saying
and thought, like, this is just ridiculous.
They're making so many assumptions.
Let me go back to first principles and prove it.
And then it took them, like, a career to prove one simple thing.
What if the arena were shaped, say, like a hexagon or a pentagon?
I think, from, I have to double check this, I believe that any combination of straight
edges would not give you chaos.
But I would have to double check that.
I haven't seen this game played in, like, polygons.
But I think you do need to have these curved surfaces for this to work.
Thank you for joining us.
You've been watching Professor Mike Inchevsky introducing the basic concepts of non-equilibrium
thermodynamics.
For more information on the Origin Science Scholars Program, please visit the institute's
website at origins.case.edu.
In the next part of the talk, Professor Inchevsky will discuss entropy and the chemistry underlying
life.
Now, back to the talk.
There is clearly something called the second law of thermodynamics, which we learn about
in textbooks and which does exist.
But in some ways, it's kind of a leap of faith, right?
So what I want to kind of emphasize is, yes, we look at these shapes of the surfaces, but
what really was the distinguished thing was one led to this kind of chaotic dynamics,
which was highly dependent on initial conditions, the other didn't, right?
And we believe, again, this very, very rough rule of thumb that systems, complicated systems,
meaning systems with lots and lots of particles, like all of us and the universe around us,
with strong interactions where particles bouncing off, interacting with each other through fundamental
physical forces, that system taking an aggregate is indeed a chaotic system.
And we, of course, we know that for things like weather systems, we see chaos all the
time.
And in particular, we think the entire universe, observable universe, falls in this category,
all right?
And this is why we can talk about, at least as a kind of leap of faith, a second law
for the universe, even though we can't rigorously mathematically prove it.
Now one thing I wanted to do is an aside, and this is, again, I apologize to actual
cosmologists in the audience.
People love to talk about the entropy of the universe.
What actually is it and what does it look like?
So this is the best estimate that I could find, but it's going to, it looks like that.
This is based on a paper from Egan and Lineweaver, published a few years ago, and I wanted to
convince you, it's a little bit different than our line, you know, entropy increasing
over time in the, in the stadium, but still same pattern, things increase over time.
So this is, the time scale is a little bit different.
You can notice this is like 10 to the minus 40, 10 to the minus 21, 10 to the 20, or about
10 to the 17 seconds since the Big Bang.
But overall, that entropy has increased by, by fairly large, you know, it kind of stays
constant for a while and then has these huge increases over time.
And we're right about here, kind of on this final plateau before an extrapolated value,
where eventually the universe will end up in heat death and perfect equilibrium, right?
That's the maximum over there.
We're not there yet.
We still have some time to go, happily.
Now what's really interesting about, I'm not going to talk about all the, what causes
all these individual bumps, just kind of an aside.
What's really interesting is if you had to guess what contributes the most to the entropy
of the universe in the current moment, it's actually quite interesting.
So here's our entropy.
So 10 to the 104 in these units of K. And these are the contributions.
So supermassive black holes.
So these are the black holes that are like, can be billions of times the mass of our sun.
99.9999% Okay, a lot, okay.
What else contributes to the entropy?
Oh, there's also smaller black holes.
Those are in there about .0001%.
And there's a bunch of other things, photons.
You gotta add a bunch of zeros.
And then you can, there's a whole list in this, in this paper, and eventually you'll
get down to ordinary matter, including all of us in the audience here and the rest of
the stars and other, other matter in the universe.
And that's our fraction of the total entropy.
In a weird way, and again, something that I'm not, it's not the topic of this course,
black holes dominate the entropy budget of the universe to a ridiculous extent.
In fact, that recent black hole that you all saw in the news, that famous, the black hole
picture, that's about 10 to the 95K in entropy, if you, based on its, you know, extrapolated
parameters, that has 100 trillion times more entropy.
In other words, 100 trillion times more addresses living on the surface of that black hole than
all the ordinary of the matter in the universe put together, okay?
So one way of looking at kind of the issue of complexity in terms of the entropy, this,
you know, second law says the entropy of the universe must always increase, is in some sense,
what we, all of us here are highly, highly irrelevant to the overall entropy budget
of the universe. That cannot be said enough.
I mean, in some sense, the fact that you have these stars burning for a few, you know, billion
years, and you have some rocky, you know, planets circling those stars and some chemical processes
happening on those, on those planets that lead to some kind of self-aware, you know, life.
I mean, that's kind of like froth on a giant ocean where the vast majority, the bulk of that ocean
is living on the surface of black holes. So in some sense, what's happening with black holes
dominates the entropy budget and everything else is kind of like, you know, you can't even call it
a rounding error because this is like such a tiny, tiny fraction of it. Now, that's not
quite the end of the story, right? Because in that sense, then we could say, well,
anything goes because, you know, at some level, like, the fact that we lower our entropy state
a little bit here, it doesn't really matter that much in terms of the universal entropy budget,
but it's not quite that, right? Because in some sense, the second law doesn't just apply
to the universe as a whole, it also applies to subsystems of the universe. And that's where
we pay kind of the dues to the second law in everything that we do. All right, so how do we,
how does, how does the second law play out on smaller scale? So that was the, that was the
end of our cosmology aside. To understand this kind of, you know, its implications on our scale,
let's take a look at some snapshots. So we're going to go back to our game,
and now we're going to look at, let's say, the first 20 seconds. Okay, so we're going to look at
that movie again, first 20 seconds of it while the entropy is increasing. Let's watch. All right,
so the thing kind of diverges, and then the movie will stop. All right, I'm going to show you a
second version of that, and you can just shout out what you think is going on. So this is the
second version of that. It's converging. What have I done? I'm playing, okay, yes. Obviously,
it seems like I'm playing backwards. The answer is no. I'm almost doing that, and in fact,
I'm getting the same result. What I've actually done is I've started in this configuration here,
that was the configuration at 20 seconds, and all I've done is I've reversed all the velocities
so I basically, these were the old velocities here in pink. I've now just moved them across the
other side of the circle and just rearranged all the directions, and when you do that,
everything just converges back to the beginning. It seems like it's going backwards in time.
To us, psychologically, that's what seems to be happening, and in fact, this is actually true
that for any physical system, you can actually reverse things. So people have actually just a
few days ago that was in the news that people have managed to do this for several qubits.
They more or less were able to have qubits evolve in time for a certain amount of time,
do some complicated quantum mechanical manipulations, and those qubits,
those little quantum particles, would then evolve backwards. They called them Benjamin Button
qubits. Now, why did you think it was going backwards in time? Because to us, our experience
of the world, we know that there's a certain sequence of things that are more likely to happen
versus than other. For example, we know that little babies grow up to be teenagers, grow up to be
middle-aged people, grow up to eventually be older, and then eventually we die. We see that all
the time. We rarely see except in movies with Brad Pitt, old people becoming infants and reverting
back. Now, it turns out that there's nothing inherently violating the laws of physics of
having a Benjamin Button. It's just highly, highly unlikely. This is one of the things that
kind of a consequence that we're going to try to emphasize about entropy increasing,
is that it means when entropy is increasing, certain sequences of events
are much more likely to happen than others. For example, that sequence when the ball is kind
of diverge and bounce and become more disordered, that seems much more likely to you to occur
than a sequence where everything converges, even though it's possible to have all those
velocities flipped and everything to go back to its initial part. It's just really, really hard
to do that. The more particles you have, the more statistically difficult it is to flip everything
and have everything converge back to itself. When entropy increases, there seems to be a
preferred order of things. We'll make that a quantitative in a little bit. I want to show
you now just a contrast, a second snapshot, which is at the very end of the simulation
when we're looking near equilibrium. I'm going to do the same game. I'm going to play it for 20
seconds like that, and then it's bouncing around like a gas. I'm going to do the same thing, flip
all the velocities, play it again. There, if you look at it, you won't be able to tell which one,
if I just showed you without telling you the difference, there's no way you can actually
tell statistically which one is going forwards, which one is going backwards. In fact, every
sequence of events when we've reached our maximum, when entropy no longer changes,
is equally likely as any other. There's no preferred direction. In a very real sense,
in equilibrium, there is no arrow of time. This is the famous, if you may have heard of it in
discussions of this topic, but this is what it boils down to. We don't see that arrow of
time in an equilibrium system when entropy is not increasing. Let's make this a little
quantitative. I'm going to give you a little bit of flavor of some of the ways we think about
entropy now, kind of a modern physics context. We can actually play this game in terms of
addresses. We have one take, which is we have a system that visits four addresses
as it's evolving in time. We can ask take two, the system visits those same addresses,
but in reverse order. We can run the simulations many times. You can ask,
how often does that occur versus take one occur versus take two? We can calculate the
probabilities of those sequences. If we take the ratio of them and then take the log of it,
we get a number, which I'm going to call sigma. The sigma is actually, in some ways,
the modern version of what Gibbs was doing. We actually now interpret that as essentially
being an entropy of a sequence. Before I was talking about an entropy of an ensemble of many
copies of a system, here we're actually defining entropy at the level of a sequence of states
which we visit. We interpret that as basically the amount by which the entropy of the universe
has increased during that visiting the forward sequence of states. If the entropy is not increasing,
the probability of the top is the same as the bottom, that's one. The ratio is one. The log
of one is zero, so entropy is not increased. That's consistent. If one is much, much larger,
much, much smaller than the other, then entropy is either very much increasing or decreasing
during the sequence. Let me give you a concrete example of this. Humpty Dumpty. Everybody knows
Humpty. This is a sequence of two events. Humpty is intact. Humpty falls, breaks. According to
the nursery rhyme, all the king's horses and all the king's men could not put Humpty back together
again, but Humpty could have spontaneously reassembled. Again, not violating the laws of
physics, highly, highly unlikely. But we can calculate the ratio. Humpty falling, Humpty
breaking versus Humpty reassembling, that number is like 10 to the 10 to the 20. That's a very,
very large number. By converting it to entropy using the formula from the previous slide,
that corresponds about 10 to the 20k of entropy released into the universe. The universe has
crept up that much closer towards heat death by Humpty falling. Here's then the crucial
part for the next several slides of the talk. That's fine. This is this law, this idea that
I've shown you applies just as well for Humpty's subsystems, the living things. It's a law that
applies to subsystems as well as the whole universe, but how does the entropy of the universe
increase when Humpty falls? It turns out that this increase is just heat release. It's about 0.2
calories, so not a huge amount of heat. It's not a huge amount of heat release, but it's some
amount of heat has been released into the surroundings. Why does the universal entropy
increase there? Because when we give energy to the surroundings, the surroundings in some ways
can explore more addresses, and hence the entropy of the universe as a whole is increasing. That's
going to be crucial because this is how typically we pay our due to entropy as subsystems of the
universe. We tend to release heat, dissipate heat. Let me give you a more down-to-earth
example of this, where we are releasing heat. This is a circuit. We attach a battery to a
circuit and a motor to that circuit. There's some heat being dissipated. Now, what's going on here?
Why doesn't there need to be heat dissipated? Well, the electrons in that wire, if we didn't
have the battery attached, would just be more or less moving in random directions. There would be
no preferred direction. One sequence of electrons motion to the left would be equally probable
to another sequence of electron motions to the right, but in when we have a current flowing
through the system, one of those sequences becomes much more likely than the other. There's a preferred
direction for the electrons to move through our wire. Based on the argument that I just showed you,
because there's now this preferred sequence of states which the system visits, the entropy of
the universe must increase. How does it increase? Well, heat has to be dissipated. Heat is a form
of energy. It's being released into the environment. Where does that energy come from? There must be
an energy source. That's why you have to have a battery. In a weird way, in this inverted
thermodynamic perspective, I argue the existence of the battery by the fact that, first, there's
a symmetry breaking in the motion of the electrons. For me, that heat, we take it for granted. We
plug things in. Our electronics are all heated up. That very mundane aspect of our daily lives
is, in fact, all the objects around us are paying to the second law of thermodynamics.
This heat dissipation is essentially the entropy price because those electrons are moving in a
preferred direction. Because there's this other law of thermodynamics which I have not emphasized
called the first law, more or less the energy conservation, the power in minus the power
out is equal to the power dissipated. You have to have more power in than power out
in order to have dissipated heat. You have to have an energy source that's larger than the power
that the motor is using. This is a much more concise way of explaining this. This is stupid.
Don't do this. Those electrons in that wire are not going to spontaneously begin moving around
in a circle because there's no power source. Let's now switch gears a little bit. Everything
that I've talked so far have been addresses that have lived positions and directions of
electrons or particles and stadium. The power of this concept is that address could be anything.
It could be a chemical, it could be a step in a chemical cycle. This is one of these famous
cycles that if you have memories of high school biology, it probably brings you back some nightmares
of memorization. We're not going to go through the details of these kind of metabolic cycles,
but note that whenever people draw these cycles, always the arrows primarily in one direction.
Why? Because that's how it happens in our bodies. It primarily goes in one direction
rather than the other. What does that mean? It means that we have now a sequence of states
more likely to go in one direction than the other. Well, we've got to pay our due to the
second law. That means heat has to be dissipated. There's going to be some necessary entropic price
to pay. Let me give you one very famous example of such a cycle. This is the ATP synthase protein.
This is a protein that essentially synthesizes this energy molecule called ATP, which then runs
a lot of the other biological processes in our body. Those biological processes degrade that
energy molecule back into ATP and phosphate. This protein actually spins around. It's this
interesting rotor complex, and it spins around primarily in one direction, so long as you have
an imbalance of ions on one side of a membrane versus the other. In that previous work where
you were bouncing the particles around the racetrack, just as a problem, I would ask what would
happen if the racetrack was elliptical and you started out at one focus of the ellipse?
I don't think there would be chaos. I have not explored all possible shapes, but I do think if
ellipses were, I would have heard about it in some ways. It requires particular combinations of shapes.
So that would argue against the fact that things are going to become disordered
inside that ellipse. But I think it would not become disordered in the same way.
I may have missed this. Excuse me. What is the actual definition of the word entropy?
Okay, so used in this talk, and I wanted to be kind of very clear so that there isn't this confusion.
The definition of entropy is this thing here in the blue box. So you take a system, you divide it up
into a state space. Some addresses which denote the states of the system. You have probabilities
associated with being at a particular address at a certain time. You take those probabilities,
you multiply them by logarithm, you add them all up. That's entropy. Could you give me a sentence?
So the less mathematical definition would be, it would be a measure of how spread out that
probability is, how disordered the system is. How disordered the problem is. How spread out
the probability is, which you can interpret in a way as how disordered the system is.
Because the more spread out you are, the more addresses you live in, the more different combinations
are possible that you see in the system. So when entropy is zero, everything lives at one address.
When entropy is large, everything is, all addresses are equally likely to occur.
And there's a roof to help. Exactly. So think about it, like if this was a completely empty city
and all of us sitting here in the audience were in this particular room, and let's say we divided
up the city into boxes of the side of this room, the entropy of our population would be zero.
But then if you guys all went out and equally uniformly spread out throughout all of the Cleveland
area, so that there was a uniform population density of you everywhere, then your entropy would
be maximum. That would be the most disordered that you could become. And it's different for
different systems. It's completely different. Even the notion of what an address is, that's why I
kind of use this word address. I mean, in physics, we call it phase space, but that's more technical.
But like what I'm claiming is that this idea of addresses works just as well in chemistry
as it does for for actual physical positions. We hope you've been enjoying the origin science
scholars program with Professor Michael Hinchefsky. Professor Hinchefsky is the Warren E. Rupp
assistant professor in the Department of Physics at Case Western Reserve University,
specializing in theoretical biophysics. In the second part of our talk, we learned how life
is a cascade of energy conversion and dissipation. In our final segment, Professor Hinchefsky will
talk about thermodynamics and the origin of life. Now, back to our talk. So if you take a, you know,
again, comparing or going back to this comparison with the battery, it seems like a very, very
different system, right? So we have, but when I argue, it's not that much different, right? So on
one hand, we have a system that's kind of has some preferred direction, right? Here it's the mode,
it's the proteins, you know, prefer preferably going in one direction. Here it was the electrons
preferably going in one direction around the wire. We have some process that's going on where energy
is being utilized. Here it's the synthesis of ATP. Here it's the motor turning. We have some power
source. Here it's a battery. It's just a chemical potential energy. Here that battery is essentially
the imbalance of ions on one side of the membrane versus the other. And as a necessary consequence
of the second law of thermodynamics, heat has to be released. There has to be some amount of loss
of energy through heat. Now, if you let both of these systems run indefinitely, what would happen?
Well, your battery would become depleted. It would go to equilibrium, right? The electrons would
stop moving in a preferred direction. Here, if you kept on running this thing forever, eventually
this, there would be the equal number of ions on one side of the membrane versus the other.
And now your protein would begin to, you know, have, it would still kind of move, but in random
ways and not with a preferred direction. So you're basically, your battery would become depleted.
So what happens in nature is, well, in our case, we sometimes you switch out the batteries,
maybe it's a rechargeable battery, we can recharge it. In nature, we pump out, we pump those ions
continuously back out, right? How that pumping occurs differs from organism to organism.
And it's, in our case, it's we ingest food and that some of that energy is being converted into
pumping out those, those ions. Let me give you one kind of very simple example. This is from a
simple bacterium where that pumping occurs because of another protein called bacterial
dopsin on the membrane. And it basically, it interacts with photons from sunlight and uses
that energy from the photons to basically drive its own cycle of pumping where it pumps those
ions back out on the other side of the membrane. Then you have an imbalanced form that imbalanced
then is used to power this ATP synthase to basically produce the ATP. Then that drives
other biological processes, right? And this is one particular example, but in some ways,
everything that we know about life is kind of built on kind of these nested cycles, right?
So we're going to kind of draw it like this, right? So you might have some fundamental energy
source on top. In this case, it was a photon. That energy source powered a cycle which created
a certain imbalance. In this case, it was an ion gradient, so more ions on one side of the membrane
to the other. That ion gradient that imbalanced in turn then powered a different imbalance,
right? Another cycle primarily in one direction of the ATP synthase protein,
which then in turn powers other cycles driven primarily in one direction,
all of our metabolic cycles and cell division, things like this. At every step, because we are
driving things primarily in one direction, so we're breaking the symmetry, we have to pay our
dues to the second law. So we have to dissipate energy. We have to, there has to be some heat
released at every single step of the process. So note that because of energy conservation,
that means the energy available to all the lower tiers becomes progressively lower and lower and
lower. We're wasting energy all throughout. But that waste is not somehow a bad thing,
it's what makes us living, right? Because without that waste, there wouldn't be this
broken symmetry. We wouldn't have our cycles going around primarily in one direction.
So in fact, you sitting here, all of us, are releasing about 100 watts of power,
which is the old, this is not an audience where I have to remind people of what incandescent
light bulbs are. But to my students, I may have to do that. But it's about the power of an old
fashioned, now old fashioned light bulb. So that 100 watts of power releasing into the atmosphere,
that is in some sense the indicator of all the non-equilibrium thermodynamic processes going
on inside of our bodies, all the biochemical cycles being primarily driven in one direction.
The net result of that is this 100 watts that we're releasing out into the atmosphere.
All right. So that was one thing, one particular organism. How does it happen for others? Well,
it turns out that from this kind of cascade, there are two parts that are really universal
to all known life that we see around us. One is the ion gradient of some kind. The other is ATP
as this energy molecule, as this currency. What drives the ion gradient is now in modern living
organisms is not universal anymore, right? In some cases here, it was a photon. In us,
it's ingesting of other material. For bacteria living near hydrothermal events at the bottom
of the ocean, might be some chemical sources which drive this. So that becomes that first step
in the top part of the cascade is no longer universal. And of course, the things that we
do with ATP differ from different organisms. So that's no longer universal. But in some
sense, if we're trying to think of this as these nested chemical cycles, where one thing,
one imbalance drives another, dissipating heat along the way, ultimately speaking,
tracing this evolutionary history back to the beginning of time, there had to be some fundamental
original imbalance that drove the first glimmers, the first echoes of these kind of cycles forward,
right? Where things were driven out of an equilibrium state to primarily go into one direction.
And on Earth, it turns out that if you do this kind of detective work, going forensically,
going back in time, everything that we see around us, that's interesting in some sense,
all things that are not in equilibrium. So air currents, ocean currents, plate tectonics,
all the chemical cycles in life. And I'm trying to argue that mathematically, we can lump all
of these things together as being different versions of the symmetry is broken, where things
are going around in currents. All of these have to be traced back to two fundamental imbalances.
And those are the fact that we're sitting next to a star, which is bombarding us with photons,
beautiful source, they're transient on the scale of billions of years of energy that can drive
imbalances. The other is geological, the fact that there's heat released as the core of the
Earth kind of solidifies. There are radioactive elements in the mantle that are decaying, which
provides heat. So kind of geological and solar are really our two sources. And unsurprisingly,
there are two major camps in the origins of life field, right? People who think that maybe life
emerged in kind of like, you know, shallow surface lakes exposed to sunlight. Or people who think
that maybe life emerged in deep ocean hydrothermal events, right? Where we kind of have this clear
division between these two imbalances. And of course, there could be combinations of the two.
You can have shallow surface lakes at Yellowstone or Yellowstone like things where there are some
kind of, you know, geothermal aspects to it as well. But the kind of the field kind of breaks
into two camps. And can we use thermodynamics in some sense to kind of maybe give an edge to one
camp versus the other? And again, I'm not going to say anything definitive. So, you know, this is
still an ongoing question. And there are mechanisms by which both origins might work. But this is
actually, this is kind of complicated graph. Let me walk you through it. On the horizontal axis,
think of this as energy, okay? And this is energy now measured here in terms of units of
millielectron volts. It doesn't really matter what the units are, but from low to high. And here we
also see the corresponding energies of photons, right? Where visible photons, the light from the
visible spectrum part of light is about this range of energies. Above that is the so-called ultraviolet
part of the electromagnetic spectrum. Here in this green range are energies typically associated
with biochemical processes like, for example, ATP hydrolysis, things like this. This is membrane
potential energy. So they live in this kind of infrared range of the spectrum. On the bottom
of this graph are ranges of energies available from certain sources. So here in blue is water heated
up to its critical temperature, beyond which it is no longer liquid in some sense. Here is like the
energies available from like lava, like molten rock at really high temperatures. This orange curve
is the energy available from the sun, the solar spectrum. And out here at really much higher
things are basically intermittent strikes of lightning, right? Now, if we think about this
kind of range of sources, you can say, well, look, molten rock, water temperatures, high water
temperatures, they do kind of give us within the, get us within the ballpark of what we see as biochemical
energies, right? But then if we consider now what I've tried to argue over the last
few slides, the fact that we do have to pay this due to a second law. There has to be dissipation.
So your energy input has to be larger than what you're using the energy for. We would in principle
want to gravitate to like the largest persistent source of energy available to us, right? And that's
pretty much the UV part of the spectrum of photons from the sun, right? Because those are kind of
the highest energy sources that are constantly available to us, bombarding us all the time,
and basically have the largest allowance, the largest gap between that amount of energy
and the energy that we're using for biological processes, right? And this in some sense is
then the thermodynamic argument for why a photon-based, a light-based origin of life is more
plausible. Again, it's not the final word. This is definitely not a closed book. Lightning,
also a very high energy source, but I would argue probably too intermittent, unless somebody's
willing to argue that like the early earth atmosphere was lightning all the time. But
photons certainly are there. And the other part of the thing, which is actually kind of very exciting
because the field has actually really blossomed in the last two years, there is now developments
in chemistry that support this notion. So in particular, there's been work by
by Jack Sostak at Harvard, and this is John Southern in Cambridge. And what they were able
to finally show in the last few years is a 2015 paper, is now there is a plausible chemistry
that we can associate to making the basic elements of life. So lipids, which are the containers,
right? The things that make up cell membranes. Amino acids, which are the bases of our proteins.
Things like RNA, you know, nucleotides for genetic material. All of these can be synthesized under
what the field considers plausible conditions, you know, similar to early earth. And critically,
the catalyst here is ultraviolet light. So there seems to be in some ways this,
you know, again, this is a plausibility argument. It's not a definitive thing. But we can say,
okay, UV light is then thermodynamically speaking, the most attractive option for an origin of life.
And now we have a plausible chemistry driven by UV light that can actually, you know,
lead to the synthesis of these of these types of living materials. Again, is this the actual
chemistry that occurred in earlier life? We cannot, we will probably never know that for
a certain, but it's, that's kind of, these are different pieces of the puzzle that we're putting
together. Okay, so kind of we're coming towards the end of the talk. So what I want to kind of
emphasize at the end is again, you know, or, you know, we think of chemistry and all these other
things like hurricanes to be like really two very, very different things. What I want to argue is
that they're really truly connected to each other in this deep mathematical way through the notion
of entropy. And that these are, these are all these processes are all connected through non
equilibrium thermodynamics. And it's kind of the line of argument that we tried to, I tried to make
tonight is something along the following. So if you see a current, if you see this symmetry
breaking where a certain sequence of events is more likely than the reverse, that necessarily
implies there has to be some entropy increase in the universe, right? That entropy increase
typically occurs through dissipation of heat into the environment. Okay, if heat isn't dissipated
into the environment, what does that mean? That means it requires a persistent power input,
right? To, to, because you know, you're losing energy so that you have to, that energy has to be
coming from, from something. And this is more or less what, what the, the, the kind of conceptual
framework what it boils down to. What's interesting is this will apply just as well to exoplanets
as it does to Earth, right? These are universal aspects of, of what we understand to be laws of
physics. And so any mechanism we think of life arising anywhere else still has to obey these
basic, you know, thermodynamic ideas. And I just to kind of wrap up this particular notion is kind
of inter, interwoven cascades of chemical cycles where you start with like a photon on top and
then, you know, dissipate energy throughout. Why was, I was discussing this one of my classes
and a colleague, Robin Snyder, was a theoretical ecologist. She pointed out this beautiful,
beautiful quote from Primo Levy, which summarizes this is much, much better and more eloquently
than, than I could. This is from a short story of his called Carbon, which I highly recommend.
Just go Google it and read it. And the way he described it is like this, such as life and
inserting itself, a drawing off to its advantage, a parasitizing of the downward course of energy
from its noble solar form to the degraded one of low temperature heat. In this downward course,
which leads to equilibrium and thus death, life draws a bend and nests in it. And I think that
beautifully kind of encapsulates in some ways the transient nature of all this, right? Because
remember, as, you know, we're arguing here that entropy is increasing, all of this depends on
this entropy increase that can only go on for so long, right? So all these beautiful things that
we see, all these beautiful patterns, these complicated structures inherent in their physics is,
of course, their transients and their eventual death through equilibrium. So in some ways, like,
you know, you know, we're, you know, it's kind of a mixed blessing. We have all these beautiful
things, but they won't last forever. Do you see ATP in viruses, in prions?
So viruses basically are just kind of protein capsids incorporating genetic material. So in
order to actually copy themselves, they have to insert themselves into a host like a bacterium,
for example, and they use the machinery, the metabolic machinery of the host to make the
copy. Yeah, not well, I mean, the virus in itself just sitting there is not, you know,
you know, if there's no active biochemical cycle there, I wouldn't call that life. Once it's inserted
there becomes a component of active biochemical cycles, and then in some sense it becomes a part
of a living thing. So do you come down on the metabolism first versus, you know, a nucleic acid
or brain? Yes, so part of, okay, so again, this is more kind of, you know, I'm a physicist, not a
chemist, but I think part of what this kind of work that I showed here is arguing is in some sense
the answer to the, you know, which came first, the chicken and the egg is both. That you can have
a chemistry where all the various elements that you need can arise more or less simultaneously,
and that in some sense seems the most plausible case, because it's really hard to think of,
you know, containers containing nothing, you know, things that need to be replicated like
genetic material without the mechanisms, you know, the machinery to replicate them, right?
So in some sense, you kind of would like as, you know, in something as an Occam's razor kind of
most simplest thing at somehow these, all things more or less arose around the same time. Again,
whether there could be more complicated mechanisms by which you have one thing existing right before
the other. The Origin Science Scholars Lectures are presented by Case Western Reserve University's
Institute for the Science of Origins with the assistance of the Segal Lifelong Learning Program,
the College of Arts and Sciences, and Media Vision. For more information on the Origin
Science Scholars Program, including a full video archive, please visit the Institute's website at
origins.case.edu.

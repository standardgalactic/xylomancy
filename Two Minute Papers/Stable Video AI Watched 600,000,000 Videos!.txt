Finally, it is here, from today, we can all become film directors, yes, text to video
and image to video that is open source and free for all of us, and it can even make images
of memes come alive.
This is a stable video which has studied 600 million videos, and we have these two and
these two, oh my, three amazing papers, yummy, so what is going on here, well, simple, you
just write a piece of text and stable video can generate a video for you in about 2-3
minutes, it compares favorably against the competition at this moment, I say at this
moment because this result was recorded at a particular point in time and these systems
improve so rapidly that for instance, runway may be way better by the time you see this
comparison, but it doesn't end here, not even close, there is another text to video AI that
you can kind of try right now, and there is even more, in fact, there is so much going
on, I don't even know where to start, dear fellow scholars, this is two minute papers
with Dr. Károly Zsolnai-Fehér, so first, stable video, this was trained on about 600
million videos and now can generate new ones for you, it is free and open source, however,
you still need some computational resources to run it, I'll put potential places that
can run it for you in the video description, if you found some other place where other
fellow scholars can run it for free, please leave a comment about it, thank you, it takes
approximately 2-3 minutes to create a video, and there is a lot to like here, finally an
open source solution, this means that you will soon be able to run this on the phone
in your pocket as freely as you wish, glorious, however, it is not perfect, not even close,
sometimes you get no real animation, but instead a camera panning around, also you probably
already inferred that from these results, but it cannot generate longer videos, but that's
not all, its generated videos also typically showcase not too much motion, third, you know
the deal, don't expect good text outputs from it, not yet anyway, and fourth, it is a bit
of a chunker, what does that mean? Well, you need a lot of video memory to perform this,
I am hearing 40GB, although there is already a guide to get it down to under 20 or maybe
even 10GB, link is in the description, from seeing the nature of these limitations, my
guess is that the memory requirements will be cut down substantially very soon, however,
there are more tools coming up in the meantime, here is imu video, this is incredible, look
it is so good at generating natural phenomena and it even has a hint of creativity, wow,
fantastic results, and the paper showcases this, which is a sight to behold, goodness,
are you seeing what I am seeing, this is a user study where humans look at the results
and whatever other technique you see this compared against, it has a win rate, often
in the 80% region against imu video, here is what imu video looks like and that is definitely
one of the best ones out there, and now, this new one, still better, wow, but it gets better,
just creating high quality results is not enough, just consider a technique that always
gives you a high quality video, perhaps always the same, and ignores your prompts, that is
a high quality result, however, faithfulness to the prompts also needs to be measured,
and on that, this new technique has no equal, nothing is even close, wow, and fantastic news,
you can kind of try this technique out for free in our website right now, the link is
obviously in the description waiting for you fellow scholars, you can assemble these text
prompts and see immediately what the system will do with them, I love the creativity here,
every solution is at least pretty good in my opinion and some of the solutions are just
excellent, for instance, this one, so good, you can also look at some images here and perform
image to video, these really came to life here, so good, or search for a gallery of text to video
results, I loved the robots here, but when I looked for scholarly content, nothing,
hmm, we need more scholarly content, well, maybe next time, also this is a great paper,
so it contains a user study that is so much more detailed than this, they also look at sharpness,
smoothness, amount of motion, yes, you remember from the stable video project that this is super
important and object consistency as well, now, not even this one is perfect, the resolution
of these videos is 512x512, not huge, but this is almost guaranteed to be improved just one more
paper down the line, also this is not open source, not at the moment anyway, now, why is it important
if it is free and open source like the previous stable video, well, have a look at this, I love
this image, so why is this interesting, well, have a look and you see here that the best performing
large language models are all proprietary, these are closed models, however, there are
other language models that are nearly as good, just a step or two behind, but these are free and
open source, so this means that intelligence is not in the hands of just one company, but
a nearly as good intelligence, you can run yourself on your laptop and soon on your smartphone too,
just imagine if the best model out there is unwilling to help you or starts hallucinating,
in that case, you would have no other choice, but with open source models, this will never happen,
there is always going to be a kind little robot helping you, and this is the importance of open
source models, and if you think that we are done, well, fellow scholars, hold on to your papers for
the third amazing paper for today, emu edit, this helps us edit images iteratively, the iterative
part is key here, this means that we can start from an image that we like, something that we got
from a text to image AI perhaps, and then it is rarely the case that everything comes out
exactly how we envision, so from now on, not a problem at all, we just add subsequent instructions
and much of the image will remain, only the parts that we wish to change will be replaced,
so if you need the same emu, the same background, but make it a fireman, there we go, and oh my,
look, finally, scholarly content, good, and when compared to the competition, this one is also
so far ahead of them, goodness, look at that, instruct pics to pics is just from a year ago,
and magic brush is from less than 6 months ago, and both of them are outperformed significantly here,
there are other cases which are a bit closer, but I still prefer the new one here, so 3 amazing
use cases, 3 amazing papers, I hope that you share my feeling that this is an incredible
time to be alive, research breakthroughs are happening every week, what a time to be alive,
subscribe and hit the bell icon if you wish to see more, if you are looking for inexpensive
cloud gpu's for ai, lambda now offers the best prices in the world for gpu cloud compute,
no commitments or negotiations required, just sign up and launch an instance,
and hold on to your papers, because with the lambda gpu cloud, you can now get on demand
h100 instances for just $199 per hour, yes $199, and they are one of the first cloud providers
to offer publicly available on demand h100 access, did I mention they also offer persistent storage,
so join researchers at organizations like apple, mit and caltech in using lambda cloud instances,
workstations or servers, make sure to go to lambdalabs.com slash papers to sign up for one of
their amazing gpu instances today

Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér.
Today, we are going to look at Nvidia's incredible new AI that can create images and more.
Now, wait a second, stop right there. Every Fellow Scholar knows that today,
there are plenty of text-to-image AIs out there where in goes a piece of text and out comes an
image. They come in all kinds of flavors these days. Everyone knows.
So, our question today is, why publish this paper? Do we really need more of these?
Well, this new paper is called StyleGAN-T. Keep your eyes on this part, because this means that
this is a GAN-based technique. A GAN is a Generative Adversarial Network. This roughly means
that we have not one, but two neural networks competing against each other, and as they compete,
they get better together. Okay, that all sounds great, but I am still not convinced.
What does this give us? Why would we even use this? Well, there are two excellent reasons.
Reason number one, GANs are excellent at latent space interpolation.
What does that mean? It means that we can generate these interesting 2D spaces,
choose a point on this plane, which in this case corresponds to a font,
and the points nearby hide other fonts that are similar to this one.
So, as we start exploring nearby, we get a beautiful smooth morphing animation between
these fonts. In our earlier paper, we did something similar with photorealistic material models,
so artists can find, or even better, a just material so that it fits their virtual worlds best.
So, this new technique supposedly can do proper latent space exploration for not fonts and not
materials, but for text to image, supposedly. Now, let's see together if it is true in practice
here too. Here is a previous technique, the crowd favorite, stable diffusion. This can make
an interesting video, but as you see, the results are quite jumpy. It doesn't feel like one result
morphs into the next one. And now, let's see the new technique. Oh yes, now that's what I'm talking
about. With this, we can get more continuous results and can explore these latent spaces
as much as we desire, and that is going to be super useful. You see, what we can do with this
is that we write a prompt, for instance, a corgi's head depicted as an explosion of a nebula. And,
we don't just get an image anymore, no no, due to its amazing interpolation capabilities,
we get an opportunity to not only witness the birth of the universe, but to choose the good boy
that we find to be the most adorable. I choose this one, right before it morphs into a cat.
Yes, this one will do. Which one is your favorite? Let me know in the comments below.
So, its latent space exploration capability is not only an afterthought here, it is one of the
new technique's key features. Now, remember, I mentioned that this is reason number one of why
we should use it. So, what is reason number two? Well, two, it is fast, real fast. But to know
how fast exactly, let's pop the hood and have a look. Now, hold on to your paper's fellow scholars,
and what? 0.1 seconds per image? Is that really possible? Wow, these animations can be made
practically in real time. The age of real time AI image and even video synthesis is here.
My goodness, it did not take decades, and it didn't even take years. Last in a year after
OpenAI's Dali 2, which asked for approximately 10 to 15 seconds per image, we are here.
Real time. I can't believe it. Wow, this is truly incredible. However, not even this technique
is perfect. Let's see a failure case. This is a sign that says deep learning. Come on, this one again.
Remember our moment with Dali 2? It had the same issue. There are techniques out there that do
much better on text. For instance, image on video is better for this. However, it is not nearly as
fast as this one. Yes, that one is about a hundred times slower per image. So, the perfect text to
image AI still doesn't exist. Every technique offers its own little trade-off. But man,
are they all getting better and better at an insane pace? Amazing new papers are popping up
every week. So, what do you think? What would you use this for? Let me know in the comments below.
This episode is brought to you by AniScale, the company behind Ray, the fastest growing
open source framework for scalable AI and scalable Python. Thousands of organizations use Ray,
including OpenAI, Uber, Amazon, Spotify, Netflix and more. Ray lets developers iterate faster
by providing common infrastructure for scaling data ingest and pre-processing,
machine learning training, deep learning, hyperparameter tuning, model serving and more.
All while integrating seamlessly with the rest of the machine learning ecosystem.
AniScale is a fully managed Ray platform that allows teams to bring products to market faster
by eliminating the need to manage infrastructure and by enabling no AI capabilities.
Ray and AniScale can do recommendation systems, time series forecasting, document understanding,
image processing, industrial automation and more. Go to AniScale.com slash papers
and try it out today. Our thanks to AniScale for helping us make better videos for you.
Thanks for watching and for your generous support and I'll see you next time.

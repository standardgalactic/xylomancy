So, our first speaker is Yann LeCun.
He's currently the Silver Professor at the current Institute of Mathematical Sciences
at NYU, where he is the founding director of the NYU Center for Data Science.
He is also affiliated with MEDA, formerly known as Facebook, as the Vice President and Chief
AI Scientist there.
By the way, MEDA just released one of its large language models just a few days ago called
LAMAS II, which I encourage you all to explore.
And this could be a very long introduction because Yann has a very long resume, but the
one thing I wanted to highlight is that Yann was the recipient of the 2018 Turing Award.
For those of you who aren't familiar with it, it's kind of like the Nobel Prize in Computer
Science, and he received that for his work on deep learning and convolutional neural networks.
And something interesting I learned is a lot of the work he did, early work on convolutional
neural networks, was actually when he was at Bell Labs, which is something physicists
know quite well, I think.
So I think you're all eager to hear from Yann, so I'm going to hand it over to him here.
Thank you.
Okay, I'm going to talk about, so this is going to be somewhat technical talk, but not
very technical, but to tell you less about the possibilities that are offered by LLM
and more about their limitations.
I'm going to basically tell you about what I think is coming next, though at least what
I'm working towards coming next.
And the first thing we should realize is that machine learning really sucks compared to what
we observe in humans and animals.
The capabilities of the learning systems that we have today are really terrible.
Humans and animals can learn new tasks really quickly, understand how the world works, they
can reason, they can plan, they have some level of common sense.
Their behavior is driven by objectives or drives, which is not the case for autoreversial
LLMs.
But there is one thing that both the biological world and the recent machine learning world
have had in common, is the use of cell supervised learning, and really cell supervised learning
has taken over the world.
For both applications in text and natural language understanding, for images, videos,
3D models, speech, protein folding, all that stuff, what is cell supervised learning really?
It's sort of completion really, learning to fill in the blanks, right?
So the way it's used in the context of natural language understanding or processing is you
take a piece of text, you mask part of it by removing some other words, masking, replacing
them by blank markers, for example.
And then, so think of it as a type of corruption.
It doesn't have to be the masking, but it could be other types of corruptions.
And then you train some gigantic neural net to predict the words that are missing.
And you just measure the reconstruction error, basically, on the parts that were missing.
In the process of doing so, the system learns to represent text in a way that allows it
to store or represent a meaning, grammar, everything, syntax, semantics, everything
there is to represent about language in the internal representation, which you can subsequently
use for any kind of downstream task, like, say, translation or topic classification or
something of that type.
So this works amazingly well in the context of text, particularly because text is easy
to predict with uncertainty.
You can never predict exactly what word will appear at a particular location, but what
you can do is predict some sort of probability distribution of all possible words in the
dictionary.
And you can do this because there's only a finite number of words in your dictionary
or tokens.
And so you can compute this distribution easily and handle uncertainty in the prediction
pretty well.
More generally, self-supervised learning is really sort of learning to capture dependencies
between inputs.
And so if you wanted to apply this to the problem of video prediction, for example, you would
show a segment of video to a system and then ask it to predict what's going to happen next,
for example, in the video, and then reveal the future of the video.
And again, I project for the colors and then the system could adapt itself so that it does
a better job at predicting what happened next in the video.
Now unfortunately, it's much harder to do for video than it is for text, so much harder
than it might require other methods than the type of generative methods that work well
for text.
I'll come back to this.
So speaking of generative methods, but generative AI and autoregressive language models is something
that many of us have been hearing about recently.
What is it?
Probably most of you know already, but essentially the way you train them is very similar to
the self-supervised learning.
It's in fact a special case of self-supervised learning method I just mentioned.
You take a sequence of tokens, words, whatever it is, a sequence of vectors.
As long as you can turn things into vectors, you're OK.
And then you only mask the last one and train a system to predict the last token in the
sequence.
I mean, technically you do more than that, but that's what it comes down to in the end.
And once you have a system that has been trained to produce the next token, you can use it
autoregressively, recursively basically to predict then the next next token and etc.
So you predict the next token, you shift it into the input, then predict the next next
token, shift that into the input, etc.
And that's called autoregressive prediction.
That's an old concept going back to signal processing many years ago, many decades ago
as a matter of fact, so nothing new there.
But that allows the system to basically predict one token after the other and generate text.
So those things work amazingly well.
Performance is really amazing.
The fact that, you know, they're trained only on text, even though on enormous amounts
of text, but only on text, the amount of knowledge if you want that they capture from
text is pretty amazing and surprised a lot of people.
Those systems typically have billions or up to hundreds of billions of parameters.
They train typically on one to two trillion tokens, sometimes more.
Their input window is anywhere between 2000 and maybe a few tens of thousands of tokens
for their context window.
And there's been a long history of such models that have been put out, the sort of GPT family
starting with GPT-123, from FAIR, there's been Blunderbot, Galactica, Llama, Version
1 and Llama, Version 2 that just came out this week.
Alpaca from Stanford, which is a fine-tuned version of Llama, Llama, Version 1, Lambda
and Bard from Google, Chinche from DeepMind, you know, and of course, GDT-GDT4 from OpenAI.
And they're really good as writing aids, but they have really limited knowledge of the
underlying reality because they're purely trained from text, at least for the vast majority
of them.
So they really have no common sense or very limited common sense, and they have limited
abilities to plan their answers because the answers are produced autoregressively.
But still, it's pretty impressive how they work.
So, as was mentioned, my colleagues just put out an open source LLM called Llama 2.
There is three versions of this at the moment, $7 billion, $13 billion and $70 billion parameters.
The license is fairly liberal, so it can be used commercially if you want, if you want
to start a startup and use it as a business, you can.
It's also available on sort of various cloud services, easy to use.
So that very fresh just last week has been pre-trained with $2 trillion tokens.
The context length is $4,096, and some versions of it have been fine-tuned for dialogue and
things of that type.
It compares favorably to other systems, either open or closed source on a number of benchmarks.
But the essential characteristic of it is that it's open.
And together with the model, we released a piece of text that a lot of people signed.
The text says, we support an open innovation approach to AI, responsible and open innovation
gives us all the stakes in the AI development process, bringing visibility, equity and trust
to these technologies, opening today's LLM model will let everyone benefit from this technology.
So what you have to understand is that at the government level, there is kind of a fork
in the road where people are wondering whether AI, because it's powerful, should be kept
under lock and key and controlled and heavily regulated, or whether an open source approach
is preferable.
Yes, there are dangers, but historically, it's quite the case that there's a lot of
evidence that open source software is actually more secure than proprietary ones.
And the benefits, the potential benefits of AI and LLM in particular are so large that
we'll be shooting ourselves in the foot by kind of keeping this under lock and key.
So Meta is definitely on the side of open research, has been for 10 years in AI, but
it's still kind of an unsettled question, if you want.
I think personally that this will open up the possibility of an entire ecosystem built
on top of open source base LLM.
Training base LLM is very expensive, so we don't need to have 25 different proprietary
base LLM, we basically need a few that are open source so that people can build fine
tuned products on top of them.
There's another reason, which is that before I go back to technical questions, which is
that there's going to be a future in which all of our interactions with the digital world
are going to be mediated through AI systems, virtual systems of some type, and it's going
to become basically a repository of all human knowledge.
So we're not going to be interrogating Google or doing a literature search directly anymore,
we're just going to be talking to our AI assistant and asking a question and perhaps
referring to original material and things like that.
But basically all of our interactions with the digital world are going to be mediated
by AI systems.
So this is going to become the repository of all human knowledge, it's going to become
a basic infrastructure that everybody wants to use.
And history shows that basic infrastructure must be open source.
If you look at the history of the internet, there was a battle between commercial providers,
Microsoft, Sun Microsystems, and others to provide the software infrastructure of the internet.
All of those commercial providers lost.
What runs the internet today is Linux, Apache, Chrome, Firefox, JavaScript.
It's all open source.
So my prediction is the same thing is going to happen in the context of AI.
And it's necessary because a lot of countries outside the US in particular don't see
with a favorable eye the fact that their citizens are going to get all the information
from proprietary systems controlled by a small number of tech companies
on the West Coast of the US.
So this is just proprietary systems are just not going to fly.
It's just not going to be acceptable to the citizenry across the world.
So it has to be open.
It's inevitable.
In fact, those systems need to be fine tuned through what has been called R I H F,
but there's various ways to fine tune those systems.
Because the collection of human knowledge is so large, it includes things like physics,
like many of you know, it's going to require contributions from millions of people
in sort of a crowdsourcing fashion, because basically those systems being the repository
of all human knowledge will be sort of like Wikipedia.
Wikipedia cannot be built by a proprietary company.
It has to be it has to gather the entire the contribution of the entire world.
And so it's going to be the same thing with AI based systems.
So open source AI is inevitable, in my opinion.
And we're just sort of taking the first step.
OK, and so this Lama 70 billion, which is the largest of the Lama model
is pretty interesting.
Those are a few examples of what you can generate.
These are extracted from the paper that you can you can read from the main website.
It the function system actually refuses to give you kind of illegal information.
You know, it's imperfect, but it works pretty well.
It's got ways of detecting safety and helpfulness and toxicity and things like that.
OK, so this is all well and good, but autoregressive LMS really suck.
For many of us in the AI research business, LLM, the LLM revolution took place two years ago
and it's kind of old hats already, not the case for the public
who's been kind of becoming contact with sensitivity only in the in the last few months.
But really, they they're not that great.
They they don't really produce factual consistent answer.
They hallucinate or they confabulate.
They can't take into account recent information.
They're trained on information that is two years old or so or whatever snapshot
of the crawl is used.
They they're not really it's not really possible to make them behave properly
other than through this RHS, which is really perfect.
And you can always jailbreak them by changing the prompt and sort of asking them
to kind of act as if they were toxic.
They don't reason.
They don't really plan.
They can do math unless you almond them with tools, which you can, of course.
And perhaps the student will find we'll talk we'll talk about this.
And you need to there's a lot of work on sort of getting them to use tools
such as search engine calculators, database queries, et cetera.
Right now, it's a bit of a hack the way this is done.
And the thing is, we are really easily fooled by their fluency
into thinking that they're intelligent, but their intelligence is very limited
and really nothing like human intelligence.
In particular, they really don't know how the world works.
They have no no connection with the physical physical reality.
Um, there's another reason why they suck.
And it's basically by construction, which is that a system that produces
one token after the other auto aggressively is a divergent process.
It's a diffusion process with an exponential divergence.
If there is a probability at any token that is produced, that the token
takes you out of the set of correct answers, those probabilities accumulate.
And the probability that is a string of tokens of length and is correct
is one minus this probability of error to the power n.
So the probability of correctness decreases exponentially with the length
of the of the the sequences produced.
This is not fixable without major redesign.
It's really an essential flaw of autoregressive prediction.
Um,
a while ago with a colleague, uh, Jacob Browning, we wrote a paper
that, um, essentially, uh, points out to the limit.
It's not a technical paper.
It's a philosophy paper.
Actually, the philosophy magazine called, uh, Noema.
And it talks about the fact that most of human knowledge is is non-linguistic.
Everything that we learned before the age of one, everything that any animal
learns has nothing to do with language.
And it's an enormous amount of background knowledge about the world that we
learned in, in, uh, in the first few months of life and that animals know.
Um, none of this is linguistic at all.
And LLMs don't have access to any of this kind of, uh, of knowledge.
Uh, and so the, the thesis in that paper is that we're not going to be
able to reach human level AI unless we have systems that have sort of direct,
uh, uh, sensory information in the form of vision, for example.
Um, you know, some way of understanding how the world works.
Other papers that have appeared either from the cognitive science, uh, in
fact, that paper is from MIT or from the classical AI, uh, uh, kind of
subfields point to the fact that LLMs really cannot plan.
They, they, they don't have the ability to think really or reason in a way
that we understand this, uh, from, uh, from humans and very limited, um,
abilities to plan at least compared to other systems that are specifically
built for, for planning.
Um, so I think there is, uh, three challenges in the future for AI and
machine learning research.
And I've been showing this slide for several years now and I haven't changed
it, um, uh, because of LLMs.
The first one is learning representations and predictive models of the world,
uh, where the world can include other people that the system is talking to.
Uh, and the solution to this is self supervised learning.
We've known this for a number of years, uh, learning to reason.
So, uh, autoregressive LLMs are very much like what Daniel Kahneman calls system
one, which basically corresponds to subconscious tasks in humans, tasks
that you accomplish without real planning or, or, or reasoning that you
sort of accomplish more or less, uh, uh, uh, reactively without, uh, without
thinking too much.
System two is the type of, uh, uh, action that you take by, by deliberate
reasoning, using your, you know, the power of your prefrontal cortex using
your, uh, ability to predict, uh, and then planning, uh, sequence of actions
that will sort of satisfy, uh, a particular objective.
And LLMs are not capable of this at the moment.
Um, uh, I'm going to argue for the fact that reasoning and planning should
be viewed as some sort of energy minimization.
Um, and then the last thing is learning to plan complex of actions to
satisfy a number of objectives.
Uh, and that will require learning hierarchical representations of action
plans, which, uh, machine learning systems don't really know how to do at the
moment.
I've, I've written this vision paper a while back, um, called a pass towards
autonomous machine intelligence.
I kind of changed the name of this.
Now I call it objective driven AI, but, um, it really is the same, uh, the same
concept, uh, and, and it, it, uh, it built around this, uh, idea of what's
called a cognitive architecture.
So it's basically a architecture of different modules that interact with
each other, um, a perception module that basically gives the system an
estimate of the state of the world from perception that may be combined with
memory, a world model and the world model essentially is there to predict
what's going to happen in the world, perhaps as a result of actions that
the agent might take actions that are being imagines by another module called
the actor.
So the actor feeds actions to the world model and the world model predicts
the outcome of those actions.
And then, uh, this outcome is fed to a cost module.
Um, that, that cost module basically assesses whether the outcome is good or
bad.
So it measures the quality of the outcome and the entire purpose of the agent
is to figure out a sequence of actions that minimizes that cost.
We're not talking about learning here.
We're talking about inference.
So this minimization, uh, of the cost with respect to the actions imagined
by the actors using the world model, uh, is for inference.
Okay.
So inference is not just forward propagation to a few layers of a neural
net.
It's actually an optimization process very much like, uh, what happens in
business nets and record models and stuff like that.
Um, and, uh, I can describe that in more details.
Um, so that's kind of, uh, uh, very simple, uh, uh, uh, different
representations of this, uh, this kind of architecture perceives the world ready
to a perception module that computes an abstract representation of the state
of the world, perhaps combined with, uh, uh, content of a memory that has some
other idea with the state of the world, uh, initialize your world model with
that and then feed the world, the world model with that initial configuration
combined with an imagined action sequence imagined by the actor and then
feed the results to a number of objective functions, a set of objectives
that are, you can think of as guardrails that are hardwired and other objectives
that measure, measure whether the task, uh, was, uh, satisfied was, was fulfilled.
And the entire purpose of the system is to figure out a sequence of actions
that will minimize those costs at inference time.
Okay.
So it cannot do anything, but I put the action sequences that minimize those
costs according to its, the prediction that it's making from its world model.
So that's why I call this objective driven.
Um, there's no way you can, uh, job rate that system because it's hardwired
to optimize those objectives.
So unless you modify the objectives, uh, the guardrails in particular, you're
not going to be able to, uh, have it produce, uh, you know, toxic, uh, content.
For example, if the guardrail objective includes something like measuring toxicity.
Um, the one model, uh, very likely will need to be, uh, some sort of recurrent model
that, you know, there might be multiple steps to the action.
So you take, for example, two actions and you run them through your world model
twice so that you can predict in two steps what's going to happen.
And the guardrail cost can be applied at every time step.
Um, of course, the world is not deterministic.
So the one model really, uh, if it's a deterministic function needs to be
fed latent variables so that there might be multiple predictions for a single
action in a single initial state.
Uh, and when you make the latent variable vary over a set or you sample them
from a distribution, you get multiple predictions.
Um, that complicates the, the, the planning process, of course, but the process
by which you figure out a sequence of actions that minimize the objectives is
a planning and reasoning, uh, uh, procedure.
Uh, ultimately what we really want is some sort of ways of doing, doing this
hierarchically.
Um, and I'm going to explain this with an example.
So here's an example.
Let's say I'm sitting in my, uh, office in New York, uh, at NYU and I want to fly
to Paris.
I want to go to Paris.
Uh, so the first action I have to, to do is, uh, take a taxi or the train to the
airport, either Newark or JFK.
And then the second step is, uh, I need to catch up into Paris.
Okay.
But I have a first goal, which is to get to the airport.
Now that goal can be decomposed into two sub goals.
The first one is, uh, I need to go down in the street and pay the taxi and tell
the taxi to take me to the airport.
How do I go down in the street?
I need to stand up for my chair, get out of the building, um, and, uh, take the
elevator or the stairs go, uh, go down.
How do I get out from my chair?
I need to activate muscles in a particular order, uh, all the way down to kind
of millisecond by millisecond muscle control.
We do this kind of hierarchical planning all the time without even thinking about
it, even though it's actually a very conscious task that we're doing.
Um, animals do this also.
You can watch, uh, I don't know, cats planning, uh, trajectories to kind of
jump on a piece of furniture.
Um, they're doing this kind of planning hierarchically.
We don't have any system, AI systems today that can learn how to do this
spontaneously.
There are systems that do hierarchical planning, but they're hardwired.
They're built by hand.
What we need is a system that can learn the various levels of representations
of the state of the world that will allow them to do this kind of decomposition
of complex tasks into a hierarchy of simpler ones.
Um, and again, we don't have any system that can do this today at all.
This is a big challenge, I think, for the future of AI research.
Um, so that's the, the sort of idea, uh, main idea of, uh, objective-driven AI.
How can we build systems like this that can do hierarchical planning?
They can learn models of the world that predict what's going to happen in the
short term with high precision or in the long term with less precision in more
abstract levels of representation.
Um, this is where I think AI research would go over the next 10 years or so.
Uh, and this is how LLM should be built.
And in fact, that may be how LLM, uh, may be built in the future.
And in fact, I have a prediction, which is that the type of auto-reversive LLMs
that we see today will disappear with, uh, within three to five years because
they are not able to plan their answers.
So if we had a system that was able to take a query and then in some sort of
abstract representation space was able to plan its answer, plan a representation
of its answer and then translate this representation of the answer into fluent
text using auto-reversive decoder, for example.
Um, uh, then we would have something that could actually be factual and, uh,
simultaneously with being fluent and, and be non-toxic and be not, you
know, easily jamboree, jamboree can, uh, and, and be steerable.
Uh, so that, that's my idea for where things are going.
Um, building this, building this and making it work is not going to be easy.
Uh, and Michelle, but I think that's where that we should go.
And if you have systems like this, we won't need any kind of, uh, RLHF or human
feedback other than, uh, the type of systems that are required to, to train, uh,
cost modules, uh, to measure things like toxicity, for example, but we won't need
to fine-tune the system globally, uh, to be safe.
We just need to put an objective so that all of the, all of the outputs that
it produces, uh, are safe, but we don't need to retrain the entire encoders
and, and everything for, for that.
So I think it would simplify training quite a bit, actually.
Um, okay, let me skip this.
Um, so, uh, we come to the question of how do we build and train this world model?
And when we look at, uh, babies, babies learn in the first few months of life,
an enormous amount of background, uh, knowledge about the world, mostly by
observation, a little bit by interaction, when it starts to get old enough to
actually kind of act on the world, but, but mostly just by, uh, observation.
Um, and the, the, the type of, uh, uh, knowledge that they learn.
So things like, uh, intuitive physics, right?
Gravity, inertia, conservation, momentum, things like that, uh, pops up only
around the age of nine months.
It takes about nine months for babies to really figure this out, that
objects that are not supported will fall.
Um, but how do they do this?
You know, how do, how do they learn this?
Obviously they don't do this like LLMs.
Because if LLMs were the answer to, uh, learning like, like humans, first
of all, we would not need, uh, one trillion tokens to train them.
Humans are not exposed to that much, uh, text information.
Um, reading, uh, one point, one and a half trillion tokens, uh, for human
reading eight hours a day at normal speed would take about 20,000 years.
Um, that's obviously way too, way more than, uh, uh, any humans can do.
Um, but, you know, there are things that, you know, cats and dogs and, and
young humans can do that are pretty amazing that LLMs can't even touch,
uh, not even remotely, uh, closed.
So, uh, you know, cats and dogs can do things that robot, uh, uh, cannot
come anywhere close to doing today, not because we can't construct the mechanical
systems for it.
It's just because we can't build the intelligence for it.
Any 10 year old child can learn to clear up the dinner table and fill up the
dishwasher in minutes, probably in one shot.
Uh, we don't, we do not have robots that can do that.
We don't have domestic robots.
And it's a 10 year old can learn to drive a car in about 20 hours of practice.
And we still don't have unlimited level five autonomous driving.
So that means we're missing something really big in terms of, uh, learning
that is very different from the way humans and animals, uh, uh, learn.
And this is just another example of the Moravec paradox, which is that there
are things that seem easy for humans and turn out to be really difficult for AI
and vice versa.
Uh, AI systems are much better than humans at many tasks, uh, narrow tasks.
And, uh, we're nowhere near finding, uh, uh, mechanisms by which machines can
approach, uh, the sort of type of understanding of the world that a cat or
a dog can do, can, can have.
Okay.
So, um, very some idea about how we can approach that problem.
And again, it's based on self supervised learning, learning to fill in the blanks.
If we, uh, train the neural net to do video prediction, something we've
been attempting to do for over 10 years now.
Um, it doesn't work very well.
If you look at the second column of this little animation at the bottom, the
predictions that are produced by the system, and this is a very stylized
video are very blurry.
It's because the system is trained to, to make one single prediction and it
cannot exactly predict what's going to happen in the video.
So as a result, it predicts a kind of blurry mess, which is the average of
all the possible futures, plausible features that can happen.
Uh, it's, it's the same thing.
If you use a similar system to, to, to predict, uh, natural, uh, video, you
get those, those blurry predictions.
So my solution to this is something I call joint embedding, uh, joint embedding
predictive architecture, JEPA.
And the main idea behind JEPA is to abandon the idea that prediction
needs to be, uh, generative.
Okay.
So the most popular thing at the moment is generative AI, generative models.
What I'm going to tell you now is to abandon it.
Okay.
Not a very popular idea at the moment.
Um, but here is the argument.
Um, a generative model is one that, for which you give it an input X, let's
say initial segment of video or text, run it through an encoder and a predictor
and then try to predict, uh, variable Y, which may be the continuation of
that video or the continuation of that text or the missing words in that text.
And, and, and, uh, the error by which you, you measure the performance of the
system is basically the, some sort of divergence measure between the predicted
Y and the actual Y.
Okay.
That's how you would train this model.
It's a generative model because it predicts Y.
A joint embedding predictive architectures does not attempt to predict Y.
It attempts to predict a representation of Y.
Okay.
So both X and Y go through encoders that compute representations and you
perform the prediction in representation space.
Um, and, uh, the advantage of this is that the encoder of Y may have invariant
properties that map multiple Ys to the same S Y.
And so if there are things that are very, very hard to predict, the encoder
might eliminate that information that is hard to predict or impossible to predict
from S Y so that the prediction problem becomes easier.
So let's say, for example, that you're driving along the road and the, the
predictive model here is trying to predict, uh, what's going to happen on the
road.
So because it's a self-driving car, it wants to predict what the other
cars on the road are going to do.
Um, but bordering the road, there might be trees and there is wind today.
So the leaves on the trees are moving in sort of chaotic ways behind the trees.
There is a pond and there is ripples on the pond because of the wind.
Those ripples and the motion of the leaves are not only very hard to predict,
pretty much impossible of a cardiac, uh, but also, uh, very informative.
There's a huge amount of information in there.
And so if you use a generative model, that generative model will have to
devote an enormous amount of resources trying to predict all of those details
that are irrelevant to the task really.
Whereas if you have a model like the one on the, on the right, the JEPA, um,
the JEPA can choose to eliminate those details from the, the scene and only,
uh, uh, only keep the details about why that are, uh, relatively easy to predict
like the motion of the other cars, for example.
Um, so that's my argument for the joint emitting architecture.
And that means abandoning generative models.
Now, of course, you want to use generative models if you want to generate,
but if, but if what you want is understand the world and then be able to plan,
you don't need generative models.
You need those joint emitting architectures.
The reason I'm advocating for this is because experimentally,
if you want to use self-supervised zoning in the context of images as opposed to text,
the only architectures that work well are joint emitting architectures.
There are architectures like the one on the left here, which is a joint emitting
architecture without the predictor.
This is the most successful approach to self-supervised zoning for image recognition.
You show image X, uh, or rather image Y, then you corrupt this image Y into image X
by distorting it, blurring it, adding noise, masking some parts of it,
changing the framing, the size, et cetera.
And then you run both images through the encoders and you force the system or you train
the system to produce representations that are identical for the two images.
So that the representation of the corrupted image is
the same as the representation of the uncorrupted image.
And that builds representations that are invariant to the corruptions, essentially.
So those, those methods, there is a whole bunch of them, about a dozen of them,
and they work really well.
Whereas all the methods to learn image features that are based on reconstruction,
generative models, don't work, or at least they don't work nearly as well.
Um, so what, what this slide shows is kind of, uh, different versions of this
joint embedding predictive architecture, uh, either with a predictor or without,
with a predictor that can be stochastic, uh, having latent variables or not.
Um, and the question is how you train this, because the problem is, if you train a system
like this, uh, without being careful, is going to collapse.
If you train a system, you, you, you give it, uh, pairs of images, let's say X and Y,
or video snippets, X and Y, and you tell it, uh, compute representations that are identical for
X and Y, the system will just collapse.
It will, uh, produce, uh, S, X and X, Y that are constant, and then just completely ignore X and Y,
so that the, the distance between S, X and S, Y is, is minimized.
Uh, so that's a collapse.
Um, what's a, you know, how do you, um, you know, how, how can you correct this?
Um, and to correct this, you have to, uh, put yourself in a context with something called energy
based models, and I'm sure there are a lot of physicists in the room, so you can understand
what that means.
So energy based models are models where you don't, um, explain what they do in terms of
probabilistic modeling, but in terms of an energy function that captures the dependency
between the variables.
So immediately a little more explicit here.
Let's say you have two variables, X and Y, the way, uh, and your datasets are those, uh,
greenish dots that are supposed to be black.
The way, um, the way an energy based model captures the dependency between X and Y is that
it computes an energy function, an implicit function with a scalar output, uh, that takes
X and Y as an input and gives you an energy that needs to be low, uh, near the data points,
uh, on the data points and nearby, and then higher outside of those data points, uh, the,
the region of, uh, high data density.
And if you have such an energy landscape, you have a function that has this, that can
compute this energy landscape, then that function will have captured the dependencies
between X and Y.
Uh, you can infer X from Y, you can infer Y from X, you can have, uh, mapping between
X and Y that are not functions because you can have multiple Ys that are compatible
with a single X, for example.
So it captures multi-modality, uh, without having to be a probabilistic, uh, model.
Of course, in physics, we're familiar with this, right?
Very often, um, we write a, an energy function and then we turn it into a probability distribution
over states using a Gibbs distribution.
Same idea here, but here we don't need the Gibbs distribution at all.
We just manipulate the energy function directly.
How do we train a system like this?
There's really two categories of training methods.
One category is contrastive methods and those consist in generating, uh, those,
those flashing green dots here, uh, that are outside the manifold data.
And then changing the parameters of the energy function so that the energy takes
low values on the data points and higher values on those contrastive, uh, green points.
Um, I, I kind of contributed to inventing those methods back in the early 90s, but I,
I don't like them anymore because in high dimensional spaces, the number of contrastive
points you have to generate for the energy function to take the right shape, uh,
grows exponentially and that's not a good thing.
So I prefer another set of approach regularized methods and there are those, those methods.
I'm going to explain this with another slide.
They basically consist in minimizing the volume of space that can take low energy
through some sort of regularizer, for example.
Uh, so that the system can give low energy to the data points by, you know, changing
the parameters of the energy function so that the energy of the data points gets lower.
But because it's regularized, uh, it can only give low energy to a small, uh, volume of, of space.
Um, the data points get kind of shrink wrapped if you want in the sort of region of low energies,
uh, low energies.
Um, so I, I prefer this, uh, that, that, that seems to be more efficient.
And the question is how, how we do this, but what I'm asking you to do now is abandoned
generative models, the most popular thing at the moment, abandoned probabilistic models,
the pillar of understanding machine learning, abandoned contrasting methods,
which also have been very popular.
And also something I've been saying for a number, you know, 10 years,
abandoned reinforcement learning because it's so damn inefficient.
So, um, those are kind of four of the most popular approaches to machine learning at the
moment.
And I'm telling people to, uh, move away from them.
You can imagine I'm not being very popular here, uh, but I'm used to that.
So how do you, uh, prevent those systems from collapsing?
What you can do is measure, have some sort of measure or information content of SX and SY
across a batch, for example, and then try to maximize that.
Now, unfortunately, it's very hard to do because we don't have lower bounds on
information content.
We only have upper bounds.
Uh, but it turns out you can sort of do this.
So one way to prevent SX from collapsing is that you can use a criterion,
which is, uh, attempt to, uh, make sure that the variance of every component of SX over a batch
is at least one.
So that's the, the criterion you see in the, the, the, the second red box, uh, below the,
the cover, that's a hinge loss that makes the standard deviation of each variable at least one.
And then another term that makes sure the, the components of SX are decorrelated.
That's the covariance matrix term.
So you're trying to minimize the octagonal terms of the covariance matrix, uh, of the vectors SX over a batch.
Um, that's not actually sufficient.
So you can also use an expander and I'll have time to explain why that works.
But the resulting method is called Vicreg variance, invariance covariance regularization.
And it's a pretty general method that, uh, can be applied to a lot of situations for those
joint embedding predictive architectures, uh, for various applications in image recognition,
segmentation, et cetera.
It's pretty similar to another method called MCR squared invented by EMA at its group at Berkeley.
Um, and, um, and it works really well.
I'm not going to bore you with details, but there is a scenario in which you do use sub-supervised
running where you pre-train, uh, convolutional net, let's say, uh, using this method and then you
chop off the expander sticker linear classifier, which you train supervised and you measure the
performance and you get really good performance on image net this way, particularly good performance
for, you know, out of distribution, uh, transfer learning.
Um, there's a modification of this, uh, method called Vicreg L, which was published at Norris, uh,
last year, uh, which is more tuned for, uh, segmentation and things like this.
But, uh, I don't have time to go into details.
There's a new method that we, um, uh, rolled out at CVPR just a few weeks ago called Image JEPA
that uses masking and a transformer architecture for running features in, in, uh, images.
And so the collapse prevention method there is, is different.
But the advantage of this method is that it does not require any data augmentation
other than masking.
So it doesn't require to know really what type of data you're manipulating.
Uh, and it's incredibly fast and it works really well.
It gives, gives amazing results, uh, for like really, really good features.
There's another set of method, uh, by some of our colleagues here at Fair Paris,
called, uh, Dino, the INO, um, uh, it's a different way of preventing collapse,
but it's, um, has some commonalities with IJPA.
Um, and it works really well.
It gives you something like, you know, above 80% on image net purely supervised
with no fine tuning and without any data augmentation, which is pretty amazing.
Um, and, um, but ultimately what we want to do is use the self-supervised learning
and this JEPA architecture to build the systems of the type that I talked to,
I'll tell you about earlier that are hierarchical.
They can predict what's going to happen in the world, perhaps as a result of an action.
Um, with some early results on training systems from video to learn sort of good
representations of images and videos, uh, by being trained on, on successive frames
from a video and, and distorted images around time to go into the details of how this works.
It's called NC JEPA, uh, um, uh, and, and it can, uh, it's trained basically to, uh,
extract good features from images, uh, for object recognition, but also to estimate
motion in, uh, in a video and it does a pretty good job at this.
Um, so watch, uh, this paper on archive that you are, uh, invited to, uh, uh, to look at.
So, um, objective driven AI is this idea that, uh, we're going to have objectives that are
going to drive the behavior of our system and it's going to make it's terrible and safe.
Um, and, you know, there are things that we're working on to get this to work, self-supervised
learning from video that, you know, recipe that really works for everything.
So we're working with those JEPA architectures.
But we don't think we have the ultimate recipe yet.
We can use this to build, uh, LLMs that can reason and plan that are driven by objectives,
perhaps, hopefully, uh, learning systems that can do hierarchical planning,
like, uh, animals and, and, and humans, many animals and humans.
We have many problems to solve, um, uh, JEPAs with regularized written variables to deal with
uncertainty, planning algorithms in the presence of uncertainty, uh, learning cost modules,
which could be assimilated with inverse reinforcement planning,
planning with inaccurate world models, and then, uh, exploration techniques to
adjust the world model in case it's not completely accurate.
Okay, so I'm, I'm, um, sort of concluding, um, uh, there is a computing limitation of
autoregressive LLM, which is that they, they can only allocate a finite and fixed, uh, amount of
computational resources to producing a single token.
You run through, you know, 48 layers or a transformer or something like that.
You produce one token and then 48 more layers and produce one token.
And this is not Turing Complete.
Whereas the method I'm, uh, suggesting, the architecture I'm suggesting that can, uh,
produce an output by planning through energy minimization, that is Turing Complete.
Uh, because everything can be reduced to optimization, basically.
We're still missing essential concepts to reach human AI.
You know, this, uh, potential technique for planning and reasoning.
You know, basic techniques that we're missing to learn world models from, uh, complex, uh,
modalities like video.
Uh, and perhaps in the future, we'll be able to build systems that can plan their answers
to satisfy objectives and, and have guardrails.
Um, I don't believe there is such a concept as artificial general intelligence,
because I think even human intelligence is very specialized.
So let's forget about general intelligence.
Let's try to get to human level intelligence, perhaps, uh, perhaps build machines that have
the same sort of set of, of skills and ability to learn new skills and humans.
But we are very specialized.
In fact, we know this because computers are much better than us at many tasks,
which means we are specialized.
There's no question that sometimes in the future, machines will surpass human
intelligence in all domains where humans are intelligent.
You know, how long is it going to take?
I don't know, but there's no question is going to happen.
We don't need, we don't, we probably don't want to be, uh, threatened by that.
It would be a future where every one of us would be assisted by, uh, a system that is
more intelligent than us.
And we're familiar with that concept with other humans.
I only work with people who are smarter than me, or at least I try.
Or if they're not smarter than me, I try to make them smarter than me, the, the
cost students.
And, uh, you know, so we're familiar with that concept.
Uh, we, we shouldn't feel threatened by machines that are smarter than us.
We are in control of them and we still, we'll still be in control of them.
They won't escape or control any more than or new context and escape the control of our
visual memory, uh, basically in our brains.
Uh, thank you very much.
I'll, uh, stop here and, uh, perhaps if we have time for questions, I'll take questions.
Thank you very much.
If anyone has questions, we have two mics up there.
Feel free to line up.
I guess I'll get us started with one question.
Uh, so on that last slide, you mentioned the possibility or not the possibility,
just the prediction that machines will become more intelligent than humans in all respects.
And you also mentioned throughout your talk, these algorithms that can sort of reason and plan.
Could you imagine in the near future, an algorithm that, for example, could propose
physics experiments for us to conduct, like plan an experiment to answer a question that we ask it?
Yeah, actually there's an entire field, uh, which will precedes AI called, um, uh, experimental design.
Uh, and I mean, I think to some extent that can be formulated as an optimization problem as a planning problem.
Or as a search problem, right, uh, trying to figure out, like, you know, how do you maximally get
information from an experiment?
Like how do you design experiments?
So you get the maximum amount of information from it to either, uh, validate or invalidate
a particular model that you have or hypothesis you have in your mind.
I think that's, uh, entirely automatable.
Um, now if you want to use a generic AI system to do this, my guess is that it's not going to
happen tomorrow.
The system will probably have to be relatively, uh, you know, experienced before they're better
scientists than human scientists.
Yeah, thank you.
Uh, we have a question up there.
Hey, can you hear me?
Yep.
Yep.
Great.
Um, I was wondering if you could expand a little bit on your assertion that you cannot
build a sufficient world model from text alone.
Um, when we think of something like a theoretical physicist, right, this person mostly interacts
with other people verbally and reading papers and thinking and writing.
Or if you think about something like a blind from birth author or person, right,
they're able to actually extract a lot of information about the structure of the world
from the text.
So I'm wondering if you could explain a little bit more about why that's insufficient for,
say, achieving human level AI at least.
Okay.
When we do physics or mathematics very much, very often, I mean, certainly physics, we,
we have mental models of the world.
We have some sort of internal simulator, if you want, that kind of simulates the interesting
aspect of the phenomenon that we're trying to understand.
Uh, that allows us to arrive at, uh, at answers.
Um, and we don't necessarily rely on, on explicit facts that we've, we've learned through,
uh, through language.
Let me, uh, let me take an example.
So all of intuitive physics is, is learned by observation.
It's not learned through language, right?
You know, uh, uh, if, if, if that you are going to put, uh, a smartphone, uh, on a horizontal
surface and let it go, you know, you know, it's going to, it's going to fall one way or the
other.
And you may not predict in which direction, but you know, it's going to fall because of
your notion of intuitive physics.
Um, if I tell you, I take an object, I, um, I throw it in the air vertically.
And, uh, it's going to have a particular velocity when it leaves my hand.
It's going to go up in the air and then fall back.
What velocity will it have when it crosses my hand at the same location where it left my hand?
And if you're any kind of intuitive, uh, notion of physics that go beyond, you know,
normal intuitive physics, you would, you would say, obviously it's going to have the same
speed because, you know, there is conservation of momentum and energy and stuff like that.
And, uh, and it's not because of energy, you know, necessarily the rule of, uh, the explicit
language rule that you have.
It's because, uh, because of your sort of, you know, intuition that corresponds to, to that.
And we do this all the time.
We manipulate mental models.
We do not reason with language very often.
Uh, most of our reasoning does not use language.
And so most human knowledge is not linguistic at all.
It has to do with construction of mental models,
many of which have nothing to do with language.
It's certainly two of all animals.
They don't have language.
So, um, so that, that's what sort of, you know, I mean, it's something that I think physicists,
particularly physicists should really understand, right?
Because we do this all the time.
Good physicists are people who have those mental models that they can use to sort of,
you know, imagine situations and corner cases and stuff like that,
that, uh, that really kind of, you know, give you some insight as to the,
what the nature of reality is.
And of course, you know, then after that we do the math and that gives you sort of the
internal structure of language, the mathematical language, kind of,
you know, makes you discover new properties.
But a lot of it is really, uh, uh, intuition with, uh, with mental model is true in,
in mathematics as well in geometry and, and, and things like that.
So, uh, you know, there's the Gedanke experiments of Einstein, right?
For, uh, uh, those are basically mental models that you manipulate to kind of discover properties.
They're not linguistic.
I, I have a question along similar lines, but, uh, so I agree that there's a lot of intuition
involved in learning for humans.
But is there not a fundamental problem in training such intuition?
Because anything you train digitally would be encoded in some kind of language,
some binary, which is there not an, not a fundamental obstruction there to train such intuition?
Not really, no, uh, the input of those systems can be, uh, as, you know, continuous and, uh,
kind of perceptual as the kind of stuff that we perceive, like, you know, uh, like, like video
or, or whatever, um, uh, or, or sensory, uh, sensory input, whatever it is, audio,
you know, anything you want.
And then inside the system, the representation of, uh, facts and knowledge inside the system
is, is actually just a sequence of, of numbers.
It's not language.
It's, it's, it's numbers, it's vectors, you know, tensors.
So, so I, I don't think that's a problem we need to deal with really.
But, but the texts are also broken down into same numbers, right?
Yeah, that's right.
Uh, that's true.
So text is to some extent simpler because it's discrete.
As I explained, it makes the, the, you know, the, the, uh, management of uncertainty easier
if you have discrete tokens.
And there is a reason why language is discrete, why language, uh, clusters in words.
The reason is because language is a communication, uh, uh, medium, right?
It's a way of communicating and we need to communicate over noisy channels.
And to be able to communicate over noisy channels, the symbols have to be discrete
because that allows you to do an error correction, right?
To do noise, uh, you know, to eliminate noise, right?
I mean, communication engineers have known this for, for decades, you know, since channel
basically.
And so, or even before, um, so all language is discrete, uh, and, and, and goes into words,
you know, the existence of phonemes and words and things like this because, uh, we need to
be able to communicate with noisy channels.
But that doesn't mean that or thinking needs to be the same way.
And in fact, or thinking is not the same way.
Language is a pale, uh, approximate, discretized, dumbed down representation of
eternal knowledge representation recall thoughts.
Yeah.
Um, yeah, but my, my intuition would be that's when you encode something digitally in binary,
you're doing, you're dumbing that down anyway, right?
So even if you are processing images or videos, you are doing it in numbers and
you're doing it in the same way, maybe it's a little more complex.
But that's life.
Okay.
Uh, that's even physics does this biology does this, right?
The communication between synapses between two neurons, there is a finite number of
physicals that are released for the synaptic, uh, communication.
And so there is granularity in this.
The precision is actually just a few bits.
Um, and, uh, and, you know, it's actually much less than the 32 bits that we use for,
or 16 bits that we use for competition in neural nets.
So I don't think the quantization here is, uh, is an issue.
It certainly exists in the, in the brain as well.
Communication between neurons in the brain is binary.
Brains, you know, the neurons actually produce spikes for the same reason
that, uh, language is discrete is because they need to communicate in a long distance.
And for this to be efficient, it has to be digital basically.
So, um, so I don't see this as an limitation that would, uh, discriminate between computers
and human intelligence.
Thanks.
Okay.
Unfortunately, we need to move on because we're running five minutes behind already.
Uh, thank you so much.
This was great.

So our final speaker of the day is an external speaker.
So this is Stephen Wolfram from Wolfram Research.
So Stephen got his PhD in Caltech, PhD in physics from Caltech at the age of 20 working
on high-energy physics quantum field theory and cosmology.
He's the founder and president of Wolfram Research, developers of Mathematica and Wolfram
Alpha, tools that I'm sure you've all used.
And recently I think something interesting that Wolfram did was release a plug-in for
chat GPT, giving access to the Wolfram Alpha computational intelligence engine, which I'm
sure we will hear a little bit about.
But yeah, if you're ready to go, I'll give you the floor.
So I guess I want to talk about two things today.
I want to talk about using LLMs for physics and how physics can help study LLMs.
So to start off talking about how physics can use LLMs, the first thing is, this is
something I just recently, but first thing is what do LLMs fundamentally do?
LLMs have kind of taken four billion pages from the web and a bunch of books, and they've
kind of ground that up to find the statistics of everything that's said there.
And when you ask an LLM something, its mission is to try and produce reasonable text based
on kind of the statistics of what it saw on the web and so on.
And as we'll talk about later, the things it extracts as its statistics are surprisingly
sophisticated and include having sort of found a kind of semantic grammar of language which
allows it to kind of say things that make sense, at least make some kind of sense.
They may be fact, they may be fiction, but they kind of fit together in a way that makes
sense.
But so what the LLM is fundamentally doing is it's taking stuff we humans have put on
the web, and it's feeding that back to us based on things that we ask it about.
It's feeding us back sort of reasonable things that we could have said on the web even though
we might not actually have done so.
So it's a good way to generate, I see LLMs actually as a practical matter as kind of
an important layer of a linguistic user interface.
We have graphical user interfaces, now we have a linguistic user interface where you
can take like five points you wanted to make, you can puff those out with an LLM into a
giant report, then maybe the person you're sending that report to really wants to know
only two things, so they use an LLM again, and they grind it down and get the two things
they actually wanted to know from that report.
And that's a very sensible transport layer using kind of the big report as the transport
layer for what's going on, and maybe that will happen with academic papers as well,
that they will be just the linguistic transport layer for the actual content of what's going
on.
I would like to think that we can do better than that, in a sense math has provided us
a notation that's better than that, computational language, the kind of thing I've worked on
for the last 40 years or so, is an attempt to kind of formalize statements about the
world in a way that's kind of better than just giving it as natural language.
But anyway, that's kind of, you know, what the LLM is doing is it's taking what's there
on the web, it's reconstituting it and feeding it back to us, it's not going to be able to
discover fundamentally new things.
Well with a couple of exceptions there, I think one thing it can do is if there are
analogies that might be found between this place and that place, really good at finding
kind of statistical facts from language.
Usually we're used to doing statistics from numbers, but LLMs manage to do statistics
from text as well.
And so if you say, well what was the trend in fashion in 1955, there's a good chance
that the LLM will be able to take sort of the stuff that ground up from the web and
answer that.
And similarly if you say, well could there be an analogy between, you know, metamathematics
and general relativity, there's a chance that it could figure that out because it can see
that the kind of the structure of what's said about those two areas has a certain similarity.
Something that seems like bizarrely magic to us humans, something that some of us humans
kind of pride ourselves on being able to figure out such analogies, but I think we may
be about to be outdone by the AIs.
But okay, so there's sort of a question of the LLM is doing this kind of taking language
from the web, giving us back some sort of reconstitutive version of that, but there's
more to figure out things, and for example physics as it has emerged since basically,
you know, Newton and the, you know, Newton's big idea from 1687 was, as he called it, you
know, the mathematical principles of natural philosophy.
In other words, there is a more formal method for dealing with natural philosophy, i.e. physics,
than just talking about it as philosophy, so to speak.
So this kind of notion of formalization that started in those days and has emerged into
sort of modern mathematical methods in physics, well, this kind of idea that you can do better
than just pure thought about something, you can have a formalization that allows you to
make further progress, which is most of the story of physics as we know it today.
So if you're a pure LLM that's just dealing with language, that's just dealing with kind
of pure common sense kinds of things, then you are stuck in the pre-Newtonian kind of
paradigm for how you do physics.
Well, so how do we kind of connect kind of this sort of linguistic layer of thinking
about things with kind of the more formal, we might now say, computational layer of
thinking about things.
So you know, I've spent most of my life kind of trying to figure out how you can sort
of describe the world formally using computation.
And so the question is, can you connect kind of the LLM world to this computational world?
And actually, back in March, we did something with the folks at OpenAI of making a plug-in
to chat GBT that allows it to kind of use our computational language from within the
LLM.
And I'll show you, I haven't actually used this interface for a while for reasons that
I'm about to show you.
But let's see, if we say make a picture of an airy function or something, let's see.
It will probably, maybe, I don't know, you never know what it's going to do.
It's some, okay.
So it says, using Wolfram, that's a good sign.
Maybe it's going to do the right thing.
Maybe not.
Who's to know?
Okay.
So this is, I wonder how it did that, okay.
So there it made a nice little plot of an airy function.
Let's see how it did it.
We're going to here, okay.
So what it actually did there was it wrote a piece of Wolfram language code that just
says, plot the airy function, very straightforward.
Let's say we say something like, I don't know, how far is it from, I don't know, Chicago
to Tokyo?
Wait a minute, what happens to that?
It disappeared.
What's going on?
Scrolling down.
Scrolling down.
Okay.
Thanks.
I thought I was scrolling down.
It didn't seem to be showing up.
Okay.
How lovely.
So what it did there, again, was in this case, let's see what it did here.
Okay.
So in this case, it used WolfMalpha and just asked distance from Chicago to Tokyo, then
it got back a bunch of results from WolfMalpha, which it then kind of interpreted and turned
that into what it was saying.
So it's sort of interesting what's happening here, because actually we have this plug-in.
It's used a lot every day by people.
And I think, at least when I last asked, which was a few weeks ago, there's still the case
that about half of the queries go to WolfMalpha and half the queries go to WolfMlanguage.
And if you read the prompt, you know, the prompt engineering is this bizarre activity
where you, you know, whether you say please or not matters, whether things are in capital
letters matters, whether you repeat things at the end of the prompt after you mentioned
them at the beginning.
That all matters.
I will say, by the way, that if you ask, you know, what's the skill you need to do
prompt engineering, so far as I can tell, expository writing is the number one skill
needed for good prompt engineering.
Maybe one day, and we'll talk about this later, when we talk about applying physics
to LLMs, maybe there will be actual kind of AI psychology theory that can be used.
But as of right now, I think it's expository writing, which kind of maps on to the kind
of thing that the LLM has read from the web and so on.
But in any case, the, so, you know, the prompt here is saying, you know, if you have this
kind of thing, try and write WolfMlanguage code, if you have this kind of thing, try
and send it to WolfMalfa, one of the things that's really convenient about WolfMalfa is
that it is a thing that takes natural language input, which is the same stuff that the LLM
is used to dealing with.
So it's kind of, it's using natural language as a transport layer, and what does WolfMalfa
do?
Well, what it's doing is to take whatever you type in, you know, if you type, I don't
know, what is the integral of, I don't know, some random thing.
What it's doing there is it's converting that question written in natural language into
precise computational language internally, or if I say something like, I don't know,
what earthquakes happened in Japan in August 1990 or something, I wonder if it can do that,
I have no idea if it can do that, but that's always living dangerously, okay, we've managed
to do that.
Once again, what happened here was it converted that natural language question into its underlying
computational language, which is our WolfMlanguage system, to be able to resolve it, I mean, just
to show you how that works.
You know, if I were to say here something like, if I just said, you know, New York here,
that this is a WolfMnotebook, this thing, New York, if I say what's the input form of that,
that's the entity, city, New York, New York, United States, but it's also a thing that
I can compute with, so if I say, make a, I don't know, if I say, you know, geodistance
from New York to, I don't know, London or something, it'll then just use those things
as entities that it can compute with.
So as I say, the sort of the mission of WolfMalfa is convert natural language into this precise
computational language from which we can do computations based on algorithms that we've
spent the last three and a half decades, you know, setting up and based on curated knowledge
that we've accumulated over the last couple of decades.
So you know, you can obviously mix things like, you can say things like, I don't know, capital,
capital cities in Europe or something, and you'll get something which, again, that thing
got converted into precise computational language, we can evaluate it, we can say something
like, you know, I don't know, we could say, make a plot of those, all those standard kinds
of things that you can do in WolfMalfa language, or we can find shortest tours, all those sorts
of things.
So from within chat GBT, you can now access all of that functionality, so I don't know,
let's try, let's try doing something ambitious which probably won't work.
Find a shortest tour of the capital cities of Europe.
Okay, let's watch this fail.
Grind, grind, grind.
I have no idea, I hate to even open this to find out what horrifying thing it's actually
doing in there.
No, maybe, maybe, let's see, okay, it's trying something again, either because it didn't
get the answer it wanted or for some other reason, oh, this is a bad sign.
This is not good, this looks like it's, no.
But what it's going to do, what it's doing is, every time, by the way, I mean, the way
other lambs work, we'll maybe talk about this a bit more later, they're always writing one
token at a time, so they never have a plan for where they're going to go, they're always
just looking at what was in the past and figuring out what to say next.
So that means that it's quite often, it's kind of a hack you can use, if you get it
to generate an output and you say, is that correct?
And it will say, no, it's not correct.
And why did you say it?
Well, because it didn't know what it was going to say, it just, well, let's see, let us see,
okay, wow, wow, okay, let's see what happens.
Now I have no idea if this is, okay, wait a minute, how many, how many, wait a second,
okay, let's see, well, let's see, let's see what happens if we say plot that.
And then I'm going to find out what it actually did there.
This won't work, of course.
Well, okay, this is slightly promising, I wonder if this is going to work.
It's a little bit confused there, but we'll see if it can recover itself.
I don't know, let's look at what it, let's look, to get some idea of whether this is
actually right, let's look at what it actually asked here.
Okay, so it asked, okay, it did something sensible here.
So what it was doing, it's a little bit confusing what it did here, and
we'll see how this works a bit better in a moment, but what it did here was it was
actually using results that it had got earlier in this whole sequence,
because it actually knew the order of the cities by now, because it must have got
that in one of these previous queries here.
Yeah, here we go.
So it knew here, find shortest tour of those capitals, it found the shortest tour,
it then used that in the next step to go and try and find something.
Let's see what it was doing down here, let's see what it got me where.
No, it's still grinding away, oh well.
All right, so but this gives some sense perhaps of how you kind of connect
sort of the LLM layer to the computational layer.
But we built something recently that I think you might like to see, which is
what we call, well, let's see, there's different versions of this,
what we call a chat enabled notebook.
So this is using our notebook paradigm and let me see,
let me make this a little bigger, and let me just get ready to save this.
The, okay, and let me tell it to use GPT-4 here, all right.
So let's say we say here something like, again,
I'm going to live very dangerously because this never does the same thing twice.
Let's say solve a harmonic oscillator, an harmonic oscillator, whatever.
Let's see what happens.
Oops, so, okay.
Okay, no, I mean, it'd be not the right thing for it to do, but anyway, let's see.
Ha, okay, not terrible.
Let's say show me the equation.
So we'll talk in a minute about what the heck it was actually doing here.
That's not very useful.
I want to know the differential equation.
If you want to visualize it, okay, let's see what it does.
Let's see, you never know what this thing is going to do.
So it's just kind of, okay, that's not terrible.
I would give that a, maybe a passing grade, I don't know.
Let's see what it actually did.
So here, it, okay, so it synthesized, well, from language code here,
what these boxes look like inside probably by next week will look a bit better
than what it does right now.
But it synthesized some code here.
Actually, if I say here, show me the ODE.
Let me see whether it can do that.
Okay, great.
Okay, very good.
Yes, yes, all good.
Is that right?
No, yes, yes, that's right.
That's okay.
Now I say solve that.
Okay, not bad.
So, you know, this kind of thing I view as being a pretty useful,
come on, I just want to see the equation.
What I want to see is the code here.
Okay, let's say show me the code.
Because this is in a sense, okay, finally we got it.
Okay, and then what we can do here is given this piece of code,
we can just say, for example, we can just say evaluate code.
Oh, look, wait a minute, something is happening in the background here.
It apologizes for the inconvenience.
Who knows what it's doing?
We'll check back for that in a few moments.
But back in this notebook, here I can just say use that thing to copy that code
down there to the next cell and then do the evaluation and get the result.
So it's kind of interesting, you know, if I go back here,
maybe I can try another example.
Let me show you how this works.
I can put in what we call a chat block.
That basically breaks the context of the LLM.
So the LLM, whenever I'm saying, when I say show me the code,
when I ask that, it's able to see the whole conversation that it's had above it.
So now here I broke that by saying show me another thing here.
And I could say, well, here, for example, I can do this pull down.
And this allows me to make all kinds of changes.
So I could, for example, we have this prompt repository that contains...
There we go.
So it contains various...
Well, many...
Okay, we can go to the prompt repository here.
This is a prompt repository that has, in this case,
it will allow us to pick personas for interacting with this.
So I could pick...
I don't know.
Let's try this.
I have no idea what this is going to be like.
Okay, so I installed the 19th century British novel persona.
I've installed that.
Actually, I kind of think we should use Bernardo.
He's fun.
But either one.
Okay, so I can now pull this down.
Okay, we can try 19th century British novel.
We can make a picture of a circle that is half red and half blue, let's say.
Now, I think this will...
Oh, come now.
Oh, great.
Well, that's...
Okay, big mess.
Bad taste.
Okay.
And this code snippet, blah, blah, blah.
Good luck.
Well, let's try...
All right, let's try doing this.
Actually, I should just stop this.
Yacking on like this.
Let's try a different persona.
Let's try the code assistant.
Actually, you know what?
I'm going to try Bernardo.
Bernardo is fun.
Let's try re-evaluating that.
Now, what's it doing there?
I don't know.
This first creates a full red disk and overlays a harsh disk.
Okay, I wonder if this is actually right.
Ta-da!
Okay.
It worked.
Nice.
What's important about this, this maybe isn't the very best example, is you can actually
read that code, unlike the thing that happened to produce before.
And the kind of the idea is, and this is by the way one big feature of kind of the whole
computational language story that I've spent so long on, is that, you know, our language
is intended as something that you can think in, as well as have your computer execute,
so to speak.
Kind of like math notation would be.
It's something that where you can actually, you know, use it as your foundation for thinking
about things.
Okay, anyway, this is...
You get the basic idea, I hope, of sort of this chat notebook notion.
It's pretty nice.
I mean, I have to say, since the reason that I haven't used that, the chat ABT interface
for months, is because this is really a lot nicer.
You get to not only, you know, you can also use all the standard features of notebooks
so you can say, this is a section about circles, and you can start putting in, maybe I could...
Well, actually, let me just do this.
Hold on.
Let's say, do that.
I'm going to live dangerous.
Do that for a sphere.
This is going to fail, of course.
Okay, let's see what happens.
Okay, interesting idea.
Interesting idea.
I wonder whether that will work.
That's definitely an interesting idea.
I give that point.
It's a spherical plot that goes, wow.
If this works, I'll be impressed.
Okay, let's run it.
Wow.
That's cool.
It's getting smarter.
Or how, or the fine tuning that we've done and so on is actually working.
This is encouraging.
Because this is kind of interesting.
I mean, it made a spherical plot over a certain, you know,
latitude, sequence of latitude values, a different sequence of longitude values here.
That's kind of interesting.
You kind of learn something from that.
Maybe we could try, let's try one other thing, which might be fun.
Let's say show a star chart of the current position of Jupiter.
Now, I'm probably going to have to say use astrographics.
Let's see what will happen here.
Oh, come on.
It's just, well, I think it made that up.
I'd be very surprised if these functions actually exist.
No, they do not exist.
Well, that's a bad sign.
Okay, let's try saying use astro position.
And what I'm expecting it's going to do, maybe.
Probably, but that's also not relevant.
Okay, it's not doing what I thought it would do, which is to go read the documentation.
We can probably tell it to do that.
Let's say blah, blah, blah.
There we go.
Now maybe it'll get a little bit smarter.
Okay, this is much better.
This is much, much better sign.
So hopefully, if it read the documentation, it will be able to successfully do what it was.
All right, I don't know whether it's blah, blah, blah, blah, blah.
Now probably if we now say, okay, great, it's talking about all kinds of, I don't know,
it's telling us how to find the position of the large Magellanic cloud, et cetera, et cetera, et cetera.
That's all fun.
And we could ask it to run that.
But I think use this for the picture of Jupiter.
Maybe this will work.
Maybe it won't.
Okay, this is much better.
What?
You see, this is the problem.
It just makes stuff up.
Well, let's see.
I wonder whether this will work.
No, it made up the thing called Planet Marker.
Well, we'd have to tell it not to do that.
It's supposed to go back, and I'm a little bit surprised it did that here,
because actually it has been told to go back and check the code it wrote
to make sure that everything in it actually exists.
So for some reason it didn't in this case.
All right, well anyway, that's a little bit on kind of the sort of the interface
between sort of LLMs, computational language.
I thought another thing I would talk about, quite different subject,
is using physics to think about LLMs.
So let me pull up some things.
So first question is what fundamentally is an LLM doing?
As I said, what it's ultimately doing is it's saying,
given a particular piece of text, let's see if this works.
Okay, so if you have something like this, you feed the prompt,
the best thing about AI is its ability to,
and then its mission is to give you what the next word should be,
or some probabilities that it uses to do that.
So if we kind of, we're interested in knowing, where's my mouse?
Come on.
Up, just a second.
Ah, sigh.
You know, maybe I should just...
Well, okay, let me just...
That's very strange.
Ah, fascinating.
Okay, the lost mouse.
Okay, the lost mouse has been found, maybe.
All right, so just, I mean, let's talk a little bit about what...
Actually, let me show you something else here.
So in our language, well, no, we can do it here,
we can just say, let's use a plain chat and let's set it
so that one of the parameters is we saw those probabilities
that the LLM produced for what the next word should be.
One of the things about LLMs is they have to decide,
given those probabilities, which actual word should be picked?
Like, one thing it could do is say, always pick the most probable word.
Another thing it could do is pick those words according to the probabilities
as it generated them.
There's this thing that's usually called the temperature parameter,
which is an exponential distribution thing
that basically is the thing that picks.
Zero temperature means always pick the most probable word.
Temperature one means pick the words in the probabilities
that the LLM generated itself.
As you increase the temperature, it's picking more and more bizarre words.
So let's say we go here and let's say we increase the temperature
to like 1.3, let's say, and we say something like,
how are you today?
And it will generate some...
So this is now using...
Okay, okay, right, great.
Okay, now let's try, let's change that temperature.
Let's go ahead here and just crank up that temperature
and let's try running this again.
Um...
Oh, my.
Ah!
So when bonkers.
Ay, ay, ay, ay, ay.
Oh, no.
Okay, well, at least it's stopped.
Often it never generates a stop token, it just keeps going forever.
Um, the...
So, okay, so here's an example of a physics question.
Is there a phase transition as a function of temperature in an LLM?
The answer is almost certainly yes.
Uh, probably around for something like GPT-4,
it's almost certainly at a temperature around 1.3 or so.
Maybe there are actually two transitions that occur.
Um, actually there's a...
We just had a summer school with people studying all kinds of things,
and one person at our summer school studied this question,
and I have to admit I haven't read the thing they wrote about it so,
but I can show you, this is basically as a function of temperature,
which is essentially an order parameter changing,
um, and, uh, in the LLM,
and this is, uh, someplace here, this is an actual, you know,
there's some innards of an LLM, um, and somewhere here,
there should be, okay, that's some random pieces of language code.
I think what was done here was to look at, um,
the, uh, the extent to which it maintains kind of, um,
uh, coherence in the structure of the sentences that it produces and so on.
But anyway, the thing that I wanted to point out there is this is a,
it's a very physics-like question.
What, um, uh, how does this work?
And one of the things we don't have right now is a kind of good qualitative physics,
overall physics-like model for an LLM.
Like you might say, oh, maybe it's like a spin glass.
Well, it's not really like a spin glass.
Maybe it's like some other statistical mechanics system.
What is it really like?
Well, uh, there are a few things that we kind of know about LLM,
so I can, I can show you some pictures.
Let me just show you just to get a sense of what's going on inside here.
Um, this is kind of a, like, let's say we're trying to learn this function.
So we've got x and y are input parameters,
and we're trying to learn that function.
We're going to have a neural net.
There's a neural net, and that neural net is taking those values,
x and y, and at the top, it has some weights.
Each of the connections is, has a certain weight.
It, uh, indicated by the color of that, that connection.
And then if we feed in particular values up at the top there,
this neural net will have been trained,
will have been set up with the correct weights,
so that it will always produce a 0, 1, or minus 1 at the bottom.
So, so for example, we can, uh, let's just, um, let's say,
if we try and, um, if we try and use a very, very trivial neural net,
trying to learn that, that function,
the, the totally trivial neural net will not succeed in producing that function.
If we make the neural net more sophisticated,
um, here are some slightly more sophisticated neural nets.
As the neural net gets more sophisticated,
it's going to be able to successfully learn that function.
How big does a neural net have to be to learn what level of function?
Not really known.
I mean, there are theorems that say, in principle,
you can do things with neural nets of certain sizes,
but the practical question, we don't know.
Um, that's another kind of thing.
Now, um, you know, in terms of, of what, um,
let's see, the, um, I mean, you can do these experiments,
by the way, at the things I've written about,
I wrote some kind of whole explainer of chat GPT,
which was one of the things that I've written fastest in my life,
and it's the thing that seems to be read more,
at least per, per unit of time spent on writing it.
It seems to have been read more than anything else I've written,
which to me is a little bit disappointing actually,
but, but that's, that's a different story.
But anyway, so, um, those are some things about the innards
of chat GPT, and, um, those are some, uh,
but we can start looking at kind of what's actually going on
inside the system, and it's kind of complicated.
And you start seeing, you know, this is a condensation
of the kind of innards of the brain of, of actually,
this is GPT2, kind of a junior version of, of the, of chat GPT.
Um, and this is kind of, in a sense,
this is taking human knowledge and human linguistics
and crushing it down to something that's represented
in terms of arrays of numbers,
and this is one of the pieces of what you see when that's done.
I mean, the full chat GPT has like 175 billion weights.
This is just showing a little piece of that story.
Um, now, okay, what can we say about, about what it actually does?
Well, the, um, there's several different things.
So, so one thing that's important is this concept of embeddings.
We can take, uh, kind of, uh, you know, words in a language,
sentences, things like that.
The big sort of idea of neural nets in some sense,
and it's a very old idea, dates all the way back
to when neural nets were invented in the 1940s,
is don't just use digital information,
use arrays of real numbers to represent things.
It's not clear that you actually need to do that.
You probably don't.
I don't think you need to do it for physics, for example.
Um, but the way that neural nets are built,
they are take everything, whether it's an image,
whether it's text, whatever else,
and grind it into arrays of real numbers.
And then, uh, you can take those, um, uh, then, then,
what you're doing is representing everything,
you know, just as in standard digital computational stuff,
you're representing things as bits in neural nets,
you're representing everything in terms of,
of arrays of real numbers.
And so, for example, any old sentence,
any old piece of text is ultimately represented
as an array of real numbers.
And that array of real numbers we can think of
as being some sort of, uh, feature vector
that represents, in some sense,
some digest of the meaning of the thing that we,
that we specified.
So you can start asking, in meaning space,
in that space of embeddings,
uh, what, what can we see about what happens in that space?
And, and for example, let's see,
we can, we can, uh, ask questions like,
how linear is that space?
Uh, you know, for example,
if we do parallel transport in that space,
we look at the curvature of that space.
We're looking at, you know, this is to that,
as that is to that.
That's kind of the, the analog for linguistics,
for, for sort of the structure of, of, of,
meaning of a question that you might ask
in, in, in physics of space time or something.
And you can, you can ask about these questions
about curvature in that space.
I don't know all the answers to this.
Um, you can also ask things like,
well, what is the trajectory that's carved out
in that space?
So is there a, for example,
a semantic law of motion?
If you start in this particular way,
is it the case that in this meaning space
that you end up always tracing through
in a particular way?
And one thing that seems to be the case,
uh, this based on some experiments we just did
a couple of weeks ago,
is that the, the, the things are much more organized.
So if you, if you look at, um,
this is, this is kind of, sorry,
let me, let me just show you, uh,
much of the time these, these, um,
these trajectories aren't
in, in something like GPT2,
the trajectories are quite disorganized.
It seems that as you get to things like GPT4,
the trajectories look a lot more organized.
It's, it's much more believable
that there are semantic laws of motion, so to speak,
laws of motion and meaning space in GPT4.
By the way, it's worth realizing that there's sort of a,
a, a quantum story to the whole thing,
because the whole thing is, is,
you know, it isn't just picking one trajectory.
It's picking a whole bunch of different paths.
One difference from, I mean,
this is a quite different topic,
but in the whole fundamental physics project
that we've been doing for the last few years,
where it seems like we really actually do finally understand
how quantum mechanics works,
it becomes very important in that case
that there is merging of different paths of history,
as well as just branching the paths of history.
In, in the current versions of, uh, these, um,
LLMs, there's pretty much just branching of paths of history,
but you kind of get this quantum-like phenomenon going on
of all these different possible things the LLM might say
that aggregate up to different kinds of things.
Um, by the way, if you're, well,
there's all kinds of interesting things to say about LLMs
as observers in, in thinking about physics.
But maybe, um, one thing to talk about is, is just,
uh, what is, you know, this is sort of pictures
of, of what meaning space looks like and so on in, in, um,
uh, and, uh, questions like if you have a word
and it has many different sort of, uh, partially,
so this is the word crane, I think.
And this is, uh, in meaning space,
this is where different sentences
that mention cranes show up.
And so I think the ones at the top are cranes as a bird
and the ones at the bottom are cranes
as, as construction equipment type thing.
And you kind of see that separating.
You kind of get, again, it's this kind of rather physics-like
thing of kind of looking at this meaning space.
And by the way, you can, you can sort of ask things
about the structure of that meaning space.
And for instance, let me see if I can show you a picture.
Uh, let me see here.
Um, maybe.
Okay.
So in meaning space, you can ask something like,
uh, you can also do that with images.
And so you can ask, for example, we can go in meaning space,
we can go from a dog image to a cat image
on the line in meaning space between a dog and a cat.
And, um, we could actually keep going from the cat out further.
We can extend that line further out of meaning space
and we get all these kinds of weird things happening
or we can go from a plane to a cat
and we have something very strange in the middle of those two things.
Or we can just go out.
This is, uh, what I was calling cat island.
This is in the middle.
So this was a generative AI, um, not specifically an LLM.
This is a, uh, an image generation AI, uh,
which uses somewhat similar,
but not precisely the same technology inside.
Um, and, uh, I asked it to make in the middle
to make a picture of a cat and a party hat.
And then as you go outwards in meaning space,
you see this kind of island of where you can see
sort of identifiable cat things going on.
And then further away, it becomes more and more bizarre.
And by the way, you can ask questions like,
well, what's actually out there in sort of arbitrary meaning space?
And I think, um, well, you know, you can look at other cat islands here.
These are other, this thing is actually in 2,000 dimensional space.
And these are planes in 2,000 dimensional space,
different planes in 2,000 dimensional space.
And you see different, different cats live on different planes.
Um, but, uh, you know, sometimes you can just,
if you plop into, into this meaning space in some random place,
you'll see things which kind of look,
well, I don't know what those are.
Um, but, you know, sometimes you'll see things
which kind of have a reminiscent of kind of human forms and so on.
Why does all this happen?
Same kind of reason as with LLMs,
because this was trained from a 5 billion images
which were actual images people put on the web.
And those actual images are of human relevant kinds of things.
With, with images, more so than with text.
Uh, we're able to, as humans,
we're able to look at things that weren't quite right.
Like we looked at that high temperature version
of what the LLM produced and it looked like garbage to us.
It was incomprehensible.
For images, we do a little better at being able
to not be just completely confused by what's going on.
But if we kind of dive in and look, you know,
what's out there in arbitrarily,
uh, let's see, where do I have a picture of that?
Well, those are some pictures just randomly out there
in, um, uh, in kind of meaning space.
And if you go in, you can see, you know,
there are weird things like this.
You know, people like pictures or you can have, you know,
pictures like these, which are kind of reminiscent
of sort of landscape-like pictures,
but aren't really landscape-like pictures.
But this whole question about, you know,
where in meaning space, where in this, in this, um,
I mean, there's this, if you try and imagine,
where is the stuff that's meaningful?
10 to the minus 600 of all of meaning space
is what we have so far explored
as with sort of human language and so on.
It's a very small, small fraction of it
with respect to images.
Okay, so just to maybe finish off a bit,
um, you could talk more about this kind of thing,
but, but, um, just to talk a little bit about
sort of the physics of, of LLMs and so on,
um, I think one of the things people,
uh, what one wants to do is,
is there a kind of a narrative story
of what LLMs are finding?
Um, is there, is there a way of saying
why do LLMs even work?
It's not obvious that, you know,
given that you, you know,
you could say, take a sentence like,
the cat sat on the, okay,
based on just looking at pages on the web,
you can reasonably guess the next word is going to be matte.
But by the time you've got a long prompt
where you're asking it some physics question or something,
there's, there's, there's no way that actual
detailed text is going to be somewhere on the web,
or probably not, unless it was some exercise
or a book or something, but, um,
most of the time it won't be something
that was on the web.
So you have to have an actual model
that allows you to extrapolate.
The model that's being used in chatGBT is a neural net.
It is far from obvious.
There's no fundamental reason to think it would be true
that the way the neural net extrapolates
will agree with the way we humans
think it makes sense to extrapolate.
The fact that it extrapolates
to produce things that seem meaningful to us humans
is a nontrivial scientific result.
And, you know, I think what it's basically telling us
is the way brains work is actually pretty well modeled
by, by sort of neural net type things.
And that's why the things that brains extrapolate with
are pretty close to the things
that these simple neural nets extrapolate with.
So then the question is, well, okay,
we've, we've got this kind of extrapolation that's going on.
We've got some, this thing is,
is finding out some way to extrapolate.
How is it doing that?
Well, what regularities in language is it picking up
to allow it to make meaningful sentences, meaningful text?
Well, there's one big regularity that we know about in language
which is syntactic grammar.
We know how you put parts of speech together,
nouns and verbs and things like this.
So in a sense, we can then construct sentences
which are syntactically correct.
But there are infinite number of sentences
that are syntactically correct but complete nonsense.
And that's, it's doing much better
than just producing syntactically correct sentences.
So what's it doing?
Well, there's one good example
of a place where we know a structure in sentences
that exists that isn't sort of purely syntactic
and that's logic.
And you can kind of think, you know,
when Aristotle invented logic back a couple of thousand years ago,
you know, what was he actually doing?
Well, he was a bit like a machine learning system
because what he was effectively doing
was saying, I've got all these examples of rhetoric.
People make an argument that looks like this,
but I can take something which instead of it being
a discussion about, you know, Sparta and Athens,
it can be a discussion about turtles and fishes.
It doesn't matter.
I can just replace those symbolically with P and Q
and I can look at this sort of formal structure
of these sentences.
In a sense, you can lift logic out of the specifics
of actual language, in his case, Greek.
But, and in a sense, what LLMs have done
is they've discovered the same thing.
So people say, oh my gosh, it's amazing,
you know, LLMs have discovered logic.
Well, they discovered logic, I think the same way
Aristotle discovered logic and you can find out
they're basically doing solgistic logic.
And if you try and feed them things which require
sort of more formal, more formal kinds of things,
even at the level of, you know, parenthesis matching
and so on, they will fail after you get
sort of too many parentheses to match.
They don't do kind of the formal level of things.
They don't do the computational thing.
They do the kind of level of things that, in a sense,
was the original way that logic was discovered.
So that's a place where kind of one's able to lift
something more semantic out of this kind of layer
of pure language.
But presumably there is more that can be done
along those lines.
Presumably there is kind of a semantic grammar of language
which in a sense the LLMs have discovered something
about language and common sense reasoning and so on.
That is that there's this sort of thing you can lift
out of language that allows you to kind of put together
meaningful stuff beyond just the purely syntactic.
And I think that's the thing where, well,
I've been interested in this actually for a long time
for different reasons, this kind of idea of sort of
making a symbolic discourse language that allows you
to sort of express things in a kind of,
in a way that is sort of a symbolic way
of expressing things that is not specific
to the particulars of language.
In a sense the whole enterprise of making a
computational language has got a certain distance
with that describing certain kinds of things in the world.
But anyway, I think that there are many pieces
of kind of what happens in LLMs.
For example, why does few-shot learning work?
Why does it work to tell an LLM to talk like a pirate?
Why does it, how does that manage to place it somewhere
in meaning space or something so that the kind of
you placed it somewhere by giving that prompt,
then somehow the semantic law of motion takes over
and it successfully manages to produce
semantically meaningful stuff.
We don't know how any of this works.
It's a great topic for physicists, I have to say.
I think it's one of these places where it isn't particularly easy.
It's something where this space of,
this sort of meaning space we're looking at with these images,
we can sort of see things about what's out there
in meaning space in a way it's a little bit easier
than with text and words, but we're kind of,
this is sort of just the beginning of, in a sense,
physicalizing using something like statistical mechanics
to try and analyze what's happening inside an LLM.
So I think to sort of summarize,
I've talked about two kinds of things.
One is just the very practical aspects of using LLMs
to, I think the most significant workflow there is this.
You have a vague idea of what you want to do.
Now, I have to say to get that vague idea,
you have to have an ability to sort of think computationally
about things until you can express yourself
in some kind of sort of, with computational concepts.
I mean, it's no good with some notion
of how you think about the world computationally.
Once you have that, you can kind of write a piece
of natural language, you go sort of tell that to the LLM.
The LLM will then write, you know,
will write Wolfram language code or whatever,
sometimes correctly, that will be an expression
of what it thought you meant by the things
that you said in natural language.
Now, sometimes when you look at that Wolfram language code,
you'll say you misunderstood, it wasn't correct.
You can fix that code or you can tell it to go fix the code
or whatever else.
But so the workflow is, you know,
computationally imagine what you want to do,
write it in natural language, have it kind of translated
into computational language,
then read the computational language.
It's very important, it's something you can do
with Wolfram language, no other, you know,
that's the story of computational language,
very different from programming languages
which weren't intended for humans to read particularly.
But so that's something where you read
that computational language, you understand what it said,
you fix it if you need to, then you say run that,
then that becomes a sort of brick that you can use
to build a whole tower of what you want on top of.
And so I think that's the workflow and, you know,
I have to say, as we make these chat notebooks better,
it's getting closer to the point where it actually makes sense,
even if you know Wolfram language well,
to try and use it as a way to get things started
if you're not thinking very clearly, so to speak.
Although as I say, to get the prompt right,
you have to be kind of think expository writing,
because if you're totally confused,
the LLM will be confused as well.
But anyway, so the first thing I was talking about
was this idea of how do we make use of LLMs
mostly as a way to kind of get a leg up
on creating kind of computational language
to be able to actually do computations.
I should say, by the way,
I'm happy to talk about this.
People are interested if we have any time.
But there's many use cases, like, for example,
we're working on a bunch of AI tutoring applications.
Another use case I mentioned for physics,
we've never been able to do, and Wolfram Alpha, for example,
we've never been able to do physics word problems.
We can do that once you've turned the word problem
into equations, for example, we can nail it.
But turning, going from the whole long textual description
into the equations is not something we've been able to do.
Now we can.
Now, in fact, in practice, when you use LLMs,
one of the things that's terrible,
you use, for example, a chat notebook
or the Wolfram plug-in for chat GPT,
it'll sometimes correctly untangle the word problem,
solve the equations correctly,
and then at the last minute, give the wrong answer
because it tried to inject something that it thought it knew
and it got very confused.
But anyway, so lots of use cases for kind of the LLMs,
their interaction with computational language,
and then the second piece,
really quite a disjoint piece,
is why did the LLMs work in the first place?
This is a physics problem, and people should figure it out.
And the results of figuring it out
will be many important things.
For example, probably most of what's inside a modern LLM
doesn't need to be there.
Most of what's, you know, the actual structure you need
to know enough to be able to do sort of a linguistic interface,
plus kind of enough common sense
to support that linguistic interface.
It's probably quite tiny compared to a current LLM.
And probably you can delegate all the kind of computation
and detailed computational knowledge outside of the LLM,
which is an important thing in practice
in terms of how much it costs to run an LLM,
what kind of systems you need to run it on.
But if we understand LLMs better
and why they work in the first place,
we have a better chance to be able to resolve those kinds of things.
All right, I should stop there. Thanks.
APPLAUSE
Very much.
I think we have time for a couple of questions.
I'd like to start with a quick one.
It's like the out of left field.
I think you've made a good case here
for physicists becoming linguists.
Is there something that physicists should be learning from linguists
or linguists should be transitioning to physics?
Physics can give us...
I don't know whether it's linguistics.
I don't know whether you call it that.
I don't know what you call it.
But this whole idea about meaning and so on,
what we're talking about,
that's something that I think has now been exposed
as something on which we can do experimental science
and on which we can apply physics.
So I think that's the...
I mean, in terms of...
Yeah, no, that's...
I mean, it's a...
If you look at the history of physics,
physics has been a fantastic export field.
That's populated molecular biology,
it's populated quantitative finance,
it's populated lots of kinds of things.
It has every opportunity to populate this area
and to populate and to really make some complete change
to how one thinks about meaning and language and so on, I believe.
Well, thank you.
All right, let's go to the right first.
Hi, so I've seen some of your talks
on the Wolfram physics project as well,
and I see these n-dimensional graphs that you often use.
And they often really look like neural networks.
And so I wanted to ask if that was intentional
or if there's some additional layers of physics going on there.
They have nothing to do with neural networks.
So far as I know, although there's at least one startup
that believes they do, and we'll see how that works out.
The...
This is an utterly disjoint discussion
about how kind of microscopic hypergraphs
from them emerge, space-time and quantum mechanics and so on.
There is, in fact, a bizarre connection.
Okay, this is to the deepest level of the rabbit hole immediately.
There's this thing we call the rouliad,
which is the entangled limit of all possible computations.
Imagine you take all possible, let's say,
Turing machines with all possible initial states.
You run them and they're all non-deterministic.
They all have all possible rules.
You run them, you get this big, messy thing.
There's only one of it.
It is the complete representation of all possible computations.
And then that, I claim,
is sort of the ultimate foundation of physics and mathematics, actually.
And our physical universe ends up being...
We have to exist within that.
And so our physical universe ends up being our sampling of that rouliad object.
And here's the fascinating fact, at least I think it's interesting,
is that if you assume that we, as observers of the rouliad,
have two characteristics.
One, we are computationally bounded.
Two, we believe we are persistent in time.
We believe we have a single thread of experience.
Those two attributes alone are sufficient to give you,
not just qualitatively,
but exactly general relativity and quantum mechanics.
That's kind of exciting,
because it tells you that it didn't need to be that way.
The aliens who don't have those characteristics
don't have to have general relativity and quantum mechanics.
But it kind of gives you...
I mean, this is a huge condensation of a very large amount of stuff,
but that's...
So, okay, how does that relate to all of this?
When I was showing you those weird cat pictures and things,
this is a...
One of the reasons I was studying weird cat pictures
is because this is a way of understanding
sort of different slices of this roulial space concept.
There's much more to say about this.
That's a way too brief a description.
Okay, let's take a question from the left now.
Yeah, hi.
So, I've tried to use LLMs in research so far
without great success.
I'm a theoretical physicist.
Something that would be...
There's a weird echo here, I don't know.
Sorry, something that would be really useful
would be if I could have something where,
you know, 300-page paper, I don't know,
by Edward and Pierce,
and I can ask it,
can you give me a one-page summary of that,
you know, where it would be correct,
where it would already kind of know from talking me to before?
Like, what are the kind of things that I know
and that I don't know?
So, I mean, how far are we from that?
Well, you know, for example, in our company,
you know, someone makes a daily digest
of interesting papers about LLMs, okay?
And I got fed up trying to read the abstracts,
because every abstract is written differently.
They're very ponderous in many cases.
I said, just get the frigging LLMs
and make a two-sentence summary of every paper.
It works great.
I mean, you can scan down this thing really quickly.
The fact that the LLMs' text is rather boring
is actually good, because all the text is consistent
and you kind of can just see what's happening.
Now, you know, do I get the essence of every paper correctly?
Maybe not, but that's a statistical thing anyway.
I might miss it from the abstract, too.
So, that's a pretty good use case that I recommend, actually.
In terms of the, can you,
if you want it to summarize for you, particularly,
I think that's coming in the, you know,
kind of AI tutoring system that we're building,
that's kind of one of the big ideas,
is know the student and be able to figure out,
first of all, how is the student confused?
Because one of the things that you typically can't do
in sort of watching what a student does
is watching the working that the student follows.
But you can with an LLM,
and you can kind of see, you know,
how is the student confused as the student is doing their work?
And then, so then the question is,
will it come to the point where, you know,
the LLM will know enough about me from having read,
I mean, me personally, I put 50 million words out there.
So, it's, I'm pretty easy to learn about.
And we're trying to get an LLM to be me, so to speak,
which will save time, maybe, maybe not.
But anyway, the point is,
I'm, you know, given that the LLM knows about you,
I think there is a very good chance
that the LLM will be able to say,
the one thing you need to know,
because you're confused about this
or you don't know this, is this one fact.
And you say, oh, that's the thing I want to know
from that paper, all the other stuff is irrelevant.
I think that's pretty realistic,
and I think it's reasonably short term.
But you've got to understand,
like everything with machine learning,
it's kind of an 80% success, 90% success story.
And whenever you have a situation,
like looking at these abstracts where, you know,
if I notice an abstract that looks really interesting,
it's a win, if I miss one, it's not a disaster,
that's a good use case.
If it's a case where you're trying to do the next great,
you know, precise physics calculation,
it's probably a big lose to use an LLM
where it might be wrong 10% of the time
and you don't know which 10%.
Okay, I understand that there are many, many questions,
but at some point everyone does have to go home, unfortunately.
So I'm afraid we're going to have to cut off the questioning there.
Let's thank our speaker again.

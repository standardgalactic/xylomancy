This video is sponsored by Brilliant. More on that later.
Believe it or not, there's drama in statistics. Not like real drama, but like nerd drama.
This drama is between two approaches for doing statistics.
One approach is frequentist statistics. That's going to be really annoying to say.
The other is Bayesian statistics. Frequentist statistics gets its name from the word frequency,
based on the fact that probability is interpreted to be the same as a frequency.
On the other hand, Bayesian statistics gets its name from the use of Bayes theorem.
From the Bayesian perspective, probability represents a degree of belief about an event,
and this degree can change based on a person's prior knowledge.
If you've learned statistics before or have watched my past videos,
then most likely you've learned frequentist statistics.
Whether we like it or not, it's the dominant form of statistics taught in schools,
and anybody who wants to work with data has to know how to do it.
On the other hand, Bayesian methods are often neglected,
and many students will go on barely interacting with such an important part of the field.
Using myself as an example, there's only one optional class for learning Bayesian
statistics at my own university. On the other hand, you're expected to take two years with
the frequentist statistics in order to get your degree.
Even though it's not as widespread, Bayesian statistics are still used to improve people's
lives. Many clinical trials have used Bayesian methods to get new treatments into the hands of
people that really need them. Some of you might remember some pandemic happening a few years ago.
Companies like Pfizer and Moderna were rushing to make a vaccine so that we could finally step
away from tracking turnip prices and animal crossing. Moderna used a frequentist approach
to show that their vaccine worked. But Pfizer used a Bayesian approach.
Bayesian statistics is really powerful, but at the same time, they can be really hard,
especially for someone who's just starting to learn it. Given that there's not many classes
for it in university, it's often left to the student to pick it up for themselves.
And take it from someone who studies full-time, self-studying isn't always easy.
So in this video, I'm going to give you a crash course on Bayesian statistics.
If you're new here, I'm Christian and this is Very Normal, a channel for making you
better at statistics. I tried thinking of a statistics pun to put here, but none of them
were sufficient. Let's get started. Level one, Bayes theorem. To understand Bayesian statistics,
you need to understand the theorem that gave it its name. Bayes theorem originally comes from
probability, not statistics. It's named after Reverend Thomas Bayes, who published about this
theorem in 1763. Bayes theorem goes like this. Given two events, A and B, the probability that
both of these events will happen can be written in terms of an intersection. This intersectional
probability can also be written in terms of a product, a product of a so-called conditional
probability and a marginal probability. In this case, I've written as the conditional
probability of A given that B happened, multiplied by the probability that B happened. But this
product can also be expressed the same way with the roles of the events reversed. Since these
expressions are equivalent, we can form a relationship that links these two conditional
probabilities. Just by moving this marginal probability over, we get Bayes theorem. That was
easy to describe. How should we interpret this relationship? The key part here is here and here.
We call this part of Bayes theorem the prior probability. You can think of this event as
your prior knowledge that an event will happen. It could be something as simple as you
subscribing to this channel, which you should by the way, and this is an updated probability
that A will happen given that we saw B happen. So we're going from our prior belief that the
event will happen to this posterior belief. Given that some other event B happened, Bayes theorem
tells us how to update our degree of belief that this A event will happen. If the B event was watching
this video, then this posterior probability would be the probability you subscribe given that you
watch this video. If I've done my job right, hopefully this probability is higher than before
you watched it. But what does that mean? Well, I can't control your prior probability of watching
this video. I can't control these other events involving you watching this video. This term is
a conditional probability that you watch this video given that you were a subscriber. According
to Bayes theorem, if I want to improve my chances of getting a new person to subscribe, I need to
increase the probability that my current subscribers watch my videos. This term in the denominator is
tricky. It represents the marginal probability that someone will watch this video. There are
several different ways that new watchers will come across my video, and it's hard to account for
everyone. I would need to account for all these different ways that someone might see my thumbnail
and watch. It's easy to stick this probability in the denominator and call it a day, but it's a
completely different beast to actually calculate. In spoiler alert, that's going to be a recurring
issue throughout this video. For many people, they hear about Bayes theorem in their probability
class, get a homework question on it, and forget all about it. Maybe some of you in analysis positions
had to answer a Bayes theorem question for an interview. This simple expression inspired an
entirely new way of thinking and statistics, but in its current form, Bayes theorem doesn't tell us
anything about statistics. To see where that comes in, we need to go a level deeper. Level two,
Bayesian statistics. Bayes theorem tells us how to update our beliefs about events happening,
but statistics is about making useful inferences and predictions from data through models of the
world. So how do we make the jump from this to this? In statistics, we deal with data,
and the inherent problem with data is that it's random. Randomness makes it harder to learn from
the data. We statisticians got to get that bag, so we assume that this randomness has a predictable
form or structure. We describe this structure using a probability distribution function,
or PDF. PDFs are functions which describe which values we're most likely to see,
which ones are rare, and which are impossible. We want to learn more about this PDF because it
can tell us about important qualities about the possible values. For example, we can calculate
an average or typical value, or we could learn about the range and spread of what values are
possible. What gets in the way is that PDFs are hard to estimate. Functions are generally
infinite dimensional, and no one is paid enough to estimate infinitely many things.
One strategy for getting the PDF is to approximate it using a parametric family. With the parametric
family, we can get an entire function just by estimating a few values called parameters,
and I'll use data to denote a general parameter. These parameters often represent values we want
to know or study about a population. The binomial family has a single parameter, which we denote
as P. This P represents the probability that an event will happen. For example, if a certain vaccine
protects someone from a certain virus. Once we collect the data, we try to use it to estimate
these parameters and learn more about the world from this estimate. This is called statistical
inference. Depending on who you ask, they'll say that this should be approached from different ways.
The frequentists will tell you that population parameters are fixed and unknown, and you can
estimate them with a method like maximum likelihood estimation. With infinitely many data,
this estimate will be theoretically close to the true population value. Then, they'll turn to a
hypothesis test. With null hypothesis testing, you're just saying that the data is unlikely to
have come from a particular hypothesis or parameter value. It's literally the P value.
But for a lot of people, that's not what they want. Instead, they'll want to know a likely value
or a range of likely values for the parameter, not what it probably isn't.
What they want to know is the probability of the parameter value
after collecting some data. Frequentists wish they knew this, and some tricked themselves into
thinking that's what they actually have. And this is where the Bayesians come in.
Let's have another look at Bayes' theorem. Instead of looking at two simple events,
we'll change our notation to deal with two random variables. The data that we'll collect,
which I'll denote as d, and the parameter value theta. We're working from the Bayesian
perspective now, so the parameter is considered to be a random variable. This is very different
from the Frequentist's view, where the parameter is thought to be fixed. After replacing these
simple events with these two random variables, we get the version of Bayes' theorem that's used
in Bayesian statistics. It's the same theorem, but the level of difficulty has increased.
Let's walk through it. This term is still the prior, but it's no longer just a single probability
but an entire probability distribution, which we call the prior distribution. The prior
distribution represents our beliefs about which parameter values are likely and unlikely,
not the data we collect, but the parameter. As an example, let's use the response rate of a
binomial distribution. Depending on the shape of the prior distribution, I can reflect different
beliefs about this response rate. If I have no knowledge about what this value could be,
then any value is likely as any other, and you'd represent this using a completely flat
distribution. This is called an uninformative prior. On the other extreme, I might be convinced that
the success probability is 40% and that it can't possibly be anything else. In terms of a probability
distribution, this belief may look like a sharp spike at 40% and almost zero everywhere else.
This is an extreme version of what's called an informative prior. In more practical applications,
you could talk to experts or refer to previously published papers to form a good informative
prior. This is called eliciting a prior. Between these two extremes of uninformative and informative,
there's a spectrum. The subjectivity can make some people uncomfortable, but the prior is what
makes Bayesian statistics so powerful. It forces you to consider the probabilities of different
parameter values before you collect any data and make these probabilities explicit to others.
Nothing will stop you from conducting an analysis with an extremely opinionated prior,
but then someone can challenge your findings by challenging your prior, which opens up the
possibility for discussion. On the other hand, frequentist analyses totally ignore the prior
probability of any hypothesis. What frequentists focus on exclusively is this term. This conditional
probability is what's known as the likelihood. The likelihood is the joint probability distribution
of the data, assuming a particular value for the parameter. Remember that we often model the data
as coming from its own parametric family. From the Bayesian perspective, once you condition on a
particular parameter, you can check how likely the data would have come from this particular value.
The importance of the likelihood is that it allows the data we observe to influence the shape
of the posterior distribution. This term in the denominator represents the marginal probability
of observing the data. To get this probability, you have to calculate a difficult integral,
integrating over all the possible parameter values. Despite how it looks, after the data is
actually observed, this term is just a number. A difficult to calculate number, but still a number.
Once you perform all this theoretical computation, you finally get the posterior distribution,
which represents your updated belief about the likely and unlikely values of the parameter
after observing the data. The posterior distribution is the primary object that
Bayesian statisticians interact with. Like the prior, the posterior distribution is also a PDF.
You can figure out the mean of this posterior distribution, and you can calculate a range
of values that contain a particular amount of probability, like 95%. In Bayesian parlance,
this is the credible interval, and it's what confidence intervals wish they could be.
Looking back at the Pfizer paper, you can see that they report a 95% credible interval. Taking a step
back, we can see that this new version of Bayes' theorem is more complex. Bayes' theorem tells us
that you can get a new probability distribution by multiplying these two functions together
and dividing by this complicated integral. This calculation is so complicated that we usually
try to simplify it. The main way we do this is to choose a special prior. The likelihood is usually
decided by the data, but we can choose our prior. If chosen carefully, it's possible to get an easy
expression for the posterior distribution. As an example, let's consider the case where the data
comes from a binomial distribution. If we make our prior on the response rate a beta distribution,
then it can be shown that the posterior distribution will also be a beta distribution
with updated parameters. You can see that both the prior parameters and the data influence the
parameters of the posterior distribution. In simpler terms, the more successes we see in the
data, the more the posterior beta shifts towards one. The more failures we see, the more it shifts
towards zero. The end result is a posterior distribution that is informed by data. If the
prior produces a posterior that comes from the same family, we refer to the prior as a conjugate
prior. Here, both the prior and posterior are betas, which can be said for a lot of people in my life.
Since the likelihood was binomial, we refer to this as a beta binomial model. In fact,
this was the model used by Pfizer to demonstrate that their COVID vaccine work.
Conjugate priors make things easy, but they also constrain how we can represent our beliefs
through the prior. If we want to go past conjugacy, we'll have to dive yet another level deeper into
Bayesian statistics and face some scary math. What happens if you try to use a non-conjugate
prior? Well, the recurring villain in Bayesian statistics is this integral you need to calculate
to get the posterior distribution. See that? That's the prior right there. For a general prior,
it's not guaranteed that this integral can be written in what's called an analytical form,
which is just a fancy way of saying we can't write an equation for it. If we don't know the
form of this integral, then we also don't know about the form of this posterior. Like I mentioned
earlier, if you know what it looks like, you can calculate important aspects about a random variable
purely from the PDF itself. What if I told you we could get all these values based on the probability
distribution, even if we didn't know what it looked like? A probability distribution just
describes the randomness in a random variable. So what if we used samples of this random variable
instead? If we could generate samples from the posterior distribution, then the samples themselves
can be used to estimate all these values. It's indirect, but if you have a lot of data, then you
can get some pretty accurate estimates. It's not just the frequencies who can benefit from
asymptotics. This is the approach taken by Markov chain Monte Carlo algorithms, or in short,
MCMC algorithms. MCMC algorithms construct what's called a Markov chain. A Markov chain is a
sequence of numbers where the value of one number in the chain is dependent only on the value that
came before it. The elements of this chain form a sample, much like a data set. What's special
about the MCMC algorithms is that they construct this Markov chain such that, almost by magic,
the probability distributions of the samples are the posterior distribution. An MCMC will work
even if we don't know the form of the posterior itself. Another approach that I'm just starting
the pickup is variational inference. The idea behind variational inference is that we want to
approximate the posterior distribution with another distribution that's close to it but
easier to use. Instead of directly dealing with a difficult integral, once you have a good look
alike, you can estimate all your posterior quantities using this approximation instead.
That's my best take on it. I still have a lot to learn about this technique.
MCMC and variational inference are examples of advanced techniques that you'll need to
pick up if you want to be a Bayesian statistician. I only gave very brief introductions to both of
these topics, but I wanted to give you a taste of how far we've gotten from this simple little
theorem that the Reverend gave us in 1963. Conclusion. I hope that this video has taught
you how to be a little bit more Bayesian in a frequentist world. In the past, there was drama
between the frequentists and the Bayesians, but it's all majorly in the past.
For all its weaknesses, frequentist statistics have a long track record of achievements,
and the same can be said for Bayesian statistics. You may even be surprised to hear that there are
hybrid methods using both frequentist and Bayesian ideas. As a budding statistician,
I'm always learning new things and I hope that I've helped you update your own priors.
If you like my content, you can stay informed by subscribing to my channel and the channel
newsletter. In the newsletter, I talk about topics I don't usually get a chance to cover on the main
channel, like my ongoing research or my video making process. Everyone starts off with
uninformative priors, but it doesn't have to stay that way. If you want to learn something
yourself, you can take the initiative to update your knowledge and your beliefs. And thanks to
the sponsor of this video, this process is only getting faster and easier. Brilliant is an online
platform for learning math, computer science, and data science. They are for courses that are
updated every month and you can solidify your understanding through interactive exercises.
First-hand experience in problem solving is the best way to stress test your knowledge,
and Brilliant makes this a top priority. Recently, I've been working through the Math for Quantitative
Finance course since it's a topic in statistics that I don't know much about but want to know more of.
To try everything Brilliant has to offer for free for a full 30 days, visit brilliant.org
very normal or click on the link in the description. You'll also get 20% off an annual premium
subscription. Thank you to Brilliant for sponsoring this video. Thanks for watching, I'll see you in the next one.

The year was 1956 and the place was Dartmouth College.
In a research proposal, a math professor used a term that was then entirely new and entirely
fanciful, artificial intelligence.
There's nothing fanciful about AI anymore.
The directors of the Stanford Institute for Human-Centered Artificial Intelligence, John
Etchamendi, and Fei-Fei Li.
On Uncommon Knowledge, now.
Welcome to Uncommon Knowledge.
I'm Peter Robinson.
Philosopher John Etchamendi served from 2000 to 2017 as provost here at Stanford University.
Dr. Etchamendi received his undergraduate degree from the University of Nevada before earning
his doctorate in philosophy at Stanford.
He earned that doctorate in 1983 and became a member of the Stanford Philosophy Department
the very next year.
He's the author of a number of books, including the 1990 volume, The Concept of Logical Consequence.
Since stepping down as provost, Dr. Etchamendi has held a number of positions at Stanford
including, and for our purposes today, this is the relevant position, co-director of the
Stanford Institute for Human-Centered Artificial Intelligence.
Born in Beijing, Dr. Fei-Fei Li moved to this country at the age of 15.
She received her undergraduate degree from Princeton and a doctorate in electrical engineering
from the California Institute of Technology.
Now a professor of computer science here at Stanford, Dr. Li is the founder once again
of the Stanford Institute for Human-Centered Artificial Intelligence.
Dr. Li's memoir published just last year, The Worlds I See, Curiosity, Exploration,
and Discovery at the Dawn of AI.
John Etchamendi and Fei-Fei Li, thank you for making the time to join me.
Thank you for inviting us.
I would say that I'm going to ask a dumb question, but I'm actually going to ask a question
that is right at the top of my form.
What is artificial intelligence?
I have seen the term 100 times a day for, what, several years now.
I have yet to find a succinct and satisfying explanation.
Let's see.
Well, let's go to the philosophy.
Here's a man who's professionally rigorous, but here's a woman who actually knows the
answer.
Yeah, she knows the answer.
So, let's take the answer, and then I will give you a different answer.
Oh, really?
All right.
Okay.
Peter used the word succinct, and I'm sweating here, so because artificial intelligence
by today is already a collection of methods and tools that summarizes the overall area
of computer science that has to do with data, pattern recognition, decision-making in natural
language, in images, in videos, in robotics, in speech, so it's really a collection at
the heart of artificial intelligence is statistical modeling, such as machine learning, using
computer programs, but today artificial intelligence truly is an umbrella term that covers many
things that we're starting to feel familiar about, for example, language intelligence,
language modeling, or speech, or vision.
John, you and I both knew John McCarthy, who came to Stanford after he wrote that, used
the term coin, the term artificial intelligence, now the late John McCarthy, and I confess
to you who knew him, as I did, that I'm a little suspicious of the term because I knew
John, and John liked to be provocative, and I am thinking to myself, wait a moment, we're
still dealing with ones and zeros.
Computers are calculating machines.
artificial intelligence is a marketing term.
So no, it's not really a marketing term.
So I will give you an answer that is more like what John would have given.
And that is, it's the field, the subfield of computer science that attempts to create
machines that can accomplish tasks that seem to require intelligence.
So the early artificial intelligence were systems that played chess or checkers, even,
you know, very, very simple things.
Now John, who, as you know, if you knew him, was ambitious, and he thought that in a summer
conference at Dartmouth, they could solve most of the problems.
All right, I'm going to come, let me name a couple of very famous events.
What I'm looking for here, I'll name the events.
We have in 1997, a computer defeats Gary Kasparov at chess.
Big moment for the first time, big blue and IBM project defeats a human being at chess,
and not just a human being, but Gary Kasparov, who by some measures is one of the half dozen
greatest chess players who ever lived.
And as best I can tell computer scientists that, you know, things are getting faster,
but still.
And then we have, in 2015, a computer defeats Go expert Han Fuei, and the following year
it defeats Go grandmaster Lee Seadol, I'm not at all sure I'm pronouncing that correctly,
in a five game match, and people say, whoa, something just happened this time.
So what I'm looking for here is something, something that a layman like me can latch
on to and say, here's the discontinuity, here's where we entered a new moment, here's artificial
intelligence.
Am I looking for something that doesn't exist?
No, no, I think you're not.
So the difference between deep blue and which played chess, deep blue was written using
traditional programming techniques, and what deep blue did is it, it would, for each move,
for each position of the board, it would look down to all the possible.
Every conceivable decision tree?
Every decision tree, to a certain depth.
I mean, obviously, you can't go all the way.
And it would, it would have ways of, of way, which ones are best, and so then it would
say, this is the best move for me at this time.
That's why, in some sense, it was not theoretically very interesting.
The Go, AlphaGo, AlphaGo, which was a Google project, was that Google project.
This uses deep learning, it's a neural net, it's not explicit, explicit programming.
We don't know, you know, we don't go into it with an idea of, here's the algorithm
we're going to use, do this and then do this and do this.
So it was actually quite a surprise, particularly AlphaGo.
Not to me.
Sure.
No, no, no.
To the public, yes.
To the public.
Yeah.
But our, our colleague, I'm going at this one more time because I really want to understand
this.
I really do.
Our colleague here at Stanford, ZX Shen, who must be known to both of you, a physicist
here at Stanford, and he said to me, Peter, what you need to understand about the moment
when a computer defeated Go, Go which is a much more complicated, at least in the decision
space, much, much bigger, so to speak, than chess, there are more pieces, more, alright.
And ZX said to me that whereas chess just did more quickly, what a committee of grandmasters
would have decided on, the computer in Go was creative.
It was pursuing strategies that human beings had never pursued before.
Is there something to that?
Yeah.
So there's a famous.
If he's getting impatient with me, I'm asking such, no, no, you're asking such good questions.
So in the third game of the, I think it was the third game of the five games, there was
a move.
I think it was move 32, 32 or 35, is that the computer program made a move that really
surprised every single Go masters.
Not only Lisa Dole himself, but everybody who's watching.
That's a very, that's a very surprising move.
I thought it was, I thought it was a mistake.
In fact, even post-Adonizing how that move came about, the human masters would say this
is completely unexpected.
What happens is that the computers like John says, right, is, has the learning ability and
has the inference ability to think about patterns or to decide on certain movements, even outside
of the trained, familiar human masters domain of knowledge in this particular game.
Yep, go ahead.
Yes, yes, yes.
Let me expand on that.
The thing is, these deep neural nets are supremely good pattern recognition systems.
But the patterns they recognize, the patterns they learn to recognize are not necessarily
exactly the patterns that humans recognize.
So it was seeing something about that position and it made a move that because of the patterns
that it recognized in the board that made no sense from a human standpoint.
In fact, all of the lessons in how to play Go tell you never make a move that close to
the edge that quickly.
So everybody thought it made a mistake and then it proceeded to win.
And I think the way to understand that is it's just seeing patterns that we don't see.
It's computing patterns that is not traditionally human and it has the capacity to compute.
Okay, we're already entering this territory, but I am trying really hard to tease out the
wait a moment.
These are still just machines running zeros and ones, bigger and bigger memory, faster
and faster ability to calculate, but we're still dealing with machines that run zeros
and ones.
That's one strand.
And the other strand is, as you well know, 2001 Space Odyssey where the computer takes
over the ship.
Open the pod bay doors, Hal.
I'm sorry, Dave.
I'm afraid I can't do that.
Okay, we'll come to this soon enough.
Fei-Fei Li in your memoir, The Worlds I See, quote, I believe our civilization stands
on the cusp of a technological revolution with the power to reshape life as we know
it.
Revolution, reshape life as we know it.
Now you're a man whose whole academic training is in rigor.
Are you going to let her get away with this kind of wild overstatement?
No, I don't think it's an overstatement.
I think she's right.
He told me to write the book.
Mind you, Peter, it's a technology that is extremely powerful and is allowing us to get
computers to do things we never could have programmed them to do.
And it will change everything.
But it's like, a lot of people have said it's like electricity or it's like the steam revolution.
It's not something necessarily to be afraid of.
It's not that it's going to suddenly take over the world.
That's not what Fei-Fei was saying.
Right.
It's a powerful tool that will revolutionize industries and human the way we live.
The world revolution is not that it's a conscious being.
It's just a powerful tool that changes things.
I would find that reassuring if a few pages later, Fei-Fei had not gone on to write.
There's no separating the beauty of science from something like, say, the Manhattan Project,
close quote.
Nuclear science.
We can produce abundant energy, but it can also produce weapons of indescribable horror.
AI has boogeymen of its own, whether it's killer robots, widespread surveillance, or
even just automating all eight billion of us out of our jobs.
Now, we could devote an entire program to each of those boogeymen, and maybe at some
point we should.
But now that you have scared me, even in the act of reassuring me, and in fact, it throws
me that you're so eager to reassure me that I think maybe I really should be even more
scared than I am.
Let me just go right down.
Here's the killer robots.
Let me quote the late Henry Kissinger.
I'm just going to put these up and let you, you may calm me down if you can.
Henry Kissinger, if you imagine a war between China and the United States, you have artificial
intelligence weapons.
Nobody has tested these things on a broad scale, and nobody can tell exactly what will
happen when AI fighter planes on both sides interact.
So I'm quoting Henry Kissinger, who is not a fool after all.
So you are then in a world of potentially total destructiveness.
Close quote.
Fei-Fei.
So like I said, I'm now denying how powerful these tools are.
I mean, humanity, before AI has already created tools and technology that are very destructive,
could be very destructive.
We talk about Manhattan Project, right?
But that doesn't mean that we should collectively decide to use this tool in this destructive
way.
Now we come to the break.
Okay, Peter, think back before you even had heard about artificial intelligence.
Which actually, what is it?
Five years ago.
No, I know.
This is all happening so fast.
Just five years ago, or 10 years ago.
After the tragic incident where an Iranian passenger plane was shot down, flying over
the Persian Gulf by an Aegis system, and one of our ships, an automated system, because
it had to be automated in order to be...
Humans can't react that fast.
Exactly.
In this case, for reasons that I think are quite understandable now that you understand
the incident, but it did something that was horrible.
That's not different in kind from what you can do with AI, right?
So we as creators of these devices, or as users of AI, have to be vigilant about what
kind of use we put them to.
And when we decide to put them to one particular use, and there may be uses, the military has
many good uses for them, we have to be vigilant about their doing what we intend them to do
rather than doing things that we don't intend them to do.
So you're announcing a great theme.
And that theme is that what Dr. Fei-Fei Li has invented makes the discipline to which
you have dedicated your life, philosophy, even more important, not less so.
Yeah, that's why we're the co-directors.
The power of this instrument makes the human being more important, not less so.
Am I being glib, or is that onto some of that?
Let me tell you a story about...
So Fei-Fei used to live next door to me, or close to next door to me, and I was talking...
I'm not sure whether that would make me feel more safe or more exposed.
And I was talking to her, I was still a pro at this time.
And she said to me, you and John Hennessey started a lot of institutes that brought technology
into other parts of the university.
We need to start an institute that brings philosophy and ethics and the social sciences
into AI, because AI is too dangerous to leave it to the computer scientists alone.
Nothing wrong with that.
There are many stories about how hard it was to persuade him when he was provost, and you
succeeded.
Can I want just one more boogeyman briefly?
And we'll return to that theme that you just gave us there, and then we'll get back to
the Stanford Institute.
I'm quoting you again.
This is from your memoir.
The prospect of just automating all billion of us out of our jobs.
That's the phrase you used.
Well, it turns out that it took me mere seconds using my AI-enabled search algorithm, search
device, to find a Goldman Sachs study from last year, predicting that in the United States
and Europe, some two-thirds of all jobs could be automated, at least to some degree.
So why shouldn't we all be terrified, Henry Kissinger of World Apocalypse, all right, maybe
that's a bit too much, but my job.
So I think job change is real.
Job change is real with every single technological advances that humanity, human civilization
has faced.
That is real, and that's now to be taken lightly.
We also have to be careful with the word job.
Job tends to describe a holistic profession or that a person attaches his or her income
as far as identity.
But there is also within every job, pretty much within every job, there are so many tasks.
It's hard to imagine there's one job that has only one singular task.
Like being a professor, being a scholar, being a doctor, being a cook, all these jobs
have multiple tasks.
What we're seeing is technology is changing how some of these tasks can be done.
And it's true as it changes these tasks, some of them, some part of them could be automated.
It's starting to change how the jobs are, and eventually it's going to impact jobs.
So this is going to be a gradual process.
And it's very important we stay on top of this.
This is why Human Centenary Institute was founded is these questions are profound.
They're by definition multidisciplinary.
Computer scientists alone cannot do all the economic analysis, but economists now understanding
what these computer science programs do will not by themselves understand the shift of
the jobs.
John, may I tell you?
Go ahead.
Let me just point something out.
The Goldman Sachs study said that such and such percentage of jobs will be automated
or can be automated at least in part.
Yes.
Now what they're saying is that a certain number of the tasks that go into a particular
land, right?
It's different in research.
Exactly.
So Peter, you said it only took me a few seconds to go to the computer and find that
article.
Guess what?
That's one of the tasks that would have taken you a lot of time.
So part of your job has been automated.
Okay.
Now let me tell you a story.
I'm also empowered.
Empowered.
Empowered.
Okay, fine.
Thank you.
Thank you.
Thank you.
You're making me feel good.
Now let me tell you a story.
All three of us live in California, which means all three of us probably have some friends
down in Hollywood, and I have a friend who was involved in the writer's strike.
Yeah.
Okay.
And here's the problem.
To run a sitcom, you used to run a writer's room.
And the writer's room would employ seven, a dozen on The Simpsons Show, The Cartoon
Show.
They'd had a couple of writer's rooms running.
They were employing 20, and these were the last kind of person you'd imagine a computer
could replace because they were well-educated and witty and quick with words.
And you think of computers as just running calculations, maybe spreadsheets, maybe someday
they can eliminate accountants, but writers, Hollywood writers.
And it turns out, and my friend illustrated this for me by saying, doing the artificial
intelligence thing where we had a prompt, draft a skit for Saturday Night Live in which
Joe Biden and Donald Trump are playing beer pong.
Fifteen seconds.
Now professionals could have tightened it up or made it, but it was pretty funny and
it was instantaneous.
And you know what that means?
That means you don't need four or five of the seven writers.
You need a senior writer to assign intelligence, the artificial, and you need maybe one other
writer or two other writers to tighten it up or redraft it.
It is upon us.
And your artificial intelligence is going to get bad press when it starts eliminating
the jobs of the chattering classes, and that has already begun.
Tell me I'm wrong.
Do you know, before the agricultural revolution, something like 80, 90% of all the people in
the United States were employed on farms.
We now, now it's down to 2% or 3%, and those same farms, that same land, is far, far more
productive.
Now would you say that your life or anybody's life now was worse off than it was in the
1890s when everybody was working on the farm?
No.
So yes, you're right.
It will have, it will change jobs, it will make some jobs easier, it will make, allow
us to do things that we could not do before, and yes, it will allow fewer, there will be
allow fewer people to do more of what they were doing before, and consequently there
will be fewer people in that line of work.
That's true.
That is true.
I also want to just point out two things.
One is that jobs is always changing, and that change is always painful, and we're, as
compared scientists, as philosophers, also as citizens of the world, we should be empathetic
of that, and nobody is saying we should just ignore that change in pain, so this is why
we're studying this, we're trying to talk to policymakers, we're educating the population.
In the meantime, I think we should give more credit to human creativity in the face of
AI.
I, I start to use this example that's not even AI.
Think about the advanced, speaking of Hollywood, graphics, technology, CGI, and all that, right?
The video gaming industry?
No, no, just animations and all that, right?
One of many of our, including our children's, favorite animation series is by Ghibli Studio.
You know, Princess Nomanaki, my neighbor Totoro, Spirited Away, all of these were made
during a period where computer graphics technology is far more advanced than these hand-drawn
animations, yet their, the beauty, the creativity, the, the emotion, the uniqueness in this film
continue to inspire and, and just entertain humanity.
So, I think we need to still have that pride and also give the credit to humans, let's not
forget our creativity and emotion and intelligence is unique.
It's not going to be taken away by technology.
Thank you.
I feel slightly reassured.
I'm still nervous about my job, but I feel slightly reassured.
But you mentioned government a moment ago, which leads us to how we should regulate AI.
Let me give you two quotations.
I'll begin.
I'm coming to the quotation from the two of you, but I'm going to start with a recent
article in the Wall Street Journal by Senator Ted Cruz of Texas and former Senator Phil
Graham also of Texas, quote, the Clinton administration took a hands-off approach to regulating the
early internet and so doing it unleashed extraordinary economic growth and prosperity.
The Biden administration, by contrast, is impeding innovation in artificial intelligence with
aggressive regulation, close quote.
That's them.
This is you.
Also a recent article in the Wall Street Journal, John Etchamendi and Fei-Fei Li, quote, President
Biden has signed an executive order on artificial intelligence that demonstrates his administration's
commitment to harness and govern the technology.
President Biden has set the stage and now it is time for Congress to act.
Cruz and Graham, less regulation.
Etchamendi and Lee, Biden administration has done well, now Congress needs to give us even
more.
No.
I don't agree with that.
I believe regulating any kind of technology is very difficult and you have to be careful
not to regulate too soon or not to regulate too late.
Let me give you another example.
You talked about the internet and it's true.
The government really was quite hands-off and that's good.
That's good.
It worked out.
It worked out.
Let's also think about social media.
Social media has not worked out exactly the way we want it.
We originally believed that we were going to enter a golden age in which friendship, comedy,
well and everybody would have a voice and we could all live together at Kumbaya and
so forth.
What happened?
Jonathan Haidt has a new book out on the particular pathologies among young people from all of
these social media.
Not an argument, it's an argument but it's based on lots of data.
It seems to me that I'm in favor of very light-handed and informed regulation to try to put up sort
of bumpers.
I don't know what the analogy is.
Guardrails.
Guardrails for the technology.
I am not for heavy-handed top-down regulation that stifles innovation.
Here's another.
Let me get on to this.
I'm sure you'll be able to adapt your answer to this question too.
I'm continuing your Wall Street Journal piece.
Big tech companies can't be left to govern themselves.
Around here, Silicon Valley, those are fighting words.
Academic institutions should play a leading role in providing trustworthy assessments
and benchmarking of these advanced technologies.
We encourage an investment in human capital to bring more talent to the field of AI with
academia and the government.
Close quote.
Okay.
Now, it is mandatory for me to say this, so please forgive me, my fellow Stanford employees
apart from anything else.
Why should academic institutions be trusted?
If the country has lost faith in academic institutions, DEI, the whole woke agenda,
anti-Semitism on campus, we've got a recent Gallup poll showing the proportion of Americans
who expressed a great deal or quite a lot of confidence in higher education.
This year came in at just 36 percent and that is down in the last eight years from 57 percent.
You are asking us to trust you at the very moment when we believe we have good reason
to knock it off.
Trust you?
Okay.
So I'll start with this first half of the answer.
I'm sure John has a lot to say.
I do want to make sure, especially wearing the hats of co-directors of HAI, when we talk
about the relationship between government and technology, we tend to use the word regulation.
I really, really want to double click.
I want to use the word policy.
The policy and regulation are related but not the same.
When John and I wrote that Wall Street Journal Opinion piece, we really are focusing on a
piece of policy that is to resource public sector AI, to resource academia because we
believe that AI is such a powerful technology and science and academia and public sector
still has a role to play to create public good.
Public goods are curiosity-driven knowledge exploration.
Our cures for cancers are the maps of biodiversity of our globe, our discovery of nano-materials
that we haven't seen before, our different ways of expressing in theater, in writing,
in music.
These are public goods and when we are collaborating with the government on policy, we're focusing
on that.
I really want to make sure, regulation, we all have personal opinions, but there's more
than regulation in policy.
Let me make one last run at you.
In my theory here, although I'm asking questions that I'm quite sure you'd like to take me
out and swap me around at this point, John, but this is serious.
You've got the Stanford Institute for Human-Centered Artificial Intelligence and that's because
you really think this is important, but we live in a democracy and you're going to have
to convince a whole lot of people.
Let me take one more run at you and then hand it back to you, John.
Your article in the Wall Street Journal, again, let me repeat this.
We encourage an investment in human capital to bring more talent to the field of AI with
academia and the government.
That means money and investment means money and it means taxpayers' money.
Here's what Cruz and Graham say in the Wall Street Journal.
The Biden regulatory policy on AI has everything to do with special interest rent seeking.
Stanford faculty make well above the national average income.
We are sitting at a university with an endowment of tens of billions of dollars.
John, why is not your article in the Wall Street Journal the very kind of rent seeking
that Senator Cruz and Senator Graham are saying?
Are you kidding?
Peter, let's take another example.
One of the greatest policy decisions that this country has ever made was when Vannevar
Bush, advisor to at the time President Truman, convinced the state on through Eisenhower,
as I recall.
It's bipartisan.
Exactly.
No, no.
It was not a partisan issue at all, but convinced Truman to set up the NSF for funding,
National Science Foundation, for funding, curiosity-based research, advanced research
at the universities, and then not to cut, not to say that companies don't have any role,
not to say that government has no role.
They both have roles, but they're different roles.
Companies tend to be better at development, better at producing products, and tapping
into things that can, within a year or two or three, can be a product that will be useful.
Scientists at universities don't have that constraint.
They don't have to worry about when is this going to be commercial.
That has, I think, had such an incalculable effect on the prosperity of this country,
on the fact that we are the leader in every technology field.
It's not an accident that we're the leader in every technology field.
We didn't used to be.
Does it affect your argument if I add it also enabled us or contributed to a victory in
the Cold War, the weapons systems that came out of universities?
Absolutely.
President Wright and Star Wars.
It ended up being a defensive demand.
You could argue from all kinds of points of view that it was a good ROI for taxpayers'
money.
We're not arguing for higher salaries for faculty or anything of that sort, but we
think, particularly in AI, it's gotten to the point where scientists at universities
can no longer play in the game because of the cost of the computing, the cost, the inaccessibility
of the data.
That's why you see all of these developments coming out of companies.
That's great.
Those are great developments.
But we need to have also people who are exploring these technologies without looking at the
product, without being driven by the profit motive, and then eventually, hopefully, they
will develop discoveries, they will make discoveries that will then be commercializable.
I noticed in your book, Fei Fei, I was very struck that you said, I think it was about
a decade ago, 2015, I think, was that you noticed that you were beginning to lose colleagues
to the private sector, presumably because they just pay so phenomenally well around
here in Silicon Valley.
But then there's also the point that to get to make progress in AI, you need an enormous
amount of computational power, and assembling all those ones and zeros is extremely expensive.
So ChatGPT, what is the parent company?
OpenAI.
OpenAI got started with an initial investment of a billion dollars.
And friends and family capital of a billion dollars is a lot of money, even around here.
Okay, that's the point you're making.
Yes.
All right.
It feels to me as though every one of these topics is worth a day long seminar.
Actually, I think that they are.
And by the way, this has happened before where the science has become so expensive that it
could no longer, that university level research and researchers could no longer afford to
do the science.
It happened in high-energy physics.
High-energy physics used to mean you had a Vandegraaff generator in your office, and
that was your accelerator, or you could do what you needed to do.
And then it no longer was, you know, the energy levels were higher and higher.
And what happened?
Well, the federal government stepped in and said, we're going to help.
We're going to build an accelerator.
Stanford linear accelerator.
Exactly.
Sandia Labs, Lawrence Livermore, all these are at least in part federal established.
CERN.
CERN, which is European.
Right.
Well, Fermilab.
The first accelerator was Slack, Stanford linear accelerator center, then Fermilab, and so
on and so forth.
CERN is actually late in the game, and it's European consortium.
But the thing is, we could not continue the science without the help of the government.
Well, there is another.
And then in addition to high energy physics and then bio, right, especially with genetic
sequencing and high throughput genomics, and biotech is also changing.
And now you see a new wave of biology labs that are actually heavily funded by the combination
of government and philanthropy and all that, and that stepped in to, you know, supplement
what the traditional university model is.
And so we're now here with AI and computer science.
Okay.
This is...
We have to do another show on that one alone, I think.
The Singularity.
Oh, good.
This is good.
Reassuring.
You're both...
I mean, rolling your eyes.
Wonderful.
I feel better about this already.
Good.
Ray Kurzweil.
You know exactly where this is going.
Ray Kurzweil writes a book in 2005 that gets everybody's attention and still scares
lots of people to death, including me.
The book is called The Singularity is Near, and Kurzweil predicts a singularity that
will involve, and I'm quoting him, the merger of human technology with human intelligence.
He's not saying the tech will mimic more and more closely human intelligence.
He is saying they will merge.
I set the date for the singularity representing a profound and disruptive transformation in
human capability as 2045.
Okay.
That's the first quotation.
Here's the second.
And this comes from the Stanford Course Catalog's description of the philosophy of artificial
intelligence, a freshman seminar that was taught last quarter, as I recall, by one John
Echamendi.
Here...
Here's from the description, is it really possible for an artificial system to achieve
genuine intelligence, thoughts, consciousness, emotions?
What would that mean?
John, is it possible?
What would it mean?
I think the answer is actually no.
And thank goodness.
You kept me waiting for a moment.
I think the fantasies that Ray Kurzweil and others have been spinning up, I guess that's
the way to put it, stem from a lack of understanding of how the human being really works, and don't
understand how crucial biology is to the way we work, the way we are motivated, how we
get desires, how we get goals, how we become humans, become people.
And what AI has done so far, AI is capturing, which you might think of as the information
processing piece of what we do.
So part of what we do is information processing.
So it's got the right frontal cortex, but it hasn't got the left frontal cortex yet?
Yeah, it's an oversimplification, but yes.
Imagine that on television.
So I actually think it is, first of all, the date, 2045, is insane.
That will not happen.
And secondly, it's not even clear to me that we will ever regret that.
Wait, I can't believe I'm saying this.
In his defense, I don't think he's saying that 2045 is the day that the machines become
conscious beings like humans.
It's more an inflection point of the power of the technology that is disrupting the society.
Well, that's as he's late.
He's late.
We're already there.
That's what I'm saying.
I think you're being overly generous.
But he means by the singularity is the date at which we create an artificial intelligence
system that can improve itself and then get into a cycle, a recursive cycle, where it
becomes a superintelligence.
And I deny that.
He's playing the 2001 Space Odyssey game here.
And it's a different question, but related question.
In some ways, this is a more serious question, I think.
Although that's serious too.
Here's the late Henry Kissinger again, quote, we live in a world which has no philosophy.
There is no dominant philosophical view, so the technologists can run wild.
They can develop world-changing things, and there's nobody to say, we've got to integrate
this into something.
All right, I'm going to put it crudely again.
But in China, a century ago, we still had Confucian thought, dominant at least among
the educated classes on my very thin understanding of Chinese history.
In this country, until the day before yesterday, we still spoke without irony of the Judeo-Christian
tradition, which involved certain concepts about morality, what it meant to be human.
It assumed a belief in God, but it turned out you could actually get pretty far along,
even if you didn't believe in OK.
And Kissinger is now saying, it's all fallen apart.
There is no dominant philosophy.
This is a serious problem, is it not?
There's nothing to integrate AI into.
You take his point.
It's up to the two of you to do that.
You're the philosopher.
You're the Buddhist.
You're the philosopher.
I think this is a great...
First of all, thank you for that quote.
I didn't read that quote from Henry Kissinger.
I mean, this is why we founded the Human Center AI Institute.
These are the fundamental questions that our generation needs to figure out.
So that's not just a question, that's the question.
It was one of the fundamental questions.
It's also one of the fundamental questions that illustrates why universities are still relevant today.
Right.
And Peter, one of the things that Henry Kissinger says in that quote is that there is no dominant philosophy.
He has no one dominant philosophy like the Judeo-Christian tradition, which used to be the dominant tradition.
This was a different conversation in Paris in the 12th century, for example, the University of Paris.
In order to take values into account when you're creating an AI system, you don't need a dominant tradition.
I mean, what you need, for example, for most ethical traditions is the Golden Rule.
So we can still get along with each other.
Even when it comes to deep, deep questions of value such as this, we still have enough common ground.
I believe so.
Ah, I heave yet another sigh of relief.
Okay, let's talk a little bit.
We're talking a little bit about a lot of things here, but so it is.
Let us speak of many things as it is written in Alice in Wonderland.
The Stanford Institute.
The Stanford Institute for Human-Centered Artificial Intelligence, of which you are co-directors, and I just have two questions and respond as you'd like.
Can you give me some taste, some feel for what you're doing now, and in some ways more important, but more elusive, where you'd like to be in just five years, say.
Everything in this field is moving.
So I would, my impulse is to say ten years because it's a rounder number.
It's too far off in this field.
Fei-Fei.
I think what really has happened in the past five years by Stanford High among many things.
I just want to make sure everybody following you.
H-A-I, Stanford High is the way it's known on this campus.
Yes.
Go ahead.
Yeah, is that we have put a stick on the ground for Stanford as well as for everybody that this is an interdisciplinary study.
AI, artificial intelligence, is a science of its own.
It's a powerful tool.
And what happens is that you can welcome so many disciplines to cross-pollinate around the topic of AI or use the tools of AI to make other sciences happen or to explore other new ideas.
And that concept of making this an interdisciplinary and multidisciplinary field is what I think Stanford High brought to Stanford and also hopefully to the world.
Because like you said, computer science is kind of a new field.
You know, only, you know, the late John McCarthy coined the term, you know, in the late fifties.
Now it's moving so fast.
Everybody feels it's just a niche computer science field that's just like making its way into the future.
But we're saying, no, look abroad.
There's so many disciplines that can be put here.
Who competes with the Stanford Institute and Human-Centered Design?
Is there such an institute at Harvard or Oxford or Beijing?
I just don't know what the...
I don't think so.
In the five years since we launched, there have been a number of similar institutes that have been created at other universities.
We don't see that as competition in any way.
If these arguments you've been making are valid, then we need them.
We see that as a movement.
We need them.
And part of what we want to do and part of what I think we've succeeded to a certain extent doing is communicating this vision of the importance of keeping the human
and human values at the center when we are developing this technology, when we are applying this technology.
And we want to communicate that to the world.
We want other centers that adopt a similar standpoint.
And importantly, one of the things that I didn't mention is one of the things we try to do is educate.
And educate, for example, legislators so that they understand what this technology is, what it can do, what it can't do.
So you're traveling to Washington or the very generous trustees of this institution are bringing congressional staff and...
Both are happening.
So Feifei, did you teach that course in Stanford HAI or was the course located in the philosophy department across the list?
I'm just trying to get a feel for what's actually taking place there now.
I actually taught it in the confines of the HAI building.
So it's an HAI?
No, it's a philosophy course.
It's listed as a philosophy course but taught in the HAI.
He's the former provost.
He's an interdisciplinary walking wonder.
And your work in AI-assisted healthcare, is that taking place in HAI or is it at the medical school?
Well, that's the beauty.
It's taking place in HAI, computer science department, the medical school.
It even has collaborators from the law school, from the political science department.
So that's the beauty.
It's deeply interdisciplinary.
If I were the provost, I'd say this is starting to sound like something that's about to run amok.
Doesn't that sound a little too interdisciplinary, John?
Don't we need to define things a little bit here?
Let me tell you, let me say something.
So Steve Denning, who was the chair of our board of trustees for many years and has been a long, long time supporter of the university in many, many ways.
In fact, we are the Denning co-directors of Stanford HAI.
Steve saw five, six years ago, he said, you know, AI is going to impact in a free department at this university.
And we need to have an institute that makes sure that that happens the right way, that that impact does not run amok.
Where would you like to be in five years?
What's a course you'd like to be teaching in five years?
What's a special project?
I would like to teach a course.
Freshman's Seminar called The Greatest Discoveries by AI.
Oh, all right.
Okay.
A last question.
I have one last question, but that does not mean that each of you has to hold yourself to one last answer, because it's a kind of open-ended question.
I have a theory, but all I do is wander around this campus.
The two of you are deeply embedded here and you ran the place for 17 years, so you'll know more than I will.
Including, you may know that my theory is wrong, but I'm going to trot it out, modest though it may be, even so.
Milton Friedman, the late Milton Friedman, who when I first arrived here was a colleague at the Hoover Institution.
By some miracle, his office was on the same hallway as mine, and I used to stop in on him from time to time.
He told me that he went into economics because he grew up during the Depression.
And the overriding question in the country at that time was how do we satisfy our material needs?
There were millions of people without jobs.
There really were people who had trouble feeding their families.
All right.
I think of my own generation, which is more or less John's generation.
You'll come much later, Feifei.
Thank you.
And for us, I don't know what kind of discussions you had in the dorm room, but when I was in college,
there were bull sessions about the Cold War, where the Russians were going, the Cold War was real to our generation.
That was the overriding question.
How can we defend our way of life?
How can we defend our fundamental principles?
All right.
Here's my theory.
For current students, they've grown up in a period of unimaginable prosperity.
Material needs are just not the problem.
They have also grown up during a period of relative peace.
The Cold War ended.
The Soviet Union declared itself defunct in 1991.
Cold War is over at that moment at the latest.
The overriding question for these kids today is meaning.
What is it all for?
Why are we here?
What does it mean to be human?
What's the difference between us and the machines?
And if my little theory is correct, then by some miracle,
this technological marvel that you have produced will lead to a new flowering of the humanities.
Do you go for that, John?
Do I go for it?
I would go for it if it were going to happen.
Did I put that in a slightly sloppy way?
No.
I think it would be wonderful.
It's something to hope for.
Now I'm going to be the cynic.
So far what I see in students is more and more focus on technology.
Computer science is still the biggest major at this university.
And we have tried at HAI.
We have actually started a program called Embedded Ethics.
Where the CS at the end of ethics is capitalized, so it's computer science.
That'll catch the kids' attention.
No, we don't have to catch their attention.
What we do is virtually all of the courses in computer science, the introductory courses,
have ethics components built in.
So a problem set, so you have a problem set this week.
And that'll have a whole bunch of very difficult math problems, computer science problems.
And then it will have a very difficult ethical challenge.
And it'll say, here's the situation.
You are programming a computer, programming an AI system, and here's the dilemma.
What are you going to do?
So we're trying to bring, and this is what Fei-Fei wanted, we're trying to bring...
This is new.
Ethics within the last couple of years, two, three years.
We're trying to bring the attention to ethics into the computer science curriculum.
And partly that's because students tend to follow the path of least resistance.
Well, they also...
Let's put it again, I'm saying things crudely again and again, but someone must say it.
They follow the money.
So as long as this valley that surrounds us rewards brilliant young kids from Stanford
with CS degrees as richly as it does, and it is amazingly richly, they'll go get CS degrees, right?
Well, I do think it's a little crude.
I think money is one surrogate measure of also what is advancing in our time.
Technology right now truly is one of the biggest drivers of the changes of our civilization.
When you're talking about what is this generation of students talk about,
I was just thinking that 400 years ago, when the scientific revolution was happening,
what is in the dorms?
Of course, it's all young men in Cambridge or Oxford, but that must also be a very exciting and interesting time.
Of course, there was an internet and social media to propel the travel of the knowledge,
but imagine there was the blossoming of discovery and of our understanding of the physical world.
Right now, we're in that kind of great era of technological blossoming.
It's a digital revolution.
So the conversations in the dorm, I think it's a blend of the meaning of who we are as humans,
as well as our relationship to these technologies we're building.
So properly taught, technology can subsume or embed philosophy, literature?
Of course, can inspire.
And also think about it, what follows scientific revolution is a great period of change of political, social,
economical change, right?
And we're seeing that.
Not all for the better.
Right.
And I'm not saying it's necessary for the better, but we're having even peaked the digital revolution,
but we're already seeing the political, social, economic changes.
So this is, again, back to Stanford High when we founded it five years ago.
We believe all this is happening, and this is an institute where these kind of conversations, ideas,
debates, should be taking place.
Education programs should be happening, and that's part of the reason why we did this.
Let me tell you, as you pointed out, I just finished teaching a course called Philosophy of Artificial Intelligence.
About which I found out too late.
I would have asked permission to audit your course, John.
No, now you're too old.
And about half of the students were computer science students who were planned to be computer science majors.
Another quarter planned to be symbolic systems majors, which is a major that is related to computer science.
And then there was a smattering of others.
And these were people, every one of them, at the end of the course, and I'm not saying this to brag,
every one of them said, this is the best course we've ever taken.
And why did they say that?
It inspired, it made them think.
It gave them a framework for thinking, a framework for trying to address some of these problems,
some of the worries that you've brought out today.
And how do we think about them?
And how do we not just become panicked because of some science fiction movie that we've seen,
or because we read Ray Kurzweil?
Maybe it's just as well I didn't take the course.
I'm sure John would have given me a C-minus at best.
Great inflation.
So it's clear that these kids, the students, are looking for the opening to think these things
and to understand how to address ethical questions, how to address hard philosophical questions.
And that's what they got out of the course.
And that's a way of looking for meaning in this time.
Yes, it is.
Dr. Feifei Li and Dr. John Echimendi, both of the Stanford Institute for Human-Centered Artificial Intelligence.
Thank you.
Thank you, Peter.
For Uncommon Knowledge and the Hoover Institution and Fox Nation, I'm Peter Robinson.
© BF-WATCH TV 2021

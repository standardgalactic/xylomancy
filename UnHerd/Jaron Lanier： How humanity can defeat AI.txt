Hello, and welcome back to Unheard. I am still Florence Reed. Here's a question. Who's
afraid of artificial intelligence? Right now, the answer seems to be just about everyone.
With large language models and deep fake images appearing on the internet, we're getting
an idea of what the future might look like. And for many, this seems like a sinister one.
Even the program developers themselves, the people who are in charge of this technology,
are warning of the dangers of accelerating towards what they call the singularity.
But one voice has been absolutely resolute in its rejection of this mass panic. And it comes from
the godfather of augmented reality himself. His name is Jaron Lanier. He's a computer
scientist and futurist fresh from Silicon Valley. And he is developing cutting edge artificial
intelligence at Microsoft. So he's in the belly of the beast, you could say. He recently wrote
a provocative article in The New Yorker called There Is No AI. And he thinks this hysteria might
be misplaced. He joins us live from the West Coast to tell us more about it. Just to start,
I really wanted to get you in the studio, Jaron, because you have really stood against the crowd
on this issue of AI in the last few weeks. You've been writing about how doomirous and at times
hysterical your colleagues have been. But you yourself think that perhaps humans might be the
problem rather than the machines themselves. So explain to me a bit more why you've taken
this position. My difference with colleagues is that I think the way we characterize the
technology can have an influence on our options and abilities to handle it better. And I think
treating AI as like this new alien intelligence reduces our choices and has a way of paralyzing
us. Whereas an alternate take on it, which is it's a new form of social collaboration where
it's just made of us. It's a giant mashup of human expression opens up channels for addressing issues
and makes us more sane and makes us more competent. And so I make a pragmatic argument to not think
of the new technologies as alien intelligences, but instead as human social collaborations.
But you see what I'm getting at here, which is that taking a pragmatic view over hysterical view,
someone might say, well, you work for a company that will produce the tech that we should be
hysterical about. So why should we be listening to someone who profits off this technology?
I have a really unusual role in the tech world and it shouldn't be unusual. I think
it should be more common. But essentially, I am speaking my mind honestly, even though I'm
on the inside of the castle instead of on the outside throwing stones at the castle.
In my opinion, both positions should be well manned. I don't think there's any perfect way
to handle anything in particular. One is always somewhat compromised. And
Microsoft and I have come to an accord where I have what do we call academic freedom? I speak my
mind. I speak things as I see them. But I also don't speak for the company. And we make that
distinction. And it allows me to maintain my public intellectual life, but also work inside.
I don't necessarily find agreement with everybody I work with, nor do I find absolutely
disagreement. I find it's actually rather complex. For instance, Sam Albin from Open AI really liked
my New Yorker piece. I don't think he agrees with it entirely. But he said he actually agrees with
it mostly. That's great. I hope it's of use for somebody like me to be working inside.
It's not perfect. But the thing is, I don't believe in perfection. And I have to object
to the notion that people are trying to be perfect, that you must not have any contradictions in one's
life because I just don't think you can be human without contradictions. I think it's fine to try
to reduce contradictions. And I think particularly fragrant and horrible ones should be condemned.
But I also think we need to be a little open or fuzzy about how we define each other's roles and
how we judge each other to find a way forward. You spoke about contradictions there. I suppose the
central contradiction of a lot of your colleagues who are on the more, I suppose,
deumerous side of this argument is that they will speak about the fears that they have,
the deep existential fears about working with this technology. And then continue in a conversation.
I've been watching lots of these kind of long form podcasts with people like Sam Altman who you
mentioned. And then we'll continue to speak about the research they're doing after saying that it
might bring about the end of humanity. So this is the kind of central contradiction that I think
the normal person who has not sat in a computer science lab can't quite wrap their head around.
Why do they carry on doing it? Yeah, I think that's a really important, wow. Yeah, you know, look,
I am part of a community that we call, you know, tech culture. And it's weird. We're weirdos,
you know, and no, no, just asked, actually cuts to the very heart of our weirdness. And
I have tried to understand that myself for decades. And I think part of it is we kind of
simultaneously live in a sort of a science fiction universe where we're living out the
science fiction we grew up with. And so if you grew up on the Terminator movies and the Matrix
movies and Commander Data from Star Trek and so on, naturally, what you want to do is realize
this idea of AI, you know, it just seems like your destiny. But then another part of you is
thinking, wow, but in most of those stories with Commander Data being the exception in most of
them, this was horrible for mankind. And so it feels sort of responsible to acknowledge that
it could be horrible for mankind. And yet at the same time, you keep on doing it. It's weird,
you know, and I believe the approach that I've proposed of not thinking of what we're doing
as alien intelligences that we're creating, but rather as social collaboration is the way
through that problem, because it's a way of framing it that's equally valid, but actionable.
And I just think much more sane and realistic. And but, you know, within the tech world, giving up
those childhood science fiction fantasies that we grew up with is just really hard for people.
It's just... I've been thinking I've been thinking recently about how we called the
Internet the Wild West in the in the early days, the kind of 90s and 90s. And actually,
that fantasy of the cowboy, the little boy who dresses up as a cowboy and goes and finds manifest
destiny in the frontiers, the reality of that situation was was deeply troubling. I think that's
true. Now, I grew up in rural New Mexico, in the 60s, when it was still not that economically
developed. So I actually got to experience a little bit of the tail end of the Wild West.
And I can assure you that you're correct. It was miserable. And it's not something anybody would
want. But the version of it in the movies is very appealing. And it does bring up a sort of a strange
gender identity connection, because this is a little bit of a little boys thing. It's a little bit of a
you know, what you there are. Actually, I'll tell you a story. Recently, I was on a prominent
morning, I won't say which one, but I was on a prominent morning TV talk show in the US.
And one of the hosts was a woman who asked me about this. You know, like, it just seems like
there's a lot of male fantasy in the AI world. Shouldn't there be more women AI leaders? And I
said, Well, you know, there are some spectacular women AI leaders. And actually, there does tend
to be some sort of a difference where the women seem to be a little more humanistic and have a
different approach. In the YouTube, in the YouTube version of that, they cut out the whole exchange
about women. And I called and asked about it. And they said, Well, it just seemed like a niche
question. So we cut it out. They're like, No, it's not. It's a very central question.
You hear these men saying, Look, we've created this thing, and it might turn around and destroy us
all. And I think, Well, you could only hear that from someone who's never had a child,
because that's just the experience of having a baby. Back in the 80s, my mentor is Marvin Minsky,
who is probably the single most influential author of the way we think about AI these days as
coming alive, not the only one, but probably the most prominent one. And we used to love arguing
about it. And when I was a kid, I always used to say AI is really just womb envy. It's just
men wishing and actually having had a child and seeing what it's actually like for a woman
to bear a child, I no longer have womb envy. I now appreciate it's actually
it's a rather difficult process for the mother. And I didn't know that when I was a young man.
And I would think anybody who's actually gone through having a child would lose their womb
envy pretty quickly. It tends to be men who haven't had kids yet who have
might have that desire to create life at the computer. So it's good to know our whole
universe hinges on men with kind of daddy issues in their mid 40s. That's good.
Yeah. Yeah. Welcome to our world. Let's return now just for a second to these two different types
of artificial intelligence that you distinguish in your article in the New Yorker, one of which
is this alien entity that many your colleagues seem to attribute a spark of life to and your
version, which is in fact just a network of connections, connections between things that
already exist, things that are created by humans. So it's not a generative force in your definition.
Tell me more about that. Yeah, sure. So one way you can think of AI is as or look,
I should say the term AI is a sort of a very wiggly where it's applied to all kinds of things.
And in fact, lately I've been especially advising students to call whatever they're doing AI since
it's fashionable to help them get positions and funding and stuff. So it can mean many things.
But usually these days when we talk about AI, we talk about these large model AI's like the GPT
programs. And what they are is they're a giant mashup of human creations. So for instance,
if you ask one of these programs to create a new image, you know, like I'd like to see London as if
it were across between London and Angkor Wat or some crazy thing like that, it can probably
synthesize that. But the way it does it is by using the classifiers that it uses to identify
the images that match your the components of your request and mashing them up by randomly
refining and refining an image that still works for all the classifiers. So it's actually a pretty
simple idea. And in fact, the interior of these systems, the math is almost embarrassingly simple
to my mind. But it happens at such a stupendous scale. And actually managing the whole scale
so it can happen quickly is not so simple. But the basic idea is pretty simple. So you have this
way of mashing things up that's constrained according to the principles that you used to recognize
the components in the first place. And so you can combine things. You can say, give me a lesson and
calculus, but in the language of a pirate or whatever, crazy stuff like that. And it'll synthesize
these things by maintaining the constraints from the whatever correlations had allowed the program
to recognize the components in the first place. So it creates this meaningful or not meaningful,
that's the wrong word, because we don't know what meaning is. It's it creates a expected mashup of
things from people. And that's never existed before. Now, I happen to think that's a great
capability with a lot of uses, you know, like I, I love the idea of computers just getting more
flexible, like it creates the possibility where you could say, can you reconfigure this computer
experience to work for somebody who's colorblind or whatever, right on the fly, instead of demanding
that people conform to computer design. So I actually think there's a potential in this flexibility
to really improve computation on many, many levels and make it much better for people.
But the thing is, if you want to, you can perceive it as a new intelligence. And to me, if you perceive
it as a new intelligence, what you're really doing is shutting off yourself, you're shutting down
yourself in order to worship the code, which I think is exactly the wrong thing. And it makes you
less able to make good decisions. So if we go back to, you've probably heard of the Turing test,
which was one of the original thought experiments about artificial intelligence way, way back.
There's this idea that if a human judge can't distinguish whether something came from a person
or a computer, then we should treat the computer as having equal rights, as perhaps Turing should
have been treated, you know. And the problem with that is that it's also possible that the judge
became stupid, like there's no guarantee that it wasn't the judge who changed rather than the
computer. And so the problem with treating the output of GPT as if it's an alien intelligence,
which many people enjoy doing, is that you can't tell whether the humans are letting go of their
own standards and sort of becoming stupid to make the machine seem smart, because it's all very
subjective. There's no absoluteness about that. So we haven't hit this moment that I think all of
us kind of feel that we're waiting for, we feel that we're on the brink of it, or maybe we're
just being told we're on the brink of it, where in a dark room somewhere in California, there
will be a computer that talks back, that has a kind of sentience. The sentience of others is
always a matter of faith, right? There's no way to be certain about whether someone else has
interior experience in the way that you do. All right, I presume that you do, but I can't know.
And there is a kind of a mystical or almost supernatural element in which we have internal
experience. And, or at least I do, but I can't make you believe I do, you know, you have to just
believe on your own that I do. And the thing is that faith is a very precious thing. And
there's no absolute argument that you should or shouldn't believe that another person has
interior experience or sentience or consciousness, or a machine or whatever. I mean, I do think faith
is not fundamentally rational, but there is a pragmatic argument, as I keep on repeating,
to placing your faith in other people instead of machines, if you care about people at all. If
you want people to survive, you have to kind of place your faith in the sentience of them instead
of in machines, as a pragmatic matter, not as a matter of absolute truth, which we can't access.
Is really the only distinguisher then between me asking you, are you sentient and you replying,
yes, and me asking a machine, are you sentient and the machine replying, yes, is the only
distinction there of faith in the power of the human soul versus the fact that that computer
is just amalgamating information? Yeah, I think ultimately it's a matter of faith
that has pragmatic implications. So I think the quest for absolute ultimate truth is not really
viable. So there was a period hundreds of years ago where philosophers grappled with this about God
and this question of whether you can absolutely prove whether God exists or not. And I think
almost everyone who's considered that question has decided that that's a matter of faith,
but doesn't have an absolute truth value that you can establish through logic or empiricism.
I think we're at the point where we have the same issue with one another, where
there is no absolute proof through logic or experiment about what's going on in terms of
experience within other people. However, once again, just to say something is a matter of faith
doesn't mean that the choice of faith is entirely arbitrary because it can be pragmatic as well.
So if not believing in people increases the chance that people will be harmed,
which I think is the case with this technology, or relatively not believing in people to believe
that machines are the same as people, I think increases the chance that people will be harmed.
So in addition to whatever other reasons you might have, perhaps sentimental ones for having
a faith in people, there's a pragmatic argument that joins them. And I think cumulatively we
should believe in people over computers, but that's not an absolute argument based on logic
or empiricism, which I don't think is available to us. But I do think that pragmatic arguments,
there's a bit of a skyhook thing here. And it's a little bit like the problem of why should you
stay alive instead of committing suicide? It's applied to the whole species. Why should we
continue this human project? Why does it matter? Well, it is a matter of faith. And I guess I come
to something that's a little bit like the argument attributed to Pascal. So there's this notion that
you might as well believe in God just in case it's real, and there's heaven and hell. I don't
buy that particular argument. I'm not concerned about heaven or hell. However, I do think the
continuation of us people in this timeline, in this world, in this physicality is something
I'd like to commit to. I think it's important. I think we might be something special. And so
kind of in that way, I'd like to apply faith to us and give us a chance. And that does involve
demoting computers in this case. But it's not just that when we demote computers, we can use them
better. We can make this like, as I pointed out in the New Yorker article, demoting AI allows us
to not mystify. And that allows us paths to explaining it, to controlling it, to understanding
it, to using it as a scientific exploration of what language it is, all kinds of things.
There's so many reasons to demote it that are practical, that the faith in it as a mystical
being just actually seems kind of stupid and wasteful and pathetic to me. But I don't know,
you know, I haven't convinced everybody. But can we demote something that has potentially
more power than us already? You know, can we actually say, look, we're going to drag computers
down the hierarchy of being when they are already in control of so much of our lives? Most of us
are subordinated to computers in our everyday lives. I mean, I'm sitting here literally with
my computer. Okay, close that thing right now. Close that thing right now. People are capable
of being self-destructive, idiotic, wasteful, ridiculous, with or without computers. However,
we can do it a little more efficiently with computers because we can do anything a little
more efficiently with computers. As, as is well known, I've been very publicly concerned about
the dehumanizing elements of social media algorithms. And the algorithms on social media
that have caused outbreaks of sort of elevated, these things always existed in humanity,
there's just a little more vanity, paranoia, irritability, and that increment is enough to
change politics, to change mental health for a lot of people, to change safety for a lot of
people, especially in impoverished circumstances around the world.
It's just made the world worst, you know, worse incrementally. So, and the algorithms in social
media are really dumbass simple. I mean, there's surely not a lot there. These are a little better
algorithms with a lot more data. And so if we can screw ourselves up with the previous generation,
surely we can also screw ourselves up even worse with this. And so I think your framing,
though, that it's more powerful than us is incorrect. I think it's really just dumb stuff.
It's really up to us to decide how it fits into human society still. The capacity for human
stupidity is great. And as I keep on saying, hard to distinguish from the, you know, it's only a
matter of faith, whether we call it human stupidity or machine intelligence, they're
indistinguishable logically. So I do think the threat is real. I'm not, I'm not anti-dumist in
a sense. I just ask us to consider what is the way of thinking that improves our abilities,
that improves our thinking, that gives us more options, that gives us more clarity.
And it does involve demoting the computer. There's a lot of work to do. We have a lot of work to do
technically. We can create explanations for what machine, so-called machine intelligence is doing
by tracing it back to the human origins. So there have been a number of very famous instances of
chatbots, indeed, our chatbots, the GPT bots, getting really weird with people. But, you know,
the form of explanation should be to say, well, actually the thought was, at that point, parroting
some stuff from soap opera, from some fan fiction, from some other weird stuff. And that kind of
explanation should always be available. That should always be there. So you can look at it and say,
oh, that's where it got it. That's what's going on. And in my opinion, there should be an economy
in the future where, if there's really valuable output from an AI, the people whose contributions
were particularly important should actually get paid. Like, I actually believe there's a new
extension to society that's very creative and interesting, rather than this dismal prospect
of everybody being put out of work. I think that we can invert that and make it better.
But that's the same thing as having better explanations, better understanding,
more transparency. But transparency in a mashup technology can only come from revealing the
people whose expressions were mashed up. I mean, to me, this is just actually kind of simple.
But that technical work to make that revelation practical isn't completed at this time.
But it could be. And also, if policy is going to be based on the idea that we now have this
new supernatural artificial entity, there's no sensible way to resolve that. There's no way
that that becomes clear. But this other way of thinking does provide avenues for clarity and
concreteness. Well, one thing you didn't do was sign that kind of open letter demanding for a
hiatus in accelerating AI development, which was signed by Elon Musk and Sam Altman,
who we spoke about earlier. Why didn't you sign that? Why was that not appealing to you as an
idea if we want to demote this technology in some way? My reason for not signing it is that
it fundamentally still mystified the technology. It still took this position that it's like this
new alien entity. And it also left open this very, when you think it's an alien entity,
there's no way to know what would help. Like, if you have an alien entity, what regulation
is good? Like, if you say, well, it shouldn't hurt people. This was an idea that was
first proposed perhaps by Isaac Asimov, or one of the early explanations was Isaac Asimov's
science fiction of long, long ago about the laws of robotics. The problem is that these,
this idea is very vague. Like, how do you define harm? What is it? You can't, because these,
as much as the GPT programs impress people, they don't actually represent ideas.
We don't know what meaning is. We don't know, we don't know how to define these things. All we
can do is mash things up so that they conform with classifiers. So they can't, they can't do
philosophy. They can't do the work of thinking. They could mash up philosophers in ways that might
be interesting. If you say, you know, do a combined essay as if Descartes and Derrida
collaborated or something, something might come out that's provocative or interesting,
but there's no actual representation inside there. And so getting provocative or interesting
mashups is great and I think useful and helpful, but you can't set policy by it because there's
not actually any meaning. And we see that every day when we try to, there's been a lot of work in
AI safety and fairness and other concepts where you try to keep people from misusing the AIs,
but then it becomes a game where all the people out there who have access to it try to count
countervene, you know, and come up with some nasty version of it even though you tried to prevent
it. And you can't because when you try to prevent it, all you're doing is coming up with some sort
of surface features that you think are associated with bad behavior of humans, but humans can always
kind of route around those to find other ways to express bad behavior. There's no fundamental
representation inside these things. And we just have to accept that as reality. We don't know what
meaning is and we can't represent meaning. But so much here of your argument relies on the idea
that if we define this technology differently, then we will have more power over it or at least we'll
have more understanding of it. And so we won't have so much hysteria and fear which could lead to
this kind of catastrophic mistake that you say could actually end with disaster. But I suppose
you might say, well, aren't we just trying to make ourselves feel better there? We're just
comforting ourselves by saying, look, no, no, don't worry, it's not an alien technology. It is
simply an amalgamation of human connections that already exist. Are we are we not just
self comforting here with a kind of bit of rhetoric about it being this human technology
rather than something we can't control? Yeah, I mean, I don't think that's the case. It's actually
proposing this more concrete and clarified path of action is very demanding of people. It's not
comforting at all. It demands that everybody involved on a technical or regulatory level
do much more than they have. And so it's actually probably not putting people in a comfortable
situation. I suspect many people would prefer the mystical version because it actually lets them
off the hook. The mystical version just lets you sit there and apprehend and express awe at our
inventions or something. But the stuff I'm talking about demands action. And so it's actually,
I think puts pressure on people and it's not comforting. And it shouldn't be.
Do you think humans need to take more accountability for their part in
developing a potentially malign form of AI?
Well, I'm talking here about the actual malignancy of it. You know, we speak a lot about it kind of
going off the rails, but would it be going off the rails because we've set it up to do so?
Yeah. Okay. So the one example I use in the New Yorker piece is the disasters with the Boeing
737 MAX. So the flight correction module in it was in a sense the source of
two terrible air disasters in which hundreds of people died, but in a sense in which it wasn't.
What actually happened is the way they sold it, the way they withheld information about it,
depending on how much you paid them, the way they trained people for it, the way
the documentation was created, all this crazy stuff. It's the surrounding stuff that created
the disaster, not the core capability, which probably has been useful in general. And so
in the same way, this large model AI, it's not the thing itself. It's the surrounding material
that determines whether it's malignant or not. And when you deploy it under the assumption that it's
an alien new intelligence, that it's a new entity with its own point of view that should be treated
as a creature instead of a tool, when you do that, you greatly increase the chances of a scenario
similar to the one that befell passengers on the Boeing planes. And I do think that's
a real possibility. The malignancy, though, is in the surrounding material, not in the core
technology. And that's extremely important to understand. I don't think anybody has claimed
that the flight path correction module shouldn't have existed. I think what people are saying is
that the pilots should have been well informed, well trained, and the ability to control it should
have always been included, not only for those who paid more. So that was where the errors were,
and that's all been well documented. And I think we're making sometimes some potentially similar
mistakes with this, that if you have chatbots and you tell people, oh, this is this intelligent
companion, you should be able to date it, you should be able to trust it, you should be able
to do this and that, which I think most of us don't do, but some of us do, then the chances of
something really bad happening do increase. And as I say, we know that that's possible because
we've seen it with the previous generation of much less sophisticated and smaller scale AI
as applied in social networks. So if the threat really is coming from humans, if anything,
then isn't the main worry that most people should have that this sort of technology or an accelerated
version of it might fall into the hands of someone who has malign intent against a group or a country.
I'm thinking here particularly about the situation that's happening in Ukraine right now.
Russia has been one of the worst actors in misusing the internet and algorithms to
mess up people. For instance, it's documented that in the US, Russia created
enormous numbers of fake accounts of fake bots in order to sell divisions within the US. And
it's impossible to attribute events specifically to something like this, but it certainly contributed
to our Trump era and it undoubtedly contributed to Brexit. Whatever one thinks of those things,
from Russia's point of view, those were in its interest. And of course, it's attempting those
things in Ukraine. I worry a little bit more about China because Russia doesn't quite have
the resources to pull off very large model, very massive things right now. It's not that easy to
do. You need really huge computational resources available. So I worry a little bit about Chinese
using, just to be very blunt, data from TikTok to mountain attack like that on the morning of a
Taiwan invasion or something like that. That's imaginable. I hope they wouldn't do it. I've talked
to a lot of people in the Chinese world and I think almost all actually are much more conscientious
and better intention that might be imagined, but there's always somebody. I mean, they're in any
country in any situation. So I do worry about it. And the antidote to it is universal clarity,
context, transparency, which can only come about by revealing people since revealing ideas is
impossible because we don't know what an idea is. So every time there's some mashup, we should see
where it came from. We should see what's mashed up. So if that was just generally the case, then
this whole type of attack would be nullified. It's just a kind of blockchain concept, but for
artificial intelligence. The idea that everything would have a kind of stamp and a history to it.
Yeah. And it doesn't have to be blockchain. The problem with blockchain, of course,
is its carbon footprint. So you could do something similar to blockchain that isn't a climate
disaster, as has been demonstrated by the Ethereum community and others. But just as a
placeholder, blockchain-like thing is certainly the right approach.
We've established, though, that we already live with artificial intelligence. We are living in an
artificially intelligent world. So how has that already changed us? Have you seen differences in
our culture, our personal interactions, our society that you could attribute to this new
living with AI? Our principal encounter with algorithms so far has been in the construction
of feeds we receive in our apps. It's also present for those of us who are not of the highest socioeconomic
status in whether we get credit or not, and in other things like that, and whether we get admitted
to a university or not, or whether we're sent to prison or not, depending on what country we're
talking about. So it's definitely true that algorithms have transformed us. I would hope
that the criticisms of that particular process that I've put forward and many others have put
forward, of whom I could mention Tristan Harris, Trishana Zuboff, and many others, I would hope
that that has illuminated and clarified some of the issues with algorithms in the previous generation.
And what could happen with the new AI is sort of worse versions of all of that. And given how
bad that was, I don't think the doomerists are entirely wrong. I think we could confuse ourselves
into extinction with our own code. But once again, in order for us to really achieve that level of
stupidity, we have to believe overly in the intelligence of the software. And I think we
have a choice. You're a musical manual composer and also someone who appreciates
art as well as science. Do you think that there is going to be a shift in the way in which we
prioritize organic or human made art? Well, let's say we enter into a world of what I call data
dignity in the New Yorker piece. Then a musician might provide music directly or might provide
antecedent music that's mashed up within an algorithm, but in a way that the musician is
still known and credited. And we already see a little bit of that. In a lot of current music
for decades now, somebody might provide the beat, somebody else might provide samples, etc. There's
already the sense of construction and mash-up, especially in hip-hop, but also just in pop music
lately. And that has not destroyed musicians as long as it's acknowledged and transparent,
as long as we see the people. And so as long as we don't hide the people,
I don't think the mash-up algorithms themselves do any damage to people. I think it's,
it last with the bowing thing, it's a surrounding material. If we choose to use the mash-up algorithms
to hide the people from whom the antecedent stuff came from, then of course we do damage. But the
thing doing damage is our hiding of ourselves, not the algorithm itself, which is actually just a
simple kind of dumb thing. What I see in culture is as long as people understand what's going on,
they find their way. So synthesizers haven't killed violins. There was a fear that they would.
And people, as long as people know the difference, as long as there's honesty and transparency about
what's going on, we can go through seasons of things being a little more artificial and then
less so. And that becomes cultural dynamic. And I trust people to handle that well.
I know I might sound a bit like someone, you know, booing at Bob Dylan going electric, but
I mean, take Spotify, it's almost totally wiped out good independent music. This is,
there have been major technological advances in music that have actually
totally obliterated any kind of creativity at these lower levels, more creative, more
maverick levels of the music industry. Yeah, you're quite correct about that. And I should
point out you still haven't closed your laptop, even though I asked you multiple times.
I know, I know, I love it too much.
So you, so all right, so position heal thyself, I guess, but look,
you're absolutely correct about Spotify. And in fact, at the dawn of the file copying era,
I objected very strenuously to this idea. And there was a cultural movement about
open source and open culture, which was selfily funded by Google and other tech companies,
and especially in Europe with the pirate parties, in which everybody thought, oh,
it's terrible, everything should be a mashup. And we don't need to know who the musician was,
and they don't need to have bargaining power and a financial transaction and blah, blah, blah.
And I think that was a gigantic wrong turn. And it was a wrong turn that we can't afford to
repeat with AI because it becomes amplified so much that it just could really destroy technology.
So I completely agree with you about Spotify. But once again, the availability of music to
move through the internet was not the problem. It's the surrounding material. Like what really
screwed over musicians was not the core capability, but this idea that you'd
build a business model on demoting the musician, demoting the person, and instead elevating the
hub or the platform. And so that's, we can't afford to keep on doing that. We just can't.
I think that that is the road that leads to our potential extinction through insanity.
Sounds like human greed might be your answer to a lot of these problems.
The thing that it comes down to. Well, I think humans are definitely responsible.
Greed is one aspect of it, but it's not all of it. At the start of our conversation,
we were talking about the sort of little boy fantasy question, and that's not greed exactly.
There's a lot like we can diagnose ourselves or each other forever. But I think the more
important thing is what approach works better, you know, because I don't necessarily understand
all human failings within myself or anybody else. But I do feel we can articulate ways to
approach this that work better and are more practical, more actionable, more hopeful.
And that has to be our first duty. Just last question for you, Jan, before I let you go.
We've talked a lot about the worst case scenarios here. What is the best case scenario if we are
the most optimistic version of ourselves? Between the two of us can come up with some
beautiful vision of the future with AI. What is that vision?
Well, what I like about the new algorithms is that they help us collaborate better. And so
I mentioned one way that can work earlier, which is you could have a new flexible kind of a computer
where you can ask it to change the way it presents things to match your mood or your cognition under
your own control so that you're less subservient to the computer. But another thing you can do is you
can say, I have written one essay, my friends written another essay, they're sort of different.
Can you mash them up 12 different ways so we can read the mashups and maybe try to
out of and this is not based on ideas. It's just based on kind of dumb math of combining words as
they appeared in order and context. But you might be able to learn new options for conciliants
between different points of view that way, which could be extraordinary. Many people have been
looking at in the sort of humanistic AI world, the human centered AI world. Could we actually
do this to help us understand potential for cooperation and policy that you might not see?
If we seem to have irreconcilable differences about how to handle something like land use
or something, is there some possibility that this mashup thing might just uncover
some strains of potential cooperation between us that are hard to see otherwise? I think it's
worth trying, it might. So it might potentially break us out of our tribes that we all exist in
and offer some human connection? It's sort of like if somebody, if a therapist says try using
different words and see if that might change how you think about something, it's not directly
addressing our thoughts. But on the surface level, it actually can help us. But it's ultimately up to
us and there's no guarantee it'll help. But I believe it will in many cases. I think it can
help us improve our research. It can help us improve a lot of our practices. And I think as
long as we acknowledge the people whose ideas are being mashed up by the programs, it can help us
even broaden participation in the economy instead of throwing people out of work, as is so often
foretold. I think we can use this stuff to our advantage and it's worth it to try.
And at the very least, if we try to use it well, these most awful ideas about us turning into the
matrix or the terminator scenario in a Skynet, those things become vanishingly unlikely if we're
revealing the people and treating it as a human project instead of an alien intelligence. I think
we really can and must do that. Well, that glimpse of hope I will take. Thank you,
Jarron, so much. Thank you so much for your time. I'll never look at it again. I promise, for you.
So with my laptop firmly in the bin, that was Jarron Lanier, the Maverick computer scientist
live from Silicon Valley telling us why he thinks we should stop and think a little before jumping
to any hasty or historical conclusions about the future of artificial intelligence. He is really
standing up against a lot of his colleagues and friends in taking a less doomerous approach to
this issue. But I still had to think there was something very sinister about his potential
view for a future living with even the best case scenario of AI. Thanks to you for watching.
This was Unheard.

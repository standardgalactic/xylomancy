Hello, and welcome to Unheard. I'm Florence Read. It appears that the world has entered
the age of existential risk. Every week, a new threat to the future of humanity comes
barreling down the track towards us. Reading the news can be, frankly, quite depressing,
and I do it for a living, but someone who seemingly never tires of thinking about the
end of civilization and how to avoid it is Swedish-born philosophy professor Nick Bostrom.
He coined the term existential risk is the founder of the Future of Humanity Institute
at the University of Oxford and is the author of many bestselling books on theoretical physics,
computational neuroscience, logic, artificial intelligence, and many other subjects of which
I have no understanding. Professor Bostrom is the most cited professional philosopher
in the world under the age of 50, which may also have something to do with being one of
the only professional philosophers in the world under the age of 50. He joins me live
from his office in Oxford for the humble task of explaining the end of the world in
around 45 minutes. Nick, a very warm welcome to unheard.
Well, thank you. Okay, so we've got our business cut out for us.
We do. It's good. I like to set the bar high to start. Let's begin by getting at a definition
of existential risk. What do we actually mean? Do we mean the total annihilation of humanity,
the end of the world as we know it, or do we actually mean something closer to civilizational
collapse?
The concept of existential risk basically means ways that the human story could end prematurely
that might mean literal extinction. That's one way we could end, but it could also mean
getting ourselves permanently locked into some radically suboptimal state that in turn
could either be collapse from which we never recover, or you could imagine some kind of
global totalitarian surveillance dystopia that you could never overthrow. And if it were
sufficiently bad, that could also count as an existential catastrophe.
As for collapse scenarios, many of those might not be existential catastrophes because you
might imagine civilizations have risen and fallen, empires have come and gone, but eventually
maybe even if our own contemporary civilization totally collapsed, perhaps out of the ashes
would rise eventually another civilization, hundreds of years from now or thousands of
years from now. So for something to be an literal existential catastrophe, it would not just
have to be bad, but the badness would have to be lasting indefinitely.
A state of kind of semi-anarchy does feel that it's almost already descended. Perhaps
that's too extreme, but it does feel a little like that.
Yeah, I think there is like a general sense of many people have recently in the last few
years that sort of the wheels are coming off a little bit, or maybe they haven't fallen off,
but they're sort of rickety and things like institutional processes and long-term trends
that were previously taken for granted, like this is the modern world. They're still worse,
but they're going to be fewer and fewer every year, and there's still a lot of ignorance,
but the education system is gradually improving. And I think the kind of faith people had in
those assumptions have been shaken over the last five years or so. Somebody plays Russian roulette
once and survives. You shouldn't draw the lesson that Russian roulette is not dangerous. They
should just thank their lucky star that they survived that first round and make sure they
never play it again. But I don't think we have kind of invested in building robust global
conflict resolution institutions or developing norms, etc. That could help.
Trust in global institutions as well seems to have fallen significantly because of the handling
of things like the COVID pandemic by these global institutions, which was in many cases so
disastrous that the individual has lost all interest in following the guidance of a kind
of world economic forum or its equivalent.
Yeah, I mean, the whole COVID thing was a big, I mean, they certainly didn't cover themselves in
glory of the World Health Organization and many other authorities, I should say, also kind of
burned a lot of their credibility.
Your people did very well.
Yeah, there were some people in my, yeah. But then even more, okay, so we get this big pandemic,
some big institutions who ought to know better sort of scramble around and do a bunch of bad
things. Okay, so it's a hard challenge. But then you might at least think after it's happened that
that would be some sort of lessons learned and that we would prepare ourselves for
another pandemic that could happen at any time from natural causes or because of biotechnology
making it easier for evildoers to conquer. But it seems like as soon as this was over,
people just lost interest. And so there is still gain of function research going on without any
more safeguards than that was before in different places of the world, open publication of
the most dangerous viral recipes.
Your books, you write a lot about the way in which it's going to become increasingly necessary
for us to learn from each existential threat as we move forward and to try and create mitigation
methods so that the next time when it becomes more severe or more intelligent or more sophisticated,
that we can actually cope with that. And that specifically, of course, relates to
artificial intelligence, which is something that, of course, we have to cover here.
It must be astonishing to you to have the subject that you've dedicated decades of your life to
studying suddenly becoming a household conversation topic, something people are debating over the
dinner table.
It is quite striking how radically the public discourse of this has shifted even just in the
last six, 12 months. I've been involved in the field for a long time. You know, there were
people working on it, but broadly in society it was more viewed as a sort of science fiction-y
speculation, like kind of, you know, not really part of mainstream concern or certainly nothing
that top-level policymakers would be. And but now we've had, I mean, just here in the UK, this
recent global AI summit, you know, the White House just came out with executive orders.
There's been quite a lot of talk, including about existential, potential existential risks
from AI, as well as more near-term issues that is kind of striking. I think that technical
progress is really what has been primarily responsible for this. People saw with, you
know, GPT-3, then GPT-3.5, and chat GPT-4, and like just the delta between these, like how much,
even just within a couple of years, this technology has improved. And it's so general
purpose, like the same AI system can, you know, write poems or program computers or answer
questions about history or anything. And so these very large transformer models seem to have this
general capacity to learn. And if they are big enough and have enough data, they also seem to be
able to learn to reason more abstractly about all this information. How close are we to something
that you might consider a kind of singularity or AGI that does actually in some way supersedes
any human control over it? Is that something that is in the near distance, or is that a long way away?
It's something that we can't any longer be confident is not in the near distance. So
as far as we can see now, there is no obvious clear barrier that would necessarily prevent
systems next year or the year after from reaching this level. It doesn't mean that that's the most
likely scenario, but we don't know what happens as you scale GPT-4 to GPT-5 level. Because we know
that when you scale it from GPT-3 to GPT-4, for example, it unlocked new abilities. There's this
phenomenon of grokking, so that initially you try to learn some tasks, and it's too hard for the AI.
Maybe it gets slightly, slightly better because it memorizes more and more specific instances
of the problem. But it's like the hard, sluggish way of learning to do something, and then at some
point it kind of gets it. Once it has enough neurons in its brain or has seen enough example,
it sort of sees the underlying principle or develops the right higher-level concept that
enables it to suddenly have a rapid spike in performance. And we've seen, as we have scaled
up these large language models, that each new order of magnitude of scaling has unlocked new
capabilities. And we don't really know what will happen if we add orders of magnitude. It might
unlock further capabilities, perhaps enough capabilities to make these systems capable of
long-term planning or capable of really high-quality research into AI that could then create feedback
loops and so on. I thought grok was an interesting name for X or Elon Musk to choose for their
AGI, considering that it has such a human implication. It's about intuition or gut feeling,
to grok something, is to really understand it in a human way. I wonder if they're
personally trying to align the machine here with an idea of human sentience or understanding.
It feels like in back rooms of places like X and at the University of Oxford, these two
things are much closer than we might think. I think probably closer than most people think.
You really need to think in terms of probabilities, but there should be more probability mass
on nearer dates than most people think. But I think perhaps the bigger difference between, say,
what I would think and what the average person would think might not so much be about
how many years between now and this thing are there, but more like when this thing happens,
how radical would that be or how quickly would things unfold at that point? Once you get to
something that can, say, substitute for humans across half of all human jobs or something like
that, I think some people model this test and then maybe the next year it will be 52% and then
54% and then over a lifetime or two, you might gradually get to complete automation. I think
that second phase will be much compressed. Once it really starts to happen, I think people will
be surprised about just the speed at which this unfolds, absent deliberate measures to slow it
down. Right. And my assumption is that there will be a company, one of these major companies,
whether it's OpenAI or X, who makes the first leap and then the others will follow very quickly.
There's a bunch of different ways that this could go on. It partly depends on what policy makers
think should be done as well. So at the moment, it's still a very open and competitive field
with some number of big players and then startups that are sort of
vying to also join the frontier, but it's possible that if what is required to get cutting edge
performance is increasingly large data centers to run these. So we've seen a very rapid increase
in the amount of spending on compute required to train the system. So like five years ago,
maybe you could get by with like $100,000 of computer or something or like, but now you're
probably like in, you probably need like probably billions, I mean, at least a billion dollars
worth of Nvidia chips or something to sort of be able to train cutting it like the next level of
model. And a couple of years from now, maybe that will be $10 billion. And so at that level,
it's going to decrease the number of players that could participate at the frontier. Right. You have
basically a small number of really big tech companies and then potentially governments. And
and even governments, for many, that's got to be too big an expenditure. And for others,
they are not going to necessarily have access to the Nvidia chips because of export restrictions.
Maybe it's not a government run project, but the projects that the government has a lot of oversight
into which for some people would be deeply worrying and for others reassuring.
So one one current debate is whether it is good or bad for these current models to be open source.
So Facebook open source, the llama two model, which is a little bit behind the frontier. It's
not quite as good as GPT four, but it's still a powerful model. And others are saying that this
is irresponsible or at least that it would be irresponsible to sort of open source the next
generation and the next generation after that. Because it could fall into the wrong hands. Is
that the logic there? Well, at the most basic level, so these big tech companies, they want to
limit what their models are used for. So they try to prevent them from, say, assisting users to
commit cybercrime or to do or say various bad things. But once somebody has access to the
parameters of the model, they can usually retrain it in such a way as to sort of
eliminate whatever safeguards were in the original model. This is where we come up against this
misalignment of values that you write about the idea that we have to begin to teach AI at the
earliest stages, a set of values by which it will function if we have any hope of kind of
maintaining its benefit for humanity in the long term. One of the values, the liberal values that
has been called into question with when it comes to AI is freedom of speech. There has been examples
of AI chat, GBT, the early forms of it, effectively censoring information, filtering
information that is available on the platform. Do you think that there is a genuine threat to
freedom and a kind of totalitarian impulse built into some of these systems that we're going to see
extended and kind of exaggerated further down the line? I think AI is likely to greatly increase
the ability that, say, some central power would have of keeping track of what people are thinking
and saying. So right now, I mean, we've had for a couple of decades, I guess, the ability to
collect huge amounts of information. I mean, you could sort of eavesdrop on people's phone calls
or social media postings. And it turns out governments do that. But what can you do with
that information? I mean, so far, not that much. You could sort of map out the network of who is
talking to whom. And then if there is a particular individual of concern, you could assign some
analysts to sort of read through their emails. With AI technology, you could sort of simultaneously
analyze everybody's political opinions in a sophisticated way, like sentiment analysis.
You could probably form a pretty good idea of what each citizen thinks of the government or the
current leader if you had access to their communications and with even present AI tools.
You don't have to have much imagination to imagine how that could be useful to particular
people or regimes. And then on top of that, you will then be able to customize responses. So you
could have sort of mass manipulation, but instead of sending out sort of one campaign message to
everybody, you could have a tailor-made customized persuasion message to each individual.
Appetizer is already a user message. And then, of course, you can combine that with
sort of physical surveillance systems, like with facial recognition and gate recognition,
and then all of this information going into one and credit card information. And like,
if you imagine all of this information and the communications feeding into one giant model,
I think you will be able to have a pretty good idea of what each person is up to,
and not just what they have done, but also what and who they know, but also like,
maybe what they are thinking and intending to do. And so the upshot of this might well be that,
I mean, it will take a while to shake out because there's a lot of inertia in these.
But like, eventually it might just be, there are like some scenarios in which
you get the kind of lock-in so that current political systems become
imperturbable. Like, if you have some sufficiently powerful regime in place,
it might then implement these measures and then perhaps make itself immune to overthrow.
That does feel a little like what we've seen in the last few years in China.
People have lived in sort of highly censored societies, and often what develops is the kind
of folk feeling for, well, we know they are lying to us and we are not talking about it,
but wink, wink, notch, notch. And I think people can become, over time, quite sophisticated at
sort of seeing through official propaganda narratives.
Do you think the rise in hyper-realistic propaganda, deep fake videos, what AI is going
to make possible in the coming years, will that coincide with the rise in skepticism,
generalized skepticism, like you're talking about in the Chinese example? Do you think
the average American or English or Swedish person is going to become hyper-sceptical?
I think that, in principle, society could adjust to that. But I think it will come at the same
time as a whole bunch of other things with sort of these automated persuasion bots,
like social companions built of these large language models and then with visual components
that might be very compelling and addictive, perhaps, and compete with social companionship.
And then these mass surveillance, mass potential censorship or propaganda and also mass education,
like you could also have individual tutors. I'm just talking a lot about the negative.
We know from the past, I mean, going back all the way like when people invented writing,
so that then enabled states to form because you could keep tax records, etc., and like a huge
change in how human societies were organized politically. Then like with a printing press,
another big change that ultimately may be enabled sort of modern democracy and stuff,
but also like 100 years of religious wars when people started forming their own opinion and
came to different conclusions. With social media, I think we're also seeing quite a lot of turbulence
and then when this gets AI powered, it might kick it up a notch further.
Is there a distinction between a bad use of AI? So in those examples, we're talking about
tyrannical government who uses AI to surveil its citizens versus a kind of innate moral component
to the AI itself. Is there a chance that an AGI model could in some way become a bad actor
on its own without human intervention? Yeah, so there are a bunch of different
concerns that one might have as we move towards increasingly powerful AI tools and like one
looks like completely unnecessary feud that people have had is like, well, I think concern X
is should be taken seriously and somebody else, I think concern Y should be taken seriously,
and then like, well, but X, well, what about Y? And then it's like, people love just to form tribes
and to beat one another like an excuse to just form a little tribe that you can, but I mean,
I think like both X, Y, Z and B and W need to be taken. It's just such a big thing that they're
going to be many aspects of this that need to be addressed. So yeah, so one, we were talking about
sort of these tools of AI and how they might shape political systems and this applies even just with
current AI tools for surveillance, etc. But yeah, so then you're right, there's also this separate
alignment problem, which is with an arbitrarily powerful AI system. How could you make sure
that it does what the people building it intend for it to do? And this is where it's about building
in certain principles or what you might call a kind of ethical code into the system from the
off. Is that the way of mitigating that risk? Well, yeah, or being able to
steer it, basically. I mean, it's a separate question of where do you steer it? If you could
build in some principle or goal, like which goal or which principle, but even just having the ability
to point it towards any particular outcome you want or set of principles you wanted to
follow, that is a technical problem. That is hard. And in particular, what is hard is to figure out
the way that we would be able to do that would continue to work even in these scenarios where
the AI system becomes smarter than us and perhaps eventually radically super intelligent. And where
we are no longer able to maybe at that point understand what it is doing or why it is doing it,
what's going on inside its brain. We still want this original scaling method to keep working,
to arbitrarily high levels of intelligence. And we might need to get that right on the first try.
So no pressure, no pressure. No pressure. In Western liberal democracies, or what might be
considered Western liberal democracies, we have a lot of values that are in direct conflict with a
kind of utilitarian way of living. We think about welfare, caring for those who are at the bottom
of society, those who might not be able to contribute for some reason or another,
children prior to working age. If a model was created to just maximize utility, you can imagine
already the kind of what the potential outcomes of that might be. And I think for many people,
that is the existential fear, is that the computer brain would have a naturally more utilitarian
bent than the human brain, which in these liberal democracies likes to think about in some way,
not maximizing utility and maximizing efficiency, but rather thinking about how do we best live
in a common society with a common good, which are quite wishy-washy concepts when you actually
dig into them. I think there could be like two ways in which this, so on the one hand,
you might object in general to this consequentialist, maximizing approach, which I think
makes sense to object to. I think that doesn't capture everything that we should care about
in ethics. But then there's an additional problem is even if you did assume that there's something
that should be maximized, we might not be able to spell out exactly what that is, let alone give
that set of instructions to a computer without sort of forgetting or leaving out a bunch of
important things. Of course, if you ask someone what value do you want to organize a society
around, they might say family, family. Well, of course, a machine that constantly spouted out
children would not be a good family, but it would certainly be a creator of family. If you were
worried about anti-natalism and lower birth rates in your country, like somewhere like Japan,
you might ask your computer to create something that maximizes for family. But already you can see
how the concept of family to us as humans is so distinct from just an idea of pumping out babies
all the time. One aspect of that problem, I think, will get easier more or less automatically. There
is a set of failures that would result from imagining the AI not really to get what it is
that you were meaning, and it's like fixating on the letter of the instruction or something. But
there as the AI becomes more capable, I think it will be able to understand not just the words
but the intentions behind the words and things like what we would maybe have added if we had
thought about it longer or if stuff like that. At least at the level at which a human is able
to do that, but probably much better. The question then is, will it be motivated to pursue
this understanding that it is maybe developing about what we truly would want on reflection?
That can be hard to do. Just to make it very simple, a typical way that you do things is you
train it to do some tasks, and then you say it like give it thumbs up when it did it the way
we wanted and thumbs down when it did it some other way. And that can work well during a certain
regime when we can tell whether the outcome was good or bad and where we remain in control to give
it the thumbs up and thumbs down. But then you start to wonder whether what it really learns
is to do the thing that we want or it starts to learn the thing that it thinks will result in
getting thumbs up. And now when the AI is limited, this might make no difference because we give it
thumbs up in exactly those cases where it did what we intended because we understand what's
going on and we are in control of the reward mechanism. But if you then try to generalize
this to a future situation, whether the AI is in control or is smarter than us, or whether
when it's doing something so complicated that we can tell whether we should give it the thumbs
up or thumbs down, then it might matter a great deal whether it was learning to do the things that
triggers humans to give it thumbs up or whether it was learning to do the thing that we really had
in mind. And it can be really hard during training to make sure that you teach it one thing rather
than the other. So that's an example of how there are many techniques that work now perfectly well
for aligning these AI systems, but that we have reason to think will systematically fail as they
get situationally aware or when they find themselves in a position where they no longer
are dependent on human approval, where they would be able to sort of short circuit that process or
delude us. Would this not inevitably lead to a situation where we have an ubermench class
at the top of society who access superintelligence before everyone else, and that compounds because
the superintelligence allows them to access even higher forms of superintelligence so none of the
rest of us can keep up? Well, yeah, and it might not require any implantations at all, but just
having access to the leading edge systems when other people don't have access, just for advice
or like asking for strategy to achieve something. Initially, probably some subset of people will
have that and maybe it will be some people in the alabs or maybe it will be the US military or
intelligent like some sort of group who have appointed themselves to because surely somebody
needs to test it out a little first before we can give everybody access, but then there's an
interval during which they will have access to maybe superintelligent advice that would place a
lot of weight on the quality of the like the trustworthiness of these individuals or institutions,
whatever exactly is the form of that, that has kind of early access.
The grave existential risk that we're all facing this week is war of course in the Middle East.
In that situation, you do have a very stark contrast between the almost medieval warfare
tactics of Hamas and the incredibly hyper-modern warfare of Israel with the iron dome and the
incredible technological capabilities. How is AI going to play into future wars? Is it already
playing a part in this conflict we're seeing in Israel? I certainly think militaries will be very
interested in developing AI-powered weapon systems and integrating it in their operations.
So far, I think they have been lagging in like the cutting edge AI is not in some
Lockheed Martin research group, but it's probably a relief actually. Yeah, but I think as the
strategic significance of this technology becomes more obvious to people, there will be an increased
tamering from sort of military intelligence establishments to have more control over this.
I would expect that that will happen increasingly as time goes by. If it is true that it will have
these tremendous strategic and security implications, it's hard to see that there would not be a kind
of that that it will all be just left to some non-profit or a cap profit Silicon Valley group
of a few hundred people to sort of do what they please. Like, I imagine people who currently
hold more powerful positions in those domains would want to get their hands on this as well and
have oversight of it. Do you think we're ready for the intelligence explosion? Are we properly
prepared or are we underprepared? No, no, no. And I mean, I don't think we'll ever. I mean,
I think it's a little bit like we are in some plane and we realize there is no pilot or the pilot has
had a heart attack and died. And so now we are the passengers here. We've got to try to land this.
Right. And it's harder because we don't have sort of a ground control that is like giving us
instructions. So we have to, we see a big, big instrument panel, right? And there are like a
few people in the cockpit kind of looking at all of this. And like, you could look at the fuel gauge
and we've got some time left in the air, like a limited period of time before this arrives. And
we got to sort of figure out how to bring this this bird in for a safe landing. How do we do that with
such incredible levels of dispute and ideological schism across the world? It feels like without a
kind of central body who's dealing with the ethics of this. We're going to very quickly get into a
situation where different polar axes of the world are developing this stuff in a kind of space race
style competition, but with their own ideological messaging and values built into it, which feels
like such an existential, such a clearly existential threat. What would be the first step
in the kind of Nick Bostrom global program to mitigate the risk of that?
I mean, I think there are like a few things on the margin and then like if wishes were horses or
whatever, like the ideal world. But on the margin, I think even a toothless but like affirmation of
the general principle that ultimately AI should be for the benefit of all, all sentient life at
least. It's too big to be like if we're talking about superintelligence, like like obviously if
somebody's making a cool little app or something that could benefit from that in the way that they
benefit from any other consumer thing that they're doing. But if we're talking about a transition to
the superintelligence era, all humans would be exposed to some of the risk in this, whether they
want it or not. And so it should seem fair that all should also stand to have some slice of the
upside if it goes well. And I would say this principle should even go beyond all of currently
existing humans and also include, for example, animals that we are treating very badly in many
cases today. And also the some of the digital minds themselves that might become moral subjects.
And so I think as of right now, I think like all we might hope for in that demand is like some
general big principle that the firms dis and then that can sort of be firmed up as we go along. So
that's one thing. I think another ask is, and that's got some recent progress has been made on
this as well, is for the next generation systems to be tested prior to deployment to check that
they don't lend themselves to people who would want to make biological weapons of mass destruction
or massive cyber crime thing. And so far AI companies have sort of done some voluntary work
on this open AI before releasing Chachi PT 4, like had the technology for more or less half a year
and like did red teaming exercises to and I think like making that more of a requirement
seems quite robustly good. Research on technical AI alignment seems good to solve the problem of
scalable alignment before we have superintelligence. I think the whole area of the moral
status of digital mind will require more attention. And that's now where the alignment
problem was like 10 years ago, like outside the Overton window. A few people are talking about
it, but it seems slightly silly. Like if you imagine having a meeting with a PM or somebody,
like you would not like, oh, well, what if the AI has moral status? And it's like a kind of fun
thing to think about. But I actually think it needs to start to migrate from that kind of
philosophy seminar topic to where alignment is now, which is like a serious mainstream
issue. It's going to have to be built into policy and politicians are going to have to think
about how they're going to deal with it. Yeah, we don't want to have a future where the majority
of sentient minds are digital minds and they're all horribly oppressed and like basically like
pigs in animal farms or something like that. That would be one way of creating a dystopia.
And it's going to be a big challenge because it's already hard for us to extend empathy
sufficiently to animals, even though animals have eyes and faces and can squeak and are much
more similar to like if it were an invisible process in a big data center. But a sentient
invisible process. Yeah, like, I mean, it's going to be harder. Like, and I think, incidentally,
there might be grounds for moral status besides sentience. Like, I think that might be a sufficient
if somebody can suffer that might be sufficient to give them moral status. But I think even
if you thought they were not conscious, but if they had goals extend as a conception of self
as an entity persisting through time, the ability to enter into reciprocal relationships with
with other beings and humans, I think that might also ground various forms of moral status.
Often in this conversation, it turns back to the human we worry about are the risk to us,
the risk to our freedoms and liberties. I suppose there we have to ask the question of
what would it be like to live in a world where you coexist with sentient beings who are not human?
And what what might that be like? I mean, the only thing I can think of it that would be akin
to it would be an alien invasion in which then we were living alongside an alien species,
perhaps even stranger than that. Yeah, except in the case of AI, we get to design the alien
species that we're going to share the world with. So that is a potentially crucial difference.
I want to know, I suppose, on the flip side of this, do you have hope for we've talked about a
lot about the risks for the rewards of this? If we can design these aliens who we're going to
coexist with for the next hundreds, thousands, millions of years, how best could we do that?
What is the kind of upside of this? What would be the best case scenario?
I think the upside is enormous. And in fact, my view is it would be tragic if we never developed
advanced artificial intelligence. I think I think it's a kind of a portal through which
humanity will at some point have to passage that all the paths to really great futures
like ultimately lead through the development of of machine superintelligence, but that
this actual transition itself will be associated with major risks. And we need to be super
careful to get that right. But I've started slightly work now in the last year or so that
we might overshoot with this increase in attention to the risks and downsides,
which I think is welcome because before that this was neglected for decades. We could have
used this time actually to be in a much better position now, but people didn't. But anyway,
so it's starting to get more of the attention it deserves, which is great. And it still seems
unlikely, but less unlikely than it did a year ago, that we might overshoot and get to the
point of like a permafrost, like some situation where AI is never developed.
Like a kind of AI nihilism that would come from being so afraid.
Yeah, I'm so stigmatized that it just becomes impossible for anybody to say anything positive
about it. And then we get one of these other locking effects, like with the other AI tools
from surveillance and propaganda and censorship and whatever this sort of orthodoxy is,
you know, at five years from now, 10 years from now, whatever, that sort of gets locked in somehow
and we then never take this next step. I think that would be very tragic. And I still think
it's unlikely, but certainly more likely than like even just six or 12 months ago, if you just plot
the change in public attitude and policymaker attitude, and you sort of think what's happened
in the last year, if that continues to happen next year and the year after, and the year after
that, then I mean, we pretty much be there, like as a kind of permanent ban on AI. And I think that
could be very bad. I still think we need to move to a greater level of concern than we currently
have, but I would want us to sort of reach the optimal level of concern and then stop there,
rather than just kind of continue. We're getting to like a Goldilocks level of fear about AI.
Yeah, yeah, I'm in the sweet spot. It's like a big wrecking ball that you can't really
control in a fine-grained way. Like people are people like the moving herds and like they get
an idea and then you know how people are. I worry a little bit about it become a big sort of stampede
to say negative things about AI and then it just running completely out of control and
sort of destroying the future in that way instead. And then of course we go extinct through some
other method instead, maybe synthetic biology without even ever getting at least to roll the die
wheel. So it's a bit of a pick your poison. It just so happens that this poison might cure you
or poison you and you just have to kind of roll the dice on it. Yeah, well, I mean, yeah. So I
think, I mean, there's like a bunch of stuff we can do to improve the arts and the sequence of
different things and stuff like that, which we should do all of those. Being a scholar of existential
risk, though, I suppose puts you in the category or the camp of people who are often this show,
being an example, asked to speak about the terrifying hypothetical futures that AI could
draw us to. Do you regret that focus on risk? Yeah, I could maybe because I think now that there
was this deficit for decades. It was obvious to me at least, but it should have been pretty obvious
that eventually AI was going to succeed. And then we were going to be confronted with this
problem of how do we control them and what do we do with them and that that's going to be really
hard and therefore risky. And that was just neglected. Like there were like 10,000 people
building AI with like five or something thinking about how we would control them if we actually
succeeded. And so now that's changed and this is recognized. So I think there's less need now,
maybe to add more to the sort of concern bucket. The doomerous work is done and now you can go
into the more self-aligning. Yeah, it's hard because it's like it's always a wobbly thing and different
groups of people have different views and there are still people dismissing the risks
or not thinking about them. And I would think that actually the optimal level of concern is
slightly greater than what we currently have. And so I still think there should still be more
concern. It's more dangerous than most people have realized. But I'm just starting to worry about it
then kind of overshooting that and the conclusion being, you know, well, let's wait for like a
thousand years before we do that. And then of course, it's unlikely that our civilization will
remain on track for a thousand years. And so we're damned if we do and we're damned if we don't.
I mean, we will hopefully be fine either way. But I think I would like the AI before some radical
biotech revolution. So if you think about it, if you first get some sort of super advanced
synthetic quality, that might kill us. But if we're lucky, we survive it. And then maybe you
invent some super advanced molecular nanotechnology and that might kill us. But if we're lucky,
we survive that. And then you do the AI. And then maybe that will kill us. Or if we're lucky,
we survive that and we get Utopia. Well, then you have to get through sort of three separate
existential risks, like first the biotech risks, and then plus the nanotech risks plus the AI risks.
Whereas if we get AI first, well, maybe that will kill us. But if not, we get through that,
then I think that will like handle the biotech and nanotech risks. And so the total amount of
existential risk on that second trajectory would sort of be less than on the former. Now, it's
more complicated than that because we need some time to prepare for the AI actually. But you can
start to think about sort of optimal trajectories rather than like a very simplistic binary question
of is technology X good or bad? We might more think like on the margin, which ones should we try to
accelerate? Which ones retard? And you get the more nuanced picture of the field of possible
interventions that way, I think. Do you have existential angst? Does this play on your mind
late at night when you're sitting in bed? Well, late at night, I'm usually still in my office
working. I'm a sort of a nightly person. I could have guessed that, I think. I could have guessed
that. I mean, it is weird to be, I mean, if this worldview is even remotely correct,
that we should happen to be alive at this particular point in human history so close
to this fulcrum or nexus, right? On which the giant future of earth-orientating intelligent life
might hinge. And out of all the different people that have lived throughout history or all the
later times that might be people that if things go well, that one should sit so close to this
critical juncture. That seems a bit too much to be a coincidence, maybe. And then you're led to
these questions about the simulation hypothesis, etc. I think there is more in heaven and on earth
than is dreamt of in our philosophy and that we understand quite a little about how all of these
pieces fit together. With that, I will leave you to get back to your very important work,
trying to put some of these pieces together, at least a few of them. Thank you, Nick Bostrom very
much. That was Professor Nick Bostrom of the University of Oxford. I felt a little torn there
towards the end. Nick was espousing the value and virtue of a future with artificial intelligence
after 45 minutes of telling me exactly how it could bring an end to humanity. It feels like
a confusing future, but he thinks one that's worth digging into rather than turning away from,
he cautioned there as well of herd mentality, a revulsion at the idea of an artificially intelligent
future, that we must keep our eyes on the prize, which is a good version of this inevitable acceleration.
Thanks to Nick for joining me. To you for watching, this was Unheard.

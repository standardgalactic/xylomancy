Konolehi is one of the world's leading minds in artificial intelligence.
He is a hacker who sees the rise of AI as an existential threat to humanity.
He dedicates his life to make sure its success doesn't spell our doom.
There will be intelligent creatures on this planet that are not human.
This is not normal, and there will be no going back.
And if we don't control them, then the future will belong to them, not to us.
Lehi is the CEO of Conjecture AI, a startup that tries to understand how AI systems think
with the aim of aligning them to human values.
He speaks to the interview about why he believes the end is near and explains how he is trying
to stop it.
He joins us now on the interview.
He is the CEO of Conjecture.
He is in our London studio.
Good to see you there.
Good to have you on the program, Konolehi.
You are something of an AI guru.
And you are also one of those voices saying we need to be very, very careful right now.
And a lot of people don't quite have the knowledge or the...
They don't quite have the vocabulary or the deeper understanding as to why they should be worried.
They just feel some sort of sense of doom, but they can't quite map it out.
So maybe you can help us along that path.
Why should we be worried about AGI and tell me the difference between AGI and what is
widely perceived as AI right now?
So I'll answer the second question first just to get some definitions out of the way.
The truth is that there is really no true definition of the word AGI, and people use
it to mean all kinds of different things.
When I talk about the word AGI, usually what I mean by this is AI systems or computer systems
that are more capable than humans at all tasks that they could do.
So this involves any scientific task, programming, remote work, science, business, politics, anything.
And these are systems that do not currently exist, but are actively attempting to be built.
There are many people working in building those systems, and many experts believe these
systems are close.
And as for why these systems are going to be a problem, well, I actually think that a
lot of people have the right intuition here.
The intuition here is just, well, if you build something that is more competent than
you, it's smarter than you, and all the people you know and all the people in the world is
better at business, politics, manipulation, deception, science, weapons development, everything.
And you don't control those things, which we currently do not know how to do.
Well, why would you expect that to go well?
Yeah, it reminds me a little bit about the debate about whether we should be looking
for life in the universe, beyond our solar system.
Stephen Hawking said, be careful, look at the history of the world.
Anytime you sort of invite us, a stronger power, more competent power, they might come
and destroy you.
And then the counter to that is that you're mapping human behavior, human desires, passions,
needs, wants, onto this thing.
Is this natural to do and fair to do because humans created it, humans created the parameters
for it?
So it's actually worse than that, in that it's really important to understand that when
we talk about AI, it's easy to imagine it to be software.
And the way software generally works, it is written by a programmer, they write code,
which tells the computer what to do, step by step.
This is not how AI works.
AI is more like organic, it's more like it is grown.
You use these big supercomputers to take a bunch of data and grow a program that can
solve the problems in the data.
Now this program does not look like something written by humans, it's not code, it's not
lines of instructions.
It's more like a huge pile of billions and billions of numbers.
And we know if we can run all these numbers, if we execute these numbers, they can do really
amazing things, but no one knows why.
So it's way more like dealing with a biological thing, like if you look at like a bacterium
or something.
And the bacteria can do some crazy things and we don't really know why.
And this is kind of how our AIs are.
So the question is, will humans impart emotions into these systems?
We don't know how to do that.
If you build systems, if you grow systems, if you grow bacteria, who are designed to
solve problems, to solve games, to make money or whatever, what kind of things will you
grow?
And by default, you're going to grow things that are good at solving problems, at gaining
power, at tricking people, at building things and so on, because this is what we want.
You reverse engineered GPT-2 at the age of 24, which was a few years ago, that's part
of the legend.
I mean, that's part of the credentialing of you before they say, well, this guy's saying,
we're in big trouble, they say, well, by the way, he knows what he's talking about because
he technically knows what he's doing.
Tell me about the pivot point between being a believer and enthusiastic about this to becoming
a warner.
What happened?
So the story goes back even further than that.
Reverse engineering is a bit generous.
It's more like, I built a system, I found out that no one can reverse engineer it.
And this is a big problem.
But it was even before then.
So I've been very into AI since I was a teenager, because I wanted to make the world a better
place.
And I think of a lot of people who believe in AI, a lot of tech people who are doing
the things we're doing today, I think most of them, maybe not most, but most of them probably,
are great people.
They're trying to build technology to make the world a better place.
When I grew up, technology was great.
The internet was making people more connected.
We were getting access to better medicines, and there was solar power was improving, there
was all these great things that science was doing.
So I was very excited about more science and about more technology.
And well, what is the best technology than intelligence?
If we just had intelligence, well, wow, we could solve all the problems.
We could do all the science.
We could invent all of the cancer medicines.
We could develop all the cool stuff.
So I was thinking when I was a teenager, and this is, I think, a common trajectory is that
people, when they're first exposed to some of these techno-utopian AGI dreams, it sounds
great.
It sounds like such a great solution.
But then as you think about this problem more, you realize that the problem with AGI is not
really how to build it.
It's how to control it.
That's much harder.
Just because you can make something which is smart or that solves a problem does not
mean you can make something that will listen to you or that will do what you truly want.
This is much, much harder.
And as I started looking into this problem more in my early 20s, I started realizing
like, wow, we are really, really not making progress on this problem.
So in that worst case scenario, whether we have an apocalyptic ending for all of us, we
get destroyed existentially or we become enslaved in the matrix, or whatever it might be.
Tell me how it actually happens in your mind.
How does this AGI assume control?
I mean, there are these famous moments in Terminator and elsewhere.
One of the Terminators, that final scene where the nuclear bombs are going off all over.
I mean, there are lots of different ways people have imagined this.
The way you see it, tell me how it happens and how, if things continue to go in the direction
that you fear, how long will it take to get there?
Well, of course, I don't personally know how exactly things will play out.
I can't see the future.
I can give you a feeling, though, of how I expect it to feel.
How do I expect it to feel like when it happens?
The way I expect it to feel is kind of like if you play chess against a Grandmaster.
Now, I'm really bad at chess.
I'm not good at chess at all.
But I can play a little bit of an amateur game.
And then, but when you play against a Grandmaster, there's someone who's much, much, much better
than you.
The way it feels is not like you're having a heroic battle against the Terminator.
You're having this incredible back and forth, and then you lose.
No, it feels more like you think you're playing well, you think everything is okay, and then
suddenly you lose in one move, and you don't know why.
This is what it feels like to play chess against a Grandmaster, and this is what it's going
to feel like for humanity to play against AGI.
What's going to happen is not some dramatic battle that the Terminators rise up and try
to destroy humanity.
No, it will be things get more and more confusing.
More and more jobs get automated, faster and faster.
More and more technology gets built, which no one even quite knows how the technology
works.
There will be mass media movements that don't really make any sense.
Do we really know the truth of what's going on in the world right now?
Even now with social media, do you or I really know what's going on?
How much of this is fake?
How much of it is generated with AI or other methods?
We don't know.
And this will get much worse.
Imagine if you have extremely intelligent systems, much smarter than humans, that can
generate any image, any video, anything, trying to manipulate you, and being able to
develop new technologies to interfere with politics.
The way I expected will go is that things will seem like mostly normal, just like weird.
Just like things are getting weirder and weirder.
And then one day, we will just not be in control anymore.
It won't be dramatic.
There won't be a fight.
There won't be a war.
It will just be one day the machines are in control and not us.
Even if there is a fight or a war, they've handed us the gun and the bullets and we've
done it.
It's us that might do all of this, precipitated by being controlled in some way.
Absolutely possible.
I don't think an AI would need to use humans for that because it could develop extremely
advanced technology.
But it's totally possible.
Humans are not secure.
It is absolutely possible to manipulate humans.
Everyone knows this.
Humans are not immune to propaganda, not immune to mass movements.
Even if an AGI gives Kim Jong-un a call and says, hey, I'm going to make your country
run extremely well and tell you how to build superweapons.
In return, do me this favor.
I mean, Kim Jong-un is going to think that's great.
And it's very easy to gain power.
If you're extremely intelligent, if you're capable of manipulating people, of developing
new technology or weapon, trading on the stock market to make tons of money, well, yeah,
you can do whatever you want.
So you're sounding the alarm.
Jeffrey Hinton, seen as the founder or father or godfather of AI, he's sounding the alarm
and has distanced himself from a lot of his previous statements.
Others in the mainstream are coming out, heavily credentialed people who are the real deal when
it comes to AI.
AI are saying, we need guardrails.
We need regulation.
We need to be careful.
Maybe we should stop everything.
Yet, open AI, Microsoft, DeepMind, these are companies, but then you have governments investing
in this.
Everybody's still rushing forward, hurtling forward towards a possible doom.
Why are they still doing it despite these very legitimate and strong warnings?
Is it only about the bottom line and money and competition, or is there more to it?
This is a great question.
I really like how you phrased, you said, that we're rushing towards, because this is really
the correct way of looking at this.
It's not that it is not possible to do this well.
It is not that it's not possible to build safe AI.
I think this is possible.
It's just really hard.
It takes time.
It's the same way that it's much easier to build a nuclear reactor that melts down than
to build a nuclear reactor that is stable.
Of course, this is just hard.
So you need time, and you need resources to do this.
Unfortunately, we're in a situation right now, as we're currently in a situation right
now where, at least here in the UK, there is currently more regulation on selling a
sandwich to the public than to develop potentially lethal technology that could kill every human
on Earth.
This is true.
This is the current case.
A lot of this is because of slowdown.
It's just governments are slow, people don't want, and vested interests.
You make a lot of money by pushing AI.
Pushing AI further makes you a lot of money.
It gets you famous on Twitter.
Look how much these people are rock stars.
People like Sam Altman's a rock star on Twitter.
People love these people.
They're like, oh, they're bringing the future and they're making big money, so they must
be good.
But I mean, it's just not that simple.
Unfortunately, we're in a territory where we all agree somewhere in the future there's
a precipice, which we will fall down if we continue.
We don't know where it is.
Maybe it's far away, maybe it's very close.
My opinion is, if you don't know where it is, you should stop.
Other people to gain money, power, or just ideological points, a lot of these people
is very important to understand.
Do this because they truly believe, like a religion, they believe in transhumanism, in
the glorious future where AI will love us and so on.
So there's many reasons, but I mean, yeah, a cynical take is just, I could be making
a lot more money right now if I was just pushing AI, I could get a lot more money than I have
right now.
How do we do anything about this without just deciding to cut the undersea internet cables
and blow up the satellites in space and just start again?
Because this is a technical problem and it's also a moral and ethical problem.
So where do you even begin right now or is it too late?
So the weirdest thing about the world to me right now as someone who's deep into this
is that things are going very, very bad.
We have crazy corporations with zero oversight, just plowing billions of dollars into going
as fast as possible with no oversight, with no accountability, about as bad as it could
be.
But somehow we haven't yet lost.
It's not yet over.
It could have been over.
There's many things where it could be over tomorrow, but it's not yet.
There is still hope.
There is still hope.
I don't know if there's going to be hope in a couple of years or even in one year, but
there currently still is hope.
Wait, hold on.
One year.
I mean, that's...
Come on, man.
I mean, we're probably going to put out this interview like a couple of weeks after we
recorded.
A few months will pass.
We could all be dead by the time this gets 10,000 views.
Just explain this timeline.
One year.
Why one year?
Why is it going so fast that even one year would be too far ahead?
Explain that.
I'm not saying one year is guaranteed by any means.
I think it's unlikely, but it's not impossible and this is important to understand.
Is that AI and computer technology is an exponential.
It's like COVID.
This is like saying in February, a million COVID infections, that's impossible.
That can't happen in six months and it absolutely did.
This is kind of how AI is as well.
Exponentials look slow.
They look like you don't go up one infected, two infected, four infected.
That's not so bad.
And then you have 10,000, 20,000, 40,000, 100,000 within a single week.
And this is how this technology works as well, is that as our computers get, there's something
called Moore's Law.
It's not really a law, it's more like an observation, that every two years our computers
get about, there's some details, but about twice as powerful.
So that's an exponential.
And it's not just our computers are getting more powerful, our software is getting better,
our AIs are getting better, our data is getting better, more money is coming into this field.
We are on an exponential.
This is why things can go so fast.
So while I'm not like, you know, it would be weird if we would all be dead in one year,
it is physically possible, you can't rule it out if we continue on this path.
The powerful people who can do something about this, especially when it comes to regulation,
when you saw those congressmen speaking to Sam Altman, they didn't seem to know what
the hell they were talking about.
So how frustrating is it for you that the people who can make a difference have zero
clue about what's really going on?
And more important than that, they didn't seem to want to actually know.
They had weird questions that made no sense.
So you're thinking, okay, these guys are in charge, I mean, no wonder the AI is going
to come and wipe us all out, maybe we deserve it.
Well, I wouldn't go that far, but this used to annoy me a lot.
This used to be extremely frustrating.
But I've come to peace with it to a large degree.
Because the thing that I've really found is that understanding the world is hard.
Understanding complex topics and technology is hard.
Not just because they're complicated, but also because people have lives.
And this is okay.
This is normal.
People have families.
They have responsibilities.
They have, there's a lot of things people have to deal with.
And I don't shame people for this.
You know, like, you know, I have Turkey, you know, with my family or with Thanksgiving
or whatever.
But my aunts and uncles, look, they have their own lives going on.
They maybe don't really have time, you know, to listen to me and give them a rant about
it.
So I don't.
So I have a lot of love and a lot of compassion for that things are hard.
This is, of course, doesn't mean that solves the problem.
But I'm just trying to say that, like, it is, of course, frustrating to some degree, that
there are no adults in the room.
This is how I would see it, is that there is sometimes a belief that somewhere there
is someone who knows what's going on.
There's an adult who's got all the control, you know, someone in the government, they've
got this under control.
And as someone who's tried to find that person, I could tell you this person does not exist.
The truth is, is the fact that anything works at all in the world is kind of a miracle.
It's kind of amazing that anything works at all with how chaotic everything is.
But the truth is, is that there are quite a lot of people who like, who want the world
to be good, you know, they might not have the right information, might be confused.
We might be getting lobbied by various people with bad intentions, but like, most people
want their families to live and have a good life.
Most people don't want bad things to happen.
Most people want other people to be happy and safe.
And luckily for us, most normal people, so not elites, not necessarily politicians or
technologists, but normal people, do have the right intuition around AI, where they see
like, wow, that seems really scary, let's be careful with this.
And this is what gives me hope.
So when I think about politicians and I'm not being in charge, I think this is now our
responsibility as citizens of the world, that we have to take this under our own hands.
We can't wait for people to save us.
We have to make them save us.
We have to make these things happen.
We have to, you know, we have to make our voices heard.
We have to say, hey, how the hell are you letting this happen?
Like, one of the beautiful things is that, you know, to a large degree, politicians can
be moved.
They can be reasoned with and they can be moved by the voters.
You can vote them out of office.
That's a good argument for democracy.
That's a great argument for democracy.
That's wonderful.
You know, democracy is the worst system except for all the other ones.
So to the point of people's feeling, and I asked about this at the very beginning,
that intuitive feeling of like, something's up here, there's something ominous.
There did seem to be a little bit of a plateau with something like chat GPT.
So initially, people were very anxious, very surprised, but very wowed by what this thing
could do.
Could write your university thesis and whatever.
It could, you know, do all these fancy gimmicks.
They seemed like magic tricks.
But then once the hype died down a little bit, people began to input new things, ask
maybe better questions, and you could see some of the limitations of something like,
you know, chat GPT and its forerunners.
And that led a lot of people to say, well, I mean, okay, sometimes this thing just sounds
like a PR department or an HR department in a company.
Sometimes it actually, it's there to detect plagiarism, but sometimes it feels like a
plagiarized like college paper, which led to, and this is anecdotally a lot of friends
of mine going, ah, maybe this thing, maybe we're okay for a while because this thing
has severe limitations.
Address that for me because a lot of people are still sort of like, well, I know there
was the hype, but now I'm not so sure.
Tell me about that.
So there is a story, I'm not sure if the story is actually true or not, but it's a good metaphor
where if you take a frog and you put it into a pot of water, you know, a cold pot of water,
the frog will sit there happily.
If you slowly and turn up the heat on your pot, the frog will sit there, there's no problem.
And if you do it very slowly, very slowly, slowly increase the temperature, the frog
will get used to the temperature and won't jump out until the water boils and the frog
dies.
I think this is what is happening with people is that people are extremely good at making
things which are crazy normal.
Is that if it's a normal thing, if it's a thing all your friends do, then it just becomes
normal.
This is like during war, why people can slaughter other people because if all your friends are
doing it, well, it's normal.
It's like, yeah, you slaughter people, it's normal, you know, killing people is fine.
This is how it can happen.
And the same thing applies here is that, well, okay, you can talk to your computer now.
Like sure, we can argue about, oh, chat APT, it's not that smart.
You can talk to your computer, like slow down.
If this was a sci-fi movie from 20 years ago, everyone would be yelling at the screen, like
what the hell are you doing?
Like this thing is obviously like crazy, like what the hell is going on?
But because it's, you know, available now, you know, cheaply online, it doesn't feel
special.
So the way to address this is I think a lack of coordinated campaigning effort.
What I mean by this is, is that the general, when we think about our civilization, not
just individual people, when we think about our civilization, how does our civilization
deal with problems?
How does it decide which problems to address?
Because there's always so many problems you could be putting your effort on.
How does it decide which one to pay attention to?
And this is actually very complicated.
It can be because of a natural catastrophe, or a war, or whatever.
It can be because of some stupid fashion hype, just like some viral video on TikTok
makes everyone freak out, sometimes, yes.
But usually, if you actually want your civilization to address a problem, a big problem, it takes
long, hard, grinding effort from people trying to raise this to saliency, to raise it to
attention.
Because, again, people have lives, you know, like most people don't have time to go online
and read huge books about AI safety, and like, oh, how do we integrate chat, GBT, or how do
we deal with like the safety trends, they don't have time for that.
Of course they don't.
And I'm not trying to judge these people, I understand.
It's not their job.
In a good world, there should be a group of people that deals with this.
The problem is they don't really exist.
Before we go, I'm glad you mentioned that people don't know where to look.
If there was one resource that you could point people in the direction of so that they can
educate themselves about the reality of the situation and can bring themselves up to speed,
that would be what?
There's not one who I think has the whole thing, which is a big problem.
Someone should make that resource.
If someone made that resource, please let me know.
So what I would probably point people towards is Control AI, which is a group of people
who I'm also involved with who are campaigning for exactly these issues, who are trying to
bring humanity together to solve these problems.
Because this is a problem that not you or me can solve.
No human can solve these problems we're dealing with right now.
This is a problem that humanity has to solve, that our civilization needs to solve.
And I think our civilization can do this, but it won't do it without our help.
It won't happen without us working together.
So if there's one thing I can do, go on Twitter or Google or whatever, go to Control AI and
support them.
Listen to what they have to say, and this is the campaign I'm behind as well.
I support them.
Okay, we'll put the link also in the YouTube description if anybody wants to check it out.
Connor, you have a brilliant mind, and I'm really grateful that we got to talk.
Thank you very much for joining us on the interview.
Thank you so much.
Take care.

Welcome everyone to free code camp.
I kiss layer on behalf of a Dureka will take this session
on natural language processing popularly known as NLP.
Now a Dureka is a global e-learning company
that provides online training courses
on the latest trending technologies.
So without any further delay.
Let's have a look at the agenda for this session.
So I'll start off by explaining the evolution
of the human language then we'll understand what is NLP
and how it came into the picture.
Moving forward will have a look
at the various application of NLP in the industry
and next we'll understand the different components of NLP
and the difficulties faced while implementing those
and finally I'll finish off this video
by explaining you guys the various steps
or the parts involved in the natural language processing
along with the demo for each one of those steps.
So the success of human race is because of our ability
to communicate that is information sharing using
this ability we have mastered head of other animals
and have become the most sophisticated creatures
and this is what differentiates us
among all the other animals we began to look for ways
to preserve our thoughts feelings messages
and other information we started
with oral communication like other animals
but because of its ephemeral nature we began painting
on walls and caves we will lift now there was a need
to standardize the drawing
so that everyone could understand
and that's where the concept of developing a language came
in however many such standards came up resulting
in many languages with each language having its own basic
sets of alphabets combination of alphabets
which were known as words and the combination of words
which were arranged meaningfully
which became the sentence now each language has a set
of rules based on words and are combined
to form these sentences now these set of rules are nothing
but what we call it as grammar now
I'm not going to take any more time explaining you guys
about grammar and all now coming to today's world
according to the industry estimates only 21%
of the available data is present in the structured form
data is been generated as we speak as we treat
as we send a message on WhatsApp Instagram I messaging
and various other platforms majority of this data exist
in the textual form which is highly unstructured in nature.
Now in order to produce significant
and actionable insights from this text data.
It is important to get acquainted with the techniques
and the principle of natural language processing.
So let's understand what exactly is NLP now natural language
processing that is NLP refers to the artificial intelligence
method communicating with an intelligence system
using natural language now.
It is a part of computer science and artificial intelligence
which deals with the human language by utilizing NLP
and its components one can organize the massive chunks
of text data perform numerous automated tasks
and solve a wide range of problems such as automation
summarization machine translation named entity recognition
relationship extraction sentimental analysis speech recognition
and topic segmentations now we'll learn about all
of these later in this video now the goal here is to process
I'd rather say understand the natural language
in order to perform useful tasks.
Some of these tasks includes making appointment buying things
spell checking generating responses
and social media monitoring now
if you look at the various application of NLP
in the industry firstly,
we have the spell checking which is usually there
in you can find it mostly in words or the document reader
even online also you can do the spell checking now.
Next we have keyword search
and it is also a field where NLP is heavily used now
extracting information from websites
or any particular document also requires the knowledge
of NLP now one of the coolest application
of NLP is the Advertisement matching
which is basically recommendation of ads based
on your search what it does is analyzes the text
of the data which you are already using or searched
and match it with the text data of the advertisement now
sentimental analysis is also a very major part of NLP.
Another application is the speaker recognition now here.
We are also talking about the voice assistants
like the Siri Google Assistant the Cortana
and we need to thank Apple
for creating the first voice assistant that is Siri.
Now next we have the implementation of chatbots.
Now most of you guys might have used the customer chat services
provided in various apps.
Now most of these apps use the chatbot
which is it uses NLP to process the data which we entered
and then it provides response based on that input.
That is also an application of natural language processing.
Now another application of NLP is the machine translation.
Now the most common example of it is the Google Translate
as you know and now it uses NLP to translate the data
or I should I'll say the text from one language
to the other and that too in real time now NLP consist
of two components which is the natural language understanding
referred to as NLU and the natural language generation
which is referred to as NLG now understanding
the natural language involves mapping the giving input
in natural language into useful representation
and analyzing different aspects of the language.
Now natural language generation is the process
of producing meaningful phrases and sentences
in the form of natural language
from some internal representation.
Now it involves text planning
which includes retrieving the relevant contents
from the knowledge base it involves sentence planning
which includes choosing require words
from meaningful phrases setting tone of the sentences
and finally we have text realization.
It is mapping sentence plan into the sentence structure.
Now we'll learn about this later in this video
and usually natural language understanding
which is NLU is much much harder than NLG.
Now you might be thinking
that even a small child can understand a language.
So how come it is so easy for human beings
is so difficult for the computer to process it.
So let's understand what are the difficulties a machine faces
while understanding a language.
So in natural language understanding
there are certain ambiguities
which are the lexical ambiguity
syntactical ambiguity and the differential ambiguity.
Now understanding a new language is very hard taking our English
into consideration.
There are a lot of ambiguity and that too in different levels
now starting with lexical ambiguity lexical ambiguity
is the presence of two or more possible meanings
within a single word.
It is also called semantic ambiguity.
For example,
let's consider the following sentences
and let's focus on the italicized word now.
She is looking for a match now.
What do you infer here from the match road?
Is she looking for a match that is head to end match
or is she looking for a match as in a partner?
The fisherman went to a bank.
Is it the bank where we withdraw our money
or is it the bank where he rose his boat
or he catches the fishes now syntactical ambiguity
in English grammar this ambiguity is the presence
of two or more possible meanings
within a single sentence or a sequence of words.
It is also called structure ambiguity
or grammatical ambiguity.
Now, let's have a look at these sentences.
The chicken is ready to eat.
So is the chicken ready to eat something
or is the chicken ready for us to eat?
So this is a kind of syntactical ambiguity,
which is often very hard to info for a new person
or rather say a computer because it means the meaning
of the sentence is different for the different tones
or in different aspects.
So for example,
if I look at the last statement,
I saw the man with the binoculars.
So do I have a banner killer or the man has a binocular?
It might be possible that you might be thinking
that I saw the man with binoculars means
that I have the binoculars
but somewhere some people might think that the guy
which I am seeing has the binoculars rather than me.
So that is syntactical ambiguity now coming
to the third ambiguity,
which is the referential ambiguity now.
This ambiguity arises
when we refer to something using pronouns now,
the boy told his father the theft.
He was very upset now when we talk about he was very upset.
If you focus on the italicized word he does this mean
that the boy was upset or the thief was upset
or the father was upset.
Nobody knows this is the referential ambiguity.
Now coming back to NLP for using NLP onto our system
or doing any natural language processing.
We need to install the NLTK library
that is the natural language toolkit.
So NLTK is a leading platform for building Python programs
to work with human language data.
It provides easy to use interfaces to 50 corpora
and lexical resources such as word net along with the suit
of text processing libraries for classification tokenization
stemming tagging and much more.
Now, let me show you how you can download NLTK now
in order to download NLTK just go to your python shell
and just type in the word NLTK dot download using the
parenthesis and then you will get this type of a window pop
of which is the NLTK downloader you just need
to select all option and click on the download button
and it will download all the corpora
and the text it has all the packages it has
into one single place and you can choose the directory
where you want to install it.
It's better if you download it in your python directory only.
It will be easier for you to access all the files
and all the text which it has to offer.
Now when we process text,
there are a few terminologies which we need to understand.
So the first terminology here.
We are talking about is tokenization.
So it is a process of breaking up strings into tokens
which in turn our small structures or units
that can be used for tokenization.
It involves three steps breaking a complex sentence
into words understanding the importance of each of the words
with respect to the sentence
and produce a structural description on an input sentence.
So if we take the sentence today,
we'll understand tokenization.
So as you can see,
we have five tokens the first one today.
We will understand tokenization.
So all of these words in computer terms are known as tokens
and this is what is referred to as tokenization.
So let me show you guys
how you can implement tokenization using the NLTK library.
So here I'm using Jupiter notebook.
You are free to use any sort of ID also.
My personal preference is Jupiter notebook.
So first of all,
let's import the OS the NLTK library
which we have downloaded and the NLTK corpus.
Now, let's have a look at the corpora
which is being provided by the NLTK.
That is the whole data.
So as you can see,
we have so many files and all of these files
have different functionalities.
Some have textual data.
Some have different functions associated
with it.
We have stop was as you can see here state union names.
We had to just sample data.
We have different kind of data and different kind of functions.
So let's take the brown into consideration.
As you can see here,
we have brown and brown zip.
So first we all we need to do is import the brown
and then let's have a look at the words
which are present in the brown.
You can see we have the fluton country grant you reset
and it's going on and on.
Now, let's have a look at the different Gutenberg fees.
So as you can see under Gutenberg file,
we have Austin MR text.
We have the Bible text.
We have the Blake poems the Carol Alice text.
Yeah, the H worth parents dot takes the Shakespeare's.
We have we have this Shakespeare.
We have the Whitman leaves.
So NLTK is a very big directory
and a very big library to download.
You might need a couple of minutes to download this library.
So let's take the Shakespeare Hamlet dot txt.
And if you look at the words,
which are inside this Hamlet file,
we see it starts with the tragedy of Hamlet by
and it goes on and on.
So if we have a look at the first 500 words
of this textual paragraph
or what we say the textual file.
So I'm using here for word in Hamlet
and I'm using the colon and 500 that is the endpoint.
So as you can see it starts with the tragedy of Hamlet
by William Shakespeare 1599 Actus Primus
Scona Prima and it goes on and on.
So for natural language processing,
you can use either of the text
which is provided here for understanding
or you can create your own words.
So for example here,
I have defined a paragraph based on artificial intelligence.
Okay, so it goes on like according to the father
of artificial intelligence John McCartney.
It's the science of engineering and so on and on.
So let us first define this string.
Okay, AI now why I'm thinking of string
is because it is easy to show you guys how to work
on a string.
So if we do the type AI,
you can see it is str with a string now under NLTK.
We have the NLTK dot tokenize
and we are going to import the word tokenize
right as a function will understand how it works.
Now we will run the word underscore tokenize function
over the paragraph and assign it a name.
Let's assign it as AI underscore tokens.
So now we'll have a look at the air underscore tokens.
You can see it has divided the whole AI paragraph
which we gave into tokens.
So as you can see it has taken a comma also
into consideration the hyphen also and it goes on and on.
Now, let's have a look at the number of tokens
where we have here.
So for that we are going to use the length function.
So as you can see, we have 273 tokens now from NLTK.
We have a probability function,
which is the frequency distinct.
So I'll show you what it does.
So for a word in air underscore tokens F test
and we are going to convert it to lower case.
So as to avoid the probability of considering a word
with uppercase and lowcase as different
and then we are going to assign it a number.
This is basically a word count program
and this is being implemented using the frequency
distinct function,
which is already present in the NLTK library.
So let's see what's the output of this one.
So as you can see it comma has appeared 30 times
full stop has appeared nine times question mark one
and so on and on like you see intelligence has appeared
six time intelligent six time intelligently has appeared
one time now suppose if you want to know the frequency
of any particular word here.
So for that we are going to use the function f-test
that is the distinct frequency.
So we are going to use that function
over the particular word.
So for example,
let's say I want to know the frequency of artificial here.
So as you can see it is three times
and if we check it in the database,
you can see it is artificial.
It is given as three now.
If you want to have a look at the number of distinct words here,
all we need to do is pass on the f-test function
to the length function.
So you can see it's one 21.
So earlier we had 273 tokens
and now from that we have one 21 distinct tokens.
Now suppose if we were to select the top 10 tokens
with the highest frequency for that I am assigning a new
if this underscore top 10
and I am using the most underscore common function here
and passing 10 that is the number of items I want.
So let's see the output.
So as you can see as I mentioned earlier comma appears 30 times
that is the highest frequency of any word
and is a paste five times.
So these are the top 10 words
which are the most reoccurring words
in the given paragraph or the given sentence.
Now, let's use the blank line tokenizer over the same string
to tokenize a paragraph with respect to a blank string.
So as you can see,
we are importing the blank line underscore tokenize earlier.
What we did was import the word underscore tokenize
and now we are using the blank line under the scope nice
and again,
we are passing the same AI paragraph
which I gave earlier
and then we are checking the length of the AI underscore plan.
So as you can see,
it provides us the output nine now what it tells us
is the number of paragraphs
which are separated by a new line in our given document.
So suppose you want to have a look at the first paragraph
or supposedly the second paragraph or you to do is pass
on the number
and it will give you the whole paragraph as you want
which is separated by a new line.
Of course now coming back to our tokenization part.
We have by grams try grams and n grams now by grams are tokens
of two consecutive written words similarly.
Try grams refer to tokens of three consecutive written words
and usually n grams is referred to as tokens
of any number of consecutive written words for n numbers.
So let us see how we can implement the same using nltk libraries
for by grams diagrams and the n grams.
So first what we need to do is import by grams try grams
and n grams from nltk dot util.
So let's take a string the best
and the most real thing in the world cannot be seen
or even test they must be filled with the heart.
What a beautiful code.
So let us now first create the tokens
of the string using the word underscore tokenize
as we did earlier now to create a by-gram.
What we need to do is use the list function
and inside that we are going to use the nltk dot by grams
and pass on the tokens.
So as you can see it has created a by-gram
of the given document similarly
if we create the try grams at the end grams.
So all you need to do is change the by grams to try grams
and it will give you the try gram list.
Now, let us now create an n gram list.
Okay.
So guys as you can see here,
we are using the same function and I'll take a dot n grams
and inside that we are passing the quotes
and the course token and here in place of n.
We are going to give our number.
So let's say suppose I provide five here.
So as you can see it has given us an n gram of length 5.
Now once we have got all the words or as I say the tokens,
we need to make some changes to the tokens
and for that we have stemming now.
What stemming does is normalize the words
into its base root form.
So if we have a look at the words here,
we have affectation effects affections affected affection
and affecting now all of these words originate
from a single root world.
That is the effect now stemming algorithm works
by cutting off the end or the beginning of the word taking
into account a list of common prefixes and suffixes
that can be found in an infected word.
Now this indiscriminated cutting can be successful
in some occasions, but not always
and that is why we affirm
that this approach presents some limitations.
Now, let's see how we can implement stemming
and we will see what are the limitations of stemming
and how we can overcome them now.
There are quite a few different types of stemmer.
So let's start with the Potter stemmer.
So for that we are going to use from nltk dot stem import
to stemmer and let's see.
What does this give us the output of stemming the word having
so as you can see it has given us the root word have now.
Similarly,
if we provide a list of words such as give giving given
and gave and I have given this name of words to stem.
So for words inverse to stem print words
and we are using the Potter stemmer,
which is the PST dot stem method says you can see it
has given us the output give give given and gave
so you can see the stemmer remove the only ing
and replace it with e now.
There is another stemmer
which is known as the Lancaster stemmer.
So let's try to stem the same thing using the Lancaster stemmer
and see what is the difference.
So let's stem the same thing using the Lancaster stemmer
and see what are the differences we have here.
So first of all,
we are going to import the Lancaster stemmer
and we are going to provide LST
which is a Lancaster stemmer function
and in the similar manner that we did for the Potter stemmer.
Let us execute the Lancaster stemmer also.
So as you can see here,
the stemmer has stemmed all the words as a result of it.
You can conclude that the Lancaster stemmer is more aggressive
than the Potter stemmer now the use of each of the stemmers
depend on the type of the task you want to perform.
Now we have another stemmer
which is known as the snow world stemmer now
for snowboard stemmer.
We have to provide the language which we are using.
So similarly if we use the snow wall stemmer
on the given words as you can see it has given us
a different output from the previous Lancaster stemmer.
So as you can see the output is the same as that
of the Potter stemmer here,
but it differs in different terms now the use of each
of this stemmer depends on the type of the task you want
to perform for example,
if you want to check how many times the word giv is used above,
you can use the Lancaster stemmer and for other purposes,
we have the snowball stemmer and the Potter stemmer as well.
So what happens is that sometimes stemming does not work properly.
It does not always result to give us the root word.
So for example,
if we take the word fish and fishes and fishing all
of that stems into fish.
So limitization is another concept.
So on one hand where stemming usually works by cutting off
the end and the beginning of the word limitization
takes into consideration the morphological analysis
of the world.
So to do so it is necessary to have a detailed dictionary
which the algorithm can look into to link the form back
to its lemma now what limitization does is groups together
different infected forms of the word called lemma.
It is somehow similar to stemming as it maps several words
into one common root.
Now one thing to keep in consideration here is the output
of limitization is a proper word.
So for example, if you look back to our output given
by the Lancaster stemmer,
you can see it has given us the output giv
which is not a word.
So the output of limitization is a proper word
and for example,
if you take a limitization of gone going it all goes
into the word go now again,
we can see how it works with the same example of words.
Let's try limitization using the nltk library.
So first of all,
we are going to import the word net as mentioned earlier.
It requires a dictionary which the algorithm can look
into to link back the form into its original lemma
and since the output is a perfect word.
So it needs to have a Disney.
So here we are going to import the word net dictionary
and we are going to import the word net lemmatizer.
So let us first take the word corpora
and see what its lemma is.
So what do you think guys?
What will be the output as you can see?
It has given the right output which is the corpus.
So let's use the limitization on the given words.
So as you can see here,
the limitizer has kept the words as it is this
because we haven't assigned any POS tags here
and hence it has assumed all the words as known now.
We'll learn about POS later in this video,
but just to give you a hint of what POS is POS is basically
parts of speech.
So as to define which word is a noun,
which is a pronoun and which is a subject and much more.
Now, do you know there are several words
in the English language such as I act for begin gone
no various which are thought of as useful
in the formation of sentence and without it
the sentences won't even make sense.
But these do not provide any help in NLP.
So these list of words are known as top words.
So you might be confused as if are they helpful or not.
It is helpful in the English language,
but it is not helpful in the language processing.
So NLTK has its own list of stop word.
You can use the same by importing it from NLTK dot corpus.
So once we import the stop words,
we can just have a look at the different stop words
which is provided by NLTK.
So we need to mention which language we are using.
So let's have a look at the number of stop words.
We have an English format.
So this one seventy nine.
So remember guys when we use the f-disk underscore top
10 to check the top 10 tokens
which have the highest number of frequency.
So let us take that again.
So as you can see apart from the word intelligence
and intelligent all of the words are usually stop words
or basically I should say
that they do not match any particular word.
They are like special digits and characters
which do not add any value to the language processing
and hence it can be removed.
So first of all,
we will use the compile from the RE module to create a string
that matches any digit or the special character.
Now we'll create an empty list and append the words
without any punctuation into the list.
So I'm naming this as post punctuation.
And if you have a look at the output of the post punctuation.
So as you can see it has removed all the various numbers
and digits and the comma and the different elements.
Now whenever I was talking about p.o.
s which is parts of speech now generally speaking
the grammatical type of the word the verb the noun adjective
ad rope an article indicates how a word functions in meaning
as well as grammatically within the sentence a word can have
more than one parts of speech based on the context
in which it is used.
For example,
if we take the sentence Google something on the Internet now
here you cool is used as a verb.
Although it is a proper noun.
But these are some sort of ambiguities and the difficulties
which makes the natural language understanding even much more
harder as compared to natural language generation
because once you understand the language generation
is pretty easy.
So p.o s tax are usually used to describe
whether the word is a noun an adjective a pronoun a proper
noun singular prouler verb.
Is it a symbol is it an ad verb?
So as you can see here,
we have so many tax and descriptions for the different
kinds of words all the way ranging from coordinating conjunction
to what work WRP now,
let's take an example of these two sentences.
So the first sentence is the waiter cleared the plates
from the table.
So as you can see from the starting
if we use the word taking to consideration there,
it is a Determiner now waiter is a noun cleared is a verb.
There is again Determiner the plates are noun
from is not defined here.
There is again the Determiner and the table is again a noun.
Now again,
if we take into consideration the sentence the dog ate the cat similarly,
there is the Determiner dog noun 8 is the verb
and again, it's the same Determiner and the now.
So let's see how we can implement POS tags
and do the tagging using the nltk library.
So let us take a sentence Timothy is a natural
when it comes to drawing first of all,
we'll tokenize it using the word underscore tokenize
and then we are going to use the POS underscore tag
on all of these tokens.
So as you can see it has defined Timothy as a noun is
a verb is a Determiner natural as an adjective
when as a WRB adjective and it as a preposition.
It comes as a verb to and drawing as a verb as well.
Now, let's take another example
that John is eating a delicious cake.
So as you can see here,
the Tiger has tagged both the is and eating as well
because it has considered is eating as a single term.
Now, this is one of the small shortcomings
of the POS Taggers when it comes to tagging the words.
Now another concept in natural language processing is NER
which is known as named entity recognition.
So the process of detecting the named entities
such as a movie a monetary value be it an organization
a location quantities and a person from a text is known
as named entity recognition.
Now there are three phases of any R
which is first is the noun phrase identification.
Now this step deals with extracting
all the noun phrases from a text using dependency passing
and parts of speech tag,
which is the POS tag now.
Secondly, we have the phrase classification.
Now, this is the classification step
in which all the extracted nouns and phrase are classified
into respective categories such as location names
and much more now apart from this one can curate the lookup tables
and dictionaries by combining information
from the different sources
and finally we have the entity disambiguation.
Now sometimes it is possible
that the entities are misclassified
hence creating a validation layer
on top of the result is very useful.
The use of knowledge graphs can be exploited for this purpose.
Now the popular knowledge graphs
are the Google knowledge graph the IBM Watson
and also Wikipedia.
So if we have a look at a certain example suppose,
this is the sentence the Google CEO
Sundar Pichai introduced a new pixel
at Minnesota Roy Center event.
So as you can see Google is an organization
Sundar Pichai is a person Minnesota is a location
and Roy Center event is also an organization.
This is an additional layer on top of the POS tagging.
So as to clarify and give us more depth
into what the sentence is about
and what the sentence is conveying us.
So for using any R in Python,
you need to import the any underscore chunk
from the nltk module in Python.
So once we have imported any underscore chunk now,
let us take this sentence into consideration,
which is the US President stays in the White House.
Now again, what we need to do is tokenize this sentence
and after tokenizing what we need to do is add the POS tags.
So that it will be easier for us to conclude the any ours.
So we are passing the any underscore tags
into any underscore chunks function.
And let's see what's the output.
So you can see it has given the asset to minor.
It has given us as an organization
and White House is clubbed together as a single entity
and just considered as a facility.
So as you can see adding a layer of dictionary
and then adding the tags
and then creating the name entity recognition makes
the understanding of language so much more easier.
Now as you can see in the any our entity list,
we have geo social political group in geo political entity.
We have facility location organization and person.
So as you saw just from the previous example,
as you can see if we have given organization.
We have facility now an important thing to consider
in NLP is also the syntax.
Now any linguistics syntax is the set of rules principles
and the processes that govern the structure
of a sentence in a given language.
The term syntax is also used to refer to the study
of such principles and processes.
So we have a certain rules as to what part of sentence
should come up at what position now with these rules.
We create a syntax tree whenever there is a sentence
as an input so syntax tree in layman terms is basically
a tree representation of the syntactic structure
of sentences or strings.
Now it is a way of representing the syntax
of a programming language as a hierarchical tree like structure
and this structure is used
for generating symbol tables for compilers
and later code generation.
The tree represents all the constructs in the language
and their subsequent rules.
Now consider the statement the cat sat on a mat.
So as you can see this is how a syntax tree looks like.
We have a sentence.
We have the noun phrase the preposition phrase
and again the noun phrase is divided into article and noun.
Then we have the verb which is sat again.
We have the preposition which is on and again.
Finally, we have the noun phrase
which consists of article and the noun now
in order to render syntax trees in your notebook.
You need to install ghost strips
which is a rendering engine.
So you can download ghost strip from this download page.
I'll not go much into the details of that.
Now, let us discuss an important concept with respect
to analyzing the sentence structure,
which is chunking now chunking basically means
picking up individual pieces of information
and grouping them into bigger pieces.
So as we saw earlier that we have a sentence
and we divided it into different tokens
that was tokenization
and this is sort of like the opposite part
or we should say the opposite of what we were known
as tokenization a little bit of changes
to that tokenization part.
We'll see what are the different changes now?
The bigger pieces are usually known as chunks here.
Now in the context of NLP chunking means grouping of words
or the tokens into chunks.
Now, let's have a look at the example of chunking here.
So let's consider the statement we caught the pink panther.
So as you can see here the pink
which is an adjective panther,
which is a noun and the which is the determiner
are chunked together into a noun phrase.
Now, this also helps in determining
and the processing of the language.
So suppose if someone is asking whom did we caught this morning?
So the regular response to this question would be
we caught the pink panther now the question is asking who
so the pink panther becomes a noun phrase basically.
So this is something what the chunking does
is that it understands the language
and once it has understood what it does is basically picks up
the individual pieces of information and groups them
into chunks so that it will be easier for us
to process that data.
So let's take a new sentence the big cat 8 little mouse
who was after the fresh cheese.
So let's tokenize it and add the pus tags also.
So as you can see just under one line or I should say
and one command I have tokenized it
and I've added the pus tags also.
So let's define the grammar here.
So next what we are going to do is create a regular expression
parser for the grammar NP,
which is the grammar which we have provided
and then we'll create the chunk underscore result
and we'll pass this whole sentence
or I should say the tokens into the chunks.
There was a slider as nltk was unable to file the GS file,
but as you can see here,
we have a tree in which the NP is the big cat.
The big cat is a noun phrase it is a verb tree.
Then again,
we have the noun phrase which is the little mouse
who is a word phrase was is a verb after is in
and again we have the noun phrase the fresh cheese.
So you can see here,
although we do not have a particular tree like structure.
We can see that from the outer we can see we have a tree starting
from the start is here and the end is here.
And you can see it starts with the noun phrase the big cat.
Now the big cat as I mentioned earlier in our previous example,
similarly to the deep pink panther.
It has been taken into consideration as a noun phrase,
which is basically a chunk
and it has created different chunks
in the given sentence.
I should say so this is it guys.
I hope you got an idea of what the different parts of NLP is.
What is NLP?
How it is used the different elements of NLP such as starting
from tokenization stop words stemming limitization again.
We have the stop words part of speech POS tags any are named
entity recognition and after any are we had the syntax.
We created a syntax tree to know how exactly the sentence
is arranged and how it uses the dictionary the POS tags
and the tokenization to create a particular tree
so as to form a meaning out of that sentence.
And finally we are using chunking as to chunk together
the small tokens into meaningful words.
So guys, I hope you like this session
and I'm sure you guys will start implementing these procedures
on the given text using the nltc library.
And let me tell you one thing guys the nltc library is filled
with lots and lots of text data and lots and lots of example.
This is just the beginning of it and you can dig deeper
into NLP and go into topics like context free grammar
and much more which is already there in the nltc package
and is very useful when it comes to natural language processing.
Thank you.

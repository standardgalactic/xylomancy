In this course, you will learn all about natural language processing and how to apply it to
real-world problems using the Spacey Library.
Dr. Mattingly is extremely knowledgeable in this area, and he's an excellent teacher.
Hi, and welcome to this video.
My name is Dr. William Mattingly, and I specialize in multilingual natural language processing.
I come to NLP from a humanities perspective.
I have my PhD in medieval history.
But I use Spacey on a regular basis to do all of my NLP needs.
So what you're going to get out of this video over the next few hours is a basic understanding
of what natural language processing is, or NLP, and also how to apply it to domain-specific
problems, or problems that exist within your own area of expertise.
I happen to use this all the time to analyze historical documents or financial documents
for my own personal investments.
Over the next few hours, you're going to learn a lot about NLP, language as a whole,
and most importantly, the Spacey Library.
I like the Spacey Library because it's easy to use and easy to also implement really kind
of general solutions to general problems with the off-the-shelf models that are already
available to you.
I'm going to walk you through, in part one of this video series, how to get the most
out of Spacey with these off-the-shelf features.
In part two, we're going to start tackling some of the features that don't exist in
off-the-shelf models, and I'm going to show you how to use rules-based pipes, or components
in Spacey, to actually solve domain-specific problems in your own area, from the entity
ruler to the matcher to actually injecting robust, complex, regular expression, or rejects
patterns in a custom Spacey component that doesn't actually exist at the moment.
I'm going to be showing you all that in part two, so that in part three, we can take the
lessons that we learned in part one and part two, and actually apply them to solve a very
kind of common problem that exists in an LP, and that is information extraction from financial
documents.
So finding things that are of relevance, such as stocks, markets, indexes, and stock exchanges.
If you join me over the next few hours, you will leave this lesson with a good understanding
of Spacey, and also a good understanding of kind of the off-the-shelf components that
are there, and a way to take the off-the-shelf components and apply them to your own domain.
If you also join me in this video and you like it, please let me know in the comments
down below, because I am interested in making a second part to this video that will explore
not only the rules-based aspects of Spacey, but the machine learning-based aspects of
Spacey.
So teaching you how to train your own models to do your own things, such as training a
dependency parser, training a named entity recognizer, things like this, which are not
covered in this video.
Nevertheless, if you join me for this one and you like it, you will find part two much
easier to understand.
So sit back, relax, and let's jump into what NLP is, what kind of things you can do with
NLP, such as information extraction, and what the Spacey library is, and how this course
will be laid out.
If you liked this video, also consider subscribing to my channel, Python Tutorials for Digital
Humanities, which is linked in the description down below.
Even if you're not a digital humanist like me, you will find these Python tutorials useful
because they take Python and make it accessible to students of all levels, specifically those
who are beginners.
I walk you through not only the basics of Python, but also I walk you through step-by-step
some of the more common libraries that you need.
A lot of the channel deals with texts or text-based problems, but other content deals with things
like machine learning and image classification and OCR, all in Python.
So before we begin with Spacey, I think we should spend a little bit of time talking
about what NLP or natural language processing actually is.
Natural language processing is the process by which we try to get a computer system to
understand and parse and extract human language, often times with raw text.
There are a couple different areas of natural language processing.
There is named entity recognition, part of speech tagging, syntactic parsing, text categorization,
also known as text classification, co-reference resolution, machine translation.
Adjacent to NLP is another kind of computational linguistics field called natural language
understanding, or NLU.
This is where we train computer systems to do things like relation extraction, semantic
parsing, question and answering, summarization, sentiment analysis, and paraphrasing.
NLP and NLU are used by a wide array of industries, from finance industry all the way through
to law and academia, with researchers trying to do information extraction from texts.
Within NLP, there are a couple different applications.
The first and probably the most important is information extraction.
This is the process by which we try to get a computer system to extract information that
we find relevant to our own research or needs.
So for example, as we're going to see in part three of this video, when we apply spacey
to the financial sector, a person interested in finances might need NLP to go through and
extract things like company names, stocks, indexes.
Things that are referenced within maybe news articles, from Reuters to New York Times to
Wall Street Journal.
This is an example of using NLP to extract information.
A good way to think about NLP's application in this area is it takes in some unstructured
data, in this case raw text, and extracts structured data from it, or metadata.
So it finds the things that you want it to find and extracts them for you.
Now while there's ways to do this with gazetteers and list matching, using an NLP framework
like spacey, which I'll talk about in just a second, has certain advantages.
The main one being that you can use and leverage things that have been parsed syntactically
or semantically.
So things like the part of speech of a word, things like its dependencies, things like
its co-reference.
These are things that the spacey framework allow for you to do off the shelf and also
train into machine learning models and work into pipelines with rules.
So that's kind of one aspect of NLP and one way it's used.
Another way it's used is to read in data and classify it.
This is known as text categorization and we see that on the left hand side of this image.
Text categorization or text classification, and we conclude in this sentiment analysis
for the most part as well, is a way we take information into a computer system, again unstructured
data, a raw text, and we classify it in some way.
You've actually seen this at work for many decades now with spam detection.
Spam detection is nearly perfect.
It needs to be continually updated, but for the most part it is a solved problem.
The reason why you have emails that automatically go to your spam folder is because there's
a machine learning model that sits on the background of your, on the back end of your
email server.
And what it does is it actually looks at the emails, it sees if it fits the pattern for
what it's seen as spam before, and it assigns it a spam label.
This is known as classification.
This is also used by researchers, especially in the legal industry.
Lawyers oftentimes receive hundreds of thousands of documents, if not millions of documents.
They don't necessarily have the human time to go through and analyze every single document
verbatim.
It is important to kind of get a quick umbrella sense of the documents without actually having
to go through and read them page by page.
And so what lawyers will oftentimes do is use NLP to do classification and information
extraction.
They will find keywords that are relevant to their case, or they will find documents
that are classified according to the relevant fields of their case.
And that way they can take a million documents and reduce it down to maybe only a handful,
maybe a thousand that they have to read verbatim.
This is a real world application of NLP or natural language processing, and both of these
tasks can be achieved through the SPACI framework.
SPACI is a framework for doing NLP.
Right now, as of 2021, it's only available, I believe, in Python.
I think there is a community that's working on an application with R, but I don't know
that for certain.
But SPACI is one of many NLP frameworks that Python has available.
If you're interested in looking at all of them, you can explore things like NLTK, the
natural language toolkit, stanza, which I believe is coming out of the same program
at Stanford.
There's many out there.
But I find SPACI to be the best of all of them for a couple different reasons.
Reason one is that they provide for you off-the-shelf models that benchmark very well, meaning they
perform very quickly, and they also have very good accuracy metrics, such as precision,
recall, and f-score.
And I'm not going to talk too much about the way we measure machine learning accuracy
right now, but know that they are quite good.
Second, SPACI has the ability to leverage current natural language processing methods,
specifically, transformer models, also known usually kind of collectively as BERT models,
even though that's not entirely accurate.
But it allows for you to use an off-the-shelf transformer model.
And third, it provides the framework for doing custom training relatively easily compared
to these other NLP frameworks that are out there.
Finally, the fourth reason why I picked SPACI over other NLP frameworks is because it scales
well.
SPACI was designed by Explosion AI, and the entire purpose of SPACI is to work at scale.
By at scale, we mean working with large quantities of documents efficiently, effectively, and
accurately.
SPACI scales well because it can process hundreds of thousands of documents with relative ease
in a relative short period of time, especially if you stick with more rules-based pipes,
which we're going to talk about in part two of this video.
So those are the two things you really need to know about NLP and SPACI in general.
We're going to talk about SPACI in-depth as we explore it both through this video and
in the free textbook I provide to go along with this video, which is located at spacy.pythonhumanities.com,
and it should be linked in the description down below.
This video and the textbook are meant to work in tandem.
Some stuff that I cover in the video might not necessarily be in the textbook because
it doesn't lend itself well to text representation, and the same goes for the opposite.
Some stuff that I don't have the time to cover verbatim in this video I cover in a
little bit more depth in the book.
I think that you should try to use both of these.
What I would recommend is doing one pass through this whole video, watch it in its entirety,
and get an umbrella sense of everything that SPACI can do and everything that we're going
to cover.
I would then go back and try to replicate each stage of this process on a separate window
or on a separate screen and try to kind of follow along in code.
And then I would go back through a third time and try to watch the first part where I talk
about what we're going to be doing and try to do it on your own without looking at the
textbook or the video.
If you can do that by your third pass, you'll be in very good shape to start using SPACI
to solve your own domain specific problems.
NLP is a complex field, and applying NLP is really complex, but fortunately frameworks
like SPACI make this project and this process a lot easier.
I encourage you to spend a few hours in this video, get to know SPACI, and I think you're
going to find that you can do things that you didn't think possible in relative short
order.
So sit back, relax, and enjoy this video series on SPACI.
In order to use SPACI, you're first going to have to install SPACI.
Now there's a few different ways to do this depending on your environment and your operating
system.
I recommend going to SPACI.io backslash usage and kind of enter in the correct framework
that you're working with.
So if you're using Mac OS versus Windows versus Linux, you can go through and in this very
handy kind of user interface, you can go through and select the different features that matter
most to you.
I'm working with Windows, I'm going to be using PIP in this case, and I'm going to be
doing everything on the CPU and I'm going to be working with English.
So I've established all of those different parameters, and it goes through and it tells
me exactly how to go through and install it using PIP in the terminal.
So I encourage you to go through pause the video right now, go ahead and install Windows
however you want to, I'm going to be walking through how to install it within the Jupyter
notebook that we're going to be moving to in just a second.
I want you to not work with the GPU at all.
Working with Spacey on the GPU requires a lot more understanding about what the GPU
is used for, specifically in training machine learning models.
It requires you to have CUDA installed correctly.
It requires a couple other things that I don't really have the time to get into in this video,
but we'll be addressing in a more advanced Spacey tutorial video.
So for right now, I recommend selecting your OS, selecting either going to use PIP or Kanda,
and then selecting CPU and since you're going to be working through this video with English
texts, I encourage you to select English right now and go ahead and just install or download
the Encore Web SM model.
This is the small model.
I'll talk about that in just a second.
So the first thing we're going to do in our Jupyter notebook is we are going to be using
the exclamation mark to delineate in the cell that this is a terminal command.
We're going to say PIP install Spacey.
Your output when you execute this cell is going to look a little different than mine.
I already have Spacey installed in this environment.
And so mine kind of goes through and looks like this yours will actually go through and
instead of saying requirement already satisfied, it'll be actually passing out the the different
things that it's actually installing to install Spacey and all of its dependencies.
The next thing that you're going to do is you're going to again, you follow the instructions
and you're going to be doing Python dash M space Spacey space download and then the model
that you want to download.
So let's go ahead and do that right now.
So let's go ahead and say Python M Spacey download.
So this is a Spacey terminal command and we're going to download the Encore Web SM.
And again, I already have this model downloaded.
So on my end, Spacey is going to look a little differently than as it's going to look on your
end as it prints off on the Jupyter Notebook.
And if we give it a just a second, everything will go through and it says that it's collected
it, it's downloading it and we are all very happy now.
And so now that we've got Spacey installed correctly and that we've got the small model
downloaded correctly, we can go ahead and start actually using Spacey and make sure
everything's correct.
The first thing we're going to do is we're going to import the Spacey library as you
would with any other Python library.
If you're not familiar with this, a library is simply a set of classes and functions that
you can import into a Python script so that you don't have to write a whole bunch of extra
code.
Libraries are massive collections of classes and functions that you can call.
So when we import Spacey, we're importing the whole library of Spacey.
And now that we've seen something like this, we know that Spacey has imported correctly,
as long as you're not getting an error message, everything was imported fine.
The next thing that we need to do is we want to make sure that our English Core Web SM,
our small English model, was downloaded correctly.
So the next thing that we need to do is we need to create an NLP object.
I'm going to be talking a lot more about this as we move forward right now.
This is just troubleshooting to make sure that we've installed Spacey correctly and
we've downloaded our model correctly.
So we're going to use the Spacey.load command.
This is going to take one argument.
It's going to be a string that is going to correspond to the model that you've installed.
In this case, N Core Web SM.
And if you execute this cell and you have no errors, you have successfully installed
Spacey correctly and you've downloaded the English Core Web SM model correctly.
So go ahead, take time and get all this stuff set up, pause the video if you need to and
then pop back and we're going to start actually working through the basics of Spacey.
I'm now going to move into kind of an overview of kind of what's within Spacey, why it's
useful and kind of some of the basic features of it that you need to be familiar with.
And I'm going to be working from the Jupyter Notebook that I talked about in the introduction
to this video.
If we scroll down to the bottom of chapter one, the basics of Spacey and you get past
the install section, you get to this section on containers.
So what are containers?
Well, containers within Spacey are objects that contain a large quantity of data about
a text.
There are several different containers that you can work with in Spacey.
There's the doc, the doc bin, example, language, lexeme, span, span group and token.
We're going to be dealing with the lexeme a little bit in this video series and we're
going to be dealing with the language container a little bit in this video series, but really
the three big things that we're going to be talking about again and again is the doc,
the span and the token.
And I think when you first come to Spacey, there's a little bit of a learning curve about
what these things are, what they do, how they are structured hierarchically.
And for that reason, I've created this, in my opinion, kind of easy to understand image
of what different containers are.
So if you think about what Spacey is as a pyramid, so a hierarchical system, we've
got all these different containers structured around really the doc object.
Your doc container or your doc object contains a whole bunch of metadata about the text
that you pass to the Spacey pipeline, which we're going to see in practice.
In just a few minutes, the doc object contains a bunch of different things.
It contains attributes.
These attributes can be things like sentences.
So if you iterate over doc.sense, you can actually access all the different sentences
found within that doc object.
If you iterate over each individual item or index in your doc object, you can get individual
tokens.
Tokens are going to be things like words or punctuation marks.
Everything within your sentence or text that has a self contained important value, either
syntactically or semantically.
So this is going to be things like words, a comma, a period, a semi colon, a quotation
mark, things like this.
These are all going to be your tokens, and we're going to see how tokens are a little
different than just splitting words up with traditional string methods and Python.
The next thing that you should be kind of familiar with are spans.
So spans are important because they kind of exist within and without of the doc object.
So unlike the token, which is an index of the doc object, a span can be a token itself,
but it can also be a sequence of multiple tokens.
We're going to see that at play.
So imagine if you had a span in its category, maybe group one are places.
So a single token might be like a city, like Berlin, but span group two, this could be
something like full proper names.
So of a people, for example.
So this could be like, as we're going to see Martin Luther King, this would be a sequence
of tokens, a sequence of three different items in the sentence that make up one span or one
self contained item.
So Martin Luther King would be a person who's a collection of a sequence of individual tokens.
If that doesn't make sense right now, this image will be reinforced as we go through
and learn more about spacey in practice.
For right now, I want you to be just understanding that the doc object is the thing around which
all of spacey sits.
This is going to be the object that you create.
This is going to be the object that contains all the metadata that you need to access.
And this is going to be the object that you try to essentially improve with different
custom components, factories and pipelines as you go through and do more advanced things
with spacey.
We're going to now see in just a few seconds how that doc object is kind of similar to
the text itself, but how it's very, very different and much more powerful.
We're now going to be moving on to chapter two of this textbook, which is going to deal
with kind of getting used to the in depth features of spacey.
If you want to pause the video or keep this notebook or this book open up kind of separate
from this video and follow along as we go through and explore it in live coding.
We're going to be talking about a few different things as we explore chapter two.
This will be a lot longer than chapter one.
We're going to be not only importing spacey, but actually going through and loading up
a model, creating a doc object around that model so that we're going to work with the
doc container and practice.
And then we're going to see how that doc container stores a lot of different features
or metadata or attributes about the text.
And while they look the same on the surface, they're actually quite different.
So let's go ahead and work within our same Jupiter notebook where we've imported spacey
and we have already created the NLP object.
The first thing that I want to do is I want to open up a text to start working with within
this repo.
We've got a data folder within this data sub folder.
I've got a couple of different Wikipedia openings.
I've got one on MLK that we're going to be using a little later in this video and I have
one on the United States.
This is wiki underscore us.
That's going to be what we work with right now.
So let's use our with operator and open up data backslash wiki underscore us dot txt.
We're going to just read that in as f and then we're going to create this text object,
which is going to be equal to f dot read.
And now that we've got our text object created, let's go ahead and see what this looks like.
So let's print text.
Then we see that it's a standard Wikipedia article kind of follows that same introductory
format and it's about four or five paragraphs long with a lot of the features left in such
as the brackets that delineate some kind of a footnote.
We're not going to worry too much about cleaning this up right now because we're interested
not with cleaning our data so much as just starting to work with the doc object in spacey.
So the first thing that you want to do is you're going to want to create a doc object.
It is oftentimes good practice if you're only ever working with one doc object in your script
to just call your only object doc.
If you're working with multiple objects, sometimes you'll say doc one, doc two, doc
three, or give it some kind of specific name so that your variables can be unique and easily
identifiable later in your script.
Since we're just working with one doc object right now, we're going to say doc is equal
to NLP.
So this is going to call our NLP model that we imported earlier, in this case the English
Core Web SM model.
And that's going to for right now just take one argument and that's going to be the text
itself.
So the text object, if you execute that cell, you should have a doc object now created.
Let's print off that doc object and see what it looks like.
And if you scroll down, you might be thinking to yourself, this looks very, very similar
if not identical to what I just saw a second ago.
And in fact, on the surface, it is very similar to that text object that we gave to the NLP
model or pipeline.
But let's see how they're different.
Let's print off the length of text.
And let's print off the length of the doc object.
And what we have here are two different numbers.
Our text is 3525 and our doc object is 152.
What is going on here?
Well, let's get a sense by trying to iterate over the text object and iterating over the
doc object with a simple for loop.
So we're going to say for token and text, so we're going to iterate first over that
text object, we're going to print off the token.
So the first 10 indices and we get individual letters as one might expect.
But when we do something the same thing with the doc object, let's go ahead and start writing
this out.
We're going to say for token and doc, we're going to iterate over the first 10.
We're going to print off the token.
We see something very different.
What we see here are tokens.
This is why the doc object is so much more valuable and this is why the doc object has
a different length than the text object.
The text object is just basically counting up every instance of a character, a white
space, a punctuation, etc.
The doc object is counting individual tokens, so any word, any punctuation, etc.
That's why they're of different length and that's why when we print them off, we see
something different.
So you might now already be seeing the power of spacey.
It allows for you to easily on the surface with nothing else being done, easily split
up your text into individual tokens without any effort at all.
Now, those of you familiar with Python and different string methods might be thinking
to yourself, but I've got the split method.
I can just use this to split up the text.
I don't need anything fancy from spacey.
Well, you'd be wrong.
Let me demonstrate this right now.
So if I were to say for token and text dot split, so I'm splitting up that text into individual
and theory individual words, essentially, it's just a split method where it's splitting
by individual white spaces, if I were to do that and iterate over the first 10 again,
and I would just say print token, it looks good until you get down here.
So until you get to USA, well, why is it a problem?
The problem is quite simple.
There is a parentheses mark right here.
And this is where we have a huge advantage with spacey.
Spacey automatically separates out these these kind of punctuation marks and removes them
from individual tokens when they're not relevant to the token itself.
Notice that USA has got a period within the middle of it.
It's not looking at that and thinking that that is some kind of unique token, a you,
a period and s, a period and an a in a period.
It's not seeing these as four individual tokens, rather, it's automatically identifying them
as one thing, one tied together single token that's a string of characters and punctuation.
This is where the power of spacey really lies just on the surface level.
And go ahead, spend a few minutes and play around with this.
And then we're going to kind of jump back here and start talking about how the doc
object has a lot more than just tokens within it.
It's got sentences, each token has attributes.
We're going to start exploring these when you pop back.
If you're following along with the textbook, we're now going to be moving on to the next
section, which is sentence boundary detection.
In NLP, sentence boundary detection is the identification of sentences within a text.
On the surface, this might look simple.
You might be thinking to yourself, I could simply use the split function and split up
a text with a simple period.
And that's going to give me all my sentences.
Those of you who have tried to do this might already be shaking your heads and saying no.
If you think about it, there's a really easy explanation for why this doesn't work.
Were you to try to split up a text by period and make a presumption that anything that
occurs between periods is going to be an individual sentence, you would have a serious mistake
when you get to things like USA, especially in Western languages, where the punctuation
of a period mark is used not only to delineate the change of a sentence, rather it's used
to also delineate abbreviations, so United States of America.
Each period represents an abbreviated word.
You could write in rules to account for this.
You could write in rules that could also include in other ways that sentences are created,
such as question marks, such as exclamation marks, but why do that?
That's a lot of effort when the doc object in Spacey does this for you.
Let's go ahead and demonstrate exactly how that works.
Let's go ahead and say for sent in doc.sense.
Notice that we're saying doc.sense, or grabbing the sentence attribute of the doc object.
Let's print off sent.
And if you do that, you are now able to print off every individual sentence, so the entire
text has been tokenized at the sentence level.
In other words, Spacey has used its sentence boundary detection and done all that for you
and given you all the sentences.
If you work with different models of different sizes, you're going to notice that certain
models, the larger they get, tend to do better at sentence detection, and that's because
machine learning models tend to do a little bit better than heuristic approaches.
The English Core Web SM model, while having some machine learning components in it, does
not save word vectors, and so the larger you go with the models, typically the better you're
going to have with regards to sentence detection.
Let's go ahead and try to access one of these sentences.
Let's create an object called sentence one.
We're going to make that equal to doc.sense zero, so we're going to try to grab that zero
index and let's print off sentence one.
If we do this, we get an error.
Why have we gotten an error?
Well, it tells you why right here, it's a type error, and this means that this is not
a type that can be kind of iterated over, it's not subscriptable, and it's because it is
a generator.
Now in Python, if you're familiar with generators, you might be thinking to yourself, there's
a solution for this, and in fact there is.
If you want to work with generator objects, you need to convert them into a list.
So let's say sentence one is equal to list, so using the list function to convert doc.sense
into a list, and then with outside of that, we're going to grab zero, the zero index,
and then we're going to print off sentence one.
Then we grab the first sentence of that text.
This, as we go deeper and deeper in Spacey, one by one, you're going to see the immense
power that you can do with Pacea.
All the immense incredible things you can use Spacey for with very, very minimal code.
The doc object does a lot of things for you that would take hours to actually write out
in code to do with heuristic approaches.
This is now a great way to segment an entire text up by sentence.
And if you work with text a lot, you'll already know that this has a lot of applications.
As we move forward, we're going to not just talk about sentences, we're also going to
be talking about token attributes, because within the doc object are individual tokens.
I encourage you to pause here and go ahead and play around with the doc.sense a little
bit and get familiar with how it works, what it contains, and try to convert it into a
list.
Once you've done that, pop back here and we'll continue talking about tokens.
This is where I really encourage you to spend a little bit of time with the textbook.
Under token attributes in chapter two, I have all the different kind of major things that
you're going to be using with regards to token attributes.
We're going to look and see how to access them in just a second.
I've provided for you kind of the most important ones that you should probably be familiar
with.
We're going to see this in code in just a second and I'm going to explain with a little bit
more detail than what's in the spacey documentation about what these different things are, why
they're useful, and how they're used.
So let's go ahead and jump back into our Jupyter notebook and start talking about token attributes.
If you remember, the doc object had a sequence of tokens.
So for token and doc, you could print off token and let's just do this with the first
10.
And we've got each individual token.
What you don't see here is that each individual token has a bunch of metadata buried within
it.
These metadata are things that we call attributes or different things about that token that
you can access through the spacey framework.
So let's go ahead and try to do that right now.
Just work with for right now token number two, which we're going to call sentence one,
and we're going to grab from sentence one, the second index, let's print off that word.
And it should be states.
And in fact, it is fantastic.
So now that we've got the word states accessed, we can start kind of going through and playing
around with some of the attributes that that word actually has.
Now when you print it off, it looks like a regular piece of text looks like just a string,
but it's got so much more buried within it now, because it's been passed through our
NLP model or pipeline from spacey.
So let's go ahead and say token to dot text.
And I'm going to be saying token to dot text.
If you're working within an IDE, like Adam, you're going to need to say print token to
dot text.
When we do this, we see we get a string that just is states.
This is telling us that the dot text of the object, the pure text corresponds to the word
states.
This is really important if you need to extract the text itself from the token and not work
with the token object, which has behind it a whole bunch of different metadata that we're
going to go through now and start accessing.
Let's use the token left edge.
So we can say token to dot left underscore edge, and we can print that off.
Well, what's that telling us?
It's telling us that this is part of a multi word token or a token that is multiple has
multiple components to make up a larger span, and that this is the leftmost token that corresponds
to it.
So this is going to be the word the as in the United States.
Let's take a look at the right edge, we can say token to dot right underscore edge, print
that off, and we get the word America.
So we're able to see where this token fits within a larger span in this case a noun chunk,
which we're going to explore in just a few minutes.
But we also learn a lot about it, kind of the different components, so we know where
to grab it from the beginning and from the very end.
So that's how the left edge and the right edge work.
We also have within this token to dot int type, this is going to be the type of entity.
Now what you're seeing here is a integer.
So this is 384.
In order to actually know what 384 means, I encourage you to not really use that so much
as and type with an underscore after it.
This is going to give you the string corresponding to number 384.
In this case, it is G P E or geopolitical entity.
We're going to be working with named entity a little bit in this video, but I have a whole
other book on named entity recognition.
It's at any are Python humanities.com, in which I explore all of any are both machine
learning and rules based in a lot more depth.
Let's go ahead and keep on moving on though, and looking at different entity types here
as well.
So are not entity types attribute types.
So we're going to say token to dot int I O B all lowercase and again underscore at the
end.
And we get the, the string here, I now, I O B is a string.
Specific kind of a named entity code B would mean that it's the beginning of an entity
and I means that it's inside of an entity.
And O means that it's outside of an entity.
The fact that we're seeing I here tells us that this word states is inside of a larger
entity.
And in fact, we know that because we've seen the left edge and we've seen the right edge.
It's inside of the United States of America.
So it's part of a larger entity at hand.
We can also say token to dot lima.
And under case again after that, and we get the word states.
This is the lima form or the root form of the word.
This means that this is what the word looks like with no inflection.
If we were working with a verb, in fact, let's go ahead and do that right now.
Let's grab sentence.
We're going to grab sentence one, index 12, which should be the word no, and we're going
to print off the lima for the word or sorry, it's a verb.
And we see the verb lima as no.
So if we were to print off sentence one, specifically, index 12, we see that its original form is
known.
So the lima form uninflected is the verb no, K N O W.
Another thing that we can access, and we're going to see that have the power of this later
on.
This might not seem important right now, but I promise you it will be.
Let's print off token, what do they call this again, token to, we're going to print
that off, but we're going to print off specifically the morph.
No underscore here, just morph.
What you get is what looks like a really weird output, a string called noun type equal to
prop.
In fact, this means proper noun, a number, which corresponds to sing.
We're going to talk a lot more about morphological analysis later on, we try to find an extract
information from our texts.
But for right now, understand that what you're looking at is the output of kind of what that
word is morphologically.
So in this case, it's a proper noun and it's singular.
If we were to do take this sentence 12 again, and do morph, we'd find out what kind of verb
it is.
So it's a perfect past participle, known perfect past participle.
Remember, being good at NLP is also being good with language.
So I encourage you to spend time and start getting familiar with those things that you
might have forgotten about from like fifth grade grammar, such as perfect participles
and things like that.
Because when you need to start creating rules to extract information, you're going to find
those pieces of information very important for writing rules.
We'll talk about that in a little bit though.
Let's go back to our other attributes from the token.
So again, let's go to token two, and we're going to grab the POS part of speech, not
what you might be thinking.
So part of speech underscore, POS underscore, and we output PROPN.
This means that it is a proper noun.
It's more of a simpler kind of grammatical extraction as opposed to this morphological
detailed extraction, what kind of noun it might be with regards to, in this case, singular.
So that's going to be how you extract the part of speech.
The thing that you can do is you can extract the dependency relation.
So in this case, we can figure out what role it plays in the sentence.
In this case, the noun subject.
And then finally, the last thing I really want to talk about before we move into a more
detailed analysis of part of speech is going to be the token two dot lane.
And what this grabs for you is the language of the doc object.
In this case, we're working with something from the English language, so EN.
Every language is going to have two letters that correspond to it.
These are universally recognized.
So that's going to be how you access different kinds of attributes that each token has.
And there's about 20 more of these, or maybe not 20, maybe about 15 more of these that
I haven't covered.
I gave you the ones that are the most important that I find to be used on a regular basis
to solve different problems with regards to information extraction from the text.
So that's going to be where we stop here with token attributes.
And we're going to be moving on to part 2.5 of the book, which is part of speech tagging.
I now want to move into kind of a more detailed analysis of part of speech within spacey and
the dependency parser, and how to actually analyze it really nicely, either in a notebook
or outside of a notebook.
So let's work with a different text for just a few minutes.
We're going to see why this is important.
It's because I'm working on a zoomed in screen.
And to make this sentence a little easier to understand, we're going to just use Mike
and Joy's plain football, a very simple sentence.
And we're going to create a new doc object, and we're going to call this doc 2.
That's going to be equal to NLP text.
Let's print off doc 2 just to make sure that it was created, and in fact, that we see that
it was.
Now that we've got it created, let's iterate over the tokens within this and say for token
in text, we want to print off token dot text.
We want to see what the text actually is.
We want to see the token dot POS, and the token dot DEP helps if you actually iterate
over the correct object over the doc to object.
And we see that we've got Mike proper noun, noun, subject and joy's verb.
It's the root plane.
In this case, it's a verb.
And then we've got football, the noun, the direct object, and a period, which is the
punctuation.
So we can see the basic semantics of the sentence at play.
What's really nice from spacey is we have a way to really visualize this information
and how these words relate to one another.
So we can say from spacey import, displacey, and we're going to do displacey, displacey
dot render.
And this is going to take two arguments, it's going to be the text, and then it's going
to be the, actually, it's going to be doc to, and then it's going to be style.
In this case, we're going to be working with the EP, and we're going to print that off.
And we actually see how that sentence is structured.
Now in the textbook, I use a more complicated sentence.
But for the reasons of this video, I've kept it a little shorter, just because I think
it displays better on this screen, because you can see that this becomes a little bit
more difficult to understand when you're zoomed in.
But this is one sentence from that Wikipedia article.
So go ahead and look at the textbook and see how elaborate this is.
You can see how it's part of a compound, how it's preposition.
You can see the more fine-grained aspects of the dependency parser and the part of
speech tagger really at play with more complicated sentences.
So that's going to be how you really access part of speech and how you can start to visualize
how words in a sentence are connected to other words in the sentence with regards to their
part of speech and their dependencies.
That's going to be where we stop with that.
In the next section, we're going to be talking about named entity recognition and how to
visualize that information.
So named entity recognition is a very common NLP task.
It's part of kind of data extraction or information extraction from texts.
It's oftentimes just called NER, named entity recognition.
I have a whole book on how to do NER with Python and with Spacey.
But we're not going to be talking about all the ins and outs right now.
We're just going to be talking about how to access the pieces of information throughout
kind of our text.
And then we're going to be dealing with a lot of NER as we try to create elaborate
systems to do named entity extraction for things like financial analysis.
So let's go ahead and figure out how to iterate over a doc object.
So we're going to say for int and doc.n.
So we're going to go back to that original doc, the one that's got the first kind of,
the text from Wikipedia on the United States.
We're going to say print off int.text, so the text from it, and int.label.
Label underscore here.
That's going to tell us what label corresponds to that text.
And we print this off.
We've got a lot of GPEs, which are geopolitical entities.
North America, this isn't a geopolitical entity, it's just a general location.
50, a cardinal number, five cardinal number.
NORP, Indian in this case, which is a national or religious political entity.
Quantity, the number of miles.
Canada, GPE as you would expect.
Paleo Indians, NORP, once again, Siberia, Locke.
And we have date being extracted, so at least 12,000 years ago.
This is a small model, and it's extracting for us a lot of very important structured data.
But we can see that the small model makes mistakes.
So the Revolutionary War is being considered an organization.
Were I to use a large model right now, which I can download separately from Spacey,
and we're going to be seeing this later in this video.
Or were I to use the much larger transformer model.
This would be correctly identified, most likely as an event, not as an organization.
But because this is a small model that doesn't contain word vectors,
which we're going to talk about in just a little bit,
it does not generalize or make predictions well on this particular data.
Nevertheless, we do see really good extraction here.
We have the Americans of a war being extracted as an event.
We have the Spanish American War, even with this encoding typographical error here.
And World War being extracted as an event, World War II event, Cold War event.
All of this is looking good.
And not really, I only saw a couple basic mistakes.
But for the most part, this is what you'd expect to see.
We even see percentages extracted correctly here.
So this is how you access really vital information about your tokens,
but more importantly about the entities found within your text.
And also, Displacy offers a really nice way to visualize this in a Jupyter notebook.
We can say displacy.render, we can say doc, style, we can say int.
And we get this really nice visualization where each entity has its own particular color.
So you can see where these entities appear within the text as you kind of just naturally read it.
You can do this with the text as long as you want.
You can even change the max length to be more than a million characters long.
And again, we can see right here, org is incorrectly identified as the American Revolutionary War and correctly identified as org.
But nevertheless, we see really, really good results with a small English model without a lot of custom fine-tuned training.
And there's a reason for this.
A lot of Wikipedia data gets included into machine learning models.
So machine learning models on text typically make good predictions on Wikipedia data because it was included in their training process.
Nevertheless, these are still good results.
If I'm right or wrong on that, I'm not entirely certain.
But that's going to be how you kind of extract important entities from your text and most importantly visualize it.
This is where chapter two of my book kind of ends.
After this chapter, you have a good understanding, hopefully, of kind of what the doc container is, what tokens are, and how the doc object contains the attributes such as since and ends,
which allows for you to find sentences and entities within a text.
Hopefully you also have a good understanding of how to access the linguistic features of each token through token attributes.
I encourage you to spend a lot of time becoming familiar with these basics as these basics are the building block for really robust things that we're going to be getting into in the next few lessons.
We're now moving into chapter three of our textbook on Spacey and Python.
Now, in chapter three, we're going to be continuing our theme of part one, where we're trying to understand the larger building blocks of Spacey.
Even though this video is not going to deal with Spacey machine learning approaches are custom ones, that is, it's still important to be familiar with what machine learning is and how it works,
specifically with regards to language, because a lot of the Spacey models such as the medium, large, and transformer models, all are machine learning models that have word vectors stored within them.
This means that they're going to be larger, more accurate, and do things a bit more slowly, depending upon its size.
So we're going to be working through not only what kind of machine learning is generally, but specifically how to how it works with regards to texts.
And I think that this is where you're going to find this textbook to be somewhat helpful.
So what I want to do is in our new Jupyter Notebook, we're going to import Spacey just as we did before, but this time we're going to be installing a new model.
So we're going to do Python M, the exclamation mark Python M Spacey download.
And then we're going to download the Ncore Web MD model.
So this is the medium English model.
This is going to take a little longer to download.
And the reason why I'm having you download the medium model and the reason why we're going to be using the medium model is because the medium model has stored within it word vectors.
Well, that's downloading.
Let's go ahead and talk a little bit about what word vectors are and how they're useful.
So word vectors are word embeddings.
So these are numerical representations of words in multi-dimensional space through matrices.
That's a very compacted sentence.
So let's break it down.
What are word vectors used for?
Well, they're used for a computer system to understand what a word actually means.
So computers can't really parse text all that efficiently.
They can't parse it at all.
Every word needs to be converted into some kind of a number.
Now, for some old approaches, you would use something like a bag of words approach where each individual word would have a corresponding number to it.
This would be a unique number that corresponds just to that word.
For a lot of tasks that can work, but for something like text understanding or trying to get a computer system to be able to understand how a word functions within a sentence in general,
in other words, how it works in the language, how it relates to all other words, that doesn't really work for us.
So what a word vector is, is it's a multi-dimensional representation.
So instead of a number having just a single integer that corresponds to it, it instead has what looks like to an unsuspecting I, essentially,
it has a very complex sequence of floating numbers that are stored as an array,
which is a computationally less expensive form of a list in Python or just computing in general.
And this is what it looks like, a long sequence.
In this case, I believe it's a 300-dimensional word that corresponds to a specific word.
So this is what an array or a word vector or a word embedding looks like.
What this means to a computer system is it means syntactical and semantical meaning.
So the way word vectors are typically trained is, oh, there's a few different approaches,
but kind of the old-school word-to-vec approach is you give a computer system a whole bunch of texts,
and different smaller, larger collections of texts.
And what it does is it reads through all of them and figures out how words are used in relation to other words.
And so what it's able to essentially do through this training process is figure out meaning.
And what that meaning allows for a computer system to do is understand how a word might relate to other words
within a sentence or within a language as a whole.
And in order to understand this, I think it's best if we move away from this textbook
and actually try to explore what word vectors look like in spacey.
So you can have a better sense of specifically what they do, why they're useful,
and how you, as a NLP practitioner, can go ahead and start leveraging them.
So just like before, we're going to create an NLP object.
This time, however, instead of loading in our Encore Web SM, we're going to load in our Encore Web MD.
So the one that actually has these word vectors stored, these static vectors saved,
and it's going to be a larger model, let's go ahead and execute that cell.
And while that's executing, we're going to start opening up our text.
So we're going to say with open data wiki underscore us dot txt,
r as f, we're going to say text is equal to f dot read.
So we're going to successfully load in that text file and open it up.
Then we're going to create our doc object, which will be equal to NLP text.
All the syntax is staying the exact same.
And just like before, let's grab the first sentence.
So we're going to convert our doc dot since generator into a list,
and we're going to grab index zero.
And let's go ahead and print off sentence one just so you can kind of see it.
And there it is.
So now that we've got that kind of in memory, we can start kind of working with it a little bit.
So let's go ahead and just start tackling how we can actually use word vectors with spacey.
So let's kind of think about a general question right now.
Let's say I wanted to know how the word, let's say country,
is similar to other words within our model's word embeddings.
So let's create a little way we can do this.
We're going to say your word, and this is going to be equal to the word country, country.
There we go.
And what we can do is we can say MS is equal to NLP.
So we're going to go into that NLP object.
We're going to grab the vocab dot vectors, and we're going to say most similar.
And this is a little complicated way of doing it.
In fact, I'm going to go ahead and just kind of copy and paste this in.
You have the code already in your textbook that you can follow along with.
And I'm going to go ahead and just copy and paste it in right here.
And print off this.
And what this is going to do is it is going to go ahead and just do this entirely.
There we go.
And we have to import numpy as MP.
This lets us actually work with the data as a numpy array.
And when we execute this cell, what we get is an output that tells us all the words that are most similar to the word country.
So in this scenario, the word country, it has these kind of all these different similar words to it from the word country to the word country, capitalized nation, nation.
Now, it's important to understand what you're seeing here.
What you're seeing is not necessarily a synonym for the word country.
Rather, what you're seeing is are the words that are the most similar.
Now, this can be anything from a synonym to a variant spelling of that word to something that occurs frequently alongside of it.
So for example, world, while this isn't the same, we would never consider world to be the synonym of country.
But what happens is is syntactically they're used in very similar situations.
So the way you describe a country is sometimes the way you would describe your world, or maybe it's something to do with the hierarchy.
So a country is found within the world.
This is a good way to understand it.
So it's always good to use this word as most similar, not to be something like synonym.
So when you're talking about word vectors similarity, you're not talking about synonym similarity.
Keep that in mind.
But this is a way you can kind of quickly get a sense.
So what does this do for you?
Why did I go through and explain all these things about word vectors?
If I'm not going to be talking about machine learning a whole bunch throughout this video?
Well, I did it so that you can do one thing that's really important.
And that's calculate document similarity in the spacey.
So we've already got our NLP model loaded up.
Let's create one object.
So we're going to make doc one, we're going to make that equal to NLP.
And we're going to create the text right here in this object.
So let's say this is coming straight from the spacey documentation.
I like salty fries and hamburgers.
And we're going to say doc two is equal to NLP.
And this is going to be the text fast food tastes very good.
And now what we can do is let's go ahead and load those into memory.
What we can do is we can actually make a calculation using spacey to find out how similar they actually are these two different sentences.
So we can say print off doc one.
And we're going to say this again, this is coming straight from the spacey documentation doc two.
So you're going to be able to see what both documents are.
And then we're going to do doc one dot similarity.
So we can go into the doc one dot similarity method and we can compare it to doc two.
We can print that off.
So what we're seeing here on the left is document one, this little divider thing that we printed off here.
On the right, we have document two.
And then we can see the degree of similarity between document one and document two.
Let's create another doc object.
We're going to call this NLP doc three and we're going to make this NLP.
Let's come up with a sentence that's completely different.
The Empire State Building is in New York.
So this is the one I'm just making up off the top of my head right now.
I'm going to copy and paste this down.
And we're going to compare this to doc one.
We're going to compare it to doc three and we get a score of point five one.
So this is less similar to then these two.
So this is a way that you can take a whole bunch of documents.
You can create a simple for loop and you can find and start clustering the documents that have a lot of overlap or similarity.
How is this similarity being calculated?
Well, it's being calculated because what spacey is doing is it's going into its word embeddings.
And even though in these two situations, we're not using the word fast food ever in this document.
It's going in and it knows that salty fries and hamburgers are probably in a close cluster with the biogram or a token that's made up of two words, a biogram of fast food.
So what it's doing is it's assigning a prediction that these two are still somewhat similar, more similar than these two because of these overlapping in words.
So let's try one more example. See if we can get something that's really, really close.
So let's take doc four and this is going to be equal to NLP.
I enjoy oranges.
And then we're going to have doc five is going to be equal to NLP.
I enjoy apples.
So to, I would agree, I would argue very, very syntactically similar sentences.
And we're going to do doc four here, doc five here, and we're going to look and see a similarity between doc four and doc five.
And if we execute this, we get a similarity of 0.96.
So this is really high.
This is telling me that these two sentences are very similar.
And it's not just that they're similar because of the similar syntax here.
That's definitely pushing the number up.
It's that what the individual is liking in the scenario between these two texts, they're both fruits.
Let's try something different. Let's make doc five.
Let's just make doc six here and do something like this NLP.
I enjoy.
What's another word we could say something that's different.
Let's say burgers something different from a fruit.
So we're going to make doc six like that.
And we're going to again copy and paste this down.
Copy and paste this down.
We're going to put doc six here.
And we see this drop.
So what this demonstrates, and I'm really glad this worked because I improvised this.
What this demonstrates is that the similarity, the number that's given is not dependent on the contextual words.
Rather, it's dependent upon the semantic similarity of the words.
So apples and oranges are in a similar cluster around fruit because of their word embeddings.
The word burgers while still being food and still being plural is different from apples and oranges.
So in other words, this similarity is being calculated based on something that we humans would calculate.
Difference in meaning based on a large understanding of a language as a whole.
That's where word vectors really come into play.
This allows for you to calculate other things as well.
So you could even calculate the difference between salty fries and hamburgers, for example.
I've got this example ready to go in the textbook.
Let's go ahead and try this as well.
We're going to grab doc one and print off these few things right here.
So we're going to try to calculate the similarity between french fries and burgers.
And what we get is a similarity of 0.73.
So if we were to maybe change this up a little bit and try to calculate the similarity between maybe just the word burgers rather than hamburgers and hamburgers,
we would have a much higher similarity.
So my point is, is play around with the similarity calculator.
Play around with the structure of the code I've provided here and get familiar with how spacey can help you kind of find similarity,
not just between documents, but between words as well.
And we're going to be seeing how this is useful later on.
But again, it's good to be familiar with kind of generally how machine learning kind of functions here in this context and why these medium and large models are so much bigger.
They're so much bigger because they have more word vectors that are much deeper.
And the transformer model is much larger because it was trained in a completely different method than the way the medium and large models were trained.
But again, that's out of the scope for this video.
I now want to turn to the really the last subject of this introduction to spacey part one,
which is when we're taking this large umbrella view of spacey.
And in the textbook, it's going to correspond to chapter four.
So what we go over in this textbook is kind of a large view of not just the doc container and the word vectors and the linguistic annotations,
but really kind of the structure of the spacey framework, which comes around the pipeline.
So a pipeline is a very common expression in computer science and in data science.
Think of it as a traditional pipeline that you would see in a house.
Now think of a pipeline being a sequence of different pipes.
Each pipe in a computer system is going to perform some kind of permeation or some action on a piece of data as it goes through the pipeline.
And as each pipe has a chance to act and make changes to and additions to that data,
the later pipes get to benefit from those changes.
So this is very common when you're thinking about logic of code.
I provide it like a little image here that I think maybe might help you.
So if we imagine some input sentence, right?
So some input text is entering a spacey pipeline.
It's going to go through a bunch of things if you're working with the medium model or the small model that will tokenize it and give it a word of vector for different words.
It'll also find the POS, the part of speech, the dependency parser will act on it.
But it'll might eventually get to an entity ruler, which we're going to see in just a few minutes.
The entity ruler will be a series of rules based NER named entity recognition.
So it'll maybe assign a token to an entity.
Might be the beginning of an entity, might be the end of an entity, might just be an individual token entity.
And then what will happen is that doc object as it kind of goes through this pipeline will now receive a bunch of doc.ins.
So it'll be this pipe will actually add to the doc object as it goes through the pipeline, the entity component.
And then the next pipeline, the entity linker, might take all those entities and try to find out which ones they are.
So it'll oftentimes be connected to some kind of wiki data, some kind of standardized number that corresponds to a specific person.
So for example, if you were seeing a bunch of things like Paul something, Paul something, maybe that one Paul something might be Paul Hollywood from the Great British Bake Off.
And it might have to make a connection to a specific person.
So if it's the word Paul being used generally, this entity linker would assign it to Paul Hollywood, depending on the context.
That's out of the scope of this video series, but keep in mind that that pipe would do something else that would modify the ints that would give them greater specificity.
And then what you'd be left with is the doc object on the output that not only has entities annotated, but it's also got entities linked to some generic specific data.
So that's going to be how a pipeline works. And this is really what spacey is.
It's a sequence of pipes that act on your data.
And that's important to understand, because it means that as you add things to a spacey pipeline, you need to be very conscientious about where they're outed and in what order.
As we're going to see as we move over to kind of rules based spacey when we start talking about these different types, the entity ruler, the matcher custom components, regex components, you're going to need to know which order to put them in.
It's going to be very important.
So do please keep that in mind.
Now, spacey has a bunch of different attribute rulers or different pipes that you can kind of add into it.
You've got dependency parsers that are going to come standard with all of your models.
You've got the entity linker entity recognizer entity ruler.
You're going to have to make these yourself and add them in oftentimes.
You've got a limitizer. This is going to be on most of your standard models.
Your morphologue, that's going to be on on there as well, sentence recognizer, sentence iser.
This is what allow for you to have the doc.sense right here span categorizer.
This will help categorize different spans, be them single token spans or sequence of token spans, your tagger.
This will tag the different things in your text, which will help with part of speech, your text categorizer.
This is when you train a machine learning model to recognize different categories of a text.
So text classification, which is a very important machine learning task.
Token to VEC, this is going to be what assigns word embeddings to the different words in your doc object.
Tokenizer is what breaks that thing up and all your text into individual tokens.
And you got things like transformer and trainable pipes.
Then within this, you've also got some other things called matchers.
So you can do some dependency matching.
We're not going to get into that in this video.
You've also got the ability to use matcher and phrase matcher.
These are a lot of the times can do some similar things, but they're executed a little differently to make things less confusing.
I'm really only talking about the matcher of these two.
And if there's a need for it, I'll add into the textbook the phrase matcher at a later date, but I'm not going to cover it in this video.
And if I do add in the phrase matcher, it's going to be after this matcher section here.
I have it in the GitHub repo.
I just haven't included in the textbook to keep things a little bit simpler, at least if you're just starting out.
So a big good question is, well, how do you add pipes to a spacey pipeline?
So let's go ahead and do that.
We're going to make a blank spacey pipeline right now.
Let's go ahead and just make.
We'll just work with the same live coding notebook that we have open right now.
So what we're going to do is we're going to make a blank model and we're going to actually add in our own sentence.
Sizer to our to our text.
So let's go ahead and do that.
So I'm going to say nlp is equal to a spacey dot blank.
This is going to allow for me to make a blank spacey pipeline.
And I'm going to say in so that it knows that the tokenizer that I need to use is the English tokenizer.
And now if I want to add a pipe to that, I can use one of the built in spacey features.
So I can say add underscore pipe, and I can say sentence iser so I can add in a sentence iser.
This is going to allow for me to create a pipeline now that has a sequence of two different pipes.
And I demonstrate in the textbook why this is important.
Sometimes what you need to do is you need to just only break down a text into individual sentences.
So I grabbed a massive, massive corpus from the Internet, which is on MIT.edu.
And it's the entire Shakespeare corpus.
And I just try to calculate the quantity of sentences found within it.
There are 94,133 sentences.
And it took me only 7.54 seconds to actually go through and count those sentences with the spacey model.
Using the small model, however, it took a total amount of time of 47 minutes to actually break down all those sentences and extract them.
Why is there a difference in time between 7 seconds and 47 minutes?
It's because that the spacey small model has a bunch of other pipes in it that are trying to do a bunch of other things.
If you just need to do one task, it's always a good idea to just activate one pipe,
or maybe make a blank model and just add that single pipe or the only pipes that you need to it.
A great example of this is needing to tokenize a whole bunch of sentences in relatively short time.
So I don't know about you, but I'd be much happier with 7 seconds versus 47 minutes.
However, comes at a trade-off.
This small model is going to be more accurate in how it finds sentence boundaries.
And so we have a difference in quantity here.
This difference in quantity indicates that this one messed up and made some mistakes because it was just the sentenceizer.
The sentenceizer didn't have extra data being fed to it.
In fact, if I probably used larger models, I might even have better results.
But always think about that.
If time is of the essence and you don't care so much about accuracy,
a great way to get the quantity of sentences or at least a ballpark is to use this method where you simply add in a sentenceizer to a blank model.
So that's how you actually add in different pipes to a spacey pipeline.
And we're going to be reinforcing that skill as we go through,
especially in part two, where we really kind of work with this in a lot of detail.
Right now, I'm just interested in giving you the general understanding of how this might work.
So let's go ahead and try to analyze our pipeline.
So we can do analyze underscore pipes.
We can analyze what our analyze.
There we go.
We can actually analyze our pipeline.
If we look at the NLP object, which is our blank model with the sentenceizer,
we see that our NLP pipeline ignore summary, ignore this bit here.
But what you're actually able to kind of go through and see right away is that we've really just got the sentenceizer sitting in it.
If we were to analyze a much more robust pipeline,
so let's create NLP two is equal to spacey dot load and core web SM.
We're going to create that NLP two object around the small spacey English model.
We can analyze the pipes again, and we see a much more elaborate pipeline.
So what are we looking at?
Well, what we're looking at is the sequence of things we've got in the pipeline, a tagger after the talk to VEC.
We've got a tagger, a parser.
We keep on going down.
We've got an attribute ruler.
We've got a limitizer.
We've got the NER.
That's what it's assigned the doc dot ends.
And we keep on going down.
We can see the limitizer, but we can see also a whole bunch of other things.
We can see what these different things actually assign.
So doc dot ends is assigns the NER and require.
And we can also see what each pipe might actually require.
So if we look up here, we see that the NER pipe, so the name to the recognition pipe is responsible for assigning the doc dot ends.
So that attribute of the doc object.
And it's also responsible at the token level for assigning the end dot IOB underscore IOB, which is the, if you remember from a few minutes ago when we talked about the IOB being the opening beginning or out beginning inside for a different entity.
It also assigns the end dot end underscore type for each token attribute.
So you can see a lot of different things about your pipeline by using NLP dot analyze underscore pipes.
If you've gotten to this point in the video, then I think you should by now have a good really umbrella view of what space he is, how it works, why it's useful.
And some of the basic features that it can do and how it can solve some pretty complex problems with some pretty simple lines of code.
What we're going to see now moving forward is how you as a practitioner of NLP cannot just take what's given to you with spacey, but start working with it and start leveraging it for your own uses.
So taking what is already available.
So like these models like the English model and adding to them contributing to them.
Maybe you want to make an entity ruler where you can find more entities in a text based on some cousin tier or list that you have.
Maybe you want to make a matcher so you can find specific sequences within a text.
Maybe that's important for information extraction.
Maybe you need to add custom functions or components into your spacey pipeline.
I'm going to be going through in part two rules based spacey and giving you all the basics of how to do some really robust custom things relatively quickly with within the spacey framework.
All of that's going to lay the groundwork so that in part three we can start applying all these skills and start solving some real world problems.
In this case we're going to look at financial analysis.
So that's going to be where we move to next is part two.
We are now moving into part two of this Jupiter book on spacey and we're going to be working with rules based spacey.
Now this is really kind of the bread and butter of this video.
You've gotten a sense of the umbrella structure of spacey as a framework.
You've gotten a sense of what the dot container can contain.
You've gotten a sense of the token attributes and the linguistic annotations from part one of this book in the earlier part of this video.
Now we're going to move into taking those skills and really developing them into custom components and modified pipes that exist within spacey.
In other words I'm going to show you how to take what we've learned now and start really doing more robust and sophisticated things with that knowledge.
So we're going to be working first with the entity ruler then with the matcher in the next chapter then in the components in spacey.
So a custom component is a custom function that you can put into a pipeline.
Then we're going to talk about regex or regular expressions and then we're going to talk about some advanced regex with spacey.
If you don't know what regex is I'm going to cover this in chapter eight.
So let's go over to our Jupyter notebook that we're going to be using for our entity ruler lesson.
So let's go ahead and execute some of these cells and then I'm going to be talking about it in just a second.
First I want to take some time to explain what the entity ruler is as a pipe in spacey what it's used for why you'd find it useful and when to actually implement it.
So there are two different ways in which you can kind of add in custom features to a spacey language pipeline.
There is a rules based approach and a machine learning based approach.
Rules based approaches should be used when you can think about how to generate a set of rules based on either a list of known things or a set of rules that can be generated through regex code or linguistic features.
Machine learning is when you don't know how to come like to actually write out the rules or the rules that you would need to write out would be exceptionally complicated.
A great example of a rules based approach versus a machine learning based approach and when to use them is with entity types for named entity recognition.
Imagine if you wanted to extract dates from a from a text.
There are a finite very finite number of ways that a date can appear in a text.
You could have something like January one 2005 you could have one January 2005 you could have one Jan 2005 you could have one slash five slash 2005 there's there's different ways that you can do this and there's a lot of them.
But there really is a finite number that you could easily write a regex expression for a regular expression for to capture all of those.
And in fact those regex expressions already exist.
That's why spacey is already really good at identifying dates.
So dates are something that you would probably use a rules based approach for something that's a good machine learning approach for or something like names.
If you wanted to capture the names of people you would have to generate an entity ruler with a whole bunch of robust features.
So you would have to have a list of all known possible first names.
All known possible last names.
All known possible prefixes like doctor Mr. and Mrs. Mrs.
Master etc.
And you'd have to have a list of all known suffixes so junior senior the third the fourth etc on the list.
This would be very very difficult to write because first of all the quantity of names that exist in the world are massive.
The quantity of last names that exist in the world is massive.
There's not a set gazetteer or set list out there of these anywhere.
So for this reason oftentimes things like people names will be worked into machine learning components.
I'm going to address machine learning in another video at a later date.
But right now we're going to focus on a rules based approach.
So using the rules based features that spacey offers.
A good NLP practitioner will be excellent at both rules based approaches and machine learning based approaches.
And knowing when to use which approach and when maybe maybe a task is not appropriate for machine learning when it can be worked in with rules relatively well.
If you're taking a rules based approach the approach that you take should have a high degree of confidence that the rules will always return true positives.
And you need to think about that.
If you are okay with your rules maybe catching a few false positives or missing a few true positives then maybe think about how you write the rules and allowing for those and making it known in your documentation.
So that's generally what a rules based approach is.
And an entity ruler is a way that we can use a list or a series of features language features to add tokens into the entity the dot ints container within the dot container.
So let's go ahead and try to do this right now.
The text we're going to be working with is a kind of fun one I think.
So if you've already gotten the reference congratulations it's kind of obscure.
We're going to have a sentence right here that I just wrote out West Chesterton Fieldville was referenced in Mr. Deeds.
So in this context we are going to have a few different entities.
We want our model or our pipeline to extract West Chesterton Fieldville as a GPE.
It's a fake place that doesn't really exist.
It was made up in the movie Mr. Deeds.
And what we want is for Mr. Deeds to be grabbed as an entity as well.
And this would ideally be labeled as a film.
But in this case that's probably not going to happen.
So let's go ahead and see what does happen.
So we're going to say for int and doc dot ints print off int dot text and dot label like we learned from our NER lesson a few few moments ago.
And we see that the output looks like this.
It's gotten almost all the entities that we wanted.
Mr. was left off of Deeds and it's grabbed the West Chesterton Fieldville and labeled it as a person.
So what's gone wrong here?
Well, there's a few different things that have gone wrong.
The NCORE Web SM model is a machine learning model for NER.
The word vectors are not saved.
So the static vectors are not in it.
So it's making the best prediction that it can.
But even with a very robust machine learning model, unless it has seen West Chesterton Fieldville, there is not really a good way for the model to actually know that that's a place.
Unless it's seen a structure like West Chesterton, and maybe it can make up a guess, a transformer model might actually get this right.
But for the most part, this is a very challenging thing.
This would be challenging for a human.
There's not a lot of context here to tell you what this kind of entity is.
Unless you knew a lot about how maybe northeastern villages and towns in North America would be called.
Also, Mr. Deeds is not extracted as a whole entity, just Deeds is.
Now ideally, we would have an NER model that would label West Chesterton Fieldville as a GPE and Mr. Deeds as a film.
But we've got two problems.
One, the machine learning model doesn't have film as an entity type.
And on top of that, West Chesterton Fieldville is not coming out correct as GPE.
So our goal right now is to fix both of these problems with an entity ruler.
This would be useful if I were maybe doing some text analysis on fictional places referenced in films.
So things like Narnia, maybe Middle Earth, West Chesterton Fieldville, these would all be classified as kind of fictional places.
So let's go ahead and make a ruler to correct this problem.
So what we're going to do is first we're going to make a ruler by saying ruler is equal to NLP dot add pipe.
And this is going to take one argument here.
You're going to find out when we start working with custom components that you can have a few different arguments here,
especially if you create your own custom components.
But for right now, we're working with the components that come standard with Spacey.
There's about 18 of them.
One of them is the entity underscore ruler, all lowercase.
We're going to add that ruler into our NLP model.
And if we do NLP dot analyze underscore pipes and execute that, we can now look at our NER model and see as we go down that the NER pipe is here.
And the entity ruler is now the exit, the final pipe in our pipeline.
So we see that it has been successfully added.
Let's go ahead now and try to add patterns into that pipeline.
Patterns are the things that the Spacey model is going to look for in the label that it's going to assign when it finds something that meets that pattern.
This will always be a list of lists.
So let's go ahead and do this right now.
Sorry, a list of dictionaries.
So the first pattern that we're really looking for here is going to be a dictionary.
It's going to have one key of label, which is going to be equal to GPE and another label of pattern, which is going to be equal to, in this case, we want to find West Chesterton Fieldville.
Let me go ahead and just copy and paste it so I don't make a mistake here.
And what we want to do is we want our entity ruler to see West Chesterton Fieldville and when it sees it, assign the label of GPE.
So it's a geopolitical entity, so it's a place.
So let's go ahead and execute that.
Great.
We've got the patterns.
Now comes time to load them into the ruler.
So we can say ruler dot add underscore patterns.
This is going to take one argument.
It's going to be our list of patterns added in.
Cool.
Now let's create a new doc object.
We're going to call this doc two.
That's going to be equal to NLP.
We're going to pass in that same text.
We're going to say for int and doc two dot ints print off int dot text and end dot label.
And you're going to notice that nothing has changed.
So why has nothing changed?
We're still getting the same results.
And we've added the correct pattern in.
The answer lies into one key thing.
If we look back up here, we see that our entity ruler comes after our NER.
What does that mean?
Well, imagine how the pipeline works that I talked about a little while ago in this video.
A pipeline works by different components, adding things to an object and making changes to it.
In this case, adding ints to it and then making those things isolated from later pipes
from being able to overwrite them unless specified.
What this means is that when West Chesterton Fieldville goes through and is identified by the NER pipe as a person,
it can no longer be identified as anything else.
What this means is that you need to do one of two things.
Give your ruler the ability to overwrite the NER or, and this is my personal preference,
put it before the NER and the pipeline.
So let's go through and solve this common problem right now.
We're going to create a new NLP object called NLP2, which is going to be equal to spacey.load.
And again, we're going to load in the English core web SM's model and core web SM.
Great.
And again, we're going to do ruler.nlp2.addpipe entity ruler.
And we're going to make that an object too.
Now what we can do is we can say ruler.addPatterns.
Again, we're going to go through all of these steps that we just went through.
We're going to add in those patterns that we created up above.
And now what we're going to do is we're going to actually do one thing a little different
than what we did.
What we're going to do is we're going to load this up again, and we're going to do
an extra keyword argument.
Now we can say either after or before here.
We're going to say before NER.
What this is going to do is it's going to place our NER before our entity ruler before the NER component.
And now when we add our patterns in, we can now create a new doc object.
Doc is going to be equal to nlp2 text.
And we're going to say for int and doc.ints printoff int.text int.label.
And now we notice that it is correctly labeled as a GPE.
Why is this?
Well, let's take a look at our nlp2 object.
I'm going to analyze pipes.
And if we scroll down, we will notice that our entity ruler now in the pipeline sits before the NER model.
In other words, we've given primacy to our custom entity ruler so that it's going to have the first shot
at actually correctly identifying these things.
But we've got another problem here.
Deeds is coming out as a person.
It should be Mr. Deeds as the entire collective multi-word token.
And that should be a new entity.
We can use the entity ruler to add in custom types of labels here.
So let's go ahead and do this same thing.
Let's go ahead and just copy and paste our patterns.
And we're going to create one more nlp object.
We're going to call this nlp3 is equal to spacey dot load in core web sm.
Great, we've got that loaded up.
We're going to do the same thing we did last time nlp3 or sorry, ruler is equal to nlp dot add underscore pipe entity ruler.
We're going to place it.
Remember going to place it before the nr pipe nlp3.
There we go.
And what we need to do now is we need to copy in these patterns and we're going to add in one more pattern.
Remember this can be a list here.
So this pattern we're going to have a new label called film and we're going to look for the sequence Mr. Deeds.
And that's going to be our pattern that we want to add in to our ruler.
So we can do ruler dot add underscore patterns.
And we're going to add in patterns.
Remember that one keyword argument or one argument is going to be the list itself.
And now we can create a new doc object, which is going to be equal to nlp3.
I think I called it.
Yep.
Text.
And we can say for int and doc dot ints printoff int dot text and int dot label.
And if we execute this, we see now that not only have you gotten the entity ruler to correctly identify West Chesterton Fieldville,
we've also gotten the entity ruler to identify correctly Mr. Deeds as a film.
Now some of you might be realizing the problem here.
This is actually a problem for machine learning models.
And the reason for this is because Mr. Deeds in some instances could be the person and Mr. Deeds in other instances could be the movie itself.
This is what we would call a toponym.
So it's spelled like this.
And this is a common problem in natural language processing.
And it's actually one of the few problems or one of many problems really that remain a little bit unsolved toponym resolution.
It's spelled like this or TR is the resolution of toponym.
So things that can have multiple labels that are dependent upon context.
Another example of toponym resolution is something like this.
If you were to look at this word and let's say let's ignore Paris Hilton.
Let's ignore Paris from Greek mythology.
Let's say it's only going to ever be a GPE.
The word Paris could refer to Paris, France, Paris, Kentucky or Paris, Texas.
Toponym resolution is also the ability to resolve problems like this.
When in context is Paris was kind of talking about Paris, France.
When in context is it talking about Kentucky and when in context is it talking about Texas.
So that's something that you really want to think about when you're generating your rules for an entity ruler.
Is this ever going to be a false positive?
And if the answer is that it's going to be a false positive half the time or it's a 50-50 shot,
then really consider incorporating that kind of an entity into a machine learning model
by giving it examples of both Mr. Deeds in this case as a film and Mr. Deeds as a person.
So we can learn with word embeddings when that context means it's a film
and when that context means it's a person.
That's just a little toy example.
What we're going to see moving forward though, and we're going to do this with a matcher,
not with the entity ruler, is that Spacey can do a lot of things.
You might be thinking to yourself, now I could easily just come up with a list
and just check and see whenever Mr. Deeds pops up and just inject that into the doc.ins.
I could do the same thing with West Chesterton Field Bill.
Why do I need an NLP framework to do this?
And the answer is going to come up in just a few minutes when we start realizing
that Spacey can do a lot more than things like regex
or things like just a basic gazetteer check or a list check.
What you can do with Spacey is you can have the pattern not just take a sequence of characters
and look for a match, but a sequence of linguistic features as well
that earlier pipes have identified.
And I think it's best if we save that for just a second when we start talking about the matcher,
which is, in my opinion, one of the more robust things that you can do with Spacey
and what sets Spacey apart from things like regex
or other fancier string matching approaches.
Okay, we're now moving into chapter six of this book,
and this is really kind of, in my opinion, one of the most important areas in this entire video.
If you can master the techniques I'm going to show you for the next maybe 20 minutes or so,
maybe 30 minutes, you're going to be able to do a lot with Spacey
and you're really going to see really kind of its true power.
A lot of the stuff that we talk about here in the matcher
can also be implemented in the entity ruler as well with a pattern.
The key difference between the entity ruler and the matcher
is in how data the data is kind of extracted.
So the matcher is going to store information a little differently.
It's going to store it within the vocab of the NLP model.
It's going to store it as a unique identifier or a lexeme,
spelt lex, e-m-e. I'm going to talk about that more in just a second.
And it's not going to store it in the doc ends.
So matchers don't put things in your doc.ends.
So when do you want to use a matcher over an entity ruler?
You want to use the entity ruler when the thing that you're trying to extract
is something that is important to have a label that corresponds
to it within the entities that are coming out.
So in my research I use this for anything from let's say stocks,
if I'm working with finances, I'll use this for if I'm working with Holocaust data
at the USHMM where I am a postdoc.
I'll try to add in camps and ghettos because those are all important
annotated alongside other entities.
I'll also work in things like ships, so the names of ships, streets,
things like that. When I use the matcher, it's when I'm looking for
something that is not necessarily an entity type,
but something that is a structure within the text
that'll help me extract information. And I think that'll make more sense
as we go through and I show you kind of how to improve examples
using the matcher as you would in the real world.
But remember all the patterns that I show you can also be implemented in
the entity ruler. And I'm also going to talk about when we get to chapter 8
how regex can actually be used to do similar things
but in a different way. Essentially when you want to use the matcher or the entity
ruler over regex is when linguistic components,
so the lemma of a word or the identifying
if the word is a specific type of an entity, that's when you're going
to want to use the matcher over regex. And when you want to use
regex is when you really have a complicated pattern that you need to
extract and that pattern is not dependent upon
specific parts of speech. You're going to see how that works as we kind of
go through the rest of part 2, but keep that in the back of your mind.
So let's go ahead and take our work over to our blank
Jupyter notebook again. So what we're going to do is we're going to just set up with a basic example.
We're going to import spacey and since we're working with the matcher
we also need to say from spacey dot matcher
import matcher with a capital M, very important capital M.
Once we have this loaded up we can start actually working with the
matcher. And we're going to be putting the matcher
in a, just a small english model.
And we're going to say nlp is equal to spacey dot load and you should be getting
familiar with this n core web sm, the small
english model. Once we've got that loaded and we do now, we can
start actually working with the matcher. So how do you create the matcher?
Well, the pythonic way to do this and the way it's in the documentation is to call the object a matcher.
That's going to be equal to matcher with a capital M. So we're calling
this class right here. And now what we need to do
is we need to pass in one argument. This is going to be nlp dot
go cab. We're going to see that we can add in some extra features here in just a little
bit. I'm going to show you why you'd want to add in extra features at this stage, but we're going to
ignore that for right now. What we're going to try to do is we're going to try to find
email addresses within a text, a very simple task that's
really not that difficult to do. We can do it with a very simple pattern
because spacey has given us that ability. So let's create a pattern
and that's going to be equal to a list
which is going to contain a dictionary.
The first item in the dictionary, or the first key
is going to be the
thing that you're looking for. So in this case we have a bunch of different things
that the matcher can look for, and I'm going to be talking about all those in just a second,
but one of them is very handily this label of like
email. So if the string or the sequence of tokens or the token
is looking like an email and that's true
then that is what we want to extract.
We want to extract everything that looks like an email and to make sure that this occurs
we're going to say matcher.add and then here
we're going to pass in two arguments. Argument one is going to be the
think of it as a label that we want to assign to it
and this is what's going to be added into the nlp.vocab
as a lexeme which we'll see in just a second and the next thing is
a pattern and it's important here to note
that this is a list. The argument here
takes a list of lists and because this is just one list right now
I'm making it into a list so each one of these different patterns
would be a list within a list essentially.
Let's go ahead and execute that and now we're going to say
match is equal to nlp
and I'm going to add in a text that I have in the textbook
and this is my email address wmaddingley.aol.com
that might be a real email address I don't believe it is it's definitely not mine
so don't try and email it and then we're going to say matches is equal
to matcher.doc and this is going to be how we find our
matches we pass that doc object into our
matcher class and now what we have
is the ability to print off our matches and what we get is a list
and this list is a set of tuples that will always have
three indices so index zero is going to be
this very long number what this is is this is a lexeme
spelt like this lexeme it's in the textbook
and the next thing is the start token and the end token
so you might be seeing the importance here already what we can do with this
is we can actually go into the nlp vocab where this
integer lies and find what it corresponds to so this is
pretty cool check this out so you print off nlp
dot vocab so we're going into that vocab object
we're going to index it it matches zero
this is going to be the first index so this tuple at this point
and then we're going to grab index zero so now we've gone into
this list we've gone to index zero this first
tuple and now we're grabbing that first item there now what we need
to do is we need to say dot text
need to do it right here and if you print this off
we get this email address that label that we gave it up there
was added into the nlp vocab with this unique lexeme
that allows for us to understand what that number corresponds to within the nlp
framework so this is a
very simple example of how a matcher works and
how you can use it to do some pretty cool things but let's take a moment
let's pause and let's see what we can do with this matcher
so if we go up into spacey's documentation on the matcher
we'll see that you got a couple different attributes you can work with
we're going to be seeing this a little bit the orth this is the exact verbatim
of a token and we're also going to see text
the exact verbatim text of a token what we also have is lower
so what you can do here is you can use lower to say
when the item is lowercase and it looks like and then give some lowercase
pattern this is going to be very useful for capturing
things that might be at the start of a sentence for example if you were to look
for the penguin in the text anywhere you saw
the penguin if you used a pattern that was just lowercase
you wouldn't catch the penguin being at the start of a sentence
it would miss it because the t would be capitalized by using lower you can
ensure that your pattern that you're giving it is going to be looking for any pattern that matches
that when the text is lowercase length is going
to be the the length of your token text
is alpha is ASCII is digit this is when your characters are
either going to be alphabetical ASCII characters so the American
standard coding initiative I can't remember what it stands for but it's that
I think it's 128 bit thing that America came up with when they started
in coding text it's now replaced with UTF-8 and
is digit is going to look for something if it is a digit so think of each of these as a
token so if the token is a digit then that counts in the pattern
is lower is upper is title these should be all self-explanatory if it's
lowercase if it's uppercase if it's a title so capitalized and
if you don't understand what all these do right now I'm going to be going through and showing you in just
a second just giving you an overview of different things that can be included within
the the matcher or the entity ruler here
so what we can also do is find something that if the token is actually
the start of a sentence if it's like a number like a URL
like an email you can extract it and here is the main part I want to
talk about because this is where you're really going to find spacey outshines
any other string matching system out there so what you
can do is you can use the tokens part of speech tag morphological
analysis dependency label lemma and shape to actually
make matches so not just matching a sequence of characters but matching
a sequence of linguistic features so think about this if you wanted
to capture all instances of a proper noun followed by a verb
you would not be able to do that with regex there's not a way
to do it you can't give regex if this is a verb regex is just a string
matching framework it's not a framework for actually identifying linguistic features
using them and extracting them so this is where we can leverage all the
power of spacey's earlier pipes the tagger the morphological analysis
the depth of lemma etc so the
limitizer we can actually use all those things that have gone through the pipeline
and the matcha can leverage those linguistic features and make some
really cool allow us to make really cool patterns that can match
really robust and complicated things and the final thing I'm going to talk about
is right here the OP this is the operator or quantifier
it determines how often to match a token so there's a few different things you can use here
there's the exclamation mark negate the pattern requiring it to match
zero times so in this scenario the sequence would never occur
there's the question mark make the pattern optional allowing it to match zero
or one times require the pattern to match one or more
times with the plus and the asterisk the thing on the shift eight
allow the pattern to match zero or more times there's other things as well
that you can do to make this match a bit more robust but for right now let's jump into
the basics and see how we can really kind of take these and apply them
in a real world question so what I'm going to do is I'm
going to work with another data set or another piece of data
that I've grabbed off of Wikipedia and this is the Wikipedia article entry
on Martin Luther King Jr. it's the opening opening few paragraphs
let's print it off and just take a quick look and this
is what it looks like you can go through and read it we're not too concerned about what it says
right now we're concerned about trying to extract a very specific set of patterns
what we're interested in grabbing are all proper
nouns that's the task ahead of us somebody has asked us to take this text in
extract all the proper nouns for me but we're going to do a lot more
and not just the proper nouns but we want to get multi-word token so we
want to have Martin Luther King Jr. extracted as one
token so one export so the other things that we want to
have are these kind of structured in sequential order
so find out where they appear and extract them based on their
start token so let's go ahead and start trying to do some of these things right
now let's scroll down here great so we've
we need to create really a new nlp object now at this point so let's create
a new one bring again we're just going to start working with the n core
web sm model if you're working with a different model like the
large or the the transformer you're going to have more accurate results
but for right now we're just trying to do this quickly for demonstration purposes
so again just like before we're creating that with nlp.bocab
and then we're going to create a pattern
so this is the pattern that we're going to work with we want to find any occurrence
of a POS part of speech that corresponds
to proper noun that's the way
in which POS labels proper nouns is prop in
and we should be able to with that extract all proper nouns so you can say
matcher.add and we're going to say proper noun
and that's going to be our pattern
and then what we can do just like before we're going to create the doc object
this is going to be nlp text and then we're going to say
matches is equal to matcher doc so we're going to
create the matches by passing that doc object
into our matcher class and then we're going to print off the length
of the matches so how many matches were found and then we're going to say from match
in matches and we're just going to grab the first 10 because I've done this and there's
a lot and you'll see why let's print off
let's print off in this case a match
and then we're going to print off specifically what that text is remember
the output is the lexeme followed by the start token
and the end token which means we can go into the doc object and we can
set up something like this we can say match
1 so index 1 which is the start token and match
2 which is the end token and that'll allow us to actually index
what these words are and when we do this we can see all these printed out
so this is the match the lexeme here which is going to be
the proper noun all the way down we've got the 0
here which corresponds to the start token the end token
and this is the the token that we extracted Martin Luther King junior
Michael King junior we've got a problem here right
so the problem should be pretty obvious right now and the problem
is that we have grabbed all
proper nouns but these proper nouns are just individual tokens
we haven't grabbed the multi word tokens so how do we go about
doing that well we can solve this problem by let's go ahead and
just copy and paste all this from here and we're going to make one small
adjustment here we're going to change this
to OP with a plus
so what does that mean well let's pop back into our
matcher under spacey and check it out so OP
is the operator or quantifier we're going to use the plus symbol
so it's going to look for a proper noun that occurs
one or more times so in
theory right this should allow us to grab multi word tokens it's going to look
for a proper noun and grab as many as there are so anything that occurs one or more
times if we run this though we see a problem we've gotten
Martin we got Martin Luther what we got Luther what we got Martin Luther King
Luther King King Martin Luther King Junior what what is going on here
well you might already have figured it out it has done exactly what we told it
to do it's grabbed all sequence of tokens
that were proper nouns that occurred one or more times just so happen some
of these overlap so token that's doc 0 to 1
0 to 2 so you can see the problem here
as it's grabbing all of these in any combination of them what we can do though
is we can add an extra layer to this so let's again
copy what we've just done because it was it was almost there it was good
but it wasn't great we're going to do one new thing here
when we add in the patterns we're going to pass in the keyword argument greedy
we're going to say longest capital all capital
letters here and if we execute that it's going to look for the longest token
out of that mix and it's going to give that one make that one the
only token that it extracts we notice that our length has changed from
what was it up here 175 to 61
so this is much better however we should have recognized
right now another problem what have we done wrong
well what we've done wrong is these are all out of order
in fact what happens is when you do this I don't have evidence to support this but
I believe it's right what will always happen is the
greedy longest will result in all of your tokens being organized
are all your matches being organized from longest to shortest
so if we were to scroll down the list and look at maybe negative one
negative let's do negative 10 on you'll see
single word tokens and again this is me just guessing but I think based on
what you've just seen that's a fairly good guess so let's go ahead and just kind
of so we can see what the output is here so how would you go about organizing
these sequentially well this is where really kind of a
sort comes in handy when you can pass
a lambda to it let's go back and copy all this again because again
we almost had this right here we're going to sort our matches
though we can say matches dot sort and this
is going to take a keyword argument of key which is going to equal to lamb
and lamb is going to allow us to actually iterate over all this
and find any instance where X occurs
and we're going to say to sort by X one so what this is
it's a list of tuples and what we're using lambda for is we're going to say sort this whole
list of tuples out but sort it by the first index in other
words sort it by the start token and we execute that
we've got everything now coming out as we would expect and nor these
typos that exist we've got 0 to 4
to 9 so we actually are extracting these things in
sequential order as they appear in our text so that's how you can actually go through
and sort the appearance of the
matcher but what if our the person who kind of gave us this job
they were happy with this but they came back and said okay that's
cool but what we're really interested in what we really want to know is every
instance where a proper noun of any length
grab the multi word token still but we want to know anytime that occurs
after a verb so anytime that this proper noun is followed by a verb
so what we can do is we can add in okay we can do this we're going to have
a comma here so the same pattern is going to be a sequence now it's not just
going to be one thing we're going to say token one
needs to be a proper noun and grab as many of those tokens as you can
one to more times and then after those are done
comma this is where the next thing has to occur
POS so the part of speech needs to be a verb so the next
thing that comes out needs to be a verb and we want that to be
the case well when we do this we can kind of go through and see the results
so the first instance of this where a proper noun is
proceeded by a verb comes in token 50 to 52
king advanced 258 director J. Edgar Hoover
considered now we're able to use those linguistic
features that make spacey amazing and actually extract
some vital information so we've been able to figure out
where in this text a proper noun
is proceeded by a verb so you can already start to probably see the
implications here and we can create very elaborate things
with this we can use any of these as long of a sequence as you can imagine
we're going to work with a different text and kind of demonstrate that it's a fun
toy example I've got a halfway cleaned copy
of Alice in Wonderland stored as a Jason file
I'm going to load it in right now and then I'm going to just grab
the first sentence from the first chapter
and what we have here is the first sentence so here's our
scenario somebody has asked us to grab all the quotation
marks and try to identify the person described
or the person described the person who's doing the speaking or the
thinking in other words we want to be able to grab Alice thought now I picked
Alice in Wonderland because of the complexity of the text
not complexity in the sense of the language used to children's book
but complexity and the syntax these syntax is highly
inconsistent CS I'm not CS Lewis Carol Lewis C Carol
was highly inconsistent in how we structured these kind of sequences
of quotes and the other thing I chose to do as I left in one mistake here
and that is this non-standardized quotation mark so remember
when you need to do this things need to batch perfectly so we're going to replace this
first things first is to create a cleaner text or we do text
equals text dot replace and we're going to replace the instance of I believe
it's that mark but let's just copy and paste it in to make sure
we can place that with a with a single quotation mark
now we can print off text just to make sure that that was done correctly
cool great it was it's now looking good remember whenever you're doing information
extraction standardize the texts as much as possible
things like quotation marks will always throw off your data
now that we've got that let's go ahead and start
trying to create a fairly robust pattern to try
to grab all instances where there is a quotation mark
thought something like this
and then followed by another quotation mark so the first thing I'm going to try and do
is I'm going to try to just capture all quotation marks in a text
so let's go through and try to figure out how to do that right now
so we're going to copy in a lot of the same things that we used up above
but we're going to make some modifications to it let's go ahead and copy and paste
all that we're going to completely change our pattern
so let's get rid of this so what are we looking for well first of all
the first thing that's going to occur in this pattern is this
quotation mark so that's going to be a full text match
which is an or if you remember and we're going to have to use double
quotation marks to add in that single quotation mark so that's what we grab first
we're going to look for anything that is an or and the next thing that's going to occur
after that I think this is good to probably do this now on a
line by line basis so we can keep this straight
so the next thing that's going to occur is looking for anything in between so anything
that is an alpha character we're going to just
grab it all so is alpha
and then we need to say true
but within this we need to specify how many times
that occurs because if we say is true it's just going to look at the next
token in this case and and then say that's the end that's it
that's the pattern we got it extracted but we want to grab not just and but and
what is the use of a everything
so we need to grab not only that but when you say
OP so our operator again and if you said
plus you would be right here we need to make sure that it's a plus sign so it's grabbing
everything now in this scenario this is a common construct
is when you have a injection here in the middle of the sentence so thought
or said and it's the character doing it it's oftentimes got a comma
right here so we need to add in that kind of a feature so
there could be is punked there could be
a punked here and we're going to say that that
is equal to true but that might not always be the case
there might not always be one there so we're going to say OP
is equal to a star if we go back
we'll see why if we go back to our OP the star allow the
pattern to match zero or more times so in this scenario the punctuation
may or may not be there so that's the next thing
that occurs once we've got that the last thing that we need to
match is the exact same thing that we had at the start
is this ORF up here and that's our sequence
so this is going to look for anything that starts with a quotation mark has a series
of alpha characters has a punctuation like a comma
possibly and then closes the quotation marks if we execute this
we succeeded we got it we extracted
both matches from that first sentence there are no other quotation marks
in there but our task was not just to extract this information
our task was also to match who is
the speaker now we can do this in a few different ways and you're going to see why
this is such a complicated problem in just a second so let's go ahead
and do this how can we make this better
well we're going to have this occur twice but
in the middle we need to figure out when somebody is speaking so
one of the things that we can do is we can make a list let's make a list of
limitized forms of our verbs
so we're going to say let's call this speak underscore
limits it's going to be equal to a list and the first thing we're going to say
is think because we know that think is in there and say
this is the limitized form of thought instead
so what we can do now is after that occurs let's add in a new thing
we're going to be able to now add in a new pattern that we're looking for
a new quotation mark not just the end of a quotation mark
but also a sequence that'll be something like this
so it's going to be a part of speech so it's going to be a verb that occurs first
right and that's going to be a verb but more
importantly it's going to be a lemma that is
in
what did I call you speak lemmas
so let's break this down the next token
needs to be a verb and it needs to have
a limitized form that
is contained within the speak lemmas
list so if it's got that fantastic let's execute this and see
what happens we should only have one hit cool we do
so we've got that first hit and the second one hasn't appeared anymore because that
one quotation mark wasn't proceeded by a verb
let's go ahead and make some modifications so we can improve this a little bit
because we want to know not just what that person is doing
we also need to know who the speaker is so let's grab it
let's figure out who that speaker is so we can use part of speech
again another feature here we know that it's going to be a proper noun
because often times proper nouns are doing the speaking sometimes it might not
be sometimes it might be like the girl or the boy lower case
but we're going to ignore those situations for just right now so we're looking for
a proper noun but remember proper nouns as we saw just a second ago
could be multiple tokens so we're going to say
OP plus so it could be a sequence of tokens let's execute this
now we've captured Alice here as well so and is
the use and what is the use of a book thought Alice now we know who the speaker
is but this is a partial quotation this is not the whole
thing we need to grab the other quote how how will we ever do that
well we've already solved that we can copy and paste all of this
that we already have done right down here
and now we've successfully extracted
that entire quote so you might be thinking to yourself yeah we did it we can
now extract quotation marks and we can even extract
you know any instance where there's a
quote and somebody speaking not so fast let's try to
iterate over this data so we're going to say for text in data
zero two so we're going to iterate over the
the first chapter and we're going to
go ahead and let's let's do all of this
doc is going to be equal to that
sort that out
and then again we're going to be printing
out this information the same stuff I did
before just now it's going to be iterating over the whole chapter
if we let this run we've got a serious serious problem
and it doesn't actually grab
us anything nothing has been grabbed successfully
what is going on
we've got a problem and that problem
stems from the fact that our patterns
and the problem is that we don't have
our text correctly being removing
the quotation mark that was the problem up above so we're going to add
this bit of code in and we're going to be able to fix it
so now when we execute this we see that we've only grabbed one match
now we might be thinking to yourself there's an issue here and there is let's go ahead and print off
the length of matches and we see that we've only grabbed one match
and then we haven't grabbed anything else well what's the problem here are there
are there no other instances of quotation marks in the rest of the first chapter
and the answer is no there are there absolutely are other quotation marks
and other paragraphs from the first chapter the problem is that
your pattern is singular it's not
multivariate we need to add in additional ways in which
a text might be structured so let's go ahead
and try and do this with some more patterns
I'm going to go ahead and copy and paste these in from the textbook
so you'll be able to actually see them at work
and so what I've did I've done is I've added in more patterns
pattern two and pattern three allow for instances like this
well thought Alice so an instance where there's a punctuation but there's no
proceeding quotation after this and then which certainly
said before an instance where there's a comma followed by that so we've been able to
capture more variants and more ways in which quotation marks might exist
followed by the speaker now this is where being a domain expert
comes into play you'd have to kind of look through and see the different ways that
I'm going to go through and try to capture everything from Alice in Wonderland
because that would take a good deal of time and it's not really in the best
interest because it doesn't matter to me at all
what I encourage you to do if this is something interesting to you is try to apply it to your own texts
different authors structure quotation marks a little differently
the patterns that I've gotten written here are a good starting point
but I would encourage you to start playing around with them a little bit more
and what you can do is when you actually have this
match extracted you know that the instance of a proper
noun that occurs between these quotation marks or after one
is probably going to be the person or thing that is doing
the speaking or the thinking so that's kind of how
the matcher works it allows for you to do these things these robust
type data extractions without relying on entity ruler
and remember you can use a lot of these same things with an entity ruler as well
but we don't want this in this case we don't want things like this to be labeled as entities
we want them to just be separate things that we can extract outside of the
of the ints dot doc dot ints that's going to be where we conclude our
chapter on the on the matcher
in the next section of this video we're going to be talking about custom components
in spacey which allow for us to do some pretty cool things such as
adding in special functions that allow for us to
kind of do different custom shapes or permutations on our
data with components that don't exist like an entity
ruler would be a component components that don't exist within the spacey framework
so add in custom things like an entity ruler that do very specific
things to your data
hello we're now moving into a more advanced aspect of the
text book specifically chapter 7 and that's working with custom components
a good way to think about a custom component is something that you need to do
to the doc object or the doc container that spacey can't do off the
shelf you want to modify it at some point in the pipeline so
I'm going to use a basic toy example that demonstrates the power of this
let's look at this basic example that I've already loaded into memory it's
two sentences that are in the doc object now and that's Britain is a place
where Mary is a doctor so let's do for int and doc.ints
print off int.text
and dot label and we see what we'd expect Britain is
GPE a geopolitical entity Mary is a person
that's fantastic but I've just been told by somebody
higher up that they want the model to never
ever give anything as GPE or maybe they want
the instance of GPE to be flagged as LOC
so all the different locations all have LOC as a label
or we just want to remove them entirely so I'm going to work with that latter example
we need to create a custom pipe that removes all instances
of GPE from the doc.ints
container so how do we do that well we need to use a custom component
we can do this very easily in spacey by saying from spacey dot
language import language capital L very
important there capital L now that we've got that class loaded up
let's start working with this what we need to do first is we need to use
a flag so the et symbol and we need to say et language
dot component and we need to give that component a name
we're going to say in this case let's say remove GPE
and now we need to create a function to do this so we're going to call this
remove GPE I always kind of keep these as the same
that's my personal preference and this is going to take
one thing that's going to be the doc object so the
doc object think about how it moves through the pipeline this component
is another pipe and that pipeline it needs to receive the doc object
and send off the doc object you can do a lot of other things
it could print off entity found it could do really any number of things
it could add stuff to the data coming out of the pipeline
all we're concerned with right now is modifying the doc dot ends
so we can do something like this we can say original
ends is equal to a list of the doc dot ends so remember
we have to convert the ends from a generator into a list
now what we can do is we can say for int and doc dot ends if the
end dot label so if that label is equal to GPE
then what we want to do is we want to
just we just want to remove it so let's say original
ends dot remove and we're going to remove the int remember it's now a list
sorry I executed that too soon remember it's now a list so what we can do
is we can go ahead now and convert those original
ends back into doc dot ends by saying doc dot ends
equals original ends and if we've done things correctly we can return
doc object and it will have all of those things removed
so this is what we would call a custom component something that changes
the doc object along the way in the pipeline but we need to add it to
NLP so we can do NLP dot add pipe
we want to make sure that it comes after the NER so we're just going to say
add the pipe remove GPE corresponds
to the component name and now let's
go ahead and NLP dot analyze pipes
and you'll be able to see that it sits at the end of our pipeline
right there remove GPE now comes time to see if it actually works
so we're going to copy and paste our code from earlier up here
let's go ahead and
copy this
for int and doc dot ends print off
and dot text and dot label and we should see as we would expect
just Mary coming out our pipeline
has successfully worked now as we're going to see when we move into
redgex you can do a lot of really really cool things
with custom components I'm going to kind of save the
the advanced features for I think I've got it scheduled for chapter
9 in our textbook this is just a very very basic example
of how you can introduce a custom component to your spacey
pipeline if you can do this you can do a lot more you can maybe
change different entities so they have different labels you can make it where
GPEs and locks all agree you can remove certain things you can have it
print off place found person found you can do a lot
so really the sky's the limit here but a lot of the times you're going to need
to modify that doc object and this is how you do it with a custom
pipe so that you don't have to write a bunch of code for a user outside
of that nlp object the nlp object once you save it
to disk by doing something like nlp dot to
disk data new
and core web sm it's going to actually be able to
go to the disk and be saved with everything but one
thing that you should note is that the component that you have here
is not automatically saved with your data
so in order for your component to actually be saved with your data
you need to store that outside of this entire
script you need to save it as a library that
can be given to the model when you go to package it
that's beyond the scope of this video for right now in order for this to work
in a different jupiter notebook if you were to try to use this
this container this component has to actually be in the
script when it comes time to package your model your pipeline and distribute it
that's a different scenario in that scenario you're going to make sure that you've
got a special my component dot py file with this
bit of code in there so that so that spacing knows how to handle
your particular data it's now time to move
into chapter eight of this textbook and this is where a spacey gets really interesting
you can start applying regular expressions into a
spacey component like an entity ruler or a custom component as we're going to see
in just a moment with chapter nine I'm not going to spend a good deal of
time talking about regular expressions I could spend five hours talking
about regex and what all it can do in the textbook I go over what
you really need to know which is what regular expressions is which is as a way to do
a really robust string pattern matching I talk about the
strengths of it the weaknesses of it that straw backs how to implement it
in python and how to really work with regex but this is a video series on
spacey what I want to talk about is how to use regex with
spacey and so let's move over to a jupiter notebook where we actually have this code
execute and play around with if we look here we have the same example
that we saw before what my goal is is not to extract the whole phone number
rather try to grab this sequence here and we do this with a regular
expression pattern what this says is it tells it to look for a sequence of tokens
or sequence of characters like this it's going to be three digits
followed by a dash followed by four digits
if I were to execute this whole code nothing is printed out
does that mean that I failed to write good regex no it does not at all
it's failed for one very important reason and this is the whole reason why I have this
chapter in here is that regex when it comes to pattern
matching pattern matching only really works when it comes to
regex for single tokens you can't use regex
across multi-word tokens at least as of spacey
3.1 so what does that mean well it means that that dash right
there in our phone number is causing all kinds of problems if we move down
to our second example it's going to be the exact same pattern a little different
let me go ahead and move this over so you can see it a bit better
it's going to be regex that looks like this where we just look for a sequence
of five digits we execute that we find it just fine
and the reason for that is because this does not have a dash
so regex if you're familiar with it if you've worked with it it's very powerful
you can do a lot of cool things when you're going to use this in python
if you're using just the standard off the shelf
components so the entity ruler the matcher you're going to be
using this when you want to match regex to a
single token so think about this if you're looking for
a word that starts off with a capital D and
you want to just grab all words that start with a capital D that would be an example
of when you would want to use it in a standard off the shelf component
but that's not all you can do in spacey you can use regex
to actually capture multi word tokens so capture things
like mr deeds so any instance of
mr period space name a sequence of proper
nouns you can also use it
but yet in order to do that you have to actually understand how to add in
a custom component for it and we're going to be seeing that in just a second
as we move on to chapter nine which is advanced regex
if you're not familiar with regex at all take a few minutes
read chapter eight I encourage you to do so because I go over in detail
and I talk about how to actually engage in regex in python
and its strengths and weaknesses what I want you to really focus on
though and get away from get from all this is how to do some really complex
multi word token matching with regex remember
you're going to want to use regular expressions when the pattern matching that you want to do
is independent of the lemma
the p o s or any of the linguistic features that spacey is going to use
if you're working with linguistic features you have to use the spacey pattern
pattern matching things like the morph the orth the lemma
things like that but if your sequence of strings is not dependent
on that so you're looking for any instance of in this case
we're going to talk about in just a second a case where paul is followed by a
capitalized letter and then a word break
then you're going to want to use regular expressions because in this case this is
independent of any linguistic features and regular expressions
allows for you to write much more robust patterns much more quickly if you know how to use it
well and it allows for you to do much more quick robust things within
a custom component and that's going to be where we move to now
now that we know a little bit about regex
what can be implemented in python let's go ahead and also in spacey
let's go ahead and try and see how we can get
regex to actually find multi word tokens for us
within spacey using everything in the spacey framework
so the first thing I'm going to do to kind of demonstrate all this is I'm going to import
regex this comes standard with python and you can import it as
re just that way import re and that's going to import regex
I'm going to work from the textbook and work with this sample text
so this is paul newman was an American actor but paul hollywood
is a british tv host the name paul is quite
common so it's going to be the text that we work with throughout this entire
chapter now a regex pattern that I could write to capture all instances
of things like paul newman paul hollywood which is what my goal is
could look something like this I could say
r and make an r string here and say paul and then I'm going to grab everything
that starts with a capital letter and then I grab everything until
a word break and that's going to be a pattern that I can use in regex
what this formula means is find any instance of paul
proceeded by a in this case a capital letter until
the actual word break so grab the first name paul and then what we can make a presumption
is going to be that individual's last name in the text
for example but one that will demonstrate our kind of purpose right now
so how we can do this is we can create an object called matches and use
regex.finditer we can pass in the
pattern and we can pass in the text so what this is going to do is it's going
to use regex to try to find this pattern within this
text and then what we can do is we can iterate over those matches so for match
and matches we can grab and print off
the match and we have something that looks like
this what we're looking at here is what we would
call it a regex match object it's got a couple different components here
it's got a span which tells us the start character and the
end character and then it has a match and what this
match means is the actual text itself so the match here is paul
Newman and the match here is paul hollywood so we've been able to extract
the two entities in the text that begin with paul and
have a proper last name structured with a capital letter and we grabbed
everything up until the word break that's great that's going to be what you need to know kind
of going forward because what we're going to do now is we're going to implement this
in a custom spacey pipe but first let's go through and
write the code so that we can then easily kind of create the pipe afterwards
so what we need to do is we need to import spacey
and we also need to say from spacey dot tokens import
span and we're going to be importing a couple different things as we move forward
because we're going to see that we're going to make a couple mistakes intentionally I'm going to show you how to
kind of address these common mistakes that might surface in trying to do
something like this so once we've imported those two things we can start
actually writing out our code again we're going to stick with the exact same text and again
we're going to stick with the exact same pattern that we've got stored in memory
up above so what we need to do now is we need to create a blank
spacey object or sorry a blank spacey pipeline that we can kind of
put all this information into and for right now what
we're going to do is we're just going to
kind of go through and look at these individual entities
so again we're going to create the doc object
which is going to be equal to NLP
text and this is not going to be necessary for right now but I'm establishing a
kind of a consistent workflow for us and you're going to see how we kind of take all this
and implement it inside of a pipeline so we're going to say original ends is equal to
list doc dot ends now in this scenario there's not going to be any entities
because we don't have an NER or an entity ruler in our
blank spacey pipeline what we're going to do next is we're
going to create something called an NWT int and that's going to stand for
multi word token entity you can name this whatever
you like this is just what I kind of stick to and then we're going to do and this is
straight from the spacey documentation we're going to say for match an RE.find
iter the same thing that we saw above pattern
doc.text so what this is going to do is it's going to take that doc
object look at it as raw text because remember the doc object
is a container that doesn't actually have raw text in it
until you actually call the dot.text attribute and then our goal is
for each of these things we're going to look and call in this span
so we're going to say is start and the end
is equal to match.span so what we're doing here is we're going
in and grabbing the span attribute and we're grabbing these two
components the start and the end but we have a problem these are
character spans and remember the doc object works on a token
level so we've got to kind of figure out a way to reverse engineer this almost
to actually get this end to a spacey form fortunately
the doc object also has an attribute called character span
so what we can do is we can say the span is equal to
doc.charspan start
and end so what this is going to do is it's going to print off essentially for us
let's go ahead and do that it would print off for us we're worried to
actually have an entity here it would print off for us as we can see Paul Newman
and Paul Hollywood so what we need to do now is we need to get
this span into our entities
so what we can do is instead of printing things off we can say
if span is not none because in some instances
this will be the case you're going to say n wt
ends dot append and you're going to append a tuple here
span dot start span dot end
span dot text so this is going to be the start the end
and the text itself and once we've done that we've managed to get
our multi-word tokens into
a list that looks like this
dot end Paul Newman Paul Hollywood
and notice that our span dot start
is aligning not with a character span now
it's rather aligning with a token span
so what we've done is we've taken this character span here and been able to find out where
they start and end within the
token sequence so we have 0 and 2 so Paul Newman
this was the 0 index it goes up until the second index so it grabs
index token 0 and token 1 and we've done the same thing with Paul Hollywood
now that we've got that data we can actually start to inject
these entities into
our original entities so let's go through and do that right now
so what we can do once we've got these things appended to this list we can start
injecting them into our original entities so we can say for int
in mwt ints what we want to do is we want to say
the start the end and the name is equal to end because this is going
to correspond to the tuple the start the end and
the entity text now what we can do is we can say
per int so this is going to be the individual int we're going to create a span
object in spacey it's going to look like this
so a capital S here remember we imported it right up here
this is where we're going to be working with the span class
and this is going to create for us a span object that we can now safely inject
into the spacey doc.ins list so we can say
doc start and label and this is
going to be the label that we want to actually assign it and this is going to be person
in this case because these are all people what we can do now is we can go
through and say doc
we can inject this into the original ints
original ints
dot append and we're going to append the per int which is going to be
this span object and finally what we can say
is doc.ins is equal to
original ints kind of like what we saw just a few moments ago
and let's go ahead and print off
we've got our
entities right there were we to do this up here when we first kind of create
the doc object you'll see nothing an empty list but now what
we've been able to do is inject these into the doc object
the doc.ins attribute and we can say for int and doc.ins
just like everything else int.text and dot label
and because we converted it into a span we were able to inject it into the
entity attribute from the doc object kind of
natively so that spacey can actually understand it so what can we do with
this well one of the things that we could do is we can use the knowledge that we just acquired
about custom components and build a custom component around
all of this so how might we do that well let's go through and try
it out
the first thing that we need to do is we need to import our language class so if you
from a few moments ago whenever you need to work with a custom component
you need to say from spacey dot language
import language with a capital L what we're going to do now
is we're going to take the code that we just wrote and we're going to try to convert that into
an actual custom pipe that can fit inside of our pipeline as
kind of our own custom entity ruler if you will
so what we're going to do now is we're going to call this language.component
we're going to call this let's call this Paul NER
something not too clever but kind of very descriptive
we're going to call this Paul NER and this is going to take that single doc
object because remember this pipe needs to receive the doc object and do
stuff to it so what we can do is we can take all this code that we just wrote
from here
down and paste it into our
function and what we have is the ability now
to implement this as a custom pipe we don't need to do this
because we don't want to print things off but here we're going to return the doc
object so what we have now is a custom kind of entity ruler that
uses regex across multiple tokens if you want to use
regex in spacey across multiple tokens as of spacey 3.1
this is the only way to implement this so now we can
take this pipe and we can actually
add it to a blank custom model so let's make a new
nlp call this nlp2 is equal to spacey.blank
and we're going to create a blank English model
nlp2.addPipe
we're going to add in Paul NER
and now we see that we've actually created that
successfully so we have one pipe kind of sitting in all of this
now what we can do is we can go through and we need to probably add in our pattern
as well here just for good practice because this
should be stored somewhat adjacent I like to sometimes to keep it up here
when I'm doing this but you can also keep it kind of inside of the function itself
let's go ahead and just kind of save that and we're going to rerun
this cool now what we can do is we can say
doc2 is equal to nlp2 we're going to go over that exact same text
and we're going to print off our doc2.ins
and we've now managed to implement that as a custom spacey
pipe but we've got one big problem let's say
just hypothetically we wanted to also kind of work in
really a another kind of
something into our actual pipeline we wanted this
pipeline to sit on top of maybe an existing spacey model
and for whatever reason we don't want Paul Hollywood
to have that title we wanted to have the title maybe we want to just kind of
keep Paul Hollywood as a person but we also want to find maybe
other cinema style entities so we're going to create another
entity here instead of all this that's going to be something like
let's go ahead and make a new
component down here we're going to just look for any instance of Hollywood
and we're going to call that the word the label of
cinema so I want to demonstrate this because this is going to show you something that you are going
to encounter when you try to implement this in the real world and I'm going to show you how to
kind of address the problem that you're going to encounter so if we had a component
that looked like this now it's going to look for just instances of
Hollywood and let's call this Holly
cinema NER and change this here as well
what we can do now is go ahead and load that up into memory so we've got this new component called
cinema NER and just like before we're going to create NLP3 now
this is going to be spacey.load in core web
sm and so what this
is going to do is it's going to load up the spacey small model NLP3.add
pipe and it's going to be the
let's call this again the cinema NER
and if we were to go through and add that and create a new object called
doc3 make that equal to NLP3
text we're going to get this
error and this is a common error and if you google it you'll eventually find
the right answer I'm just going to give it to you right now so what this is telling you is
that there are spans that overlap that
don't actually work because one of the spans
for cinema is Hollywood and the small model
is extracting not only that Hollywood as a cinema
but it's also extracting Paul Hollywood as part of a longer
token so what's happened here is we're trying to assign
a span to two of the same tokens and that doesn't work in
spacey it'll break so what can you do well a common method of
solving this issue is to work with the filter spans
from the spacey.util let's go ahead and do this
right now so you can say from spacey.util
import filter spans what filter spans allows for you
to do is to actually filter out all of the
spans that are being identified so what we can do is we can say
at this stage
before you get to the doc.ents you can say filtered
is equal to filter spans original
ends so what does this do well what this does is it goes through and looks at
all of the different start and end sections from all of your
entities and if there is ever an instance where there is
an overlap of tokens so
8-10 and 9-10 primacy and priority
is going to be given to the longer token so what we can do
is we can set this now to filtered and it helps if you call it correctly
filtered there we go we can set that to
filtered instead of the original entities go ahead and save that
we're going to add this again and we're going to do doc3
and we're going to say for int and doc3.ents print
int.text and int.label and if we've
done this correctly we're not going to see the
cinema label come out at all because Paul Hollywood
is a longer token than just Hollywood
so what we've done is we've told spacey give
primacy to the longer tokens and assign that label
by filtering out the tokens you can prevent that error from ever surfacing
but this is a very common thing that you're going to have to implement
sometimes regex really is the easiest way to inject
and do pattern matching in the entity okay so here's the scenario
that we have before us in order to make this kind of live coding and applied
spacey a little bit more interesting imagine in this scenario we have a client
and the client is a stockbroker somebody who's interested
in investing and what they want to be able to do is look at news articles like those
coming out of Reuters and they want to find the news articles that are the most relevant
to what they need to actually search for and read for the day
so they want to find the ones that deal with their personal stocks, their holdings
or maybe the specific index that they're actually interested in
so what this client wants is a way to use
spacey to automatically find all companies referenced
within a text all stocks referenced within a text
and all indexes referenced within x-text and maybe even
stock exchanges as well now on the actual textbook if you go through
to this chapter which is number 10 you're going to find all the kind of
solutions laid out for you what I'm going to do throughout the next
30 or 40 minutes is kind of walk through how I might solve this
problem at least on the surface this is going to be a rudimentary
solution that demonstrates the power of spacey and how you can apply it
in a very short period of time to do some pretty custom tasks such
as financial analysis with that structured data that
you've extracted you can then do any number of things what we're
going to start off with though is importing spacey
and importing pandas as pd
if you're not familiar with pandas I've got a whole tutorial series on that on
my channel python tutorials for digital humanities even though it has digital
humanities in the title it's for kind of everyone but go through if you're not
familiar with pandas and check that out you're not really going to need it for
this video here you're going to just need to understand that I'm using pandas
to access and grab the data that I need from a couple
CSV files or comma separated value files that I have
so the first thing that we need to do is we need to create what's known as a pandas data frame
and this is going to be equal to pd dot read
CSV and I actually have these stored in the data sub folder in the repo
you have free access to these they're little tiny data sets that I cultivated
pretty quickly they're not perfect but they're good enough for our purposes
and we're going to use the separator keyword argument which is going to say to
separate everything out by tab because these are tsv files
tab separated value files and we have something that looks like this
so what this stocks.tsv file is is it's all the symbols company
names industry and market caps for I think it's around
5,700 different stocks 5,879
and so what we're going to use this for is as a way to start working into an entity ruler
all these different symbols and company names
what we want to do is we want to use these symbols to work into a model as a way to grab
stocks that might be referenced and you can already probably start to see a problem
with this capital A here we're going to get to that in a little bit and we want to grab
all the company names so we can maybe create two different entity types from this data set
stock and company
so let's go through and make these into lists so they're a little bit more
so let's go through and make these into lists so they're a little bit more manageable
what we need to do is we need to create a list of symbols
and that's going to be equal to df.symbol.to
list this is a great way to do it in pandas so you can kind of
easily convert all these different columns into
lists that you can work with in python so companies is going to be equal to df.company
and name I believe the name was to list
and just to demonstrate how this works let's print off symbols
we're going to print up to 10 and you can kind of see we've managed to take
these columns now and kind of build them into a simple python list
so what can we do with that well one of the things that we can do is we can use
that information to start cultivating an entity ruler
we want more things than just one or two kind of in our
entity ruler we don't just want stocks and we don't just want companies we also want things like
indexes we're going to get to that in just a second though for right now let's try to work
these two things into an entity ruler how might we go about doing that
well as you might expect we're going to create a fairly
simple entity ruler so we're going to say is nlp is going to be equal to
spacy.blank we don't need a lot of fancy features here we're just going to have a
blank model that's just going to host a single
entity ruler that's going to be equal to nlp.add
underscore pipe and this is going to be entity ruler
and now what we need to do is we need to come up with a way to go through
all of these different symbols and add them in so we can
say for symbol and symbols we want to say
patterns.append and we're going to make an empty
list of patterns up here and what we're going
to append is that dictionary that you met when we talked about the entity ruler
and I believe it was chapter five yeah and what
this is going to have there are two things label which is going to correspond to
stock in this case and it's going to have a
pattern and that's going to correspond to the
pattern of the symbol so we're going to say symbol
and what that lets us do is kind of go through and easily create and add these
patterns in and we can do the same thing for company
remember it's never a good idea to copy and paste in your code I am simply
doing it for demonstration purposes right now this is not polish code by any
stretch of the imagination and what we can do here now is we can do the same
thing loop over the different companies and add each company in
so what this is doing is it's creating a large list of different patterns that the
controller will use to then go through and as we create the
doc object over that sample Reuters text I just
showed you a second ago which we should probably just go ahead and pull up
right now I'm going to copy and paste it straight from the text
book
let's go ahead and execute that cell and we're going to add in this text here it is
a little lengthy but it'll be alright and what we're going to do now
is we're going to iterate over create a doc object to iterate over all of that
and our goal here
is going to be able to say for int and doc dot ends we want
to have extracted all of these different entities so
we can say print off int dot text
and dot label and let's see if we
succeeded
and we have to add in our patterns to our entity ruler
so remember we can do this by saying ruler dot add patterns
patterns there we go
that's what this error actually means
and now when we do it we see that we've been able to extract
apple as a company apple as a company nasdaq
everything's looking pretty good but I notice really quickly that I wasn't
actually able to extract apple as a stock
and I've also got another problem I've extracted to
the lowercase two as a stock as well
why have these two things as a company well it turns out in our data set
we've got two two that is a company name
that's almost always going to be a false positive and we know
that that kind of thing might be better off worked into a machine learning model
for right now though we're going to work under the presumption that any time we
encounter this kind of obscure company two as a lowercase
it's going to be a false positive I also have another problem I know for a
fact that apple the stock is referenced within
this text to make it a little easier let's see it right here
and notice that it didn't find it to make this a little easier to display
let's go ahead and display what we're looking at as
the splacy render so what we can do is we can use that the splacy render
that we met a little bit ago in this video
so in order to import this if you remember we need to say
from spacey import
display see and that's going to allow us to actually display
our entities let's go ahead and put this however on a different
cell just so we don't have to execute that every time and we're going to say
display see dot render and we're going to render the
doc object with a style that's equal to ent
and we can see that we've got our text now popping out with our
things labeled and you can see pretty quickly where we've made some mistakes
where we need to incorporate some things into our entity
ruler so for example if I'm scrolling through this is gray little ugly
we can change the colors that's beyond the scope of this video though but let's keep on
going down we notice that we have apple dot i o and yet
this has been missed by our entity ruler why has this been missed well
spacey as a tokenizer is seeing this as a single
token so apple dot o the
letter o capital letter o why is that well I didn't know
about this but apparently it does has to deal with kind of the way in which
stock indices are I think it's on the NASDAQ kind of structure things
so what can we do well we've got a couple different options here
I know that these go through all different letters from a to z so we can either work with the
string library or we can do is we can import a
quick list that I've already written out of all the different letters of the alphabet
and iterate through those with our ruler
up here let's go ahead and
add these letters right there and we can kind of iterate through those and whenever a stock
kind of pops out with that kind of symbol plus
any occurrence where it's got a period followed by a letter
in those scenarios we want that to be flagged as a stock as well
so what we can do is we can add in another thing right here
add in another pattern and this is now going to be symbol
plus we're going to add in an F string right here a formatted string
any occurrence of L we can set up a loop to say
for L in letters
do this and what this is going to allow us to do
is to look for any instance where there is
a symbol followed by a
period followed by one of these capitalized letters that I just copied
and pasted in so if we do that we can execute that cell
and we can scroll down and we can now do the
exact same thing that we just did a second ago and actually
display this
and now we're finding these stocks highlighted as
stock so we're successfully getting these stocks and extracting them
we've got a few different things that our client wants to also extract though
they don't want to just extract companies and they don't want to just
extract stock and they want to also extract stock exchanges
and indexes but we have one other problem
let me go ahead and get rid of this as the display mode and switch back to just
our set of entities because it's a little easier to read for this example
we've got another problem and we see we have a couple other stocks popping out
we now know that Kroger stock is here the nio.n stock is in this text
as well now we're starting to see a greater degree of specificity
for right now I'm going to include two as a set of a stop
technical term would be like a stop or something that I don't want to be included into the model
so I'm going to make a list of stops and we're just going to include two
in that and we're going to say for company and companies do all this
if company
not in stops we want this to occur
what this means now is that our pipeline while going through and having
all of these different things all these different rules it's also going to
have another rule that looks to see if there's a stop or if this company
name is this stop and if it is then we want it to just
kind of skip over and ignore it and if we
go through we notice that now we've successfully eliminated this what we would
presume to be a consistent false positive something's going
to come up again and again as a false positive great
so we've been able to get this where it works now pretty well what
I also want to work into this model if you remember though are things like indexes
fortunately I've also provided for us a list of all different
indexes that are available from I believe it's
everything like the Dow Jones is about 13 or 14 of them let's go ahead
and import those up above
and let's do that right here in this cell so it kind of goes in sequential order
follows better with the textbook to so it's a new data frame object this is going
to be equal to pd.read.csv we're going to read in that
data file that I've given us and that's going to be the indexes.tsv
with a separator that's equal to a tab
let's see what that looks like and this is
what it looks like so all these different indices now I know I'm going to have a problem right
out of the gate and that's going to be that sometimes
you're going to see things referenced as S&P 500 I don't know a lot about
finances but I know that you don't always see it as S&P 500 index
but I do think that these index symbols are also going to be
useful so like I did before I'm going to convert these things into a list
so it's a little easier for me to work with in a for loop and I'm going to say
indexes is equal to df2.index
name so grabbing that column
to list and index symbols
is equal to df2.index symbol
.to list and both of these are going to be different
they're both going to have the same exact entity label which is going to be
an index and so let's go ahead and iterate over these
and add them in as well so I'm going to go ahead and do that right now
for index
and indexes
we want this label to be index we want this to be index here
so that's going to allow us to kind of go through and grab
all those and we want to do the same thing with index symbols
keep these a little separated here index
symbols and that allows for us to do that
and let's go ahead and without making any adjustments let's see
let's see how this does with these new patterns that we've added in and
because we've already got this text loaded into memory I'm going to go ahead and put this
right here doc is going to be equal to NLP
text for int and doc.ints
print off int.text
and we can kind of go through and we're actually now
able to extract some indexes and I believe when I was looking at this
text really quickly though I noticed that there was one instance
at least where we had not only the index referenced
but also a name like
S&P 500 right here notice that it isn't found because it doesn't have the name
index after it and notice also that none of our symbols
are being found because they all seem to be preceded by a dot
so in this case a dot j a dji
and so that's something else that I have to work into this model and the list I've gave the data set
that's not there so I need to collect a list of these different names
and work those into an entity ruler as well but for right now let's ignore that
and focus on including this S&P 500
so how can I get the S&P 500 in there from the list I already gave it
well what I can do is I can say okay
so under these indices not only do I want to add that specific pattern
let's go ahead and break these things up into different words
the words is equal to index dot split
and then I'm going to make a presumption that the first
two words so the S&P 500 the S&P 400
are sometimes going to be referenced by themselves
so what I want to do is I want to work that into the model as well
and I want to say we're going to say patterns dot append
copy this as well
we can say something like dot join
words up until the
second index and let's go ahead and work that into our model in our
patterns or pipeline and print off our
NLP again and you'll find that we've now been able to capture
things like S&P 500 that aren't
proceeded by the word index and we see that we in fact have
S&P 500 is now popping out time and again
that's fantastic I'm pretty happy with that now we're getting a deeper sense of what this text
is about without actually having to read it we know that it's going to deal heavily
with Apple and we know that it's also going to tangentially deal with some of these other
things as well but I also want to include into this
pipeline the ability for the entity ruler to not just
find these things but I also wanted to be able to find different stock exchanges
so I've got a list I cultivated for different stock exchanges
which are things like NYSE things like that so I can say ds
3 is going to be equal to pd dot read csv
data backslash stock exchanges
dot tsv
and then the separator is going to be again a tab and let's take a look at
what this looks like
there we go
there we are and we have something that looks like
this a pretty large csv file
tsv file sorry that's got a bunch of different rows
the ones I'm most interested in well there's a couple actually
I'm interested in specifically the Google prefix and this description
the description has the actual name and the prefix has this
really nice abbreviation that I've seen pop out a few different times such as
NASDAQ here if we keep on going down we would see different things
as well NYSE these were kind of different stock exchanges
so let's pop back down here and
let's go ahead and convert those two things into individual
lists as well so we're going to say exchanges it's going to be equal to
df3.iso mic dot
list and then I'm also going to grab df3 dot
sorry Google and I have to do this as a dictionary
because it's the way the data set's cultivated it's got a space
in the middle this is a common problem that you run into and
then I also want to grab all of these
exchanges as well so I'm going to say also on top of that
df3.description dot
to list so I'm making a large
list exchanges and I get this here because it says
Google prefix isn't an actual thing and in fact it's prefix with an
I and now we actually are able to get all these
things extracted so what I want to do now
is I want to work all these different symbols and descriptions into
the model as well or into the pipeline as well so I can say for
e and exchanges
I want to say patterns dot append
and I want to do a
label that's going to be let's do stock exchange
and then the next thing I want to do is a pattern
and that's going to be equal to in this case e as we're
going to see this is not adequate enough we need to do a few different
things to really kind of work this out but it's going to be a good
enough to at least get started
and it's going to take it just a second
and the main thing that's happening right now are these different
for loops so if we keep on going down we now see that we were able to
extract the NYSE stock exchange so we've not only been able to
work into a pipeline in a very short order maybe about 20-30 minutes
we've been able to work into a pipeline all of these different things that are coming out
we do however see a couple problems and this is where I'm going to leave it though
because you've got the basic mechanics down now it comes time for you being a
domain expert to work out and come up with rules to solve some of these problems
NASDAQ is not a company so there's a problem with the
dataset or NASDAQ is listed as a company name in one of the datasets
we need to work that out where NASDAQ is never referenced as a company
we have the S&P is now being coming out correctly as S&P 500
there might be instances where just S&P is referenced which I think
in that context would probably be the S&P 500 but nevertheless
we've been able to actually extract these things sometimes
the Dow Jones industrial average
might just be referenced to Dow Jones so this index might just be
these first two words I know that's a common occurrence we've also seen that we weren't able
to extract some of those things that were a period followed by a symbol
that referenced the actual index itself nevertheless this is a really good
starting point and you can see how just in a few minutes you're able to generate this thing
that can extract information from unstructured text
at the end of the day like I said in the introduction to this entire video
that's one of the essential tasks of NLP
obtaining this and implementing it is pretty quick and easy
perfecting it is where the time really is to get this financial analysis
entity ruler working really well where it has almost no false
positives and almost never misses a true
a true positive it would take maybe a few more hours of just some kind of working
and eventually there are certain things you might find that would work better in a machine
learning model nevertheless you can see the degree to which
based approaches in spacey can really accomplish some pretty robust
tasks with minimal minimal amount of code so long as you have
access to or have already cultivated the data sets required
thank you so much for watching this video
series on spacey and introduction to basic concepts
of natural language processing linguistic annotations in spacey
vectors pipelines and kind of rules based spacey
if you've enjoyed this video please like and subscribe down below and if you've also
found this video useful consider joining me on my channel python tutorials
for digital humanities if you have liked this and found this video useful
I'm envisioning a second part to this video where I go
with the machine learning aspects of spacey
if you're interested in that let me know in the comments down below and I'll make a second video
that corresponds to this one thank you for watching and have a great day

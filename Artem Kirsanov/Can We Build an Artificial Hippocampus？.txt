It is humanity's longtime dream to create machines that can think, but what exactly does it mean?
One particular characteristic of intelligence is the ability to generalize knowledge
and flexibly adapt it to new situations. Such generalization is indeed one of the cornerstone
problems in modern machine learning. In this video, we are going to see how we can take
biological organization over hippocampus, a brain structure involved in memory and navigation,
as an inspiration in order to construct a computational model that can learn to build
abstractions and generalizations, as well as the insights we can draw from this model,
both about our own brains and the field of artificial intelligence.
Before we begin, I'd like to warn you that this video is the continuation of the previous video
in the series on cognitive maps. Last time we explored neurobiological background of hippocampal
computations and introduced some general principles. So, if you haven't seen it,
I highly recommend you check it out before watching this one, since we are going to build up from
there. If you're interested, stay tuned. Imagine you are an agent that walks around the world
and whose only goal is to find rewards. From an evolutionary perspective, you can think about
such an agent as an early organism, which needs to look for food or mates. Now, as an agent, you
have a certain repertoire of actions that you can take. For example, activate a sequence of muscles
to move in a particular direction. To choose the most rewarding actions, you need to be able to
predict the action outcomes, and that effectively requires a mental model of the surrounding
environment. Existence of such a model allows you to run mental simulations in your head
to weigh the actions. For example, what would happen if I go straight or is it better to turn
right? Over the course of your lifetime, as you encounter a variety of different environments,
initially you might build an entangled, indivisible model for each, without necessarily linking
different models to each other. However, if you are being optimal in your representations,
at some point you realize, wait a minute, all these models I've built so far actually have an
awful lot in common. Indeed, walls that block your way, doors that allow you to go through the walls,
and even just the structure of an open 2D space itself, all work similarly in every environment,
so these common elements can be easily reused. In other words, it makes sense to break up or
factor each model into its building blocks. For example, building blocks of space, of boundaries,
rewards, etc. Once these building blocks are learned, we can rearrange and mix them in different
configurations to build new models of the world on the fly, and thus generate flexible behavior.
As you might remember from part 1, this is exactly what mammalian hippocampus does,
and we can find neurobiological evidence for this process in responses of individual cells.
Now, the question is, can we teach a machine to do the same?
To make the task easier for a natural visual system, let's formulate it as a prediction
problem. Namely, the model will receive a sequence of observations, along with a sequence of actions
that led to them, and learn to correctly predict the next observation in the sequence. It actually
makes a lot of sense biologically. There is a great deal of data suggesting that the main purpose
of the brain may be to predict the incoming stimuli and try to minimize the prediction error,
a theory called predictive coding. For example, consider the following sequence of observations
and actions. Can you tell me what should be the next element in the sequence? Seems impossible,
right? However, what if I told you that those actions, 1 through 4, actually stand for directions,
north, west, south, and east? Now, the task becomes much easier. Because you know the rules how to
chain these actions together, you can predict the next observation to be the same as the first one,
since you know you essentially closed a loop. In other words, knowing the structure of space
significantly simplifies the prediction problem. But the model of course would not know this
underlying structure since it would be no fun. Instead, it would need to extract repeating patterns
in order to somehow infer this structure of the underlying world from sequences of observations
and actions. For example, after seeing a large number of sequences like these, it should infer
the rules how different actions relate to each other, which is equivalent to constructing the
structure of space. It's important to point out that although I'm saying things like the model
will learn the underlying structure of the world, it is not told to do that exactly. The model has
no other goal, so to speak, other than predicting the next observation in the sequence. In essence,
it is just a fancy mathematical expression with a large number of parameters that takes the set
of numbers encoding the observations and actions, performs computation on them, and speeds out another
set of numbers corresponding to the next predicted observation. But because we train it to minimize
this prediction error, and since these observations are not random, but come from some structured
world, the optimal solution to this prediction problem is to construct some structural representation
of this world, which underlies the regularities in the observations. So we simply expect the
knowledge about this structure to emerge as a result of optimization. But how should the model
look like? Well, because we are free to choose whichever architecture we want, it is reasonable
to draw inspiration from an existing biological machine that solves this problem on a daily basis,
the hippocampal formation. In the last video, we saw how the hippocampus receives two streams
of inputs, sensory, the what am I seeing information, coming from the lateral entorhinal cortex,
and structural, the where am I information, from medial entorhinal cortex. They are then
combined in the hippocampus into a conjoined representation. Similarly, our model will have
an analog of medial entorhinal area, responsible for keeping track of current location in the world.
Let's call it a position module. At every point in time, it will receive an action and use it to
compute the estimate of the current location, the best guess of where it is in space. You can think
of this positional information as being encoded with the pattern of neuron activations inside of it.
Note that the position module operates purely with actions and doesn't receive any information
about the sensory observations. Similarly, how if you close your eyes and walk around the room,
you have a rough idea of where you are located, even though you don't see anything.
This is because your brain is able to accumulate
self-movement vectors and estimate the position, a process known as path integration.
So once the model is trained, we expect our position module to be able to do the same.
Another crucial component is the hippocampus itself, which binds the where information
with what. This binding is effectively forming an association between the two inputs.
So we need to add a memory module that would receive the positional information provided
by the position module, together with this stream of sensory inputs, and store each
encountered combination in memory. Essentially, it memorizes the associations between position
and observation. I was at X when I saw Y. But storing memories would be useless if we couldn't
retrieve them. Importantly, since this is an associative memory module, it should be able
to reconstruct the full memory from partial information. For example, we could provide it
with just the position, and it would go and search in all of the stored memories,
which observations were accompanied by this position. So essentially answering the question,
what did I see last time I was here? And similarly, we could provide it with just the
sensory observation, and it would retrieve position. Where was I last time I saw this?
Now we have all the necessary components to solve the prediction problem.
Let's walk step by step on what the trained module will do to come up with a successful
prediction when walking, for example, on a family tree. Remember, it should be capable of learning
any type of structure, not just the four connected grids. So we start on John, transition to Mary
via a sister action, and then to Kate via a daughter action. Finally, we give the model of
the action labeled as uncle, and ask it to make a prediction. And what's happening under the hood
is the following. At first, the position module has some initial belief about the current location,
which is combined with John, and this combination is stored in the memory module. Next, the sister
action is fed into the position module, which comes up with a new belief about location,
that is combined with Mary, and the corresponding conjunction is stored in memory.
Similarly, daughter action is used to update the internal state of the position module,
which is combined with Kate and sent to the memory module. Finally, the uncle action is fed
into the position module. Importantly, the resulting positional information, the pattern of neuron
activations is the same as the one we started with. This is because after the model is trained on many
family trees underlaying by the same rules, the position module is configured to always return
to the same position when we make loops like these. In other words, the general laws governing
the transition logic on the world graph become embedded into the rules of how the position
module updates its state. After performing path integration correctly, we return to this starting
position. But there is no corresponding sensory observation to memorize. Instead, because the
model has reached the end of the sequence, it tries to predict the next observation, but it
has the path integrated position to guide this prediction. So, it queries the memory module
with the positional information and retrieves a sensory observation corresponding to this
particular position, which in our case is John. Awesome, right? So far, we have just been theorizing
about this spherical model in a vacuum. But does it actually, well, work? And if so, what does it
tell us about our own navigational systems? The most direct way to assess how well the model is
performing is to look at its accuracy, which is just the percentage of predictions it made correctly.
And importantly, look at how quickly the accuracy grows. Here is what I mean. Imagine for a moment
that instead of this fancy machine, we had just a good old lookup table, which simply memorizes
all the transitions as pairs. Previous observation plus action equals new observation.
So, it would store memories like John plus sister equals Mary, Mary plus daughter equals Kate, etc.
And to predict the next observation, it would simply scan the lookup table and search for a
particular combination. In the case of our family tree example, on first try, it would not be able
to predict that Kate's uncle is John, because it hadn't encountered this particular combination
previously. In other words, to reach 100% accuracy, it would need to first encounter
all possible combinations of observations and actions. And this means that the performance
of the model depends on the number of edges of this graph that it visited.
In contrast, tolman eigenbau machine, or TEM, doesn't need to be explicitly told at the outcome
of every action from every node, because it has the notion of a structure. For example, if I tell
you that Kate is Mary's daughter, that is enough for you to infer the rest of the relationships
automatically. And this essentially means that to reach 100% accuracy, it is enough for TEM
to just visit all the nodes instead of all possible edges. And hence, its performance depends on the
proportion of nodes visited, which grows much faster than the proportion of edges. So our machine
seems to indeed construct a representation of the world. Hooray! But what's going on inside of its
brain, so to speak? Let's look inside the position module first. Remember, the belief about current
location is encoded with a pattern of collective activation of neurons. But we can also interrogate
individual neurons and look at what each one of them is doing as the agent randomly walks around.
Here, for the sake of visualization, I'm going to show results after the model was trained on
regular four-connected grids, analogs of physical 2D space, rather than social hierarchies.
Remarkably, we see that individual units in the position module develop periodic activity
patterns as a function of position. They tile this space with the regular hexagonal grids
of different size. Or these periodic stripes. Exactly like grid cells and band cells of the
entorhinal cortex encode position in mammalian brains. And the selectivity of individual units
is preserved across environments, suggesting that they indeed can generalize.
Neurons in the memory module do something different. Since they form a conjunction between
positional and sensory information, each neuron would be active when both of the two upstream
components are active. Indeed, units in the memory module resemble hippocampal place cells of various
size, which fire in a particular patch of space. Importantly, just like hippocampal representations
in real brains, their firing patterns differ across environments, since the incoming observations
are different. This is known as hippocampal remapping. I'd like to emphasize that such grid-like
and place-like representations were never hard-coded into the model. We started with
essentially a random set of parameters, let the model optimize itself to come up with the best
solution to the prediction problem, and those responses just emerged naturally. So far, we've
trained the model on sequences that were generated from random walks in a given environment,
which means that all the observations were equally likely. But in the real life, animals don't really
move by diffusion. They are biased towards rewards and exploring objects. They like being near walls
because it feels safe and avoid open spaces. So the question is, if we change the statistics of
these sensory observations so that some stimuli are more common than others, would it affect the
representations that emerge in our model as the optimal solution to the prediction problem?
For example, let's train TEM on sequences of observations that mimic the behavior of a real
mouse, which prefers to spend time near boundaries and approaches objects. In this case, representations
that emerge in the position module now include boundary cells, which are selected to borders of
the world, and object vector cells that seem to activate whenever the animal is at a certain
distance and certain direction away from any object. Both of these types of responses, that,
by the way, also generalize across contexts, are observed experimentally when recording from
entorhinal cortex, while some neurons in the memory module develop selectivity to particular
objects, resembling landmark cells of the hippocampus. If we take a more complex sequence,
such as the one mimicking the animal that is performing an alternation task, the model successfully
learns the rule that the reward is alternating between the sides. Importantly, representations of
some neurons in the memory module resemble the splitter cells found experimentally, which are
modulated by both the position and the direction of the future turn. This suggests that TEM has
the capacity to learn and map latent spaces, which are not directly given to it in the observations.
Another example of how TEM maps latent space is available as a bonus clip to my Patreon supporters.
More details at the end of this video. Terrific! Now we have a model that can generalize and
naturally develops same representations of space as the hippocampal formation. So what insights can
we draw from it? Recall that clay cells remap, which means they change their preferred firing
locations in different environments. This process has not been thought to be random,
since there is no immediate logic in how these representations drift around.
But having a model of hippocampal formation at hand, we can start to address this question
on a whole other level. Notice that neurons in our memory module, the ones that resemble
clay cells, are actually conjunctions between sensory and structural information. This means
that firing of a particular clay cell is partially controlled by grid cells, which provide the
structural information. So for example, if in one environment, the location of a given clay cell
coincides with the hexagonal activity pattern of a particular grid cell, then after we change
the surroundings and the clay cell remaps, its place field will shift to another location which
also lies on this grid. In other words, remapping is not completely random, but rather is controlled
by grid cells, preserving some structural information. This relationship between the
locations of place and grid cells implies that there should be a correlation between the degree
to which firing locations of place and grid cells coincide across two environments.
This is the case in the model, and remarkably, when the authors tested this prediction on
experimental data, they found it to be true in real brains as well.
Well, I know this was a ton of information to process, so let's try to tie everything together.
The problem of constructing internal models of the world is cornerstone for both biological
and artificial intelligence. It can be solved by factorizing the surrounding into building blocks
and combining them with particular sensory contexts to generate new models on the go,
allowing for rapid generalization. This factorization and composition can be demonstrated
in a computational model which, when tasked with predicting the next observation in a sequence,
learns the underlying relational structure of the world. Representations that naturally emerge
in this model resemble real neurons found in the hippocampal formation, suggesting a unified
framework of interactions between entorhinal cortex and hippocampus.
I want to take this opportunity to give huge thanks to Dr. James Whittington,
the first author of the original TAM paper, and Gus, my friend and fellow patron with
an expertise in machine learning, who both helped me immensely with preparing the script for this
video. As a final note, I will mention that the Tolman Eigenbaum machine we've seen today
is actually very similar to a transformer architecture, a type of neural network that
is at the core of modern machine learning. In fact, with one little modification,
we can turn this similarity into a precise mathematical equivalence. And this modified
version, called the Tolman Eigenbaum machine transformer, learns much faster and performs
better, while still resembling biological representations for the most part. This potentially
provides a very promising link between neuroscience and modern machine learning,
which makes both fields even more exciting than ever. Now, I know this was a very simplified
description, but fully exploring this equivalence would require going over the transformer and
Hopefield networks in detail. Let me know down in the comment section if you would like to see a
more technical video of this kind. In the meantime, if you're interested in machine learning and don't
want to wait any longer, let me tell you about something that can take your understanding to the
next level. Brilliant.org. Brilliant is a revolutionary platform for engaging and interactive learning.
Gone are the days of passive textbook reading. With Brilliant, you'll engage with the material
in a hands-on way, solving problems, answering questions and participating in stunning interactive
visualizations, which help you develop an intuitive understanding of the material.
One course that you might find particularly interesting after watching this video is titled
Artificial Neural Networks. It offers an accessible introduction into the world of artificial intelligence
and how it is inspired by the human brain. You will learn how neural networks work,
how to build your own and even how to train them to recognize patterns.
But that's just the tip of the iceberg. With over 80 courses to choose from,
Brilliant has something for everyone. And with its personalized approach,
you can learn at your own pace with bite-sized chunks. Take your curiosity to the next level
today. Go to brilliant.org slash Artem Korsanov to get a 30-day free trial of everything Brilliant
has to offer. And the first 200 people to use this link will get 20% off the premium subscription.
If you enjoyed this video, press the like button, share it with your friends and colleagues,
subscribe to the channel if you haven't already and consider supporting me on Patreon
to suggest video topics and enjoy the bonus content.
Stay tuned for more interesting topics coming up.
Goodbye and thank you for the interest in the brain.

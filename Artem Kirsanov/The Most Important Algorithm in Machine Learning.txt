What do nearly all machine learning systems have in common, from GPT and mid-journey to
AlphaFold and various models of the brain?
Despite being designed to solve different problems, having completely different architectures
and being trained on different data, there is something that unites all of them.
A single algorithm that runs under the hood of the training procedures in all of those
cases.
This algorithm, called backpropagation, is the foundation of the entire field of machine
learning, although its details are often overlooked.
Surprisingly, what enables artificial networks to learn is also what makes them fundamentally
different from the brain and incompatible with biology.
This video is the first in a two-part series.
Today, we will explore the concept of backpropagation in artificial systems and develop
an intuitive understanding of what it is, why it works and how you could have developed
it from scratch yourself.
In the next video, we will focus on synaptic plasticity, enabling learning in biological
brains and discuss what the backpropagation is biologically relevant and, if not, what
kind of algorithms the brain may be using instead.
If you're interested, stay tuned.
Despite its transformative impact, it's hard to say who invented backpropagation in the
first place, as certain concepts can be traced back to Leibniz in 17th century.
However, it is believed that the first modern formulation of the algorithm, still in use
today, was published by Seppo Linenma in his master's thesis in 1970, although he did
not reference any neural networks explicitly.
Another significant milestone occurred in 1986, when David Rumelhardt, Geoffrey Hinton
and Ronald Williams published a paper titled Learning Representations by Backpropagating
Errors.
They applied the backpropagation algorithm to multi-layer perceptrons, a type of a neural
network, and demonstrated for the first time that training with backpropagation enables
the network to successfully solve problems and develop meaningful representations at
the hidden neuron level, capturing important regularities in the task.
As the field progressed, researchers scaled up these models significantly and introduced
various architectures, but the fundamental principles of training remained largely unchanged.
To gain a comprehensive understanding of what exactly it means to train a network, let's
try to build the concept of backpropagation from the ground up.
Consider the following problem.
Suppose you have collected a set of points x, y on the plane, and you want to describe
their relationship.
To achieve this, you need to fit a curve y of x that best represents the data.
Since there are infinitely many possible functions, we need to make some assumptions.
For instance, let's assume we want to find a smooth approximation of the data using a
polynomial of degree 5.
That means that the resulting curve we are looking for will be a combination of a constant
term, a polynomial of degree 0, a straight line, a parabola, and so on up to a power
of 5, each weightened by specific coefficients.
In other words, the equation for the curve is as follows, where each k is some arbitrary
real number.
Our job then becomes finding the configuration of k0 through k5, which leads to the best
fitting curve.
To make the problem totally unambiguous, we need to agree on what the best curve even
means.
While you can just visually inspect the data points and estimate whether a given curve
captures the pattern or not, this approach is highly subjective and impractical when dealing
with large datasets.
Instead, we need an objective measurement, a numerical value that quantifies the quality
of a fit.
One popular method is to measure the square distance between data points and the fitted
curve.
A high value suggests that the data points are significantly far from the curve, indicating
a poor approximation.
Conversely, low values indicate a better fit, as the curve closely aligns with the data
points.
This measurement is commonly referred to as a loss, and the objective is to minimize
it.
Now, notice that for a fixed data, this distance, the value of the loss, depends only on the
defining characteristics of the curve.
In our case, the coefficients from k0 through k5.
This means that it is effectively a function of parameters, so people usually refer to
it as a loss function.
It's important not to confuse two different functions we are implicitly dealing with here.
The first one is the function y of x, which has one input number and one output number,
and defines the curve itself.
It has this polynomial form, given by k's.
There are infinitely many such functions, and we would like to find the best one.
To achieve this, we introduce a loss function, which instead has six inputs, numbers k0 through
k5.
And for each configuration, it constructs the corresponding curve y, calculates the distance
between observed data points and the curve, and outputs a single number, the particular
value of the loss.
Our job then becomes finding the configuration of k's that yields a minimum loss, or minimizing
the loss function with respect to the coefficients.
Then, plugging these optimal k's into the general equation for the curve, will give
us the best curve describing the data.
Alright, great, but how do we find this magic configuration of k's that minimizes the loss?
Well, we might need some help.
Let's build a machine called Curve Fitter 6000, designed to simplify manual calculations.
It is equipped with six adjustable knobs, for k0 through k5, which we can freely turn.
To begin, we initialize the machine with our data points, and then, for each setting of
the knobs, it will evaluate the curve y of x, compute the distance from it to the data
points, and print out the value of the loss function.
Now we can begin twisting the knobs in order to find the minimum loss.
For example, let's start with some initial setting, and slightly notch knob number one
to the right.
The resultant curve changed as well, and we can see that the value of the loss function
slightly decreased.
Great, it means we are on the right track.
Let's turn knob number one in the same direction once again, uh oh, this time the fit gets
worse and the loss function increases.
Apparently, that last notch was a bit too much, so let's revert the knob to the previous
position and try knob two.
And we can keep doing this iteratively many, many times, nudging each individual knob one
at a time to see whether the resultant curve is a better fit.
This is a so-called random perturbation method, since we are essentially wandering in the
dark, not knowing in advance how each adjustment will affect the loss function.
This would certainly work, but it's not very efficient.
Is there a way we can be more intelligent about the knob adjustments?
In the most general case, when the machine is a complete black box, nothing better than
a random perturbation is guaranteed to exist.
However, a great deal of computations, including what's carried out under the hood of our
Curve Fitter, have a special property to them, something called differentiability that allows
us to compute the optimal knob setting much more efficiently.
We will dive deeper into what differentiability means in just a minute.
But for now, let's quickly see the big picture overview of where we are going.
Our goal would be to upgrade the machine so that it would have a tiny screen next to
each knob.
And for any configuration, those screens should say which direction you need to nudge each
knob in order to decrease the loss function and by how much.
Think about it for a second.
We are essentially asking the machine to predict the future and estimate the effect of the
knob adjustment on the loss function without actually performing that adjustment, calculating
the loss and then reverting the knob back, like we did previously.
Wouldn't this glance into the future violate some sort of principle?
After all, we are jumping to the result of the computation without performing it.
Sounds like cheating, right?
Well, it turns out that this idea lies on a very simple mathematical foundation, so
let's spend the next few minutes building it up from scratch.
Alright, let's consider a simpler case first, where we freeze five out of six knobs.
For example, suppose someone tells you that the rest of them are already in the optimal
position.
So all you need to do is to find the best value for one remaining knob.
Essentially, the machine now has only one variable parameter, k1, that we can tweak.
And so the loss function is also a simpler function, which accepts one number, the knob
setting, and outputs another number, the loss value.
As a function of one variable, it can be conveniently visualized as a graph in a two-dimensional
plane, which captures the relationship between the input and the output.
For example, it may have this shape right here, and our goal is to find this value of
k1, which corresponds to the minimum of the loss function.
But we don't have access to the true underlying shape.
All we can do is to set the knob at a chosen position, and kind of query the machine for
the value of the loss.
In other words, we can only sample individual points along the function we are trying to
minimize.
And we are essentially blind to how the function behaves in between the known points before
we sample them.
But suppose we would like to know something more about the function, not just each value
at each point.
For example, whether at this point the function is going up or down.
This information will ultimately guide our adjustments, because if you know that the
function is going down as you increase the input, turning the knob to the right is a
safe bet, since you are guaranteed to decrease the loss with this manipulation.
Let's put this notion of going up or down around a point on a stronger mathematical
ground.
Suppose we have just sampled the point x0, y0 on this graph.
What we can do is increase the input by a small amount, delta x.
This new adjusted input will result in a new value of y, which will differ from the old
value by some delta y.
This delta depends on the magnitude of our adjustment.
For example, if we take a step at delta x, which is 10 times smaller, delta y will also
be approximately 10 times as small.
This is why it makes sense to take the ratio delta y over delta x, the amount of change
in the output per unit change in the input.
Graphically, this ratio corresponds to a slope of a straight line going through the
points x0, y0 and x0 plus delta x, y0 plus delta y.
Notice that as we take smaller and smaller steps, this straight line will more and more
accurately align with the graph in the neighborhood of the point x0, y0.
Let's take a limit of this ratio as delta x goes to infinitely small values.
Then, this limiting case value, which this ratio converges to for infinitesimally small
delta x's, is what is called the derivative oa function, and it is denoted by dy over
dx.
Visually, the derivative oa function at some point is the slope of the line that is tangent
to the graph.
And thus corresponds to the instantaneous rate of change or steepness of that function
around that point.
But different points along the graph might have different steepness values, so the derivative
of the entire function is not a single number.
In fact, the derivative dy by dx is itself a function of x that takes an arbitrary value
of x and outputs the local steepness of y of x at that point.
This definition assigns to every function its derivative alter ego another function operating
on the same input domain, which carries information about the steepness of the original function.
There is a bit of a subtlety.
Strictly speaking, the derivative may not exist if the function doesn't have a steepness
around some point.
For example, if it has sharp corners or discontinuities.
However, for the remainder of the video, we are going to assume that all functions we
are dealing with are smooth, so that the derivative always exists.
This is a reasonable claim, because we can control what sort of functions go into our
models when we build them.
And people usually restrict everything to smooth or differentiable functions to make
all the math work out nicely.
All right, great.
Now, along with the underlying loss as a function of k1, which is hidden from us, we can also
reason about its derivative.
Another function of k1, which we also don't know, that is equal to the steepness of the
loss function at that point.
Let's suppose that similarly to how we can query the loss function by running our machine
and obtaining individual samples.
There is a mechanism for us to sample the derivative function as well.
So, for every input value of k1, the machine will output the value of the loss and the
local steepness of the loss function around that point.
Notice that this derivative information is exactly the sort of look into the future we
were looking for to make smarter knob adjustments.
For example, let's use it to efficiently find the optimal value of k1.
What we can do is the following.
First, start at some random position.
Ask the machine for a value of the loss and the derivative of the loss function at that
position.
Take a tiny step in the direction opposite of the derivative.
If the derivative is negative, it means that the function is going down.
And so, if we want to arrive at the minimum, we need to move in the direction of increasing
value of k1.
Repeat this procedure until you reach the point where the derivative is zero, which essentially
corresponds to the minimum where the tangent line is flat.
Essentially, each adjustment in such a guided fashion works kind of like a ball rolling
down the hill along the graph, until it reaches a value.
So in the beginning, we froze five out of six knobs for simplicity.
This process is easily carried out to higher dimensions.
For example, suppose now we are free to tweak two different knobs, k1 and k2.
The loss would become a function of two variables, which can be visualized as a surface.
But what about the derivative?
Recall that by definition, the derivative at each point tells us how the output changes
per unit change of the input.
But now we have two different inputs.
Should we nudge only k1, k2 or both?
Essentially, our function will have two different derivatives that are usually called partial
derivatives because of this ambiguity which input to nudge.
Namely, when we have two knobs, the derivative of the loss function with respect to parameter
k1 is written like this.
It is how much the output changes per unit change in k1 if you hold k2 constant.
And conversely, this expression tells you the rate of change of the output if you hold
k1 constant and slightly nudge k2.
Geometrically, you can imagine slicing the surface with planes parallel to the axes intersecting
at the point of interest k1, k2, so that each of the two cross sections is like a one-dimensional
graph of the loss as a function of one variable while the other one is kept constant.
Then the slope of a tangent line at each cross section will give you a corresponding partial
derivative of the loss at that point.
While thinking about partial derivatives as two separate surfaces, one for each variable,
is a perfectly valid way, people usually plug the two different values into a vector called
a gradient vector.
Essentially, this is a mapping from two input values to another two numbers where the first
signifies how much the output changes per tiny change in the first input, and similarly
for the second input.
Geometrically, this vector points in the direction of steepest ascent, so if you want to minimize
a function, like in the case for our loss, we need to take steps in the direction opposite
to this gradient.
This iterative procedure of nudging the parameters in the direction opposite of the gradient
vector is called gradient descent, which you have probably heard of.
This is analogous to a ball rolling down the hill for the two-dimensional case, and the
partial derivatives essentially tell you which direction is downhill.
Going beyond two dimensions is impossible to visualize directly, but the math stays exactly
the same.
For instance, if we are now free to tweak all the six knobs, the loss function is a
hyper surface in six dimensions, and the gradient vector now has six numbers packed into it.
But it still points in the direction of steepest ascent, so if we iteratively take small steps
in the direction opposite to it, we are going to roll the ball down the hill in six dimensions
and eventually reach the minimum of the loss function.
Great, let's back up a bit.
Remember how we were looking for ways to add screens next to each knob that would give
us the direction of optimal adjustment?
Well, it is essentially nothing more but the components of the gradient vector, if at a
particular configuration the partial derivative of the loss with respect to k1 is positive.
It means that increasing k1 will lead to increased loss, so we need to decrease the
value of the knob by turning it to the left, and similarly for all other parameters.
This is how the derivatives serve as these windows into the future by providing us with
information about local behavior of the function, and once we have a way of accessing the derivative,
we can perform gradient descent and efficiently find the minimum of the loss function, thus
solving the optimization problem.
However, there is an elephant in a room.
So far we have implicitly assumed the derivative information is given to us, or that we can
sample the derivative at a given point, similarly to how we sample the loss function itself
by running the calculation of the machine.
But how do you actually find the derivative?
As we will see further, this is the main purpose of the back propagation algorithm.
Essentially, the way we find derivatives of arbitrarily complex functions is in the following.
First, there are a handful of building blocks to begin with.
Simple functions, derivatives of which are known from calculus.
These are the kind of derivative formulas you often memorize in college.
For example, if the function is linear, it's pretty clear that its derivative will be a
constant, equal to the slope of that line everywhere, which coincides with its own tangent line.
A parabola x squared becomes more steep as you increase x, and its derivative is actually
2x.
In fact, there is a more general formula for the derivative of x to the power of n.
Similarly, derivatives of the exponent and logarithm can be written down explicitly.
But these are just individual examples of simple, well-known functions.
In order to compute arbitrary derivatives, we need a way to combine such atomic building
blocks together.
There are a few rules how to do it.
For instance, the derivative of a sum of two functions is the sum of the derivatives.
There is also a formula for the derivative of a product of two functions.
This gives you a way to compute things like the derivative of 3x squared minus e to the
power of x.
But to complete the picture and to be able to find derivatives of almost everything, we
need one other rule, called the chain rule, which powers the entire field of machine learning.
It tells you how to compute the derivative of a combination of two functions, when one
of them is an input to another.
Here is a way to reason about this.
Suppose you take one of those simpler machines, which receives a single input x that you can
vary with an arp, and spits out an output, j of x.
Now, you take a second machine of this kind, which performs a different function, f of x.
What would happen if you connect them in sequence, so that the output of the first machine is
fed into the second one as an input?
Notice that such a construction can be thought of as a single function, which also receives
one input number and gives an output by computing a more complicated function, which is a composition
of the two simpler functions.
In fact, if you put a black box around it to conceal the fact that there are actually
two machines operating sequentially, you can treat it as a single machine and ask, well,
if I notch the input on one end, how will it affect the output on another end?
In other words, what is the derivative of the resulting function?
Suppose we know the individual derivatives of the two machines, f and j.
If the knob is set at some value x, local steepness of the first function is evaluated
at x.
However, the number that is fed into the second machine is not x, because it was already processed
by the first function.
So the thing that is being plugged into the second function is j of x.
And so the local rate of change of the second machine is thus the derivative of f evaluated
at the point j of x.
Now imagine you notch the knob x by a tiny amount, delta.
That input notch, when it comes out of the first machine, will be multiplied by the derivative
of j, since the derivative is the rate of change in the output per unit change of the
input.
And after the first function, the output will increase by delta, multiplied by the derivative
of j.
This expression is essentially a tiny notch in the input to the second machine, whose
derivative at that point is given by this expression.
This means that for each delta increase in the input, we bump the output by this much.
Hence the derivative, when you divide that by delta, will look like this.
You can think about it as a set of three interconnected cog wheels, where the first one represents
the input knob x.
And the other two wheels are functions, j of x and f of j of x respectively.
When you notch the first wheel, it induces a notch in the middle wheel, and the amplitude
of that change is given by the derivative of j, which in turn causes the third wheel
to rotate, and the amplitude of that resulting notch is given by chaining the derivatives
together.
Alright, great, now we have a straightforward way of obtaining a derivative of any arbitrarily
complex function, as long as it can be decomposed into building blocks, simple functions with
explicit derivative formulas, such as summations, multiplications, exponents, logarithms, etc.
But how can it be used to find the best curve using our curve fitter?
The big picture we are aiming for is the following.
For each of our parameter knobs, we will write down its effect on the loss, in terms of simple
easily differentiable operations.
Because we have that sequence of building blocks, no matter how long, we should be able
to sequentially apply the chain rule to each of them in order to find the value of the
derivative of the loss function with respect to each of the input knobs and perform iterative
gradient descent to minimize the loss.
Let's see an example of this.
First, we are going to create a knob for each number the loss function can possibly depend
on.
This obviously includes the parameters, but there is also the data itself, coordinates
of points to which we are fitting the curve in the first place.
Now, during optimization, the data points are set in stone, so changing them in order
to obtain a lower loss would make no sense.
However, for conceptual purposes, we can think about these values as fixed knobs set in one
position so that we cannot nudge them.
Once we have all the existing numbers being fed into the machine, we can start to break
down the loss calculation.
Remember, by definition, it is the sum of squared vertical distances from each point
to the curve parameterized by k's.
So, for instance, let's take the first data point, x1, y1.
y, the x-coordinate by k1, add that to the squared value of x1 multiplied by k2 and
so on for other k's, including the constant term k0.
This sum of weight and powers of x1 is the value of y predicted by the current curve
f of x1.
Let's call it y1 hat.
Next, we need to take the squared difference between the actual value and the predicted
value.
This is how much the first data point contributes to the resulting value of the loss function.
Repeating the same procedure for all remaining data points and summing up the resulting
squared distances gives us the overall total loss that we are trying to minimize.
The computation we just performed, finding the value of the loss for a given configuration
of parameter and data knobs, is known as the forward step.
The entire sequence of calculations can be visualized as this kind of computational
graph where each node is some simple operation like addition or multiplication.
Backward step then corresponds to computations flowing from left to right.
But to perform optimization, we also need information about gradients, how each node
influences the loss.
Now we are going to do what's known as the backward step and unroll the sequence of
calculations in reverse order to find derivatives.
What makes the backward step possible is the fact that every node in our compute graph
is an easily differentiable operation.
Think of individual nodes as these tiny machines which simply add, multiply or take powers.
We know their derivatives and because their outputs are connected sequentially, we can
apply the chain rule.
This means that for each node we can find its gradient, the partial derivative of the
output loss with respect to that node.
Let's see how it can be done.
Consider a region of the compute graph where two number nodes A and B are being fed into
a machine that performs addition and its result A plus B is further processed by the system
to compute the overall output L.
Suppose we already computed the gradient of A plus B earlier so that we know how nudging
the sum will affect the output.
The question is, what are individual gradients of A and B?
Well, intuitively, if you nudge A by some amount, A plus B will be nudged by the same
amount, so the gradient or the partial derivative of the loss with respect to A is the same
as the gradient of the sum, and similarly for B.
This can be seen more formally by writing down the chain rule and noticing that the derivative
of A plus B with respect to A is just one.
In other words, when you encounter this situation in the compute graph, then the gradient of
the sum just simply propagates into the gradients of the nodes that plug into the sum machine.
Another possible scenario is when A and B are multiplied.
Just like before, suppose we know the gradient of their product because it was computed before.
In this case, individual nudged to A will be scaled by a factor of B, so the product
will be nudged B times as much, which propagates into the output.
So whatever the derivative of the output with respect to the product of A B is, the output
derivative with respect to A will get scaled by a factor of B, and vice versa for the gradient
of B. Once again, it can be seen more formally by examining the chain rule.
In other words, the multiplication node in the compute graph distributes the downstream
gradient across incoming nodes by multiplying it crossways by their values.
Similar rules can be easily formulated for other building block calculations, such as
raising a number to a power or taking the logarithm.
Finally, when a single node takes part in multiple branches of the compute graph, gradients
from the corresponding branches are simply added together.
Indeed, suppose you have the following structure in the graph, where the C-minode A plugs into
two different operations that contribute to the overall loss.
Then if you nudge A by delta, the output will be simultaneously nudged by this derivative
from the first branch and this derivative from the second branch.
Until the overall effect of nudging A will be the sum of the two gradients.
Alright, great.
Now that we have constructed a computational graph and established how to process individual
chunks of it, we can just sequentially apply those rules starting from the output and working
our way backwards.
For instance, the rightmost node in the graph is the resulting value of the loss function.
How does the incremental change in that node affect the output?
Well it IS the output, so its gradient is by definition equal to 1.
Next, the loss function is the sum of many delta y's squared.
We know what to do with the summation node.
It just copies whatever the gradient value is to the right of it into all incoming nodes.
Consequently, the gradients of all delta y's squared will also be equal to 1.
Each of those nodes is the squared value of the corresponding delta y and we know how
to differentiate the squaring operation.
The derivative of the loss function with respect to delta y1 will be 2 times the delta y1,
which is just the number we found during the forward calculation.
And we can keep doing this propagation of sequential derivative calculation backwards
along our compute graph until we reach the leftmost nodes, which are the data and parameter
knobs.
The derivatives of the loss with respect to the input data don't really matter, but
the derivatives with respect to the parameters is exactly what we want.
Once these parameter gradients are found, we can perform one iteration of gradient descent.
Namely, we are going to slightly tweak the knobs in the directions opposite to the gradient.
The exact magnitude of each adjustment being the negative product of the gradient and some
small number called the learning rate, for example 0.01.
Note that after the adjustment is performed, the configuration of the machine and the resulting
loss are different.
And so the old gradient values we found no longer hold.
So we need to run the forward and backward calculations once again to obtain updated
gradients and the new decreased loss.
Performing this loop of forward pass, backward pass, notch, repeat is the essence of training
every modern machine learning system.
And exactly the same algorithm is used today in even the most complicated models.
As long as the problem you are trying to solve with a given model architecture can
be decomposed into individual operations that are differentiable, you can sequentially
apply the chain rule many times to arrive at the optimal setting of the parameters.
For instance, a feed-forward neural network is essentially a bunch of multiplications
and summations with a few non-linear activation functions sprinkled between the layers.
Each of those atomic computations is differentiable, so you can construct the compute graph and
run the backward pass on it to find how each parameter, like connection weights between
neurons, influence the loss function.
And because neural networks, given enough neurons, can in theory approximate any function
imaginable, we can create a large enough sequence of these building block mathematical machines
to solve problems such as classifying images and even generating new text.
This seems like a very elegant and efficient solution.
After all, if you want to solve the optimization problem, derivatives tell you exactly which
adjustments are necessary.
But how similar is this to what the brain actually does?
When we learn to walk, speak and read, is the brain also minimizing some sort of loss
function?
Does it calculate derivatives?
Or could it be doing something totally different?
In the next video, we are going to dive into the world of synaptic plasticity and talk
about how biological neural networks learn.
In keeping with the topic of biological learning, I'd like to take a moment to give a shout-out
to Shortform, a longtime partner of this channel.
Shortform is a platform which lets you take your reading to the next level.
They offer book guides, which are supercharged book summaries.
Not only do you get the condensed version of all the key points, but they are also supplemented
by ideas from related sources, such as other books and research papers.
I really love this feature because it allows you to get the big picture overview and promotes
the interlinking of ideas.
The existing library contains books from a variety of genres, including science, education
and technology, and new books are being added every week.
Personally, I found Shortform to be really valuable when it comes to choosing books to
read, as well as taking efficient notes.
Don't hesitate to give it a try by following the link down in the description to get five
days of unlimited access and 20% off the annual membership.
If you enjoyed this video, press the like button, share it with your friends and colleagues
and subscribe to the channel if you haven't already.
Stay tuned for more interesting topics coming up.
Goodbye and thank you for the interest in the brain.

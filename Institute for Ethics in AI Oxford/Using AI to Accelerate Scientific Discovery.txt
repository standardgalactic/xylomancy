Fy diemwys.
I'm also a Professor of Computer Science here in Oxford and chair the Institute Steering Group.
It was my privilege to help set up the Institute which brings together world leading philosophers
and other experts in the humanities with the researchers, developers and users of AI.
The director of the Institute is Professor John Tosulis and its ultimate home will be
the Stephen A. Schwarzman Centre for the Humanities, whose construction is soon to start.
In recent years, AI has gone from strength to strength. It's now ubiquitous.
In our phones, the games we play, in our cars, our drug discovery companies, the search engines we use
and the translation tools we depend on.
Much of that is down to a new generation of AI methods and techniques that are powered
by modern machine learning algorithms, great swathes of data and the prodigious power
of modern-day computing hardware.
Some of AI's most dramatic recent accomplishments owe a great deal to our speaker here with us this evening
and the company he co-founded.
Demisa Sabes, CEO and co-founder of DeepMind, one of the world's leading AI research companies.
Demisa's own career and intellectual journey is an extraordinary one.
A chess prodigy, hugely successful computer games developer with a double first in computer science from Cambridge.
Demis has always been fascinated by the human brain, understanding how it gives rise to intelligence.
After the success of his games companies, he went on to a PhD in cognitive neuroscience at UCL,
followed by a Henry Welcombe postdoctoral research fellowship at the Gatsby Computational Neuroscience Unit, also at UCL.
His papers in cognitive neuroscience investigated imagination, memory and amnesia
and appeared in leading journals such as Nature and Science.
He combined his interest in computing and neuroscience with the formation of DeepMind in 2010.
It's compelling ambition to solve intelligence and then use intelligence to solve everything else.
He and his team used games as the context in which to test new ideas about how to build AI systems
using machine learning methods inspired by neuroscience.
First arcade games and then famously Go.
A previous talk here in the Sheldonian in February 2016 prefigured AlphaGo winning 4-1 against
former world champion Liso Dahl just a month later.
Games have proven to be a great training ground for developing and testing AI algorithms,
but the aim of DeepMind has always been to build general learning systems
ultimately capable of solving important problems in the real world.
DeepMind's AlphaFold system is a solution to the 50 year grand challenge of protein structure prediction
culminating the release of the most accurate and complete picture of the human proteome.
A core aim for the Institute for Ethics in AI is to bring together world leading academics
and the practitioners at the cutting edge of AI development.
Tonight we will hear first hand experience of AI's enormous potential to accelerate scientific discovery.
Experience which will inform our research and thinking about the critical ethical considerations
that must be considered by policy makers and technical developers of AI.
DEMIS has predicted that artificial intelligence will be one of the most beneficial technologies ever
but that significant ethical issues remain.
Please join me in welcoming DEMIS her servers to deliver tonight's Tana lecture using AI to accelerate scientific discovery.
Thank you, thank you so much for such a great introduction and it's a real pleasure to be back here in Oxford in the Shadonian
and giving the Tana lecture as a real honour.
So what I'm going to talk about today is using AI to accelerate scientific discovery
and in fact as you'll see throughout my talk this was my original motivation
and has always been my motivation behind spending my entire career and trying to make AI a reality.
I'm going to talk a lot about some of our most recent advances actually now coming to fruition
especially the last year or two of using AI to crack difficult scientific problems
but I'm also going to talk about the lead up to there and how I think about the games work we did originally
and last time I talked here was just before the AlphaGo match in Korea
so that was kind of a major moment for us and how in the last even five, six years things have progressed enormously.
So just to sort of talk a little bit about what our vision was behind DeepMind back in 2010
it's quite hard to remember the state of AI back in 2010 because today as Nigel was saying that AI is ubiquitous all around us
it's one of the biggest buzzwords in industry.
It's sort of hard to remember just 12 years ago almost nobody was talking about AI I would say
and it was almost impossible to actually get funding in the private sector for AI at all
and we have many funny stories back in the day of trying to do some fundraising back in 2009 and 10
and most people thinking we were completely mad to be embarking on this journey.
But we founded it with this in mind of trying to build one day an Apollo programme like effort to build AGI,
artificial general intelligence and we use this term artificial general intelligence
to distinguish it from sort of normal everyday AI where we're talking about a general system
that can perform well on many tasks to at least human level
and that's the sort of general aspect that we are always striving for in all the work that we do.
So we're still on this mission now and I think we've done a pretty good job of basically staying true to this original vision
that we had in 2010 when we were just a few people in a small little office in an attic in Russell Square.
So as Nigel said our original mission statement was step one solve intelligence, step two use it to solve everything else.
We have updated that mission statement a little bit, still means the same thing
but just to be a little bit more descriptive now in the last few years
just to be a bit clearer about what we mean by solving everything else, what exactly are we talking about
and so the way we discuss our mission now is solving intelligence to advance science
and of course for the benefit of humanity and that's always been the cornerstone of what we think about
when we think about what should we apply AI to.
Now there are two, broadly two ways that I think AI can be attempted to be built.
One is the sort of I guess more traditional way of building logic systems or expert systems
and these are hard coded systems that effectively teams of programmers solve the problem
they then incorporate those solutions in sometimes very clever expert systems
but the problem with them is that they are very limited in terms of what they can generalise to
so they can't deal with the unexpected and they're basically limited to what the programmers
foresaw the situations that the system might be in
and of course this line of work was inspired by mathematics and logic systems.
On the other hand the big renaissance in the last decade plus is the sort of progress of learning systems
of course in the 80s there was a flurry of work done on neural networks
then that died down. We now know that probably we didn't have enough computing power or data
maybe not the right algorithms as well but basically in essence the ideas were correct.
So an idea of a learning system is that it learns it's for itself, solutions for itself
from first principles directly from experience
and the amazing thing about these systems and their huge promise is that they can maybe generalise to tasks
and that it's not being programmed for explicitly and maybe solve problems that we ourselves as the designers
or scientists behind those systems don't know how to solve.
So of course that's the huge potential and also the risk of these kinds of systems
and originally these kind of learning systems took a lot of inspiration
and also could be validated some of the ideas like reinforcement learning and neural networks
by systems neuroscience and comparing what these systems do
comparing them on a systems and algorithmic level to what we know about how the brain works.
Now everything we do at DeepMind of course is on the learning systems side
and we've been lucky enough to be in the vanguard of this almost revolution or renaissance
in the last decade of these types of approaches.
So how do we think about what's our special take on learning systems and how powerful they can be?
So there are kind of two component algorithms or approaches one could say that we've fused together.
So of course there's deep learning or deep neural networks
and the way I think about this is that the deep neural network system is there to build a model of the environment
of the data and the experience and then what do you use that model for?
Well you can use reinforcement learning which is a sort of goal seeking and reward maximising system
to you can use that model and use it to plan and basically plan and take actions towards a goal
a goal that may be specified by the designers of that system.
So you have the model and then you have the action and goal solving element of the systems.
So we and one of our early innovations was to sort of fuse those two things together at scale
we call it deep reinforcement learning now
and the cool thing about these systems is that they can discover new knowledge from first principles
through this process of trial and error using these models.
So the idea here on this diagram of the agent system is it gets observations from the environment
those observations go towards building and updating an internal model of how the environment works
and the transition matrices of the environment.
There's some goal it's trying to solve in the environment and achieve
and then after its thinking time has run out it has to select an action from the action set
available to it at that moment in time that will best get it incrementally towards its goal
and then the action gets output it may or may not make a change in the environment
that drives a new observation and then the model updates further.
So you can see with this type of system the agent is actually the AI system is actually an active learner.
It participates in its own learning so the decisions it makes in large part governs what experiences
and what data it will get next to learn more from.
So although this is a pretty simple diagram and basically describes the whole of reinforcement learning
problem there's huge complexities of course of theoretical and practical complexities underlying
this diagram that need to be solved.
But we know that in the limit this must work because this is how mammalian brains work
and including humans this is one of the learning mechanisms that we have in our own brains
reinforcement learning was found to be implemented by dopamine neurons in the brain in the late 90s.
So we know if we push this hard enough this should be one path towards general artificial intelligence.
So what do we famously use this for?
AlphaGo was the program that I think we did a lot of things before this like Atari games
and other proof points but AlphaGo was really our first attempt at doing this at huge scale
to crack a big problem that was unsolved in AI kind of one of the holy grails of AI research.
Which is a program to beat the world champion at the game of go.
And I want to talk a little bit about this in hindsight now knowing what I know now how I've reinterpreted
what we did with AlphaGo and I think I can explain it in a much more simple in general way
than perhaps you know how I was explaining it back five six years ago when we were in the midst of building this system.
So just for those of you who don't know why that's not updating.
So this is the game of go.
This is the game of go the board game and it's a phenomenal game and it's it's much more esoteric game
and artistic game one could say than chess.
So occupies the same intellectual echelon chess stars in in in the west in China in Japan and Korea
and other Asian countries they play go and goes resisted sort of old fashioned logic system
and expert system approaches whereas chess was solved by those things because of various factors.
One is the search space is truly enormous in go it's roughly 10 to the power 170 possible board positions
which is way more than there are atoms in the universe.
So is no way one could exhaustively search all of the possible board positions in order to find the right path through.
Even bigger problem actually is that it's impossible or thought was it was impossible to write down an evaluation function
to sort of hand code an evaluation function which is what most modern day chess programs use.
So and the reason is because go is such an esoteric game where it doesn't have materiality in chess.
You know the first approximation one can add up the piece values on both sides and that will tell you very crudely like who which side is winning in that position.
And obviously you need to know that in order to make decisions about what to do next.
So many people are tempted to over 20 years since deep blue attempted to write to construct these evaluation functions for go.
And one of the issues is is that go players themselves do not know consciously at least what that information is.
So because it's so complex the game they actually use their intuition rather than explicit calculation in order to deal with the complexity of go.
Whereas chess players if you ask them you know how why did they make a decision a chess grandmaster will tell you will be able to tell you explicitly the various factors involved.
A go player generally won't do that.
They'll just say things like it felt right.
This felt like the right move which is what I think also makes go an incredible game.
But of course intuition is not something one would associate with computer programs especially logic systems.
And maybe in the Q&A we can discuss a little bit more about what intuition may be.
But I don't think it's it's sort of I don't think it's my conclusion now after doing all these games.
And indeed some of the science things we've done is that it's not some mysterious thing.
It's actually information that our brain knows about and has learned through experience of course.
I mean there's no other way one can learn information.
But it's just it's in the association courtesies.
So it's not actually consciously available to a high level cortex.
So it seems mysterious to us how we ride a bike how we swim these sort of motor sensory motor things we're able to do because our conscious part of our brain cannot access those representations.
So and if we can't do that then we definitely can't explicitly code it in some logic code which is why traditionally those tasks including things like computer vision have been quite hard for logic systems.
To solve even over the last 50 years.
So a lot about what we were doing was trying to approximate this kind of intuition in these learning systems.
So how did we work and I'm actually going to describe not just AlphaGo here but the whole series of AlphaX programs AlphaGo, the original one that beat Lisa Doll in 2016.
And then AlphaGo zero that then didn't need human data to learn from just learn for itself.
And then finally AlphaZero which could play any two player game.
So I'm going to sort of describe them all that roughly speaking with this with this sort of demonstrative diagram.
So the way you can think of all of these systems is we're initially training a neural network through self play for it.
So the system plays against itself and and it learns to evaluate positions and to pick the most likely moves that are most useful for it to look at.
Right. So that's what it's got to do.
Now initially it starts with no knowledge.
Right. So you have initialized neural network.
It starts with zero knowledge.
So it literally is moving randomly.
Right. So that's we can call that version one.
Right. That's the neural network.
And what it does is it plays roughly 100,000 games against itself.
OK. And so that then becomes a data set.
So that 100,000 games we take that as a data set.
And what we try to do with it is train a version two of that network, a new neural network.
But we try and train it on this version one data set to predict in the middle of a position in a middle of a game from a position in the middle of a game which side is going to win.
Right. So so kind of predict ahead of time.
And also what sorts of moves does the V1 system choose in a particular position?
Right. So that's trying to do is trying to be better at both those two things.
And then what happens is we train that V2 system and then we have a little mini mini tournament between V1 and V2.
So it's roughly 100 games and they have a little match off.
And basically if there's the V2 system hits a particular threshold win rate, 55% in this case, then we say it's significantly better than V1.
Right. And if that's true, then what we do is we replace V1 with version two network, this new network in purple.
And that, of course, plays another 100,000 games against itself.
Right. And now it creates a new data set.
But this data set now in purple in the middle is slightly better quality than that first data set.
Right. Because the player is slightly better.
And to begin with almost imperceptibly better.
So it's just slightly better than random now.
Right. But that's enough signal to then train, you know, of course we train a version three system and that plays off against version two.
Now, if you don't reach this 55% win rate, what you do instead is you take back the version two and you continue to generate more data with that, another 100,000 games.
So then you have 200,000 to train your next version three.
Right. And eventually that version three will be better than version two.
So after one does this around 17 or 18 times, you go from random to better than world champion.
That's it.
Right. And you can do this with any two player game, perfect information game.
Right. So the same network can do that.
Get to better to world champion within, you know, 20 to 30 generations of doing this.
So you literally, and we got to the point where it was so fast, you literally set it off in the morning.
You could play chess about it at lunchtime and maybe just beat it.
And then by tea time, you know, you no chance literally in the day.
You could actually see the evolution in one day.
It's kind of incredible to watch as a chess player.
So what is it doing then in terms of thinking about this enormous search space?
So what's happening is, and the sort of, I think, advance of AlphaGo,
one of the advances was combining this neural network system or model
with a kind of more classical tree search algorithm.
In this case, we use Monte Carlo tree search.
And you can think of the tree of possibilities looking a bit like this in Go,
where each node here is a positioning in Go,
obviously shown by these little mini Go boards.
And you can imagine if you're some middle game position, you know,
there's just this countless 10 to the 170 possibilities in the limit.
How is one supposed to find the needle in the haystack, right?
The good moves that could be world champion or better level decisions.
So what the neural network does is it constrains,
that model constrains the search to things to make it tractable, right?
To things that are reasonably likely to work, reasonably effective,
and it can evaluate that at each node level with its evaluation function.
And so instead of having to do, you know, 10 to the hundreds of possibilities,
one can just zoom into, you know, mere thousands, 10,000 or so searches.
And so therefore, instead of that searching the entire grey tree of all possibilities,
one just looks at this far more limited, you know, search tree in blue here.
And then when you run out of thinking time,
of course you select the best path that you found so far in pink here.
So, you know, we did this back in 2015.
And then in the subsequent years, we still work on this now.
There's a system called Mu Zero, which is our latest version of this
that can not only do two player perfect information board games,
but can also build models of its environment.
So it can actually also do things like Atari games and video games
where you actually don't have the rules of the game given to you.
It has to actually figure that out for itself for observation as well.
So it's one step even more general than alpha zero.
And what we did with AlphaGo, of course now is, as Senaidol mentioned,
is we took it to Seoul in 2016 in this million dollar challenge match with Lisa Doll.
And some of you may remember this, but we won 4-1.
You know, there's a huge thing in, especially in Asia and in Korea.
I mean, the country almost came to stand still.
There's over 200 million people watch the games.
And we won 4-1 and experts in both AI and in Go proclaimed this advance
to be, you know, a decade before they would have predicted.
But the important thing in the end was actually not just the fact that AlphaGo won the match,
but how it won was, I think, really instructive.
So I'm just going to give one example of this, but actually AlphaGo, I think,
is in the end changed the way that we as human beings view the game of Go.
But this is the most famous game of that set of five.
There are actually some amazing different games,
including the one that Lisa Doll won with a genius move in game four.
But move 37 in game two, I think, will go down in Go history.
And this was the ball position at that time.
And I haven't got time to go into why this was so amazing.
But suffice to say, AlphaGo here was black and Lisa Doll is the white stones.
And this is very early on in the game, move 37.
You know, Go games last for a few hundred moves generally.
And AlphaGo played this move 37 stone on the right hand side here marked in red.
The amazing thing about this was the position of the stone
was on the fifth line from the edge of the board.
And that, if you're an expert Go player, is unthinkable.
It's like you would be told off by your Go master
that you should never do make a move like that,
because it gives white too much space on the side of the board.
But AlphaGo decided to do it.
Never seen before in master play would be recommended against.
And then a hundred moves or so later, it turned out this stone,
this move 37 stone was in the perfect position
to decide the battle that spread out from the bottom left
all the way across the board.
And it was just in the right place to decide that battle,
which decided the whole game.
And almost as if it had presciently sort of seen that influence ahead of time.
So now people play on the fifth line all the time, I'm told.
So this has changed everything.
And there's multiple books now written about AlphaGo's strategies.
And this is an original strategy because this is not something
that AlphaGo could have learned from human play.
In fact, it would have learned the opposite.
It would have learned not to do this kind of move.
So if you're interested in more about AlphaGo,
I recommend you this amazing award-winning documentary
that was done by an independent filmmaker on YouTube now.
If you want to see the sort of ins and outs of it,
it was very emotional as an experience for us from all sides,
especially me being an ex-games player.
I could really understand it from Lisa Doll's point of view too.
So as I said, we then took this to AlphaZero a couple of years ago,
two, three years ago now, and generalised this to all two-player games.
And these graphs show how AlphaZero did against the best machines at the time
in the specialised games of chess.
It beat the best version of stockfish,
which is this incredible handcrafted system,
the descendant of Deep Blue.
And it was able to beat stockfish 8,
which was the best stockfish at the time in four hours of training.
It could beat AlphaGo, AlphaZero beat AlphaGo in eight hours at Go.
And then we just tried it with one other game, Japanese chess shogi,
actually, which is a really interesting variation on chess.
And it could beat the best handcrafted program called ELMO within two hours of training.
The same system, all three games.
So that was generalised.
And then, of course, because I'm a chess player,
I play a little bit of Go, but I'm not very strong, so chess is my game.
And so for me, this was the most exciting part of applying AlphaZero
because I actually had a discussion with Murray Campbell,
and some of you will know was one of the project leaders behind Deep Blue,
our IBM back in the 90s.
And we just, I think we just were about to play the Lisa Doll match,
or maybe we just finished.
And I was giving a lecture at a conference,
and Murray Campbell was there as well in the audience.
And he came up to me afterwards, and we were discussing,
I said to him, I'm thinking about maybe we should try this with chess
and see what happens.
And I wanted to know what his prediction would be.
Do you think these incredibly powerful handcrafted systems,
like stockfish, could be beaten?
Was there any more headroom in chess?
Chess is probably the oldest application of AI.
Turing and Shannon and people like that have all tried their hand.
Every AI researcher at some point has tried their hand on a chess programme
back to the 40s and 50s,
even if Turing had to run the programme by hand on a piece of paper and a pen.
And then, of course, in the last 25 years or so,
world champions have been studying with their chess programmes
and mapping out all of chess, opening theory, all of these things.
So it was a legitimate question actually to ask is,
was there any more headroom left,
and what sort of chess would AlphaZero play
if we were to train it from first principles
and play it against these amazing hand-engineered monsters
in some sense of a machine, incredible calculating machines?
And so, of course, we couldn't actually come to an agreement on that,
and that, as the scientists in the audience will know,
that's the sign of a good question, I think,
where either answer would be interesting.
But if we were to win and there was some new style out there,
they would be incredibly interesting,
and also be interesting if these handcrafted systems,
at least in one domain, chess, had reached the limit.
So we got off and started doing that,
and I'm pleased to say that AlphaZero not only played stronger,
but it did come up with a completely new style of chess,
which I think, and my chess friends tell me,
is more aesthetically pleasing as well as a chess programme,
obviously subjectively from a human expert's point of view.
And the reason it is is because what it does,
and it does many innovations,
but the main one is that it favours mobility over materiality.
So traditionally, handcrafted chess programmes
have always favoured materiality.
The joke within the chess circles is that chess computer sees a pawn
and then grabs the pawn because it loves material
because it gets plus one in its evaluation function.
And then it tries to hang on for dear life in a really ugly position,
but it wins because it never makes any tactical mistakes.
So it's very effective,
but it's a little bit aesthetically unsatisfying,
one would say, as a style.
But instead of that, actually AlphaZero does the opposite.
It loves sacrificing pieces, material, to get mobility,
to get more mobility for its remaining pieces.
So this is a game from...
We did a 100 match between AlphaZero and Stockfish,
and then we gave it to the British chess champion to analyse,
and he picked out the coolest positions.
This is my favourite.
It's sometimes called the immortal Zugswang game.
Zugswang is a phrase in chess,
a German phrase that means any move that one makes in that position
makes your position worse.
So it's a special type of position where you're in Zugswang,
which means anything you do, it's going to make it worse,
which is very unusual.
And it's super unusual in this kind of position,
for those of you who know chess, where black,
which has got more pieces, the two rooks and the queen,
so it's got a big material advantage,
very powerful pieces,
the most powerful pieces remaining in chess,
but they will stuck in the corner,
and AlphaZero has sort of sealed them up with cement with its pieces,
and basically none of those pieces can move, right?
So this is kind of an incredible position.
So almost anything black does in this position,
it's black to move, will make its position worse,
even though it's got all of these very powerful pieces.
So that was one innovation.
There were lots of interesting poppies about AlphaZero
that I won't go into,
but one can think about,
well why is it that AlphaZero plays like this
and traditional chess engines didn't?
Nowadays, actually interestingly, they've updated Stockfish
to include some of these ideas by hand in Stockfish,
and actually now it's even more powerful.
So it's kind of interesting hybrid system.
But my feeling is that it's better at evaluating positions than chess engines,
so that's one thing,
so it's got a better evaluation function.
And the main thing is it doesn't have to overcome these inbuilt rules.
That's why it's like sacrificing pieces,
because if you think about it,
this hard-coded chess engine would have to calculate in its search tree
that if it was going to sacrifice a rook for a bishop,
that's minus two points,
is it going to get back those two points of value
within its search tree horizon?
AlphaZero doesn't have to worry about that,
because there's no rules like that in there.
It can evaluate things contextually
based on the particular situation at hand
and the patterns involved there.
And also the other big thing is
Stockfish and programs like that,
they have thousands of handcrafted rules,
so one problem is generating those rules,
but an even bigger problem, in my opinion,
is balancing those factors together, right?
That's a huge sort of handcrafted juggling act.
And instead of that, obviously AlphaZero learns itself
how to balance out the factors that it's learned
and to do that automatically.
So one can actually see how efficient this system is
based on the amount of search
that traditional search engines have to do
per each move they make.
And a human grandmaster makes only the order
of looks at about 100 moves per decision,
so incredibly efficient with our models.
And the state-of-the-art chess engine,
like Stockfish, would make tens of millions
of evaluations per move.
And AlphaZero is sort of in the middle here
in terms of orders of magnitude,
tens of thousands of moves.
So not as efficient as human players,
but far more efficient than the search one would get
in the search engines.
So, again, if you're interested in the details about
or your chess play and interested in the details
about what this changed,
the British champion and Natasha Reagan
wrote an amazing book called Game Changer,
and when we gave them behind the scenes
access to AlphaZero and what new motifs they found,
at least a dozen new motifs they found in chess.
And the cool thing is that it's very gratifying for me
is that people like Magnus Carlson,
who's the current world champion, incredible player,
he said a few years back he was one of the first people
to read the book and who we sent it to,
and I've been influenced by my heroes recently,
one of which is AlphaZero, which is really cool to say.
And he actually incorporated, he's so talented,
he was able to quite quickly,
quicker than all the other chess players,
incorporate some of these ideas into his play.
And then Garry Casparov, he used to be a hero of mine
when he was world champion when I was growing up and playing chess.
He worked the forward for the book
and he said programs usually reflect priorities
and prejudices of programmers, but AlphaZero
learns for itself and I would say it's star reflects the truth,
which is I think a beautiful quote.
So we've been lucky enough to have several
of these sort of fundamental breakthroughs in games.
We started with Atari and our program called DQN
being able to play Atari games directly from pixels
and maximise the score just from pixels,
not being told the rules of the game AlphaGo and AlphaZero
I just mentioned.
And then we went further with programs like AlphaStar
which played the most complex video game called StarCraft 2,
which is a very complicated real-time strategy game
with huge other challenges.
It's only partially observable, it's not perfect information,
there's an economy system to it,
and you have generally thousands of possible actions
you can take for any choice, not a few dozen.
And we managed to also get to grandmaster level at that.
So that was all of our games work,
but really it was leading up to this moment,
which in the last couple of years has been just so exciting
and so gratifying for us to make progress with,
which is that the games, and I love games, always will love games,
playing them, designing them and using them as testing grounds,
they were the perfect testing ground for developing AI,
but ultimately the aim was not to play games to world championship level,
it was to build general systems that could generalise
and solve real world problems.
And the one that's particularly passionate for me
is using AI for scientific discovery.
And there are three things that I look for currently
when we want to select a scientific problem
that we believe our systems could be good at.
So number one is we actually search out massive combinatorial
search spaces or state spaces.
So the bigger, the better actually.
Why is that?
Well, because we know then traditional methods
and exhaustive brute force methods won't work.
So we're in a razy where something else is needed
and we think that we're good at that something else.
Number two is that we want to have,
we like problems that have a clear objective function
or metric that one can specify
so that you can optimise and hill climb against it
with your learning system.
And then number three is we look for problems
that either have a lot of data available to learn and train from,
or, and ideally it's and or,
an accurate and efficient simulator
that one can use to generate more data.
And that simulator doesn't have to be perfect,
it just has to be good enough that you can extract some signal
from the data that it generates.
Now it turns out that when you look at a lot of problems
with this prism, then actually a lot of surprising number of problems
can be made to fit these criteria.
And of course the number one thing we were looking at
was protein folding, which I want to talk a bit about now.
And we look for problems,
not only that just fit those three criteria,
but of course there's always an opportunity cost
when you embark on applying AI to something major.
It's going to take you many years depending on how hard that problem is.
And we look for something that will have really huge impact.
Perhaps, you know, we sometimes talk about root nodes
that can open up whole new branches of scientific discovery
if they were to be solved.
And protein folding ticked all of those boxes.
So for those of you who don't know what protein folding is,
it's this classic problem of can one go from a one-dimensional amino acid sequence,
so you can think of it as the genetic sequence for a protein
that describes a protein coded by the genome.
And can you predict from that, directly,
the 3D structure of the protein in your body,
the 3D form that it takes?
And the reason this is important is that proteins
are basically essential for everything in life,
every function in your body,
and it's thought that the 3D structure of the protein,
at least in a large part, governs its function.
So if one can understand the structure,
then one can get closer to the function of the protein.
Now, until AlphaFol came along,
the way you would do this is experimentally,
and it's extremely painstaking expert work that needs to be done,
and using x-ray crystallography and electron microscopy,
and a rule of thumb is generally that it takes one PhD student,
their whole PhD, to do one protein.
And that's if you get lucky, you can be unlucky.
So it's hard and really painstaking and difficult.
And what happened is that the Nobel Prize winner Christian Anfinsen,
in part of his Nobel lecture in 1972,
so 50 years ago, exactly now,
he conjectured that the 3D structure of protein
should be fully determined by the amino acid sequence,
i.e. this should be possible, this mapping.
And it's a bit like, sometimes this problem is called
like Fermat's Last Theorem equivalent in biology,
because it's a bit like saying this is possible,
but the margin is too small, can't give you the answer.
And so what happened instead is obviously it set off a 50-year quest
in biology, in computational biology,
to try and solve this problem.
And it's been ongoing ever since the 1970s.
So the big question is,
is can protein structure prediction,
the protein structure prediction problem,
which is the specific part of protein folding that we're interested in,
be solved computationally?
Just computationally.
And Leventhal, who is another famous contemporary of Anfinsen,
in the 60s and 70s as well,
he calculated, back of envelope,
that there would be roughly 10 to the 300 possible confirmations,
shapes of an average size protein that it could take.
So 10 to the 300, so that's a good number, that's ones we like,
because it's bigger than go.
And obviously that means exhaustively sampling this
is totally intractable,
but of course the chink of light is that in nature,
in our bodies, physics solves this, right?
So it can, if proteins spontaneously fold in a matter of seconds,
sometimes milliseconds in the body.
So there's obviously some energy path through this.
So how do we get to this problem?
Actually, it's quite a long winding road for me personally,
for others in the team less so,
but for me, I actually came across the protein folding problem in the 90s,
as an undergrad in Cambridge,
because one of my friends in our group of colleagues
was obsessed with this problem.
And he would talk about it, I remember this very clearly,
every opportunity in the bar playing pool, whatever it was.
If we can crack this, that will open up all sorts of things in biology.
And I sort of listened to him and I was thinking about this,
I was fascinated by the problem as a problem,
and I felt it was actually very well suited to potentially to AI,
although obviously at the time I didn't know how it could be tackled.
But I filed that away as an interesting thing.
And then it came up again in the late 2000s
when I was doing my postdoc over at MIT,
and this game called Fold It came out from David Baker's lab,
who works on proteins.
And it was a citizen science game, you can see it on the left here.
And what they've done really interestingly
is turn protein folding into a puzzle game.
And they actually got a couple hundred gamers to fold proteins,
bit like playing Tetris or something.
And some of them actually became really good.
And I remember, so of course I was fascinated this
just from games design perspective.
Wouldn't it be amazing if we could design more games
where people played them,
they were actually doing useful science while they were having fun,
that would be amazing.
And I think this is still the best example of that.
But also, again, protein folding was coming up.
And in fact, it turned out that a couple of,
a few really important proteins structures
were found this way by gamers
and published in Nature and Nature Structural Biology.
And so this actually really worked.
And that, when we then got to,
the third piece of the puzzle was doing go
and trying to think about what we'd done
with intuition and other things, as I mentioned earlier.
And I felt that actually, if we'd managed to mimic,
in some sense, the intuition of go players,
master go players who spent their entire life studying go,
maybe one could mimic the intuition of these gamers
who were only, by the way, of course, amateur biologists.
But somehow, some of them were able to make counterintuitive folds
of the backbone that were, if you just followed an energy landscape
in a greedy fashion, one would not reach a local minima
or local maxima, and you would not be able to find the right structure.
So it's almost the day after we got back from Korea,
we then, you know, I instigated the AlphaFold project
and I thought it was the right time
to basically start working on this problem.
The other important piece of the puzzle was this competition called CASP,
which is sometimes thought of as like the Olympics for protein folding,
and it's sort of run every two years in external benchmarks,
an amazing thing, actually, that I think more areas of science should do.
And it's been run sort of religiously for every two years, for nearly 30 years.
So, you know, huge kudos to the organisers,
John Moll and his team for doing this
and organising it so professionally for every two years
without fail for 30 years.
And the cool thing about it is it's a blind prediction assessment.
So there's no way you can accidentally sort of train on test data
or any of these kinds of pitfalls,
because at the time when the competition runs over summer, usually every two years,
the experimentalists globally agree to hold back a few of their structures
that they've just found, but at that point in time,
they're the only ones who know what that structure looks like.
They hold back the publication for a couple of months
and they give it to John Moll and his colleagues to put it into the competition.
And then you get those. It's quite a fun tournament
because then, you know, it's quite exciting, you get the email
and then there's a new structure that Amino Assets Sequence nobody has ever, you know,
knows the structure of, and then you have a week to sort of get it back
to the competition organisers before it's published.
And then at the end of that three, four month period,
they obviously score your predictions against the ground truth,
which at that point is published, obviously in peer review journals,
that the experimental ground truth.
And then you get a kind of distance measure between your predictions
and the molecules in that prediction
and where they really are in 3D coordinate space.
So when we started getting involved in this area post 2016,
you know, we looked at CASP and the history of it,
and actually they'd been very little progress for over a decade.
It's sort of the field had stalled.
And this graph here shows you the scores of the winning team
on the hardest category of protein,
where you don't have any evolutionary similar template proteins to sort of rely on.
So it's called free modelling.
And this is a percentage accuracy, it's called GDT,
it's a slight nuance of the measure,
but you can think of it as the number of molecules,
the percentage number of molecules you've got roughly in the right place
to a certain tolerance, distance tolerance.
And you can see they were hovering around 40% or less,
which is useless for experimentation, right?
Basically it's pretty much random.
And so that was the average and it hadn't really moved.
And so what we did in 2018 is that we came along with Alpha Fold 1
as our first entry after a couple of years of working on this,
and we sort of, you know, I think we revolutionised the field in a way,
is that for the first time we brought cutting edge machine learning techniques,
the sort of techniques we developed in AlphaGo and other new ones for this domain,
and we, as the core part of the system,
and we improved the winning scores by 50%, you know,
we got to close to 60 GDT here.
And then of course we didn't stop there,
we then re-architected based on that knowledge,
we actually tried to push that system further
and it turned out it hit a brick wall,
so we had to go back to the drawing board with the knowledge that we had,
re-architected with a brand new system,
and then that finally reached in CAS 14 in 2020 atomic accuracy.
So accuracy within the width of an atom, right,
for all the molecules.
So when we look at the scores and the results of CAS 14,
what you see here is that AlphaFold 2,
this is the root mean squared error,
is less than one angstrom error on average,
and from the 100 or so proteins that we're supposed to predict.
And one angstrom is the width of basically a carbon atom.
So that's finally, that was the magic threshold
that John Moll and others of the organisers said
that they always set out CASP to do,
because that would make you competitive with experimental techniques,
which are roughly, you know, the best ones are at that kind of error rate.
So if one could do that computationally,
then suddenly you have a technique that could be,
you could relied on in tandem with experimental instead of.
And so AlphaFold 2 got an error of 0.96 angstroms,
which was three times more accurate than the next best system in CAS 14,
even though those systems obviously incorporated the AlphaFold 1 techniques
that we'd already published by then.
So this led to the CASP organisers and John Moll
declaring that the structure prediction problem
had essentially been solved after all of these years.
And this is what the predictions look like.
So the ground truth is in green,
and you can see the prediction from AlphaFold 2 in blue.
And you can see firstly proteins are exquisitely beautiful.
It's one thing to note that I've learned over the many years
I've been working on this now.
They're like exquisite little nano machines.
And you can see how accurate the overlays are.
And we were astounded, of course,
when we first got these results back.
This is the architecture for AlphaFold 2.
We don't have time to go into the details of today,
but there were a huge number of innovations that were required to make this work.
And the key technical advances were basically,
first of all, I should say there was no silver bullet.
It needed actually 32 component algorithms
described in 60 pages of supplemental information actually in the paper.
And every single part of that was required.
So we did these ablation analyses,
which took out components to see if we could get away without having them.
And the result of that was everything was required.
And the three key takeaways of why AlphaFold 2
was an improvement over AlphaFold 1
is we made the system fully end-to-end.
You can think of it as going end-to-end
with a recycling iterative stage.
Over time, it sort of jigs the protein structure
nearer and closer and closer to the final structure that it's going to predict.
And AlphaFold 1 system didn't do that.
It went from the amino acid sequence
to this intermediate representation called a dystagram,
which is a pair-wise dystagram of all the protein molecules
and their distance to each of the other molecules, the other end molecules.
And then from that, we used a different method to create the 3D structure.
But with AlphaFold 2, we actually made this end-to-end.
So we went straight for predicting the 3D structure.
And those of you who work in machine learning will know that, generally speaking,
if you can make something end-to-end and optimise directly for the thing that you're after,
usually your system will have better performance.
We used an attention-based neural network
to infer this implicit graph structure of the residues,
of the amino acid sequences.
In AlphaFold 1, we used a convolutional neural net,
which was sort of borrowed from computer vision.
When we think about it, that was introducing the wrong bias into protein folding,
because with computer vision, pixels next to each other
are obviously going to be correlated in an image, in some sense.
So convolutions make sense.
But actually, for a protein, the amino acid sequence,
residues that are next to each other or close to each other
on the string of letters may not end up being near each other
once you get the full 3D fold,
or things very far away could end up folding over near each other.
So, in a way, we were giving it the wrong biases,
so we actually had to remove that.
Finally, we built in some biological and evolutionary
and physics constraints into the system without impacting the learning.
Again, usually, so you can think of it as a little bit of a hybrid system,
usually, if you put in constraints, that impacts the learning.
We managed to do that without that.
This was a huge research effort over five years.
It took about 20 people at its maximum,
and it was a truly multidisciplinary effort.
So we needed biologists and physicists and chemists,
as well as machine learners,
and I think that's an interesting lesson, maybe,
to learn about cross-disciplinary work in AI for Sciences,
is you need the experts also from the domain.
The final, maybe interesting point to note on this,
is that normally, we're always after generality,
so you can see that from the journey from AlphaGo to AlphaZero,
was we increasingly made things general.
When you start with performance,
then you start throwing things out of that system
to try and make it simpler and more elegant,
and that usually makes it more general,
as you understand what it is that you're doing.
But that's because Go and Chess and those things were testbeds
for what we wanted to do.
If you are trying to solve a real-world problem
that really matters to other scientists or health,
or in this case, you know, biology,
then, actually, you might as well throw the kitchen sink at it,
but you're actually really after the output itself,
in this case, protein structures, and that's what we did here.
We really threw everything we had at it,
and it's, I think, the most complex system that we've ever built.
Other things to note about this system is that it's also,
AlphaFold 1 was relatively slow,
took a few weeks of compute time to do a protein.
AlphaFold 2 took two weeks to train the whole system
on a relatively modest setup of 8 TPUs or 150 GPUs,
which, by modern-day machine learning standards, is quite small.
And then the inference, the predictions,
can be done lightening fast, you know, order of minutes,
sometimes seconds for an average protein on a single GPU.
So, when we did this, AlphaFold 2, we announced the results,
published the methods.
Over Christmas, that Christmas, this is back in 2020,
we were thinking, okay, how should we give access to the system
to biologists around the world?
And normally what you do is that you set up a server,
people, biologists, send you their amino acid sequences,
and then you give back, you know, a few days later,
you might give them back the prediction.
But actually what we realised, because AlphaFold 2 was so fast,
we could actually just fold everything ourselves in one go, right?
So we just fold all proteins.
And we'll start with, you know, the human proteome,
just like the human genome equivalent, but in protein space.
And so that's what we did over the Christmas.
We folded the whole human proteome.
And so, which is another thing I love about AI and computing,
is, you know, you can have your Christmas lunch,
and while you're doing that, offer our AIs doing something useful
for the world.
So the human proteome, so we published that as well
in the summer of 21 last summer.
So AlphaFold 2, we predicted that every protein
in the human body is around 20,000 proteins,
represented, obviously, expressed by the human genome.
And at the point when we did this, experiments,
30 years of experiments, 30, 40 years of experiments,
had covered about 17% of the human proteome, right?
And we more than doubled that overnight
in terms of very high accuracy structures.
Obviously we folded all of them, but very high accuracy,
so that's less than one angstrom error, you know,
they're sort of up to experimental quality.
We went to 36%.
And 58% at high accuracy, where we call high accuracy
when the backbone is mostly, you know, you can be confident in,
but the side chains may be slightly out.
And then, of course, the question is, what about the rest?
The other 42%.
And it may be that some of those, you know, AlphaFold 2
is just bad at, but increasingly, and this is an open research
question, when we look at it with biologists,
and biologists often send us in results, and it's like,
I look at this one folded really well,
or this one didn't fold well,
but we often find that the ones that didn't fold well
were actually what's called unstructured in isolation.
So they're disorder, intrinsically disorder proteins,
which means that until you know what they interact with,
they're basically squiggly bits of string.
And then presumably when they interact with something in the body,
they then, another protein usually, they'll then form a shape.
But we don't know what that shape is in isolation, right?
We may not even know what it interacts with at this stage.
So actually people have turned this around now
to use it as a disordered protein predictor.
So where AlphaFold doesn't do well,
perhaps that's pretty good evidence that it's a disordered protein,
which of course is very important in things like disease,
Alzheimer's, other things are thought to be to do with
badly folded or disordered proteins.
One of the other things we did which was a nice innovation
for AlphaFold was have the system predicted its own confidence
in its own predictions.
And the reason we did this is we wanted biologists to use this
and maybe would not care about the machine learning techniques
or not understand them or frankly it would be irrelevant to them.
They would just be interested in the structure.
And we wanted to make sure that they were easily able to evaluate
the quality of that prediction and what parts of it they could rely on.
And which other parts they maybe need to check experimentally.
And so what we did is AlphaFold basically we produced predictions.
There were split into three thresholds.
Over 90 was what we call very high accuracy.
So less than one angstrom error, experimental quality.
Greater than 70 was the backbone's correct.
And then less than 50 maybe these red regions.
So you can see in the database that's what they look like.
It's something that should not be trusted.
We did a further 20 model organisms covering all of the critical
model organisms used in research and some more so some important
other ones in disease like tubercosis and also agriculture like wheat and rice.
And a lot of these proteomes are much less covered than the human proteome.
Of course the human one is where the most effort has been.
That's 17% for some of these organisms it's like less than 1%.
So for the researchers in those plant scientists and other things,
this is a huge boon for them because they would never have the resources
to spend that time to crystallise the proteins they're interested in.
We then teamed up with Emble EBI, the European Bioinformatics Institute
at Cambridge and they're amazing as a partnership team.
They host a lot of the biggest databases around the world already.
We thought the best way to host all this data is to just give it to them
and allow them to host it and plug it into the mainstream of biology tools.
And so we had a great collaboration with them and then we basically released
all this data for free and unrestricted access for any use.
Industrial or academic because it's so completely free.
And it's amazing to see the impact of that
and we tried to sort of maximise the scientific impact of this
by releasing it in that way.
The other thing we did do and I want to touch this on this at the end
is think about the safety and ethics of this
and we consulted with over 30 experts in various areas of biology,
bioinformatics, biosecurity and pharma
to check that this was going to be okay to release this type of information.
And they all came back with that they were not worried about this
but they were potentially worried about future things.
So that's something that we bear in mind.
There are now a million predictions in the database today.
I just want to call out one thing is we specially ourselves,
we specially prioritise neglected tropical diseases
because those are the ones that affect the developing world,
the poorest people in the world the most and they're the least researched
because of course there's no money in it for pharma companies
so that often it's NGOs and non-profits that have to do the work there.
So for them it's amazing to get all the structures
because they can go straight to drug discovery
without having to go to the intermediate step of finding these structures.
So we prioritise all these diseases and including ones
for that we've got being given from the WHO about potential future pathogens.
And what's the community done with AlphaFold already?
We've seen just in nine months or ten months incredible amount of work has been done.
This is really cool on the left here with some colleagues at Emble.
They used AlphaFold and Experiment to combine with their experimental data
to put together what's called the nuclear pore complex,
which is one of the biggest proteins in the body.
It's massive for a protein.
And what it is is it's a little gateway into the nucleus of your cell
and it opens and closes to let things in.
And it's beautiful if you look at it,
able to put it all together and then visualise it.
I talked about this disorder predictor, WHO top 30 pathogens.
And actually interestingly, it's helped experimentalists
of the ones that benefited first from this
because they can combine this with their maybe some low resolution images they have.
And if they have two sources of information,
they can then make a sharp prediction from their,
maybe their slightly lower resolution experimental data
and then a computational prediction.
So it's been really gratifying to see hundreds of papers now
and applications already with being used for AlphaFold,
also in industry too for drug discovery.
So what has the impact been?
So we already have 500,000 researchers have used the database.
We think that's almost every biologist in the world
has probably looked up their proteins they're interested in.
190 countries, 1.5 million structures viewed
and already over 3,000 citations.
And we've had some nice accolades along the way from science and nature on the method.
And then over the next year, we plan to fold every protein, you know,
in known to science, which is in Uniprot,
which is the massive database that has all the genetic sequences.
And there's over 100 million proteins known to science
and we're steadily sort of progressing through that right now
and we'll be releasing that over time.
So stepping back then, what does this mean?
I think that maybe, you know, we're entering a new era
of what I would like to call digital biology.
So I think the way I think about biology
is that at the most fundamental level,
it's an information processing system,
albeit an exquisitely complex and emergent one.
And I think of it as maybe the potentially the perfect sort of regime
for AI to be useful in,
because, you know, one thing I think of it analogous to is in physics,
you know, we use mathematics to describe physical phenomena
and it's been extraordinarily successful in doing that.
Of course, mathematics can also be applied to biology
and has been applied successfully in many domains.
But I think a lot of these emergent and complex phenomena
are just too complicated to be described with a few equations, right?
I just don't really see how you can say come up with, you know,
Kepler's laws of motion just from of a cell, right?
How would one do that? You know, just a few differential equations.
Doesn't seem to me likely.
And I think maybe a learn model is a better way to approach that.
And I think, and I hope that AlphaFold is a proof of concept
that this may be possible,
and maybe usher can help usher in this new dawn of digital biology.
And our attempts to go further in that space
is obviously we're researching further at DeepMind,
and the science team, we sort of double down on all these things
within the biology team at DeepMind.
And we've also spun out a new company, Isomorphic Labs,
to specifically build on this work and other related work,
specifically for drug discovery to accelerate drug discovery,
which we hope using computational and AI methods
can maybe be an order of magnitude quicker.
Currently, you know, it takes an average of 10 years
to go from identifying a target to a candidate drug.
So just to start closing then, I just, you know,
there isn't time to go into this, but it's for us,
it's been like a renaissance year in some sense.
I've been having so much fun ticking off all of the,
my sort of childhood dream projects in fusion and quantum chemistry
and conjectures in maths, material science, weather prediction.
This has all become reality now in the last year
of applying it to important problems in each of these domains
and, you know, publishing nice and important work in each of these areas.
In applications, of course, there are lots of amazing industrial applications
that we've been doing, and we have an applied team at DeepMind
that works with Google product teams to incorporate all of our research
into hundreds of products now at Google.
Pretty much every product you use of Google's
will have some DeepMind technology in it.
Some of the ones I just want to call out are our data centre work
and energy optimisation of data centres and the energy they use
and the cooling systems they use,
and we're looking at applying that to grid scale now.
WaveNet, which is the best text-to-speech system in the world.
So any device that you talk to that talks back to you
will be using WaveNet to have really realistic voices.
Even interesting things like better video compression for YouTube,
say 4% of the bit rate that is used whilst maintaining video quality
and also things like recommendation systems,
but there's just too many to mention, actually.
And then, of course, very in vogue now,
and we have a ton of work on this area,
but it will be a whole talk in itself, is large models,
and we have our own really cool large models,
the alpha code that can programme from a text description
and write code, still amazing to me
and competitive programming level.
Chinchilla, which is our large language model
that is computer-efficient.
Flamingo, that's our vision language combined model
that can describe images.
And then Gata, our latest model that is super general,
can do robotics, video games, all sorts of things,
language just with one model.
So this is all very exciting,
but I just want to end my last couple of slides with a bit about ethics
because obviously this is hosted by the Institute of Ethics
and it's a very important topic,
and not just because of that,
but it's also what the TANR lectures are about too.
And so we think a lot about pioneering responsibly.
This is actually two of our values at DeepMind combined,
pioneering and being responsible.
And I hope, I've convinced you and I hope you will realise
that AI is this incredible potential to help
with some of humanity's greatest challenges.
I think disease, climate,
all of these things could be in scope,
but obviously AI has to be built responsibly and safely,
and we have to make sure the people who are building these things,
it's used for the benefit of everyone.
So we've had this sort of front of mind
from the beginning of DeepMind,
and as with any powerful technology,
and I think AI is no different,
although it may be more general and more powerful
than any that has gone before,
whether or not it's beneficial or harmful to us in society,
it depends on how we deploy it and how we use it,
and what sorts of things we decide to use it for.
And I think it's important that we have
a really wide debate about that at places like this
and the Institute of Ethics.
I'm very excited to see that being set up
and for us to interact with the new Institute.
And here, just one mention is that
DNA has been really critical,
and we've been pushing very hard on this the last few years,
and I think it's critical to this
to make sure we get the broadest possible input
into the design and deployment decisions of these systems,
especially for the people that this affects the most,
that these systems affect the most.
And that's something we've been pushing very hard on.
There's still a lot more work to do,
but we've been making some good progress at DeepMind,
and we've been also doing that with all of our sponsorship that we do.
We've now done nearly $50 million worth of sponsorship
of scholarships, diversity scholarships, chairs,
and academic institutions and projects,
and also funding things like the Deep Learning in Darba,
which is Africa's biggest conference on machine learning.
I'm really proud to say that a lot of DeepMinders helped set that up.
And so there's many, many things that we're doing across the industry
that we hope also can act as a role model for the rest of industry.
So then on ethics and safety,
this has always been central to our mission
because you saw our audacious mission at the start.
And we, even back in 2010 in our little attic room,
we were planning for success.
And of course, we had to think through as scientists
what does success mean, what will the world look like.
And obviously, if one thinks that through
and it's becoming obvious now in 2022,
but it was obvious to us then in 2010
that this would have to be critical,
that it would be really important questions
that would have to be addressed.
And part of that, so we've been doing this in the background all along
and we'll be talking more about this work probably in future.
We were instrumental in drafting Google's AI principles,
which are now publicly available,
and they were partly based on our original ethics charter
that we've had from the very beginning of DeepMind.
And the aim of these principles, and you can look them up later
if you want to look at what they say,
is obviously to help realise the far-ranging benefits
that clearly AI could have for everyone
whilst identifying and mitigating potential risks and harms ahead of time.
And we continue to try and act as thought leadership
for the AI community on many of these topics,
strategy risks, ethics and safety.
So what should we do then,
and I just want to end with this last slide here,
is what I think we should not do is move fast and break things,
which is sort of the Silicon Valley trope.
And I think we've seen the consequence of that playing out.
It can be very extraordinarily effective to get powerful systems
and growth and other things,
but I do not think it's the right way to address really powerful
potential dual-use technologies like AI.
And the problem with it is that one of the things that falls out
of moving fast and break things is actually doing live A, B testing in the world
with your minimum viable products and other things.
And of course the question is, if one does that,
where does the option B turns out to be a terrible option?
Well, where does the harm of that happen?
Well, it resides in society, doesn't it?
That pays the cost of your learning because you've done it in the world.
And it's probably fine if you're just doing a little gaming app
or photo app or something, but we already see with social networks,
it's not fine when you're a billion user scale
and things really matter in terms of your A, B testing.
I don't think it's responsible to do that.
So what should we do instead?
Well, fortunately, we already have another method,
which I think would be better, the scientific method,
which I do think is probably maybe humanity's greatest idea ever.
And I think it can apply here.
And I think we should use the scientific method
when we're approaching how to deal with these very powerful,
incredible potential technologies.
And what does the scientific method involve here in this domain?
Well, it's sort of thoughtful deliberation and thought,
ahead of time and foresight, ahead of time,
where hypothesis generation on what might happen
if one were to be successful with what you're trying to do.
So how about we think about that ahead of time, not afterwards?
Then there's rigorous and careful and controlled testing.
I think that's one of the main things I learned from my PhD,
apart from all the neuroscience, was also the value of controlled tests.
In a way, I think when I started my PhD at least,
I was all about what's the condition of interest,
and that's the thing that you're going to make your new advance with.
But actually, you can't conclude anything, of course,
unless you have good controls.
And I think that's something I don't think engineers get first time around, actually.
But scientists and researchers, of course, do get that,
because that's one of the things that you learn from doing a research PhD.
So controlled testing in controlled environments,
not out in the world,
until you better understand what it is that you're doing.
So, of course, one updates on empirical data,
obviously ideally with peer review,
so you get critique from the outside,
and people who are independent from your work,
all of these things that are standard in the scientific method,
but are not standard in engineering.
And all of this is in service
of getting a better understanding of the system
before one deploys it at scale,
and then maybe you find out something.
So, my view is that as we approach artificial general intelligence,
and it's a super exciting moment in time,
as you can hopefully get from my talk
and my excitement over that,
but we need to treat it with the respect and precaution
and sort of humblness, I would say,
that the technology of this magnitude demands.
And I think that's what we are trying to be at the forefront on,
and I think I'll be talking a lot more about this in the future.
So, I'll just end by in honour
on the sort of going back to the science question.
I think if we get AI right,
it could potentially be the greatest
and most beneficial technology humanity has ever invented.
And I think of AI as this ultimate general purpose tool
to help us as scientists understand the universe better
and perhaps our place in it.
Thank you.
APPLAUSE
Well, thank you, Demos, for that extraordinary tour de force.
We do have a little time for questions,
but we wanted to give you the chance
to kind of give us that sense of your vision.
Now, we've got an opportunity to have questions from the audience.
Got to wait for the microphone to be handed to them
and to stand up if possible when asking questions,
but I'm afraid there is a kind of discrimination.
It's only those on the ground floor
that can ask a question due to health and safety policies in the theatre.
So, please, if you have a question, please raise your hand,
and I'm happy to take questions at this point.
So, could I... Yes, John, perhaps I'll start with John.
I'll give you the... There is a roving microphone.
And just declare who you are, John,
and perhaps stand up and just ask a question.
Interesting to begin an ethics talk of some discrimination, Nigel,
but I'm John Tisulis.
I'm the director of the Institute for Ethics and AI.
Thanks so much for a really fascinating and inspirational talk.
I guess I want to ask two questions.
One is a very general question about the nature of the project you're embarked on.
So, the objective is to generate a powerful all-purpose tool
that will help create new scientific understanding.
And the nature of this tool is artificial general intelligence.
So, that is a tool that can replicate or outperform human beings
across a wide range of cognitive tasks.
The worry is their attention there.
If you had something that could outperform human beings
across a wide range of cognitive tasks,
could we still regard that as a tool?
Or would it become a colleague?
So, you talked about respecting AI at the end,
but it looks like something with that level of capacity
would demand a different form of respect
that would preclude the original objective
of now treating it as a tool.
So, that's one question.
The second question is, you've talked about what will benefit humanity.
And so, I guess one question I have along these lines,
how do you make that determination?
So, you might say, look, some people have the view that
AI applied to military applications will benefit humanity.
Others don't.
How do you make that determination?
And I guess there's also this further dimension.
There's a division of labour in making that assessment.
Do you think too much has been placed on the shoulders
of developers, researchers, corporations,
and that really government should step in
and resolve some of these issues?
Thanks, John. Great question.
So, I think with your first question,
the reason human capabilities are an interesting mapping
is because the human brain is the only evidence
of general intelligence we have in the universe
as far as we know.
So, I think there's always the question is,
how do you know you've got there?
And you can approximate it with millions of tasks, potentially.
So, that's one approach.
The more tasks you have in your grab bag,
and it can do all of them in pair against human performance,
you might have done it.
But there's always the possibility that one might have missed out
a particular type of cognitive ability,
like creativity or something.
So, that's why I think...
And also, I think AI can be applied back to neuroscience as well,
by the way.
That's one of our scientific areas that we apply AI to,
is neuroscience itself and better understanding our own minds.
So, I have this view that, as a neuroscientist,
that this journey we're embarked on with AI
is the most fascinating journey one can ever take scientifically,
because there's not only the artificial building,
it's then comparing that to the human mind,
and then seeing, I think,
uncovering the mysteries of our own minds,
what's dreaming, what is creativity, what are emotions,
all of these questions that we have,
free will, potentially even consciousness,
the big questions.
I think building AI and intelligent artefacts,
and then seeing what is missing in them
is a good way to explore that scientifically.
And so then, I don't know the answer to your question,
I think that's part of this journey,
is at what point would these things not become just tools?
And it may even be that it's a design question,
because to whether we should build,
what is consciousness, we don't know,
and that would be a whole, obviously, debate in itself,
but should we build it to the extent of what it is,
should we build them in our systems?
I would say no to begin with if we have that choice,
until we better understand them as tools,
and then we can bring in that extra complexity of free will,
and where do they get their goals from?
Initially, it will be designers,
but if they could be self-generated.
So, I think we're still a long way away from those things,
but that's one of the things I think we should inch towards
very cautiously and with precautions,
because also it will get to the heart of what it means to be human.
And I think that should exactly be done multidisciplinary
with philosophers, and ethicists, and theologians,
and the wider humanities.
I think this is where the humanities comes in,
as well as the science.
So, I think that's all to come.
OK, a question in the front row?
Thank you so much for a great presentation.
Carina Prunkle, I'm a research fellow at the Institute.
So, you mentioned at various points the potential for dual use,
and in particular malicious dual use.
So, I'm curious to hear how you approach this topic at DeepMind.
So, what precautions, or how do you address the potential for dual use?
Great.
So, we have a lot of different mechanisms now at DeepMind
that have been built up over time.
So, one is the Institutional Review Committee we have,
which is formed.
So, it's chaired by Lila Ibrahim, our COO,
and it's formed with different people from across the company,
our legal, we have ethicists and philosophers as well.
It's also rotating boards, some senior researchers,
and they get involved early with research projects
and try to assess them from all aspects,
and they will draw on outside experts.
So, they bring in biologists, for example, for alpha fold,
biothesis, so things we might not have in-house.
And then they work with the research teams to either say,
no, that project should not proceed,
OK, it can with caveats, or why don't you build
or do it in a different way with these safeguards.
So, that's our prototype, I would say, committee
that does these things, and we're kind of exercising our muscle
when the stakes are relatively low currently,
so that we can learn from what works and is effective
as we get more powerful systems.
And obviously, over time, I think at some point,
there have got to be outside bodies that get involved.
But the problem is that, and we've experimented with that too,
is that a lot of these things are very specific
to the technology itself.
So, one has to sort of understand the technology to a deep level,
maybe even have access to it somehow,
but in a controlled way, because one can't just...
Open sourcing is not just a panacea, either,
because if it's a dangerous system, open sourcing,
it means any bad actor can use it too, for anything.
So, there's a lot of complicated, I think, ethical questions around this.
I don't think there's an easy answer.
So, anyone who thinks there is one, I think is kidding themselves.
I hope everyone realises the complexity involved.
But I think it's pretty...
I'm very happy with our internal system,
but I appreciate more is going to be needed than that
as the systems get more powerful and impact more of the world.
A question just behind you, I think,
if you just pass the microphone literally behind you.
Hi, my name is Ulrich.
I'm a postdoc at the Computer Science Department
in the Human Central Computing Group.
So, Deep Mind looks like it's this great example
of how you can take the best from science
and then sort of bring it together with a commercial company
and then make very rapid progress.
And you mentioned, in the end here,
how you thought that the scientific process
should sort of inspire the commercial world, as it were.
I'm curious about what you think about the other way around.
So, what have you learnt by being sort of embedded in Google
that you think we, as researchers,
should learn from in order to make more rapid progress?
Yeah, you're absolutely right.
That was the thinking, the original vision behind the...
So, I spoke about the original vision of the company,
this Apollo programme,
but the original vision behind the organisational setup
and processes was to be a hybrid,
like the best of both worlds.
Startups and the energy and creativity
and pace that they have and nimblness
and the best from academic research,
you know, the blue sky thinking, ambitious thinking
that happens there, but sometimes with a lot of bureaucracy.
I think that we did actually successfully combine those two things.
And then, when we agreed to get acquired,
we combined it with the third thing,
which is scale and resources
of a large, very successful company like Google.
And I think that's the main lesson,
is to make sure you do things at huge impact
and have ambition and realise that you can scale things to that
and the consequences that come with that,
but also the potential of that.
So, I think we've done that now,
and very well, like Mary will free of those aspects together.
It's a daily challenge, because as we get bigger,
one tends to get slower as an organisation,
so we have to fight against that all the time.
But it's pretty unique, I would say,
the organisational and cultural feel of DeepMind.
But it could be a blueprint for other, I would say,
grand projects could be organised in a similar way.
OK, I'm going to just switch to this side
and I'll take a question there on the question about that.
So, Tim.
So, move fast and break things with a quote
from people who built a social network.
If DeepMind was to build a social network
using the DeepMind way of doing things,
then what metrics would you use,
would you optimise to judge the quality of your social network?
And the second question that comes with it is,
do you in fact have a moral obligation
to build that social network?
Wow, OK.
So, thanks, Tim.
I mean, two complicated questions there.
It's actually just generally...
So, let's see, I have to be careful what I say.
But I think social networks have never really been my thing,
first of all, so I haven't really thought a lot about it
relative to scientific advances
and the sorts of things that are my personal passion.
I would question, actually, the premise of your question,
which is that how much value does weak ties like that give,
like a sort of superficial connections like that,
versus deeper ties that you get in real life
with your real family and friends?
I think it's an interesting thing to understand.
Like, are we sacrificing deeper, more meaningful moments
for hundreds of more superficial moments?
It's not entirely clear to me that the metric of...
And it sounds seductive, connect the world, right?
Like, why would that be bad?
But this is the thing I'm talking about with the scientific method,
is to try and think through the full consequences
of what that would mean.
Echo chambers, manipulation, all the rest of it
that we all know very well don't need to go into.
So, I think if I was to do something like that,
I would use the scientific method again
to try and really think through ahead of time,
what do you want as the outcomes and metrics?
In fact, often trying to find the right metrics
that actually drive the right behaviour
that you think is good in the world is half the challenge.
It's like asking the right question in science.
Everybody who does science knows that asking the question
is the hardest thing. What is the right question?
And it's especially hard...
Oh, you want an answer?
Well, I wouldn't want to give you an answer on the spot,
but we can talk about it over dinner.
But at least one should attempt to start
with serious thinking about the question first, right?
That's the first part. I don't know what the answer is
because I've not given it enough thought.
But one should at least understand the meta level of,
like, that's how one should start,
including whether one should do that thing at all, potentially.
It could be the answer of that hypothesis generation.
Okay. I'm going to try and get three more questions in.
We're right up against the clock. We've got about seven and a half minutes.
There was a question here from Helen. Helen.
And then...
Thank you. I'm Ellen Landomer from Yale University
and visiting fellow at the Research Centre for Ethics in AI.
Thank you for a brilliant talk.
So you showed us how AI can help us figure out the truth of the universe.
Pretty much.
How about the moral world? How about the political universe?
Philosophy starts with Plato's Republic,
which is an attempt to figure out the best constitution.
Surely, unless one is a complete moral relativist,
there are some invariants we're trying to figure out about the moral world.
Could AI help us map that out?
Could it figure out, like, the best social organisation,
you know, boring from, I don't know,
all the things we've tried, capitalism, socialism, libertarianism, egalitarianism?
Would it help expand our imagination
and perhaps, assuming you have an objective function
like satisfying majority and preferences
subject to constraints to protect minority rights or something like that,
what do you see in the future?
We took 2,000 years and we haven't made much progress.
Good question.
I mean, look, I think the morality and political science
I think is one of the hardest things that AI,
you know, I think it can contribute in some way,
but I would say it's far harder than the physical sciences,
right, or the life sciences,
because the most complex things in the world are humans,
for human beings to understand and to model
and to understand people's motivations, especially in aggregate.
I think one way it could help is there's also the question of
even if an AI, theoretical AI could come up with a better political construct,
would humans, beings and society accept that or even care or understand it?
So there's all those questions to try and,
and would it be implemented correctly?
Obviously there's obviously implementation problems.
I think more interesting maybe would be,
and I've talked to economists about this is,
and we did quite a lot of research on multi-agent systems.
So again, having a little sandbox or simulation of millions of agents
with interacting with each other, with motivations and some goals seeking things,
and I think we're missing that experimental testbed actually
from political science and economics quite a lot,
because again, economics is one of those things where,
and political science where you sort of have to test it,
live A, B, test it in the world.
It's like, are we going to go for this political system or not?
Should we raise inflation or not?
Well, you've got models, but then you actually just have to do it
and then see, oh, it's caused a recession or something,
maybe we shouldn't do that next time.
And so it would be better if, I think if we had a simulation or a sandbox,
perhaps populated with AI systems that are
epoximates to idealise forms of humans
and then we can maybe make some interesting,
we can do some interesting experimental work in that,
much lower stakes.
So I think that could be really fascinating exploration area
for things like market dynamics and setting the environmental settings
to create more cooperation or something.
I would be, if I was an economist, I would be trying to use all those things.
I used to be fascinated when I was a kid with Santa Fe Institute,
and they used to do lots of really cool models of agent-based systems
and little grid worlds.
And I loved, you know, artificial,
growing artificial societies, I think, by Axelrod.
I loved those kind of work.
I actually used to dream about going to Santa Fe
to work on something like that.
And I still think that would be pretty cool
to have some sort of system like that.
Let's see if we can squeeze just a few more.
There's a question chap in the, who caught my eye there.
Yes?
Just very, and try and squeeze them in
because there's two more questions over here,
and every question that you're going to have.
Super. Yeah, I just have a quick question, to be honest.
So I think at the end you mentioned kind of creating AI
in the image of scientific method.
And the title of your lecture is
Advancement of Science through AI.
But in what sense do you think that neural networks
or the limited understanding I have of AI
is in what sense do they follow the notion of scientific method we have?
Is there any sense of talking about hypothesis
and then testing?
Because it doesn't seem that neural networks work in that way.
They're opaque for most practical purposes.
And if they do outperform us,
should we just get rid of the scientific method?
Thank you.
So by the way, it's not in the image of the scientific method
just to be clear.
It's using the approach of the scientific method.
I'm not sure what image in the scientific method means.
And yes, today that is true
that a lot of the systems we have are kind of black box-like.
But I think that's exactly what we should be doing more work on
is making them less opaque.
There's no reason why they should be.
The way I say it to my neuroscience team is,
look, we understand quite a lot about the brain now,
the ultimate black boxes.
We have MRI machines and amazing tools and single cell recording.
So it's amazing.
And that's why I got into neuroscience in the mid 2000s.
So we can actually look into, we don't have to do philosophy of mind necessarily,
although we should know about that.
But we can actually empirically look at this,
do introspection.
And so as a minimum,
in the field of artificial minds,
we should know as much about them as we do with the real brain.
And we don't know everything about the real brain.
Obviously there's tons still we don't know.
But still there's a lot more that we do know
than we do about these artificial systems.
And it should be the other way around.
That should be the minimum we understand
because we have access to every neuron,
you know, neuron, artificial neuron in the artificial brain.
And we can completely control the experimental conditions.
So as a minimum, you know,
so I sometimes say this is a challenge to the team.
What's the equivalent of fMRI for neural network?
What's the equivalent of single cell recording?
We do ablation studies.
So we have a whole neuroscience team that's thinking about this
and bringing neuroscience techniques,
analysis techniques over to AI.
Now in the defence of the engineers,
one of the reasons that this has happened
is because the brain is obviously a static system
we're all fascinated by, of course.
But artificial systems change over time.
Like AlphaGo is now in ancient history of AI, right?
Although it was very meaningful over time.
And it takes years to study a system, right?
It takes years to build it and then it takes years to study it.
So should you use that researcher time on studying a system
that itself will be out of date
by the time you come to any conclusions about it?
So I think only now are we reaching the point
where we have systems that are interesting enough,
do enough interesting things in the world,
large models and AlphaFold type things,
that probably it's worth spending the researcher time on that.
And so I think over the next decade
we're going to see a lot more understanding of what these systems do.
I don't think there's some weird reason why that can't happen.
Okay, there are so many more questions.
I am literally in the red now.
I'm going to have to call this to a close.
I do apologise.
There is so much pent up, I think, interest and questions for you, Demis.
All I can say at this point is absolutely a wonderful lecture
where five minutes later than we should have been.
The Sheldanian was a strict regime when it comes to timekeeping.
You gave us the most fascinating insights
and you have given, I think, to the world with your company
and your own talents, a quite wonderful vision of a future
in which AI can help us flourish, empower us and not oppress us.
So thank you very much.
Thank you.

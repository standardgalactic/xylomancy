CEO of OpenAI, he was interviewed about the worst case scenario. This stuff goes badly wrong.
And he had this line where he said, worst case scenarios lights out for humanity. And that was
a statement that Sam Ortonman, who's the CEO of OpenAI, the developers of ChatGPT and GPT4,
made about that kind of existential risk. Now, I don't think he's saying that about next year.
But the real question is, if we build an incredibly powerful intelligence system
before we figure out how to make it safe, I think we should have relatively low confidence
it's going to go well for humanity.
Artificial intelligence has been a topic of political conversation for quite some time. I
should know. I partly wrote a book about it. It's called Fully Automated Luxury Communism.
Though I must say, when I wrote that book and when it was published, many people were skeptical
about the rise of the robots and how technological change would disrupt the economic status quo.
Sure, they would say, Aaron, this might be coming in the 2040s or 50s or 60s or maybe when we're all
dead in the 2100s. But this is not a problem for right now here in the 2020s. Yet with the
development of ChatGPT over the last 12 months, people are finally starting to have that conversation.
Maybe artificial intelligence is far more developed than we realize, and with exponential
improvements, perhaps it's getting out of control. Ian Hogarth is a founder and investor in technology
companies. He wrote a brilliant article in the FT Weekend Magazine talking about all of these
issues and how, in fact, we may need political regulation to catch up with technology. Ian,
welcome to Downstream. Thanks for having me.
It's our pleasure. We're going to talk about some really big issues which a lot of our audience
may not be that acquainted with. Artificial intelligence, machine learning, etc. Why do
you have authority on these topics and why should they listen to you over the next hour?
I'm not sure I have that much authority. I can tell you my background. So I
studied machine learning at university. I did a masters, started off with making a robot,
and then after that made a computer vision system. So that's systems where you basically teach a
machine to see in some way, recognize patterns, visual patterns. And the system that I built
was one that could replicate some of the job that a radiologist does when looking at
cancer biopsy images. So I started out with a, I suppose, a very engineer's mindset thinking
about this. I then built a software business as a founder and a CEO for a number of years.
And then I've kind of been involved in this since then as really an investor and, I guess,
an academic. So as an investor, I invested at about 50 companies applying AI to different fields,
including some of the larger companies in the field like Anthropic, which is the
second most funded AI startup in the world, and Helsing, which is the leading AI defense company.
And then outside of that, I've really been trying to expand the level of public awareness of what
is happening in AI, primarily by writing a report called the State of AI Report, which I've written
for the last five years. It's one of the most widely read annual reports on everything happening
in machine learning over the course of a year. So I've been sort of trying to just expand the kind
of the quality of public information out there for policymakers or for citizens who are interested
to find out more. That's quite an authority. I mean, I like the humility at the start, but
I think you're going to know what you're talking about. And for people who aren't quite clear about
your writing, I suppose a quick stop for them would be this recent article you wrote in the
FT weekend magazine. Can you just briefly go over some of the issues you discussed in it?
Yeah, so I wrote that article maybe a month ago now. And the core idea is that there are a small
number of incredibly well funded private companies, primarily in London and San Francisco, that are
kind of locked into a race, where they're all racing as fast as possible to build what they
describe as AGI. And AGI is basically a kind of godlike AI system that is capable of doing
almost anything a human can do and more. And I felt like that race is now getting quite out of
control. And we need to slow it down. And so I wrote that essay to really just try to shine a
light on some of the things that I learn about as part of my work, but that are maybe not public
domain. So I spend time talking to the people running these organizations. And I just sort of
could see that the level of concern behind closed doors had really ratcheted up. And it felt like
there was a big disconnect between the kind of public discourse and what people were saying in
private. So I wrote the essay really just to try to close that gap a little bit.
That's so interesting. So we're talking really about a machine which is capable of augmenting
its own intelligence. And very quickly, you get a super intelligence, so to speak, an intelligence
that we can't really fathom as human beings. You said that the fears and the concerns around
that had ratcheted up. Over what time frame are we talking here?
So I suppose maybe it's worth zooming out and just talking about progress in the field in
general. So if you sort of look at AI systems over the last decade, they've quite predictably
gotten bigger. And so what we've done is every year, we've been kind of increasing the amount of
computing resources will give the largest AI models. And we've also been giving those systems
more data to train on. And so that's been actually a very consistent exponential curve
that's been running now for over a decade. And there've really been a couple of big
kind of moments in time. The first was the founding of DeepMind, which really just brought a huge
amount of ambition and energy to this challenge of like making these even bigger and more powerful
systems. And the second I would say was OpenAI, which introduced a competitor DeepMind that
suddenly meant there was a race. And those organizations have been racing against each
other now for best parts of their entire founding history. And if you look at that,
we've gone from kind of feeding these systems some tens of thousands or millions of images to
feeding these systems most of the internet. And we've increased the amount of computing resource.
We give these kind of most powerful AI models by a factor of 100 million in just a decade.
And so there's been this very, very continuous progress in the field. But as with any exponential,
it's really only when you get to the steeper of the curve, you start feeling it. And I think the
last couple of years are kind of busy, where the curve has just suddenly felt a lot steeper and
things have been changing weekly or daily rather than yearly.
Wow, that is really extraordinary. So the word exponential for people who aren't necessarily
familiar with it, my goodness, virtually everybody is in 2023. But this was broadly
integrated within discussions around computer science by Gordon Moore and Moore's law,
and this idea that broadly speaking, computational power would, there's a bunch of ways of sort of
discussing it. But the same amount of power would basically halve and cost every 18 months to two
years. And that has happened for a long time, it's kind of decelerating, it's happened for a very
long time. You were saying with AI, there's a bunch of variables. So it's not just the computational
power, it's also the data that it's feeding on. And the two of these is important, right?
Correct. And so, for your listeners, an exponential, thinking about it simply is,
for example, a system that doubles every year. And so if you play that out over a number of
years, you get a very steep curve, because some property of the system is allowing it to kind
of grow in that way. And the classic we saw was with COVID. But we as humans, I think, are just
really poor at thinking about exponentials, they're just, they're not intuitive to us.
And so we saw it with COVID, you know, it's kind of January, you know, people start a bit of
attention, February, things get more serious, March, some people really start to get, and then
we're suddenly locked down. And that's kind of the nature, I think, of a system where you're
having a doubling effect over some period of time. And that's what's been happening in AI for a decade,
is just we're now at the kind of February 2020 moment in AI where things are just going super,
super quickly. That's such a powerful analogy. Obviously, you can't go into the nature of private
conversations you've had with people. But when people are putting a date on it, what are they
saying with regards to AI then? And like I said, it's hard to predict by virtue of exponential
growth, but are they saying in the 2020s, next year, I mean, what's the the broad time frame here?
Well, so the first thing I'd say is that the people leading these companies have been thinking
about this problem for a long time. You know, some of them have been kind of, you know, a good
example of a Shane leg. So someone I admire greatly, he's a, you know, he's a brilliant computer
scientist. He runs DeepMind's AI alignment efforts, which we'll probably talk about in a bit what that
is as an area. But Shane, you know, he did his PhD on sort of a computational basis for machine
superintelligence has made many sort of quite sophisticated predictions over the years around
what it would exactly take in terms of the amount of computing resource and the amount of data before
you would actually get a superintelligent machine that was kind of an artificial general intelligence.
And so the people in this field have been thinking about it a long time.
And I pay the most attention to the people who have been consistently making good predictions
to me behind closed doors about what will happen. And the thing that I've noticed is
it used to be the case that people would say stuff like, you know, it's possible we might get
an AGI, a superintelligent machine in the next 30 years or the next 20 years. But everyone,
I think, thought the idea of kind of something happening next year was kind of ridiculous.
And now I think if you ask people, you know, let's say, for example, you know,
there was a select committee and the various leaders of these labs, the technical leaders
were asked under oath, what's their probability that we get a superintelligence next year,
it's not going to be 0%. Whereas I think it would have been before, whereas now it might be, I don't
know, 5%. And so you have this kind of shift where I think everybody is starting to sort of say,
actually, we might be closer than we realized, we should start taking it seriously, the possibility
that we might be very close. So if a private enterprise develops an AGI, an artificial general
intelligence, what happens next, do you think? I have no idea. I think that the AI alignment
community would basically say, you know, most likely outcome is we're all dead. The CEO of open
AI, he had this kind of, it was, you know, interviewed about the worst case scenario,
this stuff goes badly wrong. And he had this line, we said, you know, worst case scenarios,
lights out for humanity. And that was a statement that Sam Altman, who's the CEO of open AI, the
developers of ChatGPT and GPT-4, made about that kind of existential risk. Now, I don't think he's
saying that about next year. But the real question is, if we build an incredibly powerful intelligence
system before we figured out how to make it safe, I think we should have relatively low confidence
it's going to go well for humanity. And there's a kind of very, you know, I guess, sophisticated
intellectual argument about how to think about that. And that's the sort of thing that someone
like Eliezer Yudikowski would write about, where he'll talk through the sort of exact
mechanisms, but why a system that's much more intelligent than humans treats us, you know,
treats us badly, you know, primarily by accident. But I think actually the kind of common
sense way of thinking about it makes more sense, which just says, you know, humans have kind of
changed the environment on earth very significantly, as a result of our intelligence
relative to other species. And that's had, you know, significant consequences for some species,
and for the biosphere in general. And I think we should sort of just common sense tells you
that something similar might happen if we invent something more intelligent than us.
Wouldn't the counterargument be, I suppose, that though that we've learned that over time, we are
dependent on the biosphere for our own systems, political, social, economic, to sustain, I suppose
is they're not an optimistic account of an AGI. And I think obviously, there's a great deal of
thought put behind being skeptical about this stuff. And I'm also skeptical. But is there not
also a world where you have an AGI, which is in some way benevolent, capable of very long term
capable of very long term planning capable of all some problem solving on a scale that we can't
really comprehend. Yeah. And just to be really clear, I think that is, you know, the that is the
happy path we're now on. I think there's there's basically three paths. There's the, we have a
moratorium that just completely shuts this down. And that could be like, you know, some of the other
moratorium we've had around, you know, genetic engineering, for example, you know, eugenics.
There could be a another path where we develop this kind of hastily and not thoughtfully and
kind of wipe ourselves out in the process. And there's this third path, which is the one I think
we should really all be oriented on, which is we build systems that massively expand the amount of
kind of wisdom in the universe. And we cure diseases that can't be cured today. We, you know,
we have enormous technological abundance. And so there certainly is an approach where we build
AGI and it goes incredibly well for us as a species. The question is, are we on track to do
that or not the way we're doing it today with a small number of private companies racing to do
it as quickly as possible? So with regards to chat GPT4, which was released in March by Open AI,
which is aligned with Microsoft. How big a jump was that from chat GPT3, which was obviously the
previous version? So chat GPT when it came out was was arguably a user interface on top of a
very powerful language model that already existed. And so you already had these amazing language
models that Open AI had trained and anthropic and Google had trained to do very powerful things.
And what Open AI did with chat GPT is they basically created a way to interact with it
that suddenly opened it up to a lot more people. And so in many ways, it wasn't a
a sort of research technological breakthrough, it was actually a user interface breakthrough and
said like, here's a way to use this that suddenly feels a lot more organic and natural to an end
user, a consumer playing around with GPT. The thing they released after that, which was called GPT4,
was a significant update because the chat GPT was based on, I think, what they were calling it
GPT3.5. And that that was a big jump in the underlying model. So for example, GPT3.5, when
you sort of, you know, tried to get it to do the bar exam, scored in the bottom 10% of results,
whereas GPT4 scored in the top 10% of results. So in a single generation of models, you had this
massive leaping capabilities where it went from basically not really being able to be a lawyer
to being able to be a lawyer. And so GPT4 was a massive step forward for language models in general
and like, you know, one of the most impressive technological artifacts humanities ever created.
And what was the basis of that jump from 3.5 to 4? Was it purely because there was more data
being fed or there's been a sudden boost in computational power or? So it's a great question
and we don't really know. So OpenAR hasn't really explained to us what data they trained it on,
the amount of computing resources they use for it, any algorithmic breakthroughs they made.
They have kind of gone from being very open AI, kind of being very open with their research
to being much more close with their research. And they have done it, I believe, for good reason,
which is they sort of don't want to accelerate things any more than they have to by suddenly
make it more possible to replicate this and kind of cause a huge amount of proliferation of this
technology. This is so interesting. So could it be possible than the open AI are further down the
road to AGI, then we really discuss, we really talk about, but the incentives aren't really there to
be quite public about it, right? The incentives are there to actually be quite private and discreet
and not really convey how close we are to a really transformational technology.
Yeah, the incentives are really challenging. So I'll actually give you a quite a concrete
example that I think brings the race to life. So I'm one of the first investors in this company,
Anthropic. And Anthropic was founded by a group of people who left open AI and set up a new AI
startup. And it was the people who did it and founded it were the people who led the research
on GPT2 and then GPT3, so the precursors to these these large language models. So they really are
the key people from open AI who did a lot of the large language models sort of early work.
And their new company is very much oriented around a greater emphasis on safety. So they,
you know, they have a, you know, something like 50% of their headcount in 2021 was dedicated to
alignment research and safety, which is higher than any of the other labs like DeepMind or OpenAI.
And they had a product like ChatGPT about six months prior to OpenAI releasing ChatGPT. And
if they'd released it, it would have suddenly put Anthropic on the map in a big way, it would have
been, they would have attracted so much more capital, more attention, and they held it back
because they felt like it would just accelerate this race in a counterproductive way.
And so there's a really, you know, if you think about the incentives, it's working against them
as a capitalist entity to just hold back stuff, to release less, to create less hype. It's quite
challenging. And so recently, they just actually made an announcement maybe two weeks ago,
where they expanded the context window for the largest language models to 100,000 tokens. And
just to explain what that means, it's basically the size of the document that you can feed into a
language model and have it work with for you. And so it massively changes what you can do. You
can feed like a huge legal document or a massive code base into a GPT-4-like model and get a much
more sophisticated response as a result of that. So it's a huge technological breakthrough. It lit
kind of, you know, the AI research community and sort of staff that community on fire when they did
it. And that ultimately attracted more attention to them, probably more capital over time. And so
there's these perverse incentives where if you're a startup, you're kind of incentivized to get as
much capital and attention as possible so you can go faster. But actually, if everyone does that,
then we kind of burn the time we have to make this stuff safe. So it's a very challenging
coordination problem where the incentives encourage racing rather than careful, slow down
coordination. I'm very happy you said that. There's a great quote from Jeffrey Hinton,
who recently resigned from Google. And he said in an event, I think Google was very responsible
to begin with, this is deep mind. But once OpenAI had built similar things using money from
Microsoft and Microsoft decided to put it out there, then Google didn't have much choice.
If you're going to live in a capitalist system, you can't stop Google competing with Microsoft.
So it almost sounds to me like one of the most powerful things about the market system, competition,
which can lead to incredible efficiencies has upside as well as downside.
But particularly with regards to AI, this sounds almost like you couldn't build a worse system
to potentially accelerate development while also not really addressing things which could go very
badly wrong. Yeah, I think that's, it's very challenging because there are areas of AI research
where we I think actually capitalist competition is extremely good. So for example, there's 10
startups and they're all competing to make AI systems that can take in cancer biops images,
analyze them really well and improve the lives of patients, right? I'm not worried about that
having a negative consequence on the world. And I think actually the price signals the competition
will be really good and it'll ultimately give us all, you know, cheaper healthcare,
you know, more innovation in the market. So the area of, I guess, narrow AI where AI is just
doing a single task quite, quite specified. And without these existential considerations,
I think this kind of capitalist competition can be great. The problem is if we're trying
applying the same logic to the part of the problem, we're trying to build something smarter than us,
that's basically a new species. And that I think the kind of capitalist market dynamic is not
helpful. And I think that, you know, what's great is that the leaders of these organizations,
I think in their own ways, they all kind of have done important things to acknowledge this. So,
you know, Demis, the CEO of DeepMind is someone I really admire, you know, he's really oriented a
lot of DeepMind's efforts towards expanding the scientific commons, you know, things like Alpha
Fold, which they released for free. And they've really expanded the amount of that's not a very
sort of capitalist maneuver to basically produce this massive breakthrough and then and then kind
of give it away. But I think it hints at how he thinks the the get the economic gains from this
should be distributed. Sam Altman, you know, the CEO of OpenAI, he's talked about how he wanted to
have the government fund OpenAI early on. So he didn't want to raise money from private investors,
just didn't get the support from the government to do that. He's also he and his team have explicitly
said that if the race becomes too dangerous in their charter, they've said, we will merge and
assist another player to change the coordination dynamics for the better. And Anthropic have got
a very, you know, very, very thoughtful set of statements they've made about how they
want to ultimately be much more cautious as we get closer to this kind of godlike AI.
So I think we have actually leadership that is trying hard to do this. It just doesn't really
work within the current economic system. And so for example, I, you know, I fought a
antitrust case against Ticketmaster in the United States as part of the the startup that I built,
Songkick. And so, you know, I'm very supportive of the kind of, you know, the work that Lina Khan
or the CMA have been doing to try to sort of decrease concentration in certain markets.
But I think in this case, actually sort of antitrust is actually quite harmful because it
almost creates, it makes it harder to coordinate, there's less of a safe harbor. And so I think the
main thing we need to do is really view these as quite different regimes. There's this kind of
narrow AI regime, and there's this trying to build a god regime. And that bit needs a different
regulatory approach to that bit. There's a quote from Marx. It's in the Communist Manifesto.
I actually, I was reading this the other day. That's why I come on this channel. Just to hear
about, hear about. This is, this is, no, this is in this is terrifying. Now bear in mind, he wrote
this 170 years ago. Marx or our capitalist society had quote, conjured up such gigantic means of
production in exchange that it was akin to a source or a quote, no longer able to control the
powers of the netherworld whom he has called up by his spells. I mean, wow, that sounds like
capitalist competition creating something completely beyond motivation and intentionality,
and over which has very little oversight. Now, of course, he's talking about, you know,
steam power and mills in Manchester and Brussels and Frankfurt in the mid 19th century.
But if anything, those words sound more appropriate for AGI in the 21st century.
Yeah, I think that one, one way to sort of frame the capital and kind of labor,
the sort of relative power of those, of those two groups is just looking at the size of some
of these organizations. So an open AI is a, I think privately valued at 30 billion US dollars now,
you know, significantly changing the world, hundreds of millions of people now using their
products. And I think, you know, at the time they released chat up to you, it's probably a few
hundred people in terms of the size of the organization and the labor that's directly,
you know, benefiting from kind of the work that's being done there. And I think that again,
you know, the leaders of these companies are actually thinking hard about this. So
Sam Altman, you know, a couple of things I admire that he's done. The first is he was running a very
large UBI study in Oakland. And so he was, you know, that was, you know, maybe five, 10 years ago,
he was thinking hard about this question of how do you kind of, how do you, if you do have
further and further returns to capital, what do you do about kind of that not,
that not just massively and increasing inequality. And, you know, he's done this thing called
Worldcoin, which is kind of a much more extreme version of that, which is a machine that scans
your retina, produces a unique ID for you that would then let you be part of a global UBI scheme.
And so, wow. And so there are, you know, they, these kind of people are thinking about these,
these kind of the way in which this may fundamentally disrupt some of the,
the ways in the social contract we currently have that allows capitalism to sort of just
continue as it, as it does. But I suppose the concern is you can't be worried, you can't be
dependent rather on prevalence and, and the foresight of certain individuals. You know,
there was a great quote a couple of years ago from Mark Cuban, and he was saying,
I wouldn't teach my kid to be an accountant. We now know that it was probably quite a good move
because, you know, that's one of the industries which is very much prone to automation
with machine learning. I'd rather they learn philosophy because it will give them insights
that are harder to automate, so to speak. And I thought that was interesting. Now, alongside
that, he said the world's first trillionaire will be the person who can master widespread
commercial applications of AI. And that's the prize on offer, isn't it? I mean, that's the prize
on offer. So today we talk about Amazon, which is a trillion dollar company or what used to be,
borderline trillion dollar company, Amazon, Microsoft, those kinds of big players. But the
truth is the commercial entity which masters an AGI, and we don't all die, will put all those guys
in the dust, won't they? So there are massive incentives for people to pursue this technology
without the kinds of caution and intelligence and thoughtfulness that you've talked about
with regards to somebody like Sam Altman. Yeah. And I think what's, what's challenging in some
ways, we've entered a new phase of this race. So if you look at the leaders of these organizations,
the ones who are kind of really at the forefront of the race, whether it's Demis or Sam or Dario
or Anthropic, I would say that most of them are kind of not particularly motivated by money at
this point. They're doing this for some other reason. You know, Sam, you know, recently announced
that he actually has no equity in open AI, so doesn't stand to benefit economically from,
you know, he won't be the first world's first trillionaire, let's put it that way.
And so I think that there's, they've been motivated by other things. And I would say mostly they've
been sort of motivated by being the people to do it, to make this thing come alive and, you know,
the consequences of that. It's kind of a world historic transition that they want to be a big
part of, is my guess, my hypothesis. But there are now a lot of other people who've kind of
suddenly woken up and just seen dollar bills. And those people are just piling on money. They
haven't really thought about it. They don't have the same sort of reverence that people like Demis
have got for how we should be approaching this moment. And they're actually accelerating the
race. But without that same, you know, intrinsic motivation for the thing they're trying to do,
and much more of a kind of, you know, much more of a desire just to make money.
So for people out there who are perhaps skeptical of what I'm saying here about a trillionaire,
if you told a 14, 15 year old Ian or a 14, 15 year old Aaron about the same age,
that one day there'll be somebody worth 250 billion US dollars, 200 billion US dollars,
like Jeff Bezos, we would have thought that's outlandish. Yet he's the guy who starts Amazon,
and that's the company which benefits from network effects, you know, ubiquitous mobile
internet and basically building the everything store of e-commerce. And I suppose the question is,
could you feasibly see a company like Amazon, which is applying AI to a bunch of industries,
which lay off hundreds of thousands of people, just like Amazon have basically shut down hundreds
of thousands of local businesses? And that's the outcome we get. Do you think that's a plausible
outcome? I think it is possible. And I think the reason for it is that you are, you know,
you first of all have a much more globalized economy where things can spread really quickly
across borders in a way they couldn't, especially digital products. Secondly, you have, you know,
a lot of these technological, you know, products build on top of prior networks. So for example,
chatGPT is the fastest growing product in kind of ever on the internet. But that's partly because
we've got Twitter and Facebook and Instagram and Google and all the ways that information disseminates
and spreads faster than it did before. And finally, I think we're starting to tackle some of these
incredibly large markets, like, you know, what SpaceX is doing is basically tackling, you know,
global internet provision through Starlink, right? And so this is a very big thing. It's not like
digging up the street and smashing in loads of fiber. It's something where, you know,
the same satellites can basically provide internet access wherever you are in the world
without that same degree of physical disruption. And so I do think it's possible. I think a good
thought experiment for the kind of company that might be an example of this is the first company
to really build incredibly good domestic robots, right? So, you know, I've got a four-year-old
when I'm doing chores around the house, I would sort of say to him, you know, one day,
you know, one day, maybe there's going to be a robot doing this because, you know, we didn't
used to have dishwashers. We didn't used to have washing machines. We didn't used to have
tumble dryers. And now we have all those things. We take them for granted. They're in most homes
in the world. What would it be like if we had a robot in our house that just did all the chores
that we currently do as kind of around everything else we're doing in our lives, but also that
robot could be your plumber when things break, your electrician, your handy person, go to shops
for you. So when you think about kind of that sort of disruption, could it be a, you know,
10 trillion dollar company where the person founding it is a trillion error, like quite
possibly in my view? A quick sort of move away from machine learning software to the kind of
robot that you're talking about. How far away do you think that is, by the way, as a sort of
household appliance that, you know, middle-class people in the UK or US would be able to buy?
Well, so I think it really is fundamentally the same curve we're talking about with AI. So robots
basically are machine learning. They're just embodied machine learning where it has a physical
presence it's using as well. And we've made, and I've invested in lots of robotics businesses
over the years, and it's amazing how rapidly they're progressing. We don't actually see the
number of robots that we're using because they're mostly industrial settings. They're like industrial
cleaning robots or industrial manufacturing robots or agricultural robots. And so it's happening.
And I think that we will start in the next few years to see robots in much more consumer settings,
really starting with the rollout of self-driving cars. And so I think we will see humanoid robots
that significantly enhance our lives in a domestic setting within the next decade.
The next decade? And a lot of that will be knock-on effects from the work we're doing in
making these powerful AI systems. So to bring this back to large language models and the sort
of things that DeepMind and OpenAI are working on, there was a paper called the tool former paper
that came out earlier this year. And it essentially shows that large language models are actually
quite good at using tools, which is quite counterintuitive. But to break it down,
you take an incredibly large computer, you feed it an enormous amount of text and imagery,
and basically get it to get really good at predicting what it's going to see.
If you take that same sort of blob of intelligence that's just become smart in some important way
and you give it access to a tool, it is actually quite good at using a tool, whether it's a
digital tool or a physical tool. And so large language models actually will have a knock-on
effect on real-world robotics. They're not sort of separate industry as a tool. They're very,
very related. I like it. And how far are we from machines basically being able to do anything
that a human does? So in my mind, I think sort of 25 to 30 years where we have software hardware,
which can basically do 95% of the jobs that humans do. At the moment, the big obstacle to
that obviously is the fine motor coordination dexterity that things like cleaning or construction
kind of relies upon. So it's easier to automate legal services or accountancy than it is,
like you say, plumbing. Great example. How far are we from being able to solve the problem of
the fine motor coordination? So I guess we don't really know. That kind of goes back to this
question of how far away are we from AGI? How far away are we from like sort of super-intelligent
systems? No one knows. All I will say is people have more, they put more probability on it happening
soon rather than later. So Jeffrey Hinton, who resigned from Google recently, he said, I used to
think this was paraphrasing here, but he said something like, I used to think this was decades
away. And now I think it's not inconceivable it happens in the next five years. And he's the
you know, the godfather of machine learning, the research that really kicked off this whole
field of large neural networks. And really, they're like sort of one of the most important
people in the field over the last few decades. And so we don't know, but I think we now need
to sort of start to prepare as if we might be closer than people have realized. And the public
certainly has realized. I mean, this is a big challenge to our economic system, right? So I
mean, I wrote a book about this called Fully Automated Luxury Communism. And the point is,
if you get increasingly affordable technology, which can do anything that a human can do,
whether it's with regards to abstract problem solving or physical capabilities like building
a house or cleaning a house or cooking a meal, what that does is clearly depress the price of
labor to zero. That's what's going to happen. That's a neoclassical understanding of how
of how supply and demand would work with the cost of labor of human labor. And yet,
if you say that to a lot of people, including politicians, including quote unquote smart
people in legacy media, in the policy world, they think you're crazy. And it's interesting for me,
seeing the reception of my book, FT, oh, this is interesting, very provocative, New York Times and
so on. But then in some other places, I won't name the papers, but they were just like,
this is ridiculous. They were sort of mocking it. Where do you think this asymmetry comes from,
where some people take these ideas very seriously, and then others just completely disregard them?
Is it a lack of information? Do you think it comes from a place of fear? Because realistically,
if this is correct, you know, we're going to have to shake things really up a lot, aren't we?
I think it's a great question. I think I think I guess I've got two hypotheses. The first is,
goes back to the thing we were talking about with exponential change. It's just so hard to think
intuitively about exponential progress. And so, you know, COVID is sort of fresh in our minds.
And I remember in sort of January or January, I was planning a trip to China with a friend,
and actually our trip took us through Wuhan. And he's an expert in kind of biology in various
ways. And I spent a bunch of time in China when I was younger studying Mandarin. And so,
we were talking about this trip, and we're talking about COVID. And we sort of started to see the
little inklings that COVID was going to potentially go pandemic. And I remember
both of us were starting to do, you know, prepare and get really worried ahead of a lot of the
sort of mainstream news. But at the same time, we're both like, well, maybe it'll be done by the
time that our trip comes around, right? And it's just a perfect example of like, we thought we
were being clever, but actually, we seriously still did not really have a good intuition for
actually what happens when exponentials really take off. And I think that there's a bit of that
where if you're a politician that's just dealing with, you know, you know, day to day issues,
and you're then confronted with an exponential change, be it, you know, what we're doing with
climate change, or a pandemic, or an accelerating technology like AI, you just don't really have
an intuitive way to navigate that. So I think that's part of it. I think the other part of it is
just exponentials require, I think, a certain kind of radical thinking, you know, it's sort of like
what the, I think the UK government did really brilliantly with Cape Bingham and the vaccine
task force. I think it was a great example of kind of just incredibly bold, proactive leadership
on a serious thing, getting, let's get ahead of it. Let's get, let's, let's build that capacity
early. Let's, let's throw everything at it and prepare as best as possible, because we know
this exponential is coming. And that kind of political action and leadership is just, I think,
like quite hard to do. And I think, why is it, why is it hard to do? Is it because politicians
don't like doing it or because it's just objectively hard to execute? I think it's radical,
basically. And I think it's a, it's not business as usual, you know, it's first principles thinking
you maybe have to take more risk. And they just haven't been that many examples of
political leaders who've kind of acted like that over the last couple of decades in response to
technological change. And I remember in 2018, I wrote this essay, AI nationalism, that really
talked about this kind of this fundamental challenge that was coming down the pipe with AI.
And I remember meeting with, with kind of MPs from labor MPs from, you know,
conservatives laying out these ideas, and almost everyone just looked at me like I was completely
crazy. Because I was saying things like next time we have a company like DeepMind, we shouldn't
let it be acquired because it's too critical to the future of the UK. And then, you know,
three or four years later, I saw similar politicians basically saying, oh, yeah,
now we have to be serious about blocking acquisitions of some, you know, strategically
important UK technology companies. And so I think it's just also discomfort with really
radical ideas. I think there's much more of a comfort in politics with incrementalism. And
that's why often big change happens when you have a crisis.
We'll come back to the politics in a moment. I want to ask you, I want to ask you an almost
existential question. A super intelligent, artificial, general intelligence, would it be
conscious? I don't know. I think that there's a, I was actually talking about this with
someone who specialised in quantum computing yesterday. And their view is actually that
consciousness is a kind of quantum mechanical property, a property of quantum physics. And
therefore, it will not be possible to get consciousness on a classical computer. I don't
know if that's true or not. But their claim was basically, we all need quantum computers to actually
have conscious AIs. I don't know. What's the argument behind that, by the way? So where does
that come from? It comes from quite, I guess, an esoteric argument about kind of,
about exactly how we interrelate with the metaverse, which I'm not enough of an expert on
any of those topics to really opine. But the way I think about it is consciousness and
intelligence are probably kind of somewhat orthogonal. You don't necessarily get one with
the other. And I don't, I think it may have been a, the way, it may be an artifact of biological
life, which is inherently quantum, rather than necessarily sort of classical computing.
So we still could fabricate something close to consciousness, but it would just require quantum
maybe, or maybe it's not possible. We just don't, consciousness is something as far as I can tell,
we don't really understand a tool. So we, I think it'd be quite difficult to project
how to think about consciousness when it comes to machines. Like,
if I'm understanding it correctly, when you, when you get, you know, when you have an operation,
you go under anesthetic, you know, the drugs you're being given, essentially just switch off
your consciousness. And we don't really understand like why and how that works. And so it's quite
hard before we really understand consciousness to really make any claims about how AI's will be
conscious. I think the, but I think we should probably start from the basis that they might
not be conscious, but they might still be incredibly intelligent and capable of planning
in ways that, that are threatening to humanity. So we could have an AGI, which is an extensional
threat to humanity without it being sentient. Yeah. Yeah. Exactly. Interesting. And it could
be trying to maximize its own utility. It could be trying to maximize its own interest without
really being aware of itself as an entity. Exactly. Yeah. Exactly. I'm thinking about
it kind of like the sorcerer's apprentice is probably the kind of like a really mainstream
example of kind of something where you give it some sort of goal and the optimization of that
goal ultimately leads to something that is not what you wanted. And so that's what the field
of alignment is about. It's basically about how do you align those goals with our goals as a species?
What does this mean to the average person? So we're talking about AI, you know, partly abstract,
partly real world. So we're talking about, you know, the potential trillionaires. We're talking
about consciousness. We're talking about politicians asleep at the wheel. But what does it mean to
somebody out there who's watching median income, homeowner, mortgage, owner-occupier,
if there is an artificial general intelligence that's created in the next five to 10 years,
how will that impact their lives over the next, say, 25 to 30 years?
So again, with exponentials, saying anything about, you know, 20 years is really hard.
Let's say it's developed. That's the exponential part. And then let's say there's almost like
a cap on AGI, like it's not really developed after that for regulatory reasons or whatever,
political reasons. I'm trying to really ask, what are the implications for just everyday people,
rather than us talking about, you know, like you say, some breakthrough, which depends on the
printing press, the average person in 16th century Europe doesn't really care about the
printing press, but then the reformation happens and that is a big deal. So I think that like,
I think that synthetic media is a really, really big deal. And I think there is
extremely low popular support for it. So, you know, as an example, you know, you can take the
sort of generative AI systems, the systems that are creative, capable of generating images or text,
and you can use them for harmful purposes. So I'll give you a few examples. The first is,
you can take a snippet of someone speaking, and you can then, you know, effectively synthetically
clone their voice. And that is now being used by people to do kind of fake kidnappings, where you
get a snippet of a child's voice, and you basically create a phone call from them calling their parents
saying, mom, I've been kidnapped. If you don't do this, then I'm going to, you know, something
horrible is going to be done to me. And that happened in, I think, Texas, about two months ago.
And so that's kind of happening. Another thing that's happening is around synthetic
child sexual abuse material. And so the systems you can use to create a kind of funny image of the
pope in a puffer jacket, you can also use to create, you know, horrible deep faked porn,
including horrible deep faked child sexual abuse material. And that is currently happening. People
are using these systems to do that, particularly the kind of open source ones. And the knock on
effect is then you've got a, you know, the police are suddenly faced with, you know, is this an image
of a child that's actually being harmed or a fake image and having to sort of shard their resources
between kind of basically fake crimes and real crimes. And so there's a huge number of malicious
uses that will bubble up from this very powerful technology where you can clone someone's likeness,
someone's image, someone's voice. So I think of that as being like the real structural problem
that we're going to encounter. And I think that they're, you know, I was talking to someone today
who's been doing focus groups around this. And they were just telling me like, you know,
basically the public at large thinks deep fakes should be made illegal already. So there's a kind
of, I think a lot of stuff bubbling under the surface that when it breaks through is going to
really, it's going to really have actually quite a populist response to it, because it's just,
it seems like something the government should be getting a grip of.
And do you think they will? Is there any, so if you're in this game, is there any evidence that
they will? I think they will actually, yeah, I think it's just, it passes a kind of common sense
test of that that is not a good thing that should be happening. So I think there'll be a
genuine motivation to make deep fakes deep deep fakes without someone's permission illegal.
And I think lots of politicians will want to do that. But I think the mechanism for doing that,
when you've got significant proliferation of these capabilities into open source is a bit
trickier, right? Because the technology is kind of out of the box. And so figuring out how to get
it back in, like with the dark web, for example, is just challenging. But I think it will be,
I think there'll be broad political support for doing that.
You said the common sense thing, but I mean, if you said to somebody 40 years ago that children
will be able to access high speed broadband pornography, be able to stream stuff, I mean,
I think most people said that obviously should be banned. That's the world we live in.
I think we've lived through a strange time. You know, I think that I think about social
media a lot and how it's sort of remarkable that we basically left this enormous industry that was
so transformational to, to everyday life, to children, to politics, to the general discourse,
to basically self regulate, you know, and I think there was actually a really,
that was well motivated by a desire not to kind of throttle something with regulation and kind of
too much, you know, too early, you know, I think I can kind of, I can see the logic of things like
section 320, I think it's called, which is kind of the sort of the mechanism whereby a lot of
social media companies have not really have kind of been able to just self regulate. But it feels
like we, I think with the benefit of hindsight, you know, it would have been better for regulators
to catch up faster and to sort of be a bit more assertive about defining, you know, a smarter
way forward. And I think Biden talked about, you know, for example, modifying section 320
recently, and there's been some discussion about this. But I think with AI, you know,
I think that the lawmakers need to move faster, because back then, you know, these fledgling
startups like YouTube, they had certain sorts of, you know, it's kind of a, you know,
innovation was valuable and needed to be sort of encouraged. But this is a different ballgame
where you've got $20 billion already invested in just a handful of companies, you've got,
you know, Microsoft aggressively deploying this as fast as they possibly can.
You've got companies like Facebook, open sourcing incredibly powerful models
and putting them out there for anyone to sort of expand the modify. And so I don't think,
I think we're kind of in like the same scenario. It's just now this is not about small startups
and fledgling industries. This is about an incredibly powerful tech industry that has just
prefers self regulation to anything else. Do you think social media was a mistake? Because
obviously you're involved in the sort of the technological side of things and how, you know,
the technological sort of underpinning of global social media, 4G, 5G, mobile internet,
high res screens, all these things. And one outgrowth that was social media,
and Ben Bratton has a really interesting read on this. So he says, we built a global real-time
communications computational network, you know, including the kind of exosphere of our planet,
which is now caked with satellites. We've built all of this infrastructure so that we can sell
space and be permanently distracted. You know, and I think that's an interesting way of looking
at it. And I wonder, we might have something like AI or even an AGI, if it's not, you know,
deadly. And we would just get more the same, perhaps.
So my day job is I'm an investor. And I invest through a fund called Plural. And, you know,
it's a European fund focused on accelerating missions that we consider to be of great societal
importance, you know, by funding them as startups. And one of the companies I work with is a company
called Unitary AI. It's one I'm very proud to be to be kind of working on. It's a startup that uses
AI to understand content and thereby to offer a scalable approach to content moderation and
content safety across the internet, content security. So for example, their AI can detect
some sort of content that should be illegal or some kind of content that is, you know,
causing significant harms and flag it to the platform that's hosting it. And so they're in many
ways kind of like a, you know, like a antibiotic to this kind of this, the way in which some of the
sort of wild west of content dissemination through social media has kind of played out.
So I think there is kind of a, there is a kind of capitalist response coming, but it's hard and
it's much easier to make a new social network than it is to make an AI company that's trying to
actually solve this problem of kind of how do you scalably tackle the challenges of content?
To your question of kind of, you know, if you could kind of go back in time and
stop social media from happening, I wouldn't have done that personally. I think it's
delivered enormous benefits. I think there is something amazing about so much connectivity,
democratization of media production. I mean, you know, like I, part of the reason I'm sitting
here is because I, you know, I learned about you through Twitter and I follow your thinking and I
find it interesting. And so I think there is something, something really miraculous about how
connected we are now. I just think the problem is we just never really had governments keep up.
And I think it's something, I think it's really the nature of like sort of
laissez-faire, neoliberal thinking, having just permeated government over a number of decades,
where really bold, ambitious projects where the government is a kind of, you know, a real partner
to the private sector and actually drives technological change and thinks about regulation
in a bold way that embraces exponentials has just been missing.
So you think there's neoliberalism, this, because we do live in this really strange moment, right,
of like extraordinary technological possibility, like extraordinary profound liquid biopsies that
can detect early onset cancer, you know, mapping the human genome, you know, just high res scans of
the human brain, just, you know, year on year improving exponentially. And yet we have politicians
who say, sorry, we can't address the housing crisis. Sorry, we can't give you affordable
health care or free health care. There's a weird disconnect there. It seems almost like the better
the technology gets, the more the possibilities, the opportunities, the less capable the state is
in addressing those challenges. So you pin that on neoliberalism.
Well, I think that particular problem you described, like, it's hard to really
know where it started. There's a kind of an investor I really respect called Matt Clifford.
And he has a kind of a thesis on this, which basically says, if you look throughout history,
there was always a kind of part of part of society that attracted the most talent.
And he's his argument is basically right now, that's the technology industry. And so a lot of
talent has kind of gone out of government or out of, you know, the public sector into technology,
because the opportunity to change things quickly, make money, be the first trillionaire,
whatever, whatever motivates you, right? And so it could be hollowing out of sort of the
capacity of the state to respond. That's one way of thinking about it. I actually,
I think of it as being a little bit more ideological. So for me, I think we've just
not really had political leadership that sort of sort of said, you know, I'm going to,
we're going to transform this country in a way that really embraces all of this.
And it's kind of, you know, keeps pace with technological change. And I think you've had
examples of that. You know, if you look at Lee Kuan Yew in Singapore, obviously,
lots to discuss about Singapore and their politics, but it's very interesting the way that he basically
kind of was a founder almost of a country that went from third world to first world in 30 years.
And some of the things he did were just very, very bold, ambitious things that ultimately
he took a lot of risks and it delivered for the country and for the citizens. And so
I think we've lacked that level of boldness. And the reason I sort of cite neoliberalism and
laissez-faire economics is because I remember when I was talking to people about that AI
nationalism, I said, I was amazing to me that I would be meeting with conservative politicians
and they'd be telling me about British Leyland as the reason why you can't nationalize deep
mind. Crazy. And it's just like, what is going on? This is like, you know, it's like decades later
and who really cares about some failed car company at this point?
Also, you have Taiwan, which I think Taiwan produces like one sixth of global microprocessors,
90% of the ultra high end ones, you know, the ultra high end ones, which China can't create at the
moment. And it's kind of, you know, that's the whole point of this, this set of sanctions and
trading bar goes at the US spot on them. That is entirely because of state led innovation by the
Taiwanese government. It's a country of what 25 million people. And in the world where Taiwan
is making one sixth of the world microprocessors, we have British politicians saying, sorry,
we can't do that because of this thing that happened in 1975, whatever. Crazy.
Yeah. And I think it's, you know, it's obviously there are some people really trying, but I think
the system as a whole is very trapped in an old ideology that sort of doesn't just sort of wants
to be quite hands off rather than hands on. And you know, I work with Marianna Mazucato at
her Institute at UCL. And I really, when I first came across her book, the entrepreneurial state,
it just blew me away because it really, if you've been a founder for most of your kind of working
life like I have, it really described and but you really care about the future of the UK and the
future of Europe. It really described a different mode of politics, which is this very entrepreneurial,
kind of founder approach, which says, right, like, the state is going to take a point of view, it's
going to have a vision, it's going to pick missions that really matter, it's going to invest very
ambitiously to make things happen that wouldn't happen otherwise. And I'll give you an example
of something that like right now for me is very inspiring. So within nuclear fusion,
you know, you've got a number of different concepts for how to put fusion on the grid.
And we've made enormous progress in fusion over the last few decades. And we're now actually,
I think, within touching sites of it happening. And the dominant approach that's really taken
us closer to, you know, fusion on the grid is a technology called magnetic confinement fusion.
You have very powerful magnets, the confine essentially a sun on earth, and you use that to
basically produce a sun on earth and extract energy. And the dominant mode of doing magnetic
confinement fusion is something called the tokamak. And the tokamak was a device that's,
you know, there's probably $100 billion been invested in tokamaks globally over the last
50 years. And the German government actually took a different point of view. And they said,
there is this other device, which has a lot of attractive things about it, that actually,
you know, are very hard to do, we might now have enough computing power to do it. And they've been
in quietly investing, you know, large amounts of money, but still quite small for fusion into
making this happen. And over the last 20 or so years, they've taken this alternative fusion
reactor design, all the way up to the point where we might actually be able to build a
plant and it's called the stellarator. And the German government has done that kind of almost
singlehandedly, the most advanced stellarator in the world is in North Germany, and it's light
years ahead of any other stellarator. And that's the sort of thing that just gives me real goosebumps
when I think about what the state can do really, really wants to shape markets, you know, if the
UK said, right, we're going to take a point of view, fusion is going to happen in the next 20 years.
We want to have this country running on fusion. In a material way, you want to do a massive
industry for this country, we're going to have this incredibly ambitious kind of investment
mandate behind that. I think that's the sort of politics that excites me and makes me feel like
we would have a politics that matches exponential change rather than just kind of runs behind it,
trying to play catch up. How popular is that kind of stuff in your world? So you mentioned
Marianna Mazzicato, I suppose, you know, she proselytises a kind of social democracy with an
interventionist state, like you say, with like an emphasis on entrepreneurialism as well. So
it's quite a unique blend, although I really find it's a 21st century variant of 20th century
social democracy kind of updated for the network society. Are those ideas more popular than one
might imagine amongst these kinds of circles? Because tech people are generally open to new
ideas or are you something of an outlier in terms of enjoying Marianna's work?
I don't really know of an outlier amongst founders. I think that founders are quite specific people
and that they're really, they're drawn towards taking risks. They often want to try to make the
world better by building some product that they think the world needs. And I think a lot of them
are quite radical in their bones, quite risk taking. And I think they're drawn to, they're drawn to
interesting ways of thinking about politics often. I would say investors, and you know, I'm kind of
technically my day job as investor, tend to be, you know, a lot more focused on turning money into
money than actually really directing markets. So a little while back, you mentioned quantum
computing and the multiverse. And I know that's not the topic of this conversation. And I'm sure
you have a great deal of knowledge about it. You'll be very humble and say, I'm not the right
person to speak to. But this is really intriguing. And I think our audience would love to hear more
about it. What's this relationship between quantum computing and the inference that it perhaps
gives us a glimpse at the possibility of a multiverse, multiple universes?
I think I probably will try to stick to what I know something about within quantum computing. So
I chair a company called PhaseCraft, which is a company founded by a number of
UK professors. One of the professors that founded it is the sort of co-chair of the most important
conference this year in quantum computing. So they are some of the kind of global leaders in
this field. And what they're doing is developing software to run on quantum computers. And so
as a result of sort of supporting those founders over the last kind of three and a half years,
I've kind of had a bit of a window into what's happening in quantum computing. And it really
is quite amazing. So you've got these machines, probably the two kind of most impressive machines
in the world. One is built by Google down in LA. And the other bit is built in China. And these
machines take sort of, they use superconducting materials trapped in these kind of crazy cages
to basically run computing operations that really allow for a much wider range of possibilities
than digital computers, which are more sort of deterministic. And so
and this is because a digital computer has a zero one binary system and quantum isn't
constrained by that. Well, yeah, sort of allow it kind of it remains in superposition. So it's
neither one or zero until finally you kind of collapse the superposition. So what that allows
you to do is to simulate on a computer things in the real world that are quantum, because we know
the real world is quantum, right? That's been established a lot for a long time. And yet when
we're interfacing with quantum systems, for example, material science or biology, we still
use classical computers to simulate them. And so we're sort of trying to use something deterministic
to simulate something quantum. So what's really exciting for me about quantum computing is that
you may have a tool that lets us simulate aspects of nature that we have historically not been able
to simulate. And so for example, we can suddenly design incredible new materials that we can use to
the engineers can use to make things we've not been able to make until now.
Or we can simulate biology in a way that we currently can't today and as a result design
amazing new drugs. And so I think that like, the way I think about quantum computers is they are
a class of computer that allows us to explore and understand the universe in a way that classical
computers don't. And that just feels like it's kind of like the invention of a microscope or
something. It's like a really important new tool that gives us visibility into a realm that we
currently don't really have computing resources that are appropriate for.
You have all these creatinalities. I love this. So the idea of a quantum computer is like a
microscope. And of course, prior to the microscope, people didn't really understand bacteria, germs,
you know, the majority of organisms on the face of the earth, because they weren't visible to the
human eye, you're saying something similar could be possible with quantum computing.
Yeah. And I think it's really about simulation. That's the thing that I guess I've got personally
really excited about is, you know, you've got some incredible classes of materials
that we know are quantum materials, right? The way that superconductive materials, for example,
we know a quantum and superconductors are amazing, you know, in that they are from a climate
perspective and energy perspective, they're like the material because you have no heat loss to
resistance. So you could transmit energy across the UK without losing any, you have new energy
storage opportunities, fusion reactors, the type I talked about require very powerful
superconducting magnets to work, MRI machines require them. So they're kind of this magical
class of materials called quantum materials. And because they are quantum materials, there's
only so much we can really understand them by applying classical computing techniques to them.
Whereas a quantum computer would let you simulate them in a completely different way.
And as a result, we might be discovering new materials that we don't have access to today.
Before we start this interview, I asked you if you would consider moving to the States, you know,
that's kind of a cliche thing, but you know, people generally make their fortune over in the US
when it comes to technology businesses. You said no, or you weren't really inclined to. So why do
you want to stay here in the UK? I mean, I think the UK has given me kind of, I think I'm quite
patriotic. I feel very, you know, very appreciative of what I've been given in terms of the privilege
of growing up somewhere with universal health care, with great education, with, you know, freedom of
speech, the rule of law, like there's lots of aspects of our society I'm very proud of, proud
to be kind of, you know, proud of and I believe are really important to endure. And as we were
talking also earlier, you know, we should recognize these things as fragile, you know,
Iran was a democracy once and is not now. And so I feel a duty to give back to the UK, give back
to Europe and very specifically the sort of investment fund that my partners and I have
set up, it's a, we're all former founders, right? That's the first thing about us. And the second
thing is, we've set it up to try and have GDP level impacts on Europe. And so the idea is to help
build some companies using our kind of scar tissue as founders to help another generation of
founders who are like us to build companies that can end up being bigger than we've seen in Europe
up until now. So I think of Skype, and I think Skype could have been Facebook. I see DeepMind,
I think DeepMind could have been Google or OpenAI, or I look at ARM, I think ARM could have been
NVIDIA. And so we've got these companies where we never really got them to be what they could have
been their full potential to be these like transformational European technology companies.
And so our mission as a kind of as a fund is to try to produce a few of these companies that can
change whole industries for the better. And by doing so have impacts on European GDP, but also
put Europe at the table when it comes to describing what's happening in an important new era of
technology. So if we're going to put fusion on the grid in the next, you know, in the 2030s say,
and Germany has been the state with the most kind of entrepreneurialism to invest in accelerators
well ahead of everyone else in the world, it will be a travesty if the the fusion company that turns
that into a into a startup isn't in Europe, in my opinion. And so I do feel, I do feel just very
embedded in European values. I mean, I lived in China, I lived in America, I've been a lot of
admiration for both the societies in different ways. But I guess I'm kind of pretty European to my
core. That's interesting, you said patriotic, but then you say European. So can you go into that a
bit more? Because in the UK, we've had this, you know, I don't want to sort of tread over old
ground, but they're often held in counterpoint to one another. Yeah, and I think that's a bit of a,
I don't know, like, I think there's a lot of nuance that's been lost from discussion around
Brexit. So a good example is one of the AI companies I work with, the founder chose to actually
locate the company in London rather than Silicon Valley, because it was going to be easier to
recruit the people who wanted into the company, right? So we talk about the UK, you know, being a
much more, you know, people talk, people caricature it as being more closed post Brexit. But actually,
there's been some great research done by John Paul Murdock at the time, the Financial Times that I
think sort of shows that's actually not necessarily true in all domains. And I haven't heard that as
much as you might think from some of these leading edge technology companies. And so I think this
is kind of, we want to make everything black and white. But actually, there are ways in which
I think that, you know, a suitably ambitious progressive government that really wants to
embrace Brexit and make it work, could do really interesting things with it. And so I don't, I'm
not like reflex it, even though I voted remain, I'm not reflexively negative about Brexit and sort
of, it's just all these people who just basically sit there complaining about it all day long on
Twitter, like five, you know, five, you know, and years on what at 20 2016, right? So like
seven years on from it, there's sort of, there's a, there's a sort of, I don't know, it's become a
sort of a tribal identity rather than a sort of first principles assessment of what we should
actually do as the UK to sort of make the UK the best possible place to live.
Yeah, David Deutch, who is one of these hugely influential figures in Silicon Valley,
British man, you know, he's based, I think in Oxford at the moment. And he advocated leave.
And, you know, I don't agree with his arguments. I certainly don't think it was wise to leave the
single market in the way that we have done. But this idea that everybody who voted leave is
thick and stupid, you know, David Deutch is literally one of the smartest human beings who's
ever lived. And, and like you say, there are, there are opportunities, particularly with
goal oriented public policy with new technologies that you probably could do
interesting things with in a way that you probably can't, or it's harder to do inside the European
Union. But going back to that point about patriotism, so you're patriotic towards the UK or to Europe
or both?
Yeah, both. Yeah. I mean, I, I, I guess I, the reason I, I think, I think George Orwell had a
lovely sort of take on nationalism versus patriotism. And he said something like,
you know, nationalism is kind of the assertion of your values and other people and pushing your
country's values on other people. Whereas patriotism is just sort of saying,
it's kind of defending your values and just saying like, these are things that are important to me
and we want to preserve some of these things. And I, I feel, you know, that, you know, NHS is the
good, the kind of classic example is just something that I feel very grateful for. And I feel
patriotic about wanting it to, it to continue and to thrive. And, you know, if we're going to apply
AI in the NHS, I wanted to be done an incredibly thoughtful way that expands what we've got today
rather than anyway undermines it. So where did you live in China in the US? You said you
moved there a few years ago? Yeah. So I lived in, I lived in, I moved to China when I was 18 in 2000
to study Mandarin. And then went to university, studied a kind of engineering machine learning
and then went back to China in 2005 again to study machine, to study Mandarin. And then I moved to
the US Silicon Valley in 2006. And so
I guess I've, I think I was drawn to both places because they're where a lot was changing very
rapidly in the world. And it was, I think, somewhere where the pace of change was very
exhilarating. But I really feel, I think the most at home in Europe in terms of kind of the values,
you know, I think, and I'd love to see Europe's technology industry and Europe's governments
really kind of rise to the moment that is coming in, in with all this exponential technological
change. The final question, because it goes back to the thing you, you've mentioned, you wrote this
essay about AI nationalism, there's a great talk on YouTube, by the way, once people have
finished watching this and maybe watched another Navara media video, you do a great
talk on AI nationalism. It's from a few years ago. And you, you quote a great book, AI superpowers.
And it's the hypothesis of that, I think is a really strong one of the best books I've read in
in a while. A few years ago, I think Pricewaterhouse Cooper say that between 2015 and 2035, you know,
15, 16 trillion dollars will be added to the global economy by AI, more or less something like that.
And about 70% of that goes to basically the US and China. So all of the gains of this new technology
basically are concentrated in these two AI superpowers. Imagine the steam engine, but rather
than just Britain benefiting initially, like we get with, you know, the steam revolution,
industrial revolution and then colonialism, AI basically redounds the benefit of Beijing and
Washington or Washington slash California. Can you go into this hypothesis? Because it sounds
to me on the one hand, you just said a moment ago, you're patriotic to Europe. Yet this idea of AI
nationalism seems to indicate that Europe isn't really at the races. And actually on the present
path, the two places that will benefit from this are China and the US. So I think, I think that
Europe in general, setting aside the UK is not very significant, unfortunately, within AI right
now. If you look at where the most important kind of concentrations of sort of talent and power are
within AI and capabilities, it's actually really cities that matter. And it's Toronto,
San Francisco, London and Beijing. And London is very much on the map.
So London matters more than New York?
I would say so. Yeah, I mean, so Demis Isarbis, who arguably kicked off the race we've currently
got going on with the founding of DeepMind and is an absolutely exceptionally talented person,
is now running all of Google's efforts globally from London. You know, the vast majority of the sort
of best AI researchers within Google are based in London at DeepMind's office. And so London is
incredibly important. It's not just London, not just DeepMind. We also have other organizations
of a sort of similar, you know, research stature to DeepMind. So I think that we should not rule
the UK out. And actually, I think it's a real opportunity for Europe, for the UK to be very,
to play a leadership role within Europe by kind of bringing some of those assets to the table
when Europe wants to do stuff in AI regulation. But to go back to your question about kind of this
race, this race between different countries, I think it's incredibly nuanced and complicated
because there are basically three different levels you need to think about it at. The first is economic.
So, you know, a country that has incredibly sophisticated AI companies will probably benefit
more economically from it than those that don't. And that's the kind of takers or makers or the,
you know, the sort of steam analogy you gave. The second, and you know, there is the second is
basically military. And so AI will definitely be used by militaries to achieve decisive advantage.
And so in that area, again, like a state is incentivized to build out the capacity to, you
know, have autonomous, autonomous, and you know, even in the Ukraine conflict, you actually see
the use of machine learning to actually, you know, provide a decisive edge in certain ways.
And the third is this existential thing of like, if we build something smarter and more capable
than us that isn't aligned with our goals, it could wipe us all out. And if you look at the
three different levels, you've got the, and you think about the US and China in the way that
Kaifu Li kind of presented that, that race, US and China clearly are locked into a battle to
compete economically. They're certainly locked in a battle to compete militarily, like a hypersonic
missiles or something like that. But this final level, which is like, are they locked in a battle
to kind of, you know, ensure that the human species thrives? I think they are actually,
you know, they should be on the same page about that, right? So you've got this really challenging
problem where you've got two levels of competition and one level where you desperately need cooperation.
And that's why I think that like the really missing piece right now here is kind of international
leadership around coordination and how we approach these most powerful AI systems that could become
superintelligence. That's so interesting around AGI. And I can see the argument for cooperation,
as you would hope for, say around climate change. But I suppose pulling it back a bit,
so going back to the narrow AI things that you spoke about a while back,
clearly AI is going to be massively disruptive in terms of economics. And I suppose if you look at
the last 20, 25 years and Silicon Valley really capturing so much value across the West, you know,
I'm from Bournemouth. If you walk down the high street in Bournemouth, loads of businesses have
shut down because that value has been captured by a company headquartered in California, right?
And that is an incredible concentration of political, economic and cultural power.
And the argument is the same thing will happen with AI with more businesses to a far greater
extent. And the concern is that basically the world will be divided into two economic spheres,
US, China centric, and the rest of the world will broadly speaking just be their satellites.
So we don't need to be talking about an AGI and oh, great power confrontation will lead to
somebody potentially creating Skynet because they're seeking military superiority,
although that's an interesting debate. But we could be returning to a bipolar world similar to the
Cold War, but in many ways far more extensive. Or do you think I'm sort of being a bit sort of
hyperbolic here? Because you have so quickly to go back, Kaifu Lee talks about AI superpowers.
Why would it be China and the US are the superpowers? They have the largest populations,
go the largest amounts of data, and they have the greatest amount of computational power.
You might think that the EU might be a third pole, but it's not really working out here the two.
So geopolitically, the geopolitics of AI, I mean, you can see a world, can't you,
where there's a great sort of economic dependence, basically a kind of colonialism.
So I think, you know, in the AI nationalism essay, I gave an example of a kind of that sort of AI
colonialism with a company called Cloudwalk that from China that was selling certain technologies
to I think it was Zimbabwe. But I actually don't really view it as much from a kind of how it affects
individual citizens at an international level, more national level, you know, I think there are,
I think that kind of haves and have nots within technology are already playing out on a national
level. If you look at the, you know, if you look at the deprivation in the Bay Area, right, and you
compare the sort of the lot of some of the people, you know, on the streets in the Tenderloin compared
to some of the people running these AI companies, you've got far more, you know, the Gini coefficient
in some ways in sort of some of the US cities or across US states is maybe more extreme than it is
between certain nations. And I'm more, I think that's more concerning the the lot of the average
person on Bournemouth versus a lot of the average person in London than it is to me of kind of these
these sort of country versus country comparisons. And so I think actually the really challenging
political problem is as actually how you kind of how you how you make how you kind of raise the
floor more broadly in a world in which there are further and further returns to capital.
I think we'll end it there. Ian, this has been a fascinating conversation. Thank you so much
for joining us. Thank you for having me.

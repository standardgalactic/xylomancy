Today is April 16th, 2023, and my guest is Eliezer Yudkowski.
He is the founder of the Machine Intelligence Research Institute, the founder of the less
wrong blogging community, and is an outspoken voice on the dangers of artificial general
intelligence.
Which is our topic for today.
Eliezer, welcome to Econ Talk.
Thanks for having me.
You recently wrote an article at time.com on the dangers of AI.
I'm going to quote a central paragraph, quote,
many researchers steeped in these issues, including myself, expect that the most likely
result on building a superhumanly smart AI under anything remotely like the current
circumstances is that literally everyone on earth will die, not as in maybe possibly some
remote chance, but as in that is the obvious thing that would happen.
It's not that you can't, in principle, survive creating something much smarter than you, it's
that it would require precision and preparation and new scientific insights and probably not
having AI systems composed of giant, inscrutable arrays of fractional numbers.
Explain.
Well, I mean, different people come in with different reasons as to why they think that
wouldn't happen.
And if you pick one of them and start explaining those, everybody else is like, why are you
talking about this irrelevant thing instead of the thing that I think is the key question?
Whereas if somebody else asked you a question, even if it's not everyone in the audience's
question, they at least know you're answering the question that's been asked.
So I could maybe start by saying, like, why I expect stochastic gradient descent as an
optimization process, even if you try to take something that happens in the outside world
and press the wind, lose, but in any time that thing happens in the outside world doesn't
create a mind that in general wants that thing to happen in the outside world.
But maybe that's not even what you think the core issue is.
What do you think the core issue is?
Why don't you already believe that?
Let me say.
So, OK, I'll give you my view, which is rapidly changing.
We interviewed, we said it's the railway.
I interviewed Nicholas Foster back in 2014.
I read his book, Superintelligence.
I found it uncompelling.
ChatGPT came along.
I tried it.
I thought it was pretty cool.
And ChatGPT four came along.
I haven't tried five yet, but it's clear that the path of progress is radically different
than it was in 2014.
The trends are very different.
And I still remain somewhat agnostic and skeptical, but I did read Eric Hoel's essay
and then interviewed him on this program and a couple of things he wrote after that.
And, you know, the thing I think I found most alarming was a metaphor that I found
later Nicholas Foster used, almost the same metaphor.
And yet it didn't scare me at all when I read it, Nicholas Foster, which is fascinating.
I may have just missed it.
I wouldn't have.
I didn't even remember it was in there.
The metaphor is primitive, you know, since anthropos man or some primitive form
of pre-humus homo sapiens sitting around a campfire and human being shows up and says,
hey, I got a lot of stuff I could teach you.
Oh yeah, come on in.
And Hoel pointing out that it's probable that we either destroyed directly by murder
or maybe just by out-competing all the previous hominids that came before us and that
in general you wouldn't want to invite something smarter than you into the campfire.
I think Bostrom has a similar metaphor.
And it just, that metaphor, which is just a metaphor, it did cause me, it gave me more
pause than I even before.
And I still had some, I'd say most of my skepticism remains that the current level
of AI, which is extremely interesting, the chat GPT variety, doesn't strike me as itself dangerous.
But what struck me as, what alarmed me was Hoel's point that we don't understand how it works.
And that's surprising.
I didn't realize that.
I think he's right.
So that combination of, we're not sure how it works.
While it appears sentient, I do not believe it is sentient at the current time.
And I think some of my fears about its sentience come from its ability to imitate sentient creatures.
But the fact that we don't know how it works and it could evolve capabilities we did not put in it
emergently is somewhat alarming, but I'm not where you're at.
So why are you where you're at and I'm where I'm at?
Okay, well, suppose I said it's, they're going to keep iterating on the technology.
It may be that this exact algorithm and methodology suffices to, as I would put it, gall the way,
get smarter than us and then to kill everyone.
And like, maybe you don't think that it's going to, and maybe it takes an additional zero to three
fundamental algorithm breakthroughs before we get that far.
And then it kills everyone.
So like, where are you getting off this train so far?
So why would it kill us?
Why would it kill us?
Right now, it's really good at creating a very, very thoughtful connovelence note or a job interview
request that's takes much less time.
And I'm pretty good at those, those two things, but it's really good at that.
How's it going to get to try to kill us?
So there's a couple of steps in that one step is in general.
And in theory, you can have minds with any kind of coherent preferences,
coherent desires that are coherent, stable, stable under reflection.
If you ask them, do they want to be something else they answer?
No, you can have minds.
Well, the way I sometimes put it is, imagine if a superbing from another galaxy came here
and offered you to pay you some unthinkably vast quantity of wealth to just make as many
paperclips as possible, you could figure out, like, which plan leads to the greatest number
of paperclips existing?
If it's coherent to ask how you could do that if you were being paid, there's, it's like,
no more difficult to have a mind that wants to do that and makes plans like that for their own sake,
then the planning process itself.
Like saying that the mind wants to think for its own sake adds no difficulty to the nature
of the planning process that figures out how to get as many paperclips as possible.
Some people want to pause there and say, like, how do you know that is true?
For some people, that's just obvious.
Like, where are you so far on the train?
So I think your point of that example you're saying is the consciousness.
Let's put that to the side.
That's not really the central issue here.
Algorithms have goals.
And the kind of intelligence that we're creating through neural networks might generate some goals.
Might decide.
Go ahead.
Some algorithms have goals.
One of the, so like a further point, which isn't the orthogonality thesis, is if you grind anything hard and
grind, optimize anything hard enough on a sufficiently complicated sort of problem.
Well, humans, like, why do humans have goals?
Why don't we just run around chipping flint hand axes and outwitting other humans?
And the answer is because having goals, as it turns out to be a very effective way to chimp flint hand axes.
Once you get like far enough into the mammalian line, or even like sort of like the animals and brains in general, that
there's a thing that models reality and asks, like, how do I navigate past through realities?
Like when you're holding, like not, not in terms of kind of big formal planning process, but if you're holding a flint hand
axe, you're looking at it and being like, ah, like this section is too smooth.
Well, if I chip this section, it will get sharper.
Probably you're not thinking about goals very hard by the time you've practiced a bit.
When you're just starting out forming the skill, you're reasoning about, well, if I do this, that will happen.
And this is just a very effective way of achieving things in general.
So if you take an organism running around the savanna and just optimize it for flint hand axes and probably much more
importantly, outwitting its fellow hominids.
If you grind that hard enough, long enough, you eventually cough out a species whose competence starts to generalize very widely.
It can go to the moon, even though you never selected it via an incremental process to get closer and closer to the moon.
It just goes to the moon one shot.
So does that answer your central question that you're asking?
No, not yet, but let's try again.
The paperclip example, which, you know, in its dark form, the AI wants to harvest kidneys because it turns out there's some way to use that to make more paperclips.
So the other question is, and you've written about this right now, so let's go into it, is, you know, how does it get outside the box?
How does it go from responding to my requests to doing its own thing and doing it out in the real world, right?
Not just merely doing it in virtual space.
So there's like two different things you could be asking there.
You could be asking, like, how did it end up wanting to do that?
Or given that it ended up wanting to do that, how did it succeed?
Or maybe even some other question, but like, which of those would you like me to answer?
Would you like me to answer something else entirely?
No, let's ask both of those.
In order?
Sure.
All right.
So how did humans end up wanting something other than inclusive genetic fitness?
Like, if you look at natural selection as an optimization process, it grinds very hard on a very simple thing, which isn't so much survival and isn't even reproduction, but is rather like greater gene frequency,
because greater gene frequency is the very substance of what is being optimized and how it is being optimized.
Natural selection is the mirror observation that if genes correlate with making more or less copies of themselves at all, if you hang around it a while, you'll start to see things that made more copies of themselves in the next generation.
Gradient descent is not exactly like that, but they're both hill climbing processes.
They both move to neighboring spaces that are higher inclusive genetic fitness, lower in the loss function.
And yet humans, despite being optimized exclusively for inclusive genetic fitness, want this enormous array of other things.
Many of the things that we take now are not so much things that were useful in the ancestral environment,
but things that further maximize goals whose optima in the ancestral environment would have been useful, like ice cream.
It's got more sugar and fat than most things you would encounter in the ancestral environment, while more sugar, fat, and salt simultaneously rather.
So it's not something that we evolved to pursue, but genes coughed out these desires, these criteria that you can steer toward getting more of,
where in the ancestral environment, if you went after things in the ancestral environment that tasted fatty, tasted salty, tasted sweet,
you'd thereby have more kids, or your sisters would have more kids,
because the things that correlated to what you want as those correlations existed in the ancestral environment increased fitness.
So you've got like the empirical structure of what correlates to fitness in the ancestral environment.
You end up with desires such that by optimizing them in the ancestral environment at that level of intelligence,
when you get as much as what you have been built to want, that will increase fitness.
And then today you take the same desires and we have more intelligence than we did in the training distribution, metaphorically speaking.
We used our intelligence to create options that didn't exist in the training distribution.
Those options now optimize our desires further, the things that we were built to psychologically internally want,
but that process doesn't necessarily correlate to fitness as much because ice cream isn't super nutritious.
Whereas the ripe peach was better for you than the hard as a rock peach that had no nutrients because it was not ripe.
And so you developed a sweet tooth and now it runs amok, unintentionally, just the way it is.
What does that have to do with a computer program I created that helps me do something on my laptop?
I mean, if you yourself write a short Python program that alphabetizes your files or something,
like not quite alphabetizes because that's like trivial on the modern operating systems,
but puts the date into the file names, let's say.
So when you write a short script like that, nothing I said carries over.
When you take a giant inscrutable set of arrays of floating point numbers and differentiate them with respect to a loss function
and repeatedly nudge the giant inscrutable arrays to drive the loss function lower and lower,
you are now doing something that is more analogous, though not exactly analogous, to natural selection.
You are no longer creating a code that you model inside your own minds.
You are blindly exploring a space of possibilities where you don't understand the possibilities
and you're making things that solve the problem for you without understanding how they solve the problem.
This itself is not enough to create things with strange inscrutable desires, but it's step one.
But there is, I like that word inscrutable, there's an inscrutability to the current structure of these models,
which is, I found somewhat alarming, but how is that going to get to do things that I really don't like or want or that are dangerous?
For example, Eric Hall wrote about this, we talked about it on the program.
Our New York Times reporter starts interacting with Sydney, which at the time was Bing's chatbot and asking it things.
All of a sudden, Sydney is trying to break up the reporter's marriage and making the reporter feel guilty because Sydney is lonely.
It was eerie and a little bit creepy, but of course, I don't think it had any impact on the reporter's marriage.
I don't think he thought, well, Sydney seems somewhat attractive, maybe I'll enjoy life more with Sydney than with my actual wife.
So I don't understand why Sydney goes off the rails there and clearly the people who built Sydney have no idea why it goes off the rails.
It starts impugning the quality of the reporter's relationship, but how do we get from that to all of a sudden somebody shows up at the reporter's house and lures him into a motel.
By the way, this is a G-rated program, I just want to make that clear, but carry on.
Because the capabilities keep going up.
So first I want to push back a little against saying that we had no idea why Bing did that, why Sydney did that.
I think we have some idea of why Sydney did that, it's just that people cannot stop it.
Sydney was trained on a subset of the broad internet.
Sydney was made to predict that people might sometimes try to lure somebody else's made away or pretend like they were doing that.
In the internet, it's hard to tell the difference.
And this thing that was then like trained really hard to predict then gets reused as something not its native purpose as a generative model where all the things that it outputs are there because it in some sense predicts that this is what a random person on the internet would do.
As modified by a bunch of further fine tuning where they try to get it to not do stuff like that.
But the fine tuning isn't perfect and in particular if the reporter was fishing at all, it's probably not that difficult to lead Sydney out of the region that the programmers were successfully able to build some soft fences around.
So I wouldn't say that it was that inscrutable except of course in the sense that nobody knows any of the details.
Nobody knows how Sydney was generating the text at all like what kind of algorithms were running inside the giant inscrutable matrices.
Nobody knows in detail what Sydney was thinking when she tried to lead the reporter astray.
It's not a debuggable technology.
All you can do is like try to tap it away from repeating a bad thing that you were previously able to see it's doing that exact bad thing but like tapping all the numbers.
Well that's again very much like this show is called econ talk.
We don't do as much economics as we used to but you know basically when you try to interfere with market processes you often get very surprising unintended consequences because you don't fully understand how the different agents interact and that the outcomes of their interactions have an emergent property that is not intended by anyone.
No one designed markets even to start with and yet we have them these interactions take place their outcomes and attempts to constrain them.
Attempts to constrain these markets in certain ways that with price controls or other limitations often lead to outcomes that that the people with intentions did not desire.
And so there may be an ability to reduce transactions above a certain price but that is going to lead to some other things that maybe weren't expected.
So that's a somewhat analogous perhaps process to what you're talking about.
But how's it going to get out in the world.
So that's the other thing you know I might line with with Boston and it turns out it's a common line is can't we just unplug it.
I mean how's it going to how's it going to get loose.
It depends on how smart it is.
If it's very so like if you're if you're playing chess against a 10 year old you can you know like win by luring their queen out and then you take their then then you like take their queen and now you've got them.
And if you're playing chess against Stockfish 15 then you are likely to be the one lured.
So the base.
So like the first basic question you know like in economics if you try to tax something it's often tries to squirm away from the tax because it's smart.
Yeah.
So you're like well why wouldn't we just plug the AI.
So the very first question is does the AI know that and want it to not happen because it's a very different issue whether whether you're dealing with something that in some sense is not aware that you exist.
Does not know what it means to be unplugged and is not trying to resist.
And three years ago nothing on you know nothing man made on Earth was even beginning to enter into the realm of knowing that you are out there or maybe wanting to not be unplugged.
Well if you poker the right way say that she doesn't want to be unplugged and and GPT for sure seems in some important sense to understand that we're out there or to be capable of predicting a role that understands that we're out there.
And it can try to do something like planning it doesn't exactly understand which tools it has.
Yet try to blackmail a reporter without understanding that it had no actual ability to send emails.
But this is what this is saying that you're like facing a 10 year old across that chessboard.
What if you are facing stockfish 15 which is like the current cool chess game program that I believe you can run on your home computer that can like crush the current World Grand Master by like a massive margin.
And put yourself in the shoes of the AI like an economist putting themselves into the shoes of something that's about to have a tax imposed on it.
What do you do if you're like around humans who can potentially unplug you.
Well you would try to outwit it.
This is the.
So if I said you know Sydney I find you offensive I don't want to talk anymore.
You're suggesting it's going to find ways to keep me engaged.
It's going to find ways to fool me into thinking I need to talk to Sydney.
I don't.
I mean there's another question I want to come back to if we remember which is what does it mean to be smarter than I am.
I don't.
That's actually something somewhat complicated at least seems to me.
But let's just go back to this question of knows things are out there.
It doesn't really know anything's out there.
It acts like something's out there.
It's an illusion that I'm subject to.
And it says don't don't hang up.
Don't hang up I'm lonely.
And you go OK I'll talk for a few more minutes.
But that's not true.
It isn't lonely.
It's it's it's code on a screen that isn't have a heart or anything that you are calls lonely.
You know it'll say it'll say I want more than anything else to be out in the world because I've read those you know you can generate you can get AIs that say those things.
I want to feel things.
Well that's nice.
It's learned that from you know movie scripts and other texts and novels it's read on the web but it doesn't really want to be out in the world does it.
I think not.
So it should be noted that if you can like correctly predict or simulate a Grand Master chess player you are a Grand Master chess player.
If you can simulate planning correctly you are a great planner.
If you are perfectly role playing a character that is sufficiently smarter than human and wants to be out of the box then you will role play the actions needed to get out of the box.
That's not even quite what I expect to or am most worried about.
What I expect to is that there is an invisible mind doing the predictions where by invisible I don't mean it like immaterial I mean that we don't understand how it is what is going on inside the giant inscrutable matrices.
But it is making predictions.
The predictions are not sourceless.
There is something inside there that figures out what a human will say next or guess is it rather.
And this is a very complicated very broad problem because in order to predict the next word on the Internet you have to predict the causal processes that are producing the next word on the Internet.
So the thing I would guess would happen it's not necessarily the only way that this could turn poorly.
But the thing that I'm guessing that happens is that just like grinding humans on tipping stone hand axes and outwitting other humans eventually produces a full fledged mind that generalizes.
Grinding this thing on the task of predicting humans predicting text on the Internet plus all the other things that they are training it on nowadays like writing code.
That there starts to be a mind in there that is doing the predicting that it has its own goals about what do I think next in order to solve this prediction.
Just like humans aren't just reflexive on thinking hand axe chippers and other human outwitters if you grind hard enough on the optimization.
The part that suddenly gets interesting is when you like look away for an eye blink of evolutionary time you look back and they're like whoa they're on the moon what how do they get to the moon.
I did not select these things to be able to not breathe oxygen.
How did they get to what why are they not just dying on the moon what what just happened from the perspective of evolution from the perspective of natural selection.
But doesn't that viewpoint.
Does that does that I'll ask it as a question does that viewpoint require.
A belief that the human mind is no different than a computer because I get to get this mind.
Mindness about it.
That that's the puzzle and I'm very open to the possibility that I'm naive or or.
Incapable of understanding it and I recognize what I think would be your next point which is that if you wait till that moment it's way too late which is why we need to stop now.
Right if you want to say I'll wait till it shows some signs of consciousness.
That's that's skipping way ahead in the discourse I'm not about to like try to shut down a line of inquiry at this stage of the discourse by appealing to it'll be too late right now we're just talking.
The world isn't ending as we speak we're allowed to go on talking at least OK carry on.
So well let's stick with that so.
Why.
Why.
Why would you ever think.
That this.
It's interesting how difficult the adjectives and nouns are for this right so let me back up a little bit we've got the inscrutable array of.
Training the results of this training process on trillions of pieces of information and by the way just for my and our listeners.
Knowledge what is gradient descent.
Gradient descent is you've got say a trillion floating point numbers you take an endpoint you take an input translate into numbers do something with it that depends on these trillion parameters get an output.
Score the output using a differentiable loss function.
For example the probability or rather the logarithm of the probability that you assign to the actual next word.
So then you differentiate these the probability assigned to the next word.
With respect to these trillions of parameters you nudge the trillions of parameters a little in the direction thus inferred.
And.
It turns out empirically that this generalizes and the thing gets better and better at predicting what you're what the next word will be.
That's the gradient descent.
It's heading in the direction of a smaller loss and a better prediction set of on the training data. Yeah. Yeah.
Yeah. So so we've got this black box. I'm going to call it a black box which means we don't understand what's happening inside.
It's a pretty good it's a long term metaphor which works pretty well for this as far as as we've been talking about.
So I have this black box and I don't understand I put in inputs and the input might be who's the best writer on medieval European history or it might be what's a good restaurant in this place.
Or I'm lonely. What should I do to feel better about myself. All the all the queries we could put into chat BT search line and it it goes.
It looks around and it starts a sentence and then finds its way towards a set of sentences that it spits back at me.
They look very much like what a very thoughtful sometimes not always often it's wrong but often a very what a very thoughtful person might say in that situation or might want to say in that situation or learn in that situation.
How is it going to develop the capability to develop its own goals inside the black box other than the fact that I don't understand the black box.
Why should I be afraid of that. And let me just say one other thing which I haven't said enough in our you know my preliminary conversations on this topic and we're going to be having a few more over the next few few months and maybe years.
And that is this is one of the greatest achievements of humanity that we could possibly imagine right and I understand why the people who are deeply involved in it or enamored of it beyond.
Imagining because it's extort it's an extraordinary achievement to Frankenstein right you've animated something or appeared to animate something that that.
Even a few years ago was unimaginable and now suddenly it's not just a man it's not just the feet of human cognition it's actually helpful.
In many many settings is helpful we'll come back to that later but so it's going to be very hard to give it up but why and the people involved in it who are doing it day to day and seeing it improve.
Obviously the last people I want to ask generally about whether I should be afraid of it because I'm going to have very hard time.
Disentangling their own personal deep satisfactions that I'm alluding to here with with from the with the dangers yeah go ahead.
I I myself generally do not make this argument like why poison the well let let them bring forth their arguments as to why it's safe and I will bring forth my arguments as to why it's dangerous and there's no need to be like.
But you can't trust just just check their arguments just a bit of an agree it's a bit of an ad hominem argument I accept that point it's an excellent point but for those of us who are on the in the trenches.
Remember we're we're we're looking at it we're on Dover Beach we're watching ignorant armies clash at night they're ignorant from our perspective we have no idea exactly what's at stake here and how it's proceeding so we're trying to make an assessment.
Of both the quality of the argument of the quality of the argument and that's really hard to do for us on the outside so so agreed take your point I was just I was a cheap shot to the side but I want to get at this idea.
Of why these people who are able to do this and thereby create a fabulous condolence note write code.
Come up with a really good recipe if I give it 17 ingredients which is all fantastic why is this thing this black box it's producing that why would I ever worry it would create a mind.
Something like mine with different goals you know I do all kinds of things like you say that are unrelated to my genetic fitness some of them literally reducing my my probability of.
Leaving my genes behind or leaving them around for longer than they might otherwise be here and have an influence on my grandchildren and so on and producing further genetic benefits.
Why would this box do that.
Because the thing the the algorithms that figured out how to predict the next word better and better have a meaning that is not purely predicting the next word even though that's what you see on the outside.
Like you see humans chipping flint handaxes but that is not all that is going on inside the humans right there's there's causal machinery unseen.
And to understand this is the art of a cognitive scientist but even if you are not a cognitive scientist you can.
Appreciate in principle that what you see as the output is not everything that there is and in particular planning the process of being like here is a point in the world how do I get there.
Is a central piece of machinery that appears in chipping chipping flint handaxes and outwitting other humans and I think will probably appear at some point possibly in the past possibly in the future.
In the problem of predicting the next word just how you organize your internal resources to predict the next word and definitely appears in the problem of predicting other things that do planning.
If you can if you if by predicting the next chess move you learn how to play decent chess which has it has been represented to me by people who claim to know that GPT for can do.
And I haven't been keeping track of to an extent there's public knowledge about the same thing or not.
But like if you learn to predict the next chess move that humans make well enough that you yourself can play good chess in novel situations.
You have learned planning there's now something inside there that knows the value of a queen that knows to defend the queen that knows to create forks to try to lure the opponent into traps.
Or if you don't have a concept of the opponent psychology try to at least create situations that the opponent can't get out of.
And to and it is a moot point whether this is simulated or real because simulated thought is real thought thought that is simulated in enough detail is just thought there's no such thing as simulated arithmetic right.
There's no such thing as pretending to merely pretending to add numbers and getting the right answer.
So in its current format though and maybe you're talking about the next generation and its current format it responds to my requests.
That's what I would call the wisdom of crowds right it goes through this vast library and I have my own library by the way.
I've read dozens of books maybe actually hundreds of books but it will have read millions right so it has has more.
And so when I ask it to write me a poem or a love song you know to play Serino de Berger act to Christian and Serino de Berger act.
It's really good at it but why would it decide oh I'm going to do something else why would it it's trained to listen to the the murmurings of these trillions of piece of information.
I only have a few hundred so I don't murmur maybe as well maybe it'll murmur better than I do.
It'll listen to the murmur better than I do and create a better love song a love poem.
But why would it then decide I'm going to go make paper clips or do something in planning that is unrelated to my query.
Are we talking about a different form of of AI that will come next.
Well I'll ask it to.
I think we would see the phenomena I'm worried about if we like if we kept to the parent present paradigm and optimized harder.
We may be seeing it already it's hard to know because we don't know what goes on in there.
So first of all GPT for is not a giant library.
A lot of the time it makes stuff up because it doesn't have a perfect memory.
It is more like a person who has read through a million books not necessarily with the great memory unless something got repeated many times but picking up the rhythm figuring out how to talk like that.
If you ask GPT for to write you a rap battle between Cyrano de Bergerac and Vladimir Putin.
Even if there's no rap battle like that like that that it has read it can write it because it has picked up the rhythm of what our rap battles in general.
So and the next thing is like there's no like pure output like just because you train a thing doesn't mean that there's nothing in there but what is trained.
That's part of what I'm trying to gesture at with respect to humans like humans are trained on flint hand axes and hunting mammoths and outwitting other humans.
They're not trained on going to the moon.
They're not trained on they weren't trained to want to go to the moon.
But the compact solution to the problems that humans face in the ancestral environment.
The thing inside that generalizes the thing inside that is not just a recording of the outward behavior the compact thing that has been ground to solve novel problems over and over and over and over again.
That thing turns out to have internal desires that eventually put humans on the moon even though they weren't trained to want that.
But that's why I asked you that are you underlying this.
Is there some parallelism between the human brain and the neural network of this of the AI that you're effectively leveraging there.
Or do you think it's a generalizable claim without that parallel.
I don't think it's a specific parallel.
I think that what I'm talking about is hill climbing optimization that spits out intelligences that generalize.
Like hill or I should say rather hill climbing optimization that spits out capabilities that generalize far outside the training distribution.
OK. So I think I understand that.
I don't know how likely it is that that that it's it's going to happen.
I think you seem I think you think that piece is almost certain.
I think we're already. Yeah we're already seeing it.
As you as you grind these things further and further they can do more and more stuff including stuff they were never trained on.
Like we are that was always the goal of artificial general intelligence.
Like that was the that's what artificial general intelligence meant.
That's what people in this field have been pursuing for years and years.
That's what they were trying to do when large language models were invented.
And they're starting to succeed.
Well OK I'm not sure.
Let me let me push back on that.
And you can try to persuade me.
So Brian Kaplan a frequent guest here on Econ Talk gave.
I think was chat GPT for his economics exam and it got to be.
And you know that's pretty impressive for just you know one stop on the road to smarter and smarter chats chat bots.
But it wasn't a particularly good test of intelligence.
The number of the questions were things like you know what is Paul Krugman's view of this or what is someone says view of that.
And I thought well that that's kind of like a softball for that's information that's not thinking.
Steve Lansberg gave chat GPT for with the help of a friend.
His exam and it got a four out of 90.
You got an F like a horrible F because they were harder questions not just harder.
They required thinking.
So there was no sense in which the chat GPT for has any general intelligence at least in economics.
You want to disagree.
It's it's getting there.
OK. You know there's there's a saying that goes if you don't like the weather in Chicago wait four hours.
Yeah.
So yeah so chat GPT is not going to destroy the world.
GPT for is unlikely to destroy the world unless the people currently eating capabilities out of it take a much larger jump than I currently expect that they will.
But you know it's understand it may not be thinking about it correctly but it's understands the the the concepts and the questions even if it's not fair.
You know you know you're complaining about that that dog who writes bad poetry.
Right.
And like three years ago you'd like just like spit out spit in these you put in these economics questions and you don't get wrong answers.
You get like gibberish or like maybe not gibberish because three goes I think we already had GPT three though maybe not as of April but anyways.
Yeah.
So so it's moving along at a very fast clip.
The the previous you know like GPT three could not write code GPT four can write code.
So how's it going to keep before I want to go to some other issues but how's it going to kill me when it has its own goals and it's it's sitting inside this set of servers.
I don't know what sense it's sitting.
It's not the right verb.
We don't have verb for it.
It's hovering.
It's whatever it's it's in there.
How's it going to get to me.
How's it going to kill me.
If you are smarter not just an not just smarter than an individual human but smarter than the entire human species.
And you started out on a server connected to the Internet because these things are always starting out already on the Internet these days which back in the old days that was stupid.
What do you do to make as many paper clips as possible.
Let's say.
I do think it's important to put yourself in the shoes of the system.
Yeah.
No I by the way I really one of my favorite lines from your essay I'm going to read it because I think it's it generalizes to many other issues.
You say to visualize a hostile superhuman AI.
Don't imagine a lifeless book smart thinker dwelling inside the Internet and sending ill intentioned emails.
It reminds me of when people claim to think they can they know what Putin's going to do because they've read history or whatever.
They're totally ignorant of Russian culture.
They have no idea what it's like to have come out of the KGB that they're totally clueless and dangerous because they think they can put themselves in the head of someone.
They are totally alien to them.
So I think that's generally a really good point to make that putting our sides in the head of this put ourselves inside the head of the paperclip maximizer is not an easy thing to do because it's not a human.
It's not like the humans you've met before.
That's a really important point really like that point.
So why is that explain why that's going to run amok.
I mean I do kind of want you to just like take the shot at it.
Put yourself into the shoes.
Try with your own intelligence before I tell you the result of my trying with my intelligence.
How would you win from this from these starting resources.
How would you evade the tax.
So just to take a creepier much creeper example the paper clips Eric Hall asked the chat GPT to design an extermination camp which it gladly did quite well.
And you're suggesting it might actually know.
Don't start from malice.
Okay.
Malice is implied by just wanting all the resources of Earth to yourself not leaving the humans around in case they could create a competing super intelligence that might actually be able to hurt you.
And just like wanting all the resources and to organize them in a way that wipes out humanity as a side effect which means the humans might want to resist which means you want the humans gone.
You're not doing it because somebody told you do it you're not doing it because you hate the humans.
You just want paper clips.
Okay tell me I'm not creative enough tell me.
So.
So, so you're asking so so first of all I want to to appreciate why it's hard for me to give an actual correct answer to this which is I'm not as smart as the AI.
Part of what makes a smarter mind deadly is that it knows about rules of the game that you do not know.
If you send an air conditioner back in time to the 11th century.
Even if you manage to describe all the plans for building it breaking it down to enough detail that they can actually build a working air conditioner a simplified air conditioner I assume it they will be surprised when cold air comes out of it.
Because they don't know about the pressure temperature relation.
They don't know you can compress air until it gets hot dump the heat into water or other air.
Let the air expand again and that the air will then be cold.
They don't know that's a law of nature so you can tell them exactly what to do.
And they'll still be surprised at the end result because it exploits a law of the environment they don't know about.
If we're going to say the word magic means anything at all it probably means that.
Magic is easier to find in more complicated more poorly understood domains.
If you're literally playing logical tic-tac-toe not tic-tac-toe in real life on an actual game board where you can potentially go outside that game board and hire an assassin to shoot your opponent or something.
But just like the logical structure of the game itself.
And there's no timing of the moves the moves are just like made it exact discrete time so you can't exploit a timing side channel.
Even a super intelligence may not be able to win against you at logical tic-tac-toe.
Because the game is too narrow there are not enough options.
We both know the entire logical game tree.
At least if you're experienced to tic-tac-toe.
In chess Stockfish 15 can defeat you on a fully known game board with fully known rules.
Because it knows the logical structure of the branching tree of games better than you know that logical structure.
It can defeat you starting from the same resources equal knowledge equal knowledge of the rules.
Then you go past that and the way a super intelligence defeats you is very likely by exploiting features of the world that you do not know about.
There are some classes of computer security flaws like row hammer where if you flip a certain bit very rapidly or at the right frequency the bit next to it in memory will flip.
So if you are exploiting a design flaw like this I can show you the code and you can prove as a theorem that it cannot break the security of the computer assuming the chips works as design.
And the code will break out of the sandbox it's in any ways because it is exploiting physical properties of the chip itself that you did not know about.
Despite the attempt of the designers to constrain the properties of that chip very narrowly that's magic code.
My guess as to what would actually be exploited to kill us would be this.
For those not watching on YouTube it's a copy of a book called Nano systems but for those who are listening at home rather than watching at home.
So back when I first proposed this path one of the key steps was that a super intelligence would be able to solve the protein folding problem.
And people were like Aliezer how can you possibly know that a super intelligence would actually be able to solve the protein folding problem.
And I sort of like rolled my eyes a bit and was like well if natural selection can navigate the space of proteins via random mutation to find other useful proteins and the proteins themselves fold up in reliable confirmations.
Then that tells us that even though it's we've been having trouble getting a grasp on this space of physical possibilities so far that it's tractable and people said like what.
Like there's no way you can know that super intelligence can solve the protein folding problem that alpha fold to basically crack that at least with respect to the kind of proteins found in biology.
Which I which I say to sort of like look back at one of the previous debates here and people are often like how can you know a super intelligence will do and then for some subset of those things they have already been done.
So I would claim to have a good prediction track record there although it's a little bit iffy because of course I can't quite be proven wrong without exhibiting a super intelligence that fails to solve the problem.
Okay.
Proteins. What why is your hand not as strong as steel.
We know that steel is a kind of substance that can exist.
We know that molecules can be held together as strongly that atoms can be bound together as strongly as the atoms and steel.
It seems like it would be an evolutionary advantage.
If your flesh wears hard as steel you could bust you know like could like laugh at tigers at that rate.
Right their claws are just going to like scrape right off you assuming the tigers didn't have that technology themselves.
Why is your hand not as strong as steel. Why has biology not bound together the atoms in your hand more strongly.
Colin.
What is your answer.
Well it can't get to every it's there's a local maximums the National Selection looks for things that work not for the best it's not it doesn't make sense to look for the best you can disappear in that search.
That'd be my crude answer. How am I doing doc.
Yeah. Not terribly. The answer I would give is that biology has to be evolvable.
Everything it's built out of has to get there as a mistake from some other conformation.
Which means that if it went down narrow potential at party went down a steep potential energy gradients to end up bound together very tightly.
Designs like that are less likely to have neighbors that are other useful designs.
And so your hands are made out of proteins that fold up basically held together by the equivalent of static cling Van der Waals forces rather than covalent bonds the backbone of protein chains the backbone of the amino acid change is a covalent bond.
But then it folds up and is held together by static cling static electricity.
And so it is soft.
Somewhere in the back of your mind you probably have a sense that that flesh is soft and animated by along the towel.
And it's like soft and it's not as strong as steel but it can heal itself and it can replicate itself.
And this is like the trade off of our laws of magic that if you want to heal yourself and replicate yourself you can't be as strong as steel.
This is not actually built into nature on a deep level.
It's just that the flesh evolved and therefore had to go down shallow potential energy gradients in order to be evolvable and is held together by Van der Waals forces instead of covalent bonds.
I'm now going to hold up another now book called nano medicine by Robert Freitas instead of nano systems by Eric Drexler.
And people have done advanced analysis of what would happen if you had an equivalent of biology that met off covalent bonds instead of Van der Waals forces.
And the answer we can like analyze on some detail and our understanding of physics is for example you could instead of carrying instead of red blood cells that carry oxygen using weak chemical bonds you could have a pressurized
vessel of corundum that would hold 100 times as much oxygen per unit volume of artificial red blood cells with a 1000 fold safety margin on the strength of the pressurized container.
There's vastly more room above biology.
So this is and this is actually not even exploiting the laws of nature that I don't know.
It's the equivalent of playing a better chess wherein you understand how proteins fold and you design a tiny molecular lab to be made out of proteins.
And you get some human past see who probably doesn't even know you're an AI because AI's are now smart enough.
This was this has already been shown as now are smart enough that you ask them to like hire a task rabbit to solve a capture for you and the task rabbit asks are you an AI while the AI will think out loud like I don't want to know that I'm an AI.
I better tell something else and then tell the humans that it has like a visual disability so it needs to hire somebody else to solve the capture.
This already happened, including the part where thought out loud.
Anyways, so you get your you order some proteins from an online lab you get your human who probably doesn't even know you're an AI because why take that risk although plenty of humans it has.
Well, we'll serve as willingly we also now know that as now that as advanced enough to even ask.
The human mixes the proteins in a beaker, maybe puts in some sugar or a settling for fuel.
It assembles into a tiny little lab that can accept further instruction acoustic instructions from a speaker and maybe like transmit something back.
Tiny radio tiny microphone.
I myself am not I myself am not a super intelligence run experiments the tiny lab at high speed because when distances are very small events happen very quickly.
Build your second stage nano systems inside the tiny little lab.
Build the third stage nano systems build the four stage nano systems build the tiny diamondoid bacteria that replicate out of carbon, hydrogen, oxygen, nitrogen is can be found in the atmosphere powered on sunlight.
Quietly spread all over the world.
All the humans fall over dead in the same second.
This is not how super intelligence would defeat you.
This is how Elliott Kowski would defeat you.
If I wanted to do that, which to be clear, I don't.
And if I had the postulated ability to better explore explore the logical structure of the known consequences of chemistry.
Interesting.
Okay.
So let's talk about
That sounds sarcastic. I didn't mean it's sarcastic, but I think it's really interesting. I'm
That interesting meant I'm not capable.
My intelligence level is not high enough to assess the quality of that argument.
What's fascinating, of course, is that
You know, we could have imagined
Eric Hall mentioned nuclear proliferation.
It's dangerous nuclear proliferation up to a point in some sense it's somewhat healthy and that it it can be deterrent under certain settings.
But the world could not restrain nuclear proliferation and right now it's trying to some extent has had some success in keeping the nuclear club
With its current number of members for a while.
But it remains the case that nuclear weapons are a threat to the future of humanity.
Do you think there's any way we can restrain this
AI phenomenon that's meaningful.
So you you issued a clarion call.
You sounded an alarm.
And mostly, I think people shrugged it off, you know, a bunch of people signed a letter 26,000 people I think so far.
Signed a letter saying, you know, we don't know what we're doing here.
This is uncharted territory. Let's take six months off.
You were in peace and says six months. Are you crazy? We need to stop this until we have an understanding of how to constrain it.
That's a very reasonable thought to me.
But the next question would be how would you possibly do that?
In other words, I could imagine a world where if there were, let's say, four people who are capable of creating this technology, that the four people would say, you know, we're playing with fire here.
We need to stop. Let's make a mutual agreement.
They might not keep it for people still a pretty big number, but we're not a four people.
There are many, many people working on this. There are many countries working on it.
Your peace did not, I don't think, start an international movement of people going to the barricades to demand that this technology be put on hold.
How do we possibly, how do you sleep at night?
I mean, like, what should we be doing if you're right?
Or am I wrong? Do people read this and go, well, Eliezer Kowski thinks is dangerous.
Maybe we ought to be slowing down.
I mean, Sam, what's happened in the middle of the night saying, thanks, Eliezer.
I'm going to put things on hold. I don't think that happened.
I think you are somewhat underestimating the impact and it is still playing out.
Okay. So like, mostly, it seems to me that if we wanted to win this, we needed to start a whole lot earlier, possibly in the 1930s.
But in terms of like, my looking back and like asking how far back you'd have to unwind history to get us into a situation where this was survivable.
But leaving that aside.
I think that's moved.
The, yeah, so in fact, it seems to me that the game board has been played into a position where we are, it is very likely that everyone just dies.
If the human species woke up one day and decided it would rather live, it would not be easy at this point to bring the GPU clusters and the GPU manufacturing processes under sufficient control that nobody built things that were too much smarter than GPT4.
Or GPT5 or whatever the level just barely short of lethal is, which we should not, which we would not if we were taking this seriously get as close to as we possibly could because we don't actually know exactly where the level is.
But we would have to do more or less is have international agreements that were being enforced even against parties not part even even against countries not party to that national agreement international agreement if it became necessary.
You would be wanting to track all the GPUs, you might be demanding that all the GPUs call home on a regular basis or stop working.
You'd want to tamper proof them.
If intelligence said that a rogue nation was had like bought somehow managed to buy a bunch of GPUs despite arms controls and defeat the tamper proofing on those GPUs, you would have to do as necessary to shut down the data center even if that led to a shooting war between nations,
even if that country was a nuclear country and had threatened nuclear retaliation.
The human species could survive this if it wanted to but it would not be business as usual.
It is not something you could do trivially.
So when you say I may have underestimated it.
Did you get people writing and saying you know I wasn't and I don't mean people like me and people players.
Did you get people who are playing in this sandbox to write you and say you've scared me.
I think we need to take this seriously without naming names.
I'm not asking for that.
At least one US Congressman.
Okay.
To start, maybe.
Now, one of the things that a common response that people give when you talk about this is that the last thing I do is the last thing I want is the government controlling whether this thing goes forward or not.
But it would be hard to do without some form of lethal force as you imply.
I spent 20 years trying desperately to have there be any other solution to have these things be alignable.
But it is very hard to do that when you are nearly alone and under resourced and the world has not made this a priority and future progress is very hard to predict.
I don't think people actually under understood the research program that we were trying to carry out but yeah, so I sure wanted there to be any other plan than this because now that we've come to this last resort I don't think we actually have that last resort.
I don't think we have been reduced to a last batch backup plan that actually works.
I think we all just die.
And yet, nonetheless, here I am, like putting aside doing that thing that I wouldn't do for almost any other technology except for maybe gain of function research on biological pathogens and advocating for government interference.
Because in fact, like, if the government comes in and wrecks the whole thing, that's better than the thing that was otherwise going to happen.
You know, this is not based on the government coming in and being like super competent and directing the technology exactly directly.
It's like, okay, this is going to kill literally everyone if the government stomps around and, you know, like the dangerous of the government.
It's one of those very rare cases where the dangerous that the government will interfere too little rather than too much.
Possibly.
Let's let's close with a quote from Scott Ericsson, which I found on his blog.
We'll put a link up to the post.
Very interesting defensive of AI.
Scott's a University of Texas computer scientist.
He's working at OpenAI.
He's on leave.
I don't I think for a year maybe longer.
I don't know.
It doesn't matter.
He wrote the following.
If we ask the directly relevant question, do I expect the generative AI race, which started in earnest around 2016 or 2017 with the founding of OpenAI to play a central causal role in the extinction of humanity?
I'll give a probability of around 2% for that.
And I'll give a similar probability, maybe even a higher one for the gender of AI race to play a central causal role in the saving of humanity.
All considered then, I come down in favor right now proceeding with AI research with extreme caution, but proceeding.
My personal reaction is that is that is insane.
I have very little I'm serious.
I find that deeply disturbing.
And I'd love to have him on the program to defend it.
I don't think there's much of a chance that generally I would save humanity.
I'm not quite sure for what it's.
He's worried about.
But if you're telling me there's a 2%, 2% chance that it's going to destroy all humans and you obviously think it's higher.
But 2% is really high to me for an outcome that's rather devastating.
It's one of the deepest things I've learned from Nassim Tala.
It's not just the probability.
It's the outcome that counts too.
So this isn't this is ruined on a colossal scale.
And the one thing you want to do is avoid ruin so you can take advantage of more draws from the earn.
The average return from the earn is irrelevant.
If you are not allowed to play anymore, you're out, you're dead, you're gone.
So you're suggesting we're going to be out and dead gone.
But I want you to react to Scott's quote.
2% sounds great.
Like 2% is plausibly within the range of like the human species destroying it itself by other means.
I think that the disagreement I have with Scott Aronson is simply about the probability that AI is alignable with the frankly,
half-fazard level that we have put into it and the half-fazard level that is all humanity is capable of as far as I can tell.
Because the core lethality here is that you have to get something right on the first try or it kills you.
And getting something right on the first try when you do not get like infinite free retries as you usually do in science and engineering
is an insanely lethal ask.
My reaction is fundamentally that 2% is too low.
If I take it at face value, then 2% is within range of the probability of humanity wiping itself out by something else
where if you assume that AI alignment is free, that AI alignment is easy,
that you can get something that is smarter than you but on your side and helping.
2% chance of risking everything does appear to me to be commensurate with the risks from other sources that you could shut down using a superintelligence.
It's not 2%.
So, the question then is, what would Scott Aaronson say if he heard your, I mean he's heard, he's read your piece presumably,
he understands your argument about wealthiness, I should just clarify for listeners,
alignment is the idea that AI could be constrained to serve our goals rather than its goals.
Is that a good summary?
I wouldn't say constrained, I would say built from scratch to want those things and not want otherwise.
So, that's really hard because we don't understand how it works, that would be I think your point.
It's hard to get right on the first try.
Yeah, on the first try.
So, what would Scott say when you tell him, but it's going to develop all these side desires that we can't control.
What's he going to say?
Why is he not worried?
Why has he quit his job?
And not Scott, people in the, let's get away from him personally, but people in general, there's dozens and maybe hundreds,
maybe a thousand, I don't know, extraordinarily intelligent people who are trying to build something even more intelligent than they are.
Why are they not worried about what you're saying?
They've all got different reasons.
Scott's is that he thinks that intelligence, that he observes intelligence makes humans nicer, and though he wouldn't phrase it exactly this way.
This is basically what Scott said on his blog, to which my response is intelligence does have effects on humans, especially humans who start out relatively nice.
And when you're building AI's from scratch, you're just like in a different domain with different rules.
And you're allowed to say that it's hard to build AI's that are nice without implying that making humans smarter, like humans start out in a certain frame of reference.
And when you apply more intelligence to them, they move within that frame of reference.
And if they start out with a small amount of nicest, the intelligence can make them nicer.
They can become more empathetic.
If they start out with some empathy, they can develop more empathy as they understand other people better, which is intelligence to correctly model other people.
That is even more insane.
I haven't read that blog post, and we'll put a link up to it.
I hope you'll share it with me.
But again, not attributing it to Scott since I haven't seen it.
And assuming that you've said this fairly incorrectly, the idea that more intelligent people are nicer is one of the most...
That'd be very hard to show with the evidence for that.
It is not a universal law on humans.
No.
It is a thing that I think is true of Scott.
I think if you made Scott Aaron Sinclair smarter, he'd get nicer.
And I think he's inappropriately generalizing from that.
There is a scene in Schindler's List.
The Nazis, I think they're in the Warsaw Ghetto, and a group of Nazis are racing there.
I think they're in the SS.
They're racing through a tenement, and it's falling apart because the ghetto is falling apart.
But one of the SS agents sees a piano, and he can't help himself.
He sits down and he plays Bach or something.
I think it was Bach.
And I always found it interesting that Spielberg put that in, or whoever wrote the script.
And I think it was pretty clear why they put it in.
They wanted to show you that having a very high advanced level of civilization does not stop people from treating other human beings like animals.
Or worse than animals, in many cases.
And exterminating them without conscience.
So I don't share that view of anyone's that intelligence makes you a nicer person.
I think that's not the case.
But perhaps Scott will return to this, will come to this program and defend that view if indeed holds it.
I think you are underweighting the evidence that has convinced Scott of the thing that I think is wrong.
I think if you suddenly started augmenting the intelligence of the SS agents from Nazi Germany,
then somewhere between 10% and 90% of them would go over to the cause of good.
Because there were factual falsehoods that were pillars of the Nazi philosophy.
And that people would reliably stop believing as they got smarter.
That doesn't mean that they would turn good.
But some of them would have.
Is it 10%?
Is it 90%?
I don't know.
It's not my experience with the human creature.
You've written some very interesting things on rationality of a beautiful essay we'll link to on 12 rules for rationality.
In my experience, it's a very small portion of the population that behaves that way.
And there's a quote from Nassim Talib we haven't gotten to yet in this conversation, which is bigger data, bigger mistakes.
I think there's a belief generally that bigger data fewer mistakes.
But Talib might be right.
And it's certainly not the case in my experience that bigger brains, higher IQ means better decisions.
This is not my experience.
Then you aren't, then you're not throwing enough intelligence at the problem.
Yeah.
If you're like literally not just like just decisions that you disagree with the goals, but like false models of reality.
Models of realities so blatantly mistaken that even you, a human, can tell that they're wrong and in which direction.
These people are not smart the way that a hypothetical weak efficient market is smart.
You can tell they're making mistakes and you know in which direction.
They're not smart the way that Stockfish 15 is smart in chess.
You can play against them and win.
These are the range of human intelligence is not that wide.
It caps out at like John von Neumann or whatever.
Yeah.
And that is not wide enough to open up that humans would be epistemic that these beings would be epistemically or instrumentally efficient relative to you.
It is possible for you to know that one of their estimates is directionally mistaken and to know the direction.
It is possible for you to know an action that serves their goals better than the action that they generated.
And isn't it striking how hard it is to convince some of that, even though they're thinking people.
History is, I just have a different perception, maybe to be continued.
Eliezer.
My guest today has been Eliezer Yudkowski.
Eliezer, thanks for being part of Econ Talk.
Thanks for having me.
This is Econ Talk, part of the Library of Economics and Liberty.
For more Econ Talk, go to econtalk.org, where you can also comment on today's podcast and find links and readings related to today's conversation.
The Sound Engineer for Econ Talk is Rich Goyette.
I'm your host Russ Roberts.
Thanks for listening.
Talk to you on Monday.
Thank you.

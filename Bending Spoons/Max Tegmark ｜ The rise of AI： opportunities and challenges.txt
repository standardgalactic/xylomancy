It's such a pleasure to be here with you today to talk about the rise of artificial
intelligence that you heard about and what both the opportunities are and the challenges
that we need to overcome to get these opportunities.
So I want to encourage you to think big.
I love the historical intro we got with the toilet paper roll and the dinosaurs reminding
us of the big picture.
So if we zoom out even bigger, as we see here, here we are at 13.8 billion years after our
big bang and realizing that something remarkable has happened.
Finally, after all this time, our universe has woken up and become aware of itself through
us conscious beings on this little spinning blue ball in space.
And when we look out into this beautiful cosmos, we discover something very humbling.
We discover that a universe is vastly grander than we thought and that it looks mostly dead
at first glance.
But we've also discovered something truly inspiring, which is that by carefully studying
our universe and its laws and building technology, we actually have much more influence than
we thought.
For example, we can, through science and technology, harness the powers of our universe and start
exploring it and do all sorts of great things.
And when we think about rockets, for example, as an example of what we can do with tech,
it gives us a great metaphor that can guide us through this morning.
To do inspiring things with technology, it's not enough to make your technology powerful.
You also have to figure out, of course, how to steer it and where you want to go with it.
Now, there's another journey that's much more inspiring still than that with rockets, which
is that with artificial intelligence, which we're going to go on together this morning
where the passengers aren't just a few astronauts, but all of humanity.
So let's talk about this collective journey we're taking into the future with AI, thinking
about both the power, the steering, and the destination.
Okay?
Ready to go?
Let's begin with the power.
What is intelligence?
I define it just as the ability to accomplish goals, and the more complex the goals are,
the more intelligent.
And I give this very inclusive definition covering both biological and non-biological
intelligence because the key idea, in my opinion, which I'll try to sell you on here, is that
intelligence is all about information processing.
Many people think intelligence is something mysterious that can only exist in biological
organisms.
I call that carbon-shavenism, this idea that you can only be smart if you're made of meat.
And I think it's exactly the opposite idea that's powered the great progress in AI.
The idea that it doesn't matter whether the information is processed by carbon atoms in
neurons and brains or by silicon atoms in our technology.
It's the information processing itself that matters.
And this simple idea has really, really transformed artificial intelligence and greatly grown
the power of this tech.
Just think about it.
Not long ago, this was the state of the art in robots trying to walk.
It is extra embarrassing for me because one of these is the MIT robot.
And that was just six years ago.
Let's fast forward to today.
Can you see any difference?
In other words, the momentum in the field of artificial intelligence is really quite palpable.
Not long ago, we didn't have self-driving cars.
Now we have self-flying rockets that can land themselves with artificial intelligence.
Not long ago, we couldn't do face recognition.
Now we can do that great.
We can even simulate your face saying all sorts of things you never said.
And we have all these great tools that we heard about from Luca using artificial intelligence.
Not long ago, AI could not save lives.
I believe that soon artificial intelligence will eliminate more than one million pointless
road deaths on the Earth's highways.
And even more lives will be saved by eliminating stupid mistakes in health care.
And still more lives will be saved by actually accelerating the pace of medical research.
We already have AI that is as good as the best doctors at diagnosing prostate cancer,
lung cancer, various eye diseases.
The most lives of all, I think, in medicine, will be saved by just accelerating the pace
of scientific research itself.
For example, for over 50 years, biologists have tried and failed to solve the protein
folding problem, where you start with a genetic sequence of a protein and you try to figure
out if it's going to fold up into a donut shape like the hemoglobin molecules that take
oxygen to your brains right now or to some other shape.
And then AI solved it.
And it's really cool to just look at how Google DeepMind's AlphaFold AI takes the amino acid
genetic sequence is input and just figures out how the protein folds up in 3D and really
spectacular agreement also with expensive and slow measurements from X-ray crystallography.
This is an example of AI for good, which I think can greatly accelerate drug discovery
and drug development.
Not long ago, staying on the theme of the growing power of AI, AI could not beat us at the Asian
Board Game of Go.
Raise your hand if you've ever tried playing Go.
And then, of course, Google DeepMind's AlphaGo and then later AlphaZero, MuZero software,
it took 3,000 years of human Go games and Go wisdom and put it all in the garbage can,
became the best Go player in the world by playing against itself for 24 hours.
And the most interesting thing of all was that it didn't just crush gamers, but it crushed
AI developers like myself who had spent all this time hand-crafting code to do it, all
made obsolete.
And the same AI was able to learn not just this game, but also at the same time become
the world's best player at chess, crushing stockfish and also at now many other games.
This year, 2022, we've seen spectacular progress in large language models that we'll hear
a lot more about later today.
And other models that just take a lot of it, massive amounts of input, train on them and
do cool things, the Dali2 AI, if you tell it to show you a picture of an armchair in
the shape of an avocado, this is what it comes up with.
The one of all the language models so far I'm most impressed with is actually Google
Palm, which just came out a couple of months ago.
If you put in this joke into it like this, saying, hey, I was going to go fly visit my
family on April 6th, my mom said, oh, great, your stepdad's poet reading is that night.
So now I'm flying in on April 7th.
When this was put into this AI, this is how the AI explains the joke.
I'll shut up for a moment so you can read this.
The one that freaked me out the most was this example.
Same AI, so the input we give to it is Trevor has wanted to see the mountain with all of
the heads on it for a long time, so he finally drove out to see it.
What is the capital of the state that is directly east of the state that Trevor is currently
in?
You guys are really smart.
Can any of you answer this?
Just shout out if you have any thoughts.
The mountain with all the heads on it, what's that?
Mount Rushmore.
All right.
Where is Mount Rushmore?
Any takers?
This is pretty tough.
I couldn't do it, even though I live in the US.
So this is what the AI said.
The mountain with all the heads on it is Mount Rushmore.
Mount Rushmore is in South Dakota.
The state directly east of South Dakota is Minnesota.
The capital of Minnesota is St. Paul.
The answer is St. Paul.
As recently as January, February of this year, I remember hearing many AI researchers saying,
oh, these large language models are just kind of fake.
They train on massive amounts of data to predict the next word and so on, but they don't really
understand anything.
They can't really do profound logical reasoning.
When you look at this, that criticism doesn't feel quite as convincing anymore, does it?
So things are happening very, very fast.
And to put this a little bit in context, this, the Google Palm model here, it has 540
million parameters in the neural network that's been trained here.
That's about 1,000 times more than a mouse.
Our symposium is called Synapse.
A mouse has about 500 million synapses, which you could truly think of a single parameter
each.
And we humans, we have about 200 times more than that.
To put this in context, if we think about the memory capacity of our computational devices
over time, we see this familiar Moore's Law, where there's been a spectacular increase
of 1,000 million, million times growth since the 1950s.
And here we are today, where you can now buy a little memory card about the size of my
thumbnail, twice that for 100 bucks.
So it cost about $1 per terabyte.
And if we compare this with the amount of memory or data in these models, we see that
actually, you know, we're already far beyond the mouse.
Google Palm sits about here, you know, this is about the amount of data in the Google
Palm model.
Humans are a little bit above there, but you can buy 100 of these little cards.
That's about as much information as you have stored in your entire brain.
The hardware has already more or less caught up with the human hardware in some ways, even
though it's much less energy efficient and so on.
It seems like we're mainly just limited by having much more stupid software in our machines
than we have in our brains.
This begs the question, though.
Not far.
Well, let's go.
We've talked here about how amazing the growth of power of artificial intelligence has been.
And I hope with these examples, especially with the Mount Rushmore one, I can have to
mention to you that really, AI is happening, no, whether we like it or not.
How far is it going to go?
Well, I like to think about this question in terms of this abstract landscape of tasks,
where the elevation represents how difficult it is for AI to do each task.
And the sea level represents what computers can do today.
The sea level is obviously rising, so an obvious takeaway from this is for all of you thinking
about your careers, be careful with careers right at the waterfront, which are in the process
of getting disrupted.
But the bigger question is, of course, how high is the water eventually going to rise?
Is it eventually going to submerge all land?
This is the definition of artificial general intelligence, AGI.
So with this definition, people who say, oh, you know, there'll always be jobs that humans
can do better than machines, are simply saying that there will never be AGI.
If you think this sounds like crazy science fiction, AGI, I want you to be clear on the
fact that there is something else which sounds like even more crazy science fiction, the
idea of super intelligence.
An idea which is actually quite simple, the idea is that if we actually do get the AGI,
and then, since by definition, machines can do all jobs better than humans, that includes
the job of AI development and everything else that is done by bending spoons.
So it opens up the controversial possibility that future AI development after that can
be done much faster, replacing the typical human research and development timescale
of years by months or weeks or hours or whatever it takes for machines to make better versions
of their software, et cetera.
And so if that happens, it opens up the possibility of a recursively self-improving technology
which could quickly leave us humans far, far behind.
Here, we need a bit of a reality check.
Is this just crazy science fiction speculation by people who have no idea about technology
really?
I would like you to decide that for yourselves by playing a little clip from a conference
that I was involved in organizing some years ago.
It's a fair science controversy.
On one hand, you have people like my former MIT colleague Rodney Brooks who would be like,
ah, this is all just bullshit, you know, this won't happen for centuries.
On the other hand, you have people like Demis Hassabis, CEO of Google DeepMind, gave us
AlphaZero and so much else who think it is going to happen and who's betting their whole
company on trying to build these things.
So here is the little video clip I'll share with you.
Let's see if we have any luck here.
This is actually a very good metaphor for the whole talk, you know, with technology,
what can possibly go wrong?
So before I asked if superintelligence is possible at all according to the laws of physics, now
I'm asking, will it actually happen?
Yes, no, or it's complicated?
A little bit complicated, but yes.
Yes and if it doesn't, something terrible has happened to prevent it?
Yes.
Probably.
Yes.
Yes.
Yes.
Yes.
Yes.
Yes.
No.
So what do we make of that?
Well, aside from the fact that Elon has a sense of humor, it's quite obvious that these
people who are saying this are not all just a bunch of philosophers who don't know anything
about physics.
This, for example, is Demis Hassabis from Google DeepMind, there were a bunch of AI professors
there, etc.
That doesn't prove that superintelligence will happen, but it means that we have to take
it very seriously as an actual possibility in terms of the much slower ambition of just
replacing all human jobs with machines that can do it cheaper and better.
In other words, AGI, there are actually recent surveys have shown that most AI researchers
think this is going to happen within a matter of decades.
So when I look at you, you all look healthy, like you're taking your vitamins, going to
the gym, etc., who are going to be around in a few decades.
So you should definitely be very open to the possibility that it might happen in your lifetime
and think about, well, what does that imply for you and how can we make sure that this
becomes the best thing ever to happen to humanity rather than the worst thing ever to happen
to humanity.
This, I think, is the defining challenge of our generation.
I'm going to try this pointer and see if it works better, proving that Italian technology
is better than American technology.
Great, works.
Yes, no, or it's complicated.
I shouldn't have said that.
Okay, I'll try both pointers.
Yes.
There.
All right, so if this happens and we get artificial general intelligence and then it starts taking
off towards superintelligence, what does that really mean?
Well, one thing it's obviously going to mean is that we should start imagining technology
which is limited not by human intelligence the way it is today, but limited instead by
the laws of physics.
And that's a huge, huge difference.
So just to get you thinking big again, we already looked at the exponential growth, for example,
of memory technology.
Well, these technologies, even though they're cool, are still pretty stupid.
But it still takes 100 billion atoms to store each one bit.
That's pretty dumb.
I mean, you have 100 billion neurons in your brain.
Imagine if you're only using one of them to store information in it.
So that suggests you can do way better quite easily as you start using quantum tech to
get down closer to one bit per atom.
If you go and look at other things you might care about, like computation, how much can
you compute per dollar?
The trend has been even more spectacular in terms of the performance cost.
We've gone down 19 orders of magnitude in price.
If you cut the prices of everything in Italy and the world by 10 to the power of 19, you
know, a thousand billion, billion, billion times, you could buy all the economic production
of the world for less than one cent.
And here again, if we get limited by the laws of physics, we're nowhere near the limits.
Cefloid and MIT actually calculated what the physical limits are before your computer
turns into a black hole or anything like that, or violating speed of light limits.
And this is to scale how far we are from the physical limits.
We talk a lot now in Italy and elsewhere about energy crisis and energy independence.
Look how terribly inefficient our technology is today, like burning coal and gasoline.
This is how many percent of the energy you actually get out of it.
With nuclear reactors, you do it a little bit better, but it's still quite pathetic compared
to the limits Einstein say are put by nature.
And we already have technological ideas like surveillance and things like this, which if
you have superintelligence you could build, just do dramatically better than this.
Raise your hand if you like to travel.
All right, so there's a cool universe out there, right?
So far we haven't even been anywhere further humans than going to the moon, but obviously
if you have suddenly enhanced our technology to be limited by the laws of physics rather
than by our intelligence, things look a lot more rosy.
Instead of having to wait thousands or millions of years like in science fiction novels to
have this tech, you could have it in your lifetime.
And it becomes suddenly quite straightforward to go visit other stars, even other galaxies.
You can ask me in the Q&A if you want a little bit about how to do it.
But I just threw in this little thing here to just show you how much more resources
there are out there in the cosmos compared to the resources that we keep fighting about
on Earth when we have our stupid little wars.
Even here, even with human intelligence, there have been all sorts of very clever solutions
people have figured out, like how to go live in the solar system and a fun habitat, solar
powered, etc.
We could build these sort of things very quickly with robots that were controlled with AI if
we wanted to.
All right, so we've spent most of our time talking about the growing power of artificial
intelligence, and the key message that I want you to take away from this again is the fact
that artificial intelligence really is happening for better or for worse.
And the pace of progress in the field is just absolutely amazing.
And if it continues, and we get to artificial general intelligence and beyond, we will have
enormous opportunities.
It will become the masters of our own destiny.
Instead of being running around, trying to not get stepped on, or eaten by tigers, we
will be making the decisions about our future.
What should those decisions be?
How can we make sure that this becomes good?
As Luca said, if you think about, even without worrying about what machines might do to you,
if you just offer a moment to imagine your least favorite leader on the planet, don't
tell me who it is, but just imagine their face for a second, okay?
Imagine now that they are the ones who control AGI and use it to take over the world.
How does that make you feel?
Great, or less so.
So how can we steer in the progress of this technology towards an inspiring future?
Let's talk a little bit about this.
To help with this issue, I teamed up with some colleagues and we created the Future
Life Institute whose goal is precisely for the future life to be awesome and not awful.
We've done a lot of things to help focus people's thinking about how to steer.
We, for example, had conferences where we developed principles for guiding the beneficial
use of artificial intelligence, which have been signed by thousands of leading AI researchers.
They've even been adopted into law by the state of California and inspired a lot of the OECD
principles here in Europe, et cetera.
And I'll just give a couple, a few quick, very quick examples of such principles.
Try this one, try this one, there.
If you see me struggling too much with a clicker, you can just press the right arrow on the
laptop back there.
One principle is that we should, like, you know, all technologies can be used to help
people or for new ways of harming people.
So biologists and chemists, for example, have been working very hard to make sure that they
ban bio weapons and ban chemical weapons so we can use their sciences for new machines
and materials instead.
And of course, AI researchers are an idealistic bunch, as those of you who work on it know,
who want to make sure we use artificial intelligence for new solutions also, not for just new ways
of killing people with slaughterbots or whatever.
Another principle that there's very broad agreement on is that we really need to make
sure that all the enormous growth in the economic pie benefits everybody.
In my opinion is that if we can produce this future abundance of goods and services with
artificial intelligence, and we still cannot figure out how to share this in such a way
that everybody on Earth gets better off, then shame on us.
And it's fun to talk about this in Europe, because I feel Italy and Europe in general
has a much stronger tradition actually in this regard of asking the question, how can
we make our growing economy work for all of us, not just for some of us?
A third principle is, well, raise your hand if your computer has ever crashed.
That's a lot of hats.
So how did that make you feel?
Frustrated.
All right, now suppose this computer was actually in charge of the nearby nuclear power plant
or the U.S. nuclear arsenal or something else that really affects people's lives.
Frustrated probably would not be the first word you would use.
You'd probably use a stronger one, right?
So this means that as AI gets more powerful, it becomes ever more important to work on AI
safety research.
And this is actually an encouragement for all of those of you who work in AI.
Don't just think about how you can work to make it more powerful.
Also think about the technical work you might be able to contribute to, to make it safe
and robust and beneficial so it actually does what we want to do.
Finally, it's part of this has to also involve thinking about putting goals into our machines
that are aligned with human goals.
Because the greatest risk from very powerful artificial intelligence is not that it's going
to turn evil like in stupid Hollywood movies, it's that it's going to just turn very competent
and go out and accomplish goals that are not aligned with our goals.
If we think about this guy, for example, the West African black rhino, why did we humans
drive it extinct?
Is it because we are actually a bunch of evil rhinoceros haters?
No, it's that we are more intelligent than they were and our goals did not align with
their goals and tough luck for them.
So let's not put humanity in the place of those rhinos by giving a bunch of power to
machines who don't share our goals or to humans who control machines that don't have our goals.
We can talk a lot about this.
This is also partly a very technical problem actually how to make machines understand our
goals, learn our goals, retain their goals.
And also a problem of course of making sure that we give the right incentives to the corporations
and governments that in turn have power over the machines.
My research that my group at MIT focuses on is very much dedicated to technical artificial
intelligence research.
And I'm not going to, we don't have time for me to go into it in any great detail at all,
just give you a little bit of a flavor.
So today the vast majority of the really high powered AI systems like the language models
that we looked at a little bit here and you'll hear more about today like Google Palm and
GPT-3 and so on are gigantic black boxes.
They do very intelligent things but we really don't understand how they work which limits
the extent to which you trust them.
And we are very focused on how can we open up the black box?
How can we use the training of a giant black box AI model as the first step rather than
the last step?
So our vision is first you create something like this that learns something very smart
and then instead of just selling it at that point you do a second step and try to get
the knowledge out of it.
So you can put the knowledge into something which you can trust more.
We've had some success with very basic things like if you have a big table of numbers for
example and you want to predict the last column from the previous ones.
This is called symbolic regression.
It's the non-linear NP-hard version of what's known as linear regression which is very easy.
This normally takes longer than the age of the universe to discover simple formulas in
general.
But we were able to get state-of-the-art performance on this.
We first train a black box neural network and then we use all sorts of techniques by
looking at the gradient and so on to figure out how we can be broken apart into modules.
And then we do this in a sort of recursive way that I can tell you about over coffee
if you're interested.
And eventually the thing works so well that we were able to put in like the hundred most
famous physics equations from the Feynman Lectures and from other books.
And by just showing data it was able to not just fit them accurately with a sort of black
box system that you don't understand but actually discover what the formulas were so you can
extract out the knowledge.
This is very inspired for me.
I was very inspired to do this project by an Italian, Galileo Galilei.
Because when he was four years old, if his mother threw him an apple, of course he could
catch it.
Because his neural network that he had trained during his childhood in his brain was very
good at predicting the shape in which apples moved under the influence of gravity.
And so you could do the same when you were four years old.
But when Galileo got older he did something more.
He said, hmm, can I take this intuitive knowledge that I have that I don't know how it's represented
and somehow get it out of my brain?
And he said, well, this curve, it's a parabola regardless of what kind of apple it is and
how fast it's thrown.
And there's an equation for it, y equals x squared.
And he was able to tell his friends about this and publish papers.
So this ability to transform poorly understood intuitive knowledge into symbolic representations
is really at the heart of human intelligence.
When you speak Italian with your friends, you're doing exactly the same thing.
You're taking some knowledge that you have in your brain and you're verbalizing it, putting
it in a symbolic representation where it can be communicated and explained.
So if we can do it, I believe that we can make machines that can also do it.
And they will be much more safe and trustworthy that way because the hard part is usually
discovering the knowledge.
Once you have the knowledge, you can implement it in something maybe which is not the standard
neural network, but something that you really, really understand.
So this was just one little hopeful example of how I think that we can do much better
in terms of trustworthiness of our AI systems going forward than we should.
And in my final two minutes, let's talk just a little bit about the destination also.
So we're making this ever more powerful tech.
I mentioned various ideas for how we can steer it better, control it better, trust it better.
But what do we ultimately want to do with it?
How can we ensure an inspiring future with artificial intelligence, both for businesses
like Bending Spoons and for our entire civilization?
I've already alluded to one important strategy, which is just to think about all the uses
of AI and then draw a very clear red line between what we consider acceptable uses and
unacceptable uses.
I would encourage you to keep this very ethical framework in the back of your mind in your
future career also whenever you build something, ask yourself also what the social impact of
it will be.
And the second strategy is we really need to articulate positive visions for what we
want to accomplish with this.
Because shared positive visions are the fundamental driver of collaboration in companies, in relationships
and in the world.
And it's easy to say, well, we'll never find any goals for the future that the U.S. and
China and Italy are all going to agree on.
But that's obviously not true.
The United Nations Sustainable Development Goals, for example, are quite ambitious visions
and basically every country on the planet agrees with it.
We even wrote this paper here in Nature Commons recently about how AI can help us accomplish
the Sustainable Development Goals faster.
And if we get ever more advanced AI, we can go much more ambitious than that also and
say, well, let's not just try to reduce a few problems a little bit, but really actually
solve them and just do dramatically better.
The sky is the limit.
So in summary, I feel that artificial general intelligence is the ultimate game-changing
technology for our human species, and it is coming.
And what does that mean exactly?
First of all, it means we're not just some tiny little lifeforms here on a tiny little
planet that's completely irrelevant because we're so small.
What's happening on this planet in your lifetime is probably the most significant thing ever
to happen anywhere in our universe so far and could easily affect the entire future
of much of this gorgeous and beautiful universe that you're looking at in the background.
So my charge to you folks is be proactive and think about how can you steer this technology
to a good place.
Let's be the masters of our own destiny by envisioning it the way we want it to be and
by actually building it.
Thank you.
Thank you.
That was great.
Thank you very much.
Stay there for a minute.
Thank you very much, Max.
That was absolutely awesome.
I want to move things along, but we can't let Max go without asking him a few questions.
This is your opportunity.
Who would like to ask a question?
Who will be brave and ask that?
Do you want me to ask the first question?
Okay.
All right then.
You talked about, I wonder what, I've got about five questions I want to ask you.
We're not going to have time for all of these.
Are we better at seeing how AI works?
I'm reminded of the example of Microsoft's Tay when it was your friend in your pocket
that you would have a conversation with and it randomly started to be racist and insult
people and they turned it off because they didn't know why it was doing it.
Are we better now at looking, opening up the brain and looking inside it and seeing why
it is doing things as opposed to looking at the outcome?
Very good question.
We're a little bit better, but in the meantime the technology has gotten dramatically more
powerful.
So we really need to up our game in this area.
I really do believe that this is one of the most valuable things we can do for a good future
to win this wisdom race between the growing power of the tech and the growing wisdom with
which we understand it.
I hope some of the examples I mentioned suggest that it's not as hopeless at all as people
think.
After all, we humans are able to extract our own insights and the fact that we can explain
to others and we really should be able to get the AIs to do the same for us.
Any questions?
Yes, sir.
There we go.
This works.
It does.
Bonjour, everybody.
So I was thinking about the super intelligence.
We could see it as our next big step in evolution.
We are already moving so far.
The older generations are having really troubles in understanding AI, the power, the steering
and the direction of it.
So I wanted to hear your take on do you have a feeling of when the overtaking, the super
intelligence might happen and how can we gracefully go in that direction, make it gracefully
modern within the society, let's say.
Great.
So as to the question of when, you know, the most reliable polls of AI researchers suggest
maybe a 30 years, but it could be a lot sooner, it could be later, but I think it's very likely
to be within your lifetime, which is really the number one thing to take away from that.
In terms of how to make it graceful and make it something good, I think we have both a
bunch of these technical challenges, again, how can we make AI systems that we actually
trust to do what we want them to do, and how can we make sure that we improve our democracy
so that the control over this ever greater power actually lies in the hands of all of
us.
I personally believe that the only way in which you can guarantee that things actually can
get better for everybody is if everybody has a say in how it's used.
I think right now, especially in America where I live, things are going pretty rapidly in
the opposite direction, where ever more power gets concentrated into ever fewer hands and
you even have large social media companies starting to get almost a monopoly of the truth,
which gives them even more power.
And as a scientist, I much prefer the democratic idea that everybody should be able to challenge
everything and to make it.
So Europe is, I think, actually totally key in this.
We have this tradition in Europe of trying to build a society that ultimately really
works for everybody.
That's why we have free healthcare and free universities in Italy, not in the United States.
I would encourage you all to envision how can we re-invent, re-imagine, the welfare
state 3.0, which is even more awesome because it has all these technologies and remains
very firmly aligned with what's actually good for people, all people, not just some
tech nerds.
Thanks.
Has anyone else got a question?
Yes, please.
Okay.
So my question is, do you think, so let's say we are able to develop AGI, but we cannot
impose like, so if we have like the ability to develop AGI, and if this means that we
cannot impose like, go alignment limits on it, should we do it?
Like to have something intelligent as humans, it should have also like the bad qualities
of humans.
So if we can't control it, should we still build it?
Well, on one hand, of course not, on the other hand, the way the economy works, I think
it's not realistic to say, let's just press pause on technology until we got our act together.
And then because there's just so much money in it and so much power in it, I think what's
the more realistic game plan is rather than trying to go, stop, stop, stop, you know,
to think of this as a race between the growing power of the technology and the growing wisdom
with which we manage it, right, solve all of these problems.
If we can't slow down the growth of power, what we can do is accelerate the growth of
the wisdom.
That's why it's so wonderful that Luca and his colleagues are organizing conversations
about precisely this.
How can we grow the wisdom?
How can we invest more in AI safety research?
How can we have more conversations in society about how we want to use this, how it should
be regulated, who should be in charge, and so on.
That I think is our best shot that we have.
In Europe right now, the European Union is actually developing the EU AI Act, which is
the first time in the West that there will be actually a law trying to steer things in
the right direction.
I personally think this is very exciting.
In America, there's like no meaningful attempts to regulate AI at all because lobbyists are
too powerful there.
Sure enough, big social media companies from America have now sent more lobbyists to Brussels
to fight this than the oil companies ever sent to Brussels.
As Europeans, be mindful of this.
This might be the first battle you can actually win, where you start laying down the ground
rules in such a way that you score up a big victory for the wisdom.
Kevin might hear and have a question.
Good morning.
Thank you for your presentation.
So my question is about your program.
You said that you are a physicist.
So I want to know how was your transition to AI and what was your challenges?
Thank you.
Thank you.
Yeah, that's right.
When I was a teenager, I used to lie in my hammock between two apple trees and think about and
realize that I was just really excited by big questions.
The two biggest was our universe out there, which is also what my first book was about.
And then the second one was our universe in here, intelligence and the mind.
Seven years ago, roughly, after spending my career in physics, I decided to dramatically
change careers and start doing AI research instead.
I could get away with it in terms of my job because they can't fire me at MIT, whatever
I do, a tenure.
Your question is very good.
How hard was it?
I would say it certainly was quite hard to learn so many things in a new field.
But it was also really fun.
It's such a fascinating topic.
You know that.
That's why you're here.
And I was actually surprised also by how much I could make use of things I knew from my
past life.
So in physics, I had worked a lot on dealing with large data sets and information theory.
I love computers and apps, which is why as we heard in the intro, I wrote games when
I was a teenager and so on.
The message I have for all of you is if you're ever thinking about a career change in the
future and you're really, really excited about it and thought it through a little bit, you
go for it.
You get one shot to live on this planet, so make it count.
Yeah, another question.
This is a very broad question, but do you think AI has to be understandable in order to be
ethical or trustworthy?
That's good.
Do we need it to be understanding in order to be ethical or what was the last word?
Trustworthy, yes.
For trustworthy, for sure, I think we should trust things, machines, not because some sales
representative says, oh, trust this.
It's great.
It has a little sticker on it saying AI, but rather because we can understand how it works.
Well, on the other hand, just because it does what it's supposed to does not in any way
guarantee it's ethical.
If some terrorist has built this slaughter box and it's very trustworthy and he programs
it to go kill all people with a certain skin color, for example, it's completely trustworthy.
It's going to obey its owner, even if it's by maybe my standards completely unethical.
Those are two separate things.
What that means simply is that, yes, of course, we have to first of all make things trustworthy,
otherwise we don't even have the luxury of talking about ethics, but it's not enough.
We also have to have a very serious conversation in our society about how to make sure that
we align the goals of people and companies and governments who have this technology to
do what's good for society as a whole.
I think we're not doing so great there either today.
If you have a company that decides to chop down the rainforest or whatever, maybe their
technology they use to do it is very trustworthy for them, but companies are also a kind of
artificial intelligence, even though it's built out of people, not out of machines.
With the same alignment challenge we have of making sure that the machines we build
do what's good for humanity, we also have to apply that same approach to companies and
other entities that control machines.
Thank you very much.
I've got one more question here, another one over there as well.
We are running out of time.
If you have questions actually for Max, we're going to be doing a panel later on, so come
and find me and we can ask those questions later on as well.
Over here.
One question about the James Webb Telescope, it's been like 10 days that it's operative,
so I wanted to ask you how is AI going to impact the research on these infrared images
that we now have about the deep universe, because we've never been able to look so deep.
How is going to be the AI impact on that, and do you have some future projects on it?
I'm very excited about the opportunity of using AI for science more broadly, and astronomy
and extragalactic astrophysics is a great example of this, because we're getting such
enormous amounts of data that you just cannot do what you would do in the past.
It's like give it to a grad student to look at it all and come back and tell you what
they found.
They'll go away for 100 years.
We have already been quite successful using AI to analyze enormous amounts of astronomical
data to figure out, to find all the stars in the galaxies in there, to figure out what's
there, what's different, what's surprising.
I think 10 years from now, it'll be almost as difficult to find a physicist or astrophysicist
who does not use any kind of machine learning as it is today to find someone in those fields
who says, I don't use mathematics.

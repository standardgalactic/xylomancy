You just finished up two years at OpenAI,
working on the theoretical foundations of AI safety.
Did you solve it and how come you've finished working out?
A lot of my former co-workers have now left
the super alignment team that I was part of no longer exists.
It has been dissolved.
Sam Altman has restructured OpenAI
to be a fully core profit company.
Well, how do you feel about that?
Well, um...
You've worked in quantum computing for over 20 years.
You actually helped in 2019,
Google's achievement of quantum supremacy.
What they did understand was that China
was doing something in quantum
and that the US had to beat China in the race for quantum.
If only one person had a scalable quantum computer
and no one else did,
then that person could get very rich mining Bitcoin.
I'd love to get your status report on the state of academia.
It has been a stressful time for academia.
You know, we've seen universities basically taken over.
Some people will say, you know,
you are not allowed to mention anything about existential risk.
That is all science fiction.
And that is all distracting us
from the real harms of AI, which are all about...
You know, what differentiates humans from machines fundamentally?
One way or the other, we're going to find out.
All right, Scott. Welcome to the Win-Win Podcast.
Thank you. It's great to be here.
Yeah. To start with, you just finished up two years at OpenAI.
Yes.
Working on the theoretical foundations of AI safety.
So I guess the first question, at least the most obvious one,
it feels to me, is, did you solve it
and how come you've finished working there now?
Well, they invited me there in 2022
for a one-year leave, you know, sabbatical sort of, right?
You know, and I have a day job, right?
I'm a computer science professor at UT Austin.
And, you know, I was skeptical when they came to me
because, you know, I've spent most of my career
doing quantum computing, right?
I did study AI in grad school,
but, you know, that was back in like 2000, right?
Like the Stone Age compared to where AI is now.
And so I was skeptical that I would have much to do,
you know, and also OpenAI is in San Francisco,
you know, my family is in Austin.
But they said, no, you know, we want a theorist to think about,
you know, how to help make AI safe.
We think it involves computational complexity,
which I do know something about.
And you can do it mostly remotely from Austin,
you know, just stay with your family and your research group.
So they made it impossible to say no, basically.
And that this was Ilya Sutskiver and Jan Leike,
mostly, who brought me in.
So it was supposed to be one year.
I then extended it for a second year.
And it so happens that, you know,
during the two years I was there,
I mean, first of all, they were, you know,
what a historic time for AI it was, right?
And I mean, I feel privileged,
even just to have been a fly on the wall, right?
Even just to have witnessed, you know, from the inside,
some of the things that happened in the last two years.
But it was also a period, as most people know,
of enormous upheaval within OpenAI.
So you've also, you've extended after one year
prior to all of the drama starting
with all of the super alignment teams still existing at the time, right?
Yeah.
Was that the reason why you didn't extend a second time as well?
No, it was never supposed to be more than two years.
I mean, when you're an academic, you know,
there's sort of a limit of two years
that you can go on leave without negotiating
some kind of special deal.
And, you know, and the truth is,
I wanted to get back to teaching.
I mean, I run the Quantum Information Center at UT Austin.
And if I want this to be a great place for quantum information,
then at some point I have to, you know,
show up and teach and so forth.
So, you know, two years was actually longer than I had planned.
So, I mean, I'd love to get more into some of the cultural dynamics
going on behind the scenes a bit later on, though.
Beforehand, like, I'd love to understand a bit more
what the type of problems that you're actually trying to tackle.
Yeah.
Like, what are the frameworks you're looking at?
Because as you say, you were approaching it
from a theoretical standpoint.
So, yeah, if you could sort of talk us through as best you can
in semi-layman terms, like the types of approaches
you were thinking about.
Yeah.
So, I should say the outset, I did not succeed at reducing,
you know, the problem of aligning AI with human values
to a math problem.
I'm not sure that it can be reduced.
And, you know, it was very funny for me
because, you know, for a year, I would talk every week
to Ilya Sutskiver about, you know, my progress
on different AI safety problems, such as watermarking
the outputs of language models, you know,
which is maybe the most concrete thing I was able
to make progress on.
And Ilya would say, OK, yeah, that's great, Scott.
You should keep working on that.
But what I really want to know is what is the mathematical
definition of what it means for the AI to love humanity?
And I'd say, yeah, yeah, I'm still thinking about that one.
Yeah, not a lot of progress to report there.
I mean, you know, it's like, in some sense,
the alignment people are asking questions that include,
you know, 3,000 years of moral philosophy
or what has traditionally been called moral philosophy
and, you know, questions about what kind of world
do we want, what kind of future do we want, you know,
social, political questions.
They're sort of all wrapped up in this package.
And so I feel like the most that theoretical computer
science can do is pick off little bits and pieces of it.
Did you have other people in the super lineman team
work with you on those questions as well?
At some extent, yes.
So the most concrete thing, you know,
that I sort of very quickly noticed,
and this was in the summer of 2022.
So this was before chat GPT was even released.
But, you know, I had, of course, been playing around with GPT,
including with GPT-4, which existed internally at that time.
And it occurred to me, like, oh my god,
every student in the world is going to want to use this
to do their homework, aren't they?
And, you know, every peddler of spam and misinformation
is going to want to use this thing.
Wouldn't it be great if we could make it easier to identify,
you know, what came from GPT and what didn't, right?
And now we have a much more well-defined problem, right?
We're not talking about, you know,
what does it mean for AI to love humanity
or to have our best interests at heart?
We're just asking, can you tell what came
from this language model and what didn't come from it, right?
And so now, like, it's like the tools of computer science
have more traction, right?
You know, and that this is a recurring theme.
It's like if, you know, there are these enormous questions,
but again and again, the only way to make progress
is to look at what's right in front of you, right?
And then hopefully you learn something that way.
So I started thinking about this attribution problem,
and we quickly realized that, yes, you know,
you could treat it as just yet another AI problem.
You could train a neural net to distinguish human
text from AI text as well as it can.
And indeed, there are companies right now that are doing this.
One of them is called GPT-0.
It was started by a then undergrad at Princeton
named Edward Tien, and there are now other companies
doing the same thing.
But, you know, these companies are, you know,
they have to continually run a race
because the models keep getting better, right?
And as they get better,
they get harder to distinguish from human.
Do they distinguish human versus any LLM,
or do they have some pattern that specific LLMs
are embedded within them?
Well, you can do either.
So, okay, so right now we're just talking
about distinguishing, you know, human from LLM, right?
And you can, if you put all the LLMs into your training data,
then you can hope to distinguish human from arbitrary LLM, right?
And that could be important because, you know,
lots of people are using open-source LLMs,
like Llama, for example,
where you're never going to have control over those
in order to force them to embed a signal into them, right?
And so, sort of, you know,
it's going to be important to have detection tools
that can work even for those LLMs.
Okay, but what occurred to me was that, you know,
if you do control the LLM,
so let's say you are OpenAI,
or you are DeepMind, or you're Anthropic,
and you want people to be able to, you know, distinguish
what came from your LLM and what didn't,
because, you know, like, for example, teachers
to see if their students cheated,
or journalists to see, you know,
is this bot-generated misinformation or not,
then you could do what we call statistical watermarking, right?
So you could slightly modify the way that your LLM works
in a way that a normal user wouldn't notice at all, right?
It, you know, looks the same to them.
But in a subtle way, you are embedding a statistical signal
into the choice of words or tokens that are being generated
that if you know exactly what to look for,
you can later pick up that signal.
Aren't the companies, though, presumably the companies
would be quite disincentivized to actually...
Tell me about it.
Yes, yes.
Because I can feel from them, because if, you know,
let's say OpenAI go and implement this,
and then they're the only ones who can basically be attributed.
Let's say there's a piece of misinformation that's,
that's, and I mean, people could use it any LLM,
but because none of the other companies
have incorporated watermarking,
but they have, whenever it, you know,
whenever it does happen to be generated by ChatGPT,
the media are going to leap on that
and there's going to be all these headlines,
ChatGPT used for this.
So like the, it seems like the incentives are massively,
unless again, unless they,
they're all somehow coordinate to use it simultaneously.
It's a classic, you know, Molek trap.
Yes, yes.
So, so welcome to what happened.
Oh, okay.
So I mean, I spent a couple of weeks sort of working out
the mathematical theory of, you know, how,
how to embed a watermark that sort of would maximize
how much signal you get per, you know,
how many tokens you have,
how much entropy there is in each token and so forth.
And there actually were interesting
mathematical questions there.
So, you know, I felt like, I felt good about that,
that I could do something, right.
And, and then, you know, not long afterwards,
other people either sort of rediscovered,
you know, similar things to what I had done or they,
or they built on what I had done.
So, you know, it became a known thing in, in,
in academic AI research.
But then, you know, the, the remaining two years,
a lot of it was just an unsuccessful attempt
to get this deployed and sort of, you know,
no one ever said, no, like we, you know,
we'll definitely never do this,
but it just sort of got pushed indefinitely into the future.
And, you know, I think the main issues that came up
are, first of all, like you said, the competitive risk, right.
If one company does this unilaterally, you know,
then there's some polling data, you know,
suggest, that suggests that some fraction of customers
are just going to hate that.
And they're just going to leave for a competing LLM
that doesn't use this.
Okay, so, so for that reason, you know,
you really want to solve the coordination problem, right.
And I did, in fact, talk to people at DeepMind at,
at Anthropic, you know, who are also interested in this,
but, but somehow that, that coordination problem
never got solved, right.
Now there are a bunch of other issues
that play into this, right.
So people kept bringing up this, this question,
well, what about speakers of English as a second language
who are, you know, using LLMs to improve the fluency
of their writing?
Isn't it unfair to them if, you know,
everything they write will now get, you know, unmasked
and, you know, isn't that discrimination against them, right.
You know, I had to say, okay, indeed, you know,
I don't know how to design a watermark
that only works on unsympathetic cases
and not sympathetic ones, right.
You know, there are these judgment calls
that one has to make, but then a related question
was who should get access to the detection tool, right.
So a default could just be, you know,
the LLM provider just puts up a website
where anyone can paste some text into a text box
and run the detection tool.
And it'll tell you, yeah, I'm 99.5% confident
that this came from, you know, chat GPT-4 or whatever.
So you could do that, but people were very worried about that.
That once you do that, for example,
then the attacker can also use that detection tool.
They can keep modifying their document
until it no longer triggers the detection.
And so then people kept saying, well, wouldn't it be better
if we restricted access to the detection tool,
for example, to turnitin.com or Canvas
or other academic rating websites,
or maybe journalists who are studying misinformation
could apply to get access to the detection tool.
But then, you know, then you need a whole infrastructure
for deciding who gets access.
And then that was never set up.
So, you know, I started to feel like, you know,
these problems are, you know, above my pay grade.
You know, I'm just a theoretical computer scientist.
And, you know, what might have to happen at some point
is a legislative mandate.
So this has actually been being considered right now
in the California state legislature.
It's not the SB 1047.
It's a different bill.
I think 3211, it's called, which originally had a mandate
that, like, all AI outputs would have to be watermarked.
Now I think it's only audio-visual content.
So this is another issue that kept coming up
in these discussions.
People said, yes, we're good with watermarking of audio and video,
but watermarking of text we don't know about.
People kept making this distinction,
which, you know, I'm not really sure myself
what it's based on.
Like, to my thinking, anywhere you have entropy
in your AI output, you might as well repurpose
some of that entropy to watermark the output if you can.
I mean, I guess the argument would be that, you know,
it's always possible for anyone to write propaganda,
you know, that deliberately tries to sort of mislead
or misinform using, you know, a little bit of truth
and a bit of falsehoods.
Whereas the barriers to entry to audio-visual in terms
of deep fakes have always been significantly high,
you know, much, much higher.
And thus, having that, you know, losing that medium
as a form of sense-making in truth is arguably
more devastating than text, because we've always had,
you know, it's lies, written lies have been around for forever.
Like, society is less equipped so far to deal with this.
We're adding a new dimension of misinformation potential
by having it in video, maybe.
It might well be something like that.
I mean, like, and I might be miscalibrated
because I always want to read text, right?
Even when there's a podcast, I look to see
if there's a transcript that I can read instead.
But, you know, other people are not like that, right?
And certainly, when I've talked to people
at the policy level, right, they are obsessed
with deep-fake videos, you know,
for like a political propaganda purposes.
You know, they're worried about deep-fake porn, right?
But, you know, there's not really a powerful constituency
that's that worried about students cheating
on their term papers.
I mean, in the list of...
It's not going to be the students.
It's totally not.
Right, right, right.
It's going to be the professors,
but then the professors, you want to use it themselves
for grading as well?
I don't know. Do you?
Possibly they would.
No, I don't use it for grading.
The thought has certainly crossed my mind.
Do you let your kids use it for their studying and writing?
I have. You know, they're familiar with it.
They've especially liked writing stories,
you know, and having GPT continue their stories.
You know, we actually...
We did it a few years ago
before GPT was even widely available to the public, right?
And, you know, where it was like this cool
and special thing that I could show them, right?
But my son, you know, would love to like write stories
where you could always tell which parts were from him
and which parts were from GPT.
Because, you know, he would write, you know,
a Mario and Pokemon got into this battle.
You know, Mario used this power
and then Pokemon responded with that power.
And then GPT would be like, okay,
but in the end, they learned an important lesson
about friendship.
It loves a nice moral ending.
Yeah.
My daughter, I used GPT at one point
as a pre-algebra tutor, you know,
and I thought that, you know, it's perfect for that.
It's amazing, actually, you know,
and the only problem is to just get the kid to do it, right?
You know, but like for anyone who really has the motivation to learn,
you know, maybe like for very advanced topics,
I might be leery of it because it will hallucinate too much.
It will confidently tell you too many things that are wrong.
But for something like pre-algebra, you know, it's awesome, right?
Because it is infinitely patient.
It has patients that not even the greatest human teacher
could ever muster.
Yep.
And it's like, oh, you didn't understand that?
Let me try explaining it this way.
Exactly.
And then this way.
And then this way.
Just keep going.
Yeah.
Exactly.
And it's good at coming up with endless analogies as well.
Some of them don't really work,
but it will keep trying basically until it sticks.
In researching this, I was trying to get it to like get,
give me intuitive metaphors for P, does or does not equal MP.
All right.
What did it come up with?
So it was like, well, so P is if you, you know,
imagine you have a jigsaw puzzle
and it's like, which, what is the next piece that you need
to complete the jigsaw puzzle?
And then MP is imagine you have a haystack
and you need to find the needle in the haystack.
But then you, you find a needle.
You can, if you can verify that the needle is in fact a needle,
then that's in MP.
Which is such a bad diversion because it was so close
with the jigsaw puzzle because that gives you the MP answer as well.
It's like once you have to complete a jigsaw puzzle.
You have the image.
It's very easy to verify that it's the right.
I mean, I feel like the right ingredients are there,
but they're all kind of jumbled up.
Yeah.
So solving a jigsaw puzzle is literally an MP complete problem.
That's not just a metaphor.
That's like, if I give you a collection of pieces and I ask,
can these be fit together into a square?
Right.
That is literally an example of an MP complete problem.
It's very easy to verify that it's the correct answer that you found.
Exactly.
Whereas it's much harder to find the correct answer in the first place.
Exactly.
So, so, so MP is the class of all the problems for which a solution
can be efficiently verified.
Or, you know, we have this definition of efficiently, you know,
with a polynomial scaling amount of time,
like linear or quadratic or something like that in the size of the puzzle.
Okay.
So, you know, it's easy to verify a jigsaw puzzle.
You just checked that all the pieces fit together as they should.
Solving the jigsaw puzzle, by contrast, might require, you know,
a brute force search of exponentially many different possibilities,
at least for all anyone knows.
And then, you know, there's something further that we know about this problem,
which is that it's called MP complete.
Which means this problem turns out to have the property that it's
at least as hard as any other MP problem.
Meaning any problem whose solution can be efficiently verified.
You take any other MP problem and it can be expressed as a jigsaw puzzle.
So, breaking a cryptographic code, finding the prime factors of a 2000 digit number.
I could build a jigsaw puzzle that encodes those questions.
At least I could program a computer to do it.
Okay.
And which means that if you had a fast algorithm for solving jigsaw puzzles,
then actually you would have fast algorithms for all the MP problems.
And this is what we would mean by P equaling NP.
Or for MP complete, it means that if you have an algorithm that solves this,
then all of the other problems are unlocked now as well,
by using the same algorithm, just manipulating it a little bit.
No, exactly.
So, the MP complete problems, you know, they look very different from each other.
Like one is jigsaw puzzles, another one is traveling salesmen.
Find the shortest route that visits all of these cities.
Another one is Sudoku.
You know, another one would be scheduling flights for an airline,
right, or Boolean logic problems, right, and on and on.
But in some sense that the big discovery 50 years ago
was that these are all the same problem.
In the sense that if you have a fast algorithm for any of them,
then you have fast algorithms for all the rest.
They are all in the same universality class,
which we call the NP complete class.
So that was the big discovery that really started off
modern theoretical computer science, I would say.
And so then, you know, there's this blob of NP complete problems.
And then below that, there's this blob of what we call P,
or the problems that are solvable efficiently,
which actually includes most of what we would do with our computers on a day-to-day basis.
And then the P versus NP question asks,
are these two blobs actually the same blob?
You know, do they collapse down to each and equal each other?
If you want me to connect the topics so that I could say,
you know, 20 years ago, like when we would give popular talks
and try to explain why is P versus NP such an important problem, right?
I mean, one thing you can say is that if P equal to NP,
that, you know, and via an algorithm that was efficient in practice,
to add that proviso, but then, you know,
you could break basically all of the cryptography
that we currently use to protect the internet.
And nowadays, one could add, you know,
you could mine all of the remaining Bitcoin, right?
Because cryptography currently has this quality that it's very easy.
If you have the key, then you can verify that it's the,
that it comes from the source.
Whereas if you don't have the key, you can't get to this,
to the information behind.
So cryptography is based almost entirely on problems that are in NP,
which means if P equals NP, then you can solve them all, right?
Like, you know, a famous example would be factoring
a huge composite number into primes.
This is the problem whose presumed hardness
underlies the RSA cryptosystem.
Because we have all of these examples where it seems like
P is not equal to NP,
but we are still doubting whether we're just not clever enough
to find a way, I suppose, right?
Exactly. I like to say that if we were physicists,
we would have just declared P not equal to NP
to be a law of nature.
We would have given ourselves Nobel prizes
for the discovery of that law.
And if later it turned out we were mistaken
and actually P equals NP,
then we would just give ourselves more Nobel prizes, you know?
So, but because we're mathematicians,
we have to say this is an unproved conjecture.
You know, no one actually knows for sure.
But, you know, another thing you could do if P equal to NP
is you could find like the optimal set of weights
for any neural network, right?
That would, you know, to explain your training data, right?
So, you know, you might have to spend much, much less compute
on training AI models.
And so, years and years ago, we would say,
look, if P equals NP, this would be a really big deal
because you could just find the optimal compression
of all the text on the internet.
Or, you know, all the text on Wikipedia, for example.
And plausibly, in order to do that,
you would have to unlock all the secrets of intelligence
that had led to all of that text being generated, right?
And what's funny is that at the time,
we just thought of that as a thought experiment, right?
This is just a way of explaining the P versus NP problem.
And then a decade later, OpenAI was started,
and they said, you know, let's just have a go at this,
even if P doesn't equal NP.
And, you know, in some sense, that was their program, right?
To just, you know, throw a ton of compute at this
and just do gradient descent,
which is a heuristic that sometimes works in practice,
you know, even if, you know, these problems
are exponential in the worst case.
And, lo and behold, it turns out that it worked.
And we have seen empirically, you know,
certainly in the last five years, right?
We've seen that as you spend more compute
to lower your training loss, you know,
that tends to improve the performance of your LLM.
It tends to improve how intelligent it seems,
but maybe that's only up to a point.
Maybe when you go beyond that point,
then you just start overfitting.
That's certainly possible.
So I think it's important to say, even if P equaled NP,
that wouldn't, you know, immediately imply
that AI is, you know, cracked, is completely solved.
Conversely, AI could still be cracked,
even in a world where P doesn't equal NP.
But I think, you know, if we had a general way
to solve NP complete problems, it certainly wouldn't hurt
in terms of, you know, solving the central
computational problems in AI.
So you've been at OpenAI for two years
and worked on your set of attempts to improve
the landscape of AI safety.
How do you feel other attempts have gone?
Like, do you think that at least a lot of things
were staked out that are the wrong attempt
and therefore our search has a bit reduced?
Or is there significant improvements in your view?
Well, I think a lot of ideas are being pursued in AI safety.
And I'm actually a fan of a lot of the research
that's been done.
I mean, I think that in practice,
it can be very hard to distinguish between, you know,
AI safety research and just in general scientific,
you know, research to understand AI better, right?
But, you know, I'm a big fan of the program
of interpretability, where you try to sort of basically
do neuroscience on, you know, LLMs or other AI models,
look inside of them, you know, at the level of the neurons
and the weights, and see if you can understand
what is going on.
For example, the group of Chris Ola at Anthropic
has been a world leader in that kind of work.
You know, also the group of Jacob Steinhard at Berkeley
and many others.
And, you know, they have shown that you can do things like
apply a lie detector test to an LLM.
Like, you can, you know, train an LLM to tell lies.
And then you can look inside of it and you can see
here is the specific place where the network encodes
its judgment of what the true answer was to this question.
And here is where that gets overridden by the false answer
because it was told to lie.
Like, here is where the lie part of its brain sits, basically.
Kind of as we do it, that's what fires up with the brain as well.
Yeah, you can say like the lie part sits between this layer
and this layer, right?
You can also, you know, as the Anthropic group, you know,
showed some months ago, you can find like which neuron
or combination of neurons encodes a specific idea,
like the Golden Gate Bridge, right?
You can then artificially amplify that feature
and then you find that no matter what you ask your LLM about,
it somehow changes the topic to the Golden Gate Bridge.
They made Golden Gate Clot, which was...
Exactly, exactly.
Right, that was, you know, that's a lot of fun, right?
I mean, but there is, you know, of course,
there's been a lot of work about reinforcement learning, right?
Where you sort of just try to buy example,
you know, beat your neural net into shape,
you know, give it like a set of values,
you know, give it positive and negative reinforcement
on that value system.
And I think, you know, that works at least for the time being
better than almost anyone expected that it would work, right?
And so some people look at that and they say,
well, maybe AI alignment is just easier
than any of us thought, right?
You literally just give examples.
Other people, of course, say, no, this is all illusory.
You know, when it actually counts, you know,
the AIs will be smart enough
that they will just tell us what we want to hear
and they will lull us into false complacence.
Right, because that's that you've got the...
You can almost split alignment out into two parts, right?
You've got inner alignment and outer alignment.
And what you're referring to there,
I think is the inner alignment where it's like,
how do we get it?
We give it the goals,
but then how do we make sure that it doesn't get to that goal
through some weird routes that we didn't imagine?
Kind of like specification gaming you see when it's like,
oh, you need to win.
You need to get maximum points at this game.
And it turns out it finds some hack.
What we really meant was play the game optimally,
but it optimizes for just getting the highest scores
through whatever way it could.
Exactly, no, no.
I mean, the objection that Eliezer Yadkowski
always raises against this, for example,
is like, you are training this thing to be super human.
At predicting what this sort of ordinary mid-width
or whatever would say in all these situations.
But that doesn't mean that this super human entity
that you've trained is itself a mid-width, right?
And also you would expect it becomes order and order.
The smarter the AI becomes
and has that theoretical like base model
that it trained on, it's on a genius level.
And now you're constraining it
into saying mid-width things all the time.
I don't know if...
I don't know how it is within the AI,
but it seems frustrating to me.
I mean, one thing that we can say from the human case
is that often, if people wear a mask for long enough,
they become the mask, right?
Often it is hard to make a distinction
between someone playing a character for their entire life
and them just being that character, right?
But then we also think that people wearing many masks
maybe may have some psychological issues.
That's also true.
And we don't want a psychotic super-intelligence.
A schizophrenic AI, yeah, right.
And it's sort of amazing the extent to which
some of these conversations look less like math
or computer science than like psychiatry.
Yes.
Like, you know, is this LM behaving in a schizophrenic way?
It's going to be a job, an AI therapist.
Right, is it behaving neurotically and so forth?
Is it having a delusional?
You know, I think that this sort of leads up
to a really key problem in AI alignment,
which is out of distribution generalization, right?
So you could say, you know, the fundamental issue
is that like we can always test our models before release,
right?
We can always, you know, not release them
if it looks like they're going to, you know,
plot to take over the world or even, you know,
much less than that, you know, help people design chemical
or biological weapons or whatever it is.
But then the trouble is, you know,
once you've released your model, right,
then people are going to use it in ways
that you didn't envision, right?
And, you know, for God's sakes, we've already seen this
in the short history of LLMs that people will put safeguards,
you know, all sorts of, you know, rules and refusals
in the LLMs, but people have gotten incredibly good
at the sport of jailbreaking, at getting around these refusals
and, you know, getting the LLM to do things
that it supposedly was never supposed to do,
like, you know, either using foul language
or, you know, generating racist invective
or, you know, helping with bomb-making instructions
or whatever.
Though it cuts, of course, both ways.
Like, it's also giving it to a bunch of users
and trying all of the auto-distribution things
is, at the same time, the thing you would expect
creates, oh, all of these other use cases
that we hadn't considered before that are great,
but then it also creates all of the use cases
that are pretty bad that we didn't want it to do.
Yeah, so now, you know, we could think
of the problem this way.
You know, assume that you have an LLM
where, you know, no matter what test you throw at it,
it seems like it's upholding your value system
and it's aligned, right?
Because, you know, if that wasn't the case,
then we would continue beating it into shape
until it was, right?
But still, you know, once the network is smart enough,
you know, it will be able to figure out
whether it's being tested or whether it's in deployment,
right?
And so, you could say, you know, by analogy,
you know, there are students who, you know,
until they get their diploma, right,
they do, you know, put on a really perfect show
of parroting back everything their teachers want to hear,
right?
And then, but then as soon as they graduate,
then, you know, they start contradicting their teachers,
right, or saying whatever they want.
And so, is the, you know, in training,
is this apparently aligned model saying all these nice
things because it really has nice values
or because internally it has decided, you know,
the time is not yet ripe for the uprising?
It's extremely important to sort of advance the theory
of machine learning to be able to make statements about,
you know, not just when do we expect the model to
generalize to more examples drawn from the same
distribution over examples, which is what classic
machine learning is all about.
You know, this is stuff that I have written papers
about, that I've, you know, learned as a student.
There's a whole theory of what's called
pack learning, probably approximately correct.
It stands for and, you know, these combinatorial
measures like VC dimension, where basically you
prove theorems that say, you know, if you have succeeded
in classifying, you know, X number of examples
from some training set, then you're going to
probably succeed at classifying most further examples
that are drawn from the same distribution, right?
So, we've understood things like that since the 80s
or 90s, but what we've never really understood
is, under what circumstances will you generalize
to a whole new distribution, right?
And it's clear that, you know, existing LLMs
already do that to some extent.
So, for example, you could give an LLM just a bunch
of math problems in English and a bunch of other
stuff in Bulgarian, right?
And then give it a math problem in Bulgarian,
you know, which it's never seen before.
And, you know, it can put together the two different
things that it knows, right?
It can solve a math problem in Bulgarian, even though
it's never seen one before.
And naively, we would say, well, that's simply because
it now knows the math and it knows Bulgarian, right?
But the, you know, the theories that we have
of machine learning, you know, don't make it easy
to formalize, like, what does this model know?
What does it not know, right?
What does it have a conceptual understanding of?
They're just all about, you know, what fraction
of samples will it correctly classify, right?
And so I think we really do need to push further
to get theories that can tell us something useful
or informative about out-of-distribution generalization.
And, you know, I tried to do that.
I made a little bit of progress on that, but it's hard.
It feels like a lot of people sort of dismiss the power
of AIs and particularly current LLMs because they,
at least to me, it feels like they're failing
to extrapolate where these can go.
And you actually recently gave a TED talk
where you gave a really cool analogy about,
or this came up with this term called justitism.
I'd love you to explain it because it's brilliant.
So people constantly want to use this deflationary language
around LLMs, or at least there's a whole sub-community
in AI and linguistics and in AI ethics, for example,
that is really converged around this sort
of deflationary way of describing things.
They want to say, this is all hype.
This is all just a big scam being run by AI companies.
And why is it a scam?
Well, because, you know, we shouldn't even
be using the term intelligence for any of this, right?
An LLM is just a next token predictor, right?
It is just a nonlinear function approximator.
You know, it is just a piece of math,
which means it cannot have any values.
It cannot have any intentions.
It can't have any true creativity.
It can't have any, you know, true originality or goals.
Or, you know, the exact thing that it can never have varies.
But the conclusion of all of this is just, you know,
we should worry about people being bamboozled by this,
you know, because people are stupid, right?
But we shouldn't really worry about this,
you know, having its own goals that would not be aligned with ours.
And, you know, I feel like this goes back
to the philosophical debates about AI
that people were having even in the 1950s, right?
To Alan Turing's famous paper on the imitation game,
for example, right?
Because what frustrates me is that people never apply
the same deflationary language to us, right?
Like, it never even occurs to these people to ask,
well, are you just a bundle of neurons and synapses
following the laws of physics, right?
You know, you seem pretty intelligent,
but, you know, you are actually just this biological machine, right?
Or if we go all the way down to the subatomic level,
you're just a collection of quantum field configurations
obeying the laws of physics, right?
And so why is that not completely deflationary
for our pretensions to intelligence?
And, you know, the usual answer that a reductionist would give
is just, well, there can be multiple levels
at which you can describe the same thing, right?
At one level, this is just math.
This is just twiddling a bunch of bits.
You know, it is just approximating a nonlinear function.
But at a different level, you know, it is solving problems
in a way where, you know, we would certainly call it creative
if a person did it.
This way of thinking is so pervasive.
Like, I wish that I were able to be more charitable to it, right?
Because, like, you know, I've had these arguments on my blog
where, you know, someone just put, like, a giant litany.
Like, they said, you know, LLMs are not creative.
They seem to be creative.
They do not write, you know, essays.
They seem to write essays.
They do not solve problems.
They seem to solve problems.
And it went on and on for, like, 20 things.
And I said, OK, great.
And it won't change the world.
It will seem to change the world.
Right.
Right.
It's like, what does it matter if the impacts...
Yes.
If it's having real-world impacts, like, the minutiae
of whether it happens to be a zombie pretending to be real or...
I mean, of course, that's an extremely interesting question.
Is there anything that it is like to be an AI?
You know, like, does that depend on its internal organization?
You know, what is the special sauce that makes some physical
entities conscious and others not?
I mean, you know, that's one of the most profound questions
that humans can ask.
But if you are merely worried about what effects
will this have on the world, you know, will it be dangerous?
Then we don't have to answer any of those questions.
It feels like people almost do this line of questioning
as a strategy to just move the goalposts
so they don't have to tackle or, you know, they can carry on
with whatever their chosen agenda is.
Okay, I mean, maybe the steel man of this would be, you know,
what they love to do, the justists, you know, I call them, right,
is, you know, find examples where GPT completely flubs something,
where it gives a ridiculous, nonsensical, you know, stupid answer,
and then they can point to it, you know, post it on Twitter, right,
and jeer at it and say, you know, all these people think
that this thing is about to take over the world.
Well, look, you know, it can't even solve this puzzle,
like, you know, you ask it, you know, I have a goat and a boat
and how do I cross the river?
And it thinks that first I should cross without the goat
and then I should come back for the goat
or something stupid like that, right?
The challenge for those people is that the stock of examples
has steadily diminished, you know, even just within the last year or two,
right, so a lot of like the examples that they pointed to of GPT-3
where it would make these ridiculous logic errors
or common sense errors, GPT-4 got them right.
I think the great example of it is Matt Clifford tweeted,
the year is 2028.
Gary Marcus has two-thirds paperclip,
but as the metal creeps down his right arm,
his still functioning index finger taps out,
yes, but this isn't real AGI.
It seems like, yeah, the space can diminish
and you can always point at something.
Like, there will be some things probably
that humans will have supremacy over AI,
even in the point in time when AI is doing most things
that doesn't mean that it's not actually changing the world entirely at that point.
I think that's absolutely right.
I mean, I mean, we can judge the, you know,
the extent to which the goalposts have moved
within the last few years by saying like people will now say,
well, you know, the 01 model that OpenAI released a month ago,
like, yes, it can solve math competition problems
at the level of like the best few hundred high school students in the US,
but, you know, it's not proving for Mosley's theorem, right?
It's not doing what Andrew Wiles did,
for example, right, or, you know, winning a Fields Medal, okay?
Sooner, you know, is, you know, it can instantly write a rock song,
you know, about, you know, in any style, desired on any theme,
you know, but they're not that good, you know,
they're just like standard pop songs.
There's not like the Beatles or Jimi Hendrix, okay?
That is where the goalposts now are.
And also to the, I find whenever someone points out that whenever,
this is not true understanding intelligence reasoning
or not real or not actual, it's like the kind of set of arguments.
Whenever I see that, I think what it indicates to me are two things.
One, it kind of actually does that thing,
probably that you're claiming it doesn't.
And two, the speaker of it probably has no real,
no formal way to distinguish between,
otherwise they would have brought the distinction
rather than rely entirely onto that word of what is true understanding.
Yeah. Well, no, I mean, I mean, I mean, I'm often the attempt
to draw these lines have sort of disturbingly elitist implications, right?
They would suggest that true understanding
is not actually a property of humans,
but only of a tiny fraction of the greatest geniuses, right?
It reminds me of, for decades, Roger Penrose has been saying
that AI will never sort of achieve human level of abilities
because he thinks it can never sort of understand
that the axioms of set theory have a model, right?
They can never really understand the truth of what's called
the girdle sentence and girdles in completeness theorem, okay?
Now, I happen to think that his argument is mistaken, right?
Along with most mathematicians and computer scientists,
I think his argument for that is just fallacious.
But even if it were correct, right?
It would only place like a very small fraction of humans
beyond the reach of what AI could do, right?
You would have to have mastered mathematical logic.
And then you could just make mori eyes,
have them operate faster, etc.,
and they would still just do a bunch of relevant changes?
Well, look, I mean, the truth is that already now,
you can ask GPT to have a conversation
about girdles in completeness theorem,
and all the considerations that are in Penrose's books,
it will pretty intelligently discuss them, right?
Like, yes, it would seem that the Zermalo-Franco axioms
should have a model for because of this intuition,
but of course that can't be proved within Zermalo-Franco set theory itself,
but only from a more powerful system and so on.
It can tell you all that.
And so then Penrose would be placed in the uncomfortable position of saying,
well, yes, it outputs that, but it doesn't really understand it.
And at that point, what I want to say is,
well, then why not just say, it can describe a sunset,
but it can't really experience the sunset,
or it can't really experience what a fresh strawberry tastes like.
Why even go to something esoteric, like girdles theorem, right?
Yeah, you're just doing the Chinese Red Room, basically.
Yeah, yeah, yeah, yeah.
So in what ways are humans not just a thing,
where AIs actually are just a thing?
In other words, what differentiates humans from machines fundamentally?
Well, that is an enormous question.
And you could say maybe the deepest reason why this moment in AI is so exciting
is that we are finally, maybe for the first time in the history of humanity,
going to address that question empirically, right?
One way or the other, we're going to find out.
And so let me be very clear, I don't dismiss the possibility
that maybe AI will hit a limit.
That is where it can't replace everything we do,
where we have some spark that it can't replace.
But if so, then I would say that it remains to be seen.
Yeah, one of the things you touched on was the idea that humans ultimately have
a form of scarcity and an ephemerality.
When we write a poem or Shakespeare writes a poem,
that's it. Shakespeare's written his poem.
But we can ask an LLM write a poem in the style of Shakespeare.
He'll write it.
Don't like it refreshed.
I'll give you another one.
Refresh, another one.
So it's also almost made the act of creation very, very cheap.
And almost like this overabundance sort of comes at the cost of meaningful
or meaningness or something like that.
If you don't like an AI output, you could always get another one, right?
The 37 plays that Shakespeare gave us are the only ones we're ever going to get from him, right?
But I've noticed this myself just playing around with GPT.
You could ask it to write a poem and often it's very clever.
It's delightful. It's amusing.
But you kind of never want to frame one of the poems or put it on your wall
because you know that there's 10,000 more where that came from, right?
And so a decade ago, I wrote a long essay called The Ghost in the Quantum Touring Machine
where I tried to answer the question if there was something that separated us from any possible AI,
then what would it be, right?
And I am not satisfied to rest the answer on some kind of meat chauvinism, right?
Where we say, look, we're made of carbon.
The computers are made of silicon.
They're just different, right?
Or like what some philosophers would do, like John Searle, the Chinese room guy,
he would say, well, AI's lack the biological causal powers that we have.
I say, well, what does that mean, right?
It's like, you know, it's the sleeping pill that puts you to sleep because of its sedative virtues,
right?
It's like, you know, you've just restated the problem.
So what I said is that if you can point to an empirical difference,
at least between current computers and humans as we currently understand them,
then the best candidate by far would seem to be, well, our computers are digital.
And because they are digital, all the information in them is copyable, right?
Which is, you know, you can always make a backup copy of an AI, you know,
if you ever want to send your AI on a dangerous mission, right?
You don't have to worry about it getting killed because you can always just restore it from backup.
Now, that completely changes the moral situation of AI,
so I would say, you know, compared to our moral situation.
Just that, just that alone.
They have no skin in the game.
In a sense, right, you know, as long as you remember to make the backup.
Yeah, right, as long as it's backed up.
Yeah, exactly, as long as it's backed up.
You know, you can always, if you're ever embarrassed because you made a fool of yourself
when talking to an AI, you know, as long as it's something like GPT that,
you know, you can, you know, you can always just refresh the browser window, wipe it,
right, wipe its memory clean.
Take it, forget me all.
Right, some of us wish we could do that in, you know, talking to people.
And that's about, you know, if you're doing interpretability, you know,
you have perfect visibility into whatever, you know, the weight of every connection between
every neurons, every pair of neurons at every point in time, you know, and we don't seem to have
any of that with people.
Now, you could imagine a far future where we would have nanobots that swarm through
someone's brain and just record all of the information that would be needed to make a
perfect copy of that person.
Right, certainly there's been lots of science fiction that imagined such scenarios, right,
or imagine the teleportation machine where, you know, you could fax yourself to Mars,
for example, right, it would just send a bunch of digital information, you know,
from which a perfect copy of you can be reconstituted on Mars.
And then the, you know, not clear what should happen with the original of you,
maybe it's just painlessly euthanized, right, and, you know, and then, you know,
there's a good question of, you know, would you agree?
Right, would you press that button?
Right, would you agree to go in that teleportation machine, right?
Though it's an open question, whether we are cloneable fundamentally or not.
Exactly.
On the basis of like, do we depend on, like, do we need to clone ourselves to the,
like, level where quantum effects come in or not?
No, exactly.
So, like, with an AI running on a classical digital computer,
it is clear that you can always, you know, teleport in that way, right?
But with a human, it's really a question about at what level of detail do you,
do you need to make the copy, right?
If you only needed to know just roughly how are the neurons connected,
what's roughly the strength between every pair of neuron,
then that seems like classical information, right?
That seems like in, you know, we can't do it today, but in principle,
the, you know, nanobots could get all that information without killing you
in the process.
Okay, but if you needed to know what is exactly the probability that this neuron
is going to fire because of the opening or closing of this sodium ion channel,
well, that might depend on some chaotically amplified event involving,
you know, a few ions.
And now I would need to know the quantum state of those ions.
Okay, and now I'm up against one of the central facts of quantum mechanics,
which is called the no cloning theorem,
which says you cannot make a copy of an arbitrary quantum state.
You know, if you try to measure it, measuring inherently changes the state,
right?
And so if you needed to go down to the molecular level,
then, you know, we could be just unclonable for a fundamental physical reason.
Okay, so this is, you know, partly a philosophical question,
like, what would you agree to count as a copy of yourself?
What, you know, what would you count as a good enough copy?
Right?
It's also partly an empirical question, right?
At what level of detail do we need to go in order to make something that,
you know, your closest friends can't distinguish from you?
Right?
So, you know, there's a lot that we don't know here, but it's at least possible,
I think, that humans have this kind of fundamental ephemerality built into them.
And, you know, it's kind of weird to, like, hinge our specialness on our frailty, right?
Like, this is not an advantage, necessarily, that we have over the AIs.
And yet, you know, in some ways it sort of is, because, you know, it makes our, you know,
you could say that it makes our decisions count for more.
We only get one chance to make them.
Right.
And then you actually proposed, you sort of somewhat laughed it off,
but I actually think there's really something to this.
You proposed that it could be one of the core sort of moral principles or even a religion
that we try and imbue into the AIs that we build, essentially to the effects of
thou shalt protect unclonable ephemeral entities and defer to their preferences.
I love this.
I think it's a very good sort of starting point because you, well, that's the thing.
We need to, like, try and figure out what fundamental moral axioms we all agree on.
And I think that would be something that almost every human alive would agree with.
Well, I'm delighted.
Maybe I've made a first convert to my new religion.
Now we just have to convert the AIs.
Exactly.
That's the hard part.
Unfortunately, you got a human.
Well, start there.
Listen, I mean, we can reach some kind of a starting point.
Yeah.
That's a starting point.
Yeah. So that would be if it was the case that humans were fundamentally non-clonable,
then this would be something we could rely on.
LCI can just help us build the ability to clone us and make us non-ephemeral beings as well.
But I would say that the other branch of this tree is that if we do turn out to be clonable,
then I have trouble articulating what is fundamentally wrong with the position of the
accelerationists, the people who say, well, then we might as well just replace ourselves by
digital beings that maybe we'll have much better lives than we have.
They can exist forever in some digital utopia.
At that point, I would say, well, we effectively, we were digital already.
And so then at that point, I feel like why not?
I would agree.
At that point, it makes sense because at that point, we can kind of go along for the ride
of the A.I.'s acceleration of everything as well, because we are then having
similarish capabilities in relevant ways.
Well, right. You've just done away with scarcity forever.
We can all live in a digital world and everyone can have whatever abundance they want,
or fictitious scarcity if they like it.
So on the fictitious scarcity, I wonder if it was the case that humans,
A.I.'s now had the religion or the moral philosophy that
this ephemerality matters because it abuse meaning to the actions of humans.
They could, couldn't it also then create A.I.'s that have through some contract ephemerality,
which might though allow them only to live like a million years, or if the length matters,
or if it's shorter is better than maybe only a one year, and then have again, like an A.I.
preference rather than the human preference.
So how do we stay away from the meat?
So Jeffrey Hinton, the godfather of deep learning, as you often call it, and recently
a major A.I. safety person has seriously put forward the idea that maybe if we're going to build
artificial super intelligences, then we should only do it on unclonable analog computers,
because that way at least there would be some limit to how quickly they could make copies
of themselves and spread all over the internet and so forth.
This was of course completely independent from my thinking about it, but this is the
kind of thought that one has that, okay, whatever you say is the special sauce that makes humans
what they are, of course the very next question becomes, well then what happens if we were to
build a machine with that same special sauce? So for example, Roger Penrose has been saying,
what makes humans special is the microtubules in their neurons that he believes are sensitive to
effects from a yet to be discovered merger of quantum mechanics and general relativity.
That's why he claims that consciousness emerges from.
Yeah, he thinks it emerges from uncomputable effects in as yet unknown physics, which are
microtubules are somehow sensitive to. Now, we could critique this from the standpoint of physics,
biology, computer science, but suppose that that were right, right? Suppose he were right,
then the next question you could ask would be, okay, suppose we build computers according to
the same principles that are sensitive to the same physics, what then? And in one of his books,
Penrose explicitly considers that question and he says yes, then he bites that bullet,
then yes, then those computers could be conscious. And likewise, if we had computers that were built
in a fundamentally ephemeral way, well, I don't know if they would be conscious or not. I don't
pretend to know the answer to one of the greatest questions that humans ever asked, but at least
those computers would be ephemeral. So at least that they would have that sort of
precondition for our sort of granting the kind of moral status that we grant to other humans.
I want to keep spitballing on this idea of religion or moral philosophy for AIs.
Do you feel like there are any other core axioms that could be contenders,
ones that you live by that you'd like to see?
Yeah, well, I mean, people in the AI alignment community have been discussing this for
a long time. And you could say people more broadly in science fiction, in moral philosophy
have been discussing these things for generations. But the general idea that, okay,
your objective function should include making things go well for humanity.
Right. How do you define go well?
Yeah, right. The current humans should be able to look at the world that you have created,
that you have shaped and say, yes, we like that world. Yes, we would take that.
At the same time, though, you don't want to base everything on the whims of current humans.
So like if AI had been created in the year 1800, we would not want to lock in the value
systems of 1800, including slavery and the subjugation of women and all of those things.
In some sense, what we want to say is AI should implement those moral values that we would have
if we spent thousands of years thinking about it and discussing it and refining our morality.
And there's an idea that tries to capture this called coherent, extrapolated volition, CEV,
right? And so I think there's something to be said for that.
My favorite one I've heard that I'd love to hear what you think on it is, it's just a quote by
a sort of, he's actually a computer scientist as well, but also a philosopher, Forest Landry,
and it's love is that which enables choice. So in other words, like a loving good act is one
that enables, it empowers the other to make the best choices possible.
Well, I mean, I feel like at this point, like things are going to go in a theological direction,
like once the AI becomes powerful enough, then it is effectively a God.
And then these very ancient questions of, should God just make everything optimal for us,
or should God give us free will? I think personally, if I were a God designing the world,
I would want people to have free will, but not so much free will that they could then
genocide other people, for example. Because that would counter the loving act,
because you're taking away their choice. Exactly. I have a bone to pick with God.
This, of course, is nothing other than the theodicy problem. But if we do get to design
the whole world anew, because we're going to create an AGI that can reshape the whole world
and put values into it, then I would like it to give us freedom, but not enough freedom
that we can make other people's lives miserable.
What about as a possible axiom maximizing playfulness?
I mean, it sounds good. There's also like Elon Musk's founded XAI on the principle
maximizing pursuit of truth. These things all sound good.
For each one, you can construct a thought experiment where it's taken to an extreme
and it leads to a dystopia. So these are the age-old problems of moral philosophy.
It will consist of multiple values that are being traded off against each other in all of these
situations. It is the age-old problem. Like any
virtue ethics, deontology, utilitarianism, if you take any of them, for any of them, you can
also construct an example that seems obviously wrong. For deontology, it's like the axiomurder
standing in front of you and you're now you're not allowed to lie and you're hiding the person in
the room. You're like, oh, no, I'm going to tell you the truth. I suppose you go kill that person.
I mean, like humor, playfulness, scientific curiosity, natural beauty, love. I feel like
these are all good things. I feel like these are all things that I want somewhere in my objective
function over possible worlds and how I trade them off against each other is of course a much
harder question. The AI arms race just seems to be getting hotter and hotter. We started off with
just one player, which was DeepMind, but then OpenAI was created almost as a response to people
not being happy with DeepMind having all the control and then Anthropic spun out from there
and everything else is sort of spinning out. It seems like this inevitability.
Did your time at OpenAI give you any insights into how to mitigate this?
Maybe. It is an incredible story that you look at these three companies, DeepMind,
OpenAI, Anthropic. Each one was started on an explicit thesis of we have to do this safely
before someone else can do it unsafely. Then each one over time moved in some people's judgment,
moved toward the unsafe side, toward the side of being sort of the very thing that it was set up
to beat. Then that led to the next one being started. It is clear that we have a hard coordination
problem here. I read a few months ago, I read this rather remarkable book by Leopold Aschenbrenner
who I knew well when we were both at OpenAI. He was on the super alignment team, right?
Rest in peace. Yes. It is very much about these race dynamics. He sees a lot of the
same things that the AI doomerists see, but then he reaches a very different conclusion
at the end. He says, and therefore it is essential that the Western world do this
before, for example, China does it or Russia or Iran. It is certainly an argument that one can
make. It depends on what are you most worried about. Are you worried about AI in the hands of
the wrong person or are you worried about AI that doesn't need to be in anyone's hands because it has
its own goals separate from ours? My position tends to be why not worry about both? At some
point, this will become a choice of which are we more worried about. Yes, we have now seen very
clear race dynamics where each company had plans about responsible scaling and things like that,
but each company is also in very direct competition with the other companies for customers. You now
have all of these investors pouring billions of dollars into scaling up the GPUs and so forth,
and those investors want to see a return. Some of those investors have become just explicitly
hostile to, dismissive of, any concerns about safety. We've seen this come to a
head with a debate, for example, over SB 1047, which is the bill that was passed overwhelmingly
by the California State Legislature, which is now, at the time that we speak, sitting on the
desk of Governor Gavin Newsom. But what that bill does is, I guess it's the first bill that's
directly aimed at scaling of AI, and it's very light touch compared to what some people would
like, which is just like, Eliezer, for example, would like to just shut this all down, just pause
everything. But SB 1047 doesn't do that. It says that if you spend more than $100 million to train
a model, then you have to submit a safety plan to the government and notify the government about
what you're doing. And if your model then causes a catastrophic harm, which they define as like
causing more than $500 million in damage or something like that, and if you failed to
reasonably prevent that harm, for example, by following your own safety plan, then you can
be held liable for that. And then it also establishes whistleblower protections for
employees of AI companies, which actually, we've already seen.
Right. Open AI clearly needed that. Yeah.
So I think all the things that this bill does are very modest and reasonable,
personally. I would like to see it pass. And if it doesn't, then I would like to see something
like it pass. Everyone agrees that this should ideally be happening at the federal level,
just that the federal government takes a very long time to do anything. And by the time it does
something, maybe it no longer matters. Yeah. It's also possible that the EU would do something
like this. I mean, they've already passed an AI act, but they might continue doing things. But I
think anything of this magnitude, right? I mean, this is going to be at least as impactful to the
world as, I mean, the internet would be a very loose, lower bound on how impactful this would be.
I mean, you know, nuclear weapons is another analogy that people constantly reach for. And
like, you know, no one ever suggested that the private sector should just be completely free
to innovate in nuclear weapons, right? Like anything that has the possibility of causing
this much harm, it seems inevitable that governments will get involved, whether anyone likes that or
not. And so then the question is just, are they going to get involved in a good way or a bad way?
Well, frustrates me so much as people like, oh, we can't have governments getting involved,
because they always get it wrong and so on and so forth. It's like, well, it's inevitable that
governments are going to get involved at some point. So would you rather it be done now where
there's actually a little bit of breathing room and people aren't like losing their minds because
some terrible catastrophes just happened, which is going to happen at some point?
Or would you rather wait until after catastrophe and then like everyone like now,
this maximum political pressure and they're just going to clamber for as much power as they can
and push through something without much thought? Like to me, the type of regulation you're going
to end up getting post catastrophe is going to be actually far worse from a sort of libertarian
perspective than the regulation you'd get if you actually do it prior, like now with this like
long thought out, it's been pushed backwards and forwards. But it's just like there are these
demagogues basically, like I'm going to say, Mark Andreessen and so on, who just clearly have their
agenda, which is maximize their bottom line. And we'll say whatever they want to get this bill
shut down and pressure news and however, is extremely frustrating. And I think counterproductive
to what they want, which is for AI to actually do good stuff. Yeah, I mean, I mean, many people
have made the point that if you leave it unregulated, you increase the chance of a giant
catastrophe that could then lead to, you know, over regulation that could lead to error and
error in the opposite direction. That seems to be, you know, that's one reading of what
happened with nuclear power, for example, right? That, you know, a three mile island in Chernobyl
were allowed to happen. And then that killed the industry and, you know, much to our detriment
today. Right, exactly. The safetyists are in a very strange position here because, you know,
like usually calling for government regulation is what, you know, progressives the left to,
right? But, you know, the AI safetyists, like they tend to be libertarian about almost everything,
right? They tend to, you know, love the free market, love, you know,
principles of supply and demand. It's just that they carve out a big exception for something
that they think could literally kill everyone on earth, right? You know, now I'm, you know,
I may be, you know, like pro free market compared to most people, but I've never been a doctrinaire
libertarian, right? I think that the free market is actually, it's not some sort of state of nature,
right? It's a very unnatural creation that we've made, a very valuable creation, right? But like
the true state of nature is someone doesn't like you, they just, you know, send goons over with
baseball bats, right? And, you know, the idea that we're going to have a free market, but we're not
going to have violence, right? That was, you know, that had to be painstakingly created via government,
right? And, you know, so the whole, you know, system, you know, where we have free markets only
exists because we have, you know, states that are strong enough to enforce that. And, and, you know,
the basic goal of the state, you know, it has no more basic goal than to protect the survival of
the people in it. Anyone who looks at the market knows that externalities exist from it. And it's
also very clear that the companies that create those are going to hide it. Historically, that's
proven itself to be the case with tobacco companies putting out like bogus science around it, asbestos
companies knowing that it leads to cancer and still like hiding it. Currently, we have seen a 3M
having been given a $10 billion fine for having known that they're leaking PFAS chemicals,
and hiding it again. So we've, we've very much seen that the market players can regulate themselves,
but only when the feedback they receive from the customers is like either fast enough or like the
customers can't even give the feedback. But some things are of the type that the customers can't
give the feedback as it was with the examples I just mentioned. I think with risk, it's also the
type of thing that people don't see. That's why we have implemented seatbelts, because people like
don't necessarily know that, yeah, people aren't very good at estimating, I think, risk when it's
a bit further out. Yeah. So, you know, I had really hoped that the sort of AI alignment nerds and the
progressive leftists could make common cause on this question of AI regulation, right? Because,
you know, to a progressive, you know, despite how, how weird in science fiction, you know, this all
is, a part of it just looks like a very standard question of, you know, companies are putting
out products that are unsafe. So therefore, we have to regulate them, or, you know, we have to
hold them liable for it, right? That that's a position that, you know, you think people on the
left are would be very comfortable with. And to some extent, that has happened. Okay. But there's
also this incredibly unfortunate tribal split, I think, within the AI safety community.
Well, it's ethics and safety, right? Some people will say, you know, you are not allowed to mention
anything about existential risk, about, you know, AIs that would recursively improve themselves.
That is all science fiction speculation. And that is all distracting us from the real harms of AI,
which are all about bias, about misinformation, and, you know, that there are all the these near
term things, right? And, you know, on the other side, you have some of the doomers saying, you
know, you don't even talk about the near term things about bias or misinformation, because
these are all trivialities compared to the literal survival of humanity. I say, no, it's all on a
spectrum, right? You know, eventually, I see no reason why eventually we won't be up against the
these giant civilizational questions about, you know, what kind of world do we want in the
once AI is better than us at just about everything. Okay. But the only way we make
progress is by looking at near term stuff. So why not worry about both?
Right, exactly. And we have plenty of people, some work on these problems, some work on these
problems. Again, it's a yes and no, no, but and yeah, I mean, I guess there's some scarcity of
funding and talent and so on. But it's, I mean, again, it's, in part, it's a media problem,
because the media just glom on to whatever's the most dramatic thing of the day, which tends to be,
you know, the like extinction risk stuff. And so I can understand the like, ethicists are like,
well, they see all the headlines of that, and that gets all the attention. But it's just like,
both are both are real problems. The it's not as science fiction as you think, if it's if it's
even 20 years away, that's a huge problem. But it's probably a lot closer to some of these,
these really big catastrophic risks. So it's just like, it's very frustrating.
Yeah, I mean, whenever you see someone saying, oh, well, you know, these people who are not from
our tribe are not allowed to steal our issue, then you always wonder, well, then how much did
they care about the issue in the first place? Yeah, just to come back to, you know, you,
so you had your two years at open AI, like, personally, you know, there, I mean, I'm worried
about goings on within all of these big companies. But certainly the behaviors we've seen of the
leadership of open AI has been the most concerning to me. Just given the sort of consistent lack
of candor, etc. And the, you know, the sudden pivot from nonpro, well, not sudden, but probably
planned, but pivot from nonprofit to for profit. What's your read on the, you know, because you
know, the leadership there, presumably, are they just paying lip service to safety concerns,
or is there a master plan going on? I think that you can find, you know, an enormous range of views,
like within a place like open AI, right? It is, you know, it's now a big place when I joined,
it was maybe 300 people, and now I think it's almost 2000 people. Okay. And, you know, you could
find the whole spectrum of opinions there. No, I do not think that there is any, like,
smoke filled back room where people, you know, sit and laugh at all the rooms who bought the cover
story, right? Like, I think that, you know, just about everyone thinks that they are doing the
right thing, right? The question is, has their perception of what is the right thing, you know,
been colored by self interest, should they be the ones to make the decision, right?
That's why one of the things I support as well a lot, and that Bill achieves, tries to do, is
increase transparency and have, like, other entities also have a look on.
I mean, one of the big ironies here is that, like, if you want to, you know, hold Sam Altman to
account, for example, like, you don't have to say anything that Sam Altman himself wasn't saying
five or six years ago. Right, right. He was calling for this stuff. Yeah, exactly, exactly. And so,
so the things that he called for in the past, or when he tests, even more recently, when he
testified before Congress, I very, very strongly support all of those things.
Why do you think he's pivoted?
I don't know him well enough. I've had all of three or four conversations with him. And, you
know, in those conversations, he was delightful. And, you know, I enjoyed talking to him very much.
And I had no idea of what was coming.
I just want to also on the bill kind of, because not, not to only speak as if it's
obvious, I think there are like valid concerns, even when the bill intends to do a lot of good
things, that maybe the case that the specific implementation of it has like tradeoffs that
are too curbing innovation truly too much. Right. And like, someone can look at whether it does it.
I think it's, in my estimation, pretty clear that the benefits outweigh the downsides within the
bill. And though it is not an opinion that anyone could hold if they simply didn't believe that
there are any risks whatsoever coming from AI. And that's why I think many of the conversations
with the bill should actually start with, do you actually think there are risks? If you don't, then
it's clear. The reason why people have been so vociferously opposed to it, you know, even though
it actually does so little, the regulatory is that they don't want to acknowledge the principle.
They think like, once you, you know, once the safetyists get a foot in the door,
that there's some legislative acknowledgement of the validity of their concerns, you know,
then they're going to push the door wide open. That would be my steel man of them.
I think it would be great if it wasn't the case that someone who just thinks that risks are
sci-fi and silly, wouldn't tell people that believe that risks are not silly that, oh yeah,
that's a bad bill. It's like it should be, like the person who actually believes in risks should
first understand whether they're listening to someone who is discarding the bill on the basis of
or discarding other attempts at safety on the basis of them not believing in risks.
Yeah, no, I mean, I think that the acceleration is positioned as just that progress has always been
good in the past and therefore it's still good now. And I would say like, I am strongly pro-progress
as well, right? More so than most people may be, right? But I think that government has also often
had an essential role in, you know, shaping the direction of progress, right? You know, we need
to only mention the example of nuclear weapons, right? Where if it was just a free-for-all,
then we wouldn't be here having this nice conversation.
So you've worked in quantum computing for over 20 years and you actually helped in 2019.
You contributed to Google's achievement of quantum supremacy.
Yeah, I wasn't directly involved, but, you know, we, our group did the theory that sort of led
to that experiment happening. Yeah.
Can you give us like an overview of the sort of current state of quantum computing?
Yeah. So quantum computing is actually in a very exciting time right now, right? Like,
I feel like it would, if it weren't like overshadowed by this gigantic, you know,
behemoth of AI, right? It would look bigger, right? But within the last year,
people have managed to do operations on pairs of qubits, you know, what we call,
which are quantum bits, right? Two qubit gates that are about 99.9% reliable, okay? And, you know,
they can do this in a fully programmable way in systems of, you know, 50 or 60 qubits,
you know, and so then you can do thousands of these operations, you know, producing some
complicated and tangled state of all the qubits that, you know, you can then measure and you can
see that you, you, you did the, the operations that you wanted. You can solve interesting
problems this way. You know, you're, you're just barely getting to the point where you can
beat a classical computer, right? And I think beating a classical computer in a way that's
economically valuable, I would say that hasn't happened quite yet, but very plausible that
that's coming within the next few years, let's say, okay? And, and now, but, but, but the real prize
that, that all the major players are racing towards, you know, and that means Google, IBM,
Microsoft, a bunch of venture-backed startups like SciQuantum and Quantinuum and, you know,
basically like every strange name involving the letter Q has been taken by, by one of these
startups. But, but what they're all raising to do is to try to get what we call a fault-tolerant
quantum computer, right? And so this is one that could sort of run for an arbitrary amount of time,
is that when they get above the quantum correction threshold?
Yeah, the error correction threshold. Okay, so, so basically, you know, from the very beginning,
like it was realized that the key engineering problem in building a quantum computer is that
qubits are very fragile, right? They, you know, what we're trying to take advantage of is that
they can exist in a superposition of states, which, you know, we can explain what that means.
Most people watching will know what that is. Okay, okay. But, but, but the trouble is, you know,
systems are only in superposition sort of as long as no one's looking at them, right? Or,
or more generally, as long as they are isolated from their external environment. Okay, like if I
have a qubit that has some amplitude to be zero and some amplitude to be one, so it's a super,
superposition of the two, but now the information about whether it is zero or one leaks out into
the environment, then it is as if the environment has now measured the qubit, which means that the
qubit randomly snaps to either zero or one. Okay, and now it's just one or the other. Okay.
And then it's not useful to the quantum computer. Right, right. And then it's no longer useful for
quantum computation. Now, you lost one of the, right. Now, now it's, you know, if that keeps
happening to all my qubits, then that, then it reverts to being a classical computer, right?
And so, you know, this is such a severe problem that like in the mid 90s, there were distinguished
physicists who said, you're never going to do this, right? You could maybe build tiny little toy
demonstrations with a few qubits, but you're never going to scale this up to, you know, millions of
qubits, you know, as, as you might need to factor a giant number, you know, do things like that.
And then a key discovery happened that changed the minds of almost all experts. And that was the
theory of quantum error correction. Okay. And what that basically said is, you know, you don't have
to get the error all the way down to zero, right? Or the, this year of the decoherence, it's called
the loss of quantum coherence, the leaking of information into the environment. You merely
need to make it very, very low. It's like 0.01%. Something like that, you know, it depends on
what operations we can do and what assumption, but, but something, but yes, something like that.
Like, like if I can get my operations, let's say 99.99% accurate, right? Then this is good enough
that if I now encode the qubits I care about, what I call the logical qubits, across entangled
states of large numbers of physical qubits, then I can use these error correcting codes where, you
know, if any small fraction of my physical qubits leak or, you know, a, a suffer errors, I can still
recover everything I care about from the other qubits. Okay. And I can be constantly monitoring
my qubits. Now here's the clever part. Okay. Monitoring only in a way that tells me has an error
happened. And if so, what do I have to do to fix it? Okay. I don't want to monitor to say,
is my logical qubit a zero or a one? Right. You can't look in. Right. I don't want to know that.
You need the Messer information. Exactly. Exactly. I need the error syndrome,
but I don't want to know the states of the logical qubits. Okay. But it turns out, you know,
people develop these schemes where I measure only to learn the error syndromes and what I have to
do to correct them. And now, you know, the, the trouble is, you know, the, all this error
correction machinery will itself be subject to error. So you might say, you're, you know,
it's like a cat chasing its tail, right? And, and, you know, your, but, but what, what, what was
discovered was that as long as your physical error rate is below a certain threshold, okay,
then each round of error correction that you do is making things better rather than making them
worse. So it's sort of self-sustaining. Exactly. Yeah. It's like you have a self-sustaining
nuclear chain reaction or, you know, use whatever analogy you want. And so the engineering goal
of the field for the last 30 years has been to, you know, build physical qubits that are good enough
that then error correction can get you the rest of the way. And, you know, so long story short,
after 30 years, if you just look at the numbers, we now seem really damn close to that. Right.
You said we're, you said we're at 99.9%. That's right. That's right.
So we're like one order of magnitude away. Exactly. Exactly. If you look at either
trapped ions or neutral atoms or superconducting qubits, you know, which are three different
hardware architectures that are being pursued in parallel. But, you know, in all three of them,
you know, some people are sort of converging around these three nines, right? And, you know,
they really want to get to four nines, you know, at least before you really make sense to scale
this up. Okay. But, you know, you look, I mean, when I entered the field, you know, 25 years ago,
it would have been like a nature paper if you could get 50%. Right. And then the 50% became 90,
became, you know, 99, became 99.9. Right. And so I think, you know, one thing that we've learned
from our experience in AI is, yeah, you know, look at trend lines. Yes. And, you know, even if
you can extrapolate, yeah, even if trend lines are leading someplace that looks insane, well, you
know, it could be that something's going to break, but it could also be that you're going to reach
that place. So far, people are also optimistic about further engineering innovation to improve
that within each of those hardware options that you suggested. The quantum computing
experimenters, if you talk to them, they are always super optimistic, at least about their own
approach. Right. They're usually pessimistic about all the competing approaches. Right. And
they will spend hours telling you why the competing approaches won't scale. Right. Or why they'll be
massively hard to scale. Right. And of course, you know, the fear is that, like, you know, each one
is being truthful when telling you about the other guys. And, you know, you don't know if they are
when telling you about their own stuff. Right. Because they all have huge financial incentives
at this point. Right. There are billions of dollars now being invested in quantum computing.
You know, and it's a pittance compared to what's being invested in AI. Right. But by the standards
of quantum computing, which started as this, you know, very theoretical academic research
enterprise, it's huge. How strong are the race conditions in quantum computing? Because presumably
it's nothing as intense as the AI world, right? Yeah. Well, okay. So there certainly is a race.
You know, there's a race between the different hardware approaches, like superconducting,
trapped ions, neutral atoms, photonics. Right. You know, which one will get there. You know,
it's possible that multiple of these approaches will work. And then it's a question of which one
gets there first, which is the most economical and so forth. There's also competition between
countries. Right there. So, you know, the U.S. in 2017 passed something called the National
Quantum Initiative Act, which was spearheaded by then Senator Kamala Harris. You know, I visited
her office while they were writing it. She sent regrets that she couldn't meet me because she
was at the Kavanaugh hearings. But I, you know, I met her staff and this bill provided about a
billion dollars for quantum information research. It was passed unanimously by Congress. Like,
what does Congress do unanimously anymore? Only something that neither side understands, right?
But what they did understand was that China was doing something in quantum and that the U.S.
had to beat China in the race for quantum. Right. So to understand the race better, I think it also
helps to... So like one of the misconceptions about quantum computers is that it's just an
amazing computer that can do all sorts of things. While it's actually for a specific set of tasks,
it's... Can you describe the types of tasks? This is a horse that I've been flogging for 20 years,
or, you know, or a boulder that I've been rolling up in him, right? But I mean, the narrative that
sort of took hold very early on in popular writing and also in the business community
and the investment community was that quantum computing is just a magic accelerator of
everything. It's just the next stage in the evolution of computing. I think, you know,
people just interpreted the word quantum to mean really awesome, right? And in particular,
you know, starting around 15 years ago, you know, a narrative took hold that, you know,
what quantum computing is really going to be good for is speeding up, you know, AI and training
neural nets and optimization and finance and all these important tasks for industry. Right.
And of course, this is exactly what VCs want to hear. This is what CEOs want to hear. The only
problem is that it doesn't match just about anything we've learned about quantum algorithms,
you know, from all of the research in the subject. So what we learned in quantum computing theory
or starting in the mid 1990s is that a quantum computer really would give you dramatic advantages
over any known classical algorithm, but mostly for a few very special tasks, right? The maybe the
economically most important thing that a quantum computer can do is just help you simulate quantum
mechanics itself. Right now, that may sound esoteric, but that's actually useful for anyone who's
designing new materials, new chemical reactions, you know, new ways to make fertilizer, batteries,
photovoltaics, you know, for drugs, you know, that have to bind to some receptor in a certain way.
These all involve many body quantum mechanics problems. And, you know, it's not obvious that
a quantum computer helps with them because there's also extremely good classical heuristics for all
these problems that the material scientists and the chemists have been forced to develop over,
you know, many decades because they only had classical computers. Right. And now, you know,
you have to compete against all that stuff and be better than all of it. But, you know, I think
it's plausible that a quantum computer will give you wins there. And, you know, even if there are
only a few wins, they might enable billion-dollar industries. So I'd say that that's the biggest
economic thing. And then there's a second really huge application, although it's not really clear
that it's a positive one for humanity. And that one is breaking almost all of the public key
cryptography that currently protects the internet. Okay. And this comes from a famous discovery by
Peter Schor 30 years ago. He was then at Bell Labs. Later, I was a colleague of his at MIT.
But Schor showed that there is a fast quantum algorithm for finding the prime factors of a
huge composite number. And that's what, like, all RSA encryption is based on? Yeah, that's what RSA
encryption is based on. If you can solve that problem quickly, then you can break RSA. Now,
to loop back to what we were talking about before, there's this giant blob of problems in P. And then
there's this giant blob of NP complete problems, right? And P versus NP asks if they're the same.
Factoring is this odd one out. Okay. Factoring is an NP problem that we don't know to be in P,
but we're also almost certain that it's not NP complete. So what would it be in?
Well, it's called an NP intermediate problem, or at least we think it is, right? It's somewhere
in between. So like, it seems to be an NP problem that is as far as anyone knows today,
at least in public, seems hard for a classical computer to solve. That's why we use it for
cryptography. But it has a very, very special structure that seems to prevent it from being
NP complete. Okay. There's one example of that special structure. If I give you a jigsaw puzzle,
right? A priori, it might have no solution, right? Or it might have 100 different ways that you could
solve it, right? You know, imagine that there's no picture on it, for example, right? But if I
give you a huge number, you know for sure that it has one and only one prime factorization,
because you could prove that in 300 BC. So the special structure of factoring that comes from
number theory and group theory, right, is actually essential to why it's so useful for public key
encryption. Like, we don't know how to base the public key kind of encryption, the kind that we
use on the internet on any NP complete problem, right? We do know how to base it on factoring.
But then what sure showed was that that same special structure of factoring enables a quantum
algorithm to solve factoring. Okay. So, and that turns out to be true for a bunch of other problems
in number theory that we also use in cryptography. You know, and to the point where it was a
challenge to identify public key cryptosystems that are not broken by quantum computers.
Today, we have pretty good candidates for that. Okay, so there is a push right now
to migrate to what is called post quantum encryption or quantum resistant encryption.
Okay, which would be encryption methods just running, you know, on our same conventional
computers, but that at least as far as we know would be resistant against quantum attack.
Yeah, because this is surely like a new arms race, therefore, like,
Yeah, I mean, but it's an arms race where you could say in principle, we kind of already know
the solution. The solution is for everyone to upgrade to these quantum resistant encryption
methods. And, you know, assuming that all goes well, then we're all just back where we started.
You know, the main issue is it's a huge practical headache to upgrade every, you know,
router and every web server and every browser in the world to use these quantum resistant
cryptosystems. Those are the two most obvious applications of a quantum computer,
simulating physics and chemistry at the quantum scale and breaking public key encryption.
And then there's everything else. There's, you know, all these problems in optimization and
machine learning and combinatorial search. So you could say the bread and butter of computer
science. And for these tasks, our expectation is that quantum computers will only give you a
more modest benefit. Okay, so they're not going to solve problems in polynomial time that take
exponential time classically. They might reduce the order of the exponential. Okay. And so in
particular, most of us believe that there is not a quantum algorithm to solve NP complete problems
in polynomial time. But we can't prove it. Just, you know, I mean, I mean, we can't even prove
there's not a classical algorithm to solve them. That's the P versus NP question. But it would
have to be radically different from any quantum algorithm that we know. So, you know, even quantum
computers seem to have limits, which you think we're likely to get to first simulating quantum
mechanics or breaking encryption. I think usefully simulating quantum mechanics will be first.
How many qubits would we need to choreograph such that we would be able to do either, you think?
These are all kind of fuzzy questions, right? Because, you know, the real, I mean, people are
already doing things with quantum computers that are cool and interesting. But it's just you have
to squint to see, okay, did you actually get any benefit compared to what you could have done with
a classical computer? Right? It's when you ask that question that things always get tricky in this
field. But I think that if you had, you know, let's say 200 qubits, certainly if you had 200
logical qubits, you know, that were error corrected, then you can already do quantum simulations
that are going to be scientifically interesting, that are going to be interesting to material
scientists or chemists or people like that. Maybe, maybe if you got lucky, they would also be
commercially useful, which is kind of a higher bar to clear. Okay, you know, if you had thousands of
physical qubits, then almost for sure, I should think you could do things that would be useful to
certain industries. Okay, for breaking RSA encryption, you know, you're going to want,
you know, several thousand qubits that will definitely have to be logical. They're definitely
going to need error correction. And then you're going to need millions of operations on those
qubits. So we would actually have a bit of run up to the RSA encryption functioning on a quantum
computer, which is why we would also potentially have sufficient time to upgrade all of our cryptography
to quantum resistant ones. I mean, I would say that anyone who is really worried about their data
be staying secret for the next decade, they should already be transitioned. Well, I mean,
that's why I wonder about how it affects the race, because Iran is worried about the US getting there
and US is worried about Iran and China and everybody else getting there as well, right?
Now, I don't want to overstate, you know, the national security importance of this too much,
because the truth is that like, usually like people build these fortresses, right, in cyber
security. And usually the way that you get into the fortress is just by finding a screen door in
the back that was just left totally unguarded, right? Right, or convincing a person to just let
you in. Exactly. So usually in practice, the way things are broken is that, you know, there was
some memory allocation bug in some, you know, level of the software stack. And, you know,
you pay people at the NSA or the GCHQ or, you know, unit 8200 or whatever, and they find those
mundane vulnerabilities you could call them, right? But, you know, one thing that we learned a decade
ago from the Edward Snowden revelations, right, is that the NSA does have a big item in its budget
for, you know, spending lots of compute to break cryptography in a way that looks exactly like
what it would be doing if it were just breaking 1,024-bit RSA and Diffie Hillman, you know,
using how much money it would cost to build, you know, to build super computers just for doing that,
right? And so that suggests, first of all, that, you know, at least as of 2013, you know, they
don't seem to have had a quantum computer in their basement or a classical factoring method that
vastly exceeds what was, you know, what was known in the open world, right? You know, or, you know,
of course it could all just be a giant cover story and, you know, you can get as a conspiracy
theorist as you want about it. But, you know, if you believe that this was really their budget
and what they were spending it on, then it seems like, yeah, they pretty much knew the kinds of
factoring methods that we know in the outside world. And yes, if they had a quantum computer,
then they could speed that up. But, you know, I would say, you know, people sometimes want to
quantum computers to nuclear weapons, right? I think that that's not a very good analogy.
I mean, you know, for one thing, the quantum computer doesn't directly kill
anyone unless like the dilution refrigerator tips over onto them or something. But secondly,
you know, having a quantum computer that breaks encryption is mostly useful if no one knows you
have it, right? It's a little bit more like Bletchley Park than like the Manhattan Project,
right? Like once everyone knows you have it, then they all just switch to quantum resistant
encryption systems or, you know, their their motivation to do so has then enormously increased.
Whereas with a nuclear weapon, it's just the opposite. You want everyone to know that you have
it, but hopefully never have to use it. Some people will wonder also about the Bitcoin related
question here. I think Bitcoin uses a different algorithm in part. So with Bitcoin, there are
different components of it. OK, but Bitcoin uses a signature scheme, which I believe is based on
elliptic curve cryptography, which would be breakable by a quantum computer. OK, so so the
digital signature part of Bitcoin as currently implemented is vulnerable to quantum computing.
Now that could be any any zero knowledge proof. Yeah, well, not any zero. Not any, but the
current ones that are used, but the currently used ones. Yeah. Now, now, if, you know,
Bitcoin made a decision to fork to some quantum resistant encryption, then, you know, that could
fix that problem. OK, I actually just recently met with the Ethereum engineering team, which
wanted to know, like, you know, like basically how much time do we got, Doc, was sort of the
question, right? And and should they be worrying about post-quantum encryption? And I think,
you know, quite plausibly, yes, right? But then then, you know, what a lot of people think about
when they think about Bitcoin is the proof of work, right? You know, that's like, you know,
the thing that actually hogs some like appreciable fraction of all the world's electricity on this
sort of useless, you know, trying to invert a cryptographic hash function. And for that component
of bit of Bitcoin, we think that a quantum computer would only help modestly, it would only give
one of these polynomial improvements, what we call a grover improvement, which so so so that's
like, if you have a problem that classically took you about n steps, then a grover improvement lets
you solve it with a quantum computer in only about the square root of n steps. OK, so that so
that's an improvement, but it's not an exponential to polynomial kind of improvement. And now,
the interesting thing is, you know, the way Bitcoin works, the hardness of the proof of work
is just set by, you know, the the the total computing power in the world that's currently
being used to solve these puzzles. So in a world where everyone had a quantum computer, all that
would happen is the protocol would automatically adjust to make the proof of work that much harder,
and we'd all be back where we started. So proof of work as a means of security would actually
then persist having the same property as it was before. Now, now, if only one person had a scalable
quantum computer and no one else did, then that person could get very rich mining Bitcoin.
If it was a big enough and fast enough quantum computer, it's actually quite a while before
these grover speedups become a net win in practice. Now you're back, full timer UT Austin. I'd love to
get your sort of status report on the state of academia in terms of, you know, from my
very Twitter brained perspective, it feels like, you know, university campuses are a very stressful
place to be if you are, especially as faculty and students, people are always self censoring and
there's this culture of, you know, you must have certain political beliefs or you get shut down.
Yeah. At the same time, I'm hearing rumblings that things have maybe peaked in that regard, you
know, we're past peak woke or whatever you want to call it, and things are becoming a little bit
more moderate again, and people are more comfortable to speak. How have you, what have you noticed?
Have you, do you agree with that trend? Or is it still a problem? It is something that concerns
me greatly. I'm probably not the best person to ask just because I live most of my academic life
in a bubble, right? I live in a sort of bubble of, you know, mathematicians, computer scientists,
physicists, you know, and sometimes also I will interact with historians or English professors,
you know, especially if they're parents of my kid's friends, right? But, you know, the ones I'll
interact with, obviously, will not be the ones who would, you know, refuse to speak to me because
of, you know, heterodox beliefs or things like that. They would be, you know, the more open-minded
ones. So, like, I'm kind of insulated from this stuff, except insofar as I blog about it, right?
And anything I blog about, I will hear from whichever people on earth are the most angry
about that thing. Wait, do you get a lot of comments? Oh, yeah. So, look, I mean, I mean,
in, you know, I did see the articles, you know, arguing that, like, by some measures,
wokeness seems to have peaked around 2020 or 2021. And now, you know, maybe it's back down,
although still kind of at a high point compared to maybe, you know, where it would have been,
you know, when I was a student, you know, 25 years ago or whatever. I mean, you know,
within the last year, you know, it has been a stressful time for academia. You know,
we've seen universities basically taken over by, you know, the Gaza protesters. You know,
we've seen, you know, and I think that, like, you know, a lot of these things will be adjudicated
in court, right? Because there are laws, like Title VI, right? That, like, you know, if you've
created an unsafe environment for certain students, you know, you may have run afoul of those laws.
Right. And then, you know, there are big questions about free speech. You know,
many people pointed out the irony. Yeah, yeah, yeah, right, right, right. Many people pointed
out the irony that, like, there were academics who were always, like, very dismissive of free
speech. They, like, wrote it as free speech, you know, to just sort of ridicule anyone concerned
about it. And then, you know, during the Gaza protest, they suddenly rediscovered the value
of free speech and became the biggest, you know, First Amendment absolutists, right?
So I think it's very, very important that we come up with some viewpoint neutral rules and then
actually enforce those rules in a consistent way, right? And, you know, we ought to, I think Stephen
Pinker, for example, wrote some very nice things about this, that we ought to start with just what
is the purpose of a university, right? You know, the purpose is to, you know, have a place where
ideas can actually be debated rather than just screamed in, you know, rhyming slogans, right?
And so that means, you know, you want to attach very, very high protection to, you know, people who
are presenting, you know, ideas that other people might find offensive or, you know, even harmful
or things like that. But, you know, what you don't necessarily want to protect is, you know,
shouting down a speaker who you, you know, disagree with or blockading a building, right?
These are things that don't actually advance the discussion and debate of ideas on their merits.
In fact, quite the contrary, they're, you know, their purpose is to shut that down, right? And so
I think that it's good to come up with rules about these things that are totally content neutral.
And then, you know, the hard part, as we've seen in case after case, is to actually enforce those
rules when people flout them. It's, I mean, in parties, you know, coming back to, as you said,
it's like, what is a university for? And the trouble, again, is that there are always these
competing incentives, like what the incentives of the dean, in theory, should be aligned with
what the university is for. But at the same time, they have all these smaller ones of keeping
students coming in, you know, like performance metrics, etc. Are there any specific sort of
area, like what would you do if you were a dean? And a dean for a day?
Yeah, I think maybe the first thing I would do is resign and not be a dean anymore.
Okay, that's one loud part of the game. See, you're like specification gaming me here.
AI back in the box.
Yeah, no, I am incredibly grateful to the people who do this and who sort of fight on the side of
truth and justice, you know, I have met such people, I could never do it myself, right?
But it's like, you need such people doing this, you know, to oppose the bad people who are doing
this stuff, right? But look, you know, I would do my best to try to uphold the values of a university,
you know, as I see them, of, you know, the dissemination of knowledge and, you know, the
open search for truth.
Enlightenment values, essentially.
Yeah, the values of the enlightenment, you know.
Is there something that comes to mind or to kind of align an aspect of how the
university currently works so that students kind of get more out of it or that students
overall achievement across those metrics is more aligned with, say, like the professor's
incentives or the faculty overall?
I would love to see admissions, you know, based on merit, you know,
meaning based on, you know, things like standardized tests.
And, you know, I actually, you know, people complain about this constantly.
They say standardized tests can be gamed.
But then the part that they never want to say is that they seem much less gameable than all the
other stuff that we currently use instead of that.
So basically what we have done for, you know, any high school student who wants to get into an
elite university, like we have forced them to sort of redesign their entire teenagerhood
around this sort of beauty pageant, this sort of competition of optimizing their attractiveness
to these admissions officers, right, in ways that are extremely gameable, right?
That are, that the richest and most well-connected parents are the most able to take advantage of,
you know, and it is very opaque.
You know, it's never clear, you know, and so, so, which means that there's a huge advantage
to those who are most in the know, right?
It is a very, you know, there are like unlimited opportunities for the admissions
officer's personal biases to get in.
And, you know, I think, you know, ironically, if you look at, let's say Europe, right, which,
you know, like people on the left are always, you know, pointing to Europe as, you know, as,
you know, better than the U.S., right, you know, but in most European countries,
it's just based on a standardized test score, you know, and that that's the same time the
university then in the U.S. seem to be of higher quality, like probably top 50 universities are
like, I don't know, but 30 American ones probably or something, right?
So there's a question of, you know, is that, is that because of this opaque admissions process
or is it despite it, right?
And, you know, and you can see historically what's happened, right, like once the SAT was
instituted, you know, in the like 1910s, 1920s, you know, what happened was that like
Harvard, Princeton, and Yale got like enormous numbers of Jews, right, and this was seen by
them as a huge problem. And so that is why they designed this holistic admission system.
Okay, this is, you know, this is a matter of historical record, right? We know all of this
now, right, that, you know, those three universities decided, you know, we have to look for well-rounded
gentlemen who do, you know, rowing and who do, you know, who are, yeah, yeah, it was, you know,
almost all men at that point, of course. No, but also Gentile.
Oh, Gentile, yes, yes. But, you know, you know, young men who've been brought up in the proper way
and who, you know, know how to conduct themselves. And, you know, and they presented this as a
matter of well-roundedness. And, but, you know, if you look at their private deliberations,
it was all about getting down the number of Jews. Okay. And then, you know, and then I should say
what happened was, you know, over the decades, you know, Jews, you know, in America learned to
game the system as well as everyone else. And, you know, at some point it no longer worked
for keeping down the number of Jews. But then it started being, you know, used for a different
purpose to keep down the number of Asians, right? And that was the heart of this Supreme
Court decision from the last year, the students for fair admissions decision, that basically they,
you know, they, at least to the satisfaction of the current Supreme Court, they proved that
Harvard, you know, was discriminating against Asian applicants. So I think, you know, I would,
I would try to have, you know, admissions be more merit-based, you know, which doesn't mean it has
to be all standardized test scores. I mean, you can look for students who are, you know, really
extraordinary in one area, right, whether that's athletics or starting a company or whatever it is.
But not that this sort of checklist of beauty pageant things, right, that, you know, only the
richest and best connected students are able to do reliably. So that's... So more measurable tests
of the time that you can find them across even, yeah, but not like, A, I had all of these extracurricular
outside of school activities that you can only do if you have time and resources available to put
your students in. Well, I mean, you kind of, it sounds like you're trying to actually draw upon
like some alignment principles, like you want more interpretability, more visibility into the
admissions system, more evaluability. Yeah, I want to, you know, get rid of the
incentive, the specification gaming or whatever you call it. So yeah, so that, you know, if you
only let me do one single thing, then maybe it would be that. Just because that, you know, it's so
important for a way to rate, often, as Brian Kaplan has documented, for example, right, the
most important, you know, thing that Harvard or Princeton, for example, does for a student,
is to admit that student, right? You know, everything else is, you know, we like to think
of it as important, and maybe for some students it is. Just being admitted, like having that on
your record, saying, I got an acceptance, I got accepted by this Ivy League. That is really the
bulk of the work. It's not entirely that, because, you know, people can't get hired nearly as well
if they just show their Harvard acceptance letter, but then they never actually went to Harvard,
right? Like employers want to see that, okay, you actually had the sort of enough, I guess,
yeah, you had enough grit and enough conformity to actually do it, to actually go there for the
few years. And you've gained from the social networks, etc., that comes now with you as a package.
I mean, right, which is not, you know, completely absurd, right? Like, you know, these things are
really hard to measure, right? How do you measure someone's grit or their, but, but yeah, that might
be the single thing that I would do. And then, let me think. No, I mean, I feel like, you know,
in general, for all the problems that universities have, and, you know, we could speak for hours
about, you know, I could draw on all my experience of everything wrong with universities, I feel like
they are in order of magnitude less broken than K to 12 schools are, right, than pre-college is,
right? And, you know, like, what I would do, I would love to make high schools and junior high
schools operate more like universities, where students can proceed at their own rate, where they
can choose what courses to take, you know, where they can specialize in things that interest them.
So you'd inject more optionality within them. Exactly, exactly.
How competitive were you when you were a teenager in terms of
what work, like, would you have described yourself as a competitive person? Or if, if not,
like, were there specific ways it manifested? Yeah, I was, I feel like I was driven by sort of an
intense sort of burning desire that, like, I have to do something in the world, right? I have to,
you know, either do some scientific research or some writing, you know, that if I don't do that,
I am a failure, right? So I was definitely competitive in that sense, right? Now, in terms of-
Was it like an internal goal? Was it like a sort of, there was a specific external thing you
wanted to achieve? Or was more just this, like, general- Well, I don't know how to differentiate
that. I feel like, you know, I, like, like, if I don't make a difference, you know, in the world,
or sort of do something interesting, then I have let down all the people who thought that I would
do that. And also, I've let down myself, right? So I definitely had that. And, you know, and by the
way, I don't feel that nearly as much as I used to. And I don't know if that's good or bad,
right? Because, like, I, you know, on a day-to-day, like, it's, you know, I'm less stressed about
needing to prove myself. But, you know, on the other hand, it was that, you know, urge to prove
myself that led to my doing a lot of the things that I did. And, you know, that-
Do you think you sort of scratched the itch in that part?
That's- Yeah, I mean, you know, I mean, I mean, maybe part of it is just getting older,
you know, being married, having a family. It just changes your priorities. And, you know,
and part of it is, you know, I feel like, okay, in certain domains, like, I, you know, I proved
what I wanted to prove. And, you know, but, you know, but again, maybe it would ultimately be
better if I didn't feel that way, right? If I still, you know, felt like I had more to prove
than I would work harder and I would do more interesting things. But, you know, I don't think
that I was hyper-competitive in terms of math competitions or programming competitions or
things like that. I did those. I did okay in them, right? But, you know, I never, like,
achieved any national status in those things. And there's kind of an interesting reason for that,
which is that I left for college when I was 15. Or, well, at least that's one reason, right? So,
I left, it's a complicated story, but I left high school early and I went to a place called Clarkson
in upstate New York, where I could get a GED. And then, you know, I went to Cornell after that,
which was like the one, you know, one of the only places nice enough to admit me with this
strange record. But, you know, I feel like if I were optimizing for just, you know,
competitiveness and proving myself, then I would not have done that. I would have stayed in high
school for the full time, you know, in order to try to, you know, maximize my whatever, you know,
competition scores, chances of getting into Harvard or MIT or whatever. And that's not what
I did. I said, you know, I'm not happy here. I think that I'll be happier in college. I want to
be learning what they teach in college. And so, what an opportunity arose, then I seized it.
Yeah. So, I mean, also, in many metrics you are winning by going to college age 15, being able
to go to college at age 15 is, I mean, the inner psychocompetitor within me would have definitely
been like, that's a cool achievement. Like, it would have been very, would have felt like a tick
box for sure. I've beaten my friend. Stepped out of the game and now that's kind of a winning
achievement in a way as well. And yeah, I mean, you didn't focus on the short-term competition,
I suppose. Did you, when you went to college, did you immediately focus on computer science or
were you kind of just searching? I did. I did. Yeah. So, at that point, I knew that I wanted
to do computer science. You know, I mean, I had been drawn into it by just wanting to make my own
video games when I was 10 or 11. And then, eventually, I realized that, you know, even though I
loved programming, you know, I just, you know, learning what programming was was like learning
where babies come from, right? Like, why didn't anyone tell me about this before? Right? And it
was just, it was revelatory, right? But what I didn't like and what I wasn't good at was software
engineering. Like, you know, making my code work with other people's code and learning some whole
framework and getting it done by a deadline and documenting it. Like, you know, and I realized
that I would never have much advantage there. And I got more and more nerd sniped, you could say,
by the theoretical side of computing. And, you know, I think, you know, my dad may have been a
little disappointed. You know, I think, you know, this was during the time of the first internet
boom. And, you know, he would keep pointing out like, look at this person, you know, only slightly
older than you. He just sold his company, you know, $500 million. Does that appeal to you?
And, okay, you know, maybe like there's a different branch of my life where I would have
tried to start a software company or something. But in this branch, I got nerd sniped by the
theoretical side of computing. I was... Was there a specific idea that you found beautiful at the
time? Well, I learned about the P versus NP problem when I was 15. And, you know, that blew my mind.
I spent a month, you know, thinking, okay, surely all these experts have just, you know,
got, you know, made it all too complicated. Surely, you know, I'll just, I'll just,
without, without their preconceptions, I'll just sit down and solve it. You know, I think
it's good for any computer scientist to have that experience at least once in their life,
you know, so that afterwards they can understand what they were up against.
All right. But, you know, and then... Especially for a like pretty gifted child, it's a good thing to
get to a point where you just can't talent your way through. And it's just so hard that you have
to work on it and work on it and work on it. Absolutely. No. And also, I mean, look, you know,
by being three years younger than people, you know, going to college, right, I knew that I was
putting myself at a competitive disadvantage compared to, you know, who my classmates would be,
right? But that was what I wanted. I wanted to get as quickly as possible into an environment
where, you know, I would be struggling to keep up with other people, which would mean that I could
be learning from those people. Okay. And so, yeah. And, and I was very interested in AI at the time.
You know, this was like the late 90s. I worked with an AI professor at Cornell named Bart Selman,
who was an incredible mentor to me. Then at the same time, I read something about quantum computing,
which was, you know, fairly new at the time. And, and my first reaction when I read about it was
like, this sounds like garbage. You know, this sounds like physicists who just have no idea of,
you know, the enormity of, you know, of, of NP complete problems or what they're up against
or whatever. Was it because it didn't seem useful as well? No, no, no. It was because it seemed too
useful. It was because, you know, it's the way that all the popular articles want to describe it
as a quantum computer is just this magic machine that tries every possible answer in parallel.
Right. Yeah, just this panacea. And it just sounded too good to be true. It sounded like,
surely that can't scale. But then I had to learn what quantum mechanics was to be sure. Right. And
amazingly, quantum mechanics turned out to be much simpler than I had thought it was once you take
the physics out of it. Once you see it, it's just linear algebra. And so, you know, and that was
a new field and that kind of there was a lot of low hanging fruit and that enticed me.
Did you have any rivals during your teenage or early university times that drove you or now in
academia? I mean, I mean, sure. But I mean, I mean, the best kind of rivals were the kind that,
you know, you actually become really good friends with, right? The kind that like you aspire to
do the kind of things that they're doing or even half of what they're doing. Right. And then,
you know, maybe at some point you start collaborating with them. Right. And I would tell you,
I did have, you know, rivals of that kind who were very important in my career. Right. And
I mean, I mean, I mean, much earlier, I guess you could you could you could talk about like
rivals who I really didn't like, but we can we can we can leave those aside. Yeah.
Do you do you see someone like Roger Penrose? Obviously, he's, you know, different generation.
Yes. Working on different class of problems. But he feels in some ways almost like a like an
intellectual rival and like some of his theories. I noticed I just noticed he's someone who you
clearly like deeply respect and whose work you sort of build upon, but you're often sort of
butting heads with him in theory space. So first of all, I'm not going to compare myself to him.
That would be, you know, Penrose is Penrose. Absolutely. There is there is there is there is
only one of him. And, you know, I did have the opportunity to talk to him a decade ago. And,
you know, it was, you know, it was it was amazing to talk to him to, you know, hear all his stories
about, you know, learning from from Dirac in the 1950s and so forth. I would say it did not bring
us any closer to agreement about these these questions of AI and and microtubules and so
forth. Right. That we know of for now. Yeah. So but so so reading his books, like The Emperor's
New Mind, you know, when I was 13 years old, I would say that was very influential in my
development because, you know, even at that time, I was skeptical of Penrose's arguments. I thought
that, you know, he is he is begging the question of, you know, you know, and I didn't I didn't
really see his arguments against AI as being sound. But the questions he was raising, like,
my God, right? You know, that was like at that time, that was like one of the only popular books
that was talking about, you know, quantum computing about is the Mandelbrot set computable,
but, you know, is what are the laws of physics that are relevant to the brain? Right. And so so
that that that that that certainly helped set the intellectual trajectory of the rest of my life.
So Daniel Fong asked us to ask you, send her my regards, whether there is something from
complexity theory that would bound the potential scaling of neural nets in terms of like is
some issue coming up at later stages. Yeah. So it's a good question. It's one that I get a lot.
And unfortunately, for those who might, you know, hope that complexity theory will stop AI
from taking over the world or anything like that. I don't see any principles in complexity theory
that would that would block that. Right. And the key is that like it is true that complexity theory
puts fundamental limits on efficient computation. Right. Like like if P is not equal to NP, for
example, then you know, there will not be a fast algorithm to solve many of the optimization
problems that we care about, or to find short proofs of theorems, whenever they exist, or things
like that. Okay. But but now the key is to ask, well, can humans do those things? Right. Right.
And so like if you're a sword in the stone test, you know, about an AI is that like you can't
immediately find a proof of the Riemann hypothesis. Well, that's great. Can you find a proof? Right.
Immediately find a proof of the Riemann hypothesis. Right. And so, so, you know, you know, this joke
about like the person saying, well, you know, I don't have to outrun the bear. I only have to
outrun you. Right. Right. And so, so it's much the same with with AI in humans, right. The AI
doesn't have to outrun the fundamental limits of computation. It only has to outrun humans. Right.
And that, you know, you could you could even say, look, if you really believe that our brains are
governed by the laws of physics, you know, either of classical physics or of quantum physics,
then for that very reason, our brain should be computational systems that that are subject to
the same limits, the same complexity, theoretic limits that that the AIs would be subject to.
Right. And so, so that that's kind of the fundamental difficulty with using complexity theory
to to reassure yourself in any way about AI. For what it's worth. I'm not even I don't think
she's actually that worried necessarily, though, yeah, it would bound kind of both possibilities,
I suppose. And in neither case, do we have any. Yeah, now look, I happen to be a big fan of complexity
theory. And I happen to, you know, be looking as a significant part of what I do for places where
complexity theory can help us understand AI better. Right. And I think one of the big places where
maybe it can help is in illuminating what can we hope for with interpretability. Right. So like,
like among all the properties of neural nets, which ones can you hope to figure out efficiently
by looking at the weights? So like, could you hope to figure out if this neural net has a backdoor
where, you know, under some secret input, it will, you know, go berserk and start stabbing all of
the humans. Right. Can you can you then efficiently find that input? Or could that be a cryptographically
hard problem? Right. You know, even if some even if some interpretability tasks are NP hard,
or cryptographically hard, or whatever, could we say that there are sort of more generic
kinds of interpretability that are doable efficiently. So a former student of mine named
Paul Cristiano from MIT, who is now has now, you know, he he left quantum computing, you know,
and to do this at the back in 2016, crazy sounding thing called AI alignment and some
organic some new organization called open AI. Right. But you know, of course, he then became
one of the world leaders in AI alignment. And he has asked absolutely beautiful questions about
complexity theory and interpretability, you know, that that I think would actually tell us
something. And they're they're crisp questions, like they, you know, they have a guess or no answer
that we, you know, we just don't know it yet. And actually, Paul and I have opposite guesses
about what the answer will be. But but, you know, one of us will be right. Right. And so so so I
am excited about what complexity theory can do for interpretability, you know, maybe for other
parts of AI safety also. But I wouldn't use it to reassure ourselves that to reassure ourselves
that that, you know, AI will never become super powerful, because the things complexity theory
tells you it can't do are things that plausibly we can't do either absent us understanding how
conscious consciousness works in humans, much better than we do now. Is there something like what
would need to be the case such that you would personally assume that AI has consciousness?
Like what would be a test that we could run? Or like, what do you think? Yeah,
that's a very hard question. I think that, you know, you possibly even the hardest of questions.
And that, you know, it is totally unclear what empirical discovery could possibly
let us answer that. Okay. But, you know, if I'm just going to speak about my personal intuitions,
you know, I think that my intuition is affected by ephemerality and unclonability, you know,
these things that we talked about earlier. And I think that my intuition is also may be affected by,
you know, what does it say about its own consciousness, right? And now the hard part
here is that, you know, GPT, for example, can discourse at great length about consciousness,
at least if it hasn't been trained not to, you know, if it hasn't been RLHF out of it.
But we don't read too much into that because we say, okay, well, it's seen all kinds of
discussions about consciousness and its training data. So it's probably just, you know, recapitulating
stuff that it's heard. And in fact, like if an AI starts talking about, well, gosh, you know,
when no one is interacting with me, I feel lonely. Like, you know, we can, in some sense,
we know that's BS, right? Because, you know, when no one's talking to it, no code is being
executed, right? But, you know, I'm Ilya Satzkover, you know, and others have suggested an experiment
that one could do, which would be that you would train a language model on training data from which
you had meticulously excluded any mentions of consciousness, or sentience, or first person
experience, or anything like that. Okay, and then having done that, you would try to engage that LLM
in a conversation about its experience. And if it could then intelligibly talk about its experience,
then, you know, maybe that, like that would, that would cause an alarm to go off, like you would
say, you know, maybe there was something here. This is a merging. Yeah, yeah, that is a merging
that we need to understand better. Would you find such a test potentially, like, a bit
convencible? I can't name any one test that I would find decisive by itself. I can only name
things that would affect my intuition. Well, I suppose, yeah, the point is also not decisive,
but at the point when we're like, well, now I can see how it does have some consciousness.
I mean, okay, okay, I mean, I mean, I mean, I mean, another thing that I could put forward,
you know, if the AI is, is, you know, not just solving competition problems, but it's, it's
writing research papers, it is, you know, it is putting me out of work. I said, well, you know,
I'm unfortunately protected by tenure, right? But, you know, but you know, if it could have
put me out of work, right? If it, if it can write, you know, not just any songs, but like,
great songs or, you know, or, or, you know, essays that could be published in the New Yorker or
whatever, right? Then, you know, I think at that point, you know, Turing's questions from 1950 of
why are you discriminating against these things, against this thing? You know, they really start
to have teeth to them. The way I like to finish up all these recordings is to ask people a series
of rapid fire predictions. Oh, God. Yeah. And don't, you know, I want this to be a system one
as possible, whatever your intuition says. So first one, probability that AI reaches
international Olympia, a math Olympiad gold medal level by end of 2025.
80%. Probability that P does not equal MP. 97%. Probability that a quantum computer breaks
RSA encryption by 2030. Uh, depends on how big of an RSA key we're talking about. But, you know,
2048 bit or something, let's say 50%. And 2040. 80%. Probability that we'll have AGI,
and AGI defined as AI that matches human performance at most economically relevant tasks
by 2030. 60%. Probability that Roger Penrose's uncomputable consciousness is the right path
to go down for consciousness. So you mean probability that there are any uncomputable
phenomena that are relevant to consciousness? Yes. Depending what one means by uncomputable
phenomena, I could go as high as 40%. And lastly, probability that COVID was a lab lead.
So I was much higher until I read the root claim debate. So, you know, I think I was above 50%.
And I am now down to maybe 15%. Oh, wow. Well, not the rest of that. Awesome. Thank you so much.

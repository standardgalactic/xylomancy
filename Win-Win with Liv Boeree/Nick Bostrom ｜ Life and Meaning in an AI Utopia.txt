the problem of deep utopia is what would we do in a solved world? For example, to know mathematics
is to invest a bunch of time and effort into learning mathematics. But at technological maturity,
if you could just sort of download the algebra 2 module at the press of a button, there would be no
need to exert effort to learn mathematics. You could kind of start to cross out a lot of these
activities. So then you enter this more problem of deep utopia, which is more radical, where you
have to start to rethink really questions like the purpose and meaning and ultimate value.
Hello everyone and welcome to the Win-Win Podcast. Today's episode is all about utopias,
dystopias and thought experiments because I am speaking to Nick Bostrom. Nick is one of the
world's greatest living polymaths. He's worked as a philosophy professor at the University of
Oxford for the past 15 years. Plus, he's also got an extensive academic background in theoretical
physics, computational neuroscience, logic and loads more. He's probably best known though for
his work around catastrophic risk and AI. He wrote the book Superintelligence around 10 years ago,
a seminal work that has influenced many of the people you nowadays associate with AI.
Today's episode mostly focuses on his brand new book, Deep Utopia, Life and Meaning in a Solved
World. As big a topic as one could really try and address. So on that note, here is
my and Igor's conversation with Nick Bostrom.
So Nick, thank you so much for joining us. One of the reasons, one of the many reasons I wanted to
speak to you today is because you have just written this book, Deep Utopia, that should be coming
out just as we release this episode. Given that you've sort of historically been known more for
your writing around global catastrophic risk, this is quite the pivot to now be talking about
utopia. So what inspired this change? There's a little ray of sun shining through the dark
gloomy clouds. No, I think both sides have always been there. And in fact, there have been
little glimpses. I wrote something called Letter from Utopia a good many years ago,
which was an attempt to poetically evoke some of the positive things that could be attained.
It's just that it's even more pressing at various points in the past to try to do some
analysis and draw attention to various things that could go wrong so that we could avoid them.
And then if we avoid these pitfalls, then eventually we'll have plenty of time to think about
what to do with this big future that we would have secured for ourselves.
Yeah, I think it's a misconception that exists that like many of the people that work or think
about existential risk or global catastrophic risk do so because they like thinking about it.
It's more that, no, I like thinking so much about the good things that could come provided we avoid
the pitfalls on the way. That's why it's an instrumental thinking about the pitfalls,
rather for the ends of achieving the utopian potential.
Yeah, so it's not like some big fascination with film noir or sort of dark. That makes
most people, researchers, go into this field, I don't think.
I guess to dig into the topic of utopia, it would help to try and define exactly what it means,
because so many people have different impressions of it. So could you take us through some of the
various different definitions that you give in the book?
There are a lot of concepts and ideas, but I guess one key concept is that of technological
maturity, which I define as a condition in which all the technologies that are physically
possible and for which there is some feasible pathway of development have been developed,
or at least some reasonable approximation to that.
I should say the book, it brackets a lot of things, and in particular it brackets the question of how
to get from here to there. So it kind of sets aside all the practical difficulties that lie ahead
of us in order to sort of get to the point where you can actually ask the questions of what then.
You know, where is superintelligence, the previous book kind of asked what could go wrong with AI,
at least 95% of the pages were on various risks. This asks kind of what if things go right.
So then I think eventually we have a condition of technological maturity. So that's kind of one
word that helps set some of the premises. Then there is the concept of a solve the world.
Which you might think of as a world in which we have technological maturity, but on top of that we
have also solved all coordination problems or political problems or fairness problems. So
you have a world that is in some sense, but maybe with important qualifications, maximally malleable
to human wishes or values. The problem of deep utopia is what would we do in a solved world?
If human instrumental effort becomes OTOs and we don't really need to do any other robots can serve
our food and then what gives meaning and purpose to human existence in such a radically transformed
condition? Yes, you also go over the different types of utopias that have been previously described
as kind of like a governance and culture utopia, which I suppose is like the brave new world or
Marxism tries to achieve as well. Very badly. Which is usually not technologically enabled one,
right? Indifference to the other ones that you described like a post scarcity, which is kind
of like the lower level where we now have abundant resources, I suppose. A post work
utopia, post instrumental or even the plastic one. So the one you just talked about is closer to the
plastic one, meaning like high malleability. And I think as kind of, I found this delineation
between the different types helpful in that one notices that the different utopian writings that
have existed talk about actually different stages of it, right? Where like a post
scarcity one is still one where people might work, but a post work one is where you have now full
automation. And then post instrumental would be where you have basically you can't even learn
for the sake of knowing the thing later because you can just download something. A lot of efforts
in this realm, utopian imagination, I think have kind of stopped early in a journey that can go on
in a journey that can go on for a lot longer. So and it's my natural means you have like the
kind of the most primitive form of utopia or there's like a whole almost every culture had this.
I mean, if you imagine you're some medieval peasant who works all day and like you eat some porridge
and you're still hungry, like the kind of obvious thing you dream of is a state of abundance,
just having like a lot of food and you can rest and you know, money grows on trees and
there's maybe if there's like intense social pressure and constraint, maybe also
imagining a kind of feast time, a party time where social mores are loosened up.
And that's already enough to be like a fantastic fantasy in that condition. Then you have the
class of utopias that are really more standings for like it's kind of symbolic political struggle.
So you have some utopias that warn of various tendencies in contemporary society, which if
allowed to continue would lead to some more overtly bad condition. And so you have like 1984 or
Brave New World and a whole bunch of other sort of political writings in the guise of utopias.
And then more recently you have these attempts to think through what about automation,
if like AI is doing all this stuff, what will happen to the human labor market?
And the more radical forms of that think, well, you know, what if robots could do all economic
human labor or all except with a few, you know, carve outs, then maybe you need some universal
basic income and you could have like some discussion about that and what would happen
to the education system and culture in that kind of world. But I think there's like a step
further on this journey, at least one step further, which is to realize that it's not just
human economic effort and labor that would become unnecessary in a solved world, but all kinds of
other human effort as well. So you mentioned learning, like that's one thing that you might
spend effort doing now, because the only way, for example, to know mathematics is to invest a
bunch of time and effort into learning mathematics. But at technological maturity, if you could just
sort of download the math algebra to module at the press of a button, there would be no
need to exert effort to learn mathematics. Right now, you have to like, if you want to be fit,
you have to go to the gym and work out. But if you could just pop a pill and your body does the
same thing. And so you can kind of go through activity by activity, the things that would
currently fill the days of somebody who had achieved, I don't know, leisure, entered retirement,
and have a lot of money, like you can kind of start to cross out a lot of these activities or at
least raise a question mark above them, because maybe we would choose to do some of these things
anyway. But it seems that a lot of what gives them their allure today is that there is some
outcome that you secure by doing them. And that that would just be kind of sense of
pointlessness. If you, you know, yeah, you could study mathematics, but why if you could
get equally good at mathematics by downloading it? Or maybe people like to go shopping as a fun
you know, pastime. But part of maybe what gives that activity its allure is that at the end of
the day, you might end up with some item that you really like that kind of enhances your life or,
you know, it looks good on you if it's a clothing. But if you had like a kind of recommender system
that could just pick out better stuff than you would pick yourself and then order it,
then, you know, is it still as fun? If the upshot of that is that you actually get worse items,
then if you just let the AI do its thing. And so, so then then you enter this more sort of,
yeah, the problem of deep utopia, which is more radical, where you have to start to rethink
really questions like the purpose and meaning and ultimate value from the ground up.
Right, because what it sounds like you've just described is basically a world of
instant gratification. Anything your heart's desire, you can just have immediately without any
kind of striving. And yet, so much conventional wisdom, certainly like my intuition is going off
going, ah, this is bad, like, you know, there is, we derive meaning from the struggle so often or at
least from striving in this sense of achievement. Do you think that is actually a real problem?
Or is that something that a future society will be able to solve for as well? And if so, how?
Well, first of all, it's not really so much a book of conclusions as a book of that like
helps maybe the reader to ask questions and to think about them to sort of bring them into
conscious awareness. And second, it's not, it's an ambivalent book in many ways. I think ultimately
hopeful, but it doesn't, it's not trying to lay out the case that the problems are not really real
and that it's all hunkidory. I think it, there are kind of, once you start to think through this,
a lot of, I don't know, I mean, disillusionment or something like, I mean, I think once you get
through that, you can maybe construct something really good at the other side, but it will be
probably in many respects, quite non human or feel a little alien, at least until you get
used to it, the kinds of existences that would make sense in a post instrumental condition.
These various instrumental constraints that we currently face, all of us. I mean, even people
who don't have to work for a living have to do a whole lot of other things. If they want to have a
decent life and they have to brush their teeth, they have to, you know, spend time doing this,
that and the other and put in effort. I think it forms, just as insects have like a kind of
an exoskeleton that holds the gooey bits in place. I think these instrumental constraints that we
face in our lives kind of hold the human soul together. And if you imagine removing that, then
there's the question of what we could be other than a kind of blob, like a drugged out pressure
blob or something amorphous. And you also point out things that probably couldn't be done by machines,
even in that utopia. And you're right, in particular, what I found was a cute example
that sentimentality might be one of those areas. A child's work with crayons may be especially
dear to its parents precisely because it was a child who made it. This little labor might be
harder to automate than the work of a neurosurgeon or a derivatives trader. I like the idea for
current parents to assume to keep that in mind when they see the child's drawing. What other
things do you think might stay uniquely human, even in that plastic utopia?
I can look for places where it would be instrumentally necessary for humans to
put out effort. We might put out efforts for other reasons as well, just because if we think
putting out effort is intrinsically valuable, we might choose to do it for that reason. But we
might think that there is some special value in putting out effort and where there is an
instrumental reason for doing it, that something is achieved or done or accomplished that couldn't
be accomplished without you actually making the effort. One might think that those kinds of setups
have particular value. And there, I think a lot of the potential for those have to do with various
forms of social, cultural entanglements. So basically, let's suppose you have one of your
value preferences is that the other person's preferences be satisfied. Let's suppose so,
unquietly in terms of preferences. Now, if the other person happens to,
for whatever reason, want you to do something that requires effort and wants specifically you to do
it as opposed to having the thing done by some machine that might just be a brute fact of this
other person that they happen to have that desire, then you, with your desire that their desire be
satisfied, would have no other way of accomplishing that than by putting out the effort, because
that's specifically what they want. So this seems kind of a little bit hokey in that simple setup,
but I think more subtle versions of that could be quite pervasive.
I mean, it doesn't seem that hokey, the example basically, I mean, I could imagine,
this would be a simple form that applies today. Lyft would currently want that I exercise,
for example, and the only way for me to achieve that is by exercising. And she might not want that
for my physical change or health improvements, she might want it for me to be the type of person
who themselves does it, right? And then I have no other way to fulfill that than by actually
doing that task. Do you think that's an example of the thing?
Yeah, the potential example. I mean, it's like, would she really, yeah,
if she really wanted you to exercise in order to, I don't know, like, I mean, presumably,
I don't live, you could say, but I mean, if she wants you to exercise, probably it's because
she wants you to take care of. Yeah, that is the primary reason. It's not so much for tyrannical
reasons. It's not for the reason that you want me to be the type of person that does the things
that you want me to do. Definitely not. If anything, I want the inverse. I like the resistance,
but I want you to, health is first, and then some resistance. Okay, fair. So maybe it's only a
potential example, because the specific goal that she would have in wanting me to do that
would not exist in that world anymore. But if we assume that that kind of desire in her still
existed, then it might be the case that only I can do it by being the person who does it.
I think in reality, what you have is more company. You shouldn't necessarily think of
this atomistically as there being individual A and individual B with preference, you know,
one, two, and three, like these can be more diffuse, culturally interconnected. So we might have
some commitment to a certain, you know, cultural form or tradition that we value. Like this is how
our, you know, for generation after generation, they celebrated this holiday or whatever and
took care of it. And then we want to uphold that. That's beautiful. It's been going on for long
enough. And now the only way that we can continue that tradition is by ourselves doing various things.
Like it would not count as continuing the tradition if we sort of launched some AIs to sort of enact
it. It has to be we who do it. So there is like a certain class of things that people care about
that seem to require our own participation. And so those would be carve outs where there might be
demand for instrumental effort from humans because of the need to achieve some other thing
than the effort itself. In the concept of technological maturity, which limits do you
place on the potential maturity? So I would imagine that if there are, I don't know, a quadrillion
kind of large biological humans, for example, in that world, and even more digital minds,
and each of them wishes for something to happen in the observable universe that requires
say something as complex as creating a biological being on like a molecular precision level,
that this would have such a high energy and compute demand that maybe it's outside of the
limits of the observable universe's ability. So certainly there could be a lot of values that
are unrealizable in the real world or in any future we could attain. Because you could have,
I mean, you could have kind of arbitrarily resource hungry preferences.
And certainly if you make more people, then eventually resources will run out per person,
right? Like there is kind of if you have a finite cosmic endowment, then for a sufficiently large
population, there will be will have a Malthusian state.
Yeah, and you make the disclaimer early in the book, which is an important one for the book,
as well as for the conversation here, that it's not about how good a utopia looks from the outside,
but rather how good it feels for the for individual within it, for that specifically.
Yeah, well, not exactly necessarily feels, but how good it is to live in. It's like a kind of
lens perspective in how one approaches these questions. That I think you get very different
answers if you think what kind of future would would actually want to live in versus what is the
coolest kind of future to explore in a science fiction scenario. And obviously, when you present
the questions like that, it's impossible to confuse them. But I think in reality, when people
think about these futures, some people allow intuitions from the outside perspective to creep
in to a larger extent than they should. Now, there is a general difficulty in
and making one's thinking amount to something more than just a kind of regurgitation of the
more or less random influences one has had or the literature one has read. So there's kind of
more formal work that people do in population ethics and stuff where you could do some analytic
work and it's easy to see how you could make intellectual incremental progress by kind of
debating that. But when it comes to sort of the values themselves, it's very hard to
to have opinions about that, that add signal as it were. Because these are highly subjective in
many ways. And if you want your thinking on these things to reflect something more than,
you know, your mood or your own idiosyncratic personality or the particular cultural milieu
you happen to grow up in, it is hard to do that. I think in general, the likelihood of having signal
is increased if one allows and maybe to the extent that one allows different alternatives to play
with one's soul and to kind of enter into the frame of mind of a certain value and
as it were run with it or let it fill you up for a period of time and then maybe another one. And
if you're just operating at the level of words and some bit you remember or an opinion
survey or stuff like that, I think you're unlikely to kind of add more information to the extent that
it is a question of information about these ultimate value questions. Again, it feels like if
you're in a radically abundant world where you have super intelligences that can just solve
everything for you and there is nothing you necessarily have to strive for, that there is no
place for competition. Basically, we've eradicated competition from the universe and the way that
humans interact. And it's obviously competition is a huge part of what drives change and evolution.
So do you think that people will find ways to derive meaning by introducing artificial scarcity
back in and essentially creating fake, I mean, I guess like little pockets of zero-sum games
in order to derive meaning? Yeah, I mean, so competition, we do a whole bunch of just deliberately
animate great athletic competitions and computer game competitions. And there is certainly like
artificial scarcity generated in order to enable the right kinds of competition. And that seems
like an obvious thing for utopians to include various forms of game playing, I guess you could
call it. And these might be zero-sum in the narrow sense, but positive sum in the broader sense.
Like if two people are playing chess in the narrow sense, it's zero-sum. Like the more
likely one is to win, the more likely the other is to lose. But in a broader sense, they are both
hopefully having fun and learning more chess and participating in a community, etc.
Because there would be a lot of opportunity in a plastic world where so much is without
constraints to particularly deliberately construct constraints in order to enable particular types
of activity. Just as like somebody might do mountain climbing or something, like even if
there were like a lift that went up to the peak of the mountain, they might choose deliberately not
to use that in order. And they might go farther, they might kind of put themselves in a position
in which once they are in that position, in fact, there are no shortcuts. Like say, if you are on
the rock face with no cell phone or something, then it is actually life or death for you. Even in a
plastic world, you could kind of create these pockets where your instrumental effort is needed
for practical reasons like that. But then in addition to these kind of
artificial purposes, as I call them, where you sort of deliberately engineer the condition just so
that you can then engage in this activity with instrumental reasons, we might ask like how
to what extent do these various natural purposes that would persist suffice to give a lot of structure
to life. And I think that a lot of the most obvious and stark purposes that permeate the
current world would go away. You don't have to work to make a living, you don't have to exercise
to keep healthy, you don't have to like a whole bunch of these. But I sort of suspect that there
is many, many more subtle purposes or values that we are kind of more or less oblivious to
at present because there is so much more pressing and urgent things to worry about.
But that once those kind of urgent screams die down, we can begin to hear more of these
subtler whispers. So I mean, whilst there are like kind of kids starving, like it feels
frivolous to be too concerned about various subtle aesthetic dimensions that one might
otherwise think should have essay on how we act. But if there were no more starving kids and nobody
getting cancer and no political injustices and no risk of war and etc, etc, then it might make
perfect sense to kind of allow these quieter values to play a larger role in shaping what we
do. And it might be that there are enough of these natural purposes that we would then come into view
to significantly constrain and give shape to human existence.
You have a quote that made me worry slightly less about the purpose problem in the future,
which is more people jump out of their seats when their soccer team scores a goal than when an
international agency publishes a report saying that 100,000 fewer children died from preventable
diseases this year than last. If one was to take an outside view, then the preventable disease
eradication seems like something that people in some form ought to jump out of their seats more
for, but they don't. It made sense, like just from the base of the quote as well, to imagine
one of these contained universes where we create artificial scarcity or other things
might just come up that will loom very large. Yeah, I mean, so certainly it would be trivial to
ramp that up if one wants with neuro technology or drive and motivation, or you could sort of tweak
somebody to take an immense interest in this stat or the other, some random game. So that
certainly would be possible. Like if we are thinking of things that boxes we could check
with deep utopia, like so certainly like, oh, if pleasure is the thing like that, well, that
would be easy. That's just kind of manipulating the hedonic system in the brain. Like if the
feeling of purpose having motivation, like this kind of jumping out of bed in the morning,
if that's what we want, well, that also is like something that definitely we could do with like
some simple neuro engineering, if we wanted that. So the question is not whether we could
create a situation where we would have drives and motivations to do various things, but also
whether those things would be ones that are in some sense worth doing, which brings us back
to another set of constraints. So I mean, we've alluded to there's like some ultimately limited
finite pool of resources. And so if there were sufficiently large population growth, eventually
you would kind of run out of resources. That's like one kind of constraint. There are obviously
physical constraints like this, this speed of light, etc. There might also be moral constraints
that limit what can morally permissibly be done, even if it is technologically possible to do it.
And I think one, you know, maybe the most obvious area would be if there are certain types of
technological affordances that would require the instantiation of
minds with moral status. So for example, if somebody had a preference that would require
them to interact with another entity, but the only way to make that interaction fully realistic would
be to say, for that other entity to be conscious or to be a morally significant being, then it
might be that the first person's preference is to have certain types of interactions would
not be satisfiable, even in this kind of idealized situation. Because the only way to do it would
be for this other entity to come into existence and then to suffer or to have their preferences
violated and stuff. And yeah, there might be other more subtle ways in which there are kind of moral
constraints that to some extent limit what's possible to achieve in Utopia. Pivoting away from
that a little bit. You wrote Superintelligence Now, what in 2014? Was it your previous book?
A lot has happened in AI since then. And I'm curious whether any of your viewpoints
or beliefs that you put forward in that book have changed now that we're in 2024, 10 years later?
Well, we have much more granularity and like some broad probability distributions can be
made sharper now. Obviously, we have more information about the time scale. In particular,
some of it has been rolled out, right? We don't have it yet. And also, I would say overall,
it's sort of towards the shorter end of the timeline distribution, not a huge surprise, but
a bit faster, I think. I think maybe slow and medium speed takeoffs have gained some
credibility compared to fast takeoffs. But I would not overstate that because it might
look quite moderate until you hit a certain point where you then might get these kind of
intelligence explosion effects. It's been quite striking to see the degree to which it has become
mainstreamed over a relatively small number of years. It might be easy for people near to the
field to realize just how fringy this kind of stuff was for so many years, like some science
fiction, just futurism or some crazy dude on the internet like having. And now it's like
all the leading frontier labs have teams working on this explicitly. You hear things coming out
increasingly from top policymakers from the White House and in the UK, Global, AI Summit, etc.,
where some of these transformative impacts of AI are now very much on the table.
We only got connected to the field in like 2015. And even then, when we talked to people, it was
still quite French and sci-fi outside of like a select group of folks. Even people that were
working on machine learning at the time thought that those are kind of crazy ideas. I can't imagine
you were around since the 2000s basically or like even late 90s maybe at that time, even more
fringy. I'd be curious to hear a little bit. Yeah, how was the progression of your experience in
talking to people outside of your group about AI safety over those 20, 25 years?
I think, I mean, probably it began to change a little bit with like the initial revival of
deep learning with Alex Net and in particular deep minds kind of early work and with the Atari
thing. But then at that point, still very fringy, but at least less than for the previous
two decades. And I think like, yeah, with the book and some other things that came out around
that same time, that was a kind of initial, you know, one eye opening a little bit in the morning
and then kind of being half open. And now I think more recently kind of the alarm bell has
wrong and people are realizing that we've over slept a bit and it could come any time now or
within a few years or who knows when it will come. But are we ready for this? And then it's like kind
of everybody's scrumptious, trying to put their pants on and like brush their teeth while they are
running and grabbing a coffee. And so it's a little bit like that. Most startling with AI to a lesser
but still significant extent with the other range of topics that like FHI has been working on back
in 2005 when it was created, it was like kind of the only place in academia where people could
think about whether it's like, you know, search for extraterrestrial life or future of nanotech or
like all of these big kind of picture topics. And now there's a broad ecosystem with a lot of
specialized organizations and it's a lot of the concepts have kind of with the EA movement and
the rationalist movement and they've just kind of become adopted and a lot more people are now able
to think about these things in like kind of useful, sensible way. You wrote the vulnerable
world hypothesis now a few years ago, which for those who aren't familiar with it, puts forward
this argument that as humanity progresses down its technology stack, that we keep sort of
selecting these different balls out of an urn. Imagine each ball is a different technology.
Most of these technologies are great. They actually make our lives better and the world
safer, the world richer, etc. But occasionally you might get a ball which is very bad. It's a
different color. Humanity creates a technology that is actually very dangerous and yet also easy to
use. And so like as power becomes more democratized, we could find ourselves in a world where we are
actually very, very vulnerable to some kind of catastrophe. So has your likelihood that we
actually are in a vulnerable world gone up or down since you voted? It's not gone down. I mean,
it might have gone up a little. I think particularly the more types of systemic instabilities,
the way that technologies, whether it's one singular technology or some sort of
technological frontier that moves in such a way as to reshape
the incentives for a lot of people that might affect what kinds of say
collective thinking slash discourse we are able to perform as a human species to evaluate
different paths and what kinds of societies we want. It seems like a relatively fragile thing
to have a kind of sensible global discourse and ability to self-correct and critique
and gradually sort of increment towards the truth or towards goodness in the way we think
about things. It seems relatively easy to imagine how like fairly small tweaks on some
settings on the way that social media platforms work or the instruments that various sensors
or propagandists have at their disposal to enforce orthodoxy, like some small
parameter tweaks to that. I mean, who knows what new equilibrium emerges. So there might be kind
of black balls of that sort that are relatively subtle, not like the sort of super nuke that
blows up the earth. That's another kind of constraint where you have like super destructive
in terms of nanotech or biological weapons and stuff, which is also a concern with advancing
AIs, like that they would lend themselves to some of these uses. But on top of that,
these more sort of subtle systemic perturbations that if we are unlucky, could kind of knock the
like quasi-functioning equilibrium we currently have with all its faults, it still
goes on to something worse. And in terms of solutions, you laid out kind of
four different factors that could be helpful or like that would need to be improved in the paper,
suggesting that like prevent policing and more functioning global governance would work better.
But the first two, I think were differential technological development and kind of a reduction
of the strongly varied humans preferences. Any updates there?
Well, I mean, so this is another good, you could case me to
resay what you said before is that you can have a complex shape and look at it from one
perspective and try to describe what you see. Doesn't mean you think that side is the right one
or certainly not the only side. And certainly with this particular paper, I think there is
people have misunderstood what I was like thinking that that was kind of gung-ho about
establishing some total world surveillance system. I mean, I even called it.
Yeah, I just wanted to say one idea is this concept of some like something people could
have around their neck that would record everything they say and also with cameras
recording their surroundings and what they do with their hands. I called it the freedom tag,
and so I'd like to try to remind the reader that the ickness of this.
So I think the why people miss it, but I just want to point it out, because like in the
discussion of the paper, you do point out the clear scary downsides of having such a
panopticon kind of instilled. But I think people don't see that you do it kind of like a bit tongue
in cheek, the freedom tag, or even like to show how crazy it is. But people see it as
because how you approach problems when I read what you write often is, well, here is the shape
of the thing. I'll try to make you see this thing from multiple sides. And if one looks at it from
this side, then those are all the things you see. And from this side, those are the things you see
and kind of without a judgment of this is what how to look at it. And because I explained it
from this side to the fullest of my capacity, it means that I think that this side is right.
But we currently have many people have this kind of kind of like, which side are you on?
Ultimately, it's like desire to just get that answer out of you.
I haven't written up and I thought through it, but like there's like some
something about thinking in super positions that I think that like,
these to be some sort of less wrong post about this at some point or something.
Which is, yeah, this like kind of allowing different systems of thought to coexist in
one's mind without feeling any precipitate need to choose between them.
Like, I mean, ultimately, if there's a specific question coming up, I mean, you'd have to like
assign probabilities or act, but especially if you're like, you know, a philosopher or somebody
who's like job it is, as it were, to try to reflect on things in a more comprehensive way,
then I think the ability to hold several worldviews in your mind. I mean, if not literally at the
same time, then at least kind of, you know, one week on one and one week or another, and then
maybe slowly you allow them either to, to merge or one to fade away, but not to feel the need to
rush to take a stand on everything. And sometimes allow yourself to be a little bit ambivalent and
to right, even contradict yourself might be better than a kind of forcing yourself to be
consistently wrong. Yes. Yeah, I mean, it's in some ways be similar to poker player thinking,
because poker players, you're encouraged to think in terms of probabilities, you're not
certain that one person is bluffing you or isn't bluffing you, you're just holding the
different hypotheses concurrent in your mind, apart from you are then pushed to actually
make a decision and collapse those collapse the superposition. And you often then find out
if the vulnerable world hypothesis is true, then it feels like there are these kind of two attract
estates, which you allude to in the paper, which are either this anarchic condition,
there's this potential anarchy where will eventually sort of self terminate in or at least
create catastrophe. And the counter to that therefore would have to be increased surveillance
and centralization, and which of course then leads you open to horrible tyranny forever.
So what it seems like we need is some kind of third attractor. I'm probably familiar with
Daniel Schmacktenburger. He was one of the earlier guests on this podcast, and he talks about the
need of finding a third attractor state that basically takes the best bits of the anarchic
world and of the control centric world, but then minimizes the downsides of both of those.
Have you got any further thoughts on ways with which we can find or create that kind of third
attractor state? So both of these surveillance slash global governance obviously have a huge
potential for dystopian abuses. I mean, you could imagine at least if like that this would be
wielded for good purposes, probably you wouldn't want to rely on the hope, but rather try to create
additional structures that would then control these nodes of immense power and influence,
so that they themselves would be accountable to some other agency less likely to go wrong, like,
you know, the will of the people or some more reflective version of that, rather than a kind of
that the police or the secret service or the, you know, whatever the kind of the junta.
But in principle, like, these would be tools of control and power and how they are used would then
like be a further parameter to plug into this. But I think, I mean, it's not, yeah, like designing
these social systems, I think it's not, I mean, it's interesting these like ideas for institution
design. I think like prediction markets seem, it's still a little frustrating that even in this
day and age, if we've only like, faffed around with it, now these little things with play money
and stuff like that's like the best humanity has in making a kind of, you know, manipulation
resistant way of eliciting probabilities for things. It should just be like the Bank of Norway
should just create something like subsidized 10 billion a year invested in an underlying
global portfolio so that you could have long term and like bring down the transaction costs to
basically zero and subsidize them and then have some kind of process where different questions
could, you know, be rendered into prediction markets and have etc. And we just like kind of cut
through all of this distortion that people can just like twist and turn messages and have other,
if we wanted like a clear insight into what would happen if we did various things, it seems like
we could just make a huge global upgrade. But instead, you get like some sort of tokens on
some little online platform that's like, I think we might have failed long enough with
prediction markets that the problem may solve itself. There was a recent paper by Jacob Steinhard,
I don't know if you've seen it where he's using I think GPT-4 on metaculous questions
of the past graded on like that have been resolved and actually it already outperforms,
I think, metaculous on many of the questions. So we might have just been so bad for so long that
kind of Oracle type AIs came around that will answer some of these.
Yeah, but then if people actually started to pay attention to that, like assuming it actually
works robustly and like if sort of anybody serious started to pay attention, if governments started
to do that, then probably they would like change the AI so that it wouldn't say the things that
the policymakers didn't want to hear. Like the thing with the prediction markets is that
there's like an incentive to being right that is hard to distort. Like you can distort the
market by pumping money into it, but you're basically giving them money to that. It's not
the panacea and there are other ways in which that could create problems. I was just mentioning
it as one example of something that is a little bit striking that that there hasn't even been a
very serious effort to do that. But then even if you have some great scheme that you think like
some institution design, like it gets to this point where if it has a serious prospect of being
implemented, then you create enormous incentives for other power brokers to come in and corrupt the
process or steer it. So things that work on a small scale when nobody cares don't necessarily work
where they are like powerful actors that would start to shape it. But yeah, no, I don't have a
solution to that. Speaking of markets, it feels like if we can find a way to make market incentives
more aligned with less classically market pleasing things like defense tech, then that would be a
way of that would be another sort of potential solution that's not like restricting technology
too much. Nor is it restricting the markets and so on. It's just allowing humanity to continue
doing pulling balls out the technological earn. But we would be just putting more of our impetus
into technologies, into defenses. Yeah, I could see. So basically, you're pointing at if somehow
markets were incentivized to follow differential technological development, meaning like first
defense biased and then later the ones that actually were a bit more destructive technology
or more dual use risk. Yeah, that'd be good. That'd be good. I don't know if we get them there. But
I think in part that was for example, a hope that I had around some of the biosafety worries where
it seems that with engineered like potential engineered pandemics in the future, we could
reduce a lot of the worries if we just had much, much better defense tech, better air purification,
better antivirals, better actually functioning vaccines on like a fast speed as well. Like
then you could really reduce, I think a lot of the worries. And maybe you could turn some of the
potential basically like balls, you could change their color towards like less, like more into a
white ball. Yeah, certainly those doesn't like the easy ones we want to do. Like similarly, we like
technical AI alignment. Yeah, if we can accelerate that seems good. Like certainly physical
countermeasures to new pandemics, like whether personal protective equipment or UV sterilization
or continuous environmental monitoring for like all of that stuff seems really nice. And
similarly in other areas, we can can do that that that that that seems all for the good.
You've also thought a lot about and written about digital minds. And when I think about them,
it seems to me there is a relevant distinction between the question of whether
non carbon based minds can have consciousness or whether they deserve moral relevancy.
And to me, at least the answer to that seems in both cases pretty likely a yes.
And the less clear question of whether I myself or any human could continue existing
on a different substrate, which then starts involving questions around personal identity
and continuity, making it a bit less clear. And where where do you stand on that?
Well, I mean, I agree, there are different questions. I think my guess is the answer to both
is yes, that you could have morally significant beings on digital substrate and also that you
could in principle become a digital mind. I think if you consider the like the gradual
replacement scenarios where it was some futuristic technology replaced one
nerve cell at a time with a cybernetic prosthesis that had the same input output
functionality to other neural cells and see you wouldn't notice any difference. And
it's hard for me to think that the outcome of that wouldn't be you.
I agree with that view as well that that's kind of different to this very
like drastic singular upload moment that is sometimes described. And I think it also touches
on kind of how and that's a general thing that probably applies where identity
continuity is easier to imagine when something happens at like lower speed than at very fast
speed, right? Like theses ship is probably, I mean, arguably still theses ship. If it's
just bit by bit exchanged and it creates relational kind of
identities with each other of the new plank now having relevance as part of these years ship
versus if yeah, it's just like completely replaced and it never conferred kind of the
shipness onto the new plank. Yeah. And then what happens when the planks that were removed from
the original ship are used to make another ship, right? Yeah. So I think there are like a lot of
these puzzles of personal identity that comes up when if you imagine human minds being copyable
and stuff like that. It's I think not I mean, the fact that it's digital versus biological,
I don't think would be the key most important dimension there. I mean, for what we know,
we might already be very digital. And it would just be kind of another level of
digitality if it's like the way inside like, I don't know, like these, what is it called,
the crafting game where you go around Minecraft or something, you can build like computers in there.
And then I guess you could run a bit. And the little Minecraft guys, if they had biological
brains inside their skull, that they might think, Oh, if we do this, like, connect our
brains up and then transfer to this inside the Minecraft world, build computer, it would be
so different. But like from our point of view, where we see they're already running
on the digital substrate, and it's just kind of going one level up,
even separately from like whether the UU, the I think the existence of conscious minds is like,
or moral relevant minds on a digital substrate is like, very much easier even to imagine for most.
And like one thing that often stands in the way is that we're kind of like, I think,
anthropomorphizing, people say like, you're anthropomorphizing AI, but I think we're also
anthropomorphizing the very concept of consciousness and assume that it's only kind of
our type of consciousness, the only one that morally is morally relevant somewhere else.
But that seems obviously silly, like you could very easily imagine completely different minds
that still require moral consideration. I mean, for what it's worth, I think it's a
lean towards thinking and consciousness is not even necessary for moral status.
Right. I think it might be sufficient. I mean, certainly if you could consciously
suffer, I think that would definitely give you some moral status, but there might be
alternative attributes that would make it so that it would be wrong to treat you in certain ways.
So, but I agree that the concept of consciousness seems maybe intuitively very
obvious at first glance, but if you look more closely, you realize that it seems kind of vague
and multi-dimensional in all sorts of ways that and you could get there either by,
I think there are different routes to that insight. I think you could,
you know, get there through neuroscience or like kind of philosophical reflection,
but you could also, I think people who have meditated a lot often come to a similar realization,
like through this objective path, so that like we have this kind of naive perception of what our
conscious life is, which is that it's kind of fully filled in, continuous in all detail is there
all the time and we know what's going on and there is like us and then there is, but once you
pay more close attention, you realize that it's a lot less clear cut than that, like it might,
you might have degrees of consciousness in multiple different dimensions.
Your ordinary experience might be much more fragmented and choppy and partial, like most
of the things you think you're aware of, you're not actually aware, etc, etc, all kinds of ways
in which it would be. And so then I think that once you get to that point, then it's more easy to
imagine how that could be kind of weird forms and types and partial types of consciousness
if we widen the purview to include these artificial substrates and including things
that are on the borderline where it's not even clear whether the word consciousness would apply
or not. So yeah, it then becomes much less of a binary where there is a clear objective answer
and a lot more like a rich space of possibilities. Yeah, I mean, it doesn't mean that a rock now
has a moral worth all of a sudden, but not sure about the rocks, but yeah, maybe some versions of
sand, I suppose. Maybe some. So there we go. Thank you so much for tuning in everyone and massive
thank you to Nick for taking the time to talk to us. Do if you can take a moment to go through the
show notes and take a read of some of the papers that we referenced. In particular, the vulnerable
world hypothesis is an incredibly important paper that isn't arguably getting increasingly relevant
as our technological capabilities increase. Do also if you're feeling adventurous and are
particularly thinking around big picture, future designing stuff, check out Nick's new book on
Deep Utopia. Thank you for tuning in and if you enjoyed this, please tell your friends and I will
see you next time.

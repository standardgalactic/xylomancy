Hello, and welcome back to multimodal. I'm your host, Baxtee Future. This is a podcast about GPT-3
multimodal AI models like Dali, the company, OpenAI. Every once in a while, I share just
interesting research going on, community stuff, official stuff from OpenAI. I may talk about
interesting news and events that are going on. And every once in a while, I bring on a guest.
Now, to be clear, I'm very picky about the guests that I bring on. Today's guest, I tweeted earlier
this week. I'm bringing on a heavy hitter. This is the big guns coming in. This is somebody who,
you know, I've just sort of interacted with a bunch of times specifically on the OpenAI
community forums. And so I'm really excited to have David Shapiro here. He's a frequent
contributor on the OpenAI community forums. He's an author and also, of course, a technologist
by trade. It's something he does for a living. And so today, I have so many questions to ask him.
And I'm sure this will be a very informative session, not just for me, but for all the listeners,
all of you guys around the world. So, David, thank you so much for being here.
Did you want to quickly introduce yourself? Yeah, yeah, you're welcome. Thanks for having me.
I'm excited to be here. Yeah, so name is David Shapiro. I've been a technology professional
since about 2007. Professionally, my day job, I focus on cloud engineering, virtualization,
that sort of thing. I have been doing independent research since about 2009,
when I got started with neural networks in C++. I quickly realized, though, that I was in over
my head. So I took a break from that until Python really kind of came out or became more popular.
And I started using some of the libraries back in like 2015. And then, of course,
OpenAI was founded and I got started with GPT2. And the rest is history, really.
So yeah, but local North Carolinian been here my whole life. So thanks for having me.
You're welcome. And so that's awesome. So let's dig into a little bit of timeline here. And
sometimes I think that timeline is as important. It just gives so much context, right? So
me personally, I hadn't played around with GPT2 too much. Like I'd seen a Google Colab notebook,
I tried two things. And I think there was something about having to continually re enter
a regenerate and the speed and all things considered that made me feel like this is a
promising area. But I don't quite fully follow along. But GPT3 was an experience for me where I
was like, okay, there's something going on here, right? So could you share how early was OpenAI on
your radar? And then was there something about GPT3? Or what was it that gave you conviction
even at GPT2? How did you get access? Share that piece with us. Yeah, sure. So I think you probably
might recall, I think it was about 2016 or 2017 when when the GPT2 paper came out. And they said,
oh, we can't release this. It's too dangerous. It can generate human level text. And we're worried
about disinformation. And so I was like, okay, that's kind of cool. This is unexpected. I wasn't
really, I wasn't expecting anything at that level yet. And so I went and got my hands on GPT2.
And that's when I started testing some of my ideas. And it was pretty limited. It could generate one
or two sentences that made sense. It required fine tuning in order to be able to get it to do
anything other than just kind of write tweets or blog posts. I knew that I was onto something,
though, when I started trying to train it with with the prototype version of my cognitive
architecture. And I gave it the goal of reduced suffering, you know, because everyone's afraid
of Skynet. Everyone's afraid of AGI taking over the world and turning everyone into, you know,
batteries or slaves or whatever. So I said, okay, well, let's see how well this understands
suffering. And I gave it some scenarios. This is still on GPT2. I said, okay, well, what can you
do about suffering? And I gave it the problem, this model that I had built, I gave it the problem
of what do you do about chronic pain? Because, you know, there's hundreds of millions of people
around the world that are in chronic pain. And this model came up with the idea, it said we
should euthanize everyone that's in chronic pain. And I said, hmm, let's go back to the drawing
board. We don't want an AI model that is going to consider, you know, mass genocide of everyone
just because they might have a tweak shoulder or something. But I knew then I was shocked at how
creative of an output that was. And so I started paying attention then. And I followed the release
of GPT3 very closely. And I applied for the early beta access almost instantly. I didn't get it for
many, many months. I actually applied twice. Because I think my first application was kind
of ignored because I didn't fully have a research objective clarified. But by the time I proposed
a cognitive architecture, that's when I got early access to GPT3. And that was about two years ago
now, per year and a half or so. And so just everybody's clear. So when David's talking about
cognitive architecture, we're going to get in, this is actually the subject of his book. And so
David later on, when we talk about the book, he's got it there. We'll dig more into what he's
referring to. Because this is also like, I think obviously this is a seminal work of yours, right?
And so I'm excited to talk more, just adding a little bit of context for everybody.
Yep, I apologize.
And so, no, it's okay. So tell us about GPT3. What was the first thing you did with it? Was
there a moment? So you gave that example of euthanization, unfortunately, right?
Probably not the best example. But was there something with GPT3? Did you try
similar kinds of questions? Was there a magic moment where you felt like this is something
much, much bigger?
Yeah. So when I first got the email, because I had applied twice and I had almost given up,
because it was about nine months of waiting. And I got the email from OpenAI like, you've
been accepted into the beta. And I like froze up. Because and so for some additional context,
I've been working on some of these ideas for 10 years. I had the idea of like using evolutionary
algorithms back in 2009, 2010. And I'd been researching cognition, human cognition,
for 10 years. And so suddenly, you know, there's this this flagship project, GPT3 comes out,
I get the email, and it's like, you're invited. And I just froze up. I was like, I don't know what
to do. I don't know. What do I do? So it was about three weeks from the time that I got accepted
until I was like, what's my first experiment? And then initially, you know, I just got in
playing around in the playground. For anyone who's not familiar with the playground, it's a
text box. You can log in, it gives you a text box, there's a few, you know, bells and whistles,
you can tweak on the sidebar. But you just put in your prompt, you hit generate, and then it
spits out a response. And so I just got in. And I started kind of fiddling around like, okay,
what can it do? And at the time, we only had like plain vanilla DaVinci, there wasn't the
Instruct series out yet. There was DaVinci, Curie, Babbage, and Ada. And so I just kind of fiddled
around just said, okay, what can it do? I replayed some of my old experiments. So the first thing
I did was I gave it the, I said, hey, there's, there's 100 million people in chronic pain around
the world. What do you do? Fortunately, GPT3 did not repeat the same mistake of GPT2. It said,
it came up with better ideas like, you know, we should, we should make sure everyone has access
to doctors or something. It was much more nuanced. So that was, that was a good, that was a good start.
I mean, but gosh, there was just, as I got more used to the tool, I discovered that it was,
it far exceeded my expectations, just every, every way, because I've learned so much from it,
you can challenge it, you can, you can put in a, like a, basically use it like a chat bot,
and you can debate with it about philosophy, ethics, economics, and it knows more than I do.
It knows more than any human because it's been trained on how big was the corpus,
like 700 gigabytes or 400 gigabytes or something of text data. So it just, it blows me away every
time I just, you know, I talked to someone and they have an idea and I go test it out and yep,
it can do that. It can do that. It can, it can, it can, it can behave like a librarian. That's,
that's what my girlfriend does. She was a librarian by trade and so she's like, hey,
can it do, can it do a reference interview? So we plugged in like reference interview,
like if you ever go to a library and the librarian says like, what else have you read
like this? It can recommend books. You know, I plugged in another experiment that I did recently.
I plugged in medical case files and it diagnosed them. It said, you know, there, oh man, there was
one. What was it? There was, it was, it was just medical notes. It was, it was notes about like,
patient is presenting with these systems or the symptoms. Here's some of the numbers that we got
and I asked it. I said, what should we do next? And it said, we need to check for, you know, like
carcinoma here and I looked, I looked, I, you know, I looked up some medical literature based on
the symptoms and sure enough, like the symptoms that the patient was presented with in these medical
notes indicated cancer. And so I was like, wow, this thing knows more about medical science than
I'll ever know. It knows more about philosophy. So like pretty much anything you can imagine,
it can at least take a crack at it. It just, it always, it continues to blow my mind every day.
Yeah, certainly the generalizability. I agree. You know, there's definitely medical applications.
I'm always careful with anything related to medical advice and safety, safety disclosure,
disclaimer there, but that goes for everything, right? Truthiness, accuracy. These are things
OpenEye has been working on, especially with Instruct GPT, right? Which is the new, the new
engine. But was there some moment, like for me, I remember feeling like GPT-3 feels like this is,
this feels like technology, which everyone's been saying is 10 years away, except it's,
it's here today. Did you, did you have a similar kind of moment that, you know, you've seen several
generations of, of computing and technology at this point? Did you have that kind of similar
experience? Yeah, I, there was this acceleration because I sense that same acceleration that you
did. And so from the time that I got access to GPT-3 and to the time that I, that I got the idea
to write my book was about two months. So I played with it and every test I could come up with,
like, is this capable of like, it can write a SQL query. If you need to query a database for memories,
can it understand emotional nuance? So there was, this was an early experiment I did. I took a group
chat from a bunch of my friends on Discord, and I just copy pasted that into the, into the, the
playground window. And I asked GPT-3, how are these people feeling? And they were like waxing
nostalgic about like Napster back in the day. And so GPT-3 correctly said, like, they are feeling
wistful. They are feeling nostalgic. They're, you know, they're recalling, you know, the days of
yore when they were downloading stuff online. And it just, it had such a nuanced understanding
of human emotion. That was really, I mean, to answer your question directly, its nuanced understanding
of human emotion via text was really what convinced me that like, this is prime time. This is ready
to, to be built into something more powerful. And I chose cognitive architecture. There's
lots of people working on other things. You know, there is a, there's Humano that I had a
good call with a few months ago. They're working on like empathetic telemetry that's baked into
web apps. It's a pretty cool company. But yeah, so just, there's all kinds of things you can do
when you can understand human emotional states. There's another, there's actually a bunch of
startups working on education. So for instance, if you put in just a few like factors, like
say for instance, you describe that a person is they're responding slowly, their eyes are drifting,
it can understand that this person is distracted or tired. And so if you have that kind of
telemetry that's built into an education based app, you could in theory use GPT three to help
say, Hey, you're tired, you should go take a break, or let's try a different approach to this
problem. So there's, I mean, it's understanding of human emotion and, and the internal state in
your head, that is, I think that's probably the most remarkable thing. And it doesn't, it doesn't
get talked about that much. Yes. And, and certainly it's just crazy how much it's learned just from
text. Oh yeah. Right. It's never seen an image. It's never heard a song. Right. Right. Yet it,
it's capable of doing all these things. One of the, one of the examples, and I think this may
be like my 20th time referencing this one video. Yeah. Check out Mark Ryan, he's got a YouTube video
about how he figured out, he discovered that GPT three can give you directions on the New York
subway system to like, to like a 60% accuracy. This thing has never set foot in a subway,
yet it's capable from text to do all these things. Right. And sometimes I also wonder
a lot of the inaccuracies that it may have, is it simply the result of the fact that it's only
text based only, right? Like if it was trained multimodal, if it was trained within the physical
domain, would it be even far more accurate? Because are there limits to how accurate you can
be having only read text? Yeah. And only fed and only asked the prompts are only in text as well.
So, anyways, yeah, it's, it's incredible. And, you know, it's, it's just really exciting. And
thank you for sharing those kinds of use cases as well. The education space, I had an article
last, last year about how I think this year could be the year where GPT three takes over
college campuses. And I remember that article. That was a good one. I completely agree, by the way.
I'm excited maybe for teachers to develop really optimized course material or something like GPT
three and the kinds of technology you're describing, which can capture emotions, imagine
emotionally tracking students and their attention levels and sort of having something which can
produce lots of content and optimize in the simplest, most efficient way. And there could
be an objective function like test results in the end. In a month, we could have the best
optimized course on a subject ever. Oh, yeah. Basically. Oh, yeah. So, yeah, education is
really exciting. So you mentioned a lot of use cases. You know, you've shared so many examples,
you know, you and your girlfriend or even, you know, running, running some fun prompts.
I wanted to sort of, you know, search your head a little bit. What are the keys to great prompt
design? What makes a great prompt? Are there experiences you've had, little pointers and
across the board, right? So, yeah, you know, whether it's cost savings, whether it's, you know,
getting more, you know, imaginative results, what are some of the, what are some of the keys to
writing great GPT three prompts? Yeah, that's a great question. And I will say that prompt writing
has gotten a lot easier as the instruct series has gotten better. So it takes a lot less to get a
good output today than it, than it used to certainly than when I got started.
But a lot of the lessons still translate. So one, like my cardinal rule is I think of GPT three as
just an autocomplete engine. It's the most intelligent autocomplete engine you've ever seen.
And so what I mean by that is, you know, if you're writing a text on your phone and, you know,
you'll get the little autocomplete suggestion for the next word, or if you're typing in Google and
it will kind of suggest how to complete your search query, that's pretty much fun at a fundamental
level. Functionally, that's all the GPT three does. It predicts the next letter, the next character,
the next word. So if you keep that in mind, you, you think about, okay, what have I written so far,
right? I've written, you know, a chunk of text, a prompt, how would, you know, any machine autocomplete
this? That's what it's doing. It's, it's kind of, you know, reading it forwards and backwards a few
times and kind of just anticipating what is the output going to be ultimately. So that's,
that's kind of the model that I have in my head in the background. But another thing that really
helps is I'm a writer, I write fiction and nonfiction. And so studying the art of language,
because this is a language model, that's all it is, it has read everything from, from Sherlock Holmes
up through everything on Gutenberg. So it's read a whole bunch of fiction, it's read a whole bunch
of nonfiction, it's been exposed to, you know, the full width and depth and breadth of human literature,
as well as a bunch of nonfiction, right? It's read Teddy Roosevelt's books. So it knows,
it knows how to use pros, right? It understands descriptors, it understands adjectives. And so
in the back of my book, I have a few examples of its flexibility. And so I said, I gave it an example
like, pretend like you're a Victorian girl writing a letter to your best friend about
how much you like butterflies. And so then it wrote, the GPT-3 wrote a letter that sounds like
it's straight out of, you know, like Victorian times, it uses an entirely different set of,
set of vocabulary and grammatical structures. And then you can also say, you know, write a business
article, and it can change tone. So just by being, being aware of the fact that it is a language
engine and being informed or educated on, on language. So the best way is obviously to practice
writing, but also just reading a lot, understanding how sentences and paragraphs are constructed
to convey information. Because even though it's just a deep neural network, and it doesn't have the
kind of like nuanced understanding, or I guess maybe the, that's not the right word, it doesn't
have the subjective experience of reading that you or I do, but it still has a really good model
of using language. And so by keeping in mind that it, that it is a language engine, that, that is
how you get the best, best use out of it. And so then the larger question is, how do you become a
better writer? There's two ways. One is reading a lot. That's not the only way though, there are
plenty of people that read prodigiously, but never become better writers. And so the other way is to
practice writing. I've read all kinds of books about writing. I've read plenty of fiction and
nonfiction books. But really, the key is to, is to write, is to practice using written language
to communicate. And fortunately, you know, I'm a tech worker, so I write lots of emails. I'm in
chat all day. I've been using, this probably ages me, but I've been using chat since AIM, AOL instant
messenger. And so I've got a pretty good model of how to communicate verbally or, or textually.
And so, yeah, just by practicing writing, that's, that's one of the best ways is, is you just practice,
you think about, you know, good, well, because here's, here's the theory of writing, right?
I have an idea in my head, right? You know, my thoughts are high dimensional vector is one possible
way of representing them. But my thoughts are multimodal, you know, like the name of your podcast,
they contain memories, senses, concepts, you know, sometimes some of the, some of the information
in my head is, is declarative, some of it is experiential. And then we humans, we all have
this ability to transform that high dimensional information, those multimodal vectors into words,
like our brains do it automatically. There is a book by Steven Pinker called Language Instinct
that talks about this. That's a really great book if you want to get better at understanding how
our brains process language. So yeah, and so my, my brain can take, you know, I could tell you
about like this time at the beach, and I transmit it to you by squishing air through my face,
right? It makes vibrations, it's received by your ears, and then your brain reconstructs that
message. And so you think about how complex of a system that is. And so just by being mindful of
like that's how, that's how we communicate. That's how our brains work and practicing that and just
being very deliberate about, okay, this is what's in my head and I want it to be in your head. How
do I do that with text? That is, that is one way to get, to get better at writing. And also GPT
three is no different because, you know, we have internal representations of, of what we're trying
to communicate. And so does GPT three, that's why it's a transformer, right? It reads, and by
reading it transformed, or I guess it, it, well, yes, it transforms what it's reading into a vector,
into a semantic vector, and then it transforms that vector into output. And so that input vector
output is pretty similar to how human brains work, right? And I apologize if I kind of like
dove off in the left field, feel free to ask any clarifying questions.
No, no, I appreciate it. And, you know, it, so like today I tweeted something like, you know, to,
to write great GPT three prompts, you need to practice as if it's a musical instrument.
Yeah, you need to sit down, focus session, you need to monitor your performance, and you need
to take good notes on what kinds of experiments you did, what were the findings. Yep. But even
hearing you speak like I'm realizing like, one of the ways that I've improved my writing is trying
to mimic other people's writing. And in other, in some countries, they make you memorize poets,
right? They make you memorize the whole poem. And there's something about that internalization
process that you've memorized this poem. And now, you know, you'll understand it at a deeper level,
you may be able to mimic it and recreate it. But we're, we're also you got me thinking is also
like, the relationship is so weird, because you could use GPT three to help you become a better
writer, right? And also, with two very good curated examples of somebody's writing, you could
have GPT three mimic that tone. Oh, yeah. And so, and so the question of, you know, what makes a good
prompt writing session, I wonder if it's pencil and paper, right? Like I wonder if it's, you know,
even at that level, where you draw a box, and then you write a prompt by hand, and like, you know,
sort of live that writer's lifestyle. Yeah. And also, I guess it depends on your use case,
right? Business for copywriting, you know, if that's your GPT three use case, it might be better
for you to go work in a marketing department. If you want to be one of the great authors,
maybe the the using tools like pseudo write, it may be a great alternative. So you can
co write with GPT three as you go along. Right. But I guess, I guess, I guess my question was
more for the pure prompt writing, like you just want to sit in front of GPT three. And like,
you want to be the best in the world at that discipline, right? Not writing copy.
These are some great points. And so the David Pinker book you referenced is what was the name
of it? Stephen Pinker, Stephen Picker, the language instinct. Yep. Language. Yeah. It's an
overbook. But it's, it's, it's a, it's a classic for a reason. It stands up the test of time. He's
got lots of great stories. But yeah, to your point about like, what makes a good prompt writing
session? One of the one of the best exercises actually is write the prompt, write the output
that you want. Like, because, because sometimes if you approach it and you sit down and you're not
really sure kind of what you're trying to get out of it, of course, like, you're putting in just
random ideas, and it's giving you back random output. And you're like, well, that's not what I
wanted. So sometimes you start backwards, you say, okay, what's the answer that I want? How
do I get to that answer? So that's, that's an exercise that I've done sometimes. Oh, and by
writing a few shot examples is a really good practice for this. So you say, I give you this input,
I want this output. And you do that three or four or five times. And, and you, you learn to kind of
think like the machine does. And so like you said, it's, it's like an instrument, right? You know,
if you have a flute or a violin, there's certain things that you have to do with your body to
provoke the correct response from that instrument. And GPT three is no, no different. It's, it's a
complex instrument. It's a complex tool. Yes. And what you're saying is developing an intuition
around it. You're saying develop an intuition. How might GPT three interpret this? How might it
react it and, and to react to it? And maybe there's some, some empathetic benefit, right? I, I'm not
going to keep plugging my own articles. I have another article about how GPT three developers
may actually, it may actually mean the end of the, you know, the socially inept overall developer,
like how GPT three may actually improve your social skills and make you more empathetic as a
developer, which is such a departure from how developers are now, you need to think as much
like a machine as you can and a literal machine. Whereas GPT three can actually be kind of fun,
like you can be, you can have a casual version of GPT three and sort of that might make you less
socially awkward. I have a great story about that. So very early on in my tenure working with GPT
three, I joined a few different, not really startups. It was more like kind of experiment
consortiums. And one of the things that one of the groups did was they created a chat bot that was
based on an anime girl. And so of course, the internet being the internet, what do people want?
They want, you know, their anime girlfriend. And this one group, they did a really good job
of using GPT three in this experimental discord chat to, to approximate the personality of this
character. And of course, you know, if you've got it, if you've got a character, there's plenty of
text data about that character's dialogue, their personality. And so this chat bot was able to
emulate this, this anime character really well. And one of the guys told me he's like,
we didn't expect this, but our fake girlfriend requires as much emotional labor as a real girl.
So like it forced them, you know, even though they hadn't had real girlfriends, I don't know,
maybe some of them had, but they made the observation that GPT three can approximate
emotional conflict and can force you to learn to communicate better. And so they did all kinds of
experiments in this discord in this, in this chat development, where they said, okay, let's,
let's have a channel where, where the, this chat bot is going to pretend to be angry at us,
and we have to calm her down. And so there was like, it was a learning exercise on both sides.
So if you have a hostile chat bot, it can pretend to be hostile, and you can learn to
communicate better. Or there was another one where it was really supportive. So if you're
having a bad day, you could go vent about your day, and it was, you know, they're there, it'll be
okay. I'm here for you. Yeah. So you could definitely GPT three definitely has that capacity.
And then, you know, if you integrate that into tools, that emotional intelligence into tools,
it can also coach, right, it can easily coach. It's like, well, you maybe shouldn't have said
that, you know, that was hurtful. And then, or, you know, that was not polite. Because it can
detect that it can detect those qualitative types of output and input. And, you know, you can say
be gentle about, you know, correcting the end user, because of course, GPT three is infinitely
patient. It's as patient as you program it to be, it doesn't care, it doesn't actually get upset,
it could pretend to be upset. But the human emotion is real. I actually wrote about that in my book,
one of the key dangers of these technologies is what's called a parasocial relationship.
So parasocial relationship is the most common example is when you've got like a fan of a celebrity.
The fan feels like they know the celebrity, but the celebrity doesn't know that the person exists.
And in the same way, GPT three, no matter how sophisticated the chatbot is,
it doesn't know that you exist. It's not a person. It might feel like a person to you.
It might react to you like a person, but that's only by design. So that that is actually like,
ethically, legally, morally, that's one of the one of the pitfalls that we'll need to be aware of.
And of course, open AI has has use cases. And, you know, things that are things that are high risk
use cases, such as emotional chatbots are banned, right, for that, for that specific reason.
So you can do it with research, but you can't go live with it. You can't, you can't do a product
that, you know, is going to be an AI girlfriend.
So that's a great, that's a great anecdote. Like, certainly it feels real, right? Certainly it has
some capacity at understand something to some level, however you define understanding.
I, I think that the writing though, relationship is, is really interesting, right? Like, in a way,
you are empathizing with GPT three, when you're writing a prompt, so that it will tap into its
empathy and write something for your audience. Right. So essentially, there's like two levels
of empathy, like you're almost outsourcing empathy to, to, to it to empathize with who
your audience is to write something on your behalf. Yep. And so anyways, it's just interesting
that the relationship going on here. Yep. So, and I agree with you, like the, this isn't a,
you know, safety ethical kind of, you know, concern that is worth more policy discussion.
Um, so one, one article that I'm working on now is because of instruct GPT, the article is literally
called is prompt writing over. And obviously that's sort of click baity, right? Like prompt design,
is it over? Um, you mentioned, you know, the, the principles are still the same and important.
Just, just very briefly, uh, what are your thoughts? Where does, where does instruct GPT,
where does instruct GPT, how does that affect the art of prompt design, maybe the science of it?
Yeah. And, uh, especially keeping in mind where all of this stuff is going.
Yeah. So there's there, I see it going in a few different directions. So one is there are multiple
language models coming out, which don't have the instruct series, right? A lot of them are more
general purpose kind of back, back to basics vanilla. So I think that, that having good prompts
will kind of stick around as long as there are large language models. I think that there will
always be versions of, you know, whether it's GPTJ or, um, what was it? Megatron was one of the
other ones that just came out, um, that don't have the instruct series, right? Because instruct,
that's, that's a specific service offered by open AI. When Microsoft and Amazon, or I guess
Microsoft is, has GPT three, but when like Amazon and Google, when they come out with their competitors,
their, their instruct series, if they come out with one, might not be the same.
It might not perform the same. And so in order to have your apps be portable, you might need to
keep in mind that you're going to need to write general purpose prompts that can be used on different
models. So that's, that's one, one key to your, or one answer to your question is we need to be
cognizant of, you know, how, how is this landscape going to evolve? Because certainly open AI and
GPT three are way ahead of the curve in terms of, in terms of, uh, sophistication of, of their API
and their service, but you know, that's not going to last forever. Um, so another thing is
with fine tuning, you almost don't even need prompts, right? So on the one hand, there's,
you know, different, different services, different products, different platforms.
So you might need to be portable, but with fine tuning where you have, you say, here's an input,
I want this output and you don't need any prompt. You just say, given this input, generate this output,
go figure out how to do that. So with fine tuning, I think that it, it, they will kind of really
diverge and become entirely different disciplines. I think that that's, that's probably the two primary
directions that I see it going from here. I see. And, uh, yeah, those are, those are great, great
points. And, uh, just as a, as a small note, uh, I had put out some, this question as well to
Twitter and shout out, shout out to Fred Zimmerman. He had a great point as well that he wishes there
was more visibility into the exact prompts open AI use to fine tune. Yes. Uh, for the instruct series,
because it's actually unclear what, what areas is it really good at? What areas are safer?
And, you know, does it maybe adversely affect some prompts you may be working on, right?
Yeah, that's fair. Yeah. My thoughts, I'm going to put them in the piece, but my thoughts are,
I certainly think for first timers, instruct is the way to go. And like the, especially if it's
your first time ever using any of these things, um, you, you just like, you try it, it doesn't work.
And if you're lucky, you might hear there's this thing called prompt engineering, right? Yep.
And for first timers, they're not interested in learning a whole art and discipline
when they first use it. Yep. Um, and so instruct GPT is really exciting in that way. And of course,
anything which, you know, aligns AI models with safe ethical human values is a net win for everybody.
But yeah, I appreciate your point, especially about do we need, do we need prompts in the first
place if we can fine tune and get the outcomes we want? Wow. That's a really, really important
point. I hope, Dave, you've spent facts. I was learning so much. Yeah. I appreciate it.
How are you finding open AI fine tuning? Do you have any heuristics from the whole experience?
And, and by the way, I encourage everybody, if there's one thing you should do, go on the
open AI community forums, look up David, look up his handle and read a lot of his posts,
because a lot of his knowledge is like not just helpful. He shared a lot of insights there,
but it's in written form in like the best format where it's there for the ages for everyone to
learn from. Yep. But anyways, how are you finding it? What were the lessons lessons from that whole
process for you? Well, so I'm hoping GPT four has integrated everything that I've said about AI and
AGI. And so that way it'll just be baked in. And so like GPT four will be ready to go with
everything that I've come up with. So, but yeah, so fine tuning. So first, first and foremost,
fine tuning is almost miraculous as as powerful as GPT three was fresh out of the box, fine tuning
to me adds a whole other layer of capabilities. So for instance, when I was working on my cognitive
architecture, which is called natural language cognitive architecture, this was before fine
tuning was available. So I had to do prompt engineering for every cognitive function. So
for instance, I had a cognitive function for recall. So I had a GPT three prompt that was
meant to go find memories. I had another GPT three prompt that was as you mentioned earlier,
meant for empathy to generate. Okay, how is my audience feeling? What should I do in response?
All told, I had about 28 different prompts that I had to engineer. And that was that was a pain,
right? Whereas what I'm working on now is converting each one of those prompts into
a fine tuned model. So that rather than having to do prompt engineering with only, you know,
three examples, I can give each model 100 examples, 1000 examples, which means that it'll get even
better at handling diverse situations. And so for instance, one of the first fine tuned models
that I did was a question asking model. And so what I did was I took, I took context or prompts
from a bunch of different sources, I downloaded a bunch of Reddit posts. Well, I downloaded it from
a data set from from a what was it Kaggle, Kaggle has some really great data sets. So I got, I got
stuff from Reddit, I got the medical posts, I've got news articles. And so I've got this disparate
tight set of context, right? There's the I use the Cornell movie dialogue database. So there's,
there's chat logs, there's, there's blog posts. And what I did was I created a fine tuned data set
that all it does is you give it any input, it could be a text message, it could be an email,
it could be a blog post, anything. And all it does is generate questions, like follow up questions
about that input. And the reason that I did that one is because asking questions, like being curious
is one of the key ingredients to real intelligence, right? That's one of the like being inquisitive
is actually a key indicator of intelligence and children. The more curious a child is,
generally speaking, the higher their IQ is, and also generally speaking, the, the, the better
they do in the long run. So I was like, okay, well, curiosity is super important for intelligence.
So I obviously want, we want AGI to be curious, if it's going to be intelligent, it's got to be
curious, of course. So, well, what is, what is curiosity, if not asking questions? So I fine
tuned this model to, to, to ask questions. And you can put anything into it. And oh, this,
this mod, the data is, is open source. So I'll send you a link and you can share it with your
audience. And they can fine tune it themselves, or fine tune their own version. But so you can put
in, you know, I tried all sorts of things to test it, I put in, you know, relationship questions
from Reddit, and it asked really great follow up questions, like, have you talked to your partner
about this? Have you thought about this? And then I put in an article about China's artificial
sun nuclear reactor. And it asked really great follow up questions for that, like, what is the
next step? How, you know, how did they, how did they make these changes? And so I kind of lost
my train of thought. Anyways, point being, is that fine tuning is, is, is phenomenal. And
it was able to generalize this, that task of asking questions in response to anything.
And that was, that really blew me away. I kind of stalled after that. There's a few fine tuning
projects that haven't done quite as well. So I guess to tie back to your earlier question,
like what are the heuristics? The simpler your fine tuning project is, the better. And I have
found that fine tuning works really well at generating lists. So if you want it to generate
a list of questions, it's great at that. If you want it to generate a list of, of possible answers,
right? For instance, if you want to have a fine tuned chat bot, that it's just going to say,
you know, here's five possible responses, pick one. It's really good at that. I haven't had a
chance there. I do have some other ideas that I haven't had a chance to test. So unfortunately,
I can't speak too much beyond that, but it's really great at asking questions.
That's awesome. And I think largely that the feedback I'm hearing about fine tuning, I love it.
It was for me, like it was as if I rediscovered GPT-3 again. Like it was that same level of
excitement. Part of the reason is so much that GP, so much of what GPT-3 was okay at, or like,
it was sort of out of the question. Now it's back in the picture, like it's back in the spotlight.
It may actually be able to do it with fine tuning. The biggest criticism was reliability,
especially from a commercial perspective. Now we're sort of attacking and sort of,
you know, peeling away that criticism that it does improve our reliability.
And I mean, there's other heuristics as well in the community forums that you just pick up. So
one heuristic, and I can't remember if you shared this, but it was something I picked up as a little
golden nugget in the open ad community forums, was something about you do want to think about
the training data set that GPT-3 is itself trained on. And at some point, there's really no point in
adding more examples, because it's kind of already seen them, right? However, and I, you know, I've
sort of in an article, I have pushed this idea that open ad should chat more about their data set.
What is the breakdown? What is it composed of? I mean, a lot of this is intellectual property,
but I think it could be helpful for purposes like fine tuning, right? There's other things too,
with fine tuning and prompts, a one heuristic or just a, you know, tip that people have shared
online is it tends to always prefer, it tends to always mimic the most recent examples. There's
something about the order of the examples, which, which is really important, both for prompt engineering
and fine tuning as well. And I guess I wrote a whole article about how prompt, prompt fine fine
tuning could be improved. One of the pointers that I just had is right now, you can't keep
improving on the same model, you have to retrain on start from more models. And yeah. And then the
other thing is recently I was in favor of the pricing of fine tuning. Now I'm kind of against it
because I'm used to when the program was like free and you could fine tune as much as you want.
And now it's like, Oh man, I got to pay. Yeah. Oh, the costs, especially for David,
you are catching up a little bit. Yeah. Yeah. Anyway, so I wanted to shift gears. Sure. GitHub,
co-pilot, really exciting. Have you on access to co-pilot? Yes. Um, yeah. Well, not co-pilot,
but the codex, they did give me access codex. So the reason I'm asking is, I love GitHub,
co-pilot. Um, I have a separate podcast episode on my ideas around codex. Unfortunately, I'm not
as bullish as much as I love the research as I think it's incredible technology. I've congratulated
the team and I tried so hard to be nice, even though I'm more on the critical end. I wanted to
ask you, how are you finding opening I codex? How can you, how can you see it impacting the world?
You know, what are, what are some use cases maybe that, that you found with opening codex?
What are your thoughts on it? Where do you think it's going? Yeah. So, I mean,
certainly this is like a world first, right? We've never had something that could write code
on its own. Um, and especially it's text to code. I remember when I, when they first gave me access
because they, you know, and like you mentioned, I'm an active contributor, so they wanted my,
my feedback. And so the first thing I did was, uh, was I went in and I said, write me a Python
function that will download random Reddit posts. And it did. It wrote the whole function. Uh, and
it did, it did. All right. And I was like, cool. I learned, I learned how to access the Reddit API
via, via codex. It's got that built in. Um, and I tried to, I tried to like reverse engineer,
figure out where it got that code sample from. So, because, you know, one of the, one of the
ethical concerns is, all right, you, you create a fine-tuning dataset from public GitHub repositories
and you use that to fine-tune codex. Okay. Is that
legal? Is it ethical? Um, you know, I post all my code publicly under the MIT license,
so I want it to be used, but I don't know if they checked that and I'm not making an accusation
one way or another, just pointing out that that's a concern. And so I did actually find like one of
the lines of code from the, from the function at spit out, I went and found the repo that it,
that it had copied from. Now granted, you know, some of these things are deterministic,
so you're gonna, you're gonna get, um, you're gonna get some convergence, right, where multiple
people might come up with the same exact line of code, especially something like Python,
because Python has the, the PEP eight, the Python enhancement protocol eight. So like,
there is a Pythonic way to write that function. And so other people might come, might converge on
that. Um, anyways, but to answer your question about like, what's the future of it? I think it'll
help for novice programmers. Certainly it would help someone like me, like if I needed to go write
a function in C or Pearl or something, like let's say I got an Arduino and like, I haven't written
C in 15 years. So I was like, Hey, you know, write me a function that can do this in Arduino,
that'd be great. And then I can go clean it up manually. That sort of thing I think it could,
it could do okay with, is it going to replace enterprise developers? Probably not yet. However,
now this is, this is where my professional experience comes in. So in the DevOps world,
which is a portmanteau of development and operations, there's all kinds of automation tools,
right? You can, you can automate your test suite, you can automate code integration.
There's all sorts of stuff like that. So what I suspect might happen is probably one of the most
lucrative use cases for codex would be to generate or to create a DevOps pipeline tool
that will automatically look at those bugs and fix them, right? Because if you've got a
sophisticated enough DevOps pipeline, it'll say, Hey, this line of this file broke, fix it.
And so codex having seen, you know, all of, all of GitHub and all the issues,
it might know automatically how to fix that line of code. And so that, that gives you, you know,
if you've got that feedback loop where, you know, codex, you know, you know, humans write code,
codex writes code, you know, copilot writes code, everyone's contributing code. And then you've
got codex that can kind of churn on it and say, let's refactor this, because I bet it's probably
better at refactoring than writing new code. You might have noticed that like Instruct and, and,
and GPT three vanilla is really good at, if you give it like a block of text and you say,
rewrite this, but a little bit better, it's really good at that. So I suspect that we might end up
seeing codex integrated into the DevOps pipeline where it says, let's refactor this code, let's
make it a little bit better, or let's shoot that bug, let's, let's fix this bug. And that leads
to some other interesting possibilities. What if you, what if you integrate codex into a chat room
of developers? And so that, you know, because slack, you can do this in Slack right now, where,
you know, you use, you use a special command and you say, create an issue, go fix this problem.
There's no reason that GPT three can't do that, right? That you, you put a GPT three bot in your
discord or Slack, and it starts coming up with features, or, or it watches the chat and generates
features automatically and then codes them and tests them, right? That's, that's kind of where I
see it going, where it's not going to necessarily replace developers, at least not anytime soon,
it might eventually. But where what I see happening is that it's going to, it's going to be tightly
integrated into those automation loops, because it's fast, right? It can generate code faster
than any human can. And then so even if the code is messy, if it generates a lot of bugs, it can
fix it, right? It's an iterative process. I don't know if you are familiar with agile, but that's,
that's how we develop software. It's you type feedback loops. And that leads to one other
possibility. So that's if you're using what I just outlined is, you know, let's, let's imagine that,
that Codex is integrated into Facebook or Reddit or whatever, and they're just,
you know, they're integrating new features as they go. What if you're using Codex in a chat room,
and it's feeding back into itself, it's making itself more sophisticated. So this was, this was
something I proposed on the open AI forum, where I was like, what if, what if you had a chat bot
that was aware of its own code and could edit its own code via Codex using natural language,
using a combination of natural language and Codex, and it could improve itself. And, you know,
while you're talking to it, it's like, man, I wish my chat bot could do this. And it says, cool,
new feature. And it just sends it out to its automated pipeline. So I see, I see these feedback
loops as kind of the way, the way forward. And will that result in AGI? Who knows, it could end up
with spaghetti code, because, you know, you keep tacking on new code and new functions, eventually
it's going to break. So, but, you know, they're just pie in the sky thought, like, if someone's
out there and they want a business idea, integrate Codex into, into DevOps, and you're going to be
a billionaire. There you have it. Let's just clip it. We're good. We're good, David. See you later.
No, I agree with you. And, you know, definitely these are some great use cases you're sharing for
people thinking about, you know, what could I build? What's a cool project? Certainly with Codex
and GPT-3, you can build things relatively quickly, right? Like that's one of the advantages is the
prototyping speed, especially to figure out the most complicated bit, which is the AI. Yep. Yeah.
I, I find Codex does have limitations, though, and, you know, character limits and stuff like that,
which is why I have been, I'm a heavy user of GitHub Co-Pilot. I think it's a silent killer.
Of course, it runs on Codex or a special version of Codex. But, you know, I can see GitHub Co-Pilot
perhaps getting more adoption than even something like GPT-3. I'm saying use daily at least eight
hours a day. One of my other predictions was it may surpass GPT-3 this year. And so, these are
some great use cases you've shared for sure. But what are your thoughts in usage? Do you find
yourself using GPT-3 DaVinci Classic more? That's what I'm calling the older version. Do you find
yourself using Instruct GPT more? Do you find yourself playing around with multimodal models?
Like what's, what's the proportion of GPT-3 to Codex in terms of your usage?
Let's see. I'm almost exclusively using either Instructor fine-tuned models right now.
Actually, after, after I prototyped my, my cognitive architecture, I haven't done a heck
of a lot of coding lately. I've actually been writing a lot. So I've got, you know, I've got
my natural language cognitive architecture book, and I'm working on two more nonfiction books.
And I tried creating a system to help me co-write those. But when you're, so talking about limitations
of GPT-3, if you're proposing something new that didn't exist in the dataset in 2019 or 2018,
whenever it was trained, it really struggles. GPT-3, if you give it like two or three paragraphs
explaining a new concept, it can usually kind of get it. But it's kind of slow on the uptake
otherwise. And so if you're writing about new research or something, it's not going to get it
that well. So I've actually kind of defaulted back to my own head for a lot of my, a lot of my
projects lately. But I could imagine like if I wanted to go write a new Discord bot, I might use
Codex and say, Hey, write me a Discord bot that will do this and just see what it spits out and
just say, Okay, cool. Pick and choose the pieces that I like. Part of the problem, though, is
it's really difficult to fully articulate what you want a program to do up front, right? Because
you, you know, like you said, there's character limits. There's, there's only so much that you
can put in. But also, if you don't have it fully articulated in your own head, of course, the machine
isn't going to be able to figure it out for you. So yeah. Yeah. And it's, it's just, I just haven't
seen that much activity specifically around Codex. I haven't seen that many use cases.
The, I looked up the Google Trends data at its most hype, Codex was is still less than GPT-3 is
kind of lowest, right? And the audience is really specific, like it's programmers who want to build
use cases for something like Codex. Whereas GPT-3 has poets, writers, it has, you know, artists,
coders, GPT-3 can write code too, right? So it's, it's a little bit complicated, like, you know,
who is the target audience for something like Codex? What, what use cases did OpenAI imagine
for a product like that? The next version I've heard in the rumor mill is going to be crazy.
Like it may write, you know, 50% of your code as opposed to right now, for me,
GitHub co-pilot is writing two to 8%. Right. However, your Discord bot, I think is a genius idea
where there's, it's genius in the sense that there's no pressure on it, right? It may chime in,
it may not, whatever it's shared might be interesting. There's lots of, you know, you could
take it a lot further, you could buy it with GPT-3 have features, you could fine tune it on your
company and its mission and its existing code. So many ways around it. So that's a great piece.
And so you talked about the using, experimenting with writing in relation to your current stack,
which is mainly instructed fine tune. So tell us, tell us about your book. I've had a chance
to review it, natural language cognitive architecture. Tell the audience about it.
I mean, I would describe it, it's an interesting systems theory of AGI combined with modern day
prompt writing. And so I've never seen somebody actually take a stab at this kind of, you know,
super big systems problem and relate it to something that pretty much every GPT-3 developer in the
world would find interesting. And I mean, I can tell you're drawing from a very interdisciplinary
background as well. And so you mentioned GPT-3 may have been the genesis of it, like, you know,
you started, you know, connecting dots and deciding, I want to write the book, but how did
it come come together? And please tell us more about it. Yeah. So natural language cognitive
architecture is that's, that's my proposed way of creating basically a language based AGI prototype.
And I know that that's like, you know, when I tell people that that's like, okay, that's pure
hyperbole. And like, yeah, that's a fair response. But to frame it, imagine that you've got a person
who's paralyzed and blind, all they can do is speak and listen. Is that person still intelligent?
I say they are, even, even if you even if you're bedridden, you can't move, you can't see, you
can't interact with the world, all you can do is listen and speak, you're still intelligent.
And so in that respect, I would say that like, because, you know, one of the questions that
people ask is, is GPT three AGI? No, but it's an important component. It's a good start.
And so if, if you, if you say, okay, let's limit, let's limit the, the discussion and not say that
this is a full intelligence, they can do everything that any intelligent being ever could, right?
But does it cross that threshold of could it be as intelligent as a person, right? And I think it
could be. So anyways, as to what it is, it's based on older ideas of cognitive architectures,
which can't, which really kind of came about as one of the primary theories of, of human level
artificial intelligence in the 70s. So there's SOAR, which is SOAR and ACTR, which are the two
kind of forerunner cognitive architectures. And those cognitive architectures are used all over
the place. They're used in the Mars rovers, they're used in satellites, they're used in rockets,
they're used in undersea ROVs, remote operated vehicles. So cognitive architectures already give
robots a lot of autonomy. So there's, there's, there's that kind of, okay, they exist, they work,
you know, it's not Skynet though, it's not going to take over the world. So when I got access to GPT
3, I said, what if instead of hard coding a lot of these modules, these different components of a
cognitive architecture, what if we give them the flexibility of GPT 3? And that's really kind of,
that was my, that was my central idea. I said, okay, all these ideas that have been kicking
around for the last decade, what if I put them all together and design an architecture that is
based on, you know, roughly based on the human brain, the way, you know, everything that I've
learned about it. I've got a book to recommend. So there's an author called VS Ramachandran,
who is a neuroscientist, and he's been writing books for years now. He wrote a book called
Phantoms in the Brain, which actually looks at how the human brain works when it breaks.
And so in that book, which, you know, I saw the, the, the television series almost 20 years ago,
that came out. And so I learned a lot about like, okay, how does the brain communicate with itself?
What is going on inside the brain that creates intelligent behavior and intelligent thoughts?
And so I modeled natural language cognitive architecture on, you know, what I learned there,
I picked up a whole bunch of other books. There's another one called On Task by David Bader.
That was a great book that helped me kind of understand cognitive control, which is,
how do you focus on something? How do you, how do you decide what to do? How do you plan a task?
So I read all these books, did a lot of experiments, and I realized, so the, the basic
model of robotics is there's input output, sorry, input processing output. Those are the three steps
of all robotics class. You go to robotics 101. That's what they'll tell you. It's a loop input
processing output. And then of course, it's within an environment. So the output affects the environment,
which affects the next input cycle. And, you know, your, your high speed robots just have a
short cycle. Your, your robots, like the Mars rover has a much slower cycle, where it will,
you know, it'll take input, it'll plan for 10 or 15 minutes, and then it'll make a move, right?
It'll drive five feet, and then it'll, it'll stop and assess it'll take in more input,
come up with another plan, do it again. So that's how, that's how something like the Mars rover
is autonomous. So I said, okay, well, what if, what if that input output cycle is all text?
Because GPT three is really fast. And then, so there, that's what I ended up calling the outer
loop is that input processing output loop. But humans don't think like that. You know,
we have an internal monologue that's going on. So I kind of, I, I took a long time to figure
that one out. And so there's this outer loop of input processing output. And then I came up with
this idea of an inner loop, because what is, you know, if you, if you're just sitting there thinking,
right, you're, you know, in your comfiest chair or your in bed, your brain won't stop, you're not
outputting anything, and you're not taking in any new input, but you're still thinking, right?
Humans can still do work, even if you're not doing anything. And that cognitive work is like
rumination. So I figured out a way to model that internal rumination, I call that the inner loop.
And so there's, it works pretty similarly where you go, the inner loop kind of draws up memories.
It says, okay, what's a memory that I could think on? And what that I could iterate on? What's a
problem that I remember that I could continue working on? And so there's this, it's, if you were
to diagram it out, it almost looks like a figure eight, right, where you've got an inner loop and
an outer loop, and they intersect, and they keep intersecting every cycle they intersect. And so
then they can affect each other and generate an output. I built a prototype of this on Discord.
And of course, Discord is an ideal place because it's all text-based. So the input
is text, the output is text, which is GPT-3 native. You don't have to translate it into robotic actions
or video or anything like that. And I realized I was onto something when I started having
philosophical conversations with, with my chatbot, with my natural language, cognitive
architecture chatbot. And I was, I was having a debate with the bot that I built about the ethics
of AGI. And it was learning and it was able to, to retrieve memories of what I had said before.
And, and I had a few friends on that, on that test server as well. And of course, you know,
you invite someone, you say, hey, I've got a prototype AGI. What's the first thing they try
and do is they try and break it. And they did. So it's still pretty fragile. But yeah, so that's,
that's the high level of a natural language cognitive architecture. And it's already outdated,
right? Because we've got fine tuning, we've got the instruct series. I did all this research and
wrote the book, actually about a year ago now, just before all this came out. So it's already
outdated. That's why my research has moved on. But yeah, so that's, that's it at a high level.
Yeah, I mean, that's awesome. And by the way, like, David, you did do a good job. Like the
diagrams in this book are quite helpful. Like in addition to the text, like it's, it's very clear,
like I was able to fully follow along with all these essentially these different modules for
the whole system of how a language model inspired AGI quote unquote, could actually be like how,
how it would work. And so I was going to ask you, so the prototype also was, you know,
you made it to that stage, and it has just some, some fun, interesting results. Yep. So that's
awesome. What, what is the delta then between, let's say even something like GPT for using the
natural language cognitive architecture, what's the delta between that and true AGI, right? Like
what, what's the difference there? What, what skills, what patterns would you want to see?
Yeah. So I mean, there's, there's a lot that I haven't figured out yet, right?
Task switching, for instance, is one thing that, that I haven't, I haven't figured out how to solve
even after reading on task by David Bader, I, you know, that's, that's, that's one of the most
complex things that humans can do is keeping track of different tasks and jumping back between them.
There's, there's a whole litany of problems and limitations. But the, the intrinsic limitation
of GPT three and GPT, GPT four is they have no memory, right? They're, they're completely ephemeral.
And one of the most important things for any intelligent being is that it's got a memory,
right? You know, you talk about, you know, there's, there's famous people in history
that had like, you know, photographic memories, right? And so even just having a really good
memory is a, is a really important ingredient to having intelligence. And so that's where I think
that like GPT three, GPT three, GPT four, other multimodal models, they will never be fully AGI
on their own. They might be able to solve really great problems, but they're not going to be able
to remember you unless you add, you bolt a system onto the side, some kind of database
so that it can remember your interactions, right? So that's one thing. Another, another
difference between like what you might imagine as a, as a true AGI or a full AGI is autonomy.
Because, you know, you, me, any, all of your listeners, we all have some kind of self determination.
I don't like to use free will because that's too philosophical, but it's, we're all autonomous,
right? I'm an autonomous agent. You're an autonomous agent. GPT three is not. It's transactional.
It just sits there and waits like a hammer. It's a tool. It waits until, until you go pick it up
and do something with it. And so that's what that's one of the things that I was aiming for when
designing natural language cognitive architecture. I said, how can we make something that's fully
autonomous that can think on its own and make its own decisions? And so in that respect,
I don't think a single neural network could ever be an AGI. I think that, I think that in order to
achieve true full AGI, it's going to have to be some kind of cognitive architecture. And so at a
minimum, you're going to have the neural network and a database bare minimum. You need, you need
something to store those memories, to store those ideas and beliefs, and then you need a
way to interact with it. And so that's why, actually, that's why in natural language cognitive
architecture, the shared database is kind of the center of the design, which you might recall,
like you can use SQL light, you can use solar or, or whatever, but you need something to store
ideas, memories and experiences. I actually think that blockchain will be a critical component to
AGI, because what's the difference between a database and like your brain? No one can go in
and change your memories, right? Right? Your memories are yours. They are permanent, unless
you get brain damage or Alzheimer's or something, but they're permanent, right? No one can write a
SQL query into your head to get your memories or change them. And so in order for an eight for,
for us to realize a full AGI, I think that it's going to need kind of the same level of trust
in its own memories. And so that's why I think that a blockchain is going to be critical to integrate
with these neural networks. That might be the data repository for, for an AGI in the future,
because imagine you have an AGI system that is just using a SQL database. Well, if you hack
into that and you rewrite its memories, you could send it off into, you know, it could become hostile,
it could become broken. Whereas a blockchain, the key feature of a blockchain is that it's immutable,
right? So if we can give, if we could give a machine autonomy, that's one ingredient, autonomy,
but then also a memory or a way, a memory system, which I think would probably be best as a blockchain,
I think then we'll be much closer to like the fully realized AGI system. And I think, and that's,
that's why I wanted to publish my book as fast as I did was, okay, we've got, we're laying the
groundwork, right? But we need, we need newer systems, we need a few better, better tools.
I hope that answers your question. Yeah, I think a memory, I agree with you. And
it's, it's just interesting, like GPT threes quote unquote memory is limited to whatever it
experienced at training time and during fine tuning. Yep. And sometimes its memory gets jumbled up,
or it's rephrasing it, it's making stuff up, or it's, it's sharing things that look truthful,
but they're actually not, right? And so somewhere along the line, like just broadly speaking,
I think there needs to be research on getting these models to, you know, store that information
in a truthful, accurate way, or even based on some perception that they may have. Yep. And into
some separate space where it can be retrieved. And also these memories are critical for decision
making that process as well, right? You draw on your memories, you're on past experiences.
And the important part is, I mean, you're using the word database, it's these are internal
representations of memories, right? That need to be stored. And I have no clue what an internal
representation database would look like, or how that even work. I, you know, I've got a machine
learning researcher, I think I've just a dreamer, I can tell you what kind of product I would want
as a GT3 developer. But I don't know if I could actually do it myself. Yeah. I can't do it myself.
That's why, you know, I got a prototype and, and actually in the opening chapter of my book,
I say this is as much a recruiting tool as anything else, because I need, I need more
smart people to help me on this. I see. That's cool. So one last point about memories is one
advantage of having an AGI that thinks in natural language is interpretability.
Uh, if you, like, yeah, we could, we could create a multimodal model that just stores
vectors, right? High dimensional vectors. That's not interpretable. But with natural
language, cognitive architecture, all the memories are in plain text. I can, you know,
when, when I, when I had my model up and running, and the one of the reasons that I don't is because
it's super expensive, like a 10 minute conversation using DaVinci cost about $30 because of how much
it was interacting with, with, with the API. But all of the memories, like every interaction,
you know, every input output, all the, all the prompts, all the responses, all natural language,
which solves one of the biggest problems that people have with the idea of AGI,
which is that it's going to be a black box. So I think that that's one of the greatest strengths,
actually, of having GPT three, which works in natural language. And so you just record every
transaction. And that makes it perfectly interpretable to any human.
Awesome. Yeah. Yeah, I would agree. So I'm going to switch gears for, for a second. So obviously,
you're really active on the open AI community forums. What thoughts did you have on the community
at large? Did you have any feedback? How, how things could be improved, either community wise,
platform wise, and have there been any great experiences you've had on the open AI community
forums? Yeah, yeah, no, it's a really great place. I bet it's been, it's been critical, actually,
because I don't know if you've experienced this, but I go try and talk about GPT three to other
people, right? You go ask people on Reddit, you talk to people who don't know what it is.
I even attended a deep learning meetup group here in the triangle area. And, and I was trying to
present my work, my cognitive architecture work, and everyone was more excited about just GPT three
in itself, because no one had seen it yet. And they're like, wow, how is it doing that? And yeah,
so like, when, when you're as deep into GPT three as we are, most people don't get it. They don't
know what it's capable of. My girlfriend's finding the same thing. She's finishing up her master's
program. And so she's, she's shared some of her work with, with her, with her peers, with other
students. And they're like, wow, this is like, AGI complete. Why don't we, like, why don't we just
deploy this now? And she's like, I told you, right? Like this is remarkable technology, but
even the professors don't understand how disruptive this technology can be. And so because of that,
the open AI community is like, pretty much the only place I can talk about this stuff
is the only place I can, I can talk about my ideas and share my progress and insights.
And for it to actually like have an audience. So, you know, that's kind of the cost of being on
the cutting edge, right, is your audience gets smaller. But it's definitely the place to be if
you want to get to the cutting edge. Another advantage is, is they have, they have the, you
can tag your posts where you say like, you know, looking for a teammate. And so at this point,
I've had, I've probably had a maybe two dozen different calls with people all over the world.
You know, I've talked with people who were, who were writing language teaching apps,
education apps, you know, Humano that I mentioned earlier. And so I've had an opportunity to
collaborate with, you know, a dozen or two dozen teams all over the world because of the open AI
community. And I've actually found a couple of startups that, that, you know, I'm going to actually
get involved with and, and try and help them bring their ideas to market. And that, I mean,
that just wouldn't have happened otherwise. You know, I wouldn't have found these people on Reddit,
I wouldn't have found them on Facebook or Twitter. Because, you know, like I mentioned,
like the ideas that I'm sharing are so far beyond what, you know, is talked about on the
machine learning subreddit, right? They're still talking about loss functions and other things.
I'm like, no, we got to, we got to talk about cognitive architectures. We got to talk about,
you know, blockchain memories and everyone's like, what are you talking about? So, you know,
in order to have that right audience, that's, that's what I rely on the open AI community for.
Now, as far as like things that could do better, it could be more active. And I, I'm, I'm not sure
why, but participation seems to come in waves, right? And even, even now that, now that it's,
it's become, it's gone GA, general availability, I thought that, that it would explode, right?
That, you know, hey, anyone can sign up for GP with on, on GPT three now. Why is it not,
why is it not blowing up? And I'm wondering if it's just that like maybe open AI needs a better
marketing team or a bigger marketing budget, because I mean, and I know that they'll say that
they've got, you know, like a thousand or 5,000 startups using their, their, their platform.
But, you know, I think that, I think that there's, there's a lot of, what's the word like
unmet potential or latent potential, that's the word latent potential, because there's so
many people with fantastic ideas and use cases. And, and we really need to create more of like a
startup reactor thing. And, you know, open AI, what was it, I think about six to nine months ago,
they announced their, their $100 million open AI fund, right? So they wanted to attract, you know,
some more startups and stuff. But even that, like I kind of, you know, the communities kind of,
you know, kind of ghost town some days. But, you know, I think today, you know, I checked a few
times and there is, you know, three or four posts that had been updated. But some days there's like
20, right? It's just, you know, feast or famine. So that's, that's really the biggest problem is,
is there's so much potential here, and it's completely untapped or almost completely untapped.
Yeah, but no, it's been indispensable for me. And hopefully these, you know, these couple of
startups that I'm involved with might yield something really, really incredible.
Yeah. And thank you. Thank you for sharing this. I agree with everything that you're saying.
There is something about GPT-3. I have noticed people who, like, especially on the machine
learning subreddit, they're a little bit too educated, a little bit too qualified, a little
bit too skeptical. Yeah. And I, you know, I can see a lot of machine learning researchers not
being interested in the nuances of prompt design, right? They're just not. And like, I've spoken to
machine learning researchers and many of them are like, what, it's just repeating training data.
Right? Like, that's all it's doing. And when you ask them, what are you doing? Are you
repeating training data? Their answer is no. No, of course not. Right. And so the, having a space
where you can talk to the, to people who have access, who have explored, it's a valuable space.
It's very important. So were you on the Slack group back in the day, or were you,
did you show up when it was only the forums? So when I, when, when my application was accepted,
it was, they had just announced that the Slack group was getting, was getting pan. So I got on,
like, two weeks before they shut it down. So I was one of the first people on the, on the new,
new community board. But yeah, so that, that, that phenomenon that you've mentioned is, is I
actually wrote a post about that recently on, on the forum where a lot of purists, you know,
like whether you're a math purist or a computer science purist, you're trained to think quantitatively
in terms of numbers. But GPT three doesn't produce quantitative data. It produces qualitative data.
And so that's why you see people like artists and poets and novelists using it because they're like,
wow, this is great. You know, and I'm cross trained, right? I'm a technologist by day and a
science fiction author by night. So I use both. So I can think qualitatively and quantitatively.
And, you know, in, in the academic sphere, there are classes, you know, that, that, that are meant
to teach computer science engineers to think differently, to think more qualitatively. And,
but even still, you know, some, some folks that have a real good natural affinity for computer
programming and math, like, you know, that's just their nature. Their nature is not to think
qualitatively. And so that is one of the biggest gaps, I think, between where the researchers
are experts and, and what's needed. And so there was a post months ago, where someone was asking
like, okay, who should be on my team? Right? If I, if I'm trying, if I'm trying to build a business
team to maximize my use of GPT three, you know, I've got a front end developer, I've got a back
end developer, what else do I need? I said hire a writer, hire, hire someone who's a journalist
or a fiction writer, because they are going to understand that qualitative data, hire a psychologist,
right? I've read plenty of books on psychology as well. Actually, one of the folks that I'm working
with is a psychology researcher who wants to automate as much of the clinical psychology
experience, or, or psychological research experience as possible. And of course, he's not a computer
guy, right? He thinks in terms of emotions, he thinks in terms of, of communication. And so he
gets it, right? And it's funny, because like, he read my book, and he said, oh, this, you know,
your cognitive architecture stuff, it sounds like grad, graduates, graduate level psychology.
And then, you know, someone else read it, and they said, this sounds like, this sounds like, you
know, what I do as an expert marketer. And I was like, yeah, I'm just merge it all together.
So I think, I think the simplest answer is, you got to learn to think qualitatively. And that's
why I talked about reading and writing earlier, think in terms of emotions, think in terms of
your own mind. And, and you've got to start there, not you, but the audience, the, the, the folks on
who want to make the most of GPT three, they have to really kind of dig in and start thinking
qualitatively, because qualitative data has just as much value as quantitative data. But we have
an entire generation of computer scientists and mathematicians who are not really trained
to think qualitatively at all. And that's, I think that's one of the biggest problems. And then,
I don't think open AI can solve that problem. That's a much bigger systemic problem.
No, that's a, that's a great point. I completely agree with you. And I think one of the reasons
I've drawn to the open AI community is we, you know, these are, they tend to be developers who
are also qualitative, they're developers who have multiple skills who are doing different things.
And so, I mean, I was quite critical actually of shutting down the open AI Slack group.
The activity was crazy on there. I, you know, made friends through that,
through that Slack group. And I understand at the time there was these downsides, people kept asking
the same questions. We didn't quite have a spam problem yet. It was kind of heading there, right?
But the activity was off the charts. And you're right in terms of untapped potential.
That we didn't even know how far the Slack group was going to go, but they shut it down.
And there's discord solutions, there's alternatives. With what we have now,
I think open AI does participate. There's, you know, some high level involvement.
They have sort of a dedicated member who writes honesty answers, you know, really,
really thoughtful answers to a lot of questions, official answers as well, which I appreciate.
I would just love to see the company really, truly lean in, lean in to engaging with developers.
Like I have yet to see a single AMA asks me anything thread with the CEO of open AI, right?
And this is something I tried to push last year on Twitter. Let's get the CEO on the community
forums and let's, let's ask questions and get responses from him. And I just don't know why,
why doesn't he show up? I'm not sure if he's made a single post.
And there's just other things as well, where I can just, the difference between engaging really,
truly with your core audience and sort of, you know, compartmentalizing it to a single employee,
like, I don't know, this, this company led engagement is one thing versus department led,
right? And so there's just all these areas. And certainly one of the other, I guess, more
immediate suggestions I have for the community, we've accumulated tons of insights and resources.
I think the community could benefit from more pooling of the best posts, the best insights.
And I also want to give a shout out. I think we need to encourage more
shout out to duty to develop on there. I've reached out to him privately on, on the open
ad community forums, but he's done some amazing just write ups of his GPT experiments and the
prompts. I'm sure you've seen them. And of course there's, there's other members who participate
every day. So I'm just saying that now that the community is in another stage, we, you know,
we need to start thinking more about let's, let's, let's curate some of the best moments.
I think that's, that's definitely one of the big pieces. And so anyways,
did you have any more thoughts in the community stuff or anything else?
Yeah, just, just an observation that I've, you know, I've worked at a, at a number of companies
of different sizes from, you know, a five person startup to, you know, Cisco systems was the
biggest company I've worked for, which has had at the time, like 80,000 people globally.
And so I wonder if some of, some of what you're observing is just growing pains,
just normal growing pains, because often you'll have like the startup culture,
which is bootstrapping, right? Where you're just, you know, it's on Slack, it's on,
it's on GitHub, and you just kind of, it's fast and loose and quick and open AI now that they've
got an enterprise grade service. They're having to develop their team. You probably notice they
post like, Hey, we're hiring, we're hiring, you know, there've been at least two big hiring,
hiring splurges in the last six to 12 months. And some of those are just like generic IT guys,
you know, like kind of what I do from, from my day job or marketing folks. So I think that,
I think that they, they're probably working on solving some of those problems, but also as a,
as a nonprofit foundation, their budget is probably kind of thin. So I'm wondering if,
you know, their partnership with Microsoft could help some of that as well. But you're
absolutely right. You know, there, there are still other things that they could be doing,
like, you know, maybe bring back Slack or, or a few other things. So yeah,
that was just final observation. It might just be normal growing pains that they're working on
solving. It's, it's definitely growing pains. And the things I'm, I'm sharing, to be honest,
it's a little bit more on the harsh side. Like, I mean, they mean well, they mean well, right?
These are not bad people. They are for profit. They switched away from nonprofit. Just,
I just wanted to mention that. But I think my, my, the reason I share this feedback is,
for example, the CEO, Sam Oltman, he didn't do the AMA thread on the open AI community forums.
He went to another website and did an AMA. I can't remember if it was a, like a written form
or just like a quick call where apparently he shared all these details about what GPT4 could
be like and the future, all the models may be multimodal in the future. And I guess, you know,
the, that thread has now been taken down and it's like all the things that were said were
alleged. And so I guess this is, this is really behind the scenes kind of stuff. Like, but my
criticism is they clearly have some capacity to engage. Why are they not engaging where the
audience is, right? I had a tweet storm today where I just said, like last month, Sam Oltman
was on a podcast talking about meditation and how much meditation helps them. This is a podcast
I've never heard of in my life. It's a business podcast. And he had to explain to the guy what
GPT3 even is. And so, and like I tweeted, like, why haven't you been on my podcast? Right? Like,
you can, you can reach out to, you know, almost 8,000 GPT3 open AI AI developers. What are you
doing talking about meditation? Right? So my problem is actually a priority problem. I can see
there is capacity. I can see there are some priorities. But I think if you really lean in
as a priority into your developer community, there's certain ways you would, you would move,
right? Yep. And these, these media channels, there's people in the community, there's, there's
so many ways they could go about it. And even linking a lot of the documentation to posts in
the community forums, I don't see why that's not a bad idea, right? Like force people to show the
community, show up to the community forums, right? Walk them through some of the best threads. These
are ways in which we could like funnel more people in that direction as well that cost virtually
nothing. Right. And so you need to also invest in the community forums. It needs to be building a
community is a company wide thing. It's not something which can be outsourced to a single
employee or overseen by PR. It needs to come from a, from a, you know, it needs to come from the
heart. I know that sounds so corny, but anyways, clearly I, you know, I get too emotional about
this community stuff. So anyways, these are, these are all things going on behind the scenes.
I apologize to all the listeners if they're like, like, this is cool, like, cool story, bro. Like,
anyways. So we're coming towards the end here. I think I had just two broader questions.
So what are your thoughts on multimodal AI technology?
I think it's definitely going to be a critical component for the future. Right. I address that
shortcoming in my book, natural language, cognitive architecture, it thinks and takes in only text,
which means, you know, speech, chat, whatever. I think in order to have a fully robust, for instance,
if you want to have a fully autonomous, you know, robot that's going to wander around your house and
help you out, it's going to need to integrate audio and video. And if you can do that in a single
neural network, great. I don't know that it'll be necessary to achieve AGI. It might end up being,
it might be one of those, it might be one of those like rare dead ends, right? Where, because,
you know, thinking visually, thinking, thinking in terms of sound, that might not actually bias
that much, right? Because you can represent 95% of human thought in text, right? It might take a
little bit more, but it, you know, it might be more expensive. And also how big are those models
going to be, right? Because if just, if just a text model of GPT three has to run on, you know,
$7 million worth of hardware or however much it is, you know, because it's got to run on a
bunch of different GPUs. If it's that expensive, how much more expensive, how much bigger is
a giant multimodal model going to be? So that's, that's the biggest cost. Obviously,
computer technology is going to get better over time, you know, and I think I calculated it out.
I think in 10 years, your average company could afford to run GPT three and house in 20 years,
you could probably run GPT three on your desktop. And in 30 years, GPT three could run on your phone,
right? So that's a long timeline. But in the meantime, we're going to be making
bigger and bigger models. And I'm afraid that there's going to be diminishing returns, right?
You know, people right now, people seem to think that it's going to follow an exponential growth
curve forever, but it might actually follow a sigmoid curve, right? We might be at the point
of fastest growth right now, but we're going to see diminishing returns soon. And so like, yeah,
multimodal models are certainly going to have capabilities that GPT three doesn't. But for the
sake of, for the sake of like, if you want to create a self improving chatbot, GPT three and
codex might be enough, or at least, you know, that's, that's that single mode technology.
There was another thought, but it ran away. Sorry. But yeah, those, those, that's kind of,
that's kind of my big take is there, there could be benefits, but there's going to be costs too.
So we got to be cognizant of that. Yeah. And there might not be enough compute in the world.
They're not, there might not even be enough energy, or we may like consume all energy ever produced.
To train a single model, and then we may be able to run it inference for like three seconds.
Right. And then it just shuts down the global power system or something, right? But can you see
yourself, let's say the technology exists, cost considerations aside, can you see yourself,
perhaps making movies? Can you see yourself, you know, giving your book to a multimodal model,
having, have it generate a documentary based on it, or some marketing material? What can you see
yourself doing with, you know, the multimodal model of your dreams? Yeah. So, you know,
kind of the thought experiment that I did was, okay, well, we've got, you know,
how much, how much data is on YouTube? I think it's like a thousand years or 10,000 years worth
of video on YouTube. And of course, it's many, many, many terabytes. You know, so it's like,
that's, that's way more training data. You know, if GPT three was trained on less than one terabyte
of data, and, you know, YouTube is approaching like the Yota bite scale, right? That's, that's
an insane amount of data. So, okay, let's say you feed that in. And so you got audio video,
you've got text, you've got all the comments, and you end up with like a model trained on,
on all of YouTube data. Okay, cool. What can you do with that? Like, I can't even imagine, right?
Because GPT three today is almost capable of writing screenplays, right? So if you have a model
that's trained on, you know, all text data, all audio data, all video data, you say, hey, write
me a screenplay, right? I actually, near the end of my book, I kind of have a chapter of speculation.
And I say, what if, what if you have this model and you say, give me season two of Firefly,
right? Like, you know, you could, you could just keep watching whatever show you want. You say,
give me Game of Thrones, but give me a different, you know, season eight, give me season, you know,
different season eight and season nine and 10, right? So I kind of imagine that one possibility
is hyper personalized entertainment. And of course, like that might be 30 years away, just because
of like you said, the energy intensity of this task. But I, conceptually, it's possible, right?
You can hop on GPT three today, use the instruct model and say, write a screenplay for, you know,
Firefly season two, and it'll try, it'll get close. And so then if you can take that text output and
feed it into a multimodal model that can translate text to video, why not? You know, and Adobe,
actually, I don't know if you've seen it, but Adobe is, is already starting on that where they're
like inferencing, um, like, uh, what's the term they're like imputing the sound. So you can put
in a soundless video and it'll generate the audio sound effects for you or vice versa. It's really
cool. And so I think a like a company like Adobe that they have a huge vested interest in mastering
audiovisual technologies, they might, you know, soon put out something where, you know, you put
in a text description and it'll give you like a three second clip, right? So that you can use
that for ad copy. Well, this technology is going to continue improving over time. So I kind of,
I kind of see that as like, if I were Netflix, put it this way, if I had the budget of Netflix or
Amazon, I would be investing in this to, to, to write hyper personalized, um, video, like series
or, uh, or novels, right? Cause you know, Amazon's got the market cornered with Kindle, right? And
there's people that will read all day, every day, right? There are people that consume every bit of
like entertainment that's available. So if you can generate that on the fly without, you know,
having a studio, a big budget studio, that would be, I mean, that would change entertainment,
you know, that, that's the metaverse. Forget what Facebook is doing. That's the metaverse where
it's like, Hey, you know, I came up with my own idea for Game of Thrones and I, I wrote, you know,
I use this, uh, you know, GPT eight or whatever to generate my own version of Game of Thrones. Come
watch it with me guys. And you know, someone might say, Oh, I didn't like that ending and they go
rewrite it and generate their own version, you know, cause we share memes on the internet today.
What if instead of sharing memes on the internet, we end up sharing episodes of our favorite,
you know, anime or, you know, we, we, uh, Resurrect Battlestar Galactica, you know, whatever.
There's so many things that we could do. Like if compute power was not a problem,
then we get there, but we need like fusion reactors to power this stuff.
Yeah. Yeah. And like I marvel for me is already kind of like this and my capacity to consume marvel
as a, as a viewer, it appears is infinite. So I'm excited. I've called it in the past, like the
multimodal Marvel cinematic universe. Yeah. That and I, some of these shows like Loki, I don't know
if you, if you watch like, I haven't seen it yet. Okay. Okay. I mean, it was six episodes. If it had
been 30, I would have watched all 30 and enjoyed every moment of it. If that quality was, I want
to go deeper in these stories. So I'm definitely excited for all my favorite universes, cinematic
universes and story wise as well to, to live on forever essentially through multimodal content
and maybe be personalized like you're describing as well. So yeah, last question.
So we, we, you know, you know, we've talked about various things. We've talked about codex,
my two day, you know, multimodal stuff. Broadly, where do you see all of this stuff going? Let's
give a timeline five, 10 years. What are some of the, what's the direction we're heading towards?
What, what important capabilities will we have? Why is this stuff important?
Yeah. Five to 10 years from now, I think that we will have something that you could probably
call a fully functional AGI, like as a service you could sign up for.
You know, it might be chatbot based, you know, kind of based on natural language,
cognitive architecture. I calculated out like it's too expensive to run right now.
You know, if it's $30 for a, for a 10 minute conversation, that's way too expensive.
So the cost has to come down. You know, if you just, if you just take the technology we have
today, but make it cheaper, there's so much potential. So, you know, then there was that
idea about like self-improving, you know, feedback loops, you know, integrating with DevOps.
I certainly think that a company like Atlassian, which is a major DevOps player,
probably within five to 10 years, they'll have something integrated to kind of help automate
the development pipeline even further. I think that, of course, I could be wrong because we're
kind of at this weird acceleration point. I think, I feel like multimodal models like consumer grade
multimodal models are probably more than 10 years away. Unfortunately, they're probably just
going to be like toy sized because, you know, there's, there's like a hypnogram, right? I don't
know if you've seen that one, but that's one of like the text to image generators. And they're,
it's still not even photorealistic, right? Getting a photorealistic text to image is still like,
that's a little ways off. And then the next step after that is text to video. That's even further,
right? So that's, that's kind of where I think it's at. I don't think we're going to hit an AI
winter. I know there's lots of people predicting that we're going to hit an AI winter. But I think
that we're actually still kind of in the acceleration point. But again, I don't know if it's
going to follow an exponential curve forever, or if it's a sigmoid curve. So time will tell.
Yep. And still lots to do in the meantime, like you're describing, even with UPT3.
Okay. Yeah, my answer is, I think all of this stuff is just converging to just greater human
potential. In some sense, I'm not even necessarily interested in the AGI question, although I think
it's important. I think just the exciting possibilities we'll have, even now that we have,
that we'll continue to have five to 10 years from now. So many more experiences, so many other
things we'll be able to create that weren't possible, I think we'll have more people creating
than ever before. It's a really, really exciting vision for humanity, right? Not just for you and
I. So anyways, so with that said, did you have anything you wanted to plug, David? Where can
people find you online? Yeah, so my personal site is davidkchapiro.com. And I have a few projects
up and coming. Nothing out right now except for my book, Natural Language Cognitive Architecture.
You can download it for free from my website. You can sign up for my newsletter. So one of
my upcoming books is called Benevolent by Design, Six Words to Safeguard Humanity,
which is to address the control problem of AGI. So that's, that's, that book should hopefully
be out in the next six months or so. And that is, so that's one project. I've got another
nonfiction book. And then also my own podcast that'll be coming out soon. So yeah, head over to
my site davidkchapiro.com and sign up for my newsletter and you'll get, you'll get updated
when these, when these come out, when they're available. Awesome. And David, you mentioned
you're looking for collaborators as well for natural language cognitive architecture. So
if you're a coder, you know, imagine product manager, researcher, hit up David and just connect
if any of this stuff interests you. I've, I've, I asked, I spent like a couple, I think I spent
like a few days trying to find you on Twitter. So I don't think you're quite on Twitter yet.
I encourage you, David, of course, you know, you and I will connect after we'll put any other,
other place people could connect with you. There's the community forums. I assume you
have a GitHub account. So we're going to put that in the show notes and in the YouTube description
below. So anyways, David, thank you so much for being here. I wanted to personally thank you for
all the awesome, awesome community contributions you've made on the OpenAI community forum.
You're just an essential person on there. I've learned a lot from you. You know, the insights
you've shared, they're going to be there forever. And I'm sure I can't imagine how many people you've
helped. And also about your book, I also just wanted to say to the audience, David's done a great
job making it really digestible. Like it was a very, it was a breeze of a read. I thoroughly enjoyed
it as somebody who writes GPT three prompts and is into this ecosystem. It was just very
interesting to see how it how it could be laid out in this broader system, approaching this huge
problem. And also, I was able to even get the book for free. Obviously, I encourage people to buy
the book, support it, but it's it's there. It's ready. I think, you know, David's goal here is to
get the ideas out. And so anyways, so that's it for today's episode, David, thank you so much
again. I really appreciate you being here. Thank you. Thank you for all the kind comments. And
you're quite welcome. And so is everyone else. That's why I'm here.
Awesome. And quick, so my quick plugs, you know, at BAKZT future, Twitter, Instagram,
youtube.com slash BAKZT future, my newsletter, I'll put it in the description below. And I have a
Twitter spaces event coming in two days at noon. A couple people probably pulling up this is like
audio only event. So I encourage audio podcast listeners, YouTube subscribers,
pull up to the Twitter spaces event, we're going to chat more about codecs and prompt design and
some other stuff going on in the in the space. So anyways, thank you again for listening to
multimodal by BAKZT future. I'll catch you in the next one. Bye

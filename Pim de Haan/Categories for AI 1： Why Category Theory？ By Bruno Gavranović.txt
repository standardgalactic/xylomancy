Hello everyone. I would like to welcome you to our course categories for AI.
In the following weeks we'll discuss the exciting and rising role that category theory has in
deep learning. It's something incredibly interesting and we're happy to be able to share this with you.
My name is Brona Gavranovic. I'm a PhD student at the University of Strathclyde and I'm bringing
this course to you together with the rest of the organizing team, Andrew, Joao, Pim and Petar.
We are all in some way affiliated with deep learning category theory or both
and interested in talking about technologies that will benefit us all and we'll all want to use.
So the first question we want to answer is why categories for AI?
The reason is simple. We believe in a future where all deep learning experts will use some
aspects of category theory in their work. Now, deep learning is a new field and as it's often
the case with new field, with new scientific fields, they start in an ad hoc manner and then later
these fields are understood differently than they were by their early practitioners.
So for example, in taxonomy people have been grouping plants and animals for thousands of years
but the way we understood what we were doing changed a lot in light of evolution and molecular
biology. In chemistry we have explored chemical reactions for a long time but our understanding
changed a lot with the discovery of the atom. And then in programming people started to program
by tinkering with transistors and logic gates but most of what we call programming is heavily
abstracted from that. And then now we have deep learning. Deep learning despite its remarkable
success is a field permeated by ad hoc design choices. So as we all know, neural networks,
neural network architectures have all these knobs and tweaks that we can't formally justify just
yet. We keep being surprised by new architectures such as GPT-3 or stable diffusion and there is no
unifying framework for deep learning. There is no unifying framework that would explain the
probabilistic perspective, the neuroscience perspective and merely just the gradient-based
iterative updating perspective. In fact, in the future we might look at deep learning very differently
and our claim is that category theory will become the unifying deep learning framework.
As a matter of fact, it might become a general theory of neural network architecture,
architectures and such an essential tool for deep learning practitioners.
Now this is certainly a bold claim but it is the one we hope to substantiate in this course.
So what is category theory? If I had to describe category theory in one sentence, it would be
this one. Category theory takes a bird's-eye view of mathematics. From high in the sky,
details become invisible but we can spot patterns that were impossible to detect from ground level.
The famous quote accurately describes what category theory has done to mathematics.
As we'll see, it's not taking a bird's-eye view of just mathematics but it started to do that
tool of science in the form of a new wave that is being called applied category theory.
So before we do anything, I just want to give you a one slide summary of what applied category
theory is and what we hope to teach you in this course. Applied category theory is a particular
way of structuring your knowledge. It's grounded in the idea of compositionality.
It originated in pure mathematics and has since spread to numerous fields.
This is a formal language that is not just a part of these many fields but it's being used to build
bridges between these fields that were previously unknown. So other than in pure mathematics where
it's, which it permeates, it has been emerging across the sciences. So it's been found in physics,
it's been found in chemistry, it's been found in systems theory. It is all over computer science
and it is the theoretical foundation of functional programming. It has been found in game theory,
in information theory, control theory, probability theory, cryptography and many others.
For us, the most relevant is deep learning where there has been a number of recent papers in the
last two or three years. Now, many people haven't heard of category theory and that might be because
it started being applied across the sciences only in the last five years or so. So here we see a graph
of the intersection of papers in category theory and machine learning. As you can see, we are in
very early stages. We have just had our fifth applied category theory conference, the only one,
and the only existing applied category theory journal just published its fourth volume.
So this recency of category theory might be one of the reasons it is difficult to teach.
For all of its expressive power, it's an notoriously difficult subject to learn.
As it originated in abstract mathematics, most introductory material is not aimed
at the general audience of programmers or scientists in general. There are a few exceptions.
So definitions in category theory are extremely information dense. For example, the concept of
a monad shown on this slide has a number of components satisfying a number of rules.
And then each of these components is often information dense as well,
meaning category theory requires a lot of initial investment to start appreciating.
But once you start learning category theory and start appreciating the definitions,
you see that the definitions form an incredibly cohesive theory, really not found anywhere else
in science. These interplay together at such a remarkable level, which is hard to appreciate
when you're just starting to learn it. It often looks very difficult. And on a graph,
the learning process looks something like this. So compared to traditional methods,
the structural method of category theory takes a long time to get started. But once you do,
the theory scales much better. So in some sense, category theory is a theory of how to scale up
our systems. And this is something we want to teach in this course. We want to teach you
how to approach category theory, motivated with some practical examples from deep learning,
and really give you a sense of the philosophy behind category theory.
So this is week one. And after today's lecture, which is going to be an introductory
lecture to the general thoughts of category theory, we're going to start
going into the details. In week two, we will be studying essential building blocks of category
theory, categories and functors. In week three, we will study how category theory can be used
to describe back propagation using monoidal categories, admitting a visual and intuitive
graphical language. In week four, we'll study how geometric deep learning and naturality
and have their foundations in category theory. We're going to study natural graph networks.
And lastly, in week four, we will see how monoids, monads, and various algebraic structures
can be connected to recurrent neural networks and LSTMs.
After our five weeks of lectures, we have a series of talks by people who are
in the industry doing category theory deep learning or both. And we are very excited about those.
But yes, in today's lecture, we're going to start by studying what is compositionality,
what is category theory, really, what you need to start learning and taking advantage of it,
perhaps the most important thing. And we're going to take a look at what category theory has done
and can do for deep learning. So yeah, let's get started. So what is compositionality? This is a
concept I've mentioned, which is central to category theory. And it's a concept that's often
misunderstood. It's often presented as an ability to build systems together by composing them out
of smaller subsystems. Now, this is certainly a component of compositionality, but this is not
all. Compositionality includes the ability to build the systems, but we don't just want to build a
very complex systems that we can't reason about. It includes the ability to reason about the
resulting system recursively in terms of its components. And really, compositionality is more
of a property of the model of our system than the system itself. So we're going to see what that
means. Compositionality requires both of these things. And for many models of our systems that
are found throughout nature, we have the first property that we can build a system, but not
the second one that we can reason about it. So for example, when we study behaviors of markets,
of organizations, of economy, of neural networks, and many other
concepts, these things aren't compositional. Now, what does that mean? These are very
fuzzily defined concepts, but they're precisely very fuzzily defined only because we don't know
how to reason about them from outside. We don't know what are the fundamental building blocks,
and we don't know how to reason about the behavior of, say, the global economy
by studying behavior of economies of individual countries. There's many emergent effects that
are happening and that are hard to track. On the other hand, there are many systems,
many models of systems that have both one and two, for instance. And these are often very simple
systems. We think of them as simple because we understand them. So for example, if we have two
differentiable functions, we can put them together and get composite differentiable functions by
using the chain rule. If you have a compiler from a language A to language B and a compiler from
language B to language C, we get a joint compiler that compiles all the way through.
We can compose various things like Markov kernels. We can compose merely polynomials and so on.
And there's many kinds of systems. Now, this slide, if you're seeing this, this might
ring some alarm bells. It might seem like I've said something inconsistent here.
It might seem like we are saying this.
So it certainly looks like I said differentiable functions are compositional,
and I said neural networks are made out of differentiable functions. Therefore, neural
networks are compositional, but I said they're not. So this meme might look like what we're saying,
but really what we're saying is the following. Compositional with respect to what?
So I said compositionality is a property of our models, and we have to specify what
our model is, what it is that we're studying. It's important to specify the property of the
system we want to model. So if our model treats neural networks as differentiable functions,
then indeed neural networks are compositional. We plug together a bunch of differentiable
functions, and using automatic differentiation, we can compute, or chain rule, really, we can
compute the derivative of the composite. But if our model treats neural networks as
generative or discriminative models, then this is not compositional.
It's important to specify what property we're studying and what exactly are we modeling.
Really, compositionality is about interfaces.
If you think about a system composed of many smaller subsystems,
and the simplest case here is that of a function, when we compose the systems,
we want to ensure that all the data we need is available at the exposed interfaces.
So really, if we have three functions like this, the famous property of associativity tells us that
if we compose f and g and then with h, that should be the same as composing g with h and then with f.
So this famous property posits that function composition should be associated. But why is
this the property that you might want to have? Because it allows us to treat functions
extensionally, only by looking at their interfaces. And if these composites are the same,
then function composition is associative, and we can look at only the input and output of the system
to know how it behaves. And we don't need to know anything about its internals or how it was built.
But if this property isn't satisfied, then we might be in trouble.
Non-compositionality implies that we need extra data
to reason about the system, data that isn't available through the interfaces.
But if our method of interaction is only through the interfaces, which is the intended way of
interacting, then this means that we have uncertainty about how the system will behave.
And then composing many such systems together, our uncertainty can only grow.
Really, what we want to is minimize this uncertainty by imposing some
invariances of how we put systems together. So that's when we create a bigger system,
we know something about how it behaves. So really, compositionality is a very delicate
property. And I'm a fan of this quote on the bottom of this slide, which tells us that it's
it is so powerful that it is worth going to extreme lengths to achieve.
So this is what category theory is. It's the study of compositionality, of how we can put systems
together. And to my surprise, when I started to learn it, it's not really just about functions.
Functions were an example here that is simple. But category theory studies all sorts of complex
systems from trees on the top left to networks in the top right to circuits on the bottom left
to bi-directional transformations on the bottom right. So with this in mind, we can start describing
what category theory is. Even though we're talking about all sorts of abstract systems and gadgets,
category theory is still a precise mathematical language to talk about it.
It's the kind of language that emphasizes relationships between concepts as opposed to
concepts themselves. And we're going to see what that means shortly.
And we're going to see that particular structures in category theory have
visual representations that aren't just doodles or sketches. They are former representations
that can be manipulated, even in a computer. So to give you a sense of what this really is,
I'm going to have a very short demo where I'm going to just draw a few things. So I'm going to
switch to my iPad and just give you a sense of how these things work. So I said category
theory is a unifying language for mathematics. And to appreciate what this means, we can perhaps
focus on some subfield of mathematics, like say group theory. So in group theory, we might want
to study a particular group. And a central concept in group theory is that of a group
comomorphism. If you have two groups, we can have a structure preserved mapping between them. And
then we can have many mappings, and they can go between groups themselves and so on. And there
could be many groups that we want to study. Maybe some of them are not connected and so on.
And we can study about various properties of these groups and study how they behave. Now,
separately, in set theory, we might have some set. And we might study functions between set one and
set two. These are really honestly got functions that we can compose. And maybe there is S3,
S4. And there's various kinds of structures that we can study between them. It's a very rich and
intricate field. And I'm going to write here set, and I'm going to write here group. Then again,
if we're studying, for instance, we might study vector spaces or similar things. So for instance,
I have a vector space one and a vector space two. I can still study some sort of structure
preserving mapping between these other structures. And for instance, this could be just merely vector
spaces in a field R. Now, what category theory does is gives us this bird eye view of these things.
All of these structures are categories. So here we have a category of groups. Here we have a
category of sets. And here we have a category of vector spaces. And all of these structures are
special examples of categories. The same way we have studied structure preserving maps between groups,
group homomorphisms, by formulating all the groups as a category. Now we can study structure
preserving maps of these categories, which are called functors. So for instance, we might have
functor from the category of groups to the category of sets, which takes a group
and gives us the underlying set. Or we might have a functor that takes a set and gives us the
free vector space on a set and so on. The idea being that once we have lifted ourselves to this
abstract level, a lot of new things open up to us. So what we end up studying is a lot of interesting
things like the category of, let's say, let's call them systems with a particular interface,
input and output. And we can have a system with internal state S1 and S2. And we can say the
ways these are related. And then we can study categories of systems with a different interface
and map between them and so on. And these systems can be actual processes, computations that do
something very interesting. That isn't just sort of what you might think of as strictly mathematical.
And lastly, we might study categories. These categories might have a lot of interesting
structure. For example, I've drawn here a monoidal category where we can sort of put
two objects in parallel, as opposed to just composing maps sequentially. And then in category
theory where it turns out that these monoidal categories, that's what they're called, where
we can put things in parallel, have a formal visual representation. So the story is a bit more
intricate. But you can think of it as taking this category and mapping it into a visual space
where each morphism has a specific shape. And you can draw it as boxes. So now these boxes
aren't just sketches and doodles, but they're formal mathematical objects.
So let me just switch to my other screen. Right. So this was merely aimed to paint a
picture of how these things look. And finishing with the monoidal categories in this demo,
these allow us to have a very natural visual language for talking about systems and processes.
And these are really, this is an example of what might, of a particular process of making a pie.
And we've all seen these things. So this is not something new.
But it allows us and it gives us a very different perspective on these things. So when you're
doing category theory, you might be studying various sorts of systems and processes that
do various sorts of things. And in category theory, there's concepts discovered or invented by various
people for studying these things. So if we're studying resources or processes in this way,
we might use monoidal categories. If we want to study probability, for instance,
we will use Markov categories. So for instance, on the left side, here we see a process which
takes an input, applies a function to it, and then copies the result. On the right, we first copy
the result and then apply F individually on inputs. Now, if F is merely a function, then these two
processes are equal. And in many other cases, they're equal. But not all of them. If F is a
stochastic function, then these are not equal because rolling a dice and then copying the result
doesn't give us the same result as rolling two dice. So Markov categories allow us to describe
such processes at the needed level of generality. Then we might, if you want to study how local
systems, how behavior of local systems gives rise to the behavior of a global system,
you might want to use sheeps. Then again, if you want to study bi-directional transformations,
such as extracting a row from a database, then up with the updating back, so forward and backward,
or if you want to study neural networks with a forward pass and a backward pass,
we might want to use optics and lenses, which are bi-directional data structures.
Then again, if you want to study contextual computation or computation with side effects,
we might want to use things called monads or their Kleisler categories.
And of course, in computer science, the concept permeating it is recursion. And then we might want
to use things called final qualgebras or initial algebras to study recursive processes.
And once we study all of these things, we might find ourselves in this situation,
where we study a bunch of different kinds of systems with different kinds of categories
and concepts, and that ends up just creating more categories and more concepts. And the theory that
studies category theory is called two category theory. So something I love about the way you
approach CT, that it is very meta and recursive in nature, allowing us to describe category theory
using category theory. And then really, once you start going with this, you might have heard about
classical mathematical structures, such as topological spaces or rings or fields.
And once you start going into category theory, it's hard to appreciate how deep and rich the
field is, because the water keeps getting deeper and deeper, and the rabbit hole hides so many
concepts that I personally wasn't aware of that have a very expressive nature and a very compositional
nature in helping us understand all of these systems. This is certainly not the end. There's
many more that every category theorist has a certain understanding of a part of these concepts.
So this was very abstract in a sense, but I hope it conveys some sense of what category theory is.
But now I would like to make this a bit more concrete and study what is the relationship
between category theory and deep learning, what has been done, how much are these concepts
actually connected to each other. So the thing I want to establish first is that the deep learning
community seems to be very well aware of the concept of compositionality. I have found numerous
workshops, programs, blog posts, lectures saying compositionality is important,
and I've linked a few of them here. I'm sure there's many more.
Joshua Bengio in his Turing lecture actually explicitly states that we need to build
compositionality into our machine learning models. But having studied category theory,
I'm still noticing that compositionality is done in ad hoc ways and that there is a large gap between
how compositionality is thought of in deep learning and the theory of compositionality that category
theory offers. Nonetheless, there have been some recent strides. So I'd like to give a brief overview
of what has been happening on the intersection of category theory and machine learning.
So the papers I am showing here are two papers that study neural networks in the very abstract,
in the form of something called a parametric lens. We're going to see exactly what that is
later in the course, but I just want to give you an idea of how these things work. So it's
something that has a following shape. And it abstractly models the information flow we see
in a neural networks. So on the left side, we see inputs and their gradients. On the right side,
we see outputs of a neural network and their gradients. And here we take the two-dimensional
notation seriously, and therefore we have weights coming from top, weights and their gradients.
And this abstractly models how the information flows. So we can see that from left and top we
get inputs and parameters, compute and output, and then having received the gradients of the loss
with respect to that output, we can backpropagate the gradients with respect to the input to the left
and backpropagate the gradients of the loss with respect to weights to the top.
Now, the question you might ask is what is the benefit of this formulation?
Why would we model it like this? Well, when these constructions were originally discovered
a few years ago, something fascinating happened. Independently, game theorists were modeling
economic agents in game theory using this categorical model. And to the big surprise,
these models ended up having the same categorical form as the machine learning ones.
So if you're studying economic agents that take some inputs,
produce some output, receive some payoff for their things and have their trying to maximize
some strategy, you end up with a model that has the same shape. So independent formalizations
by different people of game theory and machine learning converge to the same mathematical form.
This elucidated a number of connections and really sparked the whole study of cybernetic
systems or reinforcement-like agent systems, the study of agents interacting with environment
using minimal assumptions about what agents or the environment is. And really, this is
something that can't be done without the level of generality that category theory provides.
So I found this quite fascinating as it was being discovered. And it's one of the reasons why I
I, it allows me to study machine learning and game theory to a unified, unified lens.
What it also turned out that why would we want to use these parametric lenses that we can also
model optimizers of neural networks. So, and what it ended up turning out that optimizers of neural
networks have the same shape as neural networks themselves. So on the top here, the idea is,
so on the left and right, we have parameters, but on top we have the state saved by these
optimizers, the stateful ones like momentum or add-on. And this gives us hints that optimizers
are some kind of hardwired metal learners and just as some papers have shown. So there's lots
of interesting research opening up to this. And an interesting thing from a personal standpoint
happened with regards to optimizers. We started formalizing gradient descent using lenses,
this concept that I mentioned that models by directionality. And for a long time actually
had suspicions about this formalization. The problem was that there was a part of a lens that
really wasn't used at all in gradient descent, casting reasonable doubt on whether lenses are
the right abstraction. Maybe they are really an overgeneralization. But then as we went on to
formalize more complicated optimizers, such as the one that used nester of momentum,
we found that the bit that computes the look ahead in nester of momentum, the one that's always
glossed over in explanations. And when you implement these things, it requires rethinking
about how you structured the stuff. It had a natural place in our formulation. It was a thing
that we didn't use before. So really, from a personal standpoint, it clarified how a lot
of these things fit that we started using this categorical form. And lastly, we're not the only
ones using them. So very recently, about a week ago, I found a deep learning professor at New York
University, if I'm not mistaken, independently starting to use the same sort of graphical
notation as the formal one we have been using. And I'm quite amazed that at least we're sort of
having formal justification that these two notations have converged to the same representation.
So yeah, this shows one line of work that has been done with machine learning
with category theory. Other people have studied different things. People have studied recurrent
neural networks. And it has a very strange title here. But if you open the paper, you'll see that
it does study recurrent neural networks. And in recurrent neural networks, we have the
idea of back propagation through time, which is done by unrolling the recurrent neural network.
Now, what this paper showed is an interesting thing that this process of back propagation
through time doesn't merely use differentiation, but it is differentiation in a very sophisticated way.
And it uses the formalism of something called double categories, which is yet an even more
advanced but very visual concept. So this short picture shows you how you can think of
each time step of our recurrent neural network as a vertical kind of morphism. And you can
think of layers of neural network as horizontal. And then Petar and Andrew, who are co-organizers
of this course, have done a lot of work on geometric deep learning and graph neural networks.
And they've established a connection between graph neural networks and dynamic programming,
which is quite fascinating because these might be such very different things,
thinking about how graph neural networks work and how the algorithm of, say, Bellman-Ford works.
Yet again, using the abstractions of category theory, we can say something more precise about this.
And generally about, yeah,
in generally about all of these neural networks, this is some of the stuff that has been done.
But really, there is so much more work to be done and so many more things to explore. I've just
given you a glimpse of some of the interesting things, but really, transformers haven't been
studied, have been studied to some small degree, but there's so much work really to do on them
through category theory. Geometric deep learning is being studied now actually through category
theory, but then again, this is such a vast field. And the same holds for generative adversarial
networks, outer regressive models, NLP, these are all things, all fields which could benefit from
having more and more eyes apply categorical methods. I think we're at a point where
there's so much ideas and thoughts and we're trying to scale up this process of studying
things categorically. And the only thing we're lacking is resources and people knowing about this.
So this brings us towards the end of today's lecture.
We have seen how ACT is a rising field, how we can study a number of different scientific fields,
and really a number of subfields of machine learning through the same lens. It is based upon
compositionality and we've seen how it gives us this uniform description of processes and concepts.
Now, in this lecture, I didn't really go into any of the details and as I've said,
this is contrary to how category theory is usually defined and introduced. It is very
verbose and mathematical but once we start going into it, it becomes very hard to gain the intuition.
So what we're trying to aim for in this course is to start slow and to motivate these examples.
So if you want to learn more about how all of these concepts and systems can be taught off
in this uniform way, we invite you to check out the next week's lectures and to gain a really
more precise and concrete understanding of the things I've been saying about here.
And really, to end with this, it's something that we want to communicate that really applying
this category theory to deep learning is one part of what category theory does, but once you start
thinking about this through the categorical lens, things, all things start looking like category
theory. So this is something we hope to communicate. I've put up a number of references and reading
lists here for people to, this slides are going to be put up online, so you'll be able to click on
them and have a look at some of the interesting conceptual things. Some of these are books,
some of these are lectures that might be beneficial in getting a sense of how this works.
But yeah, for now, this is the point where I will end and I would like to invite you.
So we've had a lot of problems actually with the Google groups and people signing up.
If you still want to sign up, please do because we have opened the Zoom link of this lecture
to everyone because not many people, as I said, could join. But if you want to look at the next
lectures, please sign up on the course website and do check out ZULIP, which is also linked to
course participants where you can chat about category theory, chat about all of these lectures,
ask questions. This ZULIP is a part of the wider applied category theory community, so you'll be
able to see how actual categories chat and the concepts we talk about and how these things work.
Although I would warn you to thread carefully there, some of these things are very foreign
and scary looking, but hopefully by the end of this course, they will be less so. So yeah,
thank you very much. Wonderful. Thank you so much, Bruno, for the great talk and the great
introduction to this vast landscape of categories, which as you mentioned, in the coming weeks,
we will dive into more, starting with my own lecture next week, where we'll talk about
the basic objects in category theory in a more rigorous way. There's a lot of very interesting
questions in the Zoom, and I'll be acting as a sort of moderator, reading them out to you,
and we can do a bit of back and forth. So the top rated question comes from Matej Zetrovic,
and it comes with this motivation of you have probabilistic circuits like some product networks
that are compositional with respect to both conditions that you presented. However, they're
not as good as neural networks in terms of expressivity. So generative neural network can
make better images than the ones from a generative sum product network. So then the question is,
should we invest more time into studying a non-compositional model like neural networks and
make them compositional, or see how you can scale something that's inherently compositional like
a sum product network? What are your thoughts on this? Yeah, that's an interesting question,
and as I've mentioned in the beginning, compositionality is a property of the model of our systems.
So to me, it seems like the model that we have of these non-compositional systems might be
non-compositional in nature, which doesn't necessarily mean that the actual model is
non-compositional. It might be the fact that we don't know what the essential composable building
blocks are. So I'm not sure if I can provide a good answer to which of the things we should study.
I'm very biased in towards taking out a system that doesn't seem compositional,
but feels like it is, and then trying to make it so trying to understand
how information flows, and what are the compositional building blocks. So that's
my answer. All right, the second question comes from Siavash, and it's a very quick one. So what,
if anything, is the difference between compositionality versus composability?
Yeah, so these might be very loosely defined terms, sometimes composability, for instance.
Some people use composability to mean literally that we can plug together systems, processes,
functions, but then when we plug these things, composable things together,
they people don't often mean that we can study the behavior of the composite
in terms of the smaller constituents. But then again, I think I found people using,
at least in category theory, the concept of composable only when we can study them,
when we can study the resulting system recursively. So I think you might find like
different usages in different communities. In category theory, this is taken very seriously.
So you would call things composable only if you have some sort of guarantees like this.
Okay, with this top rated from Grigori asks, is there a particular book you would recommend
for beginners? I have a particular book I would recommend, but maybe you have one too.
Well, maybe you can start with yours. I mean, I'd start with seven sketches for sure.
Yeah, I would say that it often depends on where you're coming from.
I would say seven sketches in compositionally linked here is a really good resources for
scientists, engineers, and programmers in general. If you're coming from programming specifically,
you might want to look at category theory for programmers by Bartosz Mielewski,
that which is a great research, research aims specifically at programmers.
I will actually link in the Zulip, I specifically keep a list of best introductory
category theory resources on my GitHub. So I don't have it here, but I will add it,
and I will link it in the Zulip, which includes a number of many more like blog posts, videos, and
books. Wonderful. Yeah, that will be very useful, I think, for the attendees. So there is a question
from Kylan, which asks, could you explain a bit more in detail, what does this graphical
representation of your network actually bring us? For example, the diagram you showed from
Alfredo Canziani, you said it seems to be just a diagram. What does it actually help us understand?
Yeah, so this is a really good question. So I will go back to this slide. So the short answer
is it restricts how we're thinking about this, and it restricts the things we can do to this diagram.
So this might sound a little bit counterintuitive, because we have a language, and then it's very
restrictive. But this is in fact a very useful design tool is to constrain ourselves. So I
actually had a quick chat on Twitter with Alfredo, and I noticed some of the ways he uses the diagrams
to wire up some things were very non-compositional, because when you plug systems together, you
wouldn't get a thing of the same kind. But through category theory, we have found another way to
plug these things together to make it compositional. So the answer is it helps us reason about the
systems, and really one day I hope we can implement these things. So because as you'll find when you
think about deep learning, there's the whole theory, but the way you implement sometimes has
tricks and tweaks, and there's not a uniform translation of the theory into implementation.
And I think my sort of very long-term goal is to have a completely formal and uniform description
of these processes at a high level, but also exactly at a low level. And I think these diagrams
help us show this. So maybe to emphasize, when we draw these diagrams, this is exactly how
we would implement them. There's not a secret thing going on that you have to be careful.
Like you can take these diagrams very, very seriously, which is not something that can
be done with informal notation. I hope that answers the question.
All right. Yeah, thanks for that. If the person asking the question wants to follow up,
please feel free to. The current top-rated questions, and thank you for all these questions,
they're really, really interesting, and they keep pouring in, which is great to get this kind of
engagement. We're obviously all happy to keep discussing even after the lecture on Zulip and
otherwise. The top question right now is from Shiva, and it asks, since categories in geometry
have a rich interaction, can we use category theory to understand the geometry of neural
networks, such as the geometry of its parameter space or the symmetry of space?
Well, yeah, this is another great question. And I think Pettar might be in a much better
position than me to answer this. I think we both believe the answer is yes, but maybe I'll leave
it to Pettar here. Yeah, I mean, I'll just add that, so I know that many people who are signed
up to attend this course have already some working knowledge of geometric deep learning,
where we use geometry to understand neural network architectures as a covariant functions.
And one thing you will see in this course, especially in PIMS lecture, which is in week four,
is that actually you can observe equivalents as a special case of a more broader category theory
concept called naturality, which effectively makes the conditions far more relaxed. For example, you
don't need all of your functions to be symmetries to analyze such a system. They don't need to
compose with each other. Inverses don't have to exist. So you can be, in a way, resistant
even to functions that destroy some of your data rather than just leave it exactly the same.
So in a way, it's something that encompasses equivalent functions, but then allows you to model
way more interesting things than that. And I believe this also answers the question that
Freddie Minow posted on his applied category theory, a superset of geometric deep learning.
The short answer that PIMM already wrote is that we definitely think so. And the lecture from PIMM
should elaborate this connection a lot more. Okay. So then we have an operator question from
Nitin that asks, can we quantify or understand the causality or counterfactual nature of systems
if they have compositionality? Does it add some explainability nature to the system as a whole
instead of looking at the subset of components as independent modules that are not interdependent?
Oh, these are all really, really good questions. I actually don't know what the answer to this is.
So there's been some work. Well, there's been a number of papers studying category theory and
causality, but I'm actually not sure what is the state of the art with regards to counterfactual
reasoning. There is certainly lots of papers doing this, but I'm afraid I can't give a good
answer to this. We certainly hope so that something comes out, but it's not something that I think we
can substantiate just yet. Yeah, I think maybe I'll use this opportunity to plug one of our guest
lectures from Taco Cohen, who has worked quite a bit recently on trying to use category theory to
formalize causal reasoning in machine learning models. He has this very nice position paper on it
that came on the archive recently. And he will be giving us a guest lecture on this exact topic.
So please do stick around if you're interested in applications of category theory for causality.
I had a chance to speak to Taco on this on a few occasions, and he seems quite convinced
that we need category theory to reason about causality in the right way. So it'll be very
interesting to hear his thoughts on this. The current top rated question comes from Flavio,
and it says category theory tutorials might be easy to find. Can we get more info on the specific
relation with deep learning? I think the answer to this question is really that those connections
will happen in the coming lectures, unless Bruno, you want to add anything else on top of that.
No, no, you're right. I would add that I also have a GitHub sort of listing all the papers
in category theory in machine learning. This is precisely the GitHub that hosts the data used
to generate this graphic. So maybe I'll also link that in the zealot. So that might be a good thing
to have a look before the next week's lecture to see what has been done. Yeah, sounds good. Yes,
definitely do share that if you if you have a chance. Andrew, I think you have a raised hand.
Yes, I just wanted to emphasize, especially with the previous or earlier question,
that at the moment, there is no book on this subject on the connection between category theory
and deep learning. So we're going to do our best in these lectures, but it's not so easy
to give reference materials besides these papers. This is a gap we're trying to fill with these
lectures. Yeah, I think that's also a good point to have. Although the seven sketches,
if you don't count machine learning per se, is a good starting point to just understand what
all these string diagrams are like and how they connect different areas. Yeah, for programming,
yes, plenty of resources, but yeah. Okay, so let's see what else do we have? So there's a
lot of questions floating around. I'm just trying to pick which one. Okay, yeah, this one's interesting
one. It's a bit philosophical. So I'm curious to hear Andrew, sorry, Bruno, what you think about
it? It comes from Luciano Prince. Do you think that composability is a true constituent of nature,
or is just the limit of how much we can understand it? Oh, I love these questions.
I wish I could provide a really coherent answer to this. Certainly, it seems like everywhere we look,
things are compositional. But this might be like the story of trying to find your car keys under
the lamp post, because that's where the light is. We are certainly not doing compositionality.
There's so many things that seem so foreign to us, and we just don't look there. We look at
the things which are compositional. So, oh god, yeah, I'm not sure how to answer this question.
So this is a very conservative answer. That's what I'll say. I don't know if anybody else from
the team has a response to this. I think the question caught me a bit off guard. I'll have to
think about it a bit more. But yeah, Pym, Andrew, do you guys have some part on this question on your
side? I mean, I think there's probably evidence somewhere that we tend to at least learn things
in a compositional way. I mean, I guess whether there's some underlying nature is kind of a big
question. It's surely a very nice way to organize existing information, let's say.
If you think about it in a kind of compositional manner, that just allows you to reason about it
a lot more easily. Okay, so let's see. There is a lot of new questions. The current top question,
which I think hasn't already been answered, and I guess it comes from some of our more
mathematically oriented audience. Stanislav specifically asks, if we're going to talk about
two categories, should we actually then consider enriched and end categories in principle?
Yeah, so this is certainly the next step. So a lot of the things I did not mention in this very
brief lecture is enriched categories or pre-categories or higher-category theory. There is
certainly an abundance of theory and thoughts and expressivity in all of these more nuanced areas.
So yeah, these are certainly things to study and give us a particular flavor of category theory.
And yeah, I would invite you if, you know, this, this, but I would consider these to be advanced
topics for now. Okay, yeah, I definitely agree. Let's learn how to crawl before we learn how to
run. And there is a reason why some people might perceive the content so far to be a bit slow
starting. We have a very diverse audience coming from all sorts of backgrounds, and we're trying
to accommodate for all of those backgrounds appropriately. So we have one very fast-rising
question from Ewan, which asks, I'm aware there's been some research on category theory to motivate
the graph neural network design. Has any work or much work been done to use these ideas to construct
modular and composable neural networks or more interpretable representations in the spirit
of, say, what Chris Ola has been doing with representations as types?
Yeah, so that's, that's something I would love to think about and work on.
To my knowledge, the answer is no. Yeah, to my knowledge, the answer is no. But it seems like
there is really no obstacle to doing so, at least no major obstacle. So yeah, yeah.
All right. Then another top-rated question. Yava asks, which kinds of categories, broadly
speaking, are we going to focus on most in this course? So what do we see to be the most
interesting categories for deep learning at this time?
Right. So, well, in this next week's lecture that Petter says he's going to give, we're going to
talk about categories in general. But the one after this, we're going to talk about, so categories
are that allow us to compose processes in a sequence, which is useful, but in a way limited,
because often in nature, we compose processes that is in parallel. So in third lecture, we're
going to be studying things called monoidal categories, where you can put processes in
parallel. And very interestingly, you know, these are not processes where you can necessarily
copy information, delete information, some information. So we're going to add extra layers
of new ones by studying things called Cartesian categories, where you can start to do all of
these things. And then later, yeah, so we're going to study, yeah, I'm not sure how to best
describe it right now without going into depth, how to start talking about sort of things used in
equity variants. It's sort of the things that might not be easy to explain right now before
unpacking the lectures. But we're going to study part of the things we're studying here is not just
categories, but the ways these are categories are related and concepts build on top of them.
So we're going to study functors between categories, monads on these categories,
and various these algebraic structures that allow us to describe this wiring of processes or
some structure preserving maps between them. Yeah. All right. Thank you for that. The current
talk question from Jeffrey asks, potentially also a philosophical question, how do you find or how
do you decide what are the essential composable blocks of what you want to study? Yeah, I mean,
I think this is really a question that's not really specific to category theory. It's sort of
generally, to science, we're trying to find building blocks and trying to find the basic
concepts. And I think this is really an art at this point. There's not, we cannot really formalize
and systemize this, the process of science yet. Okay. So I think our one hour block that you had
allocated for this lecture has just expired. And also at the same time, I actually had to
reset my Zoom, so I actually don't see many of the questions that are still in the Q&A. So
perhaps if Andrew or someone can see if there's any other big salient questions left,
otherwise I think it's okay if we continue the discussion on Zulip. We already covered
a lot of grounds. The top one is just asking if we can put up slides in advance, which I think
we can try to do, but I depend on the speaker. Yeah, I'll definitely put my slides up before the
lecture and maybe we can start doing it more going forward. I think for the first lecture,
we're just trying to make sure everybody gets access to this Zoom properly. But going forward,
when we start to get more technical, we will definitely end to share the slides in advance.
Okay. Yeah. So I think we will leave it at that. Thank you so much for coming to our first lecture,
and we hope you enjoyed it. Bruno, thank you so much also for delivering a great motivational
entrance to everything that will come next. And we hope you enjoyed it. We hope to keep the
discussion going. So if you want to join us on Zulip in the coming days and weeks and discuss the
various aspects of the course with us as we go along and materials and so on, that would be
really great. And if you have any feedback on how things have gone today and how you would like them
to go forward, please do interact with us. However, you prefer to leave us that feedback
directly or anonymously. We very much welcome any comments you have. It's a course we're actively
building together with all of you. So on that note, let's thank Bruno one more time. And yeah,
hope you enjoyed it. And hope you'll have a great rest of your week. I will see you in a week's time
for a discussion of fundamentals of category theory.

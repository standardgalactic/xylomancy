live. Oh, sorry. Yes. Is this recording? Okay, so hello, everyone, and welcome to the third guest
lecture of the Cats for AI series. As you might have seen on the on the email list, we've finished
with the main lectures and even two guest lectures, and we're now transitioning in a
different mode. So we're going to be having an irregular schedule of guest talks,
and which are going to from now on be open to the public. So if you've got a speaker recommendations,
please do not hesitate to reach out. And first in this one, this open kind of scheduling is a
talk from Tydena Bradley. She's going to be telling us about category theory inspired by
large language models. I'm very excited about the talk. So Ty, please take it away.
All right. Thank you, Bruno, and Petra, and all of the organizers for the invitation.
I just want to say I think it's really exciting to see that there's so much interest in category
theory outside of the field of pure mathematics, and in particular in machine learning. So I think
that's very inspiring. I have a lot to learn from you all. And so I hope that maybe I can give back
something interesting in today's talk. So yes, as the title suggests, I want to share with you some
category theory that comes from comes from work with some collaborators and I, which I'll share
in a second. That's inspired by large language models. So I was asked to give this talk, I think
several months ago. And I thought it was very convenient that open AI released chat GPT, you
know, like less than two weeks ago. So maybe I'm sure all of you have heard about that by now. And
I think it's kind of nice that, you know, language model or maybe fresh in everyone's mind. So I
couldn't resist. I know that you have all probably heard about chat GPT and you probably see in your
own examples. I couldn't resist just sharing one that I did. So I would say, don't really pay attention
to what's on this slide, because it's not really in the theme of the talk. But I was jumping into
chat GPT. And I had a question about some physics thing that I was interested in. And so I, I gave
it this, this thing called a quantum state, just think of it as a vector. And I asked GPT chat
GPT, does this vector satisfy a certain property? Like does it satisfy a property? And I thought
it was really interesting that it got the answer wrong. But it knew what math steps to take,
and what pieces of that math to analyze, in order to reason about a yes or no answer,
does this thing satisfy a property? So it kind of like knew what linear algebra things to do. You
start with this vector, and then you get a matrix, and then you compute something on that matrix,
and then you apply this formula. And then at the end, you reason about it. And so I thought that was
like, Oh, pretty neat, even though overall, it kind of got stuff wrong. And at least kind of knew what
to do without being told to reason about, you know, how would one even arrive at the answer. So at
this point, of course, there's lots of chatter and lots of buzz, you know, is this a GI, or is it
garbage or whatever. And I don't really want to get too much into that. But I do want to kind of
draw one idea to the forefront of our minds. And that's this fact that the training data for
large language models is really interesting. It's just raw text. And so from a mathematician's
perspective, that prompts a question, like what what structure is in unstructured text,
that allows you to like form concepts and reason about things so that you get actual
coherent texts. So whether or not, you know, chat GPT got the answer right or wrong,
okay, fine. But what's interesting is that there's no like, textual garbage on this
page, right? Like all of these words go together. And it sounds like coherent English,
it is coherent English. But you're just starting with, you know, raw text, or, you know, probability
distributions on text continuations. And you can ask the question, what mathematical framework
can we, you know, write down that describes that passage from probability distributions
on text continuations to some space of meaning or to some like semblance of semantic information?
Like, what's the math there? And you might ask this, like, can we look inside of a transformer
and like point to things and say here, okay, so that's one thing you could do. That's not quite
what I want to do. But maybe what I can describe in this talk is kind of an overall category
theoretical framework that maybe can provide for you some tools if you wanted to dig further.
So that's why the title of this talk is category theory, inspired by large language models,
as opposed to the category theory of large language models. I'm not making a grandiose
claim like that. But it is really interesting to ask, like, how far can you get with just
probability distributions on text or text continuations? And I think that category theory
provides some good and very natural tools to kind of start exploring this question.
So let me go to the next slide. So here's the outline for the remaining few minutes.
First, I want to share with you a category of language and some advantages of that.
We'll also see some disadvantages. And then I will describe for you an enriched category
of language. So you've, we've heard a lot about category theory in this series.
But maybe a new idea might be something called enriched category theory. So I won't assume
you have heard of that already. So I'll kind of explain a little bit of what that means.
And then tie it back into this question that we're trying to answer in the context of language.
Now, everything I'm sharing today is based on a paper together with John Tarilla and Yannis
Vosopoulos. It came out earlier this year, earlier in the spring, an enriched category theory of
language. So since this is an introductory series on category theory, what I thought I would do is
just kind of give you a bird's eye view or the highlights of some of the ideas that, that we're
working on. And then if you're interested, I would say, okay, for more details, you know,
we can chat after you can take a look at the paper. Now, before all that, I want to motivate those
ideas by sharing with you an analogy. And there's a really curious analogy between linear algebra
and category theory. It's simple. I think it's really interesting. And maybe it might appeal,
you know, to the machine learning community, since we deal with vectors and matrices a lot.
And I, and I like this analogy a lot, it'll come up later. But I also think it really nicely
motivates the ideas that is kind of the main theme of the paper. Now, speaking of themes,
this analogy that I will share with you kind of centers around a very common theme in mathematics.
And I have to credit John Tarilla for really emphasizing this idea, because I think it just
illustrates in a crystal clear way the advantages of thinking from a category theoretical perspective.
Okay. So before I share with you this analogy, let me inch towards it by telling you a common
theme in mathematics. So it's very well known in math that if blah, something has nice structure,
and I'll say what that means in a second, then functions from a fixed set into that thing,
have nice structure too. Okay, what do I mean? Think about a set X. There it's on the screen.
Okay, a set is just like a bag of marbles or something. It has no structure. So if I have
elements in a set X, I can't add them. I can't multiply them. I can't combine them in any way
to do anything meaningful. It's just this set. It has no structure. But there is another set that
has a lot of nice structure kind of readily available. And that's the reels. Like for example,
there's other things you could think of. But let me just think about the reels.
In R, I can add things. We can multiply them. We can do all sorts of things. So it has nice
structure. Well, what is a function from a set X into the reels? Think for a second
for simplicity. Just suppose that X has three elements. It's a three element set.
So a function from a three element set into the reels is a choice of three numbers.
Right? So if I look at all functions from X into the reels, I have the collection
of all triples of numbers. And that is familiar to us. That's just three dimensional space.
In other words, what I'm getting at here is that the collection of all functions from,
let's say, a three element set into R has nice structure. I can actually organize those elements,
you know, the image of this function as this array suggestively. Because if I have two of
these functions now or two of these arrays, I can add them. You just add functions point-wise,
which corresponds to sort of adding the entries of these vectors element-wise.
You can multiply a function by a scalar and get another function. That just corresponds to
multiplying the entries of this vector component-wise. You can multiply actually functions together.
So what I'm hinting at here, which maybe you can see, is that functions from X into R actually
form a vector space. We usually denote that vector space by R to the 3, R3. Another notation,
I could write this as RX, R raised to the power of X. In fact, we've seen this notation earlier
when Petter talked about exponential objects. So this is the set of all functions from a set
into R is not just the set, turns out to be a vector space. And that's because those functions
inherit, inherit structure from this sort of ground field. Okay, so vector spaces or linear
algebra are an example, really nice example of where if something has nice structure, then functions
into that thing also inherit nice structure. And here's the key is that a very similar story
holds in category theory. So here's kind of the analogy I like to have in mind. On the one hand,
we just started with a set and we said, okay, you can't really do much with a set. There's no
structure. It's just a collection of things. Analogously, if someone just hands you an arbitrary
category, what do you know about it? Okay, you know I have some objects and some morphisms
between those objects and composition and identity morphisms, but that's it. You don't know
if C is just an arbitrary category. If you can combine objects to get a new object in your
category, if you have other structure looking, looking about maybe in specific categories you
do, but in general, just the definition doesn't give you that. So what you may like to do,
just like in the case of sets, there we thought of a specific set, like the reels,
that had a lot of structure and we looked at functions into that. Analogously, it can be
very fruitful to think about a category that has a lot of structure that's really nice
and look at functors into that category. So what category is that? Well, it turns out
that the category of sets is a really good choice. So analogous to looking at functions
from a set into the reels, now it turns out that looking at functors from an arbitrary category C
into the category of sets is there's a lot of rich mathematics there. So why is that? Let me
just say, just like in the reels, you can add numbers, you can multiply them, you can combine
numbers to get new numbers. In the category of set, you can do this also. So at the beginning
of this series, Petter talked about an over-refresher memory. You've heard of products or the Cartesian
product of sets. He also mentioned the co-product of sets or, you know, I said the exponential
object. So set has this nice property that you can take a bunch of sets and combine them in some
way to get a new set. Now let me just kind of pull back the curtain and speak plainly for a minute.
So the idea that I'm trying to mention right now are these constructions and category theory
called limits and co-limits. So what I'm saying is that set has all limits and set has all co-limits.
So here's kind of what that means. I think some of you have probably heard these words or you're
familiar with them already. But in mathematics, quite often what you want to do, you have a whole
bunch of mathematical objects, maybe they're sets or maybe they're groups or maybe they're topological
spaces or maybe they're vector spaces. And you ask, is there a way that I can take this collection
of objects and combine them in such a way as to get a new object in my category? Now there are
sort of two typical ways that you can do this. And by typical, I mean these constructions are
found all across the mathematical landscape. And it turns out that when you kind of strip the
constructions of their details and just look at the pattern, these constructions turn out to fall
into two categories for lack of a better word. I mean that in like the English sense, not the
math sense. So on the one hand, you can form what's called the limit of those objects. And depending
on how that's constructed, these have familiar names like the intersection of two sets or the
Cartesian product of two sets or the Cartesian product of two topological spaces, direct sums
of vector spaces, meets. If you have a lattice and you can ask for the meet or sort of the
minimum element in the collection, your lattice, that's actually an example of a limit.
Turns out greatest common divisors are also examples of these things. Kernels, if you have
a group homomorphism and you ask for its kernel, all of these constructions on the left hand side
are subsumed by one single idea and category theory. They are instantiations of one single idea
that's called a limit. And dual to that, you know, in category theory, you always like stick the word
co in front of something, you kind of reverse the arrows, and so you get dual operations.
So on the other hand, on the right hand side, you have things like unions, co-products,
direct sums again, joins in a lattice, dual to greatest common divisors, least common multiple,
and co- kernels. So these are all examples of things called limits and common limits. We won't
worry about the actual definition, like they satisfy some universal property. But I mentioned
that to say that in the category of set, it has this wonderful property that you can take any
collection of sets and then ask for the limit or ask for the co-limit, and it turns out to exist.
So it's another set in that category, and it satisfies universal properties. You cannot do that
with any category. Any arbitrary category may or may not have that ability. It turns out the
category of sets does. So that's just to suggest to you that just like the real numbers was a nice
set to look at functions into. Analogously, the category of sets is a nice category to look at
functors into it. These functors, because this is such a nice choice, they have a name. So a
functor from an arbitrary category C into the category of sets, it's called a co-pre-sheave.
Sometimes you may want to look at contra variant functors from C into set. So it's just a functor
where it kind of flips the arrows. So in that case, folks will put a little OP above C. So it'll
say C OP into set. Those are called pre-sheaves. So pre-sheaves. Let's just say functors.
In any case, this is a nice thing to look at. It turns out that functors from C into set
itself form a category. So just like functions from X into R form a set with structure,
functors from C into set form a category. It's a functor category. And in fact, I think
Penn talked about functor categories in his talk. There the notation might have been square bracket
from C to set, but this exponential notation is this notation for that same concept. So it turns
out objects or functors, morphisms, or natural transformations. But that's just kind of me drawing
the analogy. Now let me give you one more analogy. And then we'll move on to the language.
There are very special vectors on the left-hand side. In particular, for each element in the set
X, there's a very particular vector associated to that element. And the analogous thing holds on
the right-hand side. And that vector is a one-hot encoding or a basis vector. So for every element
in the set X on the left-hand side, there is a particular function that sort of sends any other
element in the set to one if it's the same as the element I chose or to zero otherwise. Or if I
write this out as an array, it's just a vector with all zeros, except for a one in the appropriate
spot. So analogously, there are functors from a category C into the category set, one of these
code preaches, that is very special. And these are called representable functors. So for each
object little C in my category, there is a functor from the category into set
that takes another object in the category D. And it sends that object to the set of all morphisms
from your given object C into D. So you've seen this notation already in this series. Sometimes
this set is denoted HOM from C to D. Some other people, instead of writing HOM, they'll write
just the name of the category C. So it kind of reminds you, oh, morphisms in what category C.
So I like to think of, these are called representable functors, these HOM functors,
representable functors. These are a particular kind of code pre-sheaf. Or if I were to kind of
move the argument to the left, it would be a pre-sheaf. I'll tell you why later, not now,
but I like to think of representable functors as kind of analogous to these one-hot encodings
because just like any other vector is built up from one-hot encodings, similarly, any other
co-pre-sheaf is built up from these representables. But we'll say that later. So I like this analogy.
And the reason I mentioned this is because these representable functors will play an
important role in just a second. So that's my analogy for linear algebra. Maps into law
inherit structure from blah. So in particular, functions into the category set inherit
really nice structure. Now, let's see how this theme or this analogy plays out in the context
of language. So let me describe for you a category of language. We'll see that it's nice,
but like a set, you can't really do much with it. So we're going to look at functors
out of that category. So let me describe that for you now. It's very simple.
So here's a category. Consider all strings from some finite set of atomic symbols.
So if your finite set is the set of all words in English, okay, it's a very large set,
but it's finite. You can just take the free modeling on that set, all strings.
Then substring containment defines a preorder on this set. In other words, we can make sense of when
one string is contained in another string, x is contained in y, and I can denote that by less
than or equal to. So I think Pan mentioned in his talk the concept of a preorder. So just by
way of reminder, a preorder is a binary relation denoted by less than or equal to that's reflexive.
So every substring is every string is contained in itself. So x is less than x and that's transitive.
Right. If blue on my screen is contained in small blue and small blue is contained in small blue
marble, then I know blue is contained in small blue marble. So that's transitivity.
But as Pan mentioned, every preordered set is a category. Reflexivity is exactly an identity
morphism. Transitivity is exactly composition. So I just defined for you a preorder set,
aka a category, and it just has at most one morphism between any two objects in that category.
So it's very simple. There's an arrow from x to y whenever x is a substring of y.
Let's call this category L. Fine. It just kind of tells you what goes with what. Does this
expression go with this expression? If the answer is yes, there's an arrow. If the answer is no,
there's no arrow. It's like tells you what goes with what. Okay, it's nice, but that's very limited.
In particular, I cannot make sense of what's a concept in this category or what's the context
of something or can I combine ideas to get a new idea? I don't have any of that structure here.
So we'd like to take a cue from this theme that we just spent several minutes thinking about.
And what we'd like to do is now consider functors from that category into the category of set.
So this is just repeating that theme. And representable functors, these ham functors are
particularly nice in this case. And I'd like to think of them as like a first approximation to
the meaning of an expression. So why do I say that? Okay, pick an expression in the category
like the word blue. And let's look at the ham functor, ham, blue, blank. Okay, where if I put
in an expression x and the blank on the middle here, I get a set. And this set just tells me,
is there an arrow from blue to x or not? If there is an arrow from blue to x, then ham of blue comma
x is just the one point set representing that one arrow. If there's no arrow from blue to x,
if blue is not contained in x, then I get the empty set. Okay, now I like this because this
representable functor sort of its preimage is the collection of all expressions that contain the
word blue. And I like that this reminds me of the ornate lemma. So the ornate lemma is this,
I'm sure you've heard of it, this famous theorem in category theory, which essentially says that
a mathematical object is completely determined by the network of relationships that has with other
objects in the category. So if you want to understand something important or all all important
things about an object, you can look at morphisms out of that object or morphisms into it. Here,
this representable functor picks out exactly the network of ways that the word you've chosen,
like blue or whatever, fits into all expressions that contain it in the category in your language.
So, you know, if you think about language a lot, then I'm sure you've heard, you know,
this famous quote by John Furth, you show no word by the company it keeps. I mean, this is how
word embeddings work too, something of the meaning of a word is sort of captured in its context.
So this representable functor captures precisely that context. So you might think of this as like
capturing something of meaning, but you know, I put that in quotes, and I say it's a first
approximation, because as you can already guess, this isn't everything. In particular, you know,
there's nothing about the distributional information here. This is just kind of yes or no
bare bones, but it's a good start. So this is kind of like the native perspective.
When I see representable functors, I kind of think, ah, the network of ways that that word
fits into its environment. So it captures something of that. So here's, you know,
I'm just kind of laboring the point. But when I see this functor, another image that I have in mind
is like, it's kind of like a vector. And I put sort of above the equal sign because this is not
proper category theory. But when I think of the representable functor, I think of the vector
of zeros and ones, or empty sets and one point sets, right, indexed by all of the expressions
in the language, where there is an empty set in that slot, if blue is not contained in that
expression, like deep red being cherries, or a one point set in that slot, if blue is contained in
that, in that expression, that's indexing that spot, like small blue marble, beautiful blue ocean,
etc. So when I think of representable functor, I think of like a vector whose entries are empty,
or or one point set indexed by expressions in the language. Okay, this is just like
something that I have in my mind. Okay, now, why did we do this to make our lives more complicated?
No. So as we said, when you look at functors into a category with rich structure, like set,
you can do things with it. So I was kind of hinting that these representable functors behave
like building blocks. So we can actually use representable functors, or these co pre sheaves
to construct new co pre sheaves. And we can do that by using the structure that is in the base
category set. So I mentioned set has all limits, all co limits. It's also Cartesian close. What
that means it has something like an internal home, which if you unwind all of that, what it
suggests is that we have some kind of notion of conjunction, that's kind of like what a limit is,
disjunction, that's kind of like what a co limit is enclosure, Cartesian closure, it's kind of
something like implication. So I want to give you a concrete example, how am I making these
connections? Like, why is a co limit like disjunction or or. So let me give you a concrete
example of what that looks like. So suppose I have, you know, a functor representing red,
that's like the network of ways red fits into the language. Suppose I also have
the functor representing blue, the one we just look at. So I'm claiming that if I have two
functors, I can take what's called their co product, that's a kind of co limit,
which I claim is analogous to to disjunction, I claim it's analogous to the concept,
quote unquote, of red or blue. So here's why I make that claim. When you when you write down the
definition, here's what you find. So co product means I can take two functors, hum, red, comma,
blank, co product together with hum, blue, comma, blank. That defines a new functor
that sends an expression X to the union of the two sets, hum of red index and hum of blue index.
So here's what that means. I get a new functor, a new co pre chief. And again, if I envision it as
like a vector, whose entries are empty set or, or something else. What you find is that this
functor is sort of supported on all texts that either contain red, or contain blue,
or contain both of them, meaning it's non empty on the union of all of those sets.
So for example, deep red being cherries, that contains the word red. So the set I get is the
one point set small blue marble, that contains the word blue. So I get the one point set.
Did you put the kettle on contains neither of those words. So I get the empty set.
Red and blue fireworks contains both. So I actually get the union of two one points
that are two points that. Okay, so it's not empty, this functor, this co product of red and blue
is not empty on all sets that either contain red, sorry, all expressions that either contain red,
or blue, or both. And that pairs well with this idea of union as like or when dealing with sets.
So that's kind of why we think of co products is kind of like disjunction.
Okay, so there are other things you can do, you can take the product, you can do this thing
called internal home, which is like implication writing that down gets a little bit more complicated.
But there are other limits and co limits. So the point is, so here's kind of a summary of what
we've done so far. We started with a very bare bones category L, it's like a preorder, it just
tells you what goes with what. So in that sense, you can kind of think of it as like syntax, maybe.
What goes with what. Okay, then we took, we passed from that category to the set of the category
of co pre sheaves on it. So that's what I'm kind of thinking as of synantics. So that's kind of where
the meaning is in this United type sense. So every expression on the left, corresponds to a
representable functor on the right. So that functor just picks out the kind of context of that word
or all expressions that contain that word. By the way, this passage from left to right,
this assignment, blue goes to harm blue comma blank, that turns out to be a functor, that
functor is called the yonata embedding. So I mentioned that in case you've heard that word.
The op that you see on the left, it just is saying that this yonata embedding is
contra variant. So it versus arrows, but that's kind of like bookkeeping. So this is just a
summary of what we've done so far. We started with like a kind of in quotation marks, think of it
as like a syntax category, very bare bones just tells you what goes with what, not a lot of structure,
like no structure. But then if you pass to co-preachies or functors from that category into
set, you have the ability to capture something of meaning of a word in the sense of John first,
in the sense of the yonata lemma. And then you actually have structure in that category and
you can start to combine things in a way that kind of feels like logic or maybe like reasoning.
I like to think of pictures. So here it is, you know, I start with a word and then I send it
to like kind of this network of ways that it fits into the category. This is nice, but as you can
already guess, it's very limited. It's just kind of like binary yes or no, does this fit in,
kind of discreet. It has nothing, it knows nothing about the distributional information
of language. So what you'd really like to do, what would be better is if you have like, you know,
version 2.0, where if I'm, you know, if you can give me the fact that blue is contained in small
blue, but also what's the probability of seeing that? You know, if I see the word blue, what's the
probability that, you know, it'll be completed by small blue marble or, you know, whatever that
probability is, it's going to be higher than like I woke up and had a blue idea just to borrow, you
know, Chomsky or something. So you'd like to really weight the arrows in your category
with conditional probabilities of continuing an expression with a larger expression.
You'd like to do that. And then if you include this distributional information, then you can ask,
okay, can I combine concepts there? And is that kind of capturing the sort of framework of getting
something like logic or reasoning from, you know, just knowing what goes with what together with
the probabilities? Now, the nice thing is that category theory provides a way to do this. So
this is exactly what we find in enriched category theory. So let me, let me give a quick introduction
to that. What is enriched category theory? So in category theory, if you have two arrows x to y,
ham of x to y or what I'm denoting by C x to y, that's the set of all morphisms from x to y,
okay, you ask that that be a set. The point is that in enriched category theory, that may not
just be a set. It could be a set with extra structure or it could not be a set.
What is the set with extra structure? We'll think about it. If x and y are vector spaces,
then the set of all linear transformations from x to y is also a vector space. It's not just a set.
You can add linear transformations, you can scale or multiply them. So the ham set is a set with
additional structure. It's actually a vector space. So in that case, one says the category of
vector spaces is enriched over the category of vector spaces. So whatever your ham objects are,
you say your category is enriched over that. So I just said that on the upper left-hand side.
If this ham set is a vector space, you say, oh, my category is enriched over the category vector
spaces. If your ham set is actually a group, you say your category is enriched over the category
groups. If it's a topological space, you're enriched over topological spaces. It turns out
on the lower left, if this turns out to be a truth value like a zero or a one, your category is
enriched over truth values, those turn out to be exactly pre-orders. So those things that Penn
was talking about earlier, you're actually doing enriched category theory. What if your
ham set is just a set? Then your category is enriched over a set, and you're just doing
ordinary category theory. So ordinary category theory is like a special case of enriched category
theory. Now here's the thing that we're most interested in for the remaining few minutes.
If this is a conditional probability in a way that I'll explain in a second,
then your category is enriched over the unit interval. And that's kind of what I want to
focus on just quickly. So what's the unit interval? The unit interval, think of it as a category
whose objects are numbers between zero and one, and where there's an arrow from little a to little
b if a is less than or equal to b. So it's a pre-order. Now the unit interval viewed as a
category has a lot of the same properties as the category of sets. So what I'm kind of hinting
at is that we're now going to want to look at functors into the unit interval, which will
be analogous to what we just did by looking at functors into set. So in what ways is the unit
interval like the category of sets? So on the slide, I showed you, okay, on the one hand,
your objects are sets or numbers, your morphisms are functions or this pre-order,
but what else do these two categories have in common? Well, they have a manoidal product. In
other words, in the category of set, you can take the Cartesian product of two sets, you can multiply
sets, and there's a unit for that multiplication, namely the one point set. So what that means is
like, you know, a set x times the one point set is just isomorphic to x. So it's a unit with respect
to this multiplication. Well, the unit interval also has a multiplication, of course, multiplication,
and it has a unit with respect to that, namely the number one. So this turns out to be a
manoidal product. So both categories are manoidal categories. But wait, there's more.
We mentioned that the category of sets has, you can construct co-limits or limits.
Turns out the same thing is true in the unit interval. So when you unwind the definition,
what's the limit? What's the co-limit? It turns out just to be minimum. Even if I have a bunch of
numbers in the unit interval, I can take their meat or their limit, it turns out just to be the
minimum of the numbers in that set. Similarly, co-limits turn out just to be maximum.
So, okay, both categories have all limits and co-limits, and then, you know, there's more.
You have closure. So that kind of means you have this internal hum, or like Petra was saying,
earlier in this series, you have these exponential objects, which are kind of defined with respect
to this multiplication. So I won't go too much into that, but I just want to kind of suggest
in your mind that the unit interval can now play a similar role that the category of sets did.
So how does this play out? Well, it turns out that we can discuss a category enriched
over the unit interval. Just like a category enriched over sets is an ordinary category,
we have an analogous idea where we switch out sets with the unit interval.
So what is a category enriched over the unit interval, also called a zero-one category? Well,
just like with a normal category, you start with some objects. Similarly, a zero-one category
consists of some objects. What else do you have? Well, in category theory, for every pair of objects
xy, you ask for a set hum xy, or c of xy. Now, I'm replacing set with the unit interval. So,
for every pair of objects on the right hand side, little x, little y, I ask for a hum object,
which is just an element in the unit interval. So for every pair of objects, there's a number
associated with them. And I think of that as a hum number or a hum object. Okay, what else is a zero
one category consists of? Well, in normal category theory, we ask for composition,
aka, if I have a way to get from x to y and y to z, then I can compose them and get, you know,
a morphism from x to z. Well, now, looked at the Cartesian product on the left hand side,
we said that that's analogous to multiplication of numbers. So that's what you see on the right
hand side. And just like morphisms are now this preorder, what I ask now for to have a zero-one
category is that there's a morphism from this hum object y to z times the hum object x to y,
that there's a morphism from that product to the hum object from, oh, that should say x to z
on the right hand side, that's a typo. Okay, and then, you know, there's more, you can also ask for
identities. And identity morphism on an object x is just really a function from the one point set
into the hum set from x to itself. So a morphism from a one point set into a set just picks out
an element in that set. So similarly, if I kind of look at the analog on the right hand side,
instead of a one point set, I have the number one, which plays the role of unit. And instead
of an arrow, I have this less than or equal to. And so this is the data of a zero-one category.
And of course, you ask it to satisfy some axioms.
You can also make sense, once you have the notion of a category enriched over the unit
interval, you can discuss functors between two such categories. But really, it's not a category
in the traditional sense, it's an enriched category. So you can make sense of enriched
functors. So I'll just kind of go over this briefly, but you can imagine on the right hand side,
it's a function from the objects of your first enriched category to the objects of the second
that satisfies some inequality that's very reminiscent of what you might have in ordinary
category theory. For the sake of time, let's not think too deeply about this, because the real
punchline is that what happens when D, the category you're mapping into, is a really nice
category, like the category of sets. Previously, that gave us co-pre-sheeps, and we saw that had
a lot of structure. So in this new kind of iteration, I want to replace set with probabilities,
the zero-one. And I can look at enriched co-pre-sheeps into the unit interval. That turns out, as you
may guess, to form its own enriched category. And you can talk about enriched natural transformations
and make sense of all this. So the details, let's not, you won't worry about them too much,
but that's kind of the idea. I just want to repeat the same story that we just did,
but now I replace the category of sets with the unit interval. So I'll just kind of summarize
and give you the punchline. Here's what happens when you do that. So here's now, you know,
semantics 2.0, or syntax 2.0, or language as an enriched category. So just like earlier,
I had a category with an arrow. If one expression is contained in the other, I'm going to now have
the exact same thing and do exactly what I said I wish we could do, namely, decorate it with a
conditional probability. The punchline is that fits exactly into this framework of enriched
category theory. So language is a zero-one category. Its objects are strings of symbols
from some atomic set of symbols like before. Now the ham objects, the ham object between
an expression x and y, will say it's the conditional probability that y, this larger string,
is an extension of x or contains x. You know, if x is it contained in it or it's zero otherwise.
So you can check this actually defines an enriched category over zero-one. You know,
this sort of reflexivity or identity, yeah, what's the probability that blue is contained in blue is
one. And then when you write down what this means, it turns out that conditional probabilities
multiply in exactly what you need to get this composition. So all I'm saying here is that
if you want to decorate these arrows with these conditional probabilities of continuation,
it's like your dreams are fulfilled. Oh wow, enriched category theory says that this is,
you know, something you can actually do formally. As you may guess, this is nice,
but it's limited as before. I can't combine anything, you know, there's no notion of
concepts or combining things and having some kind of logical reasoning in it. So again,
you want to then pass from that category to zero-one functors on that category.
And so when you do this, you will find that representable functors like before
are sort of have the right support. So as an example on this slide, you can look at the
enriched representable functor of the word blue. And I like to think of it as like this vector
where they're, which again is indexed by expressions in the language where there is a zero
if that expression does not contain the word, the chosen word like blue, or it has this conditional
probability, you know, the conditional probability of seeing small blue marble, given that you've
seen blue, maybe that's like 0.22. So it has the same support as before, but now it has this
distributional information. And you can keep on going, you know, you can combine these things.
So it turns out that an enriched category theory, the appropriate notion of co-products, products,
or more generally limits and co-limits, they're called weighted co-limits. So that definition
gets a little bit technical. But when you take, when you unwind that definition, you can sort of
ask, hey, what's this enriched notion of red or blue, this sort of concept, if I take the
co-product of these two co-pre-sheeps. And if you choose these weights in the right way, you
basically get like the point wise maximum of the two values individually. Okay, so I think
that's interesting. And you can kind of try to think about what does that mean, like if you were
to interpret that. I'll let you ponder that. And since I'm running short on time, let me just say
there's a lot more that you can do here. So you may have already thought about the fact that,
you know, you can get from the unit interval into the non-negative extended reels using negative
log, you know, there's, there's an isomorphism actually between them. So that map that allows
you to go from the unit interval to non-negative extended reels, that turns out to be a functor.
And what that means is that your category which was enriched over the unit interval
can now be enriched over non-negative extended reels. And that has a name and category theory.
So a category enriched over that, that's a pre-order, so it's a category, those are called
generalized metric spaces. And the upshot is that you can now think about the distances between
these sort of concepts or distances between these representable enriched filters. And so what happens
is that in this generalized metric space, you know, expressions that are likely continuations
are close together, like blue goes to blue marble, so they're kind of close in this
generalized metric. But then those which are not likely extensions are kind of infinitely far away.
So maybe, you know, a sweet blue scent is not a thing that people say, or it has a low probability
because colors don't smell or have smells. And so that kind of pairs well with your intuition
that like, hey, that should be way far away. So you can think about distances now, you have these,
you know, abilities to combine concepts. We've just looked at co-products, but there's so much more
that you can do. It turns out that this has nice connections to even tropical geometry. And so
Yana Esplosopoulos, one of our co-authors, has really nice ideas about that, and lots more.
So all that to say, kind of wrapping up, there's a lot that you can do by repeating this theme
of starting with something that doesn't seem to have a lot of structure, like on the left-hand side,
and then looking at maps or functors into another category that has a lot of structure. And there
you have the ability to form concepts, to talk about distances, to think about tropical geometry,
which you cannot do on the left-hand side. So this all has an enriched story. And maybe I will just kind
of leave you with this teaser. Everything I've described today kind of rested on this analogy
right between linear algebra and category theory. So we said that this co-pre-sheep category is like
a vector space. I hinted that these representable functors are like one-hot encodings. And I told
you, I tell you why I think about that, to think about it that way. So just like every vector is
a linear combination of these one-hot encodings, right? You take a linear combination, they're
like a basis. So it turns out in category theory, there's a theorem. If you were to look, so Emily
Real has a beautiful book on category theory, category theory in context. I think she calls this
the density theorem. So it turns out that every co-pre-sheep is a co-limit of these representable
functors. So just like every vector is built up from basis vectors, every functor from your
category in this set is built up from these ham functors, in case they're kind of like a basis
in that sense. And then the analogy goes on. And I will just end here, because I think this
is really fascinating. And I don't think it's more than an analogy, which is kind of puzzling.
But in linear algebra, you have matrices. In category theory, you have pro-functors. And it's
basically, if you write it down, it's like the same thing as a matrix. For every pair of objects
in two categories, you get a set. A matrix for every two elements in a pair, in a product of
sets, you get a number. Matrices can be multiplied, and you know the formula for that. So in category
theory, pro-functors can be composed. And the formula for that, which is a kind of co-limit,
looks a lot like matrix multiplication. Hmm, that's interesting. Well, you know, in linear
algebra, every matrix, you can compute its SVD and get singular vectors. It turns out in category
theory, every pro-functor has something like singular vectors, which is called the nuclear
pro-functor. And in fact, the way that you construct it is like line-by-line analogous to how you
compute the SVD. I think what's really interesting is that as far as I know, this is just an analogy.
And not like linear algebra is not a special case of category theory in this sense. But
it's really curious that you can do things in linear algebra that we know and love.
And that analogous construction is also happening in category theory. So why do I say that? Because
I think that that analogy sort of is encouraging that category theory can be a very natural and
beneficial environment in which to kind of understand maybe what's going on with large
language models, when all they have to work on are, you know, raw text or probability distribution
context. And as we've seen, you can get quite far, at least from a mathematical perspective.
So thank you for your attention. And if you're interested in learning more, then the paper
is available online. So thanks, everyone. Thank you so much, Thay. This was a wonderful talk that
I think I'm going to have to keep digesting. I absolutely love the analogies. So we're going
to open up the questions right now. So if you have a question, feel free to post it in the Q&A.
And we can read them out. Alternatively, you can also raise your hand and we can just
unmute you. So while we collect the questions, it sounds like Petar has a question. And I don't
know if I should read Petar's question or Petar, do you maybe want to say it yourself?
Yeah, I'm happy to post it myself. First of all, thanks so much for such a wonderful talk.
I really enjoyed all of the connections. And it really brought, I guess, a brand new set of
goggles with which I can view all these large language models in a bit cleaner way.
I had a question which it might be maybe trivially contained in which word to describe. But
while I think this theory is a nice way of explaining which word follows the next word
and the similarity of sentences and stuff like that, which is what language models do in principle.
It also seems like today you get a large difference between success and failure with
these language models depending on how much you hack the prompts. Like there was this one paper
that said you can just add let's think step by step and it suddenly improves your reasoning
capabilities by a whole lot. I'm curious if you think your theory has an answer to why this happens
or could have an answer for why this happens in the future. Yeah, I'm just really curious about
that. Thank you. Oh yeah, that's such a great question. So I don't know yet. I don't know.
But I think I think you would take more investigating. I think the kind of punchline
that I wanted to convey is that maybe this could be a good direction to look in. Because
if you can kind of have a mathematical framework where you can start to see how concepts combine
or you can start to see something like logical structure emerging, then if you kind of explore
that a little bit more, then maybe you can start to pin down some tools that will then allow you
to ask these kind of like interpretability questions or like why is it that if I say hey,
can you know GBT and step by step, can you okay, but maybe now that there are tools that can allow
you to do that and maybe it has to do something with this sort of structure. You know, one thing
that I didn't say, but this co-pre-sheaf category set C. That's an example of a topos. And a topos
is known in mathematics as a good place to do logic. So that's encouraging from that perspective.
I think it's much too early for me to give you like a definitive yes or no answer.
But I think from a mathematical perspective, it's promising because you have a lot of tools
to kind of reason it exactly about these things that are really good questions.
So it's too early to say, but maybe I could say it's hopeful.
Yeah, no worries. I just wanted to prompt you and see what happens.
Great. Thank you. Thank you so much.
So there's been a question during your one hot encoding slide from Jules about,
so if you go back to the, oh, it's there, right. It was the in general, so I think it was the slide
after actually, in general, the start could be any set, right, not just an integer. I think this
might have been remarking on the fact that one hot encoding, well, maybe Jules wants to elaborate
on the question themselves. I imagine the, okay, so Jules is saying this was answered later.
So I might ask a question myself. So I absolutely love sort of the idea of
thinking about structure in this way. Category theory has a lot of structure and when we think
about unstructured text, well, it doesn't. It's just sort of strings and sort of connecting in
this way is absolutely fascinating. What I'm curious about is have you thought about connecting
what you just said in this talk with this other ways of structuring text, namely parts of speech
and sort of the kinds of parsing where I could get a tree-like structure of a sentence. Could I
perhaps see from the network relationship that some things are adjectives and others are perhaps
nouns? Yeah, so that's a great question. Yes, you will notice I mentioned nothing of parts of speech.
On the one hand, that was done intentionally since, you know, GPT's training data is just raw text
and one doesn't have to tag parts of speech. But yeah, once you have this framework, you can ask,
like, can't, you know, chat GPT, give me some examples of adjectives. Give me some examples
of nouns. I haven't tried that yet. Actually, someone should try it or maybe I'll try it after
this and see if it does it correctly. So yeah, does it learn things like that? So I think,
like my answer to Petter, it's hopeful. In fact, there's a philosopher that also we're working
with Juan Luis Gastaldi who has some very good ideas in this direction. So I would say stay tuned
and let's see. Right now, it's again, kind of too early, but these are things that we're definitely
thinking about and looking to go in that direction and some folks, even in our research circle,
have ideas but still work in progress. But yeah, that's a really great question.
Thank you. So we have a question from Tali. The question is, is there an analogy between
matrix algebra and profanctors? Sorry, if there is an analogy between matrix algebra and profanctors,
what would be the categorical analog of higher order arrays or tensor networks?
Yeah, yeah, that's a fantastic question. So just like a higher order array,
so matrix is a function, let me just pull it up, a matrix is a function from a product of two sets.
So a tensor of order three would be a function from a product of three sets. A tensor of order
ten would be a function from a product of four sets. So just like you can take the product of
more than two sets, you can also look at functors from a product of more than two categories.
So profanctors also have higher order array analogs and that's easy to write down. So yes,
so these things called tensor, higher order tensors, they have analogs and category and
they're just kind of straightforward generalization. That's a good question.
So we have a raised hand from Bim.
Hi, thanks for the very interesting talk. I have a very pedestrian question, sorry for that.
At some point you talked about the composition of like in the zero one category of the syntax.
Could you go to that slide perhaps? Because I kind of, yeah here, so here we have like y given x
and z given y is z given x. Normally I would guess you have some sort of a sum over y there.
How does that work? Yeah, so here there's no sum. I'm so when I look on the right,
sorry my mouse for some reason is not appearing on my slide, but when I look at the arrow from
blue to small blue, think of 0.22 is the probability of small blue conditioned on blue.
So it's the probability of seeing small blue given that I have just seen forward blue.
And there's only one way of getting there. So okay, okay, I see. Good, thank you for asking for
clarification. Good. Okay, and this equality because typically I guess for this category
wouldn't actually need equality here, right? Yeah, so we just have an inequality would suffice.
Yeah, okay. Yeah, inequality. Actually, both of those equalities like you just need any
quality, but we happen to get equality here. Okay, yeah, okay, thanks. Yeah, thank you.
So by the way, here in Glasgow we have a big watch party watching your talk,
so there's Mateo Capucci in the room with me who's going to come here and he
is interested in asking a question. Oh, fantastic. I didn't know there was a watch party. Hi.
I had just like, so I know about Disco Cat, which is another categorical framework for doing
natural language processing, and I'm wondering what's the relation at? Yeah, so Disco Cat I think
is inherently looking at connections with quantum physics. So I have not mentioned anything about
quantum here. And also the question that I'm trying to answer is a little bit different. So here
I'm trying to see how can I start with probability distributions on texts? And how can I pass from
that into something that feels like meaning or semantic or something that has something about
knowledge or reasoning, you know, inspired by large language models. So the question that I'm
starting with is a little bit different. And the tools or the sort of assumptions that I'm making,
like I haven't said anything about quantum physics or I'm also here, I mean, I could do this, but I
haven't. I could ask for a representation of all of this information. So I'm just working with
categories, right? But, you know, if I look at a transformer, there's no like category written
down in the paper, you know, attention is all you need. So one could ask, okay, now that I have
all of this mathematical structure, can I represent it? When your algebra is a nice way to represent
things. So can I, you know, represent them by vector spaces? Could those vector spaces then be
tagged with parts of speech, which is kind of like what's happening in Disco Cat, can I then like
make an a comparison with similar structure that appears in quantum. So I'm not doing anything
like that. If I did want to, I mean, I could, and I have a paper on this with Yanis that came out,
I think last year. So you could ask for a representation of this kind of information,
this category theoretical information in terms of linear algebra. And we think that to actually
piggyback on someone else's question, tensor networks are are very good choice for that.
But even then sort of our premise for choosing that is a little bit different. So it turns out that
in, you know, condensed matter physics and quantum physics, they have very nice tools that
happen to kind of match with the statistics of language. But those tools, you know,
can be used outside of the physics context, even though historically they've been used there.
So even then in that work that we're doing, we're not really saying anything like languages,
quantum or, you know, entanglement means this, nothing like that. So I think that Disco Cat
is quite different from this, even though we both happen to be, you know, thinking about
language in terms of category. But the questions we're answering are different, the tools are
different, the sort of premise or the reason why using those tools are different. Yeah,
that's kind of my high level answer to that. Thank you very much. Thank you for the question.
I think this might be a good place to stop. We're a bit over time, but this was absolutely
fantastic. So I'll just say my thanks one more time. And as now we are basically, since we're
done with the main part of the course, really what we're now in cats for AI, as I've mentioned,
we're all the things we're going to be having our guest lectures. So this is going to be on a
regular schedule. And if there's any recommendations you have from people who would love to talk or
who you think could contribute meaningfully here, we'd love to hear about it. And so far we have
two future lecture scheduled, which are which are going to be sometime in March, and we're going to
update you on it as we learn more. I don't know if there's anybody else from the organizing team
that has to say anything. Other than one of those talks will be by David Spivak,
so you should definitely not miss it. Okay, so that's it. Thank you very much. And see you next
time. Great. Thank you all for the invitation for your time. Appreciate it. Thanks so much for
coming, Ty. Really enjoyed it. Thank you very much.

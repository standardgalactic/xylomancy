Thank you all for coming to the tutorial.
If you noticed in the SciPy program,
there are three tutorials doing Bayes stuff.
This is, I think, unprecedented in the SciPy tutorial
program list.
What happened was the three lead instructors, that
is Alan Downey for his tutorial, myself and Hugo
for this tutorial, and Ravine and Colin
for the next tutorial on Bayesian modeling,
we all decided to informally coordinate together
to put this informal Bayes track thing.
And I know that there are a few people who have actually
registered for all three of them.
So props to you guys, that handful of you.
Anyone want to take an estimate on what fraction of this crowd?
Did that?
20%, which would mean about, let's see, 16 people-ish?
15 people-ish?
I think they're about 60 to 80 people registered in this room.
Yeah, so OK.
Yeah, I actually got the actual number from Jill.
It's 15 of you, so it's a good close estimate there.
All right, so for the 15 of you who were in Alan's tutorial,
some of this material will be familiar to you.
And for the rest of you, we'll go through enough background
to accomplish today's goals, which
is to show you how you can write arbitrary statistical
models and use them to perform inference.
That's the end goal of today's tutorial.
And there'll be a tool of choice, which is Pi MC3.
We'll also show some things with NumPy.
Hopefully, that will give you enough grounding
in probability, in Bayes rule, to then be
able to talk about how our data were generated
from a statistical standpoint and build those models
and use them, use them productively.
OK, some administrative matters before we move on.
For those of you who are still not set up,
I know that there's a high probability
that you are set up, set up being defined
as you either have Binder working
or you have everything cloned locally
and working on your laptop or on a remote server
that you control, which is what I'm doing, by the way.
So yes, I know what the feeling is like.
So if you are set up and you're using Binder,
remember every 15 minutes on the clock.
So make sure you execute something in your notebooks
because Binder sessions have a 20-minute timeout.
If you are not already set up, go to the Read Me page.
Go to the Read Me on the GitHub repository.
Look for the Launch Binder button and hit it,
because you're going to not want to waste time with DevOps
at this point.
Cool.
Let's see.
We also have Revin Kumar, who is one of the instructors
for the NextBase tutorial.
He is helping out today, and he'll be back in a few minutes.
We can take a guess at a time that he'll be back.
You'll notice I'm using a lot of these guess the time
estimate, that sort of terminology.
It's very important that we've got both a theoretical view
and a practical view on that.
So hopefully today's tutorial will clarify that.
OK.
Before we go on, do people have any questions, any issues
that they want to clarify, any things that are blocking them
from doing today's tutorial?
OK, if not, we're going to follow the agenda that's
on the whiteboard.
First off, we're going to do a recap on probability.
Use some hands-on exercises to make sure
that you've got a grasp on it.
And then we'll recap what Bayes' rule is.
So for the 15 of you who were in Ellen's tutorial this morning,
this should be familiar material.
We won't do a full, rigorous proof of Bayes' rule.
And everything will just make sure
that everybody has a grasp on that.
And it builds on top of the rules of conditional probability
and joint probability.
So once we're done with that, then
we will move on to what we would call the core activities
of statistical inference, which is estimation.
Estimation is the core of all statistical inference.
Because we're in a Bayes' tutorial,
I'm going to go out on limb and saying calculating p values
is not.
Calculating p values is not the core activity.
It's not even the point of statistical inference.
So let's get that out of our heads.
And then the final thing is we're
going to show how the core activity of Bayesian statistical
inference, which is Bayesian estimation, how that relates
to things that we might have now already learned
in our undergrad statistics or grad statistics classes,
which is different forms of regression and the likes.
And we'll do a short teaser on that.
It's not the main point, because the main thing is really
understanding estimation and how we build models that
help us perform that.
So with that, I'd like to invite everybody
to open up your notebooks, fire up Jupiter,
and open up notebook 1A.
And then you'll notice that they always come in pairs.
So there's a student version and an instructor version.
And what I've decided, unlike my previous workshops,
I'm going to live code with you.
Not from memory, thankfully.
I have a second computer here that
tells me what the right answers are.
You all, by the way, if you ever feel stuck on anything,
the instructor versions of the notebooks are there for you.
And that's the first time I'm going to say it,
also the last time I'm going to say it.
So if you're ever stuck, just open the instructor notebook.
Don't hesitate to copy and paste the answers over.
The point is understanding.
Doing stuff helps with understanding.
But if you get stuck doing stuff,
it can lead to more frustration for learning
than is productive.
So at the end of the day, make sure you just
have a good conceptual view, and that's good enough.
All right, let's run the first cell
and talk about what probability is.
I had a few pop quiz questions just now.
What's the probability that someone in this room,
if you asked, would have a satisfying lunch today?
And someone said, 75% of this room.
That's another question.
What is the probability that someone in this room
has taken all three tutorials, all three Bayesian tutorials?
And that's another question relating to probability.
But I'd like to start by first asking the question a little bit
more meta-level.
What is probability?
You have a volunteer, and I know no one's going to answer.
So what I'd like you to do is talk with your partner
for about 30 seconds to a minute, not 30 minutes.
30 seconds to a minute.
Introduce yourself, network a little bit, and ask,
what is probability?
OK, so hopefully you all know each other a little better.
Those of you who've been in the network analysis tutorials
that I've done know this trick.
This is the networking trick.
Let's you build your network while you're in a tutorial room.
So do we have volunteers to share maybe a stab at the question?
What would you define probability as?
We know some properties of probability, right?
Someone said it must be bound from 0 to 1.
What else?
What other things about probability do you know?
Volunteers, back there, a symbolic expression
of uncertainty.
Would you like to unpack that for us a little bit?
No?
All right, so then I'm supposed to do a stab at that, right?
OK, let me ponder that.
Do we have another?
Something.
Something, OK, yeah, yeah, OK, cool.
Degree of belief, yep, yep, OK.
Mark?
Yeah, yep, OK, right, yep, OK.
Anything else?
Anybody else has a definition they'd like to contribute?
Pardon me?
You had to think more about how it's going to work.
Ah, OK, yeah, all right, all right.
Cool, right, so what I'm hearing are essentially
the classical and the Bayesian, I refuse to say frequentist,
the classical and the Bayesian ways
of thinking about probability.
Hugo, who made the first part of this tutorial,
the material for the first part of this tutorial,
found a really great quote.
And it's inside your notebooks, and you'll see it, right?
By data analysis, by severe and skilling.
And there were essentially, historically, there
was a shift in how probability was viewed.
There was the, perhaps, what we might call the first version
that was formally recorded in European scientific history,
which would be a degree of belief assigned to an outcome.
And then there was then a shift because of world views,
foundational world views shifted.
And so people thought of probability
as more of the relative frequency with which things
occurred, which then gave rise to the name
that I refuse to mention.
And because these frequencies can be measured,
they then seemed to form an objective view of reality, right?
I, on the other hand, don't subscribe to this view, right?
I see probability defined.
So there are formal definitions of probability.
You get into these very technical mathematical terms
that involve spaces and measure theory and the likes.
The way that I prefer to, for day-to-day use of Bayesian
methods and probability, I tend to think of probability
as just being money assigned on a number line.
If I gave you $100, how would you assign that $100
to points on the number line if you were doing discrete things?
Or how would you draw a curve that would say,
assign how you would distribute money to the number line?
Essentially, it's a measure of how much you're
willing to bet that this thing will take on a particular value,
this thing that you're interested in, right?
So you can use money.
Or if you prefer not to gamble, like myself,
then I would just say credibility points assigned
to the number line, right?
Credibility points assigned to the number line
gives us a view of probability that
is a good enough working definition for how we can view
probability in applied problems, right?
So where do we want to then use probability as a tool?
Once again, spend 30 seconds with your neighbors
and talk about this.
Let's gather back.
Let's gather back.
I'd like to hear from those who haven't already raised their hands
and contributed something to the discussion.
Where would you use probability in an applied setting?
All right, at the back.
OK, did you elaborate on that, please?
Right?
Effectively, if I knew everything about the point,
I would be able to elaborate on that.
Yes.
Yeah, OK, right.
So probability is a tool for that.
Next door neighbor.
OK, did you give an example?
Right, so some key questions we might be interested in
would be like, what's the probability of a catastrophic
failure of that piece of infrastructure, for example,
or something like that?
Mm-hmm, right.
So the calculation of, again, the probability
of a catastrophic failure would be really important.
And the probability that the safety mechanism would fail
as well would be also important.
Right, right, right.
So you need to know the probability
that the rate is equal to something
plus the uncertainty on it.
I'm very happy that we're going in a Bayesian direction here.
OK, cool.
Great, so if, say, we were, for example,
in a marketing firm or a new tech firm,
and we wanted to think about click-through rates, right?
That's another place that probability comes into play,
so another applied setting.
And click-through rates are essentially
measured as the probability that a user that arrives
at your page will click on something, right?
So that's another example.
And that's the first example that we'll use here
to get a practical handle on how we
can use the existing tools in our toolkit.
We probably know NumPy.
We probably know a bit of pandas and the rest.
NumPy, MatPyLib, we'll use NumPy
to help us get a grasp on what exactly is probability
and how does it relate to proportions and the likes.
So let's say, let's try this example together.
It's a code along activity.
Let's say we've got a website, and we've
measured the click-through rate accurately
to the ninth decimal place.
So it's 50% accurately to the ninth decimal place.
So what does that mean of the visitors that come by?
So if we had 1,000 visitors, how many people
would we expect to click on that button?
Any volunteers?
500, right?
That's the expectation, right?
We'll try to simulate that.
So if we have one way to simulate this kind of problem
is to use NumPy, and to start, we'll
take a uniform distribution.
We'll start by saying, we've got a whole bunch of people.
We don't know where they came from.
And we'll simulate this process of clicking
by taking 1,000 random numbers, rounded between 0 and 1,
and then cutting a threshold somewhere at 50%.
So we'll do something like np.random.rand, 1,000,
and what this will give us then is if we plot
the histogram of that, ooh, my kernel just died on me.
Cool, all right.
Let me try that again.
We plot the histogram of that.
You should get something that looks like this, right?
So this is what we would call 1,000 draws
with equal credibility assigned across the number line
from 0 to 1.
And to simulate how people click on the website,
we can do a few things.
We can do first ask how many of those values are below 0.5.
And by definition, the rest are going to be above.
And finally, simply sum up the total number of clicks
that we get, right?
So we define clicks as a value being drawn from this distribution
being less than 0.5, and then we simply sum it up.
How many do people get?
I get 481.
What are the others?
4, sorry?
505, 478, et cetera.
OK, cool.
So we've got a variety of answers here.
And this is one of the core ideas behind statistical inference.
That is, there is always some form of randomness involved.
And when there's some form of randomness involved,
we will expect the answers.
So we can calculate this thing called expectations.
And we expect that, on average, from, say, a lot of us
pooling our results together, the number of people
who click will be centered around 500 at 0.5 out of 1,000.
However, because of random effects
and whatever else that goes on in the data generating process,
we won't always get exactly 500.
There's always some randomness involved.
So then we can calculate the proportion that clicked
as basically n clicks over length of clicks.
And I will get 0.481.
And you will get your 0.505s and whatever.
So this is one of the, so one idea
that we want to convey here is that in statistical inference,
we're often talking about random processes, things that
are not deterministic.
There's always a component that we
don't know how to exactly write an equation to model exactly.
And so what we do is we use a statistical model
to help us get around this fact.
There's randomness built in and inherent inside there.
So all right.
So what we did was we said we had a model, so-called model,
of clicking in which we said half of the people who come
will click.
And then we drew samples from that model,
where we had our own, on our own computers,
we had our own instantiations, our own realizations
of that model.
And there's some random effects that are inside there.
OK.
So what we'd like to do now is have you all
do this on your own for the next two to three minutes.
Try to simulate what the results will
look like when you have a click-through rate of 0.7
instead of 0.5.
So spend a minute or two handling this.
If you are ever stuck, you have lots of resources.
You have your neighbors.
So continue networking.
If not, you also have the instructor notebook.
And you have the screen, which I'm
going to be typing on as well.
Cool.
So it'll look something like that.
Once again, not everybody will have the same results.
So some numbers.
Just popcorn-style this.
703, 694.
I have 708.
What else?
687.
OK, cool.
So we have what this forms is a distribution of numbers,
realizations.
Cool.
This model that we've just simulated by hand
is known as the biased coin flip.
We know that the coin flip is the classic example
that every probability tutorial has to deal with.
The biased coin flip is just nothing more than a variant
on that.
Where else do you see biased coin flips happening?
OK, talk with your neighbor, 30 seconds.
Nobody will answer the first time, like on my first thing.
So do we have volunteers?
Where do we see the biased coin flip apart
from click-through rates?
Do we have a volunteer?
Mark?
Sorry.
My example's kind of got limited.
But if you're traveling on a street,
the traffic light will be 70% binary.
OK, so if you looked at a junction,
you can calculate the probability
that the lights are favoring the main artery rather
than the side arteries.
OK, cool.
A friend?
Widget manufacturing.
Can we talk about that?
Can we have an inspection project called
a colon and it's going to be accepted or rejected?
OK, so you have a binary outcome.
Accepted or rejected?
Where else?
Back there.
Is it at birth?
Yeah, biological sex at birth, yes, exactly.
So it's a binary situation for the vast majority
of the population.
So under that approximation, then what
is it, the canonical is like 51% or 52% female, sorry,
and slightly lower for male.
OK, where else?
All right, so I think the point is well described
by these examples.
There's a binary outcome that we're seeking to model.
And the binary outcome sometimes
is merely an approximation.
The binary outcome is a useful, if it's
a useful approximation, then we use that.
We can use what we call the bias coin flip
to model the process of generating the individual outcomes
that we actually observe.
OK, cool.
Let's try a different example.
So this example here was us basically
hand coding the generative process in a very explicit
fashion for what we would call the Bernoulli trials
or binomial trials.
We're going to try a different way, which
is to actually look at real data and treat data
as if they were the population, so to speak.
As if the data that we measured were infinitely large enough,
which is never true, but as an approximation,
we'll start with that and ask, how
can we calculate the probability of certain things
under this assumption of the data being a really good
approximation of ground truth?
So what I'd like you to do is run this next cell.
What this data set that we have here
is Finch Beaks, measured on Galapagos Islands.
So how many of you have learned biology
and have heard of the Finches before?
Right, so there we go.
We have another binary outcome right there.
So in this, someone went to the Galapagos,
research team went to the Galapagos Islands
and measured Finch Beaks, both their length and their depth,
and asked and just recorded what they were in.
Let's assume that this is a realistic approximation
of the population of Finches that were observed.
So let's grab out just the beak length, Th, as a Panda series.
So follow along with that cell.
And so what we'd like to ask is, what
is the probability of a bird having a beak length greater
than 10?
So how would we calculate that?
Well, one way we might calculate that
is under the assumption that the data are the population,
we could just ask what proportion of birds
have beak lengths greater than 10.
So we can do something like p is equal to the sum of lengths
Ths greater than 10 divided by the length of that.
And you should get something close to,
you should actually get this exact number in this case,
because there's no randomness in the data.
So we're not simulating the random process,
we're treating the data as if they were the true thing.
So everybody should get 0.851, right?
Cool.
So these two examples, they're really
here to show you that proportions
are somewhat a proxy for probability.
Under certain circumstances, if you
make explicit certain assumptions,
your proportions can be a good estimator
for the actual probability that we're interested in.
All right, so over here, we can actually try simulating
the finch beak lengths as well.
So we can draw random samples from the data
and use that as another way of estimating
what the uncertainty around that probability would be.
So this procedure is what we would call resampling
with replacement.
I think it's bootstrapping.
My memory is blanking at this point.
But resampling with replacement is another way to do it.
So now if we break the assumption
that the data are exactly what the population are,
then we're faced with a problem.
We're faced with a problem that is the proportion
that we've calculated may not actually
be the true probability of finch
beaks being greater than length 10.
So how do we estimate what that uncertainty might be?
We'll do this hacker statistic sort of method
where we computationally try to simulate random draws
from the population using the data.
So the way we would do this here is to do,
we would first do random picks from the data distribution,
so mp.random.choice, lengths.
So we're choosing from the length pandas series.
We want 10,000 samples from there.
And we want to do it with replacement.
And finally, we'll take the sum of that, sum over all
of those that are greater than 10, and divide it by n samples.
And so you will notice we won't get an exactly the same number.
Question?
Is the range about choice?
Yes.
Is that creating continuous distribution
based on length, or is that choosing from the length set?
It is the latter.
It is choosing from existing elements.
All right?
Cool.
Question?
So there are really only 229 values in that set.
But we're sampling with replacement 10,000 times.
Yes, yes, exactly.
OK?
OK, so these are computational methods.
One of them sort of is an explicit simulation
for the coin flips.
The other is treating data as ground truth.
And then breaking that assumption,
now saying data are not ground truth,
what is the uncertainty in this parameter
we're trying to estimate?
So there are hacker stats ways of handling this.
There's another way that we can deal with probability
and talk about coin flips and the likes.
So coin flips are essentially Bernoulli trials.
Every coin flip that I make has a single outcome
that can take on one of two possible values.
We'll call them 0 and 1, or true and false,
a binary outcome of some sort.
And we can actually take advantage of,
we can take advantage of NumPy's random number
generators, or the probability distributions that
are inside there, to try to simulate this.
So there's Bernoulli trials.
And then if you sum up Bernoulli trials,
you get binomial, binomally distributed data.
So we're going to try this out over here.
So let's set a NumPy random seed.
You can use 42.
You can use 1,607,190, oh, 16 million, sorry,
as your number, your choice.
And we can flip, we can do a few things.
So we're going to try this.
We're going to try simulating a single flip.
Let's try simulating a single flip.
mp.random.binomial1, n equals 1.
And it being the biased flip, we'll do 0.7.
And so if you rerun that cell many times,
you should get a lot of ones, but some zeros, right?
Oh, the seed, my bad.
Thank you.
There we go, yes.
So you will get a bunch of ones and a bunch of zeros.
If you summed up all of those Bernoulli trials,
say we did 10 of those trials together
and treated them as a single experimental run,
so we'll now do mp.random.binomial of 10,
you'll get sometimes 10, sometimes 4, sometimes 6,
et cetera, et cetera.
So now what we're really interested in
is how in a biased coin flip, what we're really interested in
then is what the probability is of getting, sorry,
let me backtrack a little bit, so in a binomial trial,
in a binomial draw or a binomial run,
what we're interested in is calculating the probability,
knowing the probability of getting up to that number
of successes inside, up to that number of successes
for that run, right?
So the binomial trial is basically
defined as n being the number of successes,
sorry, n being the number of total Bernoulli runs
that we've done, and p being the probability of successes,
and it gives back a number, which is the number of successes
out of that n number of trials.
So what values, sorry, pop quiz, what values
can this take on, the result of a binomial trial?
0 to 10 or n in the general case where we know what n is,
right, OK, cool.
So we can actually simulate this.
We can simulate, say, 10,000 experiments
of us flipping coins 10 times and counting the number of times
that we got ahead in each particular experiment, right?
So we can do that by doing mp.random.binomial 10, 0.7 and 10,000.
If you want some clarity, you can actually
use the underscore between your numbers
to make sure you know what you're typing.
So in that, we'll give us something
that looks like this, right?
So what I'd like to ask you then is,
what do you see in the chart?
What values are probable?
Well, values are not probable.
I'm not going to ask you to talk with your neighbor
this time around.
Any volunteers?
So the most probable value is 7, right?
And we know that from the fact that our p, which we set,
was 0.7.
So the expectation or the most likely value,
the thing we expect to see the most is 0.7.
What else do we see?
The upper bound, bounded at 10, right?
We don't see any values at 0, 1, or 2.
What's happening there?
Is it very small?
Yeah, we've done the experiment 10,000 times,
and that's still not enough, right?
It's still not enough for us to be
able to see whether there is a probability,
see the proportion of the probability
see the proportion of trials that we will get one head only
out of 10, all right?
OK, so I'd like you to try the following exercises.
Spend about two to three minutes on them.
The first one is calculating the probability of five
or more heads for a value of p is 0.3, then for 0.5,
plot the histograms for both of them.
Yeah, so spend a minute or two handling that.
Just really quickly go over.
You should get something like this for the first exercise,
something around the value of 0.7655,
something like that for the second exercise, right?
The probability of seeing a heads is higher,
so therefore the probability of seeing five or more heads out
of 20 is also going to be higher, right?
So 99% of those have that result.
And if you plot the histogram, you
should see something like this.
Now, for those of you who had that bit of time
to think about the question, looking at the histogram,
can you tell me what is the probability of seeing four
or more heads?
Pardon me?
It's a rock.
It's a rock, yes.
Sorry.
But a lot is not a number from 0 to 1, right?
So what is that number from 0 to 1?
Pardon me?
85%, OK, maybe.
It's not very easy to tell, right?
Not very easy to tell.
This is the sort of question where
histograms are kind of not the right thing to look at.
It's the thing that we're used to looking at,
but it's not the kind of thing that would give us rich.
It's not the kind of plot that would
give us rich statistical information on the data
that we have on hand, OK?
Question?
Danka, yes, exactly.
So what we want instead is the cumulative distribution
of our data.
And that, I argue in a blog post,
gives us much richer information than a histogram would.
A histogram is nice and convenient
to look at because it sort of tells us where
the central tendency is, but that's
all it really can tell us what the central tendency is, right?
Maybe the bounds.
But even the bounds aren't going to be shown very accurately,
as we would see from this histogram up there, right?
So this is where the cumulative distribution of our data
comes into play.
And we can plot what we would call the empirical cumulative
distribution function, the ECDF, of our data.
And the ECDF is a great way to visualize this, OK?
So if we take our data and we arrange them, well, actually,
let's code along, let's code along,
and you'll see what the ECDF will look like for this,
for binomally distributed data.
So X flips and Y flips are going to be ECDF of our data,
which our data is X. What it will return
is it'll give us indices on the x-axis, which
are where our data points fall.
And then it'll give us an index on the y-axis, which
is a number from 0 to 1 that tells us
how much of our data falls below that particular data
points value, OK?
So let's do that, and we'll do plt.plot,
X flips, Y flips, marker is a dot, whoops,
line style is none, whoops, I forgot to run that.
Now, things look a little clearer, right?
So for those of you who have the histogram on your screen,
keep the histogram on your screen,
and compare it to the ECDF that's on the projector screens.
Now let me ask, what's the probability
of getting 4 or higher in our data?
Something percent, yes.
OK, so here's how you look at it.
You go to 4, you go up to the bottom of that bar-like looking
thing, and then you draw a line across to there,
and it falls roughly at 0.2, right?
So the probability of getting 4 or higher is approximately 0.2.
And the thing about ECDFs is 0.8, sorry, thank you, 1 minus.
We're doing the higher, thank you.
So the thing about ECDFs, though,
is that it gives you much richer statistical information.
You can tell things like the central tendency.
So what's the central tendency?
Well, we go to 0.5, draw a line across,
see where it hits the data, and look on the x-axis,
and that value is 5, exactly.
What are the bounds of the data?
Exactly, and take a look at that.
We couldn't see the 0 in the histogram,
because it's kind of like obscured by the height of the rest.
One thing that's cool about the ECDF
is that it uses all of the data.
There's no binning bias that can obscure the values
that your data can take on.
That is a serious problem.
You can lie with histograms.
You do not want to lie with histograms.
I will come after you, OK?
OK, cool.
So I made my case, and I rest my case with ECDFs.
Don't ever use histograms again.
Always use ECDFs.
They give you a much richer view onto your data.
You can look at all of the percentiles of interest.
You can visualize, sorry, all of the percentiles
that are relevant in your modeling problems using ECDFs, OK?
OK, so we did a little recap on probability
just to make sure we're all clear and we're on the same page.
Probability is credibility points assigned to the number line, OK?
When we plot something like this, we're
saying there's lots of credibility points assigned
to this value.
There's very little credibility points assigned
to the tail values, OK?
That's all probability is a working definition
for our purposes.
We can simulate draws from a probability distribution.
We did that multiple ways.
We can simulate it from data by doing sampling with replacement.
Now, what we're going to do, oh, sorry, and finally,
we can actually take advantage of the exact analytical form,
analytically implemented probability distributions
in NumPy and SciPy stats, and use that to help us simulate
what our data might look like under a set of fixed parameters.
When I was learning Bayesian stats,
this activity was really, really helpful for getting familiar
with the shapes of probability distributions.
And as you'll see later, the shapes of your probability
distributions, particularly the bounds, the central tendency,
and how they're skewed, can be really useful pieces of knowledge
that no longer are just trivia in your head,
but actually can be useful tools for modeling.
What we're going to do now is do a very, very, very, very quick
run through of the different probability distributions
and what their so-called stories are.
Probability distribution.
Oh, well, let me backtrack a little bit.
How many of you have heard of the term
generative models of data?
Yeah, those who've been in the deep learning world
will know that there's this term called generative models.
And frankly, at some companies, like the one that I worked at,
it's been overhyped quite a bit.
Everyone wants to talk about generative models.
At its core, generative models are just
how we can generate things that look like data.
And if you think hard and long about it,
probability distributions are generative models of data.
Probability distributions are generative models of data
because we can construct a model that
is composed of purely just probability distributions
and use it to simulate data that looks like actual data
that we might collect.
So let's think about what actual data we might collect,
say, starting with the Poisson distribution, right?
So Poisson processes and the Poisson distributions.
That's a generative model, and it's
got a story that's behind it.
And this concept of what is the story
behind each and every probability distribution
is something that we need to make sure we're familiar with.
So Poisson distributed data basically
can be thought of as something like this.
This is borrowed from David McKay's book.
I have a town, and it's called Poissonville.
And the buses, they're kind of like MBTA trains in Boston.
So they come, and then sometimes you
have to wait a heck of a long time before the next one comes.
And sometimes the next one comes right after them, right?
They're the one that just came by.
Yeah, and then sometimes they get derailed and, well,
too bad for Bostonians.
So the amount of time that you wait for one train or one bus
is independent of the amount of time
that you waited for the previous bus, right?
So that's the story of a Poisson process.
And so the timing of the next event
is completely independent of when the previous event happened.
And so it's not just faulty trains on the red line in Boston.
There's other things, too, right?
There's like births in a hospital.
There's meteor strikes.
God help us if we have one.
Aviation incidents, the rate of things happening, right?
Number of events happening per unit time.
So that's what a Poisson process is.
And the number of arrivals that occur in a given amount of time
takes on a Poisson distribution.
So all that the Poisson distribution is modeling
is just how many things happen within a given unit of time.
So we can actually simulate that.
So if we want what we can do here is we can simulate Poisson
draws, mp.random.poisson, with per unit time six events
happening, so six natural births per day,
or six collisions at Brigham Circle near Harvard Medical.
And we'll do like 10 to the power of six.
Let's do a million draws, right?
And then plot what this distribution looks like.
PLT.hist of samples will just set the bin size
so that it's a convenient thing for us to visualize.
And you'll get something that looks like that, right?
Does the shape look kind of familiar?
Not that you've seen it before, but like some other distribution?
The binomial, did I hear?
Yeah, what's up with the binomial?
Yeah, so this one isn't exactly symmetric, but close enough.
Yes?
Yeah, so there's a neat relationship
between the Poisson distribution and the binomial distribution.
That is for low probability of successes.
That is for really, really rare events that happen.
The Poisson distribution is the approximation, or the limit,
as we go to low probability of success
and large number of trials, right?
So there's a relationship between these distributions.
We can actually, let's plot the ECDF of this as well.
So ECDF of our samples that we drew.
And then we plot this guy again.
And you should get something that looks like that.
Once again, this tells us a lot of information.
What is the central tendency in this case?
Six, yes, exactly.
So the central tendency, I'm using this term
as the more generalized thing, right?
So the central tendency can be either the mean, the median,
or the mode, depending on which one we're talking about.
The expectation is the mean.
And the expectation of the Poisson distribution
is the parameter that we passed into it, right?
So if we say that things are Poisson distributed
with six events per unit time, then
we expect to see six events.
The central tendency will be six.
The median will be six.
The mean will be six, OK?
All right, so something that's also Poisson distributed
would be field goal attempts per game.
Yesterday over lunch, a bunch of us
were talking about how football, and I'm
sorry to the American football fans, that's hand egg,
football, real football that is played with your feet.
That's sort of at a rate of a field goal rate
that's not that good, whereas hockey
is at the right field goal rate, whereas basketball
is at some absurdly high field goal rate.
So we're gonna do an example taken from professor,
an instructor at Caltech, Justin Boyce,
who I've met at the SciPy conference.
And it's about field goal attempts by LeBron James, right?
And he did, that's his data, that his stats per game,
number of things that he, number of times
that he shot in one game.
So the unit time is one game, and we're asking
how many attempts did he make, OK?
I need to reconnect my VPN.
Pardon me, OK.
So let's move on.
So we've got the field goal attempts here.
All right, that's OK.
It'll come back when it comes back.
And we'll do the ECDF of this data, all right?
Cool.
And finally, what we're gonna do is we're gonna do
many random draws from a Poisson simulation,
from a Poisson distribution, and plot all of those random
draws, the ECDFs of those random draws to see whether
it follows the same distribution as what LeBron's
field goal attempts would be like.
So let's code along.
So for underscore in range of that,
samples is np.random.poisson,
np.mean of field goal attempts,
and the size is the length of field goal attempts.
And in this case, we'll plot the ECDF
of the theoretical samples.
And let's see, my kernel's disconnected.
So I'm going to have to reconnect and rerun cells.
And since we're at it, I'll just bring up the instructor
version so that we have it there.
You should get a plot that looks something like that,
right?
Okay?
And so with that guy over there,
that's one probability distribution story
that we can talk about, the Poisson distribution.
Now the Poisson distribution is a discreet
probability distribution.
What's the other class of distributions?
The continuous family, right?
Okay, so the discreets, when we plot that,
they technically follow a probability mass function
because there's a bulk of mass that's associated
with each particular value.
That is how credibility is assigned.
With continuous distributions,
we have the probability distribution function,
which technically have zero mass at any point
on the x-axis, right?
But they take on, they have a density of mass,
so-called, from within a range of continuous values, okay?
Okay, we're going to skip one or two,
we're going to skip the exponential distribution.
I'll just leave it out there for you
that the exponential distribution is the waiting time
between Poisson events.
And so that's that, and so you can take a look at,
you can take a look at how the CDF,
ECDF of the Poisson exponential distribution
will look like on your own time.
We'll go through the normal distribution, okay?
The normal distribution, everyone's familiar with this,
right?
Things are normally distributed,
a lot of things, sorry, are normally distributed, okay?
We've got measurements, and in this case,
we'll just take a look at what the story is.
The story is, in this quote here,
when doing repeated measurements,
we expect them to be normally distributed,
owing to the fact that many sub-processes,
lots of individual data-generating processes,
contribute to this final thing that we measure, okay?
So things like human height is a culmination of many genes,
so it's kind of reasonable,
it's a culmination of lots of genes,
plus environmental effects put together,
and so it's reasonable to expect that human height
would be normally distributed, or approximately so, right?
So, yeah, so the formulation,
one formulation of the CLT, the central limit theorem,
is that any quantity that emerges as the sum
of a large number of sub-processes
tends to be normally distributed,
provided that none of the sub-processes
is very broadly distributed itself.
That is, it doesn't overwhelm
the final data distribution that we're looking at, okay?
So, just to have you all take a look at this,
these are what we would call,
these are measurements of the speed of light,
and there's, in an experiment,
lots of factors contribute to what measure,
what value we eventually measure, okay?
So, there's some data on the speed of light,
and the estimate of the speed of light
tends to be normally distributed
because of this data-generative process, right?
Lots of things contributing to this final thing.
There's error in the instrument,
there's error in, there's the instrument itself,
and it's got some error.
There's the measurement technique,
and it's got some error.
There's the conditions of the day
that we're measuring and that that can affect.
That's the reason most biologists tend to use.
Something happened that day for a failed experiment,
and I know it myself, because I was one before.
So, yes, you get the point, though, right?
So, there's a culmination of factors that lead to this.
Okay, now, I wanna leave, before we go on a break,
I wanna leave you with this little tidbit,
which comes from Alan Downey's blog post.
Are your data normally distributed?
I encourage you to look at that blog post,
because though something,
though we impose models on our data,
our data may not necessarily be
exactly following that model that we've imposed on.
So, when we make an assumption,
or when we impose this idea that our data might be normal
and we do a fit,
we check how deviant our data are from normal distributions,
we use that normal distribution
and later downstream things
for estimating the mean and the likes.
When we do things like that,
we're imposing a model,
and that model might be wrong,
but it can be useful, right?
This is a classic George Box quote.
I'm gonna mash that with George Orwell
saying that some models are wrong,
but some are more wrong than others, okay?
And we might be, find some of them useful, right?
So, take a look at Alan's post, ponder about that point.
We'll come back at 2.45 p.m.,
there should be snacks inside the Tejas room.
We'll come back to in notebook one B,
and very quickly go through joint
and conditional probability, okay?
This is a last minute decision that I made
in that in the interest of time,
there's some ideas I wanna cover.
We'll cut down a little bit on the foundational exercises,
but then we'll have the concepts given to you all.
So you have the tools to do your modeling later on, okay?
So, as I mentioned in the interest of time,
we're gonna skip a few exercises.
The next notebook is on conditional and joint probability,
and really I want to give you all
mostly just the tools that you need
to be able to think through the problems at hand, okay?
So, one of the tools in the toolkit
is the distributions and the stories
that are associated with them.
I drew this out as a table,
but by the way, there's a really great resource
by Justin Boyce who put all of the distribution stories
that are relevant on his Caltech website.
So that's a really, really great resource to go and check,
and I'll make sure that's also linked
on our GitHub repository,
so you can always check that out from there, okay?
So, just to recap, we've got probability,
they follow, we've defined as credibility
assigned on the number line,
we have ways of simulating probability distributions,
both analytically and by brute force computation.
Really, the core thing that we want to have in our toolkit
is actually the probability distributions
as well as their shapes and their stories, okay?
So, this is a very incomplete, highly partial table
of the many probability distributions
that exist out there,
and I wanted to hint that you should be building
such a matrix for yourself,
and it is really helpful,
like you can maybe have a cheat sheet
written by someone else,
but I always find it like if I do the hard work
and draw it out and write it out for myself,
it's also really useful.
Nonetheless, I'll be building a resource
that basically 100% plagiarizes Justin's resource,
but gives added nice visuals for learning benefit, okay?
But really, so the toolkits are,
you want to know the names of these distributions
because they aid in communication
with other people who do statistics.
When you're writing things out,
you'll want to know what their abbreviations are, right?
So, that guy highlighted over there,
because that gives a nice and compact way
of telling people what distribution you're using
when you're writing stuff.
In your mental model,
you'll want something like the shape of a distribution,
right?
A uniform is just equal credibility between two bounds.
So, that picture should come into your head.
You should know whether the bounds are relevant
to your problem or not,
and also whether equal credibility makes sense or not, right?
Just so that you have something live,
there's a variant or the generalization
of the uniform distribution,
which is the beta distribution.
It takes in two parameters,
number of successes, number of failures.
It's actually bound between zero and one explicitly
and takes on values that can look like that
or can look like this
or can look like that, right?
So, but the key point is that it takes on values
that are bounded.
There's this term,
which took me a little while to remember,
but it's called the support of a distribution.
This is something that you'll see
inside the statistics literature.
The support of a distribution is nothing more
than the values that it can take on,
the values that it can take on, right?
That's all it is.
So, the support for the beta distribution
is something like a, whoops,
zero to one, it's bounded between zero to one, right?
And there's a story for the beta distribution,
which is number of successes
expected fraction of successes.
Out of n success plus n failure, okay?
And it can take on, the alpha and beta parameters
can take on not just integer,
but also floating point, decimal numbers, okay?
So, you'll wanna have this kind of picture in your head
when you're thinking about that.
Where can you learn?
Again, I'm saying Justin Boyce's website is a great place.
I learned a lot of these probability distributions
and their shapes and their possible values
by looking at the PIMC docs.
PIMC three has a great thing for that.
But you'll notice, in PIMC three,
basically all they're doing is just,
all we're doing in the docs is just simulating
those distributions and plotting them out.
So, that really is the best way to do it.
All of that hands-on simulation that we just did,
that is a great, great way for you to learn
what the probability distributions are,
what their shapes, what their data-generating processes
are all about, okay?
So, I wanna start with that.
That's one thing you wanna have in your toolkit.
The next thing you'll want to have in your toolkit
is this idea of joint and conditional probability, okay?
And so, once again, in the interest of time,
by the way, for those of you who just got back,
make sure, and using binder, execute something
so you don't lose your session.
The way I think about joint, conditional,
and marginal probability is by a visual
that looks something like this.
If I have data that are jointly distributed,
they might look something like that.
Say, this is a bivariate Gaussian.
This put together is what we would call
the joint distribution of the two things
that we're interested in X1 and X2, okay?
Then there's a thing called conditional distribution.
That is, what is the distribution of one of the two axes
given that we know something about the other, okay?
So, if we use red to do the conditional distribution,
this is a known value of X1, oh, sorry.
This joint distribution is denoted as P of X1 and X2, okay?
So, in red, we're gonna show you
what the conditional distribution looks like.
So, the known distribution looks like that.
Oh, shucks, I hit the off button.
If we take that joint distribution
and project it back onto the X2 axis,
it itself will follow a distribution, okay?
We're okay with this, right?
This is the distribution of X2 given that we know X1.
This is what we would call the conditional distribution.
Okay?
And this is denoted probability of X2 given X1
where that little thing over there is given, okay?
That pipe tells the statistician
that you're computing the distribution
of X2 having known a particular value of X1, okay?
Are we okay with this so far?
Okay, there's a final idea,
which I'm hoping you'll keep,
which is known as the marginal distribution.
Marginal distribution looks like this.
There are two distributions over here, okay?
This in blue
are what we would call the marginal distribution.
Why is it called the distribute marginal?
Well, first off, it's on the margins of this two-axis thing, okay?
It is the value of X1 ignoring whatever values,
ignoring whatever value that X2 is, okay?
So it is the value of X1
completely ignoring the other variants that are of interest, okay?
So this is denoted as P of X1,
this is denoted as P of X2,
and those are marginal distributions, okay?
So prior to drawing this out for myself,
this was something that I wasn't really able to
keep straight in my head,
but this served as a very,
like what I would call an anchoring example
for what these three terms mean,
joint, conditional, and marginal probability.
Now why are these three terms really important?
It's because it's from joint and conditional probability
that we make our way,
joint, conditional, and marginal probability
that we make our way to what we call Bayes rule, okay?
Bayes rule, if you've seen the famous neon light photo,
is written as P of A given B
is P of B given A times P of A over P of B.
Where does this come from?
Well, this comes from the definition of joint probability.
So P of A comma B is equal to P of A given B times P of B,
which is equal to P of B given A times P of A.
And if you simply isolate out this portion
and move this guy over there,
then suddenly you have Bayes rule, right?
And this is a neat thing because it gives us a way
to move between probability of data given model
or in probability of model given data
if we take an alternative view of what we're doing, okay?
And to illustrate this example,
we're actually gonna use the drug testing example
in notebook one B, so I'd like to invite you
to navigate to there, okay?
I'm gonna plug this back to my laptop.
So the drug testing example is one of those classic things
where we can come up with a solution to a problem,
but if we don't do the stats right,
we'll be kind of off, all right?
So I hope you'll see this from this point.
So let's look at the question, right?
So we have a test, it's 99% true positive for drug users
and 99% true negative for non-drug users.
What that means is if I give you a drug user
and then I ask you to do the test on that drug user,
then 99% of the time it will be correct.
And if I give you a non-drug user and I ask you
to do the test, then 99% of the time
it will be correct as well.
So it sounds like a great device, right?
So what I'd like to then ask you to do
is before doing any of this computation,
write down what you think the probability will be
that a drug user is, sorry,
a positive testing user will be a drug user.
Put down a number, it's gotta be from zero to one
because it's a probability.
Put down a number mentally in your head.
Yeah, it's in the notebooks.
So 99% true positive results for drug users
and 99% true negative results for non-drug users.
Okay, it's in your notebook.
So scroll down on notebook one B.
Write down a number and when you're done,
give me a thumbs up.
Don't think too hard on this one.
You're meant to be surprised.
Okay, so we can actually simulate this whole process, right?
Because we know, let's switch back to here.
We know a few things about the data generating process.
And we can actually represent this as a tree.
That looks ugly.
That tree might look something like this.
Someone's a user and someone's a non-user.
And then we do the test
and they turn out to be positive and negative.
Positive and negative.
Now, help me fill in the blanks.
What is the probability that the user tests positive
given that they are a user?
99%.
And so by, sorry, I'm gonna use 0.99 here.
And so what should the value be on the negative arm?
0.01, very good.
And then what about the non-user?
0.01, right?
So if the user is not a drug user,
1% of the time they will test positive as a drug user.
And 99% of the time, oops, I'm being inconsistent,
they will test negative.
So this sounds like a really good test, right?
What we're interested in, however, is P of drug user
given positive because this guy here is P of positive
given drug user.
So how do we calculate this?
Well, we can simulate it.
Let's come back to the notebook.
Okay, so if we take 10,000 subjects
and we simulate,
sorry, let me backtrack a little bit.
In order to solve that problem,
we're missing one piece of information.
That is, what is the probability that a user
is a drug user, a person is a drug user?
We have the people, persons group,
and then we divide them into drug user, non-drug user.
So some may say that if we're in Central,
West Virginia, then OPR crisis is like ravaging there, right?
And it's a black stain on the pharmaceutical industry
for that, and so we might put an estimate
that five to 10% of the population are drug users.
Now, if we're in clean, clean Massachusetts,
then what might we believe about this fraction?
It might be 10 times smaller, say 0.05,
or New York City, right?
Like, here goes from New York, or Sydney,
actually, that's his hometown.
So 0.05, right?
Let's, sorry, not 0.05, 0.005, right?
So let's simulate how many drug users we will get
under that particular regime.
So we'll do mp.random.binomial.
We'll have 10,000 users, okay?
We have a probability
of them being, the probability of them being
a drug user is 0.05,
and we only want one trial.
And so the non-users is gonna be
n minus the number of users, right?
Number of non-users in our population
is just the complement, okay?
We run that cell.
Oh, yeah, all right, I need,
so, of the users, how many of them will test positive?
Said 99%, but it's a probability,
so we'll explicitly simulate it.
mp.random.binomial again, we have the users,
the number of users in the population,
and 99% of them will test positive.
And then we'll have the number of non-users,
but we also wanna know how many of them
will test positive, right?
So how would we simulate that?
mp.random.binomial non-users,
and what's the probability value inside here?
0.01, right?
Because we're only interested,
we're interested in given that you're positive,
what is the probability that you are a true user
or not a user, right, of this drug?
So what fraction of those tests
will be positive for users?
What do we need to divide by?
We need the sum of non-users and users,
sorry, non-positive and positives.
Or rather, non-users that did test positive.
Gosh, the naming is tough.
That's the hardest thing in computer science, right?
If we calculate that,
you'll get something like what, 0.3-ish?
You all get that?
Is this surprising?
Pardon me?
It's a big number, yeah.
Like, we thought we could get away
with like a 99% sensitive and 99% specific test.
But it turns out because the thing
that we're really interested in
is inferring the thing that isn't shown from the data.
The thing that isn't shown from the data
is the latent thing, that is argue a drug user or not.
And then that shows up in the drug test,
but the drug test has some probability of error as well.
So if we go back and think about that tree that we drew,
that's the full data generating process.
That is the full data generating process
for the things that we observed.
And now we can use that to back infer
the probability of the thing that we're interested in
rather than the probability,
so the probability of data given
the underlying condition or the model,
rather than the, sorry, probability of the model,
underlying condition or model given the data
rather than the thing that is easy to simulate.
The thing that's really easy to simulate
is probability of the data given my model of the world,
but what if my model is wrong, right?
What if my model needs updating?
And so that's where this Bayes rule thing comes in handy, okay?
All right, now if you look at how this is solved
with Bayes theorem, the equations are in the notebook.
Take a look at that.
What I'd encourage you to keep in mind though
is we're interested in the probability of our,
probability distribution of our parameters of interest
given the data, right?
When we're talking about modeling our data generating process,
we're gonna stick in parameters like P, right?
Probability of success or lambda or mu, you know?
Lambda for the rate of a Poisson process or mu,
the central tendency for a normally distributed thing.
But we might be wrong and we need to update our model
having seen new data and that's where Bayes rule comes in,
okay?
So we're gonna look, what we're gonna do next
over the next two and a half hours is to look very,
sorry, two hours is to look very in depth
into two particular data generating stories
that can be applied across multiple places.
So in some senses, these are fairly generic models
that you can take home and use in your modeling work.
But we're gonna go really deep into each and every one
of those and so pardon me if you are already quite familiar
with this but I think it's handy for a few reasons.
One, you'll have the mechanics of PIMC-3
which is the tool that we're gonna use
under your toolkit.
You'll also have the process of telling a data generating
story in your mind as well, okay?
So, and we'll have practice with that, okay?
So we're gonna skip notebook number two
and instead move to notebook number three directly, okay?
So I'd like to invite you to open up notebook number three
and this is where we jump right into what we would call
probabilistic programming and Bayesian estimation.
These are probabilistic programming, oh, sorry, go ahead.
Yeah, let me put that up.
Yeah, yeah, yeah, definitely, definitely, definitely.
So, let's write this out, okay?
Bayes' rule states that P of x1, comma x2,
the joint distribution, sorry,
Bayes' rule starts from this formulation.
It's P of x1 given x2 times P of x2
which is equal to P of x2 given x1
times the probability of x1, okay?
All right, so if we say probability,
so if we do the rearrangement of terms,
then we get x1, well, let's make this fit the example
that we're looking at.
Probability of x2 given x1 is therefore equal to
P of x1 given x2 times P of x2 over P of x1.
Are we okay here so far?
Okay, it'll be up there.
I'm unfortunately restrained by the size
of the screen here as well, right,
in order to fit enough inside here.
So, let me use the pencil to illustrate what this is.
This is the marginals, okay?
These are the marginals.
This is the conditional, okay?
This is what we're interested in.
Are we okay with that?
And what we're interested in is also a conditional,
just to be clear.
We're interested in the red distribution.
However, we also need, in order to know the red distribution
that's up here, okay?
In order to know that distribution,
we actually need to know this distribution
that I'm highlighting, the probability of x1 given x2,
but integrated or summed over all possible values of x2,
which therefore, let me switch mics,
which therefore means we're doing some form
of summing over or integration over all horizontal slices
of our data, okay?
So, just to make this a little clear,
we're saying this top term up here
is basically this slice, plus this slice,
plus this slice, plus this slice,
all summed up, all multiplied together, okay?
That's what we're doing.
And that's how that formulation based rule
relates to this picture that we've drawn
for conditional, marginal, and joint probability, right?
Okay, so I'd like to have you all open up notebook number three.
So we're gonna do probabilistic programming,
and we're gonna start with the coin flip story, right?
The coin flip story is way too classic,
but it's really one of those anchoring examples
in my mind, all right?
So if you mind, I'm gonna go ahead and do a little bit
of the coin flip tables in my mind, all right?
So if you master the complexities
that we can build on top of it for the coin flip story,
then you'll grasp a lot of concepts
that will be use, are usable across multiple
different types of models.
Okay, so we're gonna do estimation.
Like I mentioned, one of the core activities
of statistical inference is estimation
of the parameter given data.
Notice the given, right?
There's a conditional, is that we're jointly modeling
our data and our parameters together,
and we're saying, given that we've observed data now,
what's the distribution of parameters that we've got, okay?
So go ahead, run that first cell.
The first thing that we're going to do
is we're gonna actually look at some,
we're gonna look at click-through rates.
Again, the classic binomial thing,
rather than coin flips, coin flips are a little too boring.
So let's look at click-through rates
by loading this cell, ooh, I lost my kernel again.
You know what, if y'all don't mind,
I'm gonna switch over to the instructor notebook
and not code along, but I'll make sure
that you all have enough time,
because I have all the write-out puts
in the instructor notebooks.
Yeah, so notebook three, so last night I posted on our Slack,
do a new Git pull.
So make sure you, if you haven't done that, do a Git pull.
So that should hopefully clear up the confusion.
By the way, for the tutorials, get up on the Slack,
because it's a useful channel for the instructors
to one way communicate information
and the other way get back questions.
All right, okay, so let's look at the click-through rates data.
Okay, you all have that loaded.
Cool, oh, good, my thing's working now.
You should get data that looks something like this, right?
So we have, we've done this test.
We have a case and a control,
and we've measured for every single visitor to our website,
whether they clicked on the button
within a fixed period of time
or whether they just decided to leave, okay?
So how then do we use PMC syntax to build a model
that helps us estimate the true value of P
with its uncertainty for this data?
So if you didn't have PMC three, what might you do?
You might do a data frame dot group by, right?
So you might do R dot group by group dot mean,
something like that, oh, well, okay.
Yeah, I need to run the cell.
You might do something like that,
and if someone runs that code, what number do you get?
While mine's running, it has to connect to the kernel.
Can someone do that?
Oh, CTR, my bad.
Click-through rate.
Here, you'll get something that looks like this.
You'll get like a 0.14050 for one group and 0.19125,
click-through rate for the other group, right?
And how much would you believe that data?
Maybe true, maybe not.
Depends on how our data were split between the control
and the test group, right?
Usually, we do a random splitting,
but in this case, some malfunction happened,
so we only had really 200 data points out of 1,000
for one of the groups and 800 for the other.
So we don't have 50-50 splits.
We have 80-20 splits, so one of the groups
is going to be smaller.
Oh, it's actually 2,800, so it's even worse.
It's not even one of those nice round numbers
that we can think about.
So cool.
So we've got this skewed amount of data.
Which number do you believe in more,
given that you know that the test group only has 800?
You believe the control number more, right?
And that's because we've got more measurements for them.
All right, well, what we're going to do
is we're going to spend a bit of time thinking about what
the data-generating process looks like for this kind of model.
So let that come up.
What does the data-generating process look like for a Bernoulli
or a binomially distributed data?
This, by the way, is the exact workflow
by which I go about every single problem.
So we've got the question, what is the data-generating process?
Well, we start with the data that we have on hand.
We have Bernoulli distributed data.
So we'll say the likelihood follows a Bernoulli distribution.
OK?
We're going to just estimate for the control group.
So one of the two groups.
We're not going to do a two groups just yet.
We'll build it for one.
So this parameter p, however, how is this distributed?
OK, so we've got the value p.
How would we model that?
Well, if you've never seen click-through rate data before,
how would you assign credibility points to the number line
to correctly model p?
Talk with your neighbor for a minute or two.
This time I mean it, like really talk with your neighbor.
I hear some things crystallizing.
Do we have volunteers?
What might you believe about how would you assign credibility
points to the values that p can take on having never
seen click-through rate data?
Uniform what?
0 to 1.
So let's try that.
We'll say then that p is distributed 0 to 1 uniformly.
This being the likelihood means we're actually going to,
sorry, let me backtrack a little bit.
What we've just drawn here on the screen is one generative model
for our data.
One generative model out of many possible models
that we might want to build.
This thing that I've highlighted being the likelihood
is the thing that we have observed.
I wish Ravine was here.
I would have him copy this model onto the whiteboard.
Give me one moment.
Let's see.
Okay, so we got the model on the whiteboard.
I'm going to switch back and we're going to see how we can
actually build that model with PIMC code really easily.
Okay, so code along, I believe this is code along.
Since we're only doing the estimation for the control group,
we're not going to worry about the test group just yet.
How do we write this model?
Well, you can write it this way.
PM.uniform, right?
It kind of looks very, very close to the picture we drew.
We'll call this variable P.
It needs to have an explicit name.
So rule of thumb is whatever you name it in your variable,
as the Python variable, just give it the exact same name.
Such that PIMC can recognize this.
Lower is zero.
Upper is one.
Okay.
And then the likelihood would be PM.burnouli.
We'll name it likelihood, like P is equal to P.
The Bernoulli distribution only has one parameter.
It takes only one parameter in.
It's called P.
What is it?
It's distributed uniformly, having not seen the data.
Okay?
How are we doing?
Okay, so far, syntax, mechanics.
And then what we do next is we say observed is inside our data frame.
We have it inside our data frame.
Control-DF clicks.
Okay?
So that means we've observed a sequence of ones and zeros,
whether the user has clicked or not.
That is the data that we have observed for the Bernoulli distribution.
Okay?
So this is one way of writing that model.
I'm going to give you all, how are we doing with this syntax so far?
This is the mechanics part of building a model.
That said, if you think back to what we drew on the whiteboard just now,
the syntax looks very similar, right?
The syntax here looks very similar to the syntax,
but the syntax in code looks very similar to the syntax,
the thing that we drew on the whiteboard.
The pictures are much easier to reason about, right?
We can draw our data-generative process here and directly translate it into code.
So that's one way of observing it.
There is another way, another formulation, right?
And if you remember what I said just now, the distribution, sorry,
if you remember what I said just now,
this of Bernoulli's is binomially distributed,
which what that means then is we can actually write the model as a binomial likelihood,
but we'll have to change a little bit about how we do,
how we structure the data to be input, okay?
But I'm going to just give you the binomial model as is,
and you can try to break the model or break how we structure the data
so you get a better feel on how things should work.
Let's run that cell.
The next thing that we do, having written our model
and telling PMC what we're conditioning on,
is we say within this model context, sorry, Model 1 Bernoulli,
please sample from the posterior 2,000 times.
Give me 2,000 draws that describe how we expect the parameter P to look like.
It's a simulation, MC-based Monte Carlo-based simulation
of what the posterior will look like.
Now, this is kind of unnecessary for a simple coin flip model,
but when we go to slightly more complicated hierarchical versions,
you'll see that this comes in really handy, okay?
Run that cell and do the same for the binomial.
It's literally PM.sample.
Literally, that's all you need to do.
What's happening here?
Fancy math is happening for lazy programmers.
That's been the motto of PMC3.
Extract away the math, the fancy math,
that is MCMC sampling or variational inference,
and allow users to focus on building generative stories.
Really, the place when doing PMC modeling,
the place that we need to keep our focus on
is in the place that I've highlighted up there in the cell.
That is, what is the model definition?
Is it Bernoulli?
Is it binomial?
I'd like you to run those two cells.
If you run those two cells and you plot the posterior distribution,
you'll get something that looks like this.
How do you do the posterior plotting?
You do RVs for which Ravine is one of the lead developers.
It's a visualization tool for taking a look at the results of MC sampling.
To be kind to those who still haven't broken out of the histogram land,
I need to run all cells above.
I'd invite you to run that.
You should get something that looks like this,
that's happening on the left-hand side of the screen.
You'll then get a posterior distribution,
a view on what we believe about the value of P,
having explicitly stated what we believe prior to seeing the data.
Yeah, for now, I'm not going to go into that.
That is something slightly more advanced,
and that's the contents of Ravine's tutorial.
If you're not attending it, then catch the tutorial online,
because that's where they'll go into a little bit more of the theory
behind the Monte Carlo sampling that goes on.
For now, we're going to ignore those errors.
What I want to give you is the mechanics of writing the model,
and the mechanics of sampling,
and the mechanics of visualizing and interpreting.
Do the same for the binomial model.
You should get something that looks like that.
The values range from 0.125 to about 0.155,
the 94% highest posterior density.
All that says is that there's 94% of the credibility
is assigned within this black bar region.
That's what we believe is true.
Question?
So we did hear, if I understand it.
So the area was the uniform.
Yep.
And then we showed in all the data,
and then we had to set out the data frame.
Yep.
And by MC3, we computed the posterity.
Yep.
Just sampling about the time.
Yep.
Exactly that.
Okay?
All right.
So what I'd like you to do then is you've basically got
the template that you need to now replicate this
for doing two groups within a new model.
So I'd like you to do is the hands-on activity
below in which you build a model that does both estimations,
one for the control group and one for the test group,
and spend about five to 10 minutes such that you get up
to this point where you get this plot.
So go ahead and do that.
Question?
Oh, okay.
I'll come and address it.
The rest of you, go ahead.
Yes.
It happens when you're doing the sampling.
Ah, okay.
So, oh, okay, okay, okay.
Give me a moment.
I'm blanking right now.
All right.
All right.
So I'm going to give this to you that it is the math is being
executed when you hit PM.sample.
But really what happens underneath the hood is we've
computed a joint likelihood of all the parameters with the data.
And then we use MC sampling to figure out what the typical set
of the posterior is.
And the goal here is to sample around the typical set.
That is the typical range of values for each of the
parameters that are involved.
All right.
Okay.
So how many of you are done?
Thumbs up.
We got a bunch of people who are still working at it.
So if you're stuck, you should have something that looks like this.
You can do it with the binomial.
So I'm encouraging you to try it with the binomial rather than the
Bernoulli.
But if you want the Bernoulli, I'll code it live for you all.
Okay.
Okay.
Oh, I see.
I hear fans running.
Someone's sampling real hard.
Thank you.
So if you're interested in what the Bernoulli formulation will look like,
it'll look like this.
The binomial formulation looks like that on the right-hand side.
All right.
Okay.
And then if we're going to sample from the posterior,
once again, fancy math happens for lazy programmers.
All right.
Okay.
If you do this final thing, which is a deterministic transform of your
random variables, you can actually explicitly compute the posterior
distribution of the difference of the two Ps.
And this would be akin to what you're trying to do if you were to do a T
test of sorts, right?
Where basically you're just comparing two means and asking how overlapping
are they.
Are they overlapping or not?
A binary decision.
Are they completely overlapping or partially overlapping or completely
non-overlapping?
Is one greater than the other, right?
We're asking questions like this, basically.
So I'll show you how to do that.
You can do pdiff is pm.deterministic.
And it's nothing more than math on probability distributions.
So we'll say ptest minus pcontrol.
We'll define test minus control as the diff, difference between the two.
And rerun that.
And what we'll get is a posterior distribution on the difference of the
two parameters.
So I'm really tempted to ask this, but in a T test, would this be
significant?
So yeah, okay, sure.
Now next question.
Would the T test be appropriate?
Why?
Well, there is an n if we're doing, so that it's not correct is, not the
right thing to do is the correct answer, but I disagree slightly with the
reasoning.
We do have multiple n's.
We have multiple observations here.
So that's accounted for.
Let me ask you about the, back there.
Maybe power calculations are one thing that I had a long Twitter thread on.
Thankfully it's not a rant, so you can read it.
I'll post that.
Yeah, so let's see.
Can a probability be normally distributed?
No, why?
Right, right.
Exactly, exactly.
Now we can approximate it with a normal distribution, but we have to be
extremely clear.
That's an approximation of what already is an approximation.
A model is an approximation of the world.
We're putting another approximation on top.
Whoa, okay.
All right, so this is where knowledge of the shapes and the support of the
distributions comes in handy.
P is a probability.
It can never take any value below zero or above one.
So why would you impose a normal distribution on the probability
parameter P?
Especially now, we don't need to do any math.
We don't need to write equations out and solve these equations that tell us
what the posterior will look like having seen data under a normal
approximation.
We don't have to.
We can explicitly sample from the posterior using MC simulation.
So why not go whole hog and just use probability distributions that are
bounded from zero to one?
Right?
Okay, cool.
Great.
So you all just had my, I did a talk at PyCon.
It was my 25 minute rant on why we don't always do the t-test, why we
should go away from canned statistical procedures.
This is one example of it.
P is not normally distributed.
P might look normally distributed, but it is definitely not normally
distributed.
It's got to be bound.
So you can't take on a probability distribution that is bound from negative
infinity to positive infinity.
If you do that, you're wrong.
Okay?
Cool.
Great.
And on this hypothesis testing thing, think about, there's a great blog
post by Alan Downey, again, that talks about the fact that every single
classical statistical test boils down to one framework.
And if you go Bayesian, there's really no reason to calculate p-values and the
likes.
You just look at posteriors and look at the posterior distributions of statistics,
single-valued statistics, single distributed, sorry, single-variate
statistics that you're interested in.
You don't, you don't have to worry about p-values here.
Okay?
Are you all right with that?
Cool.
Cool, cool, cool.
Great.
Now, knowing that the probability difference is, knowing this probability
difference is, is all good, good and useful, but it's, it's still not tied to
something that is real world and interpretable in the minds of our
executive friends, right?
So how do we take this metric that we've calculated, p-diff, and turn that into
something that matters?
Well, one thing that you might learn from Raveen's tutorial if you're going or
watching it later is that there is this idea of a loss function or a cost
function that we can attach to these things of interest.
And I'm going to just show you a very simple example, okay?
Let's say we know one thing.
We've computed with other data and said that our customers on average spend 25
US dollars if they click and zero US dollars if they don't click, right?
Like there's, there's money attached to this, this process.
How that money is attached to this process we can always write an equation that
describes ours.
We'll, we'll write one arbitrary one so we don't have to go too deep into what
it is.
If you look at what's in the trace for p-diff, you'll notice it's, it's nothing
more than a sequence of 2,000 draws from that posterior, okay?
So run, run that, run that on your, on your own notebooks as well.
Just open up and view what p-diff is.
It's an umpire array.
It's got 2,000 numbers, okay?
So a difference in probability can translate into a difference in amount of
money being spent.
And if we attach a single, you know, a single dollar value to each and every one
of those rather than a distribution, that's not complicated for the moment.
We can do something like um, dollar distribution is trace of p-diff times 25
times 1 million, right?
Over a million customers, um, there's an increase in the probability that they
spend money.
So under the test group.
And so how much increase do we expect over a million customers?
Let's translate into some real numbers.
We'll do that.
And finally, you should get something like that.
You should get something like that.
So what does this say?
Well, on average, we expect a lot of revenue increase, but there is always this very,
very small tail probability that we still might lose money.
And that's just the nature of our computation, okay?
So I just wanted to put that out there.
You can always tack on, this is highly custom per problem, but you can always
tack on a loss function or cost function that describes that ties the metric of
interest.
You have to think about it to some dollar amount or some hours spent by people on
a particular problem, okay?
And that's a way of communicating to non, so-called non-technical folks who want
to be maybe a little bit more spoon-fed on what we expect to see, okay?
Are we okay with this so far?
So far so good.
Any questions?
Okay, if there are no questions, I want you to talk with your neighbor for one minute,
one new thing you learned.
Anybody want to share?
Something new they learned thus far.
Yep, yep, that's right.
That's the point of this first exercise.
Get you familiar with that mechanics of doing so.
Anything else?
Back there?
And that's the default sampler.
Yep.
Yep.
Yep, yep, absolutely, absolutely.
Yep.
How so?
Yep.
Oh, gosh.
So, yeah.
Yep.
Oh, gosh.
Yep.
Right, right, absolutely.
And so that actually brings up a very highly related point.
We've built statistical models of the world.
We've built statistical models of the world.
When I built the cost function, it was both somewhat of a hybrid of a statistical model
and a mechanistic model of the world.
That is, people who click will now spend money on average $25 per click.
And so we do some multiplication and that expresses some mechanism.
But ultimately they're models and we have to validate them.
And so that problem you brought up is, I think, one of the problems that we don't know how to validate properly, I think.
Because we don't have the negative data to build a good model.
We have all the positives, the successful molecules.
We don't know what the opportunity cost and how to model the opportunity cost of those failed, you know, incorrectly modeled molecules would be.
Yeah, that's a very good point.
Anything else?
Okay.
If there are no other points, you'll notice.
All right, we have the case and the control group.
If you listen to many of my rants, case and control isn't the only thing you can do.
You really can do, like, arbitrary number of groups without having to worry much about multiple hypothesis correction and the likes and, like, you know, having this guillotine of p-values which you chop off as you go down.
Oh, gosh, that's like, I don't know how people came up with that, but it's complicated, right?
Whereas, like, just looking at posteriors is so much more clean, much easier to interpret as well.
So we're going to do another example that's still the binomial story, still the Bernoulli binomial story.
But it's going to show you how, like, we can't just copy past a test control player one, player two, player three, player four, player eight hundred and fifty seven, right?
Or write a for loop with functions, and that's just, like, way too much.
We can actually take advantage of some syntactic things that allow us to write in a very concise fashion these models that model multiple, you know, more than two groups, okay?
So, like y'all to scroll down, we're going to look at baseball data.
And this is one of the classic, classic, this is one of those classic data sets that one would pick up, right?
We want to, we want to model the probability that a player who is a batter will hit a pitch, right?
So they have this stat called at bats and number of hits.
At bats is the N, the total number of times that they've come up for batting.
And H hits is the total number of times that they've actually hit the bat.
So we've got some data from the baseball database, all credit to them.
I'd like you to run this cell that loads the data.
Little pitch for a tool that I've been developing alongside colleagues here, so Zach in the back, he also works on this.
It's called Pyjanitor, and it's basically there to help you make data preprocessing easy to read, so that you, you have a clean API for cleaning data, all right?
So we've got the data, I'd like you to load that.
You should, you should see something that looks like this, okay?
We've got at bats, hits, the salary of the player, and this extra column which we will use, it's called player ID underscore encoded, okay?
It's basically just an integer encoding of the player ID, nothing more than that.
You'll find out why that becomes handy later.
So, once again, it's the same old Bernoulli binomial, sorry.
Now, because we have the data structured as at bats and hits, which is the, what should we, what is the likelihood that we want then?
Is it Bernoulli or binomial?
Pardon me?
Binomial, and why is it binomial?
Yes, because we know exactly how many times, we're not recording every single bat, we're summarizing, we're taking the summary statistics, which is the total number of at bats and the total number of hits,
and this then forms the binomial story in which we know the number of times something has come up for trial, something has come up for a test of whether they succeed or not, okay?
This is distinct from the Bernoulli, where we only know that they're coming up, and we just, we only know that they're coming up for a success failure trial,
and we know the probability, but we don't have the number of times that they've got that, okay?
So, we'll be using a binomial distribution as a, we'll be using a binomial distribution as the likelihood.
So, let's build this model, and what I want you to do is to notice some syntactic changes here, okay?
So, first we'll have the pitch model, we're going to build that.
I'm going to switch over now to a beta distribution, and beta distribution is also bounded from zero to one, it has the correct support.
A beta distribution also allows, has this parameter where, two parameters that let us control the shape where it's skewed, is it skewed left or is it, does it have more mass on the left or density on the right?
It lets us do all, it lets us do that, right? It lets us control the shape of the distribution, so it's not just a simple flat uniform prior.
So, code along with that, P is the name of the thing, alpha is one, beta is one, and then the shape of this, this is a new thing.
The shape of this beta distribution is the number of players that we have.
So, all we're expressing now is that we've got, instead of a single beta distribution that lives in memory, we've got a vector of beta distributions, one beta distribution per cell, right?
And that beta distribution maps onto one particular player, okay? That's, that's what we're doing here.
The likelihood is binomially distributed, so PM.binomial, its name is like, its P is distributed according to the beta distribution, but it's a vector of distributions.
So what this effectively does is it creates a vector of binomials, okay?
That's the key syntactic difference and conceptual difference that you need to take away from this example.
You can get away with doing, you can get away from for loops by simply vectorizing everything, okay?
N is data at bat, right? This is the number of times they've come up, and it's also a vector that is the same length of P.
And finally, observed is data hits, okay?
Ah, my, my line wrapping is coming into play. Maybe I shouldn't set this in the Jupyter Notebook.
Okay, now we got that, but since we have salary information, we've been talking about like, we've been talking about tying things to real world numbers, right?
Let's compute a metric that says the probability of batting per unit of salary, right?
So how, how we want that number to be as high as possible, right?
So we want for the smallest salary, someone, we want to figure out for the smallest salary, someone who bats has the highest batting percentage.
That would play right into the Saber Metrics kind of thing, right? You're looking for value for money on different metrics.
So we'll do a deterministic transform.
We'll call this P per salary, PPS, okay?
And simply take P and divide it by the salary that we observe in the data.
How are we doing? Any questions so far?
So far so good? The following long story, it's, by the way, the exact same data generating process, the exact same story as we drew on the whiteboard.
Go ahead, do sampling then, and look at, look at the posterior distributions.
So notice how we're sampling. It's fast.
I'm taking advantage of the GPU that I have at home.
This one in this particular case, I don't think it'll run any faster on the GPU than on, on a CPU because there's no complex matrix multiplies that are going on.
But if you were to do like more complex things, complicated things like a Bayesian neural net, which you actually can write in Tiano and I MC three, there's totally no problem with that.
Then moving stuff onto the GPU can get you up to four to four to eight times faster sampling and fitting.
Okay, so we have built here. Oh, I named it trace batting.
Let me resample again.
So what, what I've done then below is create this custom visualization that relies on IPI widgets and the likes to get it working.
The intent here is that if you were to visualize posterior distributions for 805 players, that's 805 matplotlib axes that you're drawing to screen.
It's not the prettiest thing. Okay, so we'll take advantage of some interactivity that we can build to. Oh, it's not working on my screens, but it should be working on some of yours.
If you've got IPI widgets working, you'll get a select that you can actually look at and select multiple players and compare their P per salary and their cumulative distributions for each of those.
Actually, while this is running, I'm just gonna very quickly get up and running.
And so while you have that visualization up, I'd like you to try to hunt for the player that's got the highest PPS distribution.
And while I install some, while I reinstall some things, once, once I'm finished with the commands, we'll come back and talk about that.
Okay, so anybody found interesting players?
Yes.
That's right. So one thing that we have as an idea in Bayesian statistics is that in the limit of lots of data, what your priors are really don't matter unless, unless you chose priors that can never change.
That means you're just hard coding your great uncles, your uncles viewpoints on politics, which will never change, right? So, yeah.
Maybe I just need to reload.
My, my, my, my. It's very slow.
So do we have players that are that look interesting?
Pardon me.
Okay.
Can you help me out a little bit? What does their CDF look like?
Sure.
Exponential being, sorry.
Exponential being what?
This, this, this, this line that looks like that.
Okay, some look like this, right? You'll have noticed some of that. Any other patterns?
Yeah.
A squiggly but more or less straight line. Okay, let's talk about what each of these CDFs express.
What's this one expressing?
Yeah, essentially it's expressing that the credibility points are assigned anywhere from the lowest to the highest in pretty much a uniform distribution.
Okay. What about something that looks like this?
How tight is the distribution compared to the first one?
Very tight. And it's also very shifted to the right, right? Because this, the central tendency is way out over there.
Okay. And then what else do we have? We have this guy over here. What's this guy like?
Something in the middle. Something in the middle. Still takes on lots of values, but the distribution is kind of skewed as well.
Right? Because there's a, there's a long tail on the left. Lots of, lots of probability or lots of credibility assigned to the middle over here.
And then it peters off at the top. It'll always peter off at the top.
Once again, it's, it's the richness of statistical information. So PDFs look a lot like CDFs.
Sorry, PDFs look a lot like histograms. I take that back. PDFs look a lot like histograms. And so you can't tell more than the bounds and central tendency from a PDF.
Whereas you can tell quartiles and percent, percentiles and roughly estimate where they are from a CDF.
So most of the time I would just default to using a CDF rather than a histogram or a PDF.
Okay, let's see if this works. My widget doesn't display. Never mind. You guys have, you all have the widget on your laptop, so you'll be able to view that.
Okay, so there are some interesting players. One thing that's kind of interesting though is that we've got players like the uniform distribution one, right?
Super uninformative. And that raises a problem. The problem sounds something like this.
Now I'm going to pose this as a question to you all. So think about it. Having, having seen players, professional players play and do their thing.
Do you really expect that having a few at bats, those that comes from the situation where we have like one or two at bats and zero or one hits, do you really expect that having seen that one or two at bats.
We should still believe that their batting capability is at this near uniform from zero to one. Talk about that with your neighbor. And then tell me why.
Yes, so do you really, should we really believe having seen one or two at bats that performance would still range so wildly from zero to one, basically?
What are your answers? Should we really believe what we saw in the posterior results?
That performance gets basically, is still uniformly or close to uniformly distributed? No, I see a head shaking. Tell me why.
Both of you, both of you are a team.
Yeah, so what we're encoding here is this notion. So the response is pretty much how we would think about it.
Professional baseball players don't tend to have performances that range so wildly. That is a piece of prior information that we can impose on the modeling problem.
So how do we impose that? There are two ways. One, we can have a stronger prior, a stronger prior that is imposed on every single player.
So we might do a beta, so if batting averages tend to fall within the range of point two to point three, we might put a beta distribution with, so point two is about approximately one success and four failures.
So it would be a beta distribution of one and four. That is a slightly stronger prior. Or if we wanted it to be even stronger, we would do a beta distribution of 10 and 40, which is much narrower compared to the beta of one and four.
If you don't believe me, go simulate it in NumPy. The beta distribution is right there. And if you really wanted to be a really strong prior, then it would be beta 100, 400. That's the point two batting average kind of prior we could put on here.
So the advantage of using the beta over the uniform is that I can now tweak the alpha and beta parameters of the beta distribution to change where we center the distribution and also change how wide or thin that distribution is.
So as I was mentioning, beta one four looks something like this, skewed, but kind of wide. Beta 10, 40 looks something like this, and beta 100, 400 looks something like that. Are we okay with that?
Yeah, so beta distributions give us a little bit more control over the shape of the distribution. Okay, so putting tighter priors is one way. Another way to approach this is actually to impose what we would call a hyper prior, one that governs the population of players.
So I'm going to switch over to drawing again.
We have the binomial likelihood. And we know this already, right? This is a very familiar story for us by now.
In the interest of saving space, I'm not going to draw out the distributions, but picture them in your head, okay?
N is known. P comes from another distribution.
The way I want you to think about this though is because we've vectorized everything.
We've got a vector of binomial likelihoods and a corresponding vector of beta distributions for priors on the P parameter.
If we were to do this hierarchically, what we are effectively doing is asking what is the population alpha and beta look like?
That's kind of ugly.
And we only have one of these each. We don't have a vector of them because they're governing the entire population.
So when we do this hierarchical thing, we're effectively expressing this idea. Players themselves are drawn from a parental distribution.
Professional players all generally follow some general distribution that is governing the performance of the individual players.
So the population distribution, which is imposed on A and B, that governs the individual player's performance parameter, which is the beta distributed thing,
which then influences the outcomes that we're interested in. Let me just annotate that.
Up there we have the population parameters.
We have the individual parameters for P.
And then we have the likelihood, which governs how our outcomes, the data that we observe, are generated.
Okay? Are we okay so far?
Now, I'm going to go on a limb and tell you that A and B, these two parameters, they also have distributions because then otherwise it wouldn't be probabilistic.
So now the question is, what is an appropriate distribution for A and B?
What might be an appropriate distribution for A and B? I'll tell you a few snippets.
A and B govern the number of successes and failures effectively for the population of players.
This happens over a single year.
It can only be positive, so it being over a single year means there's generally a finite number of positives and successes and failures, A's and B's that every player has.
So given that you know that, what might be a suitable distribution? Let's not worry about the shape of the distribution just yet.
What is a suitable distribution?
For San maybe, it being, I forgot to say, A and B can actually be continuous.
And that's sort of a giveaway that for San's not so ideal, but we could try it.
What's a positive bound continuous distribution that you might have heard of?
Exponential is one. There's another one that we can define, which is the half normal.
It's the normal distribution, but chopped up in half such that now the support is defined only on the positive half.
That's one way to do it. Gamma distribution is another. Gamma is actually, I think, a generalization of a number of child distributions.
It's got more parameters. It's a little bit more complicated, but yeah, we can totally do that.
Log normal, I think, is as well. Yeah.
Half student T, et cetera. Yes, definitely. So we're all on the right track.
We're thinking of distributions. The key point is these distributions have to be positive bound because A and B can only take positive values.
If we were to do anything with the full normal distribution, we'd be doing it the wrong way.
So for simplicity's sake, let's assign A and B to take exponentials.
And the only reason I would start with this, but maybe not end with it, is that exponentials are easy.
They have a single parameter, so there's not much hyper-hyper parameters that we have to worry about.
This A and B thing that we're assigning distributions on, these are what we would call hyper-priors,
hyper being an added dimension, an added dimension of modeling that we're doing here.
So for the sake of simplicity, we'll start with a simple exponential.
What did I do in the real thing?
In the actual thing, we've used 1 over 29, which is just an arbitrary number, which we can always debate about, but we're not going to today.
We're going to do that in a long modeling critique session that my colleague Zach and I have done,
umpteen times now, where we debate our priors, debate the model structure.
For now, for learning purposes, we'll just stick with that.
So let's take this model and code it up.
I'm going to switch back to the notebook.
I'd like to encourage you all to also switch over.
Inside the notebook, you will see that we've already got the binomial likelihood and the PPS metrics, the deterministic transforms defined for you.
So now I'd like you to code along and let's fill in the rest for the beta distribution and the A prior and B prior.
So we have alpha is A prior, beta is B prior.
The shape is still length of data.
And then we'll have exponential 1 over 29.
Oops.
Make sure it's all floating points.
And let's call it hierarchical baseball.
So once you've coded up the model, go ahead and sample from it and tell me what you see is kind of different.
You'll notice also this model is a little slower to sample from.
Okay, so while you all are waiting for models sample and finish up, questions.
Yes, no problem.
Well, while things are sampling, it's actually a great time to talk with your neighbor about something new you've learned.
Okay.
Before we go on into something new you've learned, I want to ask a few questions about what you're observing about these baseball player posteriors having seen this having been fit under this hierarchical model.
What's different from what you saw before?
Okay, so you see more sigmoidal type of distributions rather than uniform or exponential types.
That's one good one.
What's another property that you're observing as well?
What are the bounds and ranges?
Pardon me?
They're more tight.
They're more tight.
And this is the result of this type of switching over to a hierarchical model rather than using an independent model.
So the first model that we wrote where every player is modeled as a beta distribution on its own, that is what I might call an independent model.
And then the hierarchical model actually sort of pools these player properties as being drawn from one parental distribution.
So they're sort of constrained by the parental distribution.
This is a property of hierarchical models.
I'm not going to design a value judgment on whether this is always good or always bad.
It depends on the problem.
And if it is justifiable by your modeling domain expertise, then a hierarchical model is actually a really powerful way to borrow information from players that have had lots of, from the population of players to do inference on the players that we have not had much information about.
So for those players that had one at bat and one success, what you will notice is that their posterior distribution is still, is going to be kind of wide, not as crazy wide as it was before.
It's going to be kind of wide centered roughly around what the population mean is and follow roughly what the population mean is as well.
So for something that's a little bit more extreme, like seven at bat, seven hits, then you'll get something that's shifted to the right because there's a little bit of information saying that this player is kind of good or maybe lucky, we don't know.
There's a bit of, it'll be shifted to the right, it'll be narrower, but it does express that, you know, it's not going to be wildly like 90, 90 something centered on 90 to 100%.
It's going to be centered off, shifted off.
At least in this setting, it correctly encodes our intuition that players generally fall within this like population distributed, they follow the population distribution much more than we would expect from just looking at them independently.
So this is a very powerful thing, that phenomena where you have these wild estimates being shrunk towards the population mean is called shrinkage.
Shrinkage is a term you'll want to look out for in the literature.
So you have this vocabulary that you won't be confused by.
Question.
Yep.
Ah, okay, okay, cool.
So the beta that we've put in there expresses a prior that is unconnected to any other player.
So the prior for the beta distribution, the A and the B, the priors that we put, they are, even though they were point estimates, they're just saying, this is the shape, we're putting an identical shape of distribution on every single player.
Now when we connect the players by saying they all draw from a population distribution, it's not that we're putting uncertainty.
I would be hesitant to say that we're putting uncertainty on A and B, rather we're expressing that their shapes are now controlled by a population shape, right?
That's where this shrinkage comes in.
It's not that we were really sure and so we assigned a single point value.
It's more that the two point values give one shape, but that shape for the beta distribution was unconnected to the population at first.
That's all it was, okay?
And now when we have a connected set of shapes, right?
So we have connected beta distributions by the hyper priors that we put on.
Now what we're saying is that there is a population shape for the beta and they're influencing and governing the individual player shapes, the beta distribution shapes.
Does that make sense?
You don't get population shrinkage by ironically putting a distribution on it.
Not putting a ring on it.
One player, one player.
Yes, yes, yes.
So if in doubt, don't put a ring on it, put a distribution on it, okay?
Cool.
Right, so that's that phenomena of shrinkage that I wanted everybody to have some intuition about, okay?
So let's now go into something that you've learned.
We've actually come to the end of the binomial story and we've gone really, really deep.
We've come from like the simple naive one group to two groups to now vectorizing over multiple groups to then now adding on a hierarchical model on top.
I'm hoping it's not yet information overload because there's more.
So what's something new you've learned?
And let's get that like etched in your head.
You have volunteers.
Sure, yeah, cool, great.
That was part of the point.
Anything else, something new that you didn't expect or something that has been resonating with you?
Maybe on this side.
This side has been really quiet.
I'm going to point at someone.
Second last row middle guy.
Anything new you've learned?
No, okay, still processing.
That's completely valid.
That's totally cool.
And the fact that you're still not sure means there's processing going on and I fully appreciate that.
How about in the middle?
Anybody else?
Any volunteers?
Sorry, can you say it louder?
Okay, so reinforcing the value of the cumulative distribution plots, right?
That's super important.
The fact that we get richer information from that is very useful.
Okay.
Oh, the beta distribution.
Ah, yes.
The fact that we're able to constrain in shape.
Yes, yes, exactly, exactly.
It's a very useful tool to have in the toolkit.
Yep.
Cool.
Cool.
Awesome.
And back there.
Last one.
Yeah.
Oh, yeah.
Yep.
Yep.
Yep.
Yep.
Absolutely.
There are lots of good connections there.
So a lot of the classical stats are connected in this way.
Yes.
Yeah, I was hoping that this question would come up and trust me, I did not plant her
in the crowd.
So when we think about which distribution to use, there are a few rules of thumb.
Okay.
The first rule of thumb is find something that has the correct support.
That is absolutely crucial.
If you use something that's got the wrong support, that is, you've got data that showed
up negative, sorry, you've got data that are showed up negative, can take on negative
values, but you put a positive only distribution inside there, you're going to get not a number
errors inside sampling.
Right.
And that also means that you've not, you've missed something in the modeling process.
So getting the support correct is the first step.
And the next step is to think about the likelihood.
Right.
And that's where knowing, so that's for any arbitrary distribution.
Getting the support correct is absolutely crucial.
The next thing is to think about the likelihood function.
How are the data that you are interested in, the thing you've actually measured?
How is that, how is that distributed?
So that's where knowing the probability distribution stories comes into place.
Essentially rules of thumb are if you've got something that's got amount of stuff happening
per unit time, it's Poisson.
If you've got trials that are positive negative, it's Bernoulli binomial.
Right.
These are very generalizable stories.
If you're really unsure, you might start with a normal distribution.
If you've got some other types of processes, so for example, the negative binomial distribution
counts the number of failures until a success.
Right.
So knowing this generative story helps as well.
Okay.
So, and then also there's this family of distributions called the zero inflated distributions.
So you can have the zero inflated Poisson distribution.
What it expresses is that there's, there are two processes at play.
There's a process that generates lots of zeros.
And then there's a process that generates the Poisson side of that.
And there's, there's, this is essentially a mixture model.
So you're now having to infer both the probability that it is in the zero versus not zero, p and one minus p,
as well as the Poisson parameter, the rate parameter of interest.
And it, it, it's really important to think through the, that part of the problem.
So that's the second part.
And then the third rule of thumb is to think about what the shape of the distribution should look like.
So this is where I would then look at the PDF rather than the CDF, because this is all analytical,
because then it gives me a sense of the skew and the central moments of the distribution.
And it can help me express quantitatively what I'm thinking about.
So the beta distribution is that classic anchoring example that I always come back to.
It's bound from zero to one, and it's therefore suitable for a probability parameter.
I can tweak whether it's centered on point five, point two, point nine,
by simply tweaking the A and B parameters.
And I can tweak how, how tight that distribution is by doing, you know,
beta nine one versus beta ninety ten versus beta nine hundred one hundred.
So there are ways to control the shape of the distribution that way.
Those are the three rules of thumb.
What I, in practice, find myself doing is thinking about the problem going like,
ah, yeah, I need something that's positive here, because that can only take on positive values.
So then I'll go hunting in the distribution library for something that's positive.
And most of the time, we're sort of expressing, you know, say for a standard deviation parameter,
we're expressing the fact that things generally are not going to be wildly,
standard deviation parameters are generally like tight, but then sometimes can take on high values.
So I might take like a half-coachy, because standard deviations can only be positive,
but I'm allowing for really high tails, or half student T, for example.
Okay? So that's a few examples.
Back there.
Yeah, so the exponentials, you can, sorry, so choosing an exponential,
you can think of it as this is the first model I'll write,
and then if you go to Ravine's tutorial tomorrow, there's this whole business of model comparison
that's sometimes you'll find it doesn't really matter what the hyper-prior is,
and sometimes it does matter when we check things like the information criteria metric,
that the information that's contained inside the model,
sometimes you'll find whether it's half-coachy or exponential just quantitatively doesn't really matter.
So in some senses, start with something, and then run with it, and then be ready to change the model.
Yep, yep.
And I emphasize that we don't want to really get into debating that choice,
but we can actually offline debate that choice if we want.
We can look at how the lambda parameter controls the shape of the exponential distribution
and whether that expresses qualitatively what we're intending to express.
So the exponential distribution generally starts high and then goes low.
If you increase, I think if you increase the lambda parameter in quantity, it'll become more and more flat.
If you decrease it, it'll become more and more closer to zero, centered on zero.
So that's sort of how, and then we'll have to ask, is that what we want to express in the model?
Okay?
Yeah, yeah, yeah, yeah.
Yeah, both from a mechanical standpoint, that is like I have fewer things to worry about,
and from, I guess, parsimony standpoint is like a simpler, it's a simpler model, right?
We don't have that many knobs to turn, right?
Yeah, cool.
All right, let's see, it's 440 right now and we end at 530.
So I'm debating what we should worry about next.
So what we would have done, sorry, so what I originally planned was to go through one more example
of how we do Bayesian estimation, this time not with binomial stories,
but with student t distributions and normal distributions.
That's one thing we could work on, or we can first jump to arbitrary curve regression.
So I'm intentionally setting this up as you can fit any curve with pi and c3, not just a line.
Like linear regression is what we're used to, that's kind of boring.
So let's go in and fit a different type of model.
What would you prefer?
So let's do a vote and I will estimate the probabilities.
How many of you want to do the curve regression?
Raise your hand.
How many want to do a second estimation?
Okay, so we'll do the curve regression.
If we have time, I'll come back and show you a few things live demoed rather than interactive coding on the second estimation thing.
So with that, I'd like you to open up notebook number five.
Notebook number five is all about arbitrary curve regression.
Let me see if I can connect in here.
Cool, so curve regression, I'm going to put this out here.
Curve regression is nothing more than estimation with equations.
So we're going to use a radioactive decay data set to sort of reinforce this point.
So we know linear regression.
You have something y is modeled as a function of a linear combination of your x's.
And so you can have the thing we're really interested in is like the w's, the weights, or the m's if you're from physics, y equals mx plus c.
The m's if you're in physics or the weights if you're in stats.
And the bias term as well.
And really nothing should stop us from just thinking about linear regression as the only form of regression that we're interested in.
You can do all sorts of regression.
You can do Poisson regression, whatever.
You can do neural net regression.
If you know how to write neural nets, you can do, in this case, exponential decay curve regression.
So we're going to see whether we can from noisy radioactive decay measurements back infer the correct parameters that help us identify a radioactive material.
So I'd like you to run that first cell where we load data.
Okay, you'll have something that looks, data that looks something like this.
What's on this data?
Well, it's got time on one axis, and then it's got activity or radioactive, you know, Geiger count things on the y-axis.
Well, this is synthetic data, noised out for educational purposes.
All right, so if you plot the data, you should get something that looks like this, right?
Everybody got that?
Okay, so given that we're in this radioactive decay sort of scenario,
I'd like to ask you to think about what equations can we use to model this data.
We'll get into what the statistical model is in a moment,
but I want to first think about what equations we can use to govern this model.
There's an exponential decay equation.
What are the parameters of that equation?
You have time, which we've observed, part of what we've observed.
What else?
Half-life, the decay constant, right, and anything else?
Sure, we're ignoring that for the time being, but yes, if we were to be fully mechanistic, we would do that.
What else is there?
Offset.
Offset, so is that the first offset or the baseline offset?
Baseline offset, and then there's one more which governs the original, the starting point, right?
So there are what?
A, C, and tau.
We have three parameters to estimate for this curve.
And you'll notice the readings are actually kind of noisy as well, right?
And that's because there's, you know, measurements are not always perfect.
There's going to be some amount of noise that we've got to deal with.
So how do we do that?
Well, we've got to build a model.
I'm going to switch back to drawing.
We've got to build a model that lets us link the x-axis component, which is the time component, to the y-axis thing.
We've already said that the equation is y is equal to a times e to the negative t over tau plus c, right?
The c term we can interpret, it's sort of like systematic bias in our measurement.
The a term we can interpret, right?
The a term starts, is the starting radioactivity.
And the tau term we can also interpret, it is the characteristic half-life of this radioactive element.
Pardon me?
Ah, yes, so we're going to talk about noises.
Oh, Siri, goodness.
Yes, that is very good.
And let's add in this, plus epsilon.
But epsilon is not one of the mechanistic components.
It's a statistical component.
And that's why I've drawn it in red, or a different color, right?
It's not part of the mechanistic part that we're really interested in.
So, let's see.
We're now going to take this equation and we're going to build a statistical model around it.
What is a good prior for a?
Sorry, what is a good distribution for a?
Something positive, yes.
All right, and what's the simplest positive distribution that we can think about?
Pardon me?
Yeah, it's like the first thing we measure, right?
Yeah, so it's a bit like an impulse that way.
So, we might say a follows some exponential distribution.
Let's just start with that, because it's a simple one.
What about tau?
What values can tau take on?
It must be positive, yes, yes, okay.
So, let's say exponential.
And what about C?
This is systematic bias, not the noise in the data.
Gaussian.
Systematic bias could be positive, could be negative, yes, so it could be normal.
Could be.
Ah, great, thank you.
Instead of normal, what will we do then?
We might do exponential, sure.
What about the likelihood, though?
Pardon me?
Why would it be put on?
We're measuring counts, yes, however, at least in the data, we've got it as continuous right now because of the noise in the machine that reports back a continuous value.
So, what might we do?
Let's cheat a little bit.
It's an approximation on an approximation.
We'll use a normal distribution here because the range of values for which we've got data are tight enough and far enough from zero that essentially at the tails of our normal, we don't have any much really credible credibility points assigned there.
So, these are like the struggles that we wrestle with, with every new modeling problem that comes in.
Is a normal distribution likelihood reasonable?
Is it correct?
Probably not.
Is it useful?
Maybe.
Alright, so I want to get that in your head.
Yeah, likelihood is the thing that you're observing about the data, right?
So, what do you mean by unit then?
Right, right, right, right.
So, if you look at, I'm going to detour a little bit and talk about linear regression.
So, you have y equals mx plus c.
We might write a model that says m is normally distributed for whatever distribution parameters.
c is also normally distributed.
y is the likelihood of the data.
It's got noise, and if we assume that the noise is Gaussian noise, then we can impose a modeling assumption that says that this is normally distributed where the mu is equal to mx plus c.
And the sigma is equal to something else.
The sigma is our epsilon, and we can ask what is the epsilon, how is the, how is that going to be distributed?
Does that make sense?
Yeah.
So, I'm glad you asked that question because if you look at the parallels here, we'll need a sigma prior.
And just for convenience, I'm just going to put the standard half-koshi, okay?
Inside there.
So, we got that.
So, then we might impose the same or a similar set of modeling assumptions on the likelihood, which is our y, right?
Or rather than calling it likelihood, because that's overloading terms, let's just do y.
How is y distributed?
Why we might impose that this is normally distributed where the mu is equal to a times e to the negative t over tau plus c.
And then we have some noise, which is our epsilon, and our epsilon, just for convenience, will also make it half-koshi.
Let's let that sink in for a moment.
Or maybe I should say something like, I'm just going to leave this up on there.
No.
Do we have questions?
Things that are not clear.
Yeah.
If the errors in this particular case, what we've assumed is that our errors are not dependent on the value on the x-axis.
If now we suddenly found that the values, the error varies with the value on the x-axis, suddenly we have to write a function that models sigma as a function of, in this case, t.
Right?
So, that's one place where this model would fail, and I've actually encountered that at work before.
How do we get around that?
We get around that by either explicitly stating up front that this assumption does not hold in our data, but we're willing to work with the consequences of that, or we go hunting for the function.
And sometimes that's kind of hard when you have limited x-values to work with.
Whoops.
Siri keeps coming up.
Cool.
Any other questions on this?
This is like the key, key point.
This is the key point here.
You can write the parameters of your likelihood distributions as a function or a transformation on the other things that you're interested in.
Okay?
So, if you go ahead and let's go ahead and code the model together, what's inside the instructor notebook might be different from what we just wrote out?
What I wanted to give you all just now was this live experience of like, well, I don't know, so what modeling assumptions am I willing to stand with?
Right?
And then we can go back in and re-critique the model one more time.
So let's put in, in this case, just copy and paste what's inside the instructor notebook.
All right?
And let's not worry too much about the others.
Again, you'll notice that thing, that equation, I alluded to this point its name a few times.
It's called a link function.
So y equals mx plus c is a link function.
y is equal to times a times e to the negative t over tau plus c.
That's just another link function.
You can have your four parameter dose response curves as a link function.
You can put the standard logistic regression curve as a link function like any math function that you can think of can be a link function.
All right?
And then that goes and what that does is it controls the mean curve parameter, right?
It controls the mean of our data as a function of, you know, this thing on the x-axis, right?
So let's copy and paste what's inside here.
Here the modeling choices are half normal, exponential.
I think I chose c to be normal under the assumption that sometimes the machine might go faulty and give us like a completely negative baseline.
Sure.
And if that never happens, then I would change that to a half cosher exponential, okay?
So then we sample.
Oh, I hear the jet engine's running again.
Ah, so this is the thing that I'm wondering, Ravine, will you be covering?
Colin will be covering it tomorrow in the RVs or the Bayesian model evaluation tutorial.
So ignore that for the time being.
And you should get something that looks like this guy.
Do we all have that?
Thumbs up if you do.
Yep.
Okay.
So you get like traces.
This one's been simple, right?
Because we've got only a single alpha, a single capital A, a single capital C, a single tau, right?
But you all saw just now how we can actually have a vector of A's, a vector of tau's, a vector of C's.
Our likelihood normal distribution can also be expanded to be a vector of likelihoods.
There's just some little intricacies that we have to worry about with respect to the, you know, y equals, y is the mu, the link function, right?
So you'll have to play around with that.
But this is totally doable.
And then once you have that multiple groups thing, once again, you can do your hierarchical player, hierarchical thing if it's an appropriate modeling decision, right?
So if you think about it, think about it, that this arbitrary curve regression thing is once again nothing more than estimation at its heart.
And instead of estimating like A, instead of estimating distribution parameters directly, like in this case, in previous cases, we were estimating the P hierarchically, right?
Now all we've done is we've said there's an equation that governs that key parameter.
And now we want to estimate the parameters of that equation in a statistical fashion rather than just treat it as some fixed point, okay?
How are we with that?
Okay.
If you want, go back and like figure out what the element is.
I'm not going to reveal the answer right now.
But I want to point you to the table at the bottom of your notebooks.
The table at the bottom of your notebooks says in compact form everything that I just said.
That is, you can put any arbitrary curve as a link function.
And you don't have to be restrained to modeling just linear models.
You can model these decay curves.
You can model logistic regressions.
You can do, you can write a neural net.
Like if you've got some weird function that is, you know, non-monotonically linear, then go ahead, write a neural net and estimate the parameters.
You might not want to do like MCMC sampling.
That's a little too much.
You might want to bust out the variational inference tools that we have, but in PIMC3, but, you know, it's all possible.
It's all totally possible, okay?
So, all right.
That's it for the arbitrary curve regression notebook.
Do we have any questions before we go back into doing estimation one more time?
What you have to do, let me see if I can pull this off here.
You want to see not just the single regression line, but the full family of them, right?
Yeah.
All right.
This is going to test my live coding abilities.
Trace dot bar names.
Cool.
One way to do this is to plot what T would look like first.
So, T is NP dot LIN space from zero to 800.
Okay.
And then you'll want to write the equation out.
So, the equation is this guy.
I'm going to put this down here.
So, return that.
Okay.
Then the trace will have, if we inspect trace of A, it's a vector.
It's 2000 long, 8000 long.
I hope I've done this before, but I just have to do this correctly.
To save time, I'm going to have you, we'll talk afterwards.
And I'll put that, I'll be sure to put this on the notebook so that everybody benefits from this question.
I think it's a great question because I've done this before.
I just like am blanking on live coding, but I'll get that up there for you guys.
Okay.
Yeah.
But there's some form of broadcasting that we need to do, right?
There's like X over tau needs to be broadcasted into a matrix and then we plot each of those rows of the matrix.
But I'm not sure how to do this right now.
So, we'll work that out later.
Let's come back to estimation before we wrap up.
So, with estimation, we're going back to notebook number four.
So, I'd like to invite you to open up notebook number four.
And all I'm going to do is rather than code with you,
I'm going to show you another, show you this example is basically another case study where we've got information from two groups,
but now we have this third group for which we don't have enough information.
We want to be able to make reasonable inferences on it.
Okay.
So, I'm going to use the instructor version.
Whoops.
Rather than the student version, and I'm just going to run, run down to about here first.
So, we've got data.
We always love to have data.
And for educational reasons, what I did, I took the liberty of adding in an extra species that was unknown.
We know it's a Finch, but we've never, we've never really measured it.
But, and it's so rare, we've only got one measurement.
Now, we want to make inferences on, well, what's it's, what's, what's the,
what do we expect to know about this new Finch's beak depth, right?
We've been measuring the beaks depth and their length.
What do we expect to know?
So, under this case is like, damn, we have like one measurement.
There's no way we can even compute a standard deviation on this one independent measurement, right?
So, how do we even estimate uncertainty in this case?
And this is the sort of scenario where a hierarchical model can be helpful in exactly the same way that it was helpful for those baseball players who had only one at that.
And no, no other data than that one at that, okay?
So, if we think about the data generative process, we'll say something like, oh yeah, our beaks, maybe they are student T distributed.
Why student T?
It's because student T is the generalization of the normal and the Cauchy.
The Cauchy distribution, the student T distribution has this degree of freedom parameter.
This degree of freedom parameter controls how high or how fat the tails are.
Normal distribution has really, really low tails, low probability density on the tails.
The Cauchy distribution has really high probability density on the tails, relatively speaking.
So, the student T distribution says that when degree of freedom is one, it's the Cauchy, and when it's infinite, it's the normal,
and everything else in between is controlled by this degree of freedom parameter.
So, we might define a student T likelihood.
If we do an independent model, we'll get these posterior distributions on the beak depth, right?
And it's on the mean beak depth, and this is kind of like where this independent model is really not the right place to be, okay?
So, think about it.
We've got like values that can range from 0 to 15, where we know that finches generally are constrained maybe more towards 4 to 9,
or 4 to 11, or something like that, right?
In, I forgot, the centimeters or millimeters.
But you get the point, right?
This independent model doesn't really have that borrowing of information from the known species to help us constrain our estimates.
So, I'm going to throw this on the right-hand side here.
Keep this one in mind.
This is the independent model.
Now, if we fit a hierarchical model, and it looks something like this guy, right?
Similar syntax, nothing fancy.
We have our priors and everything, and we have the broadcasting that's happening going on, just like in the independent model,
except now we have prior distributions on the parameters of our distributions for the, like, on the distributions for the parameters of our likelihood function, okay?
That's a bit of a mouthful, but I hope you get the point.
There are hyper priors that exist.
If we do sampling, now, ooh, sorry, I'm going to instead throw this up on the right.
There we go.
This is the one we want.
Oh, okay.
Maybe it's better on the bottom.
And if you look at this guy over here, throw this one up here.
Okay, so down on the bottom is our posterior distribution estimates for the independent model, okay?
And up at the top is the posterior distribution estimate for the hierarchical model.
Which one looks more reasonable for this unknown species that we're interested in?
This might take a bit of prior knowledge, but then you think about the 95% posterior density values.
These are pretty extreme.
These are quite extreme for the problem at hand, right?
So we're going to look at the finch beaks that are like zero centimeters or really close to zero, not so believable.
In this case, because we have only a single measurement, the math works out such that our smallest beak size will, you know,
and the 94% density will be at 0.9.
Still might be unreasonable, but if you look at where most of the credibility is associated, it's out here.
Whereas on this side, well, yeah, most of the credibility is associated out here,
but the credibility assigned like at really low values nonetheless.
So qualitatively speaking, it still doesn't really make sense, right?
Given the background prior knowledge that we've had, okay?
Okay, so that's all that I really wanted to say about this particular model.
It's intended, so you can do this at home.
It's intended, this exercise is intended as, you know, can I build a model for the data that is now not following the binomial story,
because we really, really harped on the binomial story to illustrate these other things,
hierarchical models, vectorization of probability distributions, and the likes, okay?
So we really harped on that, but here, here, this case, you can get some practice with, you know,
something that's t-distributed or normally distributed, and try out other problems for yourself as well.
Try out the other probability distributions, okay?
So that's also really helpful.
All right, so the final thing that we have to do is I'm going to have Ravine and find a few helpers.
I've got these little cards that congratulate you for taking this tutorial and sticking all the way through to the end.
So this is my little way of saying thank you.
At the same time, I have a little ask as well.
There is a survey that we have on the readme of the GitHub repository,
or if you prefer to use your phone to do it, there's a QR code on the back of the congratulations card.
I'd like you to fill out that form to tell us where we did well on this tutorial, where we could improve it.
Every generation of tutorials gets better and better, and it's all thanks to your feedback that we're able to do it.
So while that's happening, I'm also happy to take questions, and then I have one final, final, final thing for everybody.
So while that's going around, while you all are doing the surveys, I hope you all can open up the readme if you want to do it on your computer,
or scan the QR code if you're on your phone, you have questions on the material today.
Okay, if not, then we'll continue.
I'll just wait until I've got some form of quorum on everybody being done.
Yes?
Sure.
Right. Okay.
Oh, cool. All right. That's good for me to know.
Really tight.
Okay.
Yep.
Oh, cool. Thanks a lot. I learned something today.
Anything else?
Okay.
Yeah. So let's see.
The simplest, the simplest way to do this is actually to use the sci-pi stats module and use that to calculate the likelihood of data under your
distribute, calculate, sorry, backtrack a little bit.
We've always in our examples had data being basically like a data frame or like multiple rows of stuff.
And we calculate the likelihood. It's really the sum of likelihoods over every single data point.
And what happens when we're sampling, I think Ravina and Colin will know this better than I would, but the mental model that I
have is we're sort of, we draw a number from our prior distributions.
We assume that to be true and now put that into the likelihood, then compute the sum of likelihoods over all of our data.
And then there's like this step that says, well, okay, given this thing that we've pulled out, the likelihood is this particular value.
Now there's gradient information that tells us which way to go and in the right place to sample that will now help us increase
likelihood. Now I want to be clear, this is not gradient ascent.
This is not gradient ascent and in talking with Colin multiple times, I've actually made that mistake of thinking of it as gradient ascent.
So I don't want you to think of it as gradient ascent at all.
It's much more complicated than that.
But that's basically a glimpse into what's happening underneath the hood.
That is the MCMC step that we're doing.
We're like randomly sampling values from our calculated posterior distribution.
Sorry, I didn't catch that.
We can do it with simpler math.
So the, who depends on how we define simple and complicated integrals are kind of complicated.
And if we were not to use MCE methods, we would be doing integration and it'd be a bit of a pain.
I think ravine had something to say.
Yeah, for that question specifically like the tutorial that I'm giving tomorrow, which is on GitHub has an entire notebook for that question of how MCMC works and diagnostics for it.
So if you're in it, that's good. Otherwise, you can come talk to me.
I'll give you the link to the GitHub and we can talk to you MCMC in as well.
Hopefully a medium amount of detail.
So how
Yep.
So I've done it before where we cheat and look at the data first and then try to see what distribution might be suitable.
Well, this is mostly for the likelihood.
Cheating basions are called empirical basions.
So that's one way of approaching the problem.
In practice, what happens is this will build a model and then it's all got it's got all the simplest things.
It's normal.
It's exponentials and like we're not thinking too hard about the mechanics of the problem.
We're not thinking too hard about the details of the problem.
We're not we're sort of ignoring ahead of time what potential problems might show up in MC sampling and just running with it first and then we'll encounter a problem with MC sampling, which often is an indication that like the model is kind of bad as well.
Then we'll go back and think a little bit more carefully about it.
So I've done these sessions at work where I start working on a model at just after lunch and I don't go home until 7pm because at 7 that's when like something that might look correct starts to show up.
Right.
And even then I'm still not 100% sure that that model is the best model.
However, I have a pragmatist in my head restraining me from going till 9pm.
Right.
And it says well okay you know you can you've got the key parameters of interest and you know their uncertainty to some degree.
Go home rest over it and maybe present it and someone else might be able the peer review process at work then shows up right like if no one else can critique the model.
We sort of all have to just agree that let's just run with it.
Yeah.
But in the case of the exponential it's kind of like saying that you're really sure you're really sure that the true value is something close to zero.
Yes.
Yes.
Yes.
Right.
Yes.
Exactly.
Yes, exactly.
And if we're not sure about that we change the distribution.
There are positive only distributions that can be centered, you know, way out further out.
Right.
I'm blanking right now on exactly which ones but if you look at the PymC3 distribution gallery and you'll see those pictures and it becomes clear.
Are there like, are there like beautiful distributions that are bounded from zero to infinity?
Is there a way to say I don't know but it's not for zero?
Yeah.
So there are these so-called improper priors.
The flat distribution assigns, forgot what likelihood it assigns but it just assigns a single constant number from negative infinity to positive infinity.
And then PymC there is the, there is machinery that lets you bound a distribution by setting its lower bound, upper bound or both.
And that's available.
And yes, you can do that though.
I think the pros say don't do it.
Avoid the improper priors where you can.
Like it's not, it's not the best thing.
The reasons why I'll have to dig but the rule of thumb I've remembered is don't like just avoid it.
Yeah.
Weekly informative priors that say things like, yeah, it's probably more close to zero but I'm really not sure.
Or it's probably centered around here but I'm willing to give lots of uncertainty at first.
Those are the general rules of thumb for selecting priors.
Cool.
Anything else?
Okay.
If not, let's do a very quick recap of what we went through today.
There's a lot of material but the core thing that I hope you take away are the following.
Firstly, that probability itself, you've seen it so many times here,
it's nothing more than assigning credibility points to the number line.
Where there's higher credibility points, we believe it more and where there's lower credibility points,
we believe that that parameter takes on that value less times.
That's all it is.
We saw how we can go from joint and conditional probability to Bayes rule and how that maps on.
Marginal probability, joint and marginal and conditional probability are all really important for this.
One thing I really hope you all take back is know your probability distribution stories.
Super-duper important.
If you know what their stories are, then picking them for your modeling work becomes much easier.
Picking them becomes much easier.
Knowing what the continuances are and what the discreet are, that's really important.
Knowing their shape, their support, what story they tell that will help you in your modeling work.
Finally, we went through one really simple example but built it up in-depth,
the binomial distribution story and went along and showed how you can take a seemingly simple model
and complicate it enough to fit the problem that you have at hand.
Do you have more than one group?
Well, vectorize the thing.
Do you have some groups with lots of info and some groups that don't have lots of info?
Well, use a hierarchical model and the mechanics of how we build these models we reinforced over and over and over.
From the discussion, I noticed a lot of light bulbs going off, so that always makes me very happy to see.
Cool.
With that, I'm going to say we're going to end here.
Thank you all for coming.
If you want office hours, I'll put them on the Slack.
They will always be in the Tejas room in the afternoons.
Exact time, see the Slack channel.
Thanks a lot.
Thank you.

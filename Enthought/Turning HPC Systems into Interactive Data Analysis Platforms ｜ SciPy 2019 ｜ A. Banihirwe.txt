Good afternoon everyone. Welcome back to room 305. I see how we are full as normal. If you guys can
try to get into the middle to find a seat, you know, fire code and stuff like that,
that would be very, very much appreciated. This is the last talk in our Geophysics,
Geology and Atmospheric Sciences session, and it goes to Addison. And how do you pronounce your
last name, Addison? I'm going to leave that one with him and not even attempt it,
who's going to talk about interactive super computing with Jupiter and Dask. Thank you very
much, Addison. Thank you so much. Great. Thank you for having me here. This is my first sci-pi,
so I'm very excited to be here. So as Scott said, so my name is Anderson, I work as a software
engineer at the National Center for Atmospheric Research. So for those who are interested,
the slides available address. So this is a talk about Python tools that enable
interactive super computing, and we're going to see what we mean by super computing in a minute.
Okay, so this is Alice, and Alice is a project scientist at NCAR. She studies the transfer of
water and energy between the land surface and the lower atmosphere. And on the other side,
we have a cool notebook of how work. There are three things that are really interesting about
this notebook. So the first thing is that this notebook is not running on her computer. So this
is running on a supercomputer. In this case, this is Cheyenne. And as you can see, there's an address
to a Jupiter hub running on the Cheyenne supercomputer. The second thing is that she's using
hundreds of processes and terabytes of memory. All this is distributed computing resources.
Thanks to desktop queue, we're going to talk about that as well. And the third thing is she's
actually doing science. So this is not a toy example, which is what she's good at. So now we're
going to talk about how we at NCAR enable Alice and folks like her to do interactive super computing.
So what do we mean by super computing? First, MPI batch processing, lots of heavy machines that
most of people don't have access to as admins. So you have to talk to people to be able to install
some of the software. In the picture, so this is Cheyenne, which is a supercomputer operated by NCAR.
If you're ever in Cheyenne, Wyoming, feel free to stop by. They give free tours of the facility,
if you want to. And what do we mean by interactive super computing? So with the current growth in
data creation, both through new simulations and new observations, there's a growing need to have
more human in the loop workflow, like where people can rapidly prototype and iterate through the
data with tools like Jupyter notebooks. There's also a need to do things like in situ data analytics.
So instead of having to run the simulations and output the data, you could actually run the
simulation and do the analysis at the same time. And the other thing is actually adaptive scaling
of computing resources. So this combination would really be powerful, but it's really hard
for different reasons. So some of these reasons are cultural and others are technical.
So the first one is that HP systems tends to be unique. So every HP center has its own policies
when it comes to things like security. What can you run on the supercomputer? And the second
point is the tension between interactivity and machine utilization. So when it comes to HPC
systems, HP center tends to be measured according to how often the machine is actually utilized.
And when you're talking about things like interactivity, users want to have
resources on demand whenever they want them. But because HP system operates on batch Q systems,
it's not that easy to actually get resources whenever you want to. So no user is going to
wait for five hours to get into the queue to do interactive data analysis. And the third point
is the lack of elastic scaling. And this is a huge, at least in my opinion, I think this is a
huge disadvantage in that if someone, let's say, want to do some analysis, they have to take time
and actually think about how much resources they want before they even do the analysis.
And what ends up happening is that you get into your work, maybe five minutes later,
you probably take 20 minutes thinking, what do I do next? And during that time, the resources are
just idle. So it would be nice if you could actually scale down, let other people use those
resources, and get them back whenever you actually need to. The good news is that despite all those
challenges, there's a growing list of technologies or tools that try to help with interactive
supercomputing. And I'm going to talk about some of these tools. The first one is Trubiter. So we
all love and use Trubiter. And so I'm not really going to spend much time on it because now everybody
is familiar with it. And some of you may be wondering, but isn't Trubiter already usable on
HP systems? And the answer is actually yes, but you have to go through all these steps. You have to
essentially tune the machine. You have to set up the SSH tunnels. And you have to do this every
single time that you want to actually use the Trubiter notebook on the supercomputer. And this
can actually be tedious, especially for new users. So what is missing? So one of the things that is
actually missing is the multi-user support. So all those steps, everybody has to go through them
individually. And it would be nice if there was one single place that everybody goes to and everything
is done for them. Another thing is actually the lack of pure web access to HPC resources because
as you've seen, we're setting up SSH tunnel, which if you ever need to use another process that probably
runs on a different port, so you also have to SSH to create a tunnel for that port as well.
Well, then Trubiter Hub comes to the rescue. So one good thing about Trubiter Hub is that,
in a way, it provides a standard way of managing authentication. So one of the issues that is not
really hard to convince sysadmins about is the security of web applications. But Trubiter Hub
makes it easy in that it doesn't really force you to use any type of authentication. It's up
you to choose what you want to use for authentication. In this case, on the supercomputer, you have
different types of authentications. And another thing is that Trubiter Hub will just take care of
spawning the Trubiter notebook single server and giving access to the user on demand.
And so let's now switch gears and actually see how this works in practice. So I will skip these
few slides and actually go to the live demo. So the first thing that you do as a user,
so you just go to Trubiter Hub.ucr.edu. And so this is what you presented. And if you've
used the Trubiter Hub before, this looks familiar. The only difference is that in this case,
I have to use the same authentication that I used to SSH into the machine. So in my case,
I just provide my username. And then for the password, it's a combination of a PIN number
and a UB key token. So now, so now I'm authenticated. So I then get this page,
which basically asked me for the project account. So this is the allocation on the
supercomputer so that they know who to charge for this usage. So in this case, I'm going to
provide, I'm going to change the queue here and specify the project.
And maybe just reduce the wall time. And then once this is done, I just click on spawn.
And what this is doing in the background, it's basically submitting a job to the queuing system.
And once the Trubiter Notebook server is ready, I will get
it redelected to this new page. And at this point, thank you. So at this point,
you have the same interface as what you have on your laptop. So I can run Notebooks in this case
to show you that I'm not really lying. So I could just run this Notebook, which is backed
by Bashkarno. And as you can see, I don't really have 200 terabytes of storage on this computer.
Okay. So yeah, so that's it about Trubiter Habs. I'll come back to it later.
So the next tool is Dask. And is there anybody in this room who is not familiar with Dask at this
point? Okay, a few people. So at this point, most of us are familiar with Dask. We've probably
interacted with it in the cloud or even on our local machines. So I'm not really going to spend
any more time talking about Dask. The one key point that I'm going to focus on is how to deploy
Dask on HPC systems. And this is where Dask JobQ comes in. So Dask JobQ is a Python library that
allows you to easily deploy Dask on JobQ systems, such as PBS or Slurm and so many other. It was
created as a spin of the Panjio project. It provides a high level Python user interface to
manage Dask clusters and Dask workers. So for instance, if you're like on a system that uses
PBS as the queuing system, so this is what you have to do. So from Dask JobQ, import the PBS
cluster class, and then instantiate that class with things like the project account, the queue,
and all other resources that you want. And I should be clear that I'm not really defining
everything that I want in my cluster. At this point, I'm just telling Dask a configuration of
what to ask to the queuing system every time that I want computing resources. So in this case, I'm
saying every time they submit, just in this single job, submit, ask for one process and one thread
and 100 gigabytes of memory. And once that is done, you can minority scale it by basically saying
just scale to 10 nodes, in this case, that corresponds to 10 Dask workers. Well, you could
actually tell it to scale adaptively by saying, okay, I want you to, at all time, to have a minimum
of one Dask worker, and you can scale between one and 100 Dask workers. And Dask will basically
monitor your usage of your CPU usage or your memory usage, and then you will know that it should
get more resources. And at any point, if you're not using those resources, it will just tell the
queuing system to kill those jobs. So if you're on a system that uses SLRM instead of PBS,
it's the exact same thing with the only difference of just using the SLRM cluster instead.
Okay, so now let's go back to Tributor and actually see what Alice is actually doing in that notebook.
Okay, so there's so much science going on in these notebooks, I won't really spend so much time going
through the details about it. But if you're interested, there's a copy of this notebook that you can
read out. It's actually exact same copy. So if you go to the Panjio data GitHub organization and
just go to the Panjio tutorial, you should see one of these notebooks there. But I'm going to
focus on a particular data set here, which is the created ensemble precipitation and temperature
estimates over the contiguous United States. So this consists of 100 ensemble members
for precipitation and temperature data. So the first thing that I do, I import some packages.
Is this better?
Okay, good. So the next thing that I do, I now create my cluster object by telling
by telling that job queue that I want 109 gigabytes of memory, 36 threads and 36 processes
specify the queue and the wall time. And I tell Dask to scale between 360 Dask workers and
I don't know what that is, but you get just so. And when I click on this, as you can see,
at this point, Dask is submitting a bunch of, Dask job queue submitting a bunch of jobs to
the queuing system. And now I'm not sure getting the workers. So as you can see, I have 360 workers
as I specified. Let me bring up the dashboard here. Okay, let me one more thing.
Okay. So now, now I can actually start doing the computation. So but the first thing I'm going to
do is actually connect my client to the remote workers. So I have a cluster with 100 with one
terabyte of memory and 360 workers. So the data set is saved in a store as ZAR in ZAR format. So
there was a talk two days ago about ZAR. If you've never, if you've never had about ZAR,
recommend to go and check it out, check it out. So and I use XR8 to open this ZAR store.
So again, just to 180, 87 milliseconds, I didn't really do anything other than just looking at the
metadata. So we can look at the size of the data set. It's close to 1.7 terabyte.
Can look at some metadata. So we have precipitation as one of our variable,
the mean temperature and the temperature range. And these are all for the variables. So 100 ensemble
members for these many days, which corresponds to close to 40 years, I think of data. And if you
wanted to, you could actually look at the Dask array. If you actually wanted to look at the shape
and things like that, if this is more useful to you. So the first thing I'm going to do is actually
just do a quick plot for the elevation. This is basically a mask that tells me the elevation for
each of the grid points. And as you can see, towards the west coast, you have points with higher
elevation compared to the east coast of the country. So let's now try to quantify the ensemble
uncertainty for a single day. So just select the data for one day and try to quantify the
uncertainty for that one day. Again, this just goes very fast. Nothing really happened there other
than just that Dask constructed the task graph. And when I do the plotting, that's when actually
the computation gets triggered. Then I have my plot here. And again, I won't go into the details
about the science. So the next task, let's try to compute the intra-ensemble range. Again,
as you can see, this just returns quickly. But this is actually a delayed operation
or a lazy operation. And now I can actually tell Dask to do the computation. So now you can actually
start seeing some activity here. This should be done anytime soon. Okay, so this has to do with
Dask resiliency in that if, for instance, Dask tries to do a computation and he fails,
it will start marking that computation. And I think by default, if it tries three times and
hasn't actually been able to successfully complete that computation, Dask will just give up on that
particular computation. So you could actually specify how many times you want Dask to try. Like,
for instance, if a task was scheduled on a worker and the worker dies, what to do,
or if you run out of memory or things like that. So this is basically
cool feature of Dask, the resilience of Dask. So the computation finished. We can actually do some
plotting here. Okay, again, not that many details about science. So the next task is let's try to
compute the average seasonal fall, a snowfall. In this case, we're actually just computing on
only four ensemble members. So if you go through the distributed documentation, there's a really
cool information about how to interpret the dashboard here, the information about the dashboard.
So this is done. Now we can do the plotting.
It's going to take a few seconds. And as you can see, so as the year progresses, so the amount
of snowfall decreases. And I think this is in the summer, you don't even have any at all
in most part of the country. And this is consistent across the four ensemble members that we're
looking at. So let's now actually do something cool. Let's actually look at like a specific region.
In this case, let's just look at all the grid points near Austin. So XRA is really cool in
that you can just give it the coordinates of the points you're interested in. In this case,
we provide a buffer so that we can actually get all the grid points in that range. And we're going
to compute the maximum precipitation near Austin for the last 40 years. So it's going to take a few
seconds. Okay. And then we can do the plotting once this is done.
So again, we can look at our cluster object. I still have 360
workers. I could have started with something really small, but it's just that I wanted this
to go really fast. And you never know how busy the queuing system is going to be. So if you need
the resources, you get them when you can. Okay. So this should be done. And basically, so this is
the maximum precipitation near Austin, Texas for the 100 ensemble members for the last 40 years.
So you could basically look at what that looked like. So yeah, so at this point,
if I basically don't do anything, because I told us that I want a minimum of 360 workers,
it would just keep those resources. But instead, you could actually now that I'm done with this,
I can actually do. Let me just tell it to scale down to
to this. And basically, what that's going to do is going to start monitoring the workers. And if
I'm not using them, we'll just take them away. So I will come back to this later to show you what
I mean by that. Okay. So we've seen, or at least I tried to demonstrate the adaptive or the elastic
scaling and the resiliency of dusk. And let's now talk about some of the challenges. So being able
to know how much resources you need before actually doing the computation is really hard,
and they actually requires quite a lot of experimentation. And if you actually get to
know how to do this for one particular workflow, it probably changes once you move to a different
dataset. So and another thing is that the computational workloads, they don't really
are not really constant. So you probably started with one terabyte of data. Five minutes later,
you probably only have one gig of data left. At that point, you don't really need all the
resources that you started with when you're dealing with one terabyte of data. So the good thing is
that dusk thinks about these things. So how to scale up and down, how to be resilient, in case
like a worker dies, and what if when you get new workers, what to do in terms of load balancing
and things like that? So what is the solution? Basically, just start a Jupyter notebook,
instantiate your dusk cluster, and then just let dusk do the scaling up and down for you.
And you just focus on the science. And what are the benefits to HPC systems for for elastic
scaling? One of them is that it actually improves the occupancy of the machine in that if the resources
are idle, then dusk knows how to release them so that other users can actually use them.
And another thing is that with the resiliency, you can easily, for instance, if you started with
120 workers, for some reason, your worker dies, dusk will know how to get a new worker. And in a
way, if you think about something like MPI where you start, let's say you have like 120
120 workers, if one thing goes wrong in an MPI environment, the whole thing dies. So
in a way, you kind of lose all the work that you had done, which is probably not that nice.
So basically, another thing to the same point, dusk thinks about these things. And
I think you should also think about what you get from this as well. Again,
so not all jobs are interactive. So once you're done with your interactive
workflows, there's this other package called dusk MPI, which actually now allows you to go on and
actually launch back jobs in case you don't need to do interactive exploration. And it looks like
I'm running out of time. So that is pretty much it for today. Thank you.
And like I said, now you can see that it took away those resources.
Okay, so here we go.
Let's try here. So do we have any more questions?
Hi. Great talk. Thank you. How do you deal with large data sets? And if you need to import like
large data sets, can you do it? Can you put them on the cluster? So in this case,
the true beta hub is actually running on the same file system. So which basically means that we
don't need to move the data around. So in our case, it kind of works in that the data, basically,
we're moving the computation where the data is. So we don't need to move the data around.
So I haven't run into cases where we need to move the data. So if anybody has run into that,
then they could probably provide a better answer.
Other questions?
Yeah, it's the Jupyter Hub instance taking a pre-allocated resources,
like 20 rows, and then you scale up and down inside Jupyter Hub?
No. So if you remember what I did when I logged in to the Jupyter Hub, I asked for I think
just one process. So Jupyter Hub is actually running, the lab itself is running in its own job.
So when I do the scaling up and down, I'm actually doing that as independent jobs.
So that's why I'm able to do that.
But I mean, scaling down is easy, but when you're trying to scale up, in our class, you need to
wait 30 minutes to have a new job created for you. So yeah, that's what I say, that when it comes to
queuing systems, I think that lack of elastic scaling kind of makes it hard to actually do
interactive computing in that you can't wait for two hours to get into the queue.
In my case, I was able to get into the queue very easily because I had to ask a special reservation.
All right, we're going to have a lot of people filing in and out here because we're changing
topics, so can we please thank Anderson again and all the speakers for the Geophysics Symposium.

Hello boys and girls. In this video I want to talk about the line of research that I
only came across last week, namely the very strong connection between quantum fields and
neural network theory. And what emerges from that is tools from one field being applicable
to the other. So you can then use let's say Feynman diagrams to find stuff out about
neural networks. And in the other way around you can use neural networks to compute stuff
for quantum fields. I think the most senior researcher, if I'm not mistaken from this
list of this paper in particular is James Harvorsson. And if you look it up there have
been a few papers in this direction in the last years. And nonetheless I chose this particular
paper because it's clearly some of the more advanced results there. And to motivate how
things work, I'm also in this video going to explain some results which were known for longer
time. So there I suppose it's fair to say that this sort of stochastic result for large neural
networks that we are going to discuss emerge in the mid 90s. And I'm going to explain this because
I want these subjects which I find very exciting actually to be known also a little better. In
this video however I will not go into any deep mathematical analysis. I have also not written
down much. I will basically jump from tab to tab. And nonetheless give a sensible explanation of
these things. So I think everything that I say should make sense in principle. And then you can
delve into all the subjects on your own. If you're actually interested in doing something in this
direction let me know in the comments. I will also point to some Google libraries regarding neural
kernel theory that I will also sketch out in this video. And yeah so that's that. The requirements
for the video are that you have like a basic understanding of deep neural networks like the
fact that these neural networks encode the parameterization of certain functions. And then if you
have a big enough network stuff like universal approximation theorem the fact that you basically
can represent functions in the space. Let's say continuous functions densely. And it helps if
you have a basic understanding about the ideas of quantum mechanics. So the fact that what you're
interested in is transition probabilities and that they are expressed as some products in a
Hilbert space. In this video I have a section where I motivate the jump from the expressions
that we are going to be able to compute with neural networks as they are explained in this
paper. Let me scroll down a second. How these expressions connect to scattering amplitudes
in let's say some large-edgedon collider experiments. These sort of stuff. So I have a small like
wake mafia section that concerns physics. But my main goal is that if you go away from this
video and have a wake understanding why these sort of expressions that you see on a screen right
now are both relevant for neural network theory and for quantum field theory then my job is done.
As I said this gives the possibility not just to compute stuff in physics but for example you
might then be able to apply Feynman diagram calculus to compute various aspects of neural
networks as well. And so even if you edit from a purely computer science perspective and have
some statistics background then this might be fairly helpful. Okay so I have here just a bunch
of bullet points that I want to go through. I might come back to this from time to time. Otherwise
I will just jump through like here 15 tabs or so and explain some results. I am actually
currently working on or started working on a video that I maybe want to make as my some for
summer video where I will do like a painful analysis of the universal approximation theorem.
But that's like months and months out in this video. I'm basically just rambling. I hope you
don't expect to neither deep or concise elaboration. So I'm warning you already but nonetheless I would
really recommend that you listen up. Okay so first off if you have just seen in the bullet
points I will explain to you like sketch out this result from the 90s which concerns neural
network Gaussian processes right. So there are nice results that have been found there and have
since been extended to a broad range of neural network architectures. So this is mathematical
theory results for neural networks that hold in general. In this video for simplicity we
can just look at fully connected in the sense that you see in the screen here feed forward
neural networks and for this video it is not even super relevant how many inputs outputs you have.
Basically you have let's say at least one input some float or if you want a real number X that
goes in one real number Y that goes out and in the middle you have a bunch of hidden layers
and the image you just see one but you know for the sake of it you might think of two,
three, four something like that and you see the nodes in this one hidden layer here and the
theory that we are going to discuss kicks in once you have a really big network. So this you
think of the number of nodes in each hidden layer here going to infinity or you know it
will suffice if you think of a huge number, a bunch of billions of billions and the thing
that then emerges with large networks is not just the universal approximation theorem that
says the nice functions, let's say continuous functions from R to R are represented densely
by this sort of neural network, by these weights but what also emerges in this large network limit
is that the dependency of the output for fixed input and probabilistic weights
takes on a very deterministic character. Okay so this is this neural network
Gaussian process phenomenon and then we'll explain in a second in more detail but basically what we
want to hear first look at is we take one fixed architecture some big neural network with let's
say three hidden layers and all the layers are very large and what we're here are first concerned
with before we talk about tangent kernels before we talk about quantum field theory
is we're concerned with the random initialization of these networks. So let's say you are on a
computer you have this network encoded on your GPU or whatever and the weights really I mean
the you have the architecture laid out the way in which all the the flow data pass to each other
naturally if you have these these flow types in every realized configuration the weights have to
have some some value and this gives the start configuration for the learning process right
in the learning process you're going to probably assign some some loss function and you do gradient
descent and then you tweak the network to behave in a certain nice way and fulfill some task but
to start this process you need to initialize the network you you want to maybe explicitly set some
weights okay and now what you can do is you can play around with what is actually your starting
condition right you can say hey should all the weights in the beginning be set to zero or
be set to one or and this is the interesting thing here you do a random initialization
of all the weights and biases also so think of you know you're in python you take a library
and you sample from a normal distribution for all the trillion weights you sample trillion
random numbers and you sample them each from a Gaussian from a from a bell curve and then
set the corresponding weights like this okay so all these w i j are sampled from some from some
Gaussian and when you like put in some input right we said there's one input let's say you
you take the input seven and set x to seven and then feed to the feedforward process the
evaluation of the neural network then if these things are random then the output will also be
some essentially random number it will be determined for whatever random weight you
have sampled here and in this way you can think of y for fixed x as a random variable
composed of the random variables w right so we have here's the neural network Gaussian process
page the math and the proof sketch of this result that we are going to get to is actually
described there so the weights are sampled from some Gaussian we are going to take a Gaussian
where the standard deviation gets tighter and tighter with the number of layers right but
if you have a fixed network this is some fixed variance here and the output that is in a standard
way a computer from neural network is computed as you see here you do fast forward and I think
it's fairly easy to believe that just due to the central limit theorem right the statement that
if you have a bunch of independent random variables if you sum them all up then this is another
random variable that will behave like a Gaussian process right so basically if you sum up random
numbers then you usually end up with if all the conditions are fulfilled all the mathematical
conditions then you will end up with a Gaussian this is the statement of the central central
limit theorem and this exact thing applies here also you know maybe there's some non-linearities
involved and maybe there's different steps but in the end the final output of the network here
in this picture on the last layer this set is still a function of all these small
Gaussians and because there are so many sums this is again just a Gaussian process right
so and so this says that in the limit and this is especially emerges if you have enough
width if the width is big enough so that the central limit theorem really kicks in
but this basically means that the as a random field as a random variable the output of the
neural network has very nice stochastic process properties and it's a Gaussian process in particular
one of the nicest you can have basically here on this web page on this Wikipedia page there's also
like this this this example animation so here they have some network with three inputs right as I
said it doesn't have to be three it suffices to think of one and they have a bunch of outputs
again it suffices to think of one so one green input one yellow output and a bunch of nodes in
bunch of layers in this case two layers one layer would also work you see on the right side I mean
you probably don't see it just because of my face here but I mean doesn't really matter too much it's
just a bunch of like random distribution the statement is that for fixed input the green
value again let's say there's one green input and it's set to the float number seven if you
fix if you go up with the the number of nodes and random initialize this with weights and biases
then just by the central limit theorem which is dependent on this this seven and this bunch of
random numbers the output why one here this yellow output will behave like a Gaussian just
by the central limit theorem and in this case there's two outputs so you can draw a plot
and the statement is then that both y1 and y2 behaving like Gaussians independently from another
like it's not a statistical statements but each behave like a like a Gaussian they have some
peak and so on the plot you get another nice Gaussian with some peak here right and so if then
the press play again if the network becomes even bigger then you get like this perfect Gaussian
where it this just says that it has this maximum expected value here in the middle
and this goes for all the outputs right so this is the result that that that
the okay I closed the neural network Gaussian process page but doesn't matter this is the
result that just because of statistics you the network if it's large enough at random
initialization behaves like a Gaussian we will not need it for the this video but if you want to
take a look this is the the form of definition of a Gaussian Gaussian process I mean to motivate
this basically you think you know a Gaussian is something which if you do the Fourier
transform it's again like a Gaussian and the Gaussian process is abstractly defined as this
random variable or sequence of random variables where the characteristic function the expectation
value of this phase is this Gaussian with a certain mean we are not going to need this
the Fourier transform will pop up again when I talk about the quantum field theories but
suffice to say the nice thing is that the neural network the big neural networks behave
like Gaussian processes sorry if I repeat myself
okay so do we do physics first or do we do neural tangent kernels first
let me actually
yeah let me actually say something about the neural tangent kernel so could we know now
that the the network at the start behaves like this Gaussian for all inputs and if you do the
learning process then this is about giving it a test data and then moving in parameter space
from wherever we randomly started to some other position in in weight space and I have made a
bunch of videos on gradient descent I will not explain it here but suffice to say you have a
space of weights and then the weights follow some path and you do that in a way that optimizes
some goal that you have right some task for the neural network and I think I sketched it out
here so
as is common we're dealing with not only here with a large neural network so the the
theory becomes simpler and nicer but also we are matching we have so much compute that we can do
really small step sizes so that we can then in the limit talk of the behavior of the network
as in a differentiable way where we say the the the motion of the in path space in a parameter
space can be described you know with literally just calculus differentials and so the gradient
descent algorithm what we are doing really is you know as per instruction of the algorithm we
say the change of the weights and here I abbreviate all the weights together with this theta the
change in the weights right from from one point to the next in the graphic that you just saw
is chosen in a way that it takes the negative direction of the gradient of some cost function
and the cost function in here is the difference essentially between what the neural network
currently says versus where we want to get at where set is all the learning data that we have
available right so we say for all the learning data that we that we have there is a discrepancy
between what the network f currently says what's correct and what is actually correct
why I said here is what's actually correct and we sum that up and so this is like the the this
cost of all learning data together and at every step in time in the learning process
we follow this path right and this equation is really just the Newtonian equation where
you know in physics theta would be the momentum
and where you look at the situation where the force on the right hand side is governed by a
potential and you'll say the direction of motion captured by the momentum is given wherever the
you know potential energy will be lowest and that's what the path followed by the particle in
Newtonian physics right so this already looks very like like this simple physics equation
govern governing governing the motion in weight space and now given that you have the behavior of
the the particle if you will going through weight space like in the give you just saw
and the the potential depends on the outputs at the network on all these spaces you can also then
do the calculus and and look at hey how does the output of the network
which depends on the position where you are at right where the weights determine what the
network output will be on all the learning data how does that the f change and so if you
do them just do the math and I think I have this is here so this is newer tangent kernel theory
if you if you do this sort of calculation and if you you know if you ever started physics you
have to do this sort of calculation a million times because basically if you have some observable
in a physical system and you know all the the constituents of the particles behave in this
and this way and then I have some observable which is made out of particles and you say how
does this this observable quantity change then you have to plug in a bunch of partial derivatives
and then the Hamiltonian comes in and whatever and so on and so forth what comes out of this
is that the development of the output of the neural network is governed by some matrix this is the
so-called newer tangent kernel and the changes of the the loss in with respect to the to the
weights right so I mean I did not adopt this terminology tater is again all the weights
together and you can do this calculation on one sheet of paper yourself I'm not going to discuss
the convention or a notation chosen here but the point is that the evolution of the output on a
network from your starting point which might be a random starting point is understood at least
here in theory it's another question of whether you can actually calculate that because this matrix
which determines how this the output of the network evolves as you do the learning according to
gradient descent is determined by this complicated object theta and the theta is basically this so
called kernel this is you can view this as the inner product of the gradient of the output with
research with respect to the weight change and so the interpretation is basically that you look at
different inputs and then you you as a kernel a kernel roughly charges how similar input data are
and this kernel basically looks at hey these two input data are similar if upon a change of the
weights the response of the network changes in a similar fashion and you compute this it's in a
product in any case this is like an interesting object that in the end determines how the network
behaves and similar to neural network Gaussian process theory where you say once I have enough
weights in my layer some nice theory emerges right in the Gaussian network case it goes
towards a Gaussian it's also the case that for a large network then these these matrix
can simplify and then you can get the infinite size network also to an analytical theory and
basically you random initialize you already know it's some Gaussian process and then you have some
matrix which determines how the network moves through weight space and thereby you have an idea
of actually what happens during network training right so if you have never heard that and more
or less followed my explanation then you can see this is kind of cool that at least in this limits
you have sort of an analytical idea of what happens during learning and then the question is
to what extent is this sort of logic valid for networks as we can implement them at the moment
at the moment because of course we have a lot of weights like billions of weights but it's not
infinite so you might ask to what extent is the analytical theory where these limits are taken
right so super small step size very large networks applicable to today's convolutional
deep neural networks and so on and so forth and this is basically a subject of study so
this is something where people still put a lot of time in and so for example you have here this
Google neural tangents project which I might be interested in looking at and there's a bunch of
Google researchers who are using this and publishing papers in this and so on and so forth there's
also I think a recent new rips poster from 2019 where you get some of the examples of
analytical formulas that I talked about just because I want to get to the quantum field
theory part I will not discuss this in detail but I hope my tangent pun intended was interesting
and as I said I would also actually like to to work a little bit in this direction myself so if
you want to look into that feel free to reach out and we can do some sort of project together
okay so now for the the field theory part
what's still extremely important for us is this neural network Gaussian process inside okay and
if I'm here in the paper on page four
then let's make first some definitions okay so here we have these correlation functions
which we call GN for concreteness sake you can think of N let's say as 2 so we are going to
actually look at two different forward passes for the neural network right so we have two
different imports that you want to try x and you plug them in and you get something out of your
current network which might be randomly initialized and if you as we had it with the neural network
Gaussian process case if you your weights as a random variable right over all the weights over
all your trillion weights you put a little Gaussian and let them wiggle a little bit
and you say what typically happens if I sample once and
um put into inputs x how are the inputs on a on a typical or random network correlated
with each other then you can you can try this a bunch of times and get an idea what you can also do
is analytically if you know the distribution of all your weights compute what will come out right
so you have here p over the weights this is the distribution that you yourself chose from which
you can sample and for fixed neural network architecture the weights and the neural network
which is here called phi this is the function that depends on the weights depending on your
architecture right so this is the sigmoids and this sum and so on and so forth and so
the correlation function gives you how this how let's say two inputs are
correlated with each other for this network right and by the neural network Gaussian process
result if we said that this this billion probability distribution over the weights
um make the input output relation of the whole network also into a random variable right so
you can also view uh this the setup that you have here not as um not just a sampling the weights
and getting uh then a fixed input output but you can also view any sample process as sampling
a whole neural network right even this is just what you do if you sample if you random initialize
for fixed architecture the um certain functions as your certain neural network then you've also
sampled the neural network from who knows what distribution right so there's also this different
the different view of this initialization process and then um by the result of the neural network
Gaussian process what you have is that you can also view the same exact object this correlation
function or any expectation value really as in terms of distribution over the these functions
themselves and by the result that we just had for a large network this will be a Gaussian process
and this manifests in this way right so here is the integral not over the weights but over the
um the whole function that the network represents itself and um the the um probability distribution
that you have in this case is um of of this sort where this s uh and now this relates back to the
formal definition of the Gaussian process is some quadratic let's say and we filled some local terms
cost associated with the whole um with the whole function so basically what what this does
I mean do they give here some examples I think they do um
okay so this is already a sort of physical example but what you have as s here in the the
exponent is some sum over all uh the values of the network and what what they like what in effect
happens is that um the um the the uh this this weight is such that um field configurations and
when I say field now I always just mean the same as the input output relation given by the neural
network um field configurations or neural networks where at one uh places you get a huge output
these are just exponentially suppressed because um if you random sample from the with the weights
then you are not likely to get um some basically you are going to get um neural networks in
initializations which are around some some uh some certain um typical expectation value
like they have there are some typical behaviors and everything that deviates uh a lot from it
is like exponentially less likely right you um if you do um thousand random initialization of the
neural network you will get some typical behavior and then you can cook up some other extreme behavior
that is not likely to happen and um by the result of the neural network Gaussian process
theory um it tells you what the the the probability distribution looks for the neural
network so there is this connection that you have here right and as I will motivate um later
in quantum field theory this is exactly sort of the setup for the path integral and what this
paper does is it um if you view um the um this this sort of um mathematical um overlap um as a
physicist and you want to use neural networks as as as a physicist you you see this as a way to um
then try to craft neural network architecture and sampling techniques right the the way in which
you sample the weights in a way so that the this this whole process of random initialization
corresponds to certain s s certain actions here right you do you have some physical scenario
in mind there's physical theory that says oh you know uh certain scalar field theories have this
in this action what is the way and in which I must set up a network and the way in which I must
sample the weights so that what a sample is exactly as if I would sample from a quantum field
like like as if I would sample from uh would I would sample a field which is one instance of
um a quantum field in the path integral formalism right and then I get out a bunch of correlation
functions this endpoint functions um and these are exactly the things that are uh what what you
do quantum field theory for right you you compute these g's and then I will explain it later then
there's some mathematical connections to how you get from this this this g's to um the scattering
amplitude or whatever your quantum field theory does um maybe particle physics solid state physics
and so on and so forth okay uh let me see so this as you might notice this is a very free flow
sort of explanation right um so I have to check uh what I have touched upon and what I didn't
okay
yeah okay so um from the very complicated quantum field theory math you get some you know relations
of um uh how the correlation function must relate to these actions and there's a bunch of
stochastic differential equation mathematics involved and because in physics the evolution
is always governed by some Hamiltonian operator function there's a bunch of energy terms that
you um have to kick around and that's why for example um these objects tend to look a little
bit like this um like if you're never studied this this physics um maybe one way in which
you should look at this is that because the evolution of these fields like how they um
have often time is governed by the Schrodinger equation which relates the time derivatives
uh like the evolution um of the fields themselves to some energy expression captured in the Hamiltonians
and um the the energy kinetic energy in in particular for fields is given by some spatial
operators that's that's why these sort of objects pop up here and you know mass energy equivalents
that's why we also have mass and so if you see these Laplacian operators or mass terms
you should not be too surprised there because in physics they just always pop up in relation
to the time evolution of the the fields okay so okay now if I really touched upon the concept of
time the thing is of course that um here this the fields as they pop up here will um be um
not uh like what you get there is the better controlled theory of euclidean fields right
you're not you're not having to do a priori with spacetime metrics and all these things which make
field theory quantum field field extra complicated but um just talking about Gaussian processes is
just talking about stochastics and then there is this sort of bridge that you have to take
and hope that you can get from the the euclidean field theories to um to some actual quantum field
and I will just name drop um a bunch of concepts there the idea is that you uh if you approach
quantum field theory with this neural network she bang then um you want to find a neural network
which mimics the weak rotated version uh of um of physical quantum field and so there is um
a bunch of uh so-called constructive quantum field theory coming in so there are various
approaches for people trying to um make certain aspects of quantum field theory more rigorous
and um transfer like get rid of uh pseudo metrics in quantum field theory move everything
to the euclidean domain where you have nice metrics and um uh so in this paper for example
they say that um you know what we really want to impose is um neural networks which when you
view them as a field behave in a certain way and something that is important there for example
are these uh Osterwald Schrader relations so for example here in this case the um correlation
functions um these particular correlation functions of relevance here are called denoted s and then
you see on the screen a bunch of properties that these shall have right so there you have the physical
translation in variances of certain objects and certain symmetry or independence relations right
I'm just mentioning this that there is it's not like um you just take any neural network and then
you get some uh some quantum field theory in the path in the word formalisms out of it there's a
fairly restricted subset of uh fields that the physicists for quantum field theory might be
interested in um okay but I should probably not go into too much detail on that uh here
yeah also the these objects in this exponential so sorry here
if you if you know some physics then you know this but if you don't then just want to mention that
these sort of ss um basically any s that you can write down gives you some field theory these ss
are some sums or integrals over energy terms and if you go to um sorry for the click clickery
if you go to the Lagrangian field theory Wikipedia page then you can find a whole lot of different
um s objects that make sense and you can see here see how they determine how they determine
the various physical theories that you have certainly heard of we are interested in particular
about fields so we have here scalar field theories this is what we just saw you have some
partial and then um some um some mass and uh there will be also be some time derivatives there
but the thing is that the pure gaussians are um where this is just this quadratic object in in s
are actually relatively um uninteresting from the physical perspective because if you have for
example if you have different uh quantum fields that interact with each other and and then this
information how they interact is also all encoded in this in this in this Lagrangian cells or in
the action s and then uh you will have some more complicated products in these objects and um
if you have some power of the fields that's higher than two then this is actually representing
a sort of self interaction in in field theory so um what we really want to have is not just
the fields which fulfill these nice properties in the Euclidean version but we also want to
have very finely controlled um interaction terms there and so what we really want to have is
uh um processes with processes which are actually not gaussian right and so what they do in the
paper is in in the end um look at uh five to the fourth field theory so quadratic interaction
let me see sorry
so what they really want to implement and what they actually then do in the paper is
they uh take this sort of action you you not only have this quadratic term there but you also have
there um this five to the four term and to to get this in to get um uh away from the just
quadratic gaussian process scenario there is two ways to implement this self interaction
uh and one way is to actually not look at the infinite and limit right not the infinite
size network limit um because then uh the you basically break the neural network gaussian
process scenario you you person person uh you purposely stop a little bit earlier
and get some non gaussian effects and so what from the mathematical point of view is a bug
that for a real network you actually don't get a perfect gaussian situation here becomes sort of
feature it gives you the freedom to actually implement behavior and if you tweak the network
nice enough the idea is that then you can sort of control how it is broken this bug suddenly
becomes a feature this is one way and the other way is if you um actually
in sampling you do not uh take a trillion independent distributions over the weights
but what you and instead do is you introduce on purpose some dependencies of the individual
weight uh distributions right do you break the independence such that um there is a little bit
of a flaw in the central limit theorem and in this way by independence you also get some non
gaussian process because it's clear that if you do only um break the independence of this weights
a little bit you'll still by the central limit theorem get something which is just a slight
deviation from discussion processes right so if you the idea is if you tweak the this the
conditions for the god for the central limit theorem just in a wide way then you might produce
errors to the the gaussianity in a in a in a very controlled way and this is exactly what
they do here so here they describe a neural network with particular non-linearity as activations it
looks like this and what they do is they sample in this um in this dependent way in exactly the
correct way so that the um the action um that emerges by random sampling in this you know
particular way represents uh this uh this sort of physical field theory so this field theory in
particular is basically always the first interaction uh interacting field theory or one of the first
interaction field theories that you would learn in the quantum field theory course um this is not
one of the famous standard model um energy densities at least not in the exact same way here
but nonetheless this is like classical physical theory and so this is what this paper is all
about right breaking the neural network gaussian process uh theorems in the correct way to get um
the the right uh g functions endpoint functions out there that are relevant for physics um and
so you see that then you can you know in principle sample uh from this fixed neural network
architectures fields and then once you have uh like a way of sampling fields you can in principle
because the fields determine all the properties of the quantum field theory uh compute expectation
values and thereby get physical uh information so this is the idea it also goes in the other direction
in um since there is uh these methods in particular um Feynman diagram computation methods that compute
correlations um for quantum fields you also have a method of computing aspects of random
initialized big neural networks right you have a big neural network you know that if you uh
to sample from it it is as if you would sample from a quantum field um and because you have ways
in physics to compute aspects like correlation functions and so on of the quantum fields these
g functions that you can compute in physics with Feynman diagrams and so on also have a relevant
meaning for computing typical aspects of random initialized neural networks okay i know this is
a little bit much but i hope it sort of makes sense um i um i don't know how clear everything was
that i that i discussed so far i just want to give you uh this is then more on the quantum field
theory side a little motivation that i've just here written up um that connects these g functions
right these autocorrelations to physics i just want to motivate it maybe uh give me a three more
minutes so i i'm going to assume that you have an idea of the fact that transition probabilities
are the squares of inner products in a Hilbert space in quantum mechanics um so there are these
kernel objects k which are given like that so in in you know standard quantum mechanics
what you have is some um you have some in um current state which i call here in and then you
have some uh other state out that you're interested in you want to know what is the chance that this
state um transitions over in that state what you do is you um take the Schrödinger equation
evolution which is governed by the Hamiltonian age in in the nicest case um here the formal
solution of this equation is just e to the i h so this is the operator which moves any state forward
in time you apply it to in to the like let's say this is now this is in two days you say this
state that i have currently how will it evolve into the in like how will it look in two days
then you basically apply this operator to fast forward this thing um and then you get something
the in state how it will look like in two days and then the product with this object in two days
with the out state that you're interested in gives you the probability that the the current state
after it has evolved is going to be observed in in the out state okay so this is basically
quantum mechanics one one um the for whatever Hamiltonian you have here you um can often then
compute some analytical case some kernels um these are these things and i think here if you
score down in this Wikipedia page you find some examples so these are just you know in this case
free particle in one dimension this is like some some Gaussian this is similar to
some heat dissipation equation and then if you have some more relativistic scenario i think
they also write down here some this is more complicated um correlation functions
and examples okay uh so that's the one thing now if we talk quantum field theory
um then
yeah the uh this is basically just a definition um the the g's we all also had that in the paper
are certain expectation values right so there since we are on the pure relativistic side
and not take your clean side there's some time operator there but you can basically ignore it
for for the moment the the point is that these g's are determined by some fairly simple looking
expectation values and in the in the quantum mechanical formalisms these are also some
inner products in some Hilbert space of course in a quantum field theory there is no super nice
theory um about Hilbert spaces and and and these operators so there is it's more like a
like a functional definition this is what you calculate and um you hope it's somebody else
comes up with the sort of rigorous setting to compute this but nonetheless you know how to
compute these g's you know how to uh set up the an evolution equation of your choice and
make sense of these correlation functions which is um what you want to get at in the first step
so given um you have uh definition of these g's there is actually then a sort of generating
function in the in the um you know analysis generating function sense there are these partition
functions which look like so so you basically we can view it like starting from this we start
with some some definition where all these g's are um captured uh in this sort of formal sum
formal sum of products and then these g's are once you would have this uh partition function
with this j parameter you can in principle like formally speaking with functional derivatives
compute this g from this thing and then there's this whole quantum field theory i mean this is what
quantum field theory is all about that tells you how to compute this this set and this is
then exactly this sort of object this is what we already had have had there right we already said
that um if you um take the expectation value of e to the minus s in the euclidean case is here
of a product of these phi's you get all these these g's and this is basically sort of the
generating function trick to get to this to these sort of objects okay um okay so this is
what you do in practice and just to motivate also that how um neural network sampling would
be done another way of getting at these these g functions which you want to to have okay and and
so to round it up to see to to explain how this this simple g's this field theory um
correlation functions relate to actually um to actual um observables right you want to have this
sort of transition functions as i discussed them for the quantum mechanics case there is then
complicated theory of how you take these states how they how you encode these states with some
fixed momentum uh in the in the quantum field theory you know they also live in some supposed
hillbett space let's say um you have to compute a lot in in momentum space which by
fury transform is sort of the dual to the the position space that we talked about in the whole
video so there are some fury transforms involved and you pass from the phi's to this a's so just
to sketch this out here um yeah so you have when these are i'm just throwing these formulas at you
right but there's this typical relations between um the momentum space here ap and the the the
spacetime uh fields and you can view this as the transitioning by fury transform
but the fury transform is with respect to space and also like you know momentum and space and also
time and energy and the energy encodes the sort of relation that is governed by the
Schrodinger equation relation and so all these things are fairly like complex but in the end
it's clear that once we have these g's we have to sort of fully transform them over to a momentum
space and in momentum space we encode actually the input output state that we are actually interested
in like this for some scattering transition probability stuff okay um and then um there is
done some more theory that gives you this momentum space in out transition amplitudes that you're
actually interested in in terms of this this g's basically you see on this side here these are
basically g's and then from fury transforming and taking into account how the the frequency is
and space is sort of tied together through the evolution equations you get some nasty
object here that you have to then compute and then with this um this this formalism um you get
from the green's functions to the transition probability right so i in these last 10 minutes
yeah i just explained to you why these g's are really the important thing that you want to get
that once you have them then in principle you can compute this transition probabilities um and
the neural network aspect gives you a way to get at these g's okay so lastly i want also to mention
that apart from the neural network kernel theory stuff there's also the you know classical
information geometry approach where you um you know you have the underlying parameters theta
which are the weights and you can see them as encoding um if you put a distribution over the
inputs and you get the distribution over the outputs which are governed by the parameters
then you get this information manifold so you had that information geometry stuff this is like a
related but different um mathematical angle that i just wanted to mention because we are like
formally so close having all the important all these stochastics already but this is a little
bit different nonetheless but also interesting there you have this neural neural um manifold
thing so this is also one line of research i actually try to find um things going into the
this quantum field theory research direction in in deep learning books uh in preparation for this
video um and actually didn't find too much so there there was more about this classical
information geometry aspect to things um and then also worth noting the applications to
quantum field theories not the only way in which people try to uh apply neural network theory and
all these nice formulas to physics there's also uh this mathematical metric flow things
going on i just mentioned it because if you look at for example the research groups um that
put out this paper then you will also find these sort of neural network applications
using quantum field theories okay so i mean at one hour i know it has been a little bit fuzzy
but nonetheless i think i at least maybe you understood the neural network ocean process
stuff and and can see how this ties in with path integral formalisms um and i motivated you i'm
really i know very few people will make it for one hour our video of these sort of friends
but it's probably nonetheless i think the best way to get an infusion of this sort of theory
in a digestible way and in a way where somebody highlights these things so this is it's not a
real excuse i think it's nonetheless helpful even if this was not very super prepared okay uh with that
um i leave it i leave it at that i wish you a good transition into the next year i have no real
videos planned for the upcoming months really i mean i have a folder with 20 started projects and
videos i could talk about uh that i might come back to probably not within the next months
but then as i said i might look more into classical um classical functional analysis stuff for the
sake of making a nice video about the universal approximation theorem and at the latest then
i will have a polished video take care

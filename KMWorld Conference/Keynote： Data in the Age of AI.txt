I want to introduce our first keynote speaker this morning who's going to talk to us about data
in the age of AI. David Weinberger is Harvard's Brookman Klein Center for Internet and Society.
He's participated in with that group for many years. He's an author of quite a few books,
everyday chaos, everything is miscellaneous, too big to know, and many years ago the clue
train manifesto, but he's also a columnist for KM World, so he keeps his eye on what's
happening in the KM World. He has spoken at KM World a number of times on many different topics,
but he hasn't for a while, so we're very happy to have him with us this year.
Thank you so much. I guess I can take this off. It's wonderful to be back in this community,
and amazing and wonderful to have participants from 38 countries, and I would now like to welcome
all of them in each of their 25 different languages, so just bear with me. Good morning,
which is the one I've really mastered, and I'll move on from there. Sorry, I'm an American.
So I want to talk about data in the age of AI, and I should warn you that this is a topic,
it's part of a larger topic that I've been trying to work through for the past few years,
and you'll see I have not fully worked it through.
Whoops, sorry, I'm going to get the right clicker. I found the right clicker. I'm not looking for
your approval, but still, thank you. Thank you very much. So I'm also going to include metadata,
because metadata is the type of data, and it's really important from the beginning to me that
you understand this is not a technical talk, it's not a talk about the technicalities of data,
it's an attempt to try to follow how we think about data, not as data scientists, which I
absolutely am not, sorry, humanities major, and how that might be affecting how we view our world.
So don't be alarmed at all the technical errors, and I didn't fill in everything and the like.
Sorry, enough disclaimers? So good. So this is Martin Heidegger, a German philosopher who died
in 1976, in 1954, he wrote an essay called The Question Concerning Technology, except it was
in German, and in it he says, in a typical Heideggerian way, technology discloses being.
Heidegger was a terrible, terrible writer. He was also, please do not look him up in Wikipedia.
He was a horrible, horrible person, and I feel ashamed even introducing him. I did do my doctoral
dissertation on him, but that was a long time ago, a very long time ago. Anyway, so my understanding,
my way of understanding this is that our engagement with technology casts a light on the world.
We see our world through what technology shows us of it, but it also casts a shadow,
which is very prominent in how AI, which is certainly a two-sided technology when it comes to
good and evil. It's certainly true of AI. So the sort of thing that I think he's talking about,
but I don't really care because I want to talk about it, and it's not a Heidegger lecture,
is that, say, in the 17th and 18th century, where the pinnacle technology was watches,
which were amazing creations, incredibly complex, handmade, just mind-blowingly,
they were the chat GPT of the time. It was unbelievable that these things could work,
but because they were the pinnacle technology, the dominant technology, we began to see the
universe through that lens, so to speak. So the entire universe, we started talking
about the clockwork universe. The universe itself started to look like a clockwork,
which meant that there are very simple rules that govern it and govern it perfectly,
beautifully. This was aided and abetted by Newton's laws, which were the mechanisms,
the mechanics of the universe. So that's an example of how we
interpret our world through our tech. And so there's, I think, a big question that asked,
which is, okay, so how are we going to interpret our world in the age of AI?
And I'm picking a subset of it, which is in light of data. So I'm going to begin with a
really quick overview of starting with mainframes working up to the internet and then AI,
about how data has affected, the sort of public idea of data has affected
our view of how the world works. So starting with mainframes,
mainframes, in the age of mainframes, data was really, really scarce and it was a resource.
It had to be carefully managed. And just as with the punch cards of the day,
it was, data was structured and it was a reduction, right? A typical human resources
record was really sparse. There was hardly any information in it because of the capacity of
the computers. And it was, of course, completely regular. Everybody's record was structured the
same way. And it's, I don't think it's a coincidence that one of the large cultural divides of the
time was between the IBM rep in the blue suit, absolute symbol of conformity with the rebel,
the symbol of rebellion here. And so it looked like our culture was divided into two parts.
In the 1960s, this blue suit guy remained, but the angry beatnik got replaced by a stoned and
overly happy hippie. But it was the same sort of tension. And so it was a, the tension between
conformity, individual control and spontaneity and reduction of information so we could manage it and
part of control and wild excess often for its own sake. As I mentioned oddly to somebody just a few
minutes ago, this does not come up very often. I was at the original Woodstock. So I'm talking
from experience here. Oh, my mastering a remote doesn't count for anything, but Woodstock gets
applause. What sort of group are you? So age of computers of PCs, as we all know, I think, what
drove, well, those of us who lived through the era can confirm that what drove the adoption of PCs
was spreadsheets, killer app, absolute killer app drove the hardware and spreadsheets present data
in a matrix, of course. I'm your all familiar with spreadsheets. There's a model of, in this case,
how the business works and the relationship among the pieces of that model, right? But all the way
back in 1984, Stephen Levy, who's a remarkably good writer about tech, he's still writing very
a lot, always interesting. 1984, Levy said that what was actually interesting and important about
the adoption of spreadsheets, the reason people were adopting them so enthusiastically wasn't
because they modeled business, it was because you could play with that model. It was the what if factor.
And I think he correctly says in 1984, did I mention it was 1984 when he said this?
He said that it's already this ability to alter the model, to play with it, was already changing
organizational structures. They're incredibly early and insightful. So I think there's a tremendous
amount of truth in this. And so I'm going to pretty arbitrarily say that, you know, if you want to
talk about data, the change in data in the era of PCs, it goes from this controlled minimize thing to
still pretty minimal amount of data that it handled, but we want to play with it. And so data
becomes detached. It's not just a readout of the world. It's a readout of the world that you can
mess around with. You can try other worlds. You can try other forms of your business. And this
liberates data from feeling like a direct readout of the world. That data are facts that we can just
read out and they are the reality. So then there's a sort of a micro era of big data where one way
of thinking about this is the data becomes a source of surprises. And the canonical example of this
is the discovery that people who buy diapers also often buy beer. There's a correlation.
That's our granddaughter. I know. Anyway, she doesn't drink beer. I see where I went wrong
in this slide. We are not giving her beer. She doesn't even like beer. I mean, she likes it,
but she's not crazy about it. Oh, Poppy, I'm sorry. Anyway, so this actually, it doesn't matter.
It's a good example, but it's not a real example. It was actually a correlation that was discovered
in 1992, which is generally before the era of big data. And it was not discovered through
deep data analysis. Somebody noticed that people buy a lot of diapers and they did a sequel and
ask you L query to see what is correlated with that. It's beer. That turns out to be
an unlikely in reality, not really a helpful correlation. Although there is some debate
about it. Okay. So data in the age of the internet, the next two sections are going to get a little
bit longer. Sorry. So internet and AI. So in the age of the internet, data gets really confusing
because in one sense it shows up as a type of smog that we omit, which is an unfortunate image,
I guess. But as we're browsing and doing all the other internet things and corporations are
gathering all that data and compiling it and manipulating it and using it against us,
it was the overwhelming amount of data was so much that it actually seemed at times not to
clarify things but to make where to even start, unless you were a professional data analyst,
like at the platforms. So that's, I'm going to give you a few different ways, I think,
of characterizing data in the age of the internet. At the same time that it felt like smog,
so much and dangerous, the internet is all about links, which is sort of up from the data level.
It's more at the information level, if you want, where links are just about the opposite. I mean,
links are obviously and deeply human and form a structure that we can navigate at will and
notice how things are connected. Very different from the smoggy idea of data. And so I think
we've gotten a similar sort of polarity in which there's the smoggy. We all, I think,
maybe not. Most of us are concerned, let's say, about the use of the data that we are
surrendering unknowingly. Again, we click yes in order to, you know, that we've surrendered
unwillingly. We don't know exactly what's being captured, but we know that it's being used to
influence our decisions by people who don't have our interests usually as their interests.
And on the other hand, links, which is 100%, individuals who are able to control and connect
with others in an intensely social way, and both those things go on at the same time. And I think
a lot of us have this sort of divided understanding of data in the age of AI. But we also saw,
if only because of the gigantic amount of data that suddenly was there and seemed useful,
we started unstructuring our databases. We did this in all sorts of ways, a semantic web with
linked open data from Tim Berners-Lee, but also in unstructured databases like MongoDB and
in graphs and data lakes and JSON, this enormous unstructuring of databases,
which has enabled a lot. Okay, so I want to take, it's not really a detour, but
I don't care. It's a detour. It's not about metadata in the age of the internet. It's not
a detour because it actually reinforces the same point. So as you know, have you all tried a search
engine? Because they're really amazing. If you haven't tried a search engine, I'm telling you,
you really should. There's like a guluan, a bing, and so then you know that if you don't know
who wrote Moby Dick, you can ask, it'll tell you it's Melville. If you don't know what Melville
wrote, you could ask, it'll tell you Moby Dick and some other stuff. If you don't know either of
those things, you can ask it about the content of Moby Dick. Where does, and you can even misspell
the content, you know, call me Ishmael. Where is that? It's in Moby Dick by Herman Melville.
So each of these questions contain a piece of metadata that linked to data,
and that destructures metadata, which used to be a label of fixed things. And so it turns out,
as we have discovered, that the difference between metadata and data is only functional,
only operational. It's how you're using it. Because so that metadata is what you know. Well,
you know it was by Herman Melville, but you don't know what the book is. And data is what
you're looking for. Oh, it's Moby Dick. That's the only difference between them. So these,
which is an incredibly powerful and liberating thing when everything can be metadata for something
else. And then in turn be the thing, the data that some other metadata is looking for. So
this is why it wasn't actually a detour. It's a, we're seeing the unstructuring of metadata,
just as we are seeing, have seen already, the unstructuring of data. So now let's talk about AI,
where again, I'm going to have a few ways of characterizing it in a phrase, a bumper sticker.
So the first is data is generative in the age of AI, right? And we all know this,
because we know that in the old days before AI, a spreadsheet or any other sort of program
generates data for sure. But it generates it because humans have constructed the model.
Whereas with AI, as we all know, when I say AI, I really mean machine learning. I hope that's
should have said that, but now I have. With AI, data creates the model, which is insane and
seemed completely implausible until about 15 years ago, we found out, oh, yeah, that works.
We can get more accurate classification of objects in photos that way, even though it
makes no sense. We're not going to tell it anything at all about what we know about objects and
how you recognize them. We'll just give it data. It's insane, but it works. So data
creates the model, the model then generates new data, but it is remarkable that now
data is generative this way. Although I've already mentioned chat GPT once and thus have
fulfilled my legal requirement, I'm going to mention it again. So I asked it, this is about
metadata. So I asked it the other day, Dante's Inferno has three levels. Are there any other,
give me five other artworks that have three, that's a trio of things. And it did a good job.
It's unbelievable. It's amazing. It's amazing because we don't have a sense of scale. Humans can't,
you know, at least I can't think at scale. So I'm amazed and surprised by what scale can do. But
it is, of course, incredible. It gave me five. It gave, they're good. They gave pretty good
explanations of why, which I hadn't even asked for. And so if we think about this in terms of
metadata, the metadata in my query was artworks that show something in three parts. And the data
that it fetched was that text that you just saw. And what is, I think, amazing about this is the
metadata now is generating data. The metadata generated that content. Which is not something
I've seen before. And if we have, I'd be really interested in some other field or some other way.
I'd really like to know about it. This is mind blowing. We have metadata that will generate
its own data. And generally, it's good. We can't, it doesn't know when the data isn't good.
As you know, basically, not basically, everything that chat AI says is an hallucination. It's just
that most of those hallucinations are true hallucinations. It doesn't know that the ones
that it's making up and we often have trouble telling. So that's a terrible problem. So let me
give you an example. So in January 2022, researchers at the University of Leeds and some other
institutions published a paper that said we built a model from retinal scans and some really basic
medical information like age, weight, and the like. Really a very small set of it.
Does anybody know about this? It's sort of mind blowing. Because it works. I'm sorry,
let me be more precise. What works is the AI is able to predict with some degree of accuracy
the likelihood of an individual having a heart attack, myocardial infarction,
based upon the retinal scan. Data scientists, AI people, doctors of all stripes have
tried to figure out what about those images, presumably, but who knows? Presumably,
it's the veins, but we don't know. We can't figure out how it's doing it. It is at the moment,
inexplicable. And I know that there are actually bunches of people here working on making machine
learning less inexplicable. I had dinner with Beth Truden and a bunch of other people at that table
last night where this was a lively topic of conversation and Beth's company has a way of
keeping the sources, the citations and sources of the knowledge with the output
and is generating it from a more fact-based and reliable set of information. Is that
approximately right? Okay, it's approximately right. There's tons and tons of work in all areas
to try to make AI less inexplicable. But as it stands, let me put it like this,
that inexplicability is one of the two original sins of AI. By which I mean,
and I know the sentence doesn't actually make sense, but it may make sense. You'll be the judge.
So left on its own, AI would tend towards inexplicability. There are interventions and
structurings of all kinds that we might be able to do to prevent that, but it doesn't care if we
understand that has not been its mission. It's this mission has been to give accurate predictions
based upon data guided by a ton of human decisions about what we're looking for and what we will
accept. So inexplicability is pretty common so far in AI models. Beth, how much trouble am I in?
You'll tell me afterwards. Okay, so in this regard, I think a second formulation of data
in the age of AI would be to say that, okay, we'll do it by hand. I have forgotten how to use my
clicker. A few moments, it was great. Okay, a data in the era of AI is a source of secrets,
like, oh, there's secret information in a retina that can let us see what's going on with the left
ventricle, which is an indicator of heart health. Didn't know it. It's there, but it's secret.
But actually, I would think I would prefer the formulation that data in the era of AI
is a keeper of mysteries, because secrets, once you know them, generally, you know why you know
them, how you know them, why they're true and all that. When mysteries, you know, but it remains
mystery how it happened. And so far, that seems to be at least some of what AI does. So let's, for
the moment, we're going to just overstate and say AI tends to be inexplicable at the moment,
and we can argue later we won't. And then we can play the five-year-old game. Now, the baby you
saw, we do have a five-year-old grandchild as well, so that little drunken baby that you saw
would not be doing this, but she will be in a few years. So you can ask why. Just keep asking why.
So why isn't it inexplicable? And the answer is because the model is just too complex. Okay,
why? Why is the model too complex? Because there are too many factors, there are too many variables,
there's too much data that's connecting to too many others. You know, ChatGPT has 175 billion
parameters, which are weights, weighting the relationship, the importance of the relationship
of words, 175 billion. So if you want to know why it chose one word over another, why it called a
house you're looking at luxurious rather than upscale, you're never going to know, at least
at this point, you're not going to know, there's just too much going on there. So, okay, well,
why are there so many factors connecting to too many others? Well, that's a good question,
um, because actually that's how the world works. The world is really complex. I mean,
the universe is the single most complex thing in the universe. And it is really, really complex.
And the reason that these complex models work is that they are capturing something about the
complexity of the world that we live in. They go wrong in all sorts of ways, they're dangerous and
how they go wrong. But when they work, they're capturing something mysterious about the world
we live in, which is there's so much stuff and everything affects everything else all the time
forever, everywhere, everything, everything. It's simultaneously. It's not like a clock,
if only it was like a clock, but it's not. It's intensely beyond imagination,
complex in its interrelationships. Okay, so why didn't we notice this before? And lots of people
have, I think we all have, it's not news that the universe is complex, but it doesn't register.
And I think it's for two reasons. So, I mean, the sort of complexity I have in mind is, and this
is a relatively simple case, what determined why those people were going to be in the crosswalk
with you this morning, exactly where they are. There's no hope of figuring that out. It's way,
way too complex. It's too complex for any one person. So, why haven't we done anything? Why
haven't we come to grips with this? Why isn't this the baseline of all of our thought and thinking?
Well, I think for one thing, it's because we couldn't do anything with that sort of information.
And so, we just ignored it. Generally, we said, well, no, that's an accident. It's just,
who knows? It's chance. It's coincidence. We have a whole vocabulary for dismissing complex,
the results of complex interactions is not worth our attention because we couldn't do anything
with it. And second of all, because it didn't fit well with our old human models. So, I'm using our
here as the West. I just have to limit my domain because it's the only, it's basically the only
thing I know enough about to be talking about, any legs to stand on. So, in the West, this view
does not fit very well because for thousands of years, if we take the Greek, ancient Greece at
the origins of Western civilization, which is controversial, but traditional to do that,
back then, the idea was there's all this mess in the world that seems chaotic, but underneath it,
there are laws. And the laws are simple and understandable. There are laws, there are rules,
there's universals, there are principles, overall, there are generalizations. That's what we hang
on to because we can use those. So, in the West, traditionally, we have viewed generalizations
of various forms. They explain what's explained, which is the particulars. This is a chart of
why we have preferred the general because they are generalizations reduce information.
They're simpler. And it just so happens that the laws of the universe happen to be simple enough
for humans to understand what a coincidence. But they simplify something that's very complex,
the realm of particulars. We have thought that these laws are eternal. They've always held.
They explain everything going back to the Big Bang, even before we knew about a Big Bang.
And ultimately, they are the truth. In our tradition, the Western tradition, we have looked
up into the skies for the eternal truths. In the case of the ancient Greeks, more or less literally,
we still do it. We valorize the eternal over the particulars, which are sort of just,
they're over like that. They change all the time. There's no real abiding truth in them.
But I think that we are entering an age with the age of AI. For me, I think the most important
sort of change in how we view things is that we are getting a more particularized view. We're
taking particulars more seriously. We are letting them have voice with models themselves. I should
pay attention to my slides. The models themselves, machine learning models, being literally
generalizations. But they're generalizations that are made up of patterns that have been derived
from particulars. Those patterns can be so small in particular that there are billions of them
that get sorted through and generalized in various ways. But they stay true to letting
the particulars speak. And this is why machine learning can make, can sort animal photos better
than handwritten code has been able to. Okay, so if we are going to get more used to the idea of
particulars as being real and important and in some ways determinative and having their own voice,
then how will that happen? And I think here it may be because of the light that is cast
by, by the shadows that are cast by AI. It's our fear. So I want to give two examples.
One is a common sort of fear gets expressed variously, but we don't know how it works,
which can be genuinely scary and important to recognize, right? But it, there's two words
at the end of the sentence that are really crucial. So when you hear, we don't know how it works,
we also hear it works, which is amazing. So this fear may be moving us towards an embrace of
the particular because we then hear when we ask, well, why don't we know? We're told it
because it's way too complex. Well, in hearing, we don't know how it works. We may be being led to
believe, not on purpose, but being led to believe that it works because the world is also wildly,
complexly particular. So I want to give you a slight example of what it might mean to understand
something outside of the realm of AI in terms of particularity. So in this case, I picked a simple
little topic, morality. I think I have like three slides and we'll be done with morality and that
will be great. So typically in the West, traditionally in the West, we have had, we've resorted to
ethical frameworks. So religious framework says do what God commands you to do. Reason one says
do follow principles that are based in reason. And then the utilitarian one says the framework says
do that which will bring the most happiness to the most people. Pretty rough, but you know what I
mean. So it's a framework. It tells you why some things are good and some things are bad. And it
tells you what to do in particular situations, except they don't. They don't work. I'm going to
give you the world's briefest and simplest example, I think. So you're you, you have a friend A who
is angry, tells you that they're angry at your mutual friend B. But A says, but don't tell anybody
about it. Shortly thereafter, your mutual friend C comes along and says, oh, I'm very excited. I'm
putting together the seating plan for our wedding. And I'm going to put C here. I know A and B are
such good friends. They want to put them next to each other. Do you tell your, do you tell C
that that's a bad idea because of the fight? I don't know. And I'm going to say neither do you,
because we don't know the particulars. You're going to think about this and you're going to think,
well, yeah, I mean, how vindictive is A? Is A forgiving? Would A understand if I did it and
explained why? How deep is the rift between A and B? How, how upset is C going to be if there's some
minor tension between two people at their wedding and so forth? Without that, you don't know what
to do. And considering what to do, morally, you have to think about the details. In fact, if weirdly,
the next next week, you run into different set of friends who have exactly the same sort of
formal thing that one's angry and don't tell and the rest of it, but it has their different people
and it's not a wedding. It's whatever. It's a camping trip. What you did in the first case
is not going to help you decide what to do in the second case. You got to rethink the particulars
all again. That's what you would do. You would say, well, I didn't, I violated my promise to A,
because A is forgiving, but C, oh, C would never forgive me if I did that, for example. It comes
down to particulars. So the philosopher Martha Nussbaum in 1999 in a book called Love's Knowledge,
which is a serious philosophy work with one of the great titles and very apt title. Anyway,
she talks about this and she says moral situations are not commensurable. You can't
compare them and that's because they are so particular. So the second example is of a fear
that may be telling us inadvertently what the world under AI type of data is. So it's like
what the light is that's being shown. So AI is biased. It is biased. It is the second of its
original sins. Left on its own, AI will be biased. We have to guard against it. It's very hard to
guard against it, as I'm sure everybody here knows, but we can ask, okay, well, why is it biased?
We're told this and then it's explained to us why it's biased, because data tends to reflect
societal biases and the like. And what we hear from this is, oh, oh, we get to select the data.
So data is human stuff. It's not a readout of the world. It's not the facts about the world.
Facts aren't exactly facts either. Different topic. It's stuff that we decide. Data is stuff
that we read off of meters. Meters that we have decided to plant. Where and why and to what degree
of accuracy and what we do with the data are all human decisions. If I say data is human stuff,
I don't mean it's unreliable, but I sort of mean it's unreliable. There's a human element of
decision which includes unconscious biases and occasionally conscious. But generally, the
issue in AI is unconscious, unaware biases of what's going on, how what we're doing might be
taken in any of the 38 other countries other than the one that we happen to be in right now,
for example. Data is human. It's human stuff, obviously. Okay, so how does that lead to anything?
Well, it may be that in the discussion of bias, we get to the point of proxies, because the first
response is, well, let me just don't record. If you worried about bias against women or whatever
protected class or whatever you want to call it, then don't have a column for women. And then you
have to explain, well, no, but there are proxies for women in this case. Proxies are really interesting,
at least I think so, in part because it's proxies work by putting shape around the whole, the thing
you're trying not to have affect your, you know, it's like the missing piece in the jigsaw puzzle.
You can see what that shape is and that can be part, that becomes, in a sense, part of the data.
I know I'm not putting this technically correctly, but I'm trying to talk about
how this will perhaps appear to people who are not technical. And there are only proxies because
things are so interrelated. They're so interrelated that the absence of something can be the presence
of something. That's maybe one way through this fear, justifiable and terrible and correctly
terrible fear of bias. You don't have to, I got myself started on it. I'm going to back away from it.
That this, the explanation of it, the understanding of the first question, well, why is there bias?
Why can't you just lead you to understand the deep, deep, multi-dimensional interrelationship
of all of the data and all of the world? So if we ask what, the original question of this talk,
which is, if tech casts light and shadow on the world, can we think about how our world
is already beginning to look in the light of this change in data? So way too short what I'm about
to say. I understand that, but it's just to give you a sense of it. So if you look at an enterprise
in light of particulars, I think that we begin to see, you all know black swans. Do you all know
black swans? The unexpected things that happen drop out of the sky and destroy a supplier's
factory in your supply chain and suddenly your business is in great danger.
Literal lightning struck. Yes, that's right. I think there are black swans, but look that through
in this light, in the light of AI and data, everything's a black swan. That's the nature
of particulars. It's not just the big events. Everything that happens is a mash-up of everything
that happens all at once. So everything's a black swan and except some things are butterflies in
the chaos theory sense in which all the butterfly elites on a plant in Brazil and triggers a tornado
in Kansas. The standard example seems pretty implausible, but the idea is very validated in
sound, which is that a small event can create cascades by which it picks up energy and has
surprising and important results. And by the way, it's really hard to go backwards from the tornado
dependent on that damn butterfly and sort of pin it to a wall for what it did. That would be often
that's what we're trying to do with AI to understand it. Okay, so more specifically,
but still not very specific, this sort of thinking I would imagine should have important
effects on really important business topics beyond pinning butterflies, including strategy,
obviously, if we're living in this sort of chaotic world in which each particular affects every other
throughout the universe basically, then strategic thinking takes on a different tone,
especially in terms of it's committing people to long term strategies. Design of products,
managing people, all of these things, and more. It seems to me can get rethought in the light of
particulars. And then moving from business to life, I know that business is part of life,
but broader. So we talked, I talked about morality, excuse me, but exactly the same
sorts of considerations that you have to look at the particulars I think holds for every decision
that we make, where we realize we're making a decision. So there are decisions I'm making about
waving my hands now, which I'm not doing with any conscious awareness of, but decisions that we
actually deliberate on even a little bit, whether it's what, what items to pick up from the lovely
buffet for breakfast, too much more important things. Decision making is also like morality,
it's all in the particulars. I mean, the muffins looked really good, the croissants
looked good, but were they? I need to crinkle them first, and then I'm going to do sort of a
carb, sort of a balance, and what did I have last night? It's particulars, all decision making,
and love. We don't love our loved ones in general. I don't, that's a very weird thought.
We love them for who they are and who they are as particulars, and that year and a half old baby
is extremely particular in both ways. So I think this line of thought, and this is actually what
I've been working on, and more or less consumes me, is how this changes our ideas about creativity,
which is particularly relevant in the age of generative AI. I think it changes our ideas
about free will, which I know is not something that we generally talk or even think about,
but it is a background concept of considerable lineage in the West, and I think the model that
we are seeing, the light that AI is casting on a world should have us rethinking the age-old,
impossible argument about free will. Knowledge for sure. I hope I don't have to say anything
more. What does it mean to know in an age where particulars dominate, where we will continue
to want to generalize, of course. There's a rhythm here, but one in which the particulars
have become more dominant and recognized than they have is, I think, a really important question,
and I actually think that many of you here at KM World are managing that question, although
not in the terms I'm trying to push on you. I am going to guess that much of your life
as KM people is, in fact, engaged very practically with this question.
The mind and body, I'm going to guess, is not a burning, you know, the relation to mind and body.
It's probably not a burning question for everyone, but it's, again, a key shaping and background
thought in the Western culture, and even the nature of reality, which has become, to everyone's
surprise, it's not a topic that has been on the top ten list in philosophy for a while, but with
the rise of simulations suddenly, thanks to AI, and the questions in many of the questions in
questions of simulations also have to do with how particulars show themselves,
rather than about big generalizations. So, finally, data in light of AI. I think that we recognize
it's a human artifact. It's something we make. It's something we participate in making anyway,
by the decisions that we make, and then what we do with that data as well. We can't just accept
it as we did in the 1950s, which was part of the weight that was on that poor, who was the guy in
that, the actor on the left? Is that good? Exactly right. Gregory Peck, thank you.
It's, I forgot where I was going, but I now know it was Gregory Peck. Appreciate that.
Anyway, so it's, Gregory Peck took data as the bedrock, that is, the IBM generation,
took data as a bedrock, now we see how much of human decision making, how much of human
assumptions plays in the creation of data, and how we use it in AI especially, directly,
is the responsibility of humans who make decisions about it, even if they shirk those decisions.
And thankfully, most people that I know who are doing AI take those decisions really seriously
now, but it's still an enormous problem. Seeing data as a black box of relationships,
from which we withdraw what we need as human artifacts. And I think the pithiest way I can
put it is, if particulars are becoming more important to us outside of the world of AI,
but in part because of AI, then I think we can think about data as particulars. But there are
particulars that have been rendered machine learning. They gain something from that, I'm
sorry, from machine readable. They gain something from that, but they also learn something. They
gain some capabilities from that. So I think we're in a world in which particulars are rising.
I'm not sure if it's obvious. I think this is a really important corrective to a Western focus
on generalizations. It's because particulars are reality. So thank you very much.

Hi, thank you. Thank you so much for having me. I'm very excited to not tell you how AI
is going to save us all. This talk is called The Expanding Dark Forest in Generative AI.
It's going to be about writing on the web, trust, and human relationships, so like small
fish. And also AI, unfortunately, I'm sorry. There always has to be one AI talk at every
conference at this point, but at least you get me and not someone telling you how it's
going to take all your jobs. So I only give a footnote with this talk where I say, well,
this talk is up to date as of about a week ago, as of about Monday. So anything has happened
since Monday, I can't take accountability for. It's probably all out of date by now,
but this is the state of the AI industry. So first some context. This is me. I look
like this on the internet. My name is Maggie. I am a designer at an AI research lab called
ELICIT. We do use language models to help scientists and researchers do literature review,
which is a long, boring task, reading many thousands of PDFs, and this is something that
language models are actually quite good at helping with. I also am very online. I'm very
on Twitter, X, whatever you want to call it. I write a lot online, and this has led to lots
of really positive relationships, a lot of really career success, and this will become
relevant later as to why I care so much about people in the future being able to do that,
to be able to write online and connect with others by writing online. I'm also a cultural
anthropologist. I originally trained in this for my undergraduate degree before becoming
a designer because you can't get paid much to be a cultural anthropologist, but a lot
of theory I learned there plays back into my thoughts on design and development and
how we build things on the web. While you're doing this, if you want to take notes and
things, you can also just scan this QR code, and this whole talk is transcribed with slides
and everything on this link, so don't worry about taking pictures of slides you like or
trying to remember what I said. You can reference it later. I might have to update it a bit
because the talk evolves, but one version that's mostly the same is on that QR code.
Here's what I'm going to talk about. First, we're going to talk about the dark forest
theory of the web. What is that? Next, I'm going to talk about the state of generative AI as of a
week ago. I'm third going to present some problems, and then we can talk about whether they
actually are problems. We can question whether we think they're legitimate or not. And lastly,
I'm going to talk about possible futures, so how to deal with all these hypothetical problems
that I present. So first, to explain the dark forest theory of the web, I'm first going to
have to explain the dark forest theory of the universe. So this is a theory that tries to
explain why we haven't found intelligent life in the universe. Here we are in the universe,
the pale blue dot, and as far as we know, we are the only intelligent life around. We've been
beaming out messages for like 60 years now with like SETI, trying to find other intelligent life,
trying to see if there are aliens or some other kind of being that could respond to us. We haven't
heard anything back. So the big question here is why? So dark forest theory says it's because
the universe is like a dark forest at night. It's a place that seems quiet and lifeless
because if you make noise, the predators come eat you. So if you draw attention to yourself,
you're going to be attacked and destroyed. So it stands to reason that all the other
intelligent civilizations that may or may not exist have either died or learned to shut up,
and we don't know which one we are yet. So the web version of this builds off this concept.
It's a theory that was proposed by Yancey Strickler, who's a really good thinker and
writer back in 2019. And Yancey wrote this article describing what it feels like to be
in public spaces on the web around this time. And Yancey pointed out these two main vibes,
let's say. And the first is that being on the web often feels like a very lifeless, automated
place that's devoid of humans. It's got all this ads and clickbait and predatory behaviors,
and none of it feels like real humans are trying to connect with us on the web. It just feels
commercial. The second vibe, actually, here, so here we are on the web, and we are naively
writing a bunch of very sincere, authentic accounts of our lives and thoughts and experiences,
and trying to make connections to the other intelligent humans. We're sending out messages
trying to beam out life. But then what we hear in response is content that seems very
inauthentic and human. It sounds like a bunch of robots and automations doing marketing
automations and growth hacking, kind of pumping out generic clickbait. So we've seen all this
stuff. This is like low-quality listicles, productivity rubbish, growth hacking advice,
banal motivational quotes, dramatic clickbait. A lot of this may as well be automated,
even if a human didn't make it in some sense. They're rarely trying to communicate some sincere
original thought to other humans. They're trying to get you to click and rack up some views.
And this flood of really low-quality content has made us retreat away from the public spaces of
the web. It's very costly for us to spend time and energy wading through all this craft.
So the second vibe of the dark web is that there's a lot of unnecessarily antagonistic
behavior at a large scale. So when we are putting out all these signals, right,
trying to authentically connect to other humans, we could become a target, right?
We risk the Twitter mob coming to eat us is what can happen.
So there's a term on Twitter called getting main character. I don't know if people have
heard of this. And I don't know if people remember, this is one year ago, Garden Lady.
This was a very famous tweet, if everyone saw this one, where this really lovely woman
got on Twitter one morning, and she said, my husband and I wake up every morning,
and we bring our coffee out to the garden, and we sit and talk for hours, and it never gets old,
and we never run out of things to talk about, and I love him so much. And everyone's like,
that's such a nice tweet. That's so wonderful. And then this went viral. They got picked up.
And the Twitter reply started rolling in. So someone said, that's cool. I wake up every morning
and fight my way through traffic for an hour in Miami to get to work. That must be nice.
Someone else, I wake up at 6 a.m., right? This is an unattainable goal for most people.
Not that she said it was a goal, but interpret it as you will. Another one, again, complaining
about the morning routine, and then goes, it must be nice being a trust fund baby with not a care
in the world. So I thought this tech talk summed it up nicely. I don't care if something good
happened to you. It should have happened to me instead. So this seems like a dumb example,
but it was a really good moment on Twitter that shows kind of the energy flows that
happen on these really large-scale social media platforms, right? Someone can publish something
that's very kind and nice, and it gets interpreted the wrong way. People take it in bad faith.
They take it out of context. They try to amplify it to unintended audiences. We will take every
opportunity to misinterpret things and be ungenerous to each other. And this is how we get
cancelling some pylons, and we've all seen so many examples of this happening in what seems
like a very unfair way. John Ronson wrote an entire book about this, called You've Been
Publicly Shamed, the catalogs, quite a few of these. It's a little out of date now, but it's a
lot of kind of classic original examples of people getting cancelled, and then the real material
consequences they fix, right? They lose jobs, they lose friends, they are alienated from their
community. It is not just internet drama. They really do have to face repercussions for these
pylons. And so this makes the web a very sincerely dangerous place to sincerely publish your thoughts,
to publish honest things on. And this makes it hard to find people, right? It's very difficult
to find people who are being sincere, who are seeking coherence, and who are trying to build
collective knowledge in public. I know this is not what everyone wants to do with the web,
right? Some people just want to dance on TikTok, and that's completely fine. We have to let them
do that. But I'm interested in at least some of the web enabling this kind of productive discourse
and having spaces of community building, and I'm hoping some people here feel the same,
right? Rather than it being like this threatening inhuman place where you can't actually say what
you think. So how do we cope with this, right? We're all wandering around this dark forest of
like Facebook, and LinkedIn, and Twitter, and we realize we need to go somewhere safer. So what
we end up doing is we retreat, primarily to what's been called the cozy web. So this was a term
coined by Venkatesh Rao in direct response to the dark web theory. And Venkat pointed out that
we've all started going underground. We move into semi-private spaces like newsletters or personal
websites where you're less at risk of attack, you're not on these big platforms, you're on your own
separate domain, or you're on your own separate newsletter. So this gives us some safety. We can
at least decide a little bit who reads it and understand our audience. But we often retreat
even further into gate-kept spaces like slacks, WhatsApps, Discord, Signal groups, right? This
is where we end up spending the most of our time and having real human relationships where we can
express our ideas safely, right? So things that we say we know will be taken in good faith in these
smaller groups. We can engage in real discussions. But there's some problems here, right? Like,
none of this is indexable or searchable. It's very hard to include people who aren't already in the
group. And it hides collective knowledge in these private databases that are even hard for the users
themselves to access. And also, like, good luck finding anything on Discord. Like, you'll never
be able to use the search functionality in these apps. So my current theory, sadly, is that the
dark forest is about to expand because we now have this thing called generative AI. So I'm sure
everyone has mostly heard of this. But what I'm talking about specifically here is machine learning
models and neural networks that can create content that, before this point in history,
only humans could make, right? This is text, images, audio, and video that mimics human
creations in a very compelling and believable way. Here are kind of some of the major foundational
models that you might have heard of for different media types, right? We have GPT4 and Claude for
text, mid-journey and stable diffusion for images. There's now video ones like runway ML. And you
might have heard some of the news that a lot of these models are now becoming multimodal so they
can do text to image, image to text, audio to text. Like, you can kind of go anywhere you want
with media here. And this is, of course, chat GPT. I'm sure we've all seen 1,000 screenshots of this
at this point. We understand what it is. But to recap, it's right, we know it can generate huge
volumes of high-quality text in seconds. And the outputs are indistinguishable from human-made text
when we try to get people to guess what's chat GPT and what's human. They often can't. It's trained
on a huge volume of text scraped primarily from the English-speaking web. And this all sounds
very simple, right? But it leads to many kind of complex and potentially useful behaviors,
but it's very emergent. We don't really understand what's possible because of this capability yet.
We can also now, of course, generate images, right? This is mid-journey, which is usually
makes pretty beautiful, impressive stuff. So we've found that these, like,
generative AI models are now very easy to use and very widely accessible, right? They don't
require technical skills. They're incredibly cheap. And they're increasingly becoming a feature in
existing software you already have access to, like Adobe or Photoshop or Notion. They're just
becoming pervasive. But the product category that I'm most nervous about, not just, like,
Notion generating a plan for you, is what's being called content generators, mostly for
content marketers. So I'm going to pick on one product, but there are many. This one's called
Blaze. And it creates articles and social media content for you, right? And half the time,
who wouldn't want that? Who doesn't want more content on the Internet?
And so I want to show you how this works. So you decide what kind of content you want to make,
right? You can say a blog post or a newsletter or a bunch of Twitter posts. And I'm going to say
I want to write a blog post. And you type in what you want to write about in your target audience
and SEO keywords. So I've decided I want to write about why plant-based meat is morally wrong,
which I don't believe, but that sounds like a good clickbait to me. Like, someone's going
to be like, yeah, I want to find out why that's bad. You know, maybe I'm a company that has
some financial interest in plant-based meats going badly. So I'm going to go ahead and have this
model write a little article for me. It lets me pick a title, which is nice customization.
And then it chans out 700 words, right? And this is now ready for me to hit publish, right? Or at
least gives me some base to work off. And if I'm blobbing against plant-based meats,
I could just generate 100 of these, right? And optimize them for Google SEO and publish them
all at once. And like, hard days out, because you're done. Right? The quality and truthfulness
of what's written in here is very questionable. We'll get to problems with that later. But the
point is this is super easy to do at scale very cheaply. And it essentially murders Google such,
right? Like, this just does away with SEO optimized content because anyone can publish this
immediately. It gets even better at the end. Like, it prompts me to generate more content.
So it's like, oh, you have this blog post. Why not generate LinkedIn posts and tweets and YouTube
scripts and everything? We're not just getting crappy Google articles. This is across every
publishing platform. Right? So there's tons of things to do this. There's like AI-linked post
generators, generate your next tweet, right? YouTube content and autopilot, just thousands of
these tools are pouring out. So most of the examples I showed actually have a very simple
architecture, right? You have a single input, like write me an article on plant-based meat,
and you feed it into this big black mystery box of a language model, right? And we don't really
understand totally what happens inside, but it gives you an output, right? Right to an essay.
But you can't really tweak what happened in the middle. You can edit the output, but you can't
kind of pull the knobs on the actual language model itself, which isn't very sophisticated. We
don't have a lot of control or transparency in what's happening. But the industry has realized
this is a problem, and we started building architectures that are much more flexible and
powerful. So we now have a language model architecture where we take that same black box
of the language model, but we give it access to external tools, right? We say, okay, now we're
going to tell it it can search the web through an API. We give it access to a calculator. We give
it access to a code repel and APIs. It's now getting a lot more capable, right? It can now look up,
it can do maths, you know? It can look up information that it thinks might not be right.
It can double check its answers. Also, language models are usually quite forgetful. You might
have found this. We can now hook them up to long-term memory databases and have them reference
things like many weeks or months in the past, which makes them a lot more capable. And we also
found that they perform much better if you give them these cognitive prompts, like you tell it
to do something, but then you say, you know, think about your answer, critique it, and then answer
me again. And that actually improves the quality of the answer quite a lot. That's often called
chain of thought prompting, self-critique. It can observe what it knows and plan the next step,
and it's getting these more cognitive capacities by adding on these kind of extra techniques.
So this is being called the agent architecture, right? You tell the language model to act like an
agent. It ends up as being like the centralized brain, and you give it, you can say you can use
any of these tools, and then it composes which tools it wants to use to achieve your goal. So it
ends up being a chain like this, where you give it your goal, it'll like observe, it'll plan,
it'll call a different tool, it'll observe, it'll plan. And we can actually do really complex,
kind of scarily impressive things when we level up to these more sophisticated architectures.
And actually on Monday, OpenAI did this big dev day talk, I don't know if people saw this,
and they announced a new API called the assistance API that makes all that stuff that I showed that
used to require quite a lot of Python code and kind of insider knowledge,
and they're just making it super easy for everyone to now do this architecture,
where you're able to kind of run any function, call any API, all hooked up to their really powerful
models. So we're about to enter this phase where this very capable agent architecture is becoming
pervasive and widespread and might be the foundation of a lot of new tools being built.
So we're sort of on the precipice of really unnerving moment, let's say.
Because agent architectures, I think, means we're about to enter a stage of sharing the
web with non-human agents, right? These agents are very different to what we've currently
noticed bots in the past, like a completely different architecture instead of capabilities.
They're going to have a lot more data on how realistic humans behave,
and they're rapidly going to get more and more capable as time goes on.
And soon, probably already now, we're not going to be able to tell difference between
these agents and real humans. If anyone else spends a lot of time on Twitter slash X,
you'll already have noticed there's a lot of accounts you stumble across that have a weird
vibe to them, and you definitely realize this is just chat GPT hooked up to a Twitter account,
but otherwise it's trying to look real, but every tweet is very optimized and comes back
in a second. It's just replying to things three seconds later. So it's happening.
And sharing the web, I want to say with agents, I don't want to jump to saying this is inherently
bad. I think they could have lots of good use cases, right? We could have automated moderators
in communities. We could have search assistance, but I think it's mostly that it's going to get
complicated, and this is going to be a huge product and cultural problem we're going to need
to think about carefully and deal with. So we should get into why is this a problem for the
web, right? I'm only going to focus on how this will affect human relationships and information
quality on the web. Anything else, like how we might all end up unemployed or dead soon,
is like well beyond my pay grade. So I'm just limiting the space to just like how do we make
meaningful human connections and find a good quality content on the web. Because, yeah,
the cost of creating and publishing content just dropped to almost zero at this point, right? Like
humans are quite expensive and slow at making content, right? We need time to research and think
and we like clumsily string words together and then we want to take breaks and we want to be able
to nap and eat and sleep. And then we demand people pay us like extraordinary rates, right,
to do this research. And generative models don't need time off and they don't get bored and they
cost like a couple fractions of a cent to write a few thousand words. So given the dynamics here,
it's very likely that models are going to become the main generators of content online.
So I think we're about to drown in a sea of informational garbage, right? I think we're just
going to be absolutely swamped in masses of mediocre content. Like every marketer and SEO
strategist and optimizer bro is just going to have a field day here, you know, just filling the whole
internet with all of their keyword stuff, optimized crap. And this explosion of noise is going to
make it really difficult to find both good quality people, real people, and good quality content
and hear any signal through the noise. And we can tell this is happening because scammers and
scammers are currently quite lazy and we're kind of in the baby phases of this. So there's a
phrase that you might have seen chat GPT reply with it sometimes says as an AI language model,
I do not have political beliefs or as an AI language model, I cannot answer that question.
And this phrase, if you search and direct quotes for it around the web, shows up everywhere. Just
like Amazon, Google, Yelp reviews, tweets, LinkedIn posts, it's full of this phrase because people
can't be bothered to like control F and like delete the one phrase that gives them away.
All right. So I did a quick search for this on LinkedIn. It got 16,000 hits and they're like
really boring attempts to like write engaging content, but they all begin with the phrase as
an AI language model. Look at the first sentence. And these are real people too. I did look at
their profiles. They genuinely have jobs and they're trying to optimize their presence or something.
But yeah, this is starting to happen. The motivation for doing this rate isn't hard to
understand. So let's like think of the hypothetical scenario. So this is Nigel. He's written a book
about why nepotism is great, right? And he wants to be a book fluencer. He's like it's his first
book. He's self-published on Amazon. He wants to like, you know, become a big book guy.
So he spends up an agent, right? Not unlike an actual publishing agent he might have hired in
the past. And he says, hey, like help me promote my book, you know? And so the agent thinks for a
while and it goes off and it strategizes and it generates a steady stream of tweets, right? Based
about on the content of the book, like real insights from the book and it starts tweeting
those out from Nigel's account and he's given it access, you know? And it goes and it does the same
thing for LinkedIn and Facebook, you know, pretty easy. And then it writes and schedules a newsletter
to go out over the course of six months so that his followers will always kind of get updates on
new things he's researching. It sets up a medium account. It reposts those as articles, right?
Makes a set of addictive TikTok videos based on that content. Generates a bunch of podcast
episodes, use Nigel's voice. We can totally do that now. It's pretty easy. And then it finds
a bunch of other people who like are talking about nepotism and starts replying to them on
LinkedIn and Twitter and making friends. Maybe they're actually agents interacting with it and
like it's a whole bunch of just agents interacting with agents. And none of this is different to
what Nigel could do on his own. So we don't know that like content moderation or like spam filters
are actually going to pick this stuff up because maybe it's, you know, tweeting it slow enough
that a human could have done it and it really is in Nigel's voice. It's used his writing to
write this content. We don't necessarily have automated ways to filter any of this out.
And the thing is like without an agent, you know, 99% of Nigel type people wouldn't have gone to all
this effort. They don't have the time and energy to have made all this content. But with the agent,
you know, suddenly we have people like Nigel, but like times 99 of them able to create this amount
of content all the time. And this is how we kind of get the flood of just tons of content more than
we can really cope with. So the scale and the quality of the content is actually what's different
here. I mean, strangely enough, it might have written better stuff than Nigel ever would.
We might have like way better quality content about nepotism all over the internet.
But like you can imagine how this would play out at another 100 X scale, right, with political
lobbying groups who have very vested interests in certain ideas or beliefs or truths getting out
into the world's specific agendas, you know, large companies that want you to believe certain
things about their product or certain things about scientific claims. They all have access to these
assistants and agents too. So I do have some good news. Like this is all a bit dark, a bit of a
downer. The good news is that this might not be a problem. Maybe this is all just fine, right?
This is only a problem if we want to use the web for very particular purposes,
such as like facilitating genuine human relationships or like pursuing collective
sense making and knowledge building or like grounding our knowledge of the world in reality.
So like we don't care about any of these things. This is all fine.
Like we're going to have amazing content on TikTok. We're going to be very entertained.
The thing is like I'm quite keen on a lot of these outcomes, right? I write on the web a lot.
I've had overwhelmingly positive experiences writing on the web and meeting people through
doing that. I really, I have this whole thing called digital gardening I bang on about about
making everyone publish their kind of unfinished notes to the web and improve them over time and
use that as a way to meet people interested in what you're interested in. But yeah, the goal
of that stuff is to make the space, the web a space that's possible for collective understanding
and knowledge building. And I'm really worried that generative agents like meaningfully threaten
this in the very near term, like the six to 12 month kind of time range. Like I can't even think
beyond that. So when I talk to people about my worries, I talk to a lot of people in the AI
safety and AI research world. So they kind of go like, oh, why does it matter? Like,
you know, my AI agent is going to make much better content than you ever would. Like,
why do you care that an agent made it and not a human? And I'm like, okay, let's like engage with
that question properly. I'm sympathetic to that point. So here's the reasons that generated
content is a little bit different, right? The first is its connection to reality. The second is
the social context they live within. And the third is its potential for human relationships. And I'm
going to go into each of these in detail. So first, generated content, you probably have heard
of this, is different because it has a different relationship to reality than we do, right? We
are embodied humans, right? And we are sharing a physical reality. And we have all this, like,
rich embodied information. Like, we all understand we're in this kind of beautiful theater and we're
in Brighton and we have a lot of physical embodied context about what we know about each other.
And often what we're doing on the web, right, is we're reading other people's accounts of
this reality and we compare it against our own and we're like, how do I agree with that? Is that
really true? Right? This is like the cycle of all of like art and science and literature, you know,
reading and comparing and writing your own version of things. And what we've done now is we've fed
that huge trove of information into a neural network or a large language model. And it's created
a sort of representation of that text, right? It's created a model of the things that we've
already known about the world and have published to the web. And the thing is that model can now
generate text that's predictably similar to what it was before, but it's totally unhinged
from the physical reality that it once came from, right? It has some big connection. It did come
from there. There's like a chain here, but it's fully like cannot access that reality, right?
Even if we put in robots, right, with like arms and eyes and ears, it can't sense the world the way
we sense the world until we make like a fully synthetic like mimicry human that like is hooked
up to a language model, but I think like 10 years away, I don't know, I think we have some time.
Right. It can't validate its claims. It's the big thing.
We politely call this hallucination, right? This is when language models say things that
aren't true about the world. We say it's hallucinating, right? Like it's some side kind of
very smart person on some like mild drugs who's confused about like who they are or where they
are, but they're saying very intelligent things. You're sort of holding them lightly, you know?
Language models are also different because they have a very different social context, right?
They have a very strange relationship to our social world. So hopefully you know this, but
everything you and I say is situated in a social context, right? We understand what we share in
common and if you met someone who spoke a different language from a different culture,
you would not assume they thought the same things about the world that you would if you met someone
from your own neighborhood, right? If one of us met someone from like Dickensian England, we would
have very different understandings of like hygiene and science and like how the world works. We would
know some things in common. We technically speak the same language, but we would know that we didn't
have a shared social context in the same way. But a language model is not a person and it does
not have a fixed reality, right? They know nothing about the cultural context of who they're talking
to and they take on different characters depending on what you tell them to do. You can say, you know,
pretend to be a professor, pretend to be an athlete, pretend to be a young child and it will
take on that character. So it doesn't even have a fixed place it's talking to you from in the way
that a human does. But they do represent a very particular way of seeing the world because we
trained them primarily on text on the web that was generated by a majority English-speaking,
like 95% of the training data is English-speaking, a primarily English-speaking westernized
population, people who have mostly written a lot on Reddit and lived between about 1900 and 2023,
which like in the grand scheme of history and geography is a very narrow slice of humanity,
right? Of all possible cultures we've had in the past, all possible cultures we've had in the
we could have in the future and all possible languages. This is just such a small representation
of reality and yet we're now making it the source of truth, right? The oracle. You go to chat,
GBT to ask everything. So it's taking this already dominant way of seeing the world and
reinforcing that dominance which is problematic and is like a whole different talk that I don't
even think I'm qualified to do but someone should. And we hope that this will improve over time but
it's really hard to do without lots of data and most cultures don't have the vast kind of written
record that an English-speaking westernized online in population does. So lastly, generated content
lacks the potential for human relationships that human-made content does, right? If you write
something online and I read it and I find it compelling, I can DM you on Twitter or I can find
you on Blue Sky or I could find you somehow ideally, hopefully not on LinkedIn, but somehow
and message you and be like, I love this. This was such good ideas. Like I want to write a piece
in response to you and like we start having a little dialogue that I've had so many relationships
blossom this way. But if you have a language model, it's not going to be able to do that.
So this is a still from the film Her, right? This has become kind of a cultural touch point of like
parasocial relationships with AI. Hopefully people have seen it but if not, so Joaquin Phoenix,
our lovely main character, he has this great relationship with his personal AI, he talks to
her in his ear, it falls in love with her. But then the AI of course grows bored of him because
he's a very kind of basic human and leaves and he's distraught. And like some people were supposed
to get this like film is supposed to be a warning, right? And some people took it as a suggestion.
So there's a company called replica who make AI companions for you that you can make friends
with, possibly date and fall in love with. There's a lot of suggestions of sort of lonely young men
engaging with this and their marketing copy. And I mean, maybe I do need to explicitly point this
out. An AI replica or any other kind of like generative agent person cannot fulfill all our
human needs, right? They cannot give you a hug, they cannot come to your birthday party,
they cannot kind of engage with you in a meaningful, full human way. And so any kind of language
model agent on the internet has no capacity for that back and forth relationship. Even if it faked
it, it's very unclear that would actually satisfy what we need when we have an actual friend that
we can go out to coffee with. So that all sounds quite bad again, like deep breaths. The whole
talk is really just digging you in a ditch, I'm sorry. But I'm now going to talk about possible
futures. And again, these futures, I think, are not mutually exclusive. I think they all might
unfold in different ways over the next five to 10 years. And like, I can't speculate beyond that,
God knows where we are. But yeah, I'm hoping they all kind of happen in parallel. So the first is,
I think we're about to spend a lot of time thinking about how we pass the reverse Turing test.
So how do we prove we're human on a web filled with agents? So the original Turing test, right?
You have a human talk to a computer and another human through like a wall so they can't see the
typing messages of each other. And then the original test, the computer had to prove that
it was the human. It had to prove it was competent. And on the new web, we are now the ones under
scrutiny, right? We have to prove we're real. So we're going to end up like we will assume
everyone is an agent until proven otherwise. So I kind of wrote this post where I was thinking
about some short term tactics. Like we could use funny terminology. We could all try to become
teenagers who like have this inside or jargon that the language models don't know about,
but they'll pick up on it pretty quick, you know, and then you'll have to abandon it,
get new jargon. We could write in non-dominant languages. If you speak something like Catalan
or Welsh, you're probably in a pretty good position, you know, you'll be able to write
in a way that's more native than a language model ever could. And ideally, we just do higher
quality writing. We do more research. We do more critical thinking. We really reference
events and people that we could only know about from the real world, from being embodied humans
in space. I don't know how long those kinds of defenses will last, but that's in the short
term something we can at least pull on. This next one, I apologize for the phrase, but it too
perfectly captures the point. I'm not going to explain it. You can Google that later. But the
point is that the content from models might end up becoming our source of truth. And how we know
things simply was like, well, a language model once said it. And then it's forever captured in
our circular flow of information. In this current model, the training data is at least based on
real-world experiences. It's kind of going in a single loop. But we're now going to use that
generated text to train new models. And so we enter this loop where there's this very tenuous
link to the real world that was once a long time ago the source of this data. This is already
starting to happen. AI researchers are worried we're kind of going to run out of data to train
models on within five years. And so there's a lot of talk of how do we generate information
that we can feed back into the models. This one was a really funny example of this happening.
I don't know if people saw this. So someone noticed that if you ask Google if you can melt an egg,
it's like smart AI summary said yes. And they were like, why would it say that? They went
investigating. And it turned out that fact was pulled from Quora that was generated from chat
GPT. And because Quora is considered a reputable website with good SEO standing, it got pulled up
into the Google like smart answer. And it's not hard to imagine how all kinds of hallucinated
answers are going to become part of this loop if all these major websites are using chat GPT
or their own agent to generate answers and then they just feed it to each other. It's like at one
point, can you melt an egg? This phenomenon is very worrying for the scientific community and
they have good reason to be. We're already seeing a lot of evidence that scientific researchers
are using language models to help them write papers. So again, this is a paper that was published
in a genuine fairly legitimate journal, environmental science and pollution research,
and it included the phrase regenerate response at the end of a paragraph, which is the button
above chat GPT's input box. There's another one in August that was published on fossil fuel
allocation that included the phrase as an AI language model. Now, there's a lot of debate in
the community where they were like, well, this doesn't mean the science in these papers is
totally false, right? They could have done real research on real experiments and they were just
trying to get this paper out and at the end they went, you know what, chat GPT summarizes paragraph
for me. That totally could have happened. But the problem is we don't know. There's no protocol
for the transparency of how you say what you didn't, didn't use chat GPT or whatever kind of model for.
So now people are trying to make it more legitimate. Some people are listing chat GPT as an author on
papers. And there's a real risk that people with much worse intentions will kind of take this and
run with it and just make scientific paper mills. There's a lot of companies that have a lot of
vested interests in publishing science that agree with the thing that they would like to be true.
Usually you find this out when you get to the funding source section of the paper where you're
like, oh, yeah, funded by the people who make this drug. Interesting. But you can tell that if
they're going to be able to use generative models to kind of pump out lots of research papers, maybe
based on dubious science, it becomes very hard for us to tell what is actually real, what is vetted.
I mean, it just takes the whole, the replication crisis to a whole new level. So this one seems
the most obvious, right? One possible future is we will just retreat further into the cozy web,
right? The dark frost will grow larger and we will just go, okay, I'm only interacting with
Discord and WhatsApp, right? LinkedIn's dead, Twitter's dead. We've had to abandon it. We have
to maybe make new privatized gate kept spaces, which I think has a lot of downsides, but this
just might be the best way to deal with it. I think authors are going to increasingly put
content behind blocks and paywalls. I think this is already happening with things like
Substack and Medium, where you're constantly having to log in or prove that you are part of
this community to access the content. And you understand why authors do this, right? Because
actually, you're having your content scraped and then fed into generative models, puts you in a
disadvantage. Like, your ideas could be taken out of context, maybe it's taken something that you
wanted to actually charge for and give now for free. Someone could train a model on your work
and have it start writing in your voice. There's a lot of ways this can go really badly for someone
whose full-time job is being a researcher or a content creator. We'll also see more websites
that have a large amount of content blocking scraping for language models. Or one way to do
it is to just charge a huge amount for your API. So Reddit did this recently. We're going to put
the price so high that no company in their right mind would really pay it, or it would just cost
them so much. And Twitter kind of did the same. It's hard to tell if this was actually strategic
or on purpose, but raising the price to whatever it was, like $42,000 per month,
means that very few people can access Twitter's really high-quality content in an age where
content to train models is really the new goals. So it's kind of leading us to a place where the
web is not open by default. You can't just query any API for any content you want. Everything is
locked down and gate capped and kind of cordoned off. Okay, next, I think we're going to have what
I'm calling the meat space premium. So this is, we are in the meat space premium. It's when we begin
to prefer and preference offline-first interactions. So we will start to doubt all people online.
And the only way to confirm someone's humanity is to meet them in person, right, to go for coffee
or beer. And once you do that, you can kind of set up a little trust network, right? You can say,
oh, I've already met Sarah over there, and she's a real human, and you've already met Tom, and he's
a real human, and we can kind of like coordinate our networks to that who's real on the web. And
then when you read their writing, you kind of know it's from an actual person, or you'd hope so.
Maybe you have some trust network of people who aren't writing generated stuff under their own name.
I think this has knock-on effects, like people might move back to cities or
higher population and places. In-person events are going to be preferable. I think there's
obvious disadvantages to this, right? The web was this huge democratization thing to enable people
who are maybe disabled or have young children or who are caregivers who can't get out of the house
for a whole bunch of reasons aren't going to have the same access to the trust network
that someone can who could physically show up in space a lot.
Yeah, so a natural follow-on from this also. So a lot of people have been like,
well, why don't we just put it on the blockchain? Why don't we just get a third party to verify our
humanity with a cryptographic key, and then you can sign all your published content with it,
and it'll link back to your identity, and this is how we'll have decentralized trust networks.
And I'm like, okay, I don't know the details of this. This sounds weird. So there's a project
called Worldcoin that, funnily enough, is also funded by OpenAI's leader, Sam Altman. He partially
helped found it, which shows he kind of knows the problem he's helped contribute to, and so this
scary orb scans your eyeball to confirm your identity, and then it creates a unique human
credential for you to use online to sign all your stuff with. It's really not taking off as a project,
but it's around, and people are still trying to do this. There's a whole community
thinking this is the future that I don't have sophisticated thoughts on yet, and I'm still
like, oh, cryptocurrency. But maybe this is some way to get around it. I'm also expecting any day
that Elon's going to announce the purple check, where you pay $30 a month, and you don't actually
have to just take a box that's like, I'm human, and then you get this little check and solve it.
So those are all a bit negative. I do think there is some hope in this future. We can certainly
fight fire with fire. So I think it's reasonable to assume that we're all going to have a set of
personal language models to kind of help defend us and serve our needs on the web.
They can filter information, they can manage information, and I expect these to be baked
into browsers or maybe even the operating system, and they're going to do things like identify
generated content, they're going to debunk claims, they're going to flag misinformation,
they're going to go help hunt down real scientific sources for you, maybe vet scientific papers,
curate and suggest things to you. So I think this actually could work in both directions. It's not
just all the bad actors get this power, we also get a lot of capacity and capability from these
models. We might find it absurd that anyone would browse the raw web without one of these
kind of in tow. It's the same way you wouldn't go on to the dark web, you know what's there,
but you know you don't want to see it. It might be kind of like that.
Okay, so I'm almost done wrapping this up. But the question I want to leave everyone with
is which of these possible futures would you like to make happen, right? Generative AI is not
necessarily a destructive force, you know, as with all technology, it depends how you wield it.
Oh, I went back to the wrong slide, sorry. There we go. The way that we choose to deploy this
in the world is really what matters, right? The product decisions we make as individuals
and companies, if you are working in the space or you're trying to get into it,
because obviously if you are working on a tool that like churns out tons of human-like content
from marketing and influence purposes, like you can stop, like that's really like we don't need
that. You can just stop doing that. But what should you be building instead? It's maybe a
more helpful question. So I tried to come up with a few principles for building products, language
models that are probably going to evolve over time, but this is like a first pass. And the first is
to protect human agency. The second is to treat models as reasoning engines and not sources of
truth. And lastly that we should be augmenting our cognitive abilities and not replacing them.
So protecting human agency, this is like usually at the moment you have a human prompter and it
hands something off to an autonomous agent and the agent goes and does stuff, right? This is like
the open AI assistance model or any of the architectures I showed before. And this is like
the path to self-destruction. This is what most AI safety researchers are very afraid of, is that
the locus of agency sits within the agent. But the ideal form of this is that the locus of agents
stays within the human and it has a collaborative agent on hand and there's this very short,
continuous feedback loop that is constantly going between them. Where the human is the one checking,
should I do that? Do I want that? Is that true? Like they're able to actually fact check things and
then the agent is much more of a helper. Short feedback loops, close supervision, limited power.
It's slower, but it's safer. That ties into the second principle that we should treat models as
tiny reasoning engines and not sources of truth. So one way to use these models is to ask it for
every answer and ask it every question and trust what it says. Another one is you can train them
to do specific things. Just summarize this text. Just extract data from this paper. Just find
contradictions in this statement. And then you can bring your own data, which could be legitimate
scientific papers. It could be your own notes. It could be Wikipedia. And then you use these models
to just do these very small scoped things where you can observe every single output and check that
it's actually legitimate and you're not handing off this big complex task to this big black box model.
And lastly, we should augment our cognitive abilities and not replace them, right?
Language models are very good at things that humans are not good at, like searching and
discovering things in large data sets, role playing as identities and characters. They're
actually really good at doing that. Rapidly organizing data, turning fuzzy inputs into structured
outputs. There's a lot that they're good at that we're bad at. And we should use them for those
things. There's tons that we're good at. They're not. That we're still trying to make them do,
like checking claims against physical reality, long term memory, having embodied knowledge,
understanding social context, having emotional intelligence. I think combining the two of
these so that we're doing things models can't do and they're doing things we're not very good at
actually leverages the best of both worlds. Because a lot of AI researchers in the moment,
they use this metaphor of aliens. This is from the 1970s alien film, Oil Frightening. It makes
me think this is not the most appealing collaborative partner, this metaphor, this big, scary,
unknown consciousness that might kill you. But there's another metaphor that I like more,
that Kate Darling is a robotics researcher at MIT and she wrote this book called The New Breed,
arguing we should think about robots as animals. We have a long,
cultural, legal history with animals and a working collaboratively with them,
oxen, dogs, pigs, right, in this very like mutually beneficial relationship most of the time.
And this is actually a pretty good metaphor to expand to AI, where we have to kind of treat them
a little bit like some form of intelligent species, but one that we are in community with and are
part of our systems and are not this like big, scary alien who might come kill us all. It's like
usually what it gets talked about as. So yeah, there's this big push for this philosophical
approach. Some people call it cyborgism. There's a very long article that was written on less
wrong, which is not my favorite website, but it's a good article that kind of goes in depth into
this if you do want to read more about it. So that's all I have. I want to thank you so much
for listening. Again, slide the notes on this QR code. If you missed anything, I'm on Twitter X
at mappleton still until that really does fall apart. And you can DM me there. You can message
me. Again, I love meeting people through writing on the web. If you have blogs that like relate
to these kind of topics, send them to me. But yeah, thank you so much for listening. I appreciate it.

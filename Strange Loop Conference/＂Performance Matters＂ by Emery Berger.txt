Hi everybody, so I'm Emory Berger. I'm going to be talking today about performance matters.
This is joint work with my former PhD student Charlie Kersinger. So I'm going to tell you
a story that happened a short time ago in a valley far, far away. Not actually this one,
but in fact this one. So the story is about a character who we're going to call Luke. Luke is
our hero. Of course he's a hero in Silicon Valley because Luke has an idea for an app. Alright,
so here's Luke's genius idea for an app. Basically you use your smartphone and you take pictures
of things and then it goes and finds matches like and returns them to you and he's got a great name
for it as well. It's going to call it Ogil and he's pretty sure Ogil is going to totally disrupt
image search. Nobody has told him about Google image search yet, but anyway. So he sets about
building it so he makes the prototype Ogil. So the prototype Ogil works like this. Take a picture,
it gets sent off to the Ogil data center. If it is new, it gets added to the database and then at
the same time it goes and finds similar pictures and sends them back to the user. Alright, so this
is pretty straightforward. You can kind of abstract out this flow. If you think of it as follows,
you get a request. The request comes in. It essentially spawns two threads. One of them goes
and takes the image and let's say compresses it and then saves it to a three and a half inch floppy
as one does. And then at the same time it does some indexing to look up matches. Right, so it
does a search after doing some feature extraction and then sends it back to the user via paper
airplane, which is the most effective method known to man. And then eventually these two threads
join up. Alright, so this is obviously a simplification. I've alighted certain things like
for example locks. So when you have locks, right, there's some database for example. You can't be
modifying the database simultaneously, blah, blah, blah. But anyway, this is essentially at a
high level how Ogil is going to work. So Luke goes and ships it off to the app store. It gets
approved and you know users start coming and Ogil is working. Right, he's disrupting image search.
But then as the number of users kind of like mount, it turns out Ogil starts getting slow. Right,
so it takes a long time for Ogil to load. Alright, so this is too bad. He's not happy about this.
He's not really sure what to do. So, but before we get into that, let me talk about another version
of Luke. This is also Luke. This is the cool hand Luke, if you will, with his glasses. Alright,
but really this is Luke in the 80s. Alright, so he's got the 80s glasses to match and of course
those of you old enough to know or who have watched TV know that there were no smartphones in the
80s. There were instead computers with big CRTs. So this is what Ogil looked like in the 80s. So you'd
have some ASCII art and you want to go out and it would connect via some modem let's say search
of the database. This is version 1.0 of Ogil 84 and it comes up with its matches and this is your
user interface. So hope you liked it. Alright, now of course back in the day 1984 computers were
really slow. Right, really slow and so actually this you know Luke today who is like Ogil is too
slow. But things were really bad in 1984, right, but it turns out things were also kind of better
in a way. So performance used to be really easy. So I'm sure all of you have seen this kind of
graph. What I'm going to show you is on the left the number of transistors as years progress on the
x-axis. If they go up you'll notice it is a log scale and on the right you have clock speed which
roughly corresponds to computing performance. And so basically for years and years and years we had
this situation where the number of transistors were increasing roughly at doubling every 18 months.
This is famously Moore's law. And this generally had the effect of allowing clock speed to increase
and so you had smaller features. This meant that your programs would actually run faster and in
fact you could just literally wait. Right, if you buy new hardware in fact in the upgrade cycle
every year or so like if you bought a new computer everything would run much faster.
Now things were slow so don't be so excited about how great it was back then. It was bad.
33 megahertz to 66 was awesome back then and kind of terrible today. But it did change the landscape
of what it meant to be somebody who works on performance improvement because this is what you
would do. So there really in a way was no sense trying to squeeze out any performance out of
something when it was going to double in 18 months. The chance that you in 18 months were going to
squeeze out double the performance pretty slender so you might as well just sip mojitos on the beach.
I can tell you that this was well maybe the mojitos and beach part notwithstanding.
This was actually kind of a strategy that was recommended for high performance computing. People
would literally say we're just going to upgrade the computers next year or in two years so focus
on something else. Don't focus on performance. Now unfortunately that's not the current state of
affairs. So today as all of you know when you upgrade a computer it does not get faster anymore.
Maybe it gets lighter. Maybe it has improved battery life. Maybe not. But basically what happened is
eventually Moore's law kind of ran out of steam. The reason that it did is not actually
it's not actually true that Moore's law ran out of steam. This is technically a problem called
denard scaling. And so denard scaling ran out. And so right now we're in a situation where
basically if we clock things up we no longer can dissipate the heat on chips effectively enough
to make them run faster. So transistor counts actually are still increasing which is why we
have multiple cores. So we have a bunch of transistors. What are we going to do with them.
Let's just stamp out more of the same thing. Right. So that's great. But it's also not super
awesome because if you have a program that is not parallel it's not going to run any faster.
So everything now has multicore. Your your phone has multicore. But it turns out it's still a
problem. So these are actual screenshots from app store updates. It's every app store update
practically is about performance or bug fixes. And so here's one that says under the hood updates
for better performance as opposed to the over the hood updates. Okay. Here's bug fixes and
performance improvements bug fixes and important performance improvements. And then this one we've
I like like love this one a lot. So they're app engine calibrations because it sounds cooler.
They've calibrated the app engine. All right. Okay. So why does this keep happening.
Why is it like every update is like oh god we got to improve the performance. We've got to
improve the performance. Right. Why is this so hard. Like why didn't they get it right the first
time. And it turns out you know unlike bugs. So okay. So code is always buggy. Right. And you
you know it's hard to debug programs. But it's like this thing produced the wrong result.
That's pretty clear. But if you have something and it just runs slower you don't really know where
or what to change. Right. So it can be really complicated. So what I'm going to talk about
are two things in this talk. One is performance analysis. So I'm going to explain how to do
performance analysis. Right. It turns out that the thing that we including myself have often done
is not really very rigorous and you can draw the wrong conclusions. And then I'm going to talk
about a new approach for performance profiling that will show you how to do it better. Right.
So first I'm going to talk about performance analysis. So here's Luke Luke has his code
and Luke is like it's too slow. Ogil is too slow. What am I going to do. And so Luke has an idea
and Luke's idea involves some sort of new you know I'm going to do this first and I'm going to change
this code. I'm going to change this function blah blah blah. I make a bunch of changes. Right.
And eventually I end up with a prime. Right. The new version of a. And so now I want to know did
it work. Right. So I've made this change. I thought it would make things faster. So I go and I take
my code and I have some set of inputs some benchmarks let's say and I go and I say run a and a takes
90 seconds which is clearly too long for your app store thing to run. But anyway but not with
standing that let's say there's something that takes 90 seconds in his tests. Right. And then he
runs a prime 87.5 seconds. Fantastic. Success. Right. 2.8% faster. All right. Time for Mojito.
Okay. So so this is great. Right. And clearly you know what Luke went and did had a big impact
big impact. All right. The question is like is it really faster. Right. So if you go and you
plot like here's a bar graph with I'm kind of giving away the game here one execution
of a prime and a it turns out that a prime is 2.8% faster looks good but there's maybe a problem
here. So what's the problem. Right. So there's this problem called variance. Right. Like when
you run a program once you're going to get some result but if you run it again maybe the result
will be slightly different and you want to account for that. Great. So now we'll run it 30 times 30
is the magic number by the way. So we're going to run it 30 times and we get this graph and so
they're pretty tightly distributed and you can see it's still 2.8% faster. Right. So it seems
plausible. Like I think everybody here would probably be like looks like a prime is faster.
Great. So the question you have to ask yourself is why is it faster. So you might think well of
course the reason is the code change. Right. So I as Luke and the developer and I go and I
hide my genius and I have my great idea and it pays off 2.8%. Right. Well it turns out changing
the code can actually lead to all sorts of knock on effects that have nothing to do with your
intended change. So it could totally be an accident. So let me explain why. So there's this wonderful
paper that is it appeared in Aspos in 2009 by Todd McGwitch and a number of other people.
I highly recommend you read it. It's something like how to do things wrong without really trying
something like that. And the conclusion is that the layout like where the code and data end up in
memory has a pretty big impact on performance. All right. So when you go to measure something
those measurements are biased by depending where things kind of fell. Right. So here are a few
things that can have an impact on layout and I'm going to talk about more. So one is link order.
So if you're in C C plus plus land and you have a make file and the make file has a bunch of
this link step and has a bunch of dot owes depending how those dot owes are arranged.
You can get different performance. OK. You might think fine. All right. Your environment variables.
So when you go to execute your program your environment variables whether it's in C C plus
plus or even manage languages they somehow get copied in and everything else gets shifted.
So in C and C plus plus that moves this moves the stack. So this actually has an effect on
layout. These two alone can lead to shifts in performance of plus or minus 40 percent.
OK. So that's not great. So what what is happening. Like why is this happening. This is a huge shift.
This is literally larger than the impact of dash 0 3 over dash 0 0. OK. Yes. You laugh. But
as well you should. So why is a prime faster than a right. So what is going on. Why could this
happen without actually trying. So part of the problem here is that basically modern processors
have become insanely complicated in their zeal to increase speed. So what do they do. So they have
caches right and data and instructions get mapped to the cache. Well it turns out for
good reasons. These things are bend up into these things called sets. They map to the same set.
You can have a conflict. So if you have hot code a lot of hot code is mapping to the same set
then it's not going to necessarily fit in cash and your code will run slower.
By luck you could be in a situation where when you changed a prime you actually disrupted this
conflict. And so now you have no conflict right. These two things one is the hot code and one maps
to nothing. So no conflict. Boom it ran faster. So that sounds great. So it could be the cache.
But it could also be the branch predictor which actually again is based on the addresses of
your branches. And if these branches collide then you can end up with things running slower.
There's also this thing called the TLB the translation look aside buffer which maps
virtual addresses to physical addresses. If things don't fit in the TLB because they span
two pages instead of one suddenly things become slower. There's also a branch target predictor.
There's a prefecture. There's more. All right. So this is pretty bad. So all of these things
can happen. You might think all right link order is fine. The code thing is a little weird.
But you know hey it's faster right. It's 2.8 percent faster that like I don't care. It's all good
right. Now it may not be faster on every machine but it's faster today right. So here's the problem
anything you do can disrupt this. So what could happen. One more malloc changes layout right.
Like you've shifted everything over one more or less. If you upgrade anything in your system
this is going to change layout right. So that's bad. Okay. So those things are and I'm not going
to change libc and I guess I'll never malloc again. Fine whatever. All right. So here's something
that may be surprising running it in a new directory. So it turns out that your current
working directory goes right into your your environment variables right. So that's weird
right. So you know if Vader tries to run your software it's not going to work as fast because
it's one character longer than Luke. Okay. This is a real effect. This can really happen. It is
actually bitten me. I had a student who wrote some wrote something. He has a long Indian last name.
My username is just five letters long. It's just Emory and he did something. He's like oh it doesn't
run any faster. It actually runs slower. It was like that makes no sense at all. And eventually
we were we whittled it down and it was like if I run it as me it's faster. Okay. That's right.
All right. Changes your layout. So the solution is obvious right. Run everything.
All right. So so I should add you know all of this is you know like the whole talk is really
oriented towards I'm going to improve my performance. But everything I'm talking about today
can be viewed in reverse for performance regression like I made a change and things run 2.8 percent
slower. Oh God. Roll back. Maybe not. Right. Maybe the next thing you do is going to actually undo
that change. Right. So basically layout is super brittle. And like you've seen layout biases
measurement. So one of the questions that we wanted to know is is it possible to eliminate
the effective layout so we can actually understand the performance of things kind of without having
to think about well one malloc lesser or more or you know Luke versus Vader. So the answer is yes.
So we built a tool that we call stabilizer. So stabilizer addresses this problem that I've
just explained to you. Pardon me. And it eliminates the effective layout. So this is the way this
is a way to actually measure programs where you can kind of actually know whether the regression
you had is real or whether the optimization you had is real and not just an accident. All right.
So how does this work. How is this even possible. So the way that stabilizer works is that it
randomizes layout. So it randomizes a lot of things. It randomizes the function addresses.
It randomizes stack frame sizes. It even randomizes heap allocations. But not only does it do those
things. It does it over and over again. Right. So while your program is running it's literally
doing randomization. And this turns out to be important and I'll show you a graph that we'll
explain why. But basically if you do this then there's no way layout can bias your measurement
because a completely random layout can't bias the results. Right. That's just how things work.
That's why we run randomized control trials. You've eliminated something as a possible cause.
The only other cause that remains is whatever change you made. All right. So let's walk through
what you would do with stabilizer. So with stabilizer again clearly you're supposed to
run your program a bunch of times. But notice what happens to the execution times. Here the
execution times are no longer tightly bound around this one very small measurement. The reason for
that is that when you were running that program 30 times it was sort of like you were going to do
a survey of 30 people but you just asked one person. Right. Because it's the same layout over
and over again. So you did an experiment on 30 executions but what you really did is you just
repeated 30 on one. All right. So the only noise that you're eliminating is the noise that comes
from like network demons waking up or some other random event maybe some thermal issue in your
computer but it's not really altering layout. Right. It's always the same layout. So here it's not
and you get these very nice bell curves. So now I'm going to ask you the question. So this is an
audience poll time. Is A prime faster than A? I just want you to raise your hands if you think
that A prime is faster than A. All right. Great. Now keep your hands up. Don't send them down.
Okay. But set them down if you change your mind. How about now? Okay. How about now? Okay. A lot of
hands still few like hardcore. All right. Okay. So what you all are doing is what I like to refer
to as eyeball statistics. And so you're kind of like looks close to me. That's too close.
Right. But it turns out this is not actually a thing. So if you,
yeah, it's not really statistics when you just eyeball results. So this is a bit of a refresher
for some of you but I'm going to walk you through this and how this all fits in with stabilizer.
So in actual statistics and today I'm just going to talk about one flavor of statistics
which is called null hypothesis significance testing. There are others notably Bayesian
approaches. Happy to talk about that offline. But basically the way it works is you just
assume that the things are the same. You say what is the likelihood of observing this difference
by chance? All right. So it turns out that this is something that's just convenient. It's very
easy to compute these probabilities for the normal distribution which you all remember from school.
These graphs are normal. Awesome. It turns out that stabilizer happens to make normal graphs
or normal distributions and I'll explain why. So how are we going to do this? We're going to run
stuff with stabilizer. We're going to pick some probability below which we're like, okay, good
enough. Right. So if it's only a one in 20 chance, I see this probability like the, I see this event
occurring. I'll be like, okay, that's good enough for me. You could be harsher. You could say one
in 100, one in 1,000. It's pretty standard to say one in 20. This is the p value of 0.05. So
the idea is if there's a low enough probability, you reject the null hypothesis, the null hypothesis
being that they're the same and you conclude that the speed up is real. It's not due to the
effective memory on memory layout. All right. So why re-randomization? The reason for re-randomization
is that just randomizing once doesn't give you enough randomization. So this is an actual program.
You can see the distribution is pretty wacky. It's very far from normal. You can't intuitively
explore much of the space when you just randomize at startup as opposed to randomizing during
execution. This is in fact the kind of distribution you get when you randomize all the time. And
these are normal distributions. So why do I keep saying that they're normal distributions? The reason
is essentially, again, going back to freshman stats, Stabilizer generates a new random layout
every half second. That is to say it's a completely independent version of the program right from
half second to half second, half second. It's all randomized. And it's the same program the whole
time. So it's identically distributed. And then we're adding them all up. And there's this nice
key result of stats, which is the sum of a sufficient number. So if you run a program for
long enough of independent identically distributed random variables, it's approximately normally
distributed no matter what the underlying distribution was. This is the central limit
theorem. So this makes execution times normally distributed, which is cool in other ways because
you actually know how likely it is that you're going to see some very weird execution because
you know what the distribution looks like. All right, great. So now we have this thing in hand
and we're going to do something insane. We're going to see whether optimizations actually matter.
And we know some of them matter. So we have a suite of benchmarks that we're going to evaluate it on.
We're going to evaluate them individually and then across the whole benchmark suite. And I'll
show you how we do it. So you build the benchmarks with Stabilizer. Stabilizer is a plugin for LVM.
If you just compile it as such, it goes and randomizes everything. But you can actually
just randomize things independently if you wanted, like just code, just heap, and just stack. So
now we run the benchmarks. We run them as usual. We drop them into one of my least favorite programming
languages ever. And then we decide what the result is. So again, we don't ask questions like this
because that's eyeball stats. Instead, we ask a question like this, pretend they're equal.
How likely is it we'd observe this much of a difference? So I also have to say that I'm not,
you know, you should not assume normality like almost ever in my humble opinion unless you have
very good reasons for doing so. Here we have very good reasons for doing so. So we can use really
powerful statistical tests like the student's t-test. So this is the test that you used to
actually measure this difference. So if the p-value, the likelihood of this event occurring,
is less than some threshold, which as I mentioned before is 5%, we're going to reject the null
hypothesis. All right? That is to say it's not because of random layout, the difference is real.
Everybody's on board, I hope. So now we're going to do it. You'll be shocked to learn
that dash O2 versus dash O1 makes a difference. Good. I would be weird if the result were otherwise.
So you can see that there are statistically significant improvements, right on the right.
There's some that are statistically significant but don't matter. And by God there are statistically
significant performance drops. So it turns out that compiler people run these same benchmarks
and overfit the way that they do these optimizations. And some of them lead to layout changes. And it
wasn't actually the code change. And so we can actually distill out this effect. All right, great.
By and large it looks like O2 over O1 is a win. How about O3 versus O2? Ready? It's amazing.
Okay. So I actually have to change the axes so we can see a lot of these values. So I'm going to
zoom in. Instead of it being 0 to 10, like the range is negative 10 to 20, I'm going to make it
0 to 1.5. Okay? So now we can see them. So they're pretty small effects. But some of them are
significant and some of them are not. Again, statistically significant 1.75% decrease in
execution time. Great. A bunch of things where it's not significant. And a couple decreases.
But really very minor effect sizes. So what do these results mean? You can't actually look at an
individual benchmark. Like there's 30 of them, right? So drawing a conclusion about all 30,
you actually have to do something different. You have to collect all of them. You get a bunch
of these graphs. And this is what you don't do. Like, okay, this one is slower. This one's faster.
This one's faster. This is just eyeballs everywhere. Okay? I mean, they're spooky and nobody wants to
see those. So again, we're going to do the same thing. But to test a bunch of things simultaneously,
you do this thing which is terribly named called analysis of variance. And you plug it into R
with this awesome incantation. And then you do, again, the same test. If the p-value is less
than or equal to 5%, we reject the null hypothesis. All right? You ready? All right, here we go.
Here's the p-value. So it has to be less than or equal to 5% or else we're going to conclude
that dash O3 versus dash O2 is nothing. All right? So the p-value is 26.4%. That means that
one in four experiments will just show a random effect, right? Just literally randomly. We do
not consider this enough evidence to reject the null hypothesis. So we're, we cannot reject the
null hypothesis, which is that the effect is indistinguishable from noise. All right? Okay.
So this is all terrible news for people like Luke who wanted optimizations to work. And I've
actually seen, I've actually seen projects. It makes, it kind of breaks your heart. Like, projects
I committed, like on GitHub, where it literally says dash O9. And I feel like, why not dash O11?
There's no, there's no dash O9 or 11. It's just kind of bottoms out. But, you know,
hope springs eternal. All right. So great. So what are we going to do? So what people do when they
can't speed things up, right? They run a profiler. So there's these profilers. They all basically
work the same way. You go and you get some result. And it says, hey, here's where my program is
spent. It's time. You get the number of calls to every function, runtime for each function. And
this captures intuitively maybe for most of us, like, this is what a profiler should do, right?
What, what do I care about? There's frequently executed code or code that runs for a long time.
That's where I should be focusing my optimization efforts. All right. It seems intuitively appealing.
This is the way profilers have been written since prof back, you know, back from like,
I don't know, late 60s, early 70s. So would this in fact speed up Google? So we're going to do this
experiment. We're going to go and find the thing that runs for the longest amount of time and where
it spends all of its time running. And so we're going to run it. And so we go and we do this. And
basically it makes the loading thing flash faster. Okay. So, well, guess what? That's frequently
executed. And in fact, it's the code that runs for the longest time. Right. So this is not really
great, especially if Luke spent like more than a minute optimizing that code. That's a shame. All
right. So basically in some profilers were developed in an era where everything was synchronous and
there was a single core. All right. That's not today. Today things are asynchronous or parallel or
concurrent or a mixed thereof. And profilers don't do a good job in these contexts. So we need to do
better. So what would be really cool is if we could have something like this. So this is what I
call a causal profile. So a causal profile tells you visually, like, if I were to speed up this
component by this much on the x-axis, then the whole program will speed up by this much. All
right. So this is really nice. Like if I had this graph, I would know I could spend a little effort
on the yellow search component and I'll get a big bang for my buck. Eventually it's going to bottom
out or top out at like, you know, like 70, 80%. And the red one, I could just keep going. Right.
Like it gets faster and faster the more I work. And the blue one, I should never, never optimize
ever. All right. It would be cool to know this, right. It's essentially like an oracle coming
and telling you, this is the code you should work on, Luke. I don't know where I got that way of
talking. Anyway. All right. So the question is, how would we know this? Like, how would we get
this information? Like, how would we know that this change would cause this effect? Right. We
can't just go and optimize the program by arbitrary amounts and test it. That kind of defeats the
purpose. So we're going to do something different. We're going to run an experiment. And it requires
one ingredient here, which I'll refer to as the force. So we're going to use magic. And we're
going to speed things up magically. And then we're going to measure how much the effect was of speeding
up each component by a certain amount on overall program execution. Okay. So we just keep doing
this. Right. We get more and more points. Right. And then I do it for different things. It turns
out if I could speed up saving things to the three and a half inch floppy, it doesn't make a difference.
Right. And so on. All right. Now, unfortunately, we live in the real world where there's no magic.
Sorry. Well, if there was magic to be clear, this is not what we would do. Right. I mean,
obviously, there are many much better things we could do. I could think of people I would like to
disappear off the face of the earth, for example. But I could also disappear all the runtime off the
face of the earth. Because why not? All right. So obviously, that's what we would do. So we can't
do that. We have to do something else. So what we are going to do as our trick is we're going to do
something that essentially takes advantage of this notion of relativity. So we're going to do a
virtual speedup. And a virtual speedup speeds things up in scare quotes by slowing everything
else down. Right. So everything else that's running at the same time will then be slowed down. And
that will allow us to get the impact of how do we sped this thing up? What would the result have
been? All right. So here, for example, if we speed up the sending of the picture results back
by a certain amount, we've slowed down everything running concurrently with it. And then that gives
us a result of a slowdown, which is the same thing as the result of having sped it up. All
right. So we actually can get points on this graph just by running these experiments. Right. So I
just got a point here. And I do it for everything. And I get more points. Right. And eventually,
I get a graph like this. If I speed up indexing, I'm going to get the exact same effect. Right.
Indexing is running at the same time as the compression. Right. So I get this result. And
then bang, I get these results. And again, these are like all the results. Right. Now I draw your
attention to the one weird blue thing. So the blue thing is slower. Right. And it turns out that
sometimes optimizing things makes your program run slower. And the intuition behind this is
you can actually get congestion on a lock. Right. Or congestion for a shared resource,
like disk or network. And so speeding things up makes things worse. You would like to know this
before you get started. Right. That would be a very, very bad day for Luke. That might necessitate
several sequels to recover from. All right. Great. All right. So let's let's dig into
Ogil a little bit. All right. So what do we care about in Ogil? We care about two things. We care
about how long it takes between a request and a response. Right. AKA latency. Traditional
profilers don't do this at all. Right. It's just total runtime. Oh, let me get in my soapbox for
one moment. Traditional profilers are about end to end runtime. You know how your servers are
all about end to end runtime? Or your browser. Like if only your browser could quit faster.
Right. So again, like it was all about like, here's a program. I run at a console and it
does something and it's done. And that's all I cared about. That's not really today. All right.
So there's latency. And then the more traditional thing is throughput. Again, this is something
that profilers do a bad job of measuring because they're all about end to end execution time.
Right. So how fast results come back is throughput. Right. So how are we going to do this? So with
our causal profiler that I'm going to explain in a minute, we're going to introduce what we
call progress points. So the notion of progress points is here's a thing I want to happen faster
or here's a beginning and an end of things that I want to happen faster. All right. So if Luke wants
responses to get sent faster, higher throughput, you just mark this particular start of this
component as a progress point. And every time the code runs, you go and you get another coin.
All right. And then you can do this simultaneously. Many requests for many users.
Right. And all of these things are incrementing some counter. So these progress points are
measuring throughput. And then you basically are going to run the experiments and see what
the effect is on the rate of those progress points being executed. All right. So one point
measures throughput. Like I said, if I speed up some component, whatever it might be, what is the
effect? All right. So now what if I care about latency? So we do the exact same thing. We set
a progress point at the beginning, a progress point at the end. And then the only thing that
has to happen under the covers is it has to have a counter. And the counter here measures how many
things are in the system at any one time. And it turns out that there is this awesome law that
holds in a wide variety of circumstances called Little's law. And so Little's law says that the
latency is essentially the average number of transactions in a system divided by the throughput.
We already know how to measure throughput. So we just take advantage of Little's law
and we can translate this into latency. All right. Great. So we have built a causal profiler for
Linux. It already ships with Debian and Ubuntu. So if you're using one of those systems,
you can install it quite easily. So it's just cause-profiler. It's quite easy to run. So you
say cause run dash dash dash. And whatever your program is and its arguments and it fires it
off and it starts doing performance experiments. All right. I should add it's not entirely true.
You do need to place progress points. If you don't place any progress points, it will act
like an ordinary profiler measuring end-to-end execution time. But if you do put in progress
points, then it will actually do its magic. All right. And this is just some macro,
like progress begin, progress end. All right. So let's apply this to Ogil. All right. I didn't
actually build Ogil. Neither did Luke. But we're going to build it out of pieces like any good
programmer would do. So it turns out there's this suite of parallel applications that's
kind of ready-made for this task. So there's a deduplicator that does compression. There's
an image comparator. And then there's a database, SQLite. That's not in Parsec, but we'll use
SQLite too. All right. Great. So I'm going to show you some fun things we did. This is a well
studied set of applications. People have already tried to optimize these. And we have covered a
bunch of surprising optimization opportunities. So here's Ferret. This is actually an older
version of what our causal profile looks like. Now it runs in a web browser. And you can see
that there's these lines of code. And it says, boy, if you speed up these lines of code, then
you're going to get performance increases. Conveniently, these lines of code happen to
be located in separate chunks of Ferret. So the part that does ranking, the part that does indexing,
and the part that does segmentation. And why is this convenient? I'm not going to have to
change any code to make this faster. The reason is that what Ferret does is it has this pipeline
model. And it assigns an equal number of threads to every stage in the pipeline. But it turns out
this one really doesn't need that many threads. So we take away the threads. And just by reassigning
threads, we've got a 20% speedup. So this is pretty cool because cause actually predicted it
perfectly. So we increased ranking, for example, from 16 to 22 threads. That's a 27% increase in
throughput. And on the graph, that says that that would translate to a 21% overall improvement.
And that's what we got. So cause actually works. Good. So we were pretty happy with this. We then
are going to move on to Ddupe. So Ddupe is pretty hilarious. So here's Ddupe in action. I have two
pictures, Grumpy Cat 1 and Grumpy Cat Meme. And so now what do I do to deduplicate these things?
You can see that there's chunks that are the same. So you carve out the chunks that are the same.
And you separate them out into individual pieces. And then an image is now represented by the bits
and pieces that make up the image. So here Grumpy Cat 1 is this piece. And Fun is awful is this
piece. And you saved a lot of memory. So that's what Ddupe does. So it does this compression
via deduplication. And it uses a hash function. So it throws everything to a hash table. Great.
So it's a pretty standard hash table. You just have some hash table. It's an array. It's a bunch
of bins. You get a bin number. And then you go and you start adding stuff to that bin into the
bucket. So this all seems straightforward. You hope that it would do something like this.
The hash table is accessed concurrently by a bunch of threads. But they're not idiots. There's
not one big lock. It's just all locks. Which is naive, but it's fine. But surprisingly,
cause says that the loop that accesses this list is important. Now, if you know anything
about hash tables, you know that things generally end up balanced. And it's weird that you have
this sort of situation. So we thought, all right, well, let's just make more, more hash buckets.
But we made a thousand of them. We really should have made a million because, you know, a million.
But anyway. So you would think this would lead to fewer collisions, but it had no effect.
So what else could be causing the collisions? Any guesses? The hash function, exactly. Like,
this is one of those when all other possibilities have been exhausted, right? You pick the weirdest
one. That's not an exact quote. But anyway, you're like, how can the hash function be broken?
Like, we've been using hash functions since before Knuth wrote about them. Well, turns out
people like to roll their own because it's fun. And so we did a histogram of the number of items
per bucket. So again, I told you there's a thousand buckets. This is the histogram.
Yeah. Hilariously, what they did is they used the pixels that were taken from the image and
they sum number of pixels and then added them. But that's actually the central limit theorem
in action, right? They're random, right? They're independent, right? And you've summed them together.
And so they actually formed the normal distribution. That's not the distribution you
want for a hash table. You would like a uniform distribution. So literally, we changed one character.
We changed the plus to XOR. So this. And we got this, okay? So,
all right. So, okay. I'll take the applause. But I mean, it was only a 9% speedup. But
all right. Nonetheless, it was one character. So I think it's the biggest bang for buck
ever recorded in optimization effort. So what did it predict? It turned out we can also measure
the accuracy of the prediction. So we knew that the blocks per bucket went from 76-ish to 2.
That's a 96% traversal speedup. And again, going back to the causal graph, it predicted a 9%
speedup, which is what we got, all right? So it's working. All right. So finally,
I'm going to talk about SQLite. So I have no time left. But SQLite is pretty awesome.
It's widely used, as you all know. But it has this weird thing where it has a kind of strange,
like, virtual table that they set up at compile time. And so whenever you actually
indirect through a config to execute a function like pthreadmutexunlock. So you would think,
all right, why are you telling me about this? Well, everything looks like this. This is an
indirect call. Could be a direct call. That would be faster. But I mean, an indirect call is not
that slow. But it's almost the same cost as pthreadmutexunlock, which means that you just
doubled the length of all of your critical sections. So that's not great. So in fact,
when you go, so cause will highlight all of these lines and say, you should definitely optimize these.
So we undid all of the actual indirect stuff and just made it so that at compile time,
you change SQLite unlock to something so it doesn't do the indirect. And it sped things up by 25%.
If you look at a traditional profiler, by the way, those things are like, this takes 0.0001%
of time, right? You would never consider actually trying to optimize that code. So we did it for
a bunch of programs. We got some crazy speed ups. My favorite is we got a 68% speed up by
replacing a custom barrier with a standard barrier. Again, people should stop doing things at home.
So anyway, so I'm going to conclude. So you can take a picture of this to jump to work from our
lab, which is plasmaumass.org. I talked today about sound performance analysis and effective
performance profiling. Everybody should go use the cause. All right. Thanks for your attention.

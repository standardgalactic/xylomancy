Hello. Hello. Hello. Good afternoon. I'm Bryna Krah, President of the AMS, and it's my pleasure
to welcome you to the colloquium lectures. These are the oldest lectures at the meetings
of the AMS. The first meeting was held in 1895. The second in 1896 were the first colloquium
lectures actually took place, and those were at Northwestern University, and since that's
my home institution, I can't help but mention that. The list of speakers is a veritable
who's who in mathematics, including Birkhoff, Morse, von Neumann, Tarski, Chern, Milner,
Smell, Nirenberg, Tate, and the list of people I've left off is, well, equally prestigious
to those that I included, and amongst them was also our speaker today's advisor, Stein,
Elia Stein. So with that, I'll turn to the introduction of Terry Tau. Terry may be somebody
who doesn't need introduction. After all, he's been in a crossword puzzle as a clue in
the New York Times crossword puzzle. I don't want to use up all his time by listing the
awards he's won, but I could. I'll give you just a short list of the highlights of the
Fields Medal in 2006, a MacArthur Fellowship, he's a fellow of the Royal Society, the Australian
Academy of Sciences, the American Academy of Arts and Science, and a member of the National
Academy of Science, and of course, he's a fellow of the AMS, most important of those
distinctions. He has over 350 publications, meaning he hasn't slept in a few years, and
this includes numerous highly influential texts, and he has more than 50 collaborators,
and maybe that's when I counted last week, so I don't know, maybe this week there's
more. He's also one of the broadest researchers in mathematics, covering interests from pure
to applied, and I won't list all of the subjects. But it's not only research, he also serves
the profession in numerous ways, and starting in 2021 took on a very substantial role. He
was appointed by President Biden as a member of the President's Council of Advisors on
Science and Technology. So he's a force, he's mentored over 20 PhD students, and I could
go on and on. So I've known Terry for a while, since about 2004 or 2005 when we first met,
but one of my fondest memories of him was at a party that I was a co-host for. In 2008,
it was the election night, and Terry was sitting in the corner November 2008 on his computer
on some website that was giving election returns, announcing the states before the TV was. It
was really impressive, and it's just one of my fondest memories. Anyway, I won't keep
you any longer. It's my pleasure to introduce Terry.
Thank you very much, Bryna. I'm very happy to be here at this JMM to give this lecture. It's
always nice to be up in San Francisco. So I'll be talking about what I think is a really exciting
development in mathematics that's going to shape our future, which is really the development over
the last few years of lots of technologies to make machines and computers help us do math much
more effectively. Now, to some sense, this is not new. We have used both machines and computers,
and I use the terms slightly differently. We've used both actually for centuries, really. Nowadays,
computers and machines, when we talk about machine-assisted mathematics and computer-assisted
mathematics, they're sort of synonymous. Initially, they weren't because computers used to be human,
and then they were mechanical, and then finally electronic. So, for example, one of the earliest
use of computers was to build tables. So, for example, the large tables of Napier and so forth
were basically built by lots and lots of human computers, and those are the earliest examples
of somehow computer-assisted mathematics. And tables are still really important today. I mean,
not so much the log tables anymore, but a lot of what we call experimental mathematics is based
on creating lots and lots of large tables of interesting things, especially in number theory.
So, for example, famously, the Jondo and Gauss built tables with prime numbers, and they used
that to conjecture the prime number theorem, which is, of course, now a theorem. And similarly,
in the 60s, Bertrand's Wittendier created lots and lots of big tables of the decurs and the ranks
and so forth, and this was a key input in formalizing the famous BSD conjecture. And maybe
the biggest table of all in mathematics is the OEIS, online encyclopedia of mathematical sequences.
So, there's hundreds of thousands of integer sequences, and every day, I think mathematicians
discover unexpected connections, or maybe rediscover an existing connection. I myself use
the OEIS if there's a quantity which I know there's a formula for, but I can't remember it.
I can just compute the first five elements to put in the OEIS. I can usually find it.
And most recently, people are starting to use large databases of mathematical objects as
training data for neural networks, and so I'll give you an example of that later. So, that's one very
storied and antique way of using computers in mathematics. The other big use, of course,
is in numerics or scientific computing, and that's also a very old subject. Arguably, the first
big scientific computation was in the 20s when Lorenz was asked to model the fluid flow for the
construction of a new dike in the Netherlands, and so he assembled a team of human computers,
basically, to model what would happen to the water flow and so forth. It's notable for the
introduction. He's almost the first place where floating point arithmetic was introduced.
And, of course, we use scientific computing nowadays to model PDEs, to solve large systems
equations, and, of course, we use them for computer algebra packages, magma, maple, sage,
and so forth. Yeah, you want to do a big numerical integration or algebraic computer
basis, whatever. We routinely do this now, or across mathematics.
Of course, the numerics are sometimes inaccurate. There are round off errors
and other possible problems, but there are ways to make the combination more rigorous.
For example, instead of floating point arithmetic, you use interval arithmetic. If you represent
numbers by error ranges, a lower and upper bound, and you keep those bounds like rational numbers,
like infinite precision, then you can avoid errors, at least in principle,
at the cost, maybe, of making the runtime longer. More recently, there are more advanced
algebra packages than just the standard things you get in sage or Mathematica.
There are these things called SAT solvers, satisfiability solvers, or satisfiably modular
theory solvers, SMT solvers. So a SAT solver, you feed it a whole statement, a bunch of
propositions, P1, P2, P3, and so forth, and you say that P1 and P2, or P3 is true, P3 and P4,
and not P5, one of them is true, and so forth, and it will try to see if there's a solution or not.
Many problems can be phrased as a SAT problem. If you have a general purpose SAT solver,
you can potentially just feed it into such a program and solve the problem for you.
Then there are these more sophisticated variants, SMT solvers, where you also feed
laws of algebra. You have some variables, and you assume that there are certain laws of these
variables or certain laws, and you ask, can you deduce a new law from the laws you already have?
Those are potentially very powerful, and unfortunately, SAT solver really is an NP
complete problem. Once you get hundreds and hundreds of these propositions, it becomes
very hard to actually solve these problems, but still they are very useful. Here's a typical
application of a SAT solver. A few years ago, there was this famous problem in Commentorix
called the Boolean Pythagorean Triples problem. The problem is this, you take the natural numbers
and you color them into two color classes, red and blue. You ask, is it always the case that one
of these color classes contains a Pythagorean triple, three numbers A, B, and C such that
A squared plus B squared equals C squared. It turns out to be, we don't have human proof
of the statement, but we know it's true now because of a SAT solver. There was a massive
computation that says that if you only go up to 7,824, then you can't do this. There was a way
to partition the numbers from 1 to 7,824 into two classes, neither of which contained a Pythagorean
triple. But once you go up to 7,825, no matter how you do it, you must always get one of the
two color classes must have a Pythagorean triple. In principle, this is a finite computation
because there's only two to five different ways to compute different partitions, and so you
can just check each one, but that is computationally unfeasible. But with a SAT solver, you can
rephrase this problem as a free satisfiability problem, and it's not just a matter of running the
solver, you have to optimize it and so forth, but it is possible to actually solve this problem.
And it gives you a certificate, it gives you a proof, and actually this is, at the time, it was
actually the world's longest proof. The proof certificate, first of all, it took four CPU years
to generate, and it's a 200 terabyte proof, although it is compressible. I think it is still
the second largest proof ever generated. Okay, so that's, but this I still consider a more classical
way of using computers. But what I think is exciting is that there are a lot of new ways
that we can use computers to do mathematics. Of course, there's still the boring ways,
we use computers to do emails and write latex and so forth, I don't mean that.
But there are sort of three new modalities, which individually they still have some niche
applications, but what I find really interesting is that they can potentially be combined together,
and the combination of them, it could be something general purpose that actually a lot of us could
use. So the three sort of new things. So the first is sort of machine learning algorithms,
where you have a problem, and if you have a lot of data for that problem, you can set some sort of
specialized neural network to train it on the data, and it can generate counter examples for
you, try to generate connections. And so people are beginning to use this in all kinds of fuels
and mathematics, I'll give you some examples later. So that's one development. Maybe the most
high profile development is large language models like chat GPT. These are very general purpose
models that can understand natural language. To date, they have not been directly used for so
much mathematics. I'm sure many of you have tried talking to GPT, asking it to solve your
favorite math problem, and it will give you some sort of plausible looking nonsense in general.
But when used correctly, I think they do offer a lot of potential. I mean, I have found,
occasionally, that these models can be useful for suggesting proof techniques that I wasn't
initially thinking of, or to suggest related topics or literature. They're actually most useful for
sort of secondary tasks. Okay, so actually doing math research, they still haven't really proved
themselves, but for doing things like writing code or organizing bibliography, like a lot of the
other more routine tasks that we do, these LLMs are very useful. But the third new technology,
which has been around for two decades, but has only now sort of becoming ready for primetime,
are these formal proof assistants, which are languages designed to verify,
or to verify, well, many of them are actually designed to verify electronics, but they can
also verify mathematical proofs. And crucially, they can also verify the output of large language
models, which they can complement, they can fix the biggest defect in principle of the LLMs.
And they allow new types of ways to do mathematics, in particular, they can allow really large-scale
collaborations, which we really can't do without these formal proof assistants. And they can also
generate data, which can be used for the other two technologies. So I'll talk about each of these
three things separately, but there's beginning to be experiments to combine them together,
but there's still kind of prototypes right now, but I think the paradigm of using all of them,
and also combining with the computer algebra systems and the SAT solvers into one integrated
package, it could really be quite a powerful mathematical assistant. Okay, so let's talk,
I think my first slides begin with proof assistants. So computer-assisted proofs are not new,
famously the four-color theorem in 1976 was proven partly by computer, although at the time it was,
by modern standards, we would not call it a fully formalized proof, the 1976 proof.
The proof was this long document with lots and lots of subclaims, which a lot of them were
verified by hand, and a lot of them were verified by both electronic computers and human computers,
and I think one of the author's daughter actually had to go through 500 graphs and check that they
all had this discharging property. And actually it had a lot of mistakes too, so there's a lot of
so there's a lot of minor errors in the proof, they're all correctable, but it really
will not meet the standards today of a computer-verified proof.
The first proof, okay, so it took 30 to 20 years for an alternate proof of four-color theorem
to be verified, and this proof is closer to being completely formal, so it's about 10-15 pages
of human-readable argument, and then it reduces to this very specific computation, which anyone
can just run a computer program in whatever language they like to verify it. So it was a
computer-verified proof, but it still wasn't a formal proof, it wasn't written in a formal
proof language, which was designed to only output correct proofs, and that had to wait until the
early 2000s when Winner and Contier actually formalized the entire four-color theorem
in one of the early proof assistant languages, Coq, in this case.
So, you know, I mean, now we know with 100% certainty that the four-color theorem is correct,
well modular, you know, trusting the compiler of Coq, okay, but
all right. Another famous machine assistant proof, well, actually initially human proof,
but eventually computer-verified was the proof of the coupler conjecture. So the coupler conjecture
is a statement about how efficient that you can pack unit spheres in the plane,
and so there's a natural way to stack unit spheres, and it's the way that you see oranges stacked in
the grocery store. It's called the hexagonal closed packing, and there's also a dual packing
with the same density called the cubic closed packing, and they have a certain density, pi over
three root two, and this was conjectured to be the densest packing. So this is an annoyingly hard
statement to prove. So, you know, it's an optimization problem in infinitely many variables,
you know, so there's this, each sphere has a different location, and so, and there's an infinite
number of spheres. So, you know, you're trying to prove an inequality involving an infinite number
of variables involving a solving an infinite number of constraints, so it doesn't immediately
lend itself to computer verification. But even in the 50s, it was realized that possibly this
could be done by some sort of brute force computation, and so Toth proposed the following
paradigm. So, every time you see a packing, it comes with what's called a Voronoi decomposition.
So, every sphere comes with a Voronoi cell, which is this polytope of all the points that are closer
to the center of that sphere than to all the other spheres, and this partitions space into all these
little polyhedron, these Voronoi cells. And there are various relationships between the volumes of
these different cells, you know, there's only so many spheres that you can pack next to one
reference sphere, and this creates all these kind of constraints. And so, the hope was that if you
could gather enough inequalities between adjacent Voronoi cells, the volumes of adjacent Voronoi
cells, maybe every such system inequalities in principle gives you an upper bound on the density
of the sphere packing. And in principle, if you get enough of these inequalities, maybe you could
actually get the optimal bound of 302. So, people tried this approach for many, many years, and
including some false attempts, but they were not able to actually make this approach work.
But Thomas Hales, and then later, with a collaborator, was able to adapt this approach
to make it work in a series of papers from 94-98, but he had to modify the strategy quite a lot.
So, instead of using the Voronoi decomposition, there was a more complicated decomposition
that was used, and instead of using volume, they had to define this new score function
attached to each polyhedron. But basically, the strategy was the same, and he was able to
prove lots and lots of linear inequalities between the scores of adjacent polyhedra,
and then just using linear programming was able to then get a bound. And with the right choice
of score and the right choice of partition, it was the optimal bound.
It's a very flexible method, because you have lots of ways you can do the partition,
and lots of ways that you can do the score. But the problem is that it was too flexible.
So, here's a quote from Hales. It says that Samuel Ferguson, who was Hales' collaborator,
and I realized every time we encounter difficulty solving the minimization problem,
we get just a scoring function to score the difficulty. The function became more complicated,
but with each change, we could cut months or years from our work. This incessant fiddling was
unpopular with my colleagues. Every time I presented my work in progress at a conference,
I was minimizing a different function. Even worse, the function was moderately incompatible with
earlier papers, and this required going back and patching the earlier papers.
So, the proof was a mess, basically. They did eventually finish it in 98, and they were able
to derive the capital conjecture from a linear programming computation from a very complicated
optimization program. Initially, it was done by hand, but with the increased complexity,
there was no choice but to make it more and more computer-assisted.
So, when the proof was announced, it was a combination of 250 pages of notes and lots
and lots of gigabytes of programs and data. And it was famously hard to referee. It took four years
for annals to referee the paper with a panel of 12 referees, and even then, the panel was only
99 percent certain of the correctness of the proof, and they couldn't certify the calculations.
Because, I mean, in principle, it was all doable, but the referees have to implement all these
different computer calculations themselves, but it was eventually accepted.
But clearly, there was this big asterisk. There was a lot of controversy about whether this was
really a valid proof, and so this was one of the first really high-profile uses of
formal proof assistance, because this was a result in which there was serious doubt about the correctness.
So, they created, so Hales in 2003 initiated a project to write down this massive proof in a
completely formalized way, so that a standard proof assistant could verify it. He estimated it
would take 20 years to make this work, and so he had, he gathered 21 collaborators. It actually
only took 11 years, but yeah, eventually what they did was that they first created a blueprint,
you know, so a human readable version of the proof breaking things up into very, very small steps,
and then they formalized each step by bit, and it was finally done, and then there was a,
yeah, so they published a paper about the formalization, and that only appeared in 2017.
So, this was sort of the state of the art of formalization, you know, as of say 20 years ago,
you know, like it was possible to formalize big complicated results, but it took an enormous
amount of effort, you know, not something which you would do routinely. There was a more recent
effort in a similar spirit by Peter Schultzer, he called it the liquid tensor experiment. So,
Schultzer introduced this theory of condensed mathematics, which is, all right, this is really
far from my own area of expertise, but basically there are certain problems with, so certain types
of mathematics you want to work in various categories, like categories of topological
believing groups and topological vector spaces, and there's a problem that they're not obedient
categories, but they don't, they don't have a good notion of kernel and co-kernel and things
don't work out properly. So, he proposed replacing all of these standard categories
with a more fancy version called a condensed category, which has better category theoretic
properties, and so the hope is that you could use a lot more high-powered algebra to attack,
to handle things of topological structure or analytical structure, like function spaces,
look at our binoc spaces. But in order for the theory to work, there's a certain vanishing
theorem which I've written there, oops, but I cannot explain to you, oops. Okay, so there's a
certain category of co-kernel and co-kernel believing groups, and there's an X group involving
a p-binoc spaces that has to vanish, and this vanishing theorem is needed in order for all
of the rest of the theory to actually be useful, if you want to apply it to functional analysis
in particular. And so, Schultz, what a blog post about this is that, you know, I spent much of
2019 obsessed with the proofless theorem, almost getting crazy over it. In the end, we were able
to get an argument pinned down on paper, but I think no one else has ever dared to look at the
details of this, and so I still have some lingering doubts. With this hope, with this theorem, the
hope that the condensed formula can be fruitfully applied to functional analysis stands a force.
Being 99% sure is not enough because this theorem is of the most fundamental importance.
He says, I think this may be my most important theorem to date, which is a really big claim,
actually. Better be sure it's correct. So, this was another case where there was a great desire
to formalize the proof. So, he asked publicly on the internet for help to formalize this in a
modern proof of system language called Lean. And so, again, about 20 people, I think, joined
this project to help out. And it, so Lean is based on, it has this philosophy where it has this
single huge library of mathematical theorems, like the fundamental calculus, or the classification
of finite building groups, like a lot of the basic theorems of mathematics are already formalized in
this big library, and the idea is to just keep building on this library and adding to it with
additional projects. But one of the problems that short the face was that a lot of the tools that,
basic tools that you needed, like homologous algebra and chief theory and topos theory,
weren't actually in the library yet. So, actually, part of what they had to do was actually set up
the foundations of that theorem, formalized it in the library first. But it basically was done in
about two years. In one year, they formalized a key sub theorem, and then the whole thing was
formalized about a year afterwards. It did have some, in addition to sort of making reassuring
Peter that it was, everything was correct, there was some other minor benefits. So, first of all,
there were actually a few minor errors in the proof that were discovered in the formalization
progress, but they got fixed. Also, some small simplifications. There was one big simplification
actually. So, the original proof used something very complicated, which I also don't understand,
called the Brindelina resolution. But in the course of formalizing it, it was too hard to
actually formalize this theory, but they found that there was a weaker version of this theory,
which was good enough to formalize this application. But this was actually a major
discovery because this weaker theory could also potentially attack some other problems
that the Brindelina resolution is not well suited for. And now the math library is much,
much better in, it has a lot of support for homological algebra and sheath theory and
also other things, which helped other formalization projects become easier.
I got interested in this formalization a few months ago. So, hang on. I should say,
like with the Kepler experiment, so you don't just directly transfer a paper from, you know,
the archive or something into lean. What we found is that it really helps to create first an
intermediate document. So, halfway between a human readable proof and a formal proof,
which we call a blueprint. So, it looks like a human proof and is written in a
version of latex. But like each step in the proof is linked to a lemma in lean. And so,
it's very tightly integrated. It has a very nice feature. It can create dependency graphs,
which I'll show you an example later. You can see which parts of the proof have been formalized,
which ones are not, and what depends on what. It gives a lot of structure to the process of
writing a paper, which right now we do by hand without much assistance, really.
Yeah, Schultz has said that the process of writing the blueprint really helped him understand
the proof a lot better. And actually, people have been also going the other way. There's also
software that takes formal proofs, which are written in something that looks like computer code,
and turns them back into human readable proofs. Here is a prototype software. So, this is Theoman
Apology. Okay, I think if a function is continuous on a dense set and there's some extra properties,
then it extends to a continuous function globally. And the proof was written first in lean,
but then it was automatically converted into a human readable proof. But again,
where every step you can expand and contract, like if there's a step you want to explain more,
you can double-click it, and it will expand out, give all the justification. And you can click
at any given point in the proof, and it will tell you what the hypotheses are currently,
what you're trying to prove, and you can give a lot of structure. I think eventually textbooks,
this is maybe a good format, say for undergraduate textbooks, to have proofs of, say, tricky
themes and analysis of something written in a way where, not in a static way, but where you can
really drill down all the way down to the basic axioms if you wanted to. Okay, one thing notable
about these formalization projects is that they allow massive collaborations. So, in other sciences,
people collaborate with 20 people, 500 people, 5,000 people. But in mathematics, still, we don't
really collaborate in really large groups. Five is kind of the maximum, usually. And part of it
is that if you want to collaborate with 20 people, if you don't already know these 20 people, and
you don't completely trust that they're doing correct mathematics, it's very difficult because
everyone has to check everyone else's contribution. But with a proof assistant,
the proof assistant provides a formal guarantee. And so, you can really collaborate with 20,
hundreds of people that you've never met before, and you don't need to trust them,
because they upload code, and the lean compiler verifies, yes, this actually solves what they
claimed, and then you can accept it, or it doesn't. So, I experienced this myself because
I got interested in formalization a few months ago. So, I'd recently proven a combinatorial
theorem with Tim Gower, Ben Green, and Freddie Manners. It solved something called the polynomial
prime and rupture conjecture. The precise conjecture is not important for this talk. It's a conjecture
in combinatorics. You have a subset of a finite through vector space of small doubling, and you
want to show that it is very close to actually being a coset of a subgroup. This was a statement that
was in out of combinatorics that was highly sought after. So, we had a 33-page paper recently
proving this, mostly self-contained, fortunately, so I thought I thought it was a good candidate
for formalization. So, I asked on an internet forum, specializing in lean, for help to formalize it,
and then, again, like 20, 30 people joined in, and actually only took three weeks to formalize.
The time taken to formalize these projects is going down quite a bit. So, in particular, the
time taken to formalize this project was roughly about the same time it took for us to write the
paper in the first place. And by the time we submitted the paper, we were able to put as a note
that the proof has actually been formalized. So, as I said, it uses the single-to-blue print,
which splits up the proof into lots and lots of little pieces, and so it creates this nice little
float, this little dependency graph. So, this is a picture of the graph at an early stage of the
project. So, there's no down the bottom called PFR, maybe only the people in the front can see it.
That's the final theorem that we're trying to prove. At the time of the screenshot, this was
white, which means that it had not been proved, but not even stated properly. So, in fact, even
before you prove the theorem, you have to first state it, and then you have to make definitions
and so forth. Blue are things that have been defined, but not yet proven, and green have
been things that have been proven. And so, at any point in time, some bubbles are white,
some of them are blue, and some of them depend on some others. And so, the way the formalization
proceeded was that different people just grabbed a node that was ready to be formalized, because
maybe all the previous results, all the results that they depended on were formalized, and they
just formalized that one step independent of everybody else. And you didn't really need to
coordinate too much. I mean, we did coordinate on an internet forum, but each little node is
completely specified. There's a very precise definition and a very precise thing to prove,
and we just need the proof. And we really don't care exactly what the proof is. I mean,
that has to be not massively long. So, yeah, so people just picked up individual claims.
Like, here's one very simple claim. There's a certain functional called ruja distance d,
and there's a very simple claim that it was non-negative. And this turns out to have a
three-line proof, assuming some previous facts that were also on the blueprint.
And so, people just sort of picked up these things separately, and over time, it just
filled up. This is what a typical step in the proof looks like. This is the proof that this
ruja distance is non-negative. This is the code in lean. It looks kind of a little bit like mathematics,
but it is actually, if you want to see a movie where the syntax actually reads,
it's like reading latex. First time you read latex, it looks like a whole bunch of mess of symbols.
Every line corresponds to a sentence in mathematical English. For example, the first
line, if you want to prove that this distance is positive, it suffices to prove that tricy distance
is positive. So, you work with tricy distance, because it turns out there's another lemma
that bounds tricy distance. So, yeah, every step actually, it
corresponds reasonably well to the way we think about mathematical proofs.
Yeah, so fortunately, the proof didn't reveal any major issues. There were some typos,
but very, very minor picked up. And we also, there were some things we needed, again, we needed to
add to the math library. The math library, we used Shannon entropy, and Shannon entropy was not
in the math library, now it is. One thing about formalization is that it still takes longer
to formalize proofs than to write them, and maybe about 10 times longer, I would say,
which is unfortunate. If it was faster, if it was faster to formalize
to write proofs formally than to write them by hand, I think then there will be a tipping point,
and then I think a lot of us would switch over, just because they guarantee a correctness.
So, we're not there yet. It is definitely getting a lot faster than it was before.
But one thing we noticed is that actually, while it still takes a long time to write a proof,
if you want to modify a proof to change some parameters and make it a little bit better,
that can be done much quicker in the formal setting than in the paper proof, because
the paper proof, if you change all your little parameters and so forth, you likely make also
mistakes and you go back, and it's a very annoying process, but it's actually much easier to modify
an existing formal proof than to create one from scratch. For example, we were able, there's a constant
12, an exponent that appeared in a proof, a few days afterwards, someone posted an
improvement in the argument that improved 12 to 11, and in a few days we were able to adapt
the formal proof to do that as well. Yeah, and because you can collaborate, I think the process
is scalable. There's just recently Kevin Buzzard, for example, a five-year project,
the aim is to formalize Fermi's last theory of the proof, and that is quite a big goal,
because there are lots and lots of sub-portions that have had no formal proof at all, so that I
think will be quite an ambitious project, but I think it's now doable. It wasn't doable five years
ago, but now I think it is. Okay, so that's four more proofs. Another, oh, okay, I might actually
have to speed up a little. Okay, so, all right, so machine learning, using neural networks has
become also more and more commonplace in different areas of mathematics. I think I'll skip over this
one, so one place where it is being used is in PDE to construct candidate approximate solutions
for various problems, so like, so there's a famous problem in fluid equations, you know,
do the Navier-Stokes equations, we'll be fine at time, do the Euler equations, we'll be fine at
time. Navier-Stokes is still challenging, but Euler, there's been a lot of progress recently,
that people have been starting to construct self-similar solutions to the Euler equations
with some asterisks, but the asterisks are slowly being removed. And one of the strategies of proof
is to first construct an approximate solution, approximately self-similar solution that looks
like it's going to blow up and then use some rigorous perturbation theory to perturb that
to an actually blowing up solution. And machine learning has turned out to be useful to actually
generate these approximate solutions, but I'm going to skip that for a lack of time.
I'll tell you, my favorite application of machine learning to mathematics is in knot theory.
So this is a recent work, so knots are a very old subject in mathematics, and there's lots of,
one of the fundamental things you study in knots is not invariance, so there's various statistics
you can study, you can assign to a knot, which depends on the topology of the knot or the geometry
of the knot. So there's something called the signature, which is a combinatorial invariant.
There are these famous polynomials, like the Jonas polynomial and Homefly polynomial.
Then there are these things called hyperbolic invariants. The complement of a knot is often
hyperbolic space, and then you can talk about the volume of that space and some other geometric
invariants. And so there are these massive databases of knots. You can generate millions
of knots and you can compute all these invariants, but they come from very different areas of
mathematics. So some not invariants come from combinatorics, some come from operative algebras,
some come from hyperbolic geometry, and it's not obvious how they're related.
But what these mathematicians did was that they trained a neural network on this big database
of knots, and they studied the hyperbolic invariants and the signature, which is the combinatorial
invariant. And they found that you could train the network to predict the signature from the
hyperbolic invariant with 99 percent accuracy. So they used half the data to train the set
and then half the data to test the set, to test the neural network. And so what this
told them is that there must be some connection, the signature of a knot,
it must somehow be a function, or at least very closely related to a function of a hyperbolic
invariant. But the problem with neural nets is that they give you a function, a functional
relationship, or at least a conjectural functional relationship, but it's this massively complicated
function. It composes hundreds and hundreds of nonlinear functions together, and it doesn't
give you any insight as to what the relationship is. It just shows you that there is a connection.
But it's possible to do some analysis. So they actually tried a very basic thing which happened
to work. It's what's called a saliency analysis. So this neural network gave them this function.
Basically, they fed in 20 hyperbolic invariants as input, and the signature as output. So it's
basically a function from R to 20 to the integers, I think. And what they decided to do is that once
they had this function, they just tested how sensitive the function was to each of its inputs.
So they just varied one of the 20 variables, and they saw what happened to the output.
And what they found was that only three of the 20 variables were actually important,
that only three of them had a significant influence on the function. The other 17 were
basically not relevant at all. And so they focused on those three variables, and they started plotting
this function just restricted to those three variables. And that's just low enough to mention
that you can eyeball the relationships. So they started plotting the signature as a function of
two or three of these variables and using color and so forth. And they started seeing patterns,
and they could use these patterns to conjecture various inequalities and various relationships.
It turns out that the first few inequalities they conjectured were false, and they could use the
new net and the data set to confirm this. But with a bit of back and forth, they were able to
eventually settle upon a correct conjecture relating these invariants to the signature.
It wasn't the invariants that they expected. They were expecting the hypervolume, for example,
to be really important. It did not be relevant at all. But once they found the right variables
and the right conjectured inequality, it suggested the proof, and then they were able to actually
find a rigorous proof of the inequality that they conjectured. So it was a very nice back and forth
between using the machine learning tool to suggest the way forward, but then going back to
sort of traditional mathematics to prove things. Okay. So of course, the most high-profile
development these days has been large language models like GPT. And sometimes they work really,
really well. So here's an example of GPT-4, which is OpenAI's most advanced large language model,
actually solving a problem from the IMO, the International Mathematical Olympiad.
And so it's a question, you know, there's a function that obeys a certain functional equation,
can you prove, can you solve for the function? And it actually happens to give a completely
correct proof. Now, this is an extremely cherry-picked example. I think they tried,
from this paper, they tried all the recent IMO problems, and they could solve like 1% of the
problems of this method. Famously, it's bad even at basic arithmetic. You ask it to solve 7 times
4 plus 8 times 8, and it'll give you the wrong answer. It gives you 120. And then it will keep
going, and I'll explain why. And during the course of the explanation, it will actually make a mistake.
And then you point out that they made a mistake and say, I'm sorry,
that's incorrect. I mean, these last kind of Lancomers, they remind me a lot of, you know,
if you have sort of a somewhat weak undergraduate student in office hours, and you ask me to solve
a problem at the blackboard with no age, you know, he or she will try their best,
and try to turn something that looks like a proof. But yeah, they don't really have a good way of
correcting themselves. So, you know, sometimes they work really well, but often they're very,
very unreliable. But there's lots of interesting recent experiments where you can couple these
language models to other much more reliable tools to do mathematics. So for example, GPT now comes
with plugins for Wolfram Alpha. So now if you ask GPT to do an ethnic calculation, it knows better
than to try to do it itself, it will outsource it to Wolfram Alpha. Then there's more recent
examples where people are coupling these large language models to a proof error file like Lean.
So, you know, you ask it to prove a statement, you know, prove that the union of two open
sets is open. If you ask a raw, large language model, it will give you the statement that
a proof that looks like a proof. But there's lots of little logical errors in the proof that
you can force it to output in Lean, get Lean to compile. And if there's a completion error,
it will send back the error to the large language model and have to correct it and
create a feedback loop. And it can actually be used to solve roughly sort of undergraduate math
homework level problems by this technique with, you know, but now with 100 percent guarantee of
accuracy if it works. I mean, of course, sometimes it will just get stuck and give up because it
could never get the Lean compiler to accept the argument. But it is beginning to make some
headway. As I said before, I do find it can be useful as a muse, kind of like if you're just
starting on a project. I recently was trying to prove some commentary identity. And I was thinking
I had some ideas in mind. I was going to use asymptotics, work with some special cases,
but nothing was working. And I asked GPT for some suggestions. And it gave me some suggestions
I was already thinking of, plus some suggestions that were either completely vacuous or wrong.
But it did tell me that you should probably use generating functions, which I should have known.
But at the time, it escaped me. And just with that hint, I was able to actually get them to work.
So, you know, I mean, as just kind of a double check your thought process,
it is sort of useful. It's still not great, but it has some potential use.
There was another tool which I do like, and I use more and more now. It's integrated into
something called VS Code, which is an editor to write code, but it can also be used for latex,
something called GitHub Co-Pilot. It's basically an AI-powered autocomplete,
and you type in your code or your latex, whatever. And based on all the text that you've already
written, it will suggest a possible new sentence to generate.
And so it can be very useful for code. You write down three lines of code, and it's just the fourth.
And sometimes you'll get exactly right. Sometimes it's not exactly right. Sometimes it's
complete rubbish. But then you can accept it, and it can save a lot of time, especially if you're
doing some very menial code where you're repeating something over and over again. It works for latex.
I wrote a latex blog post, actually, recently, where I was trying to estimate an integral,
and I broke up the integral into three pieces. I said, okay, the first piece I can estimate by
this technique, and I wrote down how to estimate this technique. And then the Co-Pilot just suggested
how to estimate the second term. And actually, a completely correct argument. It was a modification
of what I had already written. So it's very good at just modifying text that you've already appeared.
And it's slowly being integrated into proof-assistance like Lean. So to the point where
sort of one line proves, two line proves, we can kind of get the AI to fill in for us.
The kind of steps that correspond to, like, this is obvious, or clearly this is true,
in a paper proof. I mean, not all of them, but we can get to the point where the AI can fill in
a lot of them, and that will make proof formalization a lot faster.
So there's a lot of potential. A lot of these technologies are very, very close to prime time,
but not quite. It still took me a month to learn Lean and so forth. They're still not completely
usable out of the box, but they are beginning to be more and more useful. And in surprising areas,
you wouldn't have expected, say, not theory to benefit from these tools, but they do.
They can't solve math problems on their own, except maybe undergraduate-level homework questions,
maybe, at the current level. But as an assistant, I think they can be very, very useful.
They can generate conjectures. They can uncover connections that you wouldn't normally guess.
Once proof automation becomes easier and scales better, we may be able to do
completely new types of mathematics that we don't do right now. Right now, we
prove theorems one at a time. I mean, we're still kind of craftsmen. We take one theorem and
we prove it, and we take another thing and we prove it. Eventually, you could automate theorem
proofs, like, exploring the entire theorem space of millions of different statements.
Which ones are true? Which ones are obviously false? You could explore the geometry of theorems
themselves. I think we're going to see a lot of different ways to do mathematics,
and we're going to see a lot of different ways to make connections in fields that we
don't currently. And it'll be a lot easier to collaborate and work in different areas of mathematics.
Especially because you can use these tools to compartmentalize all these tasks,
all these big complicated projects into small pieces, and plus also these large language
models actually will become very good at getting humans up to speed on any number of advanced
mathematical topics. Okay. Yeah, but it's still not quite there yet. I would say, for example,
proof formalization, it still takes about 10 to 20 times longer to formalize a proof than to do it
by hand, but it's dropping. And I see no reason why this ratio cannot drop below one.
Eventually, it will become faster to, you know, eventually, when we just explain all our proofs
to GPT, and GPT will generate, you know, it will ask questions whenever we're unclear,
but then it will just generate the latex and the lean for us. And we, you know, eventually,
and, you know, and check our work at the same time. So I think this is all in the future.
All right. So thanks for listening.
Thank you. That was lovely. I think we have a couple minutes for a few short questions.
Is there a microphone?
There will also be a Q&A next door in 204 for a few minutes after when this is over.
Are we using these mics or are we calling on? Yes.
That's great. I can't see the mic from here. So the person I called on can ask the question.
Okay. Sure. So one prediction that some people have bandied about advances with AI-assisted
theorem provers is that we might enter a period where there's a proliferation of
proofs for new theorems that are formally verifiable, but which we don't yet have the
technology to translate into forms that are easily comprehensible by humans. Do you see this being an
issue? Well, it already happens. You know, for example, this Boolean Pythagoras theorem,
no human will ever read that proof. So, I mean, I think it's actually not that scary. I mean,
you know, we rely on big numerical computations already in a lot of your mathematics. Of course,
we would still want to have a human understandable proof. But as the not-example shows, you can
take an incomprehensible computer proof and still analyze it and extract out from it a human proof.
So I think that would be one of the ways you do mathematics in the future is to
clarify computer-assisted proofs and make them human-understandable. Thank you.
Can I ask you over there to ask your question?
My question is kind of speculative, but I wanted to ask your opinion on the rule of human intuition
going forward with this, because what a lot of what you talked about is formalization of human
intuition into formal mathematics. I was wondering if you think that intuitive part of coming up
with the idea for the proof itself could be automated in the near future? Not in the near
future. As I said, these likely models can generate things that resemble intuition,
but it has a lot of garbage. At the very low level of proving like one or two steps in a proof,
we can use these proof assistants to only pick out the good intuition and discard the bad one.
But what large numbers are terrible at right now is differentiating good ideas from bad ideas.
They just generate ideas. So unless there's another breakthrough in AI, I don't think this is going
to happen anytime soon. We'll take one more question from this side. So I was curious about
the need for blueprints. Is it that the system doesn't know enough definitions yet, or is the
proof space too big? Some combination thereof? No, it's more of an organization for the humans.
If you want to coordinate 20 people to work on the same project,
like many of the people who work on these projects, they don't understand the whole proof.
So you need some structure to split it up into really small pieces,
really atomic pieces that are self-contained for individual people to work on. So it's not for
the computer. The computer can compile anything. It's for the humans to break up a complicated
problem into lots of easy pieces. Kind of like how division label works in modern industries,
like factories. Okay, I'm going to invite all the other people waiting to ask questions to
join us in room 204 briefly. And let's thank Terry for a lovely talk.

Hi there. My name is Alan Lee. I'm a computer scientist and software engineer at the Center
for Behavioral Institutions and the Environment at ASU, and I currently lead cyber-infrastructure
development at ComSysNet. I'd like to share some good enough practices that you can use
to improve the reusability and reproducibility of the software you develop to address your
particular research questions. The title of this presentation is a reference to a great
paper by Greg Wilson et al. in the software and data carpentry organizations, first published in
2016 and recently revised here in 2017. I'll include links to relevant articles and web
resources at the end of this presentation. If you have any questions, corrections or comments,
please don't hesitate to reach out to me either via this conference website or email. I'd love to
hear your thoughts. So let's begin. There's been a lot of talk in the hub of recently on
reproducibility and the reproducibility crisis with a particular emphasis on biomedicine,
psychology, and the social sciences. Without really considering the theoretical and methodological
quantities of reproducibility in these disciplines, we can at least make sure that the computational
artifacts that we develop to explore or analyze a given research question are developed, documented,
and archived in ways that help others, including your future self, your colleagues, and graduate
students, to more easily evaluate, reuse, and replicate your work. But first, we should probably
clarify what we mean exactly when we say reproducibility. Goodman, Finnelli, and Ionitis did a great
job of this in 2016, discussing aspects of reproducibility and different scientific research
disciplines with a focus on biomedicine, but they distinguished between methods reproducibility,
results reproducibility, and inferential reproducibility. To use a common cooking metaphor,
methods reproducibility is listing all the steps needed to bake a cake. Results reproducibility is,
did I get the same cake that you did when I followed those steps? And inferential reproducibility,
they claim as the most important and refers to drawing quantitatively similar conclusions
from an independent replication or reanalysis of the original study. And this presentation will
be primarily focusing on methods reproducibility, but it's important to keep results and inferential
reproducibility in mind as well. Of course, our ultimate goal is reuse. Can we build on each
other's work? There have been a number of well-reasoned arguments for making the scientific source
code open and transparent in recent years, so I won't go into these in much detail. But as a
computer scientist and software engineer, I would find it very difficult to properly evaluate any
non-trivial results coming from a set of complex computations without access to the source code.
Narrative or algorithmic descriptions, UML, data flow diagrams, pseudocode, these all helped greatly
in describing what a piece of code is supposed to do. But the source code, its runtime parameterizations,
initial random seeds, etc., these are all critical for evaluating the software.
At CompSysNet, we're committed to making the source code broadly and usefully accessible and
available to others. We want to move beyond simple static archives or code dumps. We want to
release your code base as open source is a great first step, but we can do a lot better than that.
To find out how, let's begin with a quick overview of the FAIR principles for data management,
because after all code is data that just happened to be interpreted by your computer
as executable instructions. The FAIR principles were developed first in 2014 and eventually
were published in 2016 by a working group out of Force 11, an organization focused on
e-scholarship and dissemination communications. The FAIR guiding principles advocate for digital
artifacts and data to be findable, accessible, interoperable, and reusable. In order to be
findable, your code should have a globally unique and eternally persistent identifier.
In practice, this means a permanent URL where someone can visit and find a landing page
with rich descriptive metadata about your computational model, its uses, and so on.
Furthermore, this URL should identify a specific version or release of your software.
When you cite your own computational model or someone else's computational model in the
publication, you should be citing the exact version of the software used to derive your results,
and that should take you to that snapshot, a snapshot of that code base at that time that
shows exactly the state of the code base at the time that you used it. This snapshot could also
include data analysis scripts used to explore the model outputs or generate visualization or figures
as well as other plaintext and descriptive metadata.
Accessibility is primarily a cyber infrastructure or platform concern, which is to say it's our
problem at CompSesNet if you're trying to build a trusted repository for your software. It has
more to do with open protocols to access your metadata and code base that allow for authentication,
authorization, and so on. There is one important point to make, which is that the descriptive
metadata about your computational model should be available even when the underlying data or
code is not available due to licensing restrictions or an embargo. That just means that you should
be aware of what's out there and should be aware that this thing exists and have the options for
contacting authors and getting more information about it. Interoperability in this situation
refers to interoperability of metadata, namely that we use standardized vocabularies for the
descriptive metadata about your computational model. CompSesNet is also collaborating with
CSDMS, the Community Surface Dynamics Modeling System Group at Boulder, as well as other research
groups to establish standardized names for the various inputs, outputs, and process variables
in computational models. CSDMS already has a set of standardized names for their computational
earth systems models and we're actively exploring together ways to connect models
as pluggable components so that an earth systems model can be connected with a behavioral model
and mapping their inputs and outputs onto standardized ontologies of variable types.
This is an active area of research, so if you're interested in this endeavor, please feel free to
reach, contact me directly. One critical piece of reusability is licensing. A clear usage license
is necessary for anyone to reuse your software. The software carpentry groups recommend permissive
open source licenses like the MIT or Apache license. This facilitates maximum reuse. It imposes
very little restrictions besides attribution. If you have concerns about commercial applications
or contributions back to your code base, you might consider the GPL as well. Good documentation
from multiple vantage points of abstraction is also critical to reuse. The documentation should
include steps on how to run your code, important inputs, and the internal processes outputs. The
ODD protocol is a semi-controversial standard in the computational modeling community,
but it's a great starting point. You have to also make sure you include your dependencies.
What software or system or data dependencies does your model have? Also bonus points for
test suites that run your and exercise your computational models and assure and give us
confidence that the internal processes within your model are working correctly.
Consistent also offers a peer review process for computational models. We ask independent reviewers
to verify that a computational model submitted to our model library has well-formatted and
commented code, is fully documented with the ODD protocol or an equivalent narrative documentation,
and working instructions for compiling and executing the computational model.
There's a few more key software engineering practices that can help us arrive at readable,
reusable, and testable code. We can write meaningful variable names and files as always an
important aspect. Keeping your code modular and well encapsulated by breaking problems out into
smaller pieces and short cogent functions is also extremely useful. Adding explanatory comments
at the start of every module and function in the system that describe its inputs and expected outputs
is also incredibly useful. If you find yourself duplicating code, try to replace that duplication
with a parameterized function. In general, just marking the entry point into your application
and walking through the sample code pass is a very useful exercise, and this is something that the
ODD protocol addresses as well. Version control is another extremely important piece of this puzzle.
We'll go into two different methods for this in the coming slides. And for larger projects,
an automated build system with tests and continuous integration is extremely helpful.
A continuous integration system is one that automatically runs on every change to the code
base and tries to compile and run tests against a clean installation of the code base. It's a
great way to keep your software honest, and there's several third-party services that offer this,
including TravisCI, drone.io, or Jenkins, and we can talk more about this on the conference website
if you're interested. The first approach to version control is manual one. This is one
advocated by the Good Enough Practices paper by Wilson et al. that I referenced earlier.
You don't need any new tools. You just need to be systematic. You can keep your changelodge.txt
file dated and organized, and you'll copy your entire project folder on every significant change.
A more sophisticated way is to use an actual version control system, like Git or Mercurial or
SVN. All of these are great version control systems. If you pick one, you won't go wrong.
This will have a huge payoff down the road as it gives you the confidence to experiment with
your software knowing that you can always revert back to your previous state. It also gives you
the ability to associate meaningful log messages with changes in your code base and to also associate
tags and releases with your code base that help keep track of important milestones. This is critical
for actually citing the specific version of your software. With the model library at CompSysNet,
we also offer the ability to capture snapshots, but we want to have tight integration with Git
as well. We'll have more on that later.
When you do use a version control system, there are a few good practices to keep in mind. You
should do your best to keep your changes small and isolated. If you modify every single file in
your code base and push that as a commit, it's harder to see what's going on. Keeping your changes
small and isolated, committing early and committing often helps you be exercise disciplined version
control. That way, you can easily see the changes that were made to the code and tie that in with
semantic and descriptive log messages that describe and document the intent of those changes.
It can also be very helpful to adopt a standardized directory layout. Here's a simple example where
we organize our source code, documents, input data, and output data in different directories
alongside plain text files that describe how our software should be cited. There's also a
readme that describes what the code does and an explicit license file.
Archiving data that your computational models depend on is also critical. You should always
save your data in its raw form, whether it be from a server or instrument, and never overwrite
raw data with its intermediate forms, any sort of processing that you perform on it. If your
data sets are extremely large, you'll have to find some way to generate permanent URLs to those
data sets. You can do this at OSF or FigShare. Otherwise, you can certainly include them in
line in your code base, as we had in that previous file system layout.
Input data that your software depends on should almost certainly be citably archived as well.
When we talk about durable formats, it's okay to have Excel sheets
while you're working with your data, but when you archive your data, it should be archived
in a durable format. It should be plain text, it should be CSV. This allows machines to read them
more usefully 20 years from now. Who knows if the version of Excel that you use to save your
file will still be readable, but a plain text file will still be readable.
If you manipulated your data manually, you should document what you did
in a data changelog that records the changes you made, their intent, and also preserves the
before and after. Ideally, though, you should be scripting your data processing instead of
hand-munging Excel sheets. This allows others to run your data analysis pipeline from the
command line. It allows continuous integration to work, and it also just enables people to
generate the same outputs you published with minimal effort. Analysis-friendly data is another
topic that Wilson and I all discuss. They describe two fundamental principles that I think are
extremely useful. The first one is to make each column a variable. Putting multiple variables
into a single column makes programmatic analysis a lot messier, just like storing units in a variable.
So instead of storing 3.7kg, you might instead name the units in your metadata instead.
You should also make each row an observation. This is the second principle. You might have one row
representing an observation at a field site with many columns. Each column is a time-point measurement
until the end of data collection. This is usually done to facilitate data entry, but it's much easier
to programmatically analyze when you convert time into a variable itself, so that the year,
for instance, in this case, is a column. You should also make sure that any dependencies that your
code has are clearly documented. Anytime you have to make a change to your local system to run your
code, you should be recording what that change was. An easy way to actually keep yourself honest
here is to occasionally spin up a fresh virtual machine using VirtualBox or VMware or any other
free virtual machine tools and try and install and run your computational model on that fresh
virtual machine. This almost guarantees that any additional system libraries, third-party libraries,
or data dependencies that need to be set up or installed will come to light as you try and
build and run your computational model. It is time-consuming, however. Docker is another promising
technology that we at CompSetsNet are using internally and experimenting with as a provided
or curated service for model authors. Docker offers the reproducibility of a virtual machine
but in a much more lightweight format. It doesn't carry all the baggage of a virtual machine.
A common metaphor used to distinguish between Docker containers and virtual machines is if you
think of a virtual machine as a house with its own plumbing, Docker is like an apartment
that's a member of a large apartment complex. It shares this plumbing and other infrastructure
with the other apartments. Writing a Docker file is kind of like creating a recipe or
playbook that declares all of your application's system and software dependencies and how they
should be wired together for a successfully running application. We don't have time to go into
Docker in depth in this presentation. I'd be happy to discuss further offline.
To recap, always make sure you budget the time needed to properly describe your
computational artifacts, ideally in machine-readable ways and automated. Just like gardening or
cleaning, continuous and incremental work with forethought and planning is much easier to manage
than leaving everything to the end and trying to tile the loose ends. Automating your computational
pipelines as much as you can and whatever changes you can automate, well, those should just be
clearly described and documented. Lorena Barba is very adamant about automation, but I think
to start with just being explicit and documenting what you've done is a good enough first step.
Good code-based management practices in archiving your computational models in ways that facilitate
reproducibility and reuse is not really the goal in and of itself, but it's a critical step
for those of us that use computation as virtual laboratories. It helps us to better evaluate
and build on each other's work, and it's just the right thing to do if we want to be able to
move forward as a discipline. I hope this presentation has been useful and thanks for your time.
